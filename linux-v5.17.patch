From 19dd84a71b66821d1007e4d9f14097d53eac119e Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 22 Aug 2022 18:01:20 -0700
Subject: [PATCH 1/1] linux v5.17

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/vgic/vgic-debug.c              |   4 +
 arch/arm64/kvm/vgic/vgic-init.c               |   7 +
 arch/x86/events/core.c                        |  18 +
 arch/x86/events/intel/core.c                  |  39 +
 arch/x86/include/asm/kvm_host.h               | 391 +++++++++
 arch/x86/include/asm/kvm_para.h               |  24 +
 arch/x86/include/asm/msr.h                    |   8 +
 arch/x86/include/asm/pvclock-abi.h            |  15 +
 arch/x86/include/asm/pvclock.h                |   4 +
 arch/x86/kernel/apic/vector.c                 |  44 +
 arch/x86/kernel/irq.c                         |  16 +
 arch/x86/kernel/kvm.c                         | 124 +++
 arch/x86/kernel/kvmclock.c                    |  21 +
 arch/x86/kernel/paravirt.c                    |   7 +
 arch/x86/kernel/pvclock.c                     |  17 +
 arch/x86/kernel/smpboot.c                     |  18 +
 arch/x86/kernel/tsc.c                         |  10 +
 arch/x86/kernel/tsc_sync.c                    |  30 +
 arch/x86/kvm/cpuid.c                          |  78 ++
 arch/x86/kvm/cpuid.h                          |   5 +
 arch/x86/kvm/emulate.c                        | 135 +++
 arch/x86/kvm/ioapic.c                         |  45 +
 arch/x86/kvm/irq.c                            |   5 +
 arch/x86/kvm/irq_comm.c                       |  21 +
 arch/x86/kvm/kvm_cache_regs.h                 |  32 +
 arch/x86/kvm/kvm_emulate.h                    |  14 +
 arch/x86/kvm/lapic.c                          | 195 +++++
 arch/x86/kvm/lapic.h                          |  39 +
 arch/x86/kvm/mmu.h                            | 106 +++
 arch/x86/kvm/mmu/mmu.c                        | 804 ++++++++++++++++++
 arch/x86/kvm/mmu/mmu_audit.c                  |   8 +
 arch/x86/kvm/mmu/mmu_internal.h               | 100 +++
 arch/x86/kvm/mmu/paging_tmpl.h                |   6 +
 arch/x86/kvm/mmu/spte.c                       |  60 ++
 arch/x86/kvm/mmu/spte.h                       |  61 ++
 arch/x86/kvm/mmu/tdp_iter.c                   | 109 +++
 arch/x86/kvm/mmu/tdp_iter.h                   |  39 +
 arch/x86/kvm/mmu/tdp_mmu.c                    | 487 +++++++++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |  54 ++
 arch/x86/kvm/pmu.c                            | 111 +++
 arch/x86/kvm/pmu.h                            |  67 ++
 arch/x86/kvm/svm/sev.c                        |   4 +
 arch/x86/kvm/vmx/capabilities.h               |  36 +
 arch/x86/kvm/vmx/nested.h                     |   6 +
 arch/x86/kvm/vmx/pmu_intel.c                  |  75 ++
 arch/x86/kvm/vmx/posted_intr.c                | 124 +++
 arch/x86/kvm/vmx/vmx.c                        | 215 +++++
 arch/x86/kvm/vmx/vmx.h                        |  44 +
 arch/x86/kvm/x86.c                            | 702 +++++++++++++++
 arch/x86/kvm/x86.h                            |  13 +
 block/blk-map.c                               |  47 +
 block/blk-merge.c                             |  10 +
 block/blk-settings.c                          |  40 +
 drivers/acpi/acpi_processor.c                 |  11 +
 drivers/acpi/acpica/evmisc.c                  |   4 +
 drivers/acpi/bus.c                            |   4 +
 drivers/acpi/osl.c                            |  17 +
 drivers/acpi/scan.c                           |  45 +
 drivers/cpuidle/cpuidle-haltpoll.c            |  44 +
 drivers/cpuidle/cpuidle.c                     |  42 +
 drivers/cpuidle/governors/haltpoll.c          |  56 ++
 drivers/cpuidle/poll_state.c                  |  11 +
 drivers/net/virtio_net.c                      |  29 +
 drivers/pci/msi/msi.c                         |   8 +
 drivers/pci/pci-driver.c                      |   5 +
 drivers/scsi/scsi_ioctl.c                     |  10 +
 drivers/vfio/vfio_iommu_type1.c               |  10 +
 drivers/vhost/scsi.c                          |   4 +
 drivers/vhost/vhost.c                         |  96 +++
 drivers/vhost/vhost.h                         |  27 +
 drivers/virtio/virtio_ring.c                  |  54 ++
 drivers/xen/swiotlb-xen.c                     |   5 +
 fs/userfaultfd.c                              |  12 +
 include/linux/cpuidle.h                       |  28 +
 include/linux/kvm_dirty_ring.h                |  10 +
 include/linux/kvm_host.h                      | 142 ++++
 include/linux/virtio_config.h                 |   9 +
 include/linux/virtio_net.h                    |  11 +
 include/linux/vmstat.h                        |  10 +
 include/uapi/linux/kvm.h                      |   8 +
 kernel/cgroup/cpuset.c                        |  52 ++
 kernel/cpu.c                                  |  10 +
 kernel/dma/swiotlb.c                          |  81 ++
 kernel/events/core.c                          |  13 +
 kernel/irq/manage.c                           |   3 +
 kernel/power/process.c                        |  15 +
 kernel/power/suspend.c                        |   3 +
 kernel/sched/core.c                           |   4 +
 kernel/sched/core_sched.c                     |   6 +
 kernel/sched/idle.c                           |   4 +
 kernel/time/clocksource.c                     |  29 +
 kernel/time/tick-sched.c                      |   6 +
 kernel/time/timekeeping.c                     |  24 +
 lib/iov_iter.c                                |  10 +
 mm/gup.c                                      |   4 +
 net/core/dev.c                                |   3 +
 net/packet/af_packet.c                        |   5 +
 sound/core/timer.c                            |   4 +
 .../selftests/kvm/kvm_page_table_test.c       |  20 +
 tools/testing/selftests/kvm/lib/guest_modes.c |  10 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |   6 +
 virt/kvm/async_pf.c                           |   6 +
 virt/kvm/dirty_ring.c                         |  24 +
 virt/kvm/eventfd.c                            |   7 +
 virt/kvm/kvm_main.c                           | 433 ++++++++++
 virt/kvm/pfncache.c                           |  35 +
 106 files changed, 6223 insertions(+)

diff --git a/arch/arm64/kvm/vgic/vgic-debug.c b/arch/arm64/kvm/vgic/vgic-debug.c
index f38c40a76251..0ad6b9fa9591 100644
--- a/arch/arm64/kvm/vgic/vgic-debug.c
+++ b/arch/arm64/kvm/vgic/vgic-debug.c
@@ -269,6 +269,10 @@ static const struct seq_operations vgic_debug_sops = {
 
 DEFINE_SEQ_ATTRIBUTE(vgic_debug);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|320| <<vgic_init>> vgic_debug_init(kvm);
+ */
 void vgic_debug_init(struct kvm *kvm)
 {
 	debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index fc00304fe7d8..a73042109880 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -252,6 +252,13 @@ static void kvm_vgic_vcpu_enable(struct kvm_vcpu *vcpu)
  * vgic_initialized() returns true when this function has succeeded.
  * Must be called with kvm->lock held!
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|413| <<vgic_lazy_init>> ret = vgic_init(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|214| <<vgic_set_common_attr>> r = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|372| <<vgic_v2_attr_regs_access>> ret = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|309| <<vgic_v2_map_resources>> ret = vgic_init(kvm);
+ */
 int vgic_init(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index e686c5e0537b..df359a622ed5 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -647,6 +647,10 @@ int x86_pmu_hw_config(struct perf_event *event)
 /*
  * Setup the hardware configuration for a given attr_type
  */
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2464| <<x86_pmu_event_init>> err = __x86_pmu_event_init(event);
+ */
 static int __x86_pmu_event_init(struct perf_event *event)
 {
 	int err;
@@ -669,6 +673,9 @@ static int __x86_pmu_event_init(struct perf_event *event)
 	event->hw.extra_reg.idx = EXTRA_REG_NONE;
 	event->hw.branch_reg.idx = EXTRA_REG_NONE;
 
+	/*
+	 * 对于ice是hsw_hw_config()
+	 */
 	return x86_pmu.hw_config(event);
 }
 
@@ -2369,6 +2376,10 @@ static int validate_event(struct perf_event *event)
 	if (IS_ERR(fake_cpuc))
 		return PTR_ERR(fake_cpuc);
 
+	/*
+	 * hsw_get_event_constraints()
+	 * icl_get_event_constraints()
+	 */
 	c = x86_pmu.get_event_constraints(fake_cpuc, 0, event);
 
 	if (!c || !c->weight)
@@ -2445,6 +2456,9 @@ static int validate_group(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * struct pmu pmu.event_init = x86_pmu_event_init()
+ */
 static int x86_pmu_event_init(struct perf_event *event)
 {
 	struct x86_hybrid_pmu *pmu = NULL;
@@ -2992,6 +3006,10 @@ void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
 	cap->num_counters_fixed	= x86_pmu.num_counters_fixed;
 	cap->bit_width_gp	= x86_pmu.cntval_bits;
 	cap->bit_width_fixed	= x86_pmu.cntval_bits;
+	/*
+	 * 对于一般的x86_pmu.events_maskl来自下面.
+	 *   - arch/x86/events/intel/core.c|5655| <<intel_pmu_init>> x86_pmu.events_maskl = ebx.full;
+	 */
 	cap->events_mask	= (unsigned int)x86_pmu.events_maskl;
 	cap->events_mask_len	= x86_pmu.events_mask_len;
 }
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index a3c7ca876aeb..276e022323e1 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -3224,11 +3224,38 @@ struct event_constraint *
 x86_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
 			  struct perf_event *event)
 {
+	/*
+	 * intel_icl_event_constraints????
+	 *
+	 * x86_pmu
+	 * -> struct event_constraint *event_constraints;
+	 */
 	struct event_constraint *event_constraints = hybrid(cpuc->pmu, event_constraints);
 	struct event_constraint *c;
 
 	if (event_constraints) {
 		for_each_event_constraint(c, event_constraints) {
+			/*
+			 * 146 struct hw_perf_event {
+			 * 147 #ifdef CONFIG_PERF_EVENTS
+			 * 148         union {
+			 * 149                 struct { // hardware
+			 * 150                         u64             config;
+			 * 151                         u64             last_tag;
+			 * 152                         unsigned long   config_base;
+			 * 153                         unsigned long   event_base;
+			 * 154                         int             event_base_rdpmc;
+			 * 155                         int             idx;
+			 * 156                         int             last_cpu;
+			 * 157                         int             flags;
+			 * 158 
+			 * 159                         struct hw_perf_event_extra extra_reg;
+			 * 160                         struct hw_perf_event_extra branch_reg;
+			 * 161                 };
+			 *
+			 * struct perf_event *event:
+			 * -> struct hw_perf_event hw;
+			 */
 			if (constraint_match(c, event->hw.config)) {
 				event->hw.flags |= c->flags;
 				return c;
@@ -4041,6 +4068,14 @@ static struct event_constraint fixed0_constraint =
 static struct event_constraint fixed0_counter0_constraint =
 			INTEL_ALL_EVENT_CONSTRAINT(0, 0x100000001ULL);
 
+/*
+ * called by:
+ *   - arch/x86/events/intel/core.c|4101| <<icl_get_event_constraints>> return hsw_get_event_constraints(cpuc, idx, event);
+ *   - arch/x86/events/intel/core.c|4173| <<tfa_get_event_constraints>> struct event_constraint *c = hsw_get_event_constraints(cpuc, idx, event);
+ *   - arch/x86/events/intel/core.c|6015| <<intel_pmu_init>> x86_pmu.get_event_constraints = hsw_get_event_constraints;
+ *   - arch/x86/events/intel/core.c|6057| <<intel_pmu_init>> x86_pmu.get_event_constraints = hsw_get_event_constraints;
+ *   - arch/x86/events/intel/core.c|6119| <<intel_pmu_init>> x86_pmu.get_event_constraints = hsw_get_event_constraints;
+ */
 static struct event_constraint *
 hsw_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
 			  struct perf_event *event)
@@ -5574,6 +5609,10 @@ static void intel_pmu_check_hybrid_pmus(u64 fixed_mask)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2080| <<init_hw_perf_events>> err = intel_pmu_init();
+ */
 __init int intel_pmu_init(void)
 {
 	struct attribute **extra_skl_attr = &empty_attrs;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ec9830d2aabf..974ab8651327 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -129,10 +129,36 @@
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) = (1 - 1) * 9 =  0
+ * KVM_HPAGE_GFN_SHIFT(2) = (2 - 1) * 9 =  9
+ * KVM_HPAGE_GFN_SHIFT(3) = (3 - 1) * 9 = 18
+ * KVM_HPAGE_GFN_SHIFT(4) = (4 - 1) * 9 = 27
+ * KVM_HPAGE_GFN_SHIFT(5) = (5 - 1) * 9 = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) = 12 + 27 = 38
+ * KVM_HPAGE_SHIFT(5) = 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+ * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) = KVM_HPAGE_SIZE(1) / 4096 = 4K / 4K = 1
+ * KVM_PAGES_PER_HPAGE(2) = KVM_HPAGE_SIZE(2) / 4096 = 2M / 4K = 512
+ * KVM_PAGES_PER_HPAGE(3) = KVM_HPAGE_SIZE(3) / 4096 = 1G / 4K = 262144
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -436,8 +462,22 @@ struct kvm_mmu {
 	gpa_t root_pgd;
 	union kvm_mmu_role mmu_role;
 	u8 root_level;
+	/*
+	 * 在以下设置kvm_mmu->shadow_root_level:
+	 *   - arch/x86/kvm/mmu/mmu.c|5432| <<init_kvm_tdp_mmu>> context->shadow_root_level = kvm_mmu_get_tdp_level(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|5500| <<shadow_mmu_init_context>> context->shadow_root_level = new_role.base.level;
+	 *   - arch/x86/kvm/mmu/mmu.c|5588| <<kvm_init_shadow_ept_mmu>> context->shadow_root_level = level;
+	 */
 	u8 shadow_root_level;
 	u8 ept_ad;
+	/*
+	 * 在以下设置kvm_mmu->direct_map:
+	 *   - arch/x86/kvm/mmu/mmu.c|4666| <<nonpaging_init_context>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|5233| <<paging64_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu/mmu.c|5242| <<paging32_init_context>> context->direct_map = false;
+	 *   - arch/x86/kvm/mmu/mmu.c|5332| <<init_kvm_tdp_mmu>> context->direct_map = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|5495| <<kvm_init_shadow_ept_mmu>> context->direct_map = false;
+	 */
 	bool direct_map;
 	struct kvm_mmu_root_info prev_roots[KVM_MMU_NUM_PREV_ROOTS];
 
@@ -457,6 +497,17 @@ struct kvm_mmu {
 	u32 pkru_mask;
 
 	u64 *pae_root;
+	/*
+	 * 在以下使用kvm_mmu->pml4_root:
+	 *   - arch/x86/kvm/mmu/mmu.c|3989| <<mmu_alloc_shadow_roots>> if (WARN_ON_ONCE(!mmu->pml4_root)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|3993| <<mmu_alloc_shadow_roots>> mmu->pml4_root[0] = __pa(mmu->pae_root) | pm_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|4000| <<mmu_alloc_shadow_roots>> mmu->pml5_root[0] = __pa(mmu->pml4_root) | pm_mask;
+	 *   - arch/x86/kvm/mmu/mmu.c|4023| <<mmu_alloc_shadow_roots>> mmu->root_hpa = __pa(mmu->pml4_root);
+	 *   - arch/x86/kvm/mmu/mmu.c|4060| <<mmu_alloc_special_roots>> if (mmu->pae_root && mmu->pml4_root && (!need_pml5 || mmu->pml5_root))
+	 *   - arch/x86/kvm/mmu/mmu.c|4067| <<mmu_alloc_special_roots>> if (WARN_ON_ONCE(!tdp_enabled || mmu->pae_root || mmu->pml4_root ||
+	 *   - arch/x86/kvm/mmu/mmu.c|4092| <<mmu_alloc_special_roots>> mmu->pml4_root = pml4_root;
+	 *   - arch/x86/kvm/mmu/mmu.c|6066| <<free_mmu_pages>> free_page((unsigned long )mmu->pml4_root);
+	 */
 	u64 *pml4_root;
 	u64 *pml5_root;
 
@@ -527,6 +578,15 @@ struct kvm_pmu {
 	 * The total number of programmed perf_events and it helps to avoid
 	 * redundant check before cleanup if guest don't use vPMU at all.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - arch/x86/kvm/pmu.c|140| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|464| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|69| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|268| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|319| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12187| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 */
 	u8 event_count;
 };
 
@@ -619,7 +679,26 @@ struct kvm_vcpu_arch {
 	 * kvm_{register,rip}_{read,write} functions.
 	 */
 	unsigned long regs[NR_VCPU_REGS];
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_avail:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|56| <<kvm_register_is_available>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|68| <<kvm_register_mark_available>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|74| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/svm/svm.c|3848| <<svm_vcpu_run>> vcpu->arch.regs_avail &= ~SVM_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/nested.c|273| <<vmx_switch_vmcs>> vcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/vmx.c|7050| <<vmx_vcpu_run>> vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/x86.c|11526| <<kvm_arch_vcpu_create>> vcpu->arch.regs_avail = ~0;
+	 */
 	u32 regs_avail;
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|62| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|75| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|3813| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|279| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6948| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|11527| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	u32 regs_dirty;
 
 	unsigned long cr0;
@@ -640,7 +719,30 @@ struct kvm_vcpu_arch {
 	bool load_eoi_exitmap_pending;
 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
 	unsigned long apic_attention;
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_arb_prio:
+	 *   - arch/x86/kvm/lapic.c|1089| <<__apic_accept_irq>> vcpu->arch.apic_arb_prio++;
+	 *   - arch/x86/kvm/lapic.c|1225| <<kvm_apic_compare_prio>> return vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;
+	 *   - arch/x86/kvm/lapic.c|2537| <<kvm_lapic_reset>> vcpu->arch.apic_arb_prio = 0;
+	 *   - arch/x86/kvm/lapic.c|2813| <<kvm_apic_set_state>> vcpu->arch.apic_arb_prio = 0;
+	 */
 	int32_t apic_arb_prio;
+	/*
+	 * 在以下设置kvm_vcpu_arch->mp_state:
+	 *   - arch/x86/kvm/lapic.c|3092| <<kvm_apic_accept_events>> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 *   - arch/x86/kvm/lapic.c|3094| <<kvm_apic_accept_events>> vcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;
+	 *   - arch/x86/kvm/lapic.c|3103| <<kvm_apic_accept_events>> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 *   - arch/x86/kvm/svm/nested.c|813| <<nested_svm_vmexit>> svm->vcpu.arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 *   - arch/x86/kvm/vmx/nested.c|3642| <<nested_vmx_run>> vcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;
+	 *   - arch/x86/kvm/vmx/nested.c|4626| <<nested_vmx_vmexit>> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 *   - arch/x86/kvm/x86.c|9398| <<__kvm_emulate_halt>> vcpu->arch.mp_state = state;
+	 *   - arch/x86/kvm/x86.c|11283| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;
+	 *   - arch/x86/kvm/x86.c|11286| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu->arch.mp_state = mp_state->mp_state;
+	 *   - arch/x86/kvm/x86.c|11410| <<__set_sregs_common>> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 *   - arch/x86/kvm/x86.c|11699| <<kvm_arch_vcpu_create>> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 *   - arch/x86/kvm/x86.c|11701| <<kvm_arch_vcpu_create>> vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
+	 *   - arch/x86/kvm/x86.c|13027| <<kvm_arch_async_page_present>> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+	 */
 	int mp_state;
 	u64 ia32_misc_enable_msr;
 	u64 smbase;
@@ -742,6 +844,31 @@ struct kvm_vcpu_arch {
 	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
+	/*
+	 * 在以下使用complete_userspace_io():
+	 *   - arch/x86/kvm/x86.c|8759| <<x86_emulate_instruction>> } else if (vcpu->arch.complete_userspace_io) {
+	 *   - arch/x86/kvm/x86.c|10930| <<kvm_arch_vcpu_ioctl_run>> if (unlikely(vcpu->arch.complete_userspace_io)) {
+	 *   - arch/x86/kvm/x86.c|10931| <<kvm_arch_vcpu_ioctl_run>> int (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;
+	 * 在以下设置complete_userspace_io():
+	 *   - arch/x86/kvm/hyperv.c|328| <<syndbg_exit>> vcpu->arch.complete_userspace_io = kvm_hv_syndbg_complete_userspace;
+	 *   - arch/x86/kvm/hyperv.c|2246| <<kvm_hv_hypercall>> vcpu->arch.complete_userspace_io = kvm_hv_hypercall_complete_userspace;
+	 *   - arch/x86/kvm/hyperv.c|2315| <<kvm_hv_hypercall>> vcpu->arch.complete_userspace_io = kvm_hv_hypercall_complete_userspace;
+	 *   - arch/x86/kvm/x86.c|1976| <<kvm_msr_user_space>> vcpu->arch.complete_userspace_io = completion;
+	 *   - arch/x86/kvm/x86.c|8749| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_pio;
+	 *   - arch/x86/kvm/x86.c|8758| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|8863| <<kvm_fast_pio_out>> vcpu->arch.complete_userspace_io = complete_fast_pio_out_port_0x7e;
+	 *   - arch/x86/kvm/x86.c|8868| <<kvm_fast_pio_out>> vcpu->arch.complete_userspace_io = complete_fast_pio_out;
+	 *   - arch/x86/kvm/x86.c|8914| <<kvm_fast_pio_in>> vcpu->arch.complete_userspace_io = complete_fast_pio_in;
+	 *   - arch/x86/kvm/x86.c|9576| <<kvm_emulate_hypercall>> vcpu->arch.complete_userspace_io = complete_hypercall_exit;
+	 *   - arch/x86/kvm/x86.c|10839| <<complete_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|10932| <<kvm_arch_vcpu_ioctl_run>> vcpu->arch.complete_userspace_io = NULL;
+	 *   - arch/x86/kvm/x86.c|13240| <<complete_sev_es_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|13278| <<kvm_sev_es_mmio_write>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|13316| <<kvm_sev_es_mmio_read>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_mmio;
+	 *   - arch/x86/kvm/x86.c|13355| <<kvm_sev_es_outs>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_outs;
+	 *   - arch/x86/kvm/x86.c|13396| <<kvm_sev_es_ins>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_ins;
+	 *   - arch/x86/kvm/xen.c|879| <<kvm_xen_hypercall>> vcpu->arch.complete_userspace_io = kvm_xen_hypercall_complete_userspace;
+	 */
 	int (*complete_userspace_io)(struct kvm_vcpu *vcpu);
 
 	gpa_t time;
@@ -759,19 +886,69 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache cache;
 	} st;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2485| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 */
 	u64 l1_tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|574| <<nested_vmcb02_prepare_control>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+	 *   - arch/x86/kvm/svm/nested.c|864| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2542| <<prepare_vmcs02>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+	 *   - arch/x86/kvm/vmx/nested.c|3500| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4551| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2501| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+	 *   - arch/x86/kvm/x86.c|2506| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = l1_offset;
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|34| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|579| <<nested_vmcb02_prepare_control>> svm->vmcb->control.tsc_offset = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|865| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|866| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2551| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4590| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_vcpu_write_tsc_offset>> static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/x86.c|3980| <<kvm_get_msr_common>> offset = vcpu->arch.tsc_offset;
+	 */
 	u64 tsc_offset; /* current tsc offset */
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2554| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3128| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10291| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4633| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2306| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3099| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4635| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2399| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|9443| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|10697| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
 	u32 virtual_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+	 *   - arch/x86/kvm/x86.c|3532| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+	 *   - arch/x86/kvm/x86.c|3539| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+	 *   - arch/x86/kvm/x86.c|3567| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+	 *   - arch/x86/kvm/x86.c|3932| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+	 */
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
 	u64 l1_tsc_scaling_ratio;
@@ -781,6 +958,13 @@ struct kvm_vcpu_arch {
 	unsigned nmi_pending; /* NMI queued after currently running handler */
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 	bool smi_pending;    /* SMI queued after currently running handler */
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1796| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|359| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|364| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|369| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	u8 handling_intr_from_guest;
 
 	struct kvm_mtrr mtrr_state;
@@ -847,6 +1031,13 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache data;
 	} pv_eoi;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_kvm_poll_control:
+	 *   - arch/x86/kvm/x86.c|3647| <<kvm_set_msr_common>> vcpu->arch.msr_kvm_poll_control = data;
+	 *   - arch/x86/kvm/x86.c|3988| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_kvm_poll_control;
+	 *   - arch/x86/kvm/x86.c|11275| <<kvm_arch_vcpu_postcreate>> vcpu->arch.msr_kvm_poll_control = 1;
+	 *   - arch/x86/kvm/x86.c|12602| <<kvm_arch_no_poll>> return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
+	 */
 	u64 msr_kvm_poll_control;
 
 	/*
@@ -1047,14 +1238,49 @@ struct kvm_x86_msr_filter {
 #define APICV_INHIBIT_REASON_ABSENT	7
 
 struct kvm_arch {
+	/*
+	 * 在以下使用kvm_arch->n_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1720| <<kvm_mod_used_mmu_pages>> kvm->arch.n_used_mmu_pages += nr;
+	 *   - arch/x86/kvm/mmu/mmu.c|2589| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2591| <<kvm_mmu_available_pages>> kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|2627| <<kvm_mmu_change_mmu_pages>> if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2628| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2631| <<kvm_mmu_change_mmu_pages>> goal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|6232| <<mmu_shrink_scan>> if (!kvm->arch.n_used_mmu_pages &&
+	 */
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
+	/*
+	 * 在以下使用kvm_arch->n_max_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|2727| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2728| <<kvm_mmu_available_pages>> return kvm->arch.n_max_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2779| <<kvm_mmu_change_mmu_pages>> kvm->arch.n_max_mmu_pages = goal_nr_mmu_pages;
+	 *   - arch/x86/kvm/x86.c|5728| <<kvm_vm_ioctl_get_nr_mmu_pages>> return kvm->arch.n_max_mmu_pages;
+	 */
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
+	/*
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|299| <<__field>> __entry->mmu_valid_gen = kvm->arch.mmu_valid_gen;
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmutrace.h|19| <<KVM_MMU_PAGE_ASSIGN>> __entry->mmu_valid_gen = sp->mmu_valid_gen; \
+	 */
 	u8 mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
+	/*
+	 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|947| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6641| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+	 *   - arch/x86/kvm/x86.c|11639| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+	 */
 	struct list_head lpage_disallowed_mmu_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1085,37 +1311,133 @@ struct kvm_arch {
 	struct rw_semaphore apicv_update_lock;
 
 	bool apic_access_memslot_enabled;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9032| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9041| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9044| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9733| <<__kvm_request_apicv_update>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|9755| <<__kvm_request_apicv_update>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|9761| <<__kvm_request_apicv_update>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
 
+	/*
+	 * 在以下使用kvm_arch->mwait_in_guest:
+	 *   - arch/x86/kvm/x86.c|6216| <<kvm_vm_ioctl_enable_cap>> kvm->arch.mwait_in_guest = true;
+	 *   - arch/x86/kvm/x86.h|332| <<kvm_mwait_in_guest>> return kvm->arch.mwait_in_guest;
+	 */
 	bool mwait_in_guest;
 	bool hlt_in_guest;
 	bool pause_in_guest;
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下设置kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/x86.c|6312| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|11826| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|361| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2941| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2947| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3127| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 */
 	s64 kvmclock_offset;
 
 	/*
 	 * This also protects nr_vcpus_matched_tsc which is read from a
 	 * preemption-disabled region, so it must be a raw spinlock.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2601| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2656| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2912| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2944| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|2963| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|5340| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11939| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|11941| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|11944| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11946| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2611| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2666| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|11802| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2612| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|2677| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|11803| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2609| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2693| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|5343| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 */
 	u32 last_tsc_khz;
 	u64 last_tsc_offset;
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2596| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2662| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 */
 	u64 cur_tsc_offset;
 	u64 cur_tsc_generation;
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|2917| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|2934| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3006| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3008| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3101| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3114| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|11895| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_t pvclock_sc;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2828| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3020| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3127| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3256| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6553| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|3021| <<pvclock_update_vm_gtod_copy>> &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3126| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3255| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 */
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3185| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3201| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11839| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11885| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
 	struct delayed_work kvmclock_sync_work;
 
@@ -1158,6 +1480,11 @@ struct kvm_arch {
 	bool exit_on_emulation_error;
 
 	/* Deflect RDMSR and WRMSR to user space when they trigger a #GP */
+	/*
+	 * 在以下使用kvm_arch->user_space_msr_mask:
+	 *   - arch/x86/kvm/x86.c|1974| <<kvm_msr_user_space>> if (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))
+	 *   - arch/x86/kvm/x86.c|6270| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X86_USER_SPACE_MSR)>> kvm->arch.user_space_msr_mask = cap->args[0];
+	 */
 	u32 user_space_msr_mask;
 	struct kvm_x86_msr_filter __rcu *msr_filter;
 
@@ -1177,6 +1504,12 @@ struct kvm_arch {
 	 * true, TDP MMU handler functions will run for various MMU
 	 * operations.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_enabled:
+	 *   - arch/x86/kvm/mmu.h|328| <<is_tdp_mmu_enabled>> static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mmu_enabled; }
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|23| <<kvm_mmu_init_tdp_mmu>> kvm->arch.tdp_mmu_enabled = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|56| <<kvm_mmu_uninit_tdp_mmu>> if (!kvm->arch.tdp_mmu_enabled)
+	 */
 	bool tdp_mmu_enabled;
 
 	/*
@@ -1218,6 +1551,19 @@ struct kvm_arch {
 	 * It is acceptable, but not necessary, to acquire this lock when
 	 * the thread holds the MMU lock in write mode.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_pages_lock:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|32| <<kvm_mmu_init_tdp_mmu>> spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|134| <<kvm_tdp_mmu_put_root>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<kvm_tdp_mmu_put_root>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<for_each_tdp_mmu_root>> lockdep_is_held(&kvm->arch.tdp_mmu_pages_lock)) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|318| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|378| <<tdp_mmu_link_page>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|382| <<tdp_mmu_link_page>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<tdp_mmu_unlink_page>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|411| <<tdp_mmu_unlink_page>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 */
 	spinlock_t tdp_mmu_pages_lock;
 #endif /* CONFIG_X86_64 */
 
@@ -1226,6 +1572,11 @@ struct kvm_arch {
 	 * is used as one input when determining whether certain memslot
 	 * related allocations are necessary.
 	 */
+	/*
+	 * 在以下使用kvm_arch->shadow_root_allocated:
+	 *   - arch/x86/kvm/mmu.h|345| <<kvm_shadow_root_allocated>> return smp_load_acquire(&kvm->arch.shadow_root_allocated);
+	 *   - arch/x86/kvm/mmu/mmu.c|3721| <<mmu_first_shadow_root_alloc>> smp_store_release(&kvm->arch.shadow_root_allocated, true);
+	 */
 	bool shadow_root_allocated;
 
 #if IS_ENABLED(CONFIG_HYPERV)
@@ -1570,6 +1921,13 @@ static inline int kvm_arch_flush_remote_tlb(struct kvm *kvm)
 		return -ENOTSUPP;
 }
 
+/*
+ * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+ *   - arch/x86/include/asm/kvm_host.h|1796| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+ *   - arch/x86/kvm/x86.h|359| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+ *   - arch/x86/kvm/x86.h|364| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+ *   - arch/x86/kvm/x86.h|369| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+ */
 #define kvm_arch_pmi_in_guest(vcpu) \
 	((vcpu) && (vcpu)->arch.handling_intr_from_guest)
 
@@ -1823,6 +2181,32 @@ static inline unsigned long read_msr(unsigned long msr)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|674| <<nested_svm_vmrun>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/nested.c|686| <<nested_svm_vmrun>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/nested.c|799| <<nested_svm_vmexit>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/nested.c|1119| <<nested_svm_check_permissions>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|1996| <<vmload_vmsave_interception>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|4463| <<svm_can_emulate_instruction>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/nested.c|3333| <<nested_vmx_check_permission>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4953| <<handle_vmon>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4962| <<handle_vmon>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|49| <<sgx_get_encls_gva>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|134| <<sgx_inject_fault>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|168| <<__handle_encls_ecreate>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|178| <<__handle_encls_ecreate>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|186| <<__handle_encls_ecreate>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|376| <<handle_encls>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|746| <<kvm_complete_insn_gp>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|757| <<complete_emulated_insn_gp>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|1081| <<kvm_emulate_xsetbv>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|1369| <<kvm_emulate_rdpmc>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13030| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13040| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13048| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ *   - arch/x86/kvm/x86.c|13069| <<kvm_handle_invpcid>> kvm_inject_gp(vcpu, 0);
+ */
 static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 {
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
@@ -1845,6 +2229,13 @@ enum {
 #define HF_GIF_MASK		(1 << 0)
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
+/*
+ * 在以下使用HF_GUEST_MASK:
+ *   - arch/x86/kvm/kvm_cache_regs.h|202| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|208| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|220| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+ *   - arch/x86/kvm/x86.c|8240| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+ */
 #define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */
 #define HF_SMM_MASK		(1 << 6)
 #define HF_SMM_INSIDE_NMI_MASK	(1 << 7)
diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 56935ebb1dfe..e20c413c81ee 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -108,6 +108,30 @@ bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token);
 
 DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
 
+/*
+ * KVM uses #PF vector to deliver 'page not present' events to guests
+ * (asynchronous page fault mechanism). The event happens when a
+ * userspace task is trying to access some valid (from guest's point of
+ * view) memory which is not currently mapped by the host (e.g. the
+ * memory is swapped out). Note, the corresponding "page ready" event
+ * which is injected when the memory becomes available, is delivered via
+ * an interrupt mechanism and not a #PF exception
+ * (see arch/x86/kernel/kvm.c: sysvec_kvm_asyncpf_interrupt()).
+ *
+ * We are relying on the interrupted context being sane (valid RSP,
+ * relevant locks not held, etc.), which is fine as long as the
+ * interrupted context had IF=1.  We are also relying on the KVM
+ * async pf type field and CR2 being read consistently instead of
+ * getting values from real and async page faults mixed up.
+ *
+ * Fingers crossed.
+ *
+ * The async #PF handling code takes care of idtentry handling
+ * itself.
+ *
+ * called by:
+ *   - arch/x86/mm/fault.c|1524| <<DEFINE_IDTENTRY_RAW_ERRORCODE(exc)>> if (kvm_handle_async_pf(regs, (u32)address))
+ */
 static __always_inline bool kvm_handle_async_pf(struct pt_regs *regs, u32 token)
 {
 	if (static_branch_unlikely(&kvm_async_pf_enabled))
diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h
index d42e6c6b47b1..98ab3c0028b2 100644
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@ -120,6 +120,14 @@ do {							\
 	__wrmsr((msr), (u32)((u64)(val)),		\
 		       (u32)((u64)(val) >> 32))
 
+/*
+ * 在以下使用native_read_msr():
+ *   - arch/x86/kernel/paravirt.c|301| <<global>> .cpu.read_msr = native_read_msr,
+ *   - arch/x86/include/asm/msr.h|256| <<rdmsr>> u64 __val = native_read_msr((msr)); \
+ *   - arch/x86/include/asm/msr.h|267| <<rdmsrl>> ((val) = native_read_msr((msr)))
+ *   - arch/x86/kvm/svm/svm.c|3799| <<svm_vcpu_run>> svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|6928| <<vmx_vcpu_run>> vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
+ */
 static inline unsigned long long native_read_msr(unsigned int msr)
 {
 	unsigned long long val;
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226efe3e..6062f1a39619 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -24,9 +24,24 @@
  */
 
 struct pvclock_vcpu_time_info {
+	/*
+	 * 奇数表示Host正在修改,偶数表示已修改完毕
+	 */
 	u32   version;
 	u32   pad0;
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->tsc_timestamp:
+	 *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	 *
+	 * 更新pvti时, vCPU的vTSC
+	 */
 	u64   tsc_timestamp;
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->system_time:
+	 *   - arch/x86/kvm/x86.c|3073| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	u64   system_time;
 	u32   tsc_to_system_mul;
 	s8    tsc_shift;
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2c68..67c34080218c 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -81,6 +81,10 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->tsc_timestamp:
+	 *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	 */
 	u64 delta = tsc - src->tsc_timestamp;
 	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 3e6f6b448f6a..0544e08024b5 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -309,6 +309,50 @@ assign_irq_vector_policy(struct irq_data *irqd, struct irq_alloc_info *info)
 	return reserve_irq_vector(irqd);
 }
 
+/*
+ * virtio-scsi开始的时候vcpu=4, maxvcpus=8.
+ *
+ * 下面的执行4次.
+ *
+ * [0] assign_managed_vector
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] enable_irq
+ * [0] vp_enable_cbs
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * add新的vcpu的时候callback调用下面
+ *
+ * [0] CPU: 4 PID: 722 Comm: cpuhp/4
+ * [0] assign_managed_vector
+ * [0] x86_vector_activate
+ * [0] __irq_domain_activate_irq
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] irq_affinity_online_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int
 assign_managed_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 766ffe3ba313..61916908043b 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -237,6 +237,14 @@ static __always_inline void handle_irq(struct irq_desc *desc,
  * common_interrupt() handles all normal device IRQ's (the special SMP
  * cross-CPU interrupts have their own entry points).
  */
+/*
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ */
 DEFINE_IDTENTRY_IRQ(common_interrupt)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
@@ -287,6 +295,11 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_x86_platform_ipi)
 static void dummy_handler(void) {}
 static void (*kvm_posted_intr_wakeup_handler)(void) = dummy_handler;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7882| <<hardware_unsetup>> kvm_set_posted_intr_wakeup_handler(NULL);
+ *   - arch/x86/kvm/vmx/vmx.c|8254| <<hardware_setup>> kvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);
+ */
 void kvm_set_posted_intr_wakeup_handler(void (*handler)(void))
 {
 	if (handler)
@@ -314,6 +327,9 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_posted_intr_wakeup_ipi)
 {
 	ack_APIC_irq();
 	inc_irq_stat(kvm_posted_intr_wakeup_ipis);
+	/*
+	 * pi_wakeup_handler()
+	 */
 	kvm_posted_intr_wakeup_handler();
 }
 
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d77481ecb0d5..0fa6011d760a 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -65,6 +65,21 @@ static int __init parse_no_stealacc(char *arg)
 
 early_param("no-steal-acc", parse_no_stealacc);
 
+/*
+ * 在以下使用percpu的apf_reason:
+ *   - arch/x86/kernel/kvm.c|234| <<kvm_read_and_reset_apf_flags>> if (__this_cpu_read(apf_reason.enabled)) {
+ *   - arch/x86/kernel/kvm.c|235| <<kvm_read_and_reset_apf_flags>> flags = __this_cpu_read(apf_reason.flags);
+ *   - arch/x86/kernel/kvm.c|236| <<kvm_read_and_reset_apf_flags>> __this_cpu_write(apf_reason.flags, 0);
+ *   - arch/x86/kernel/kvm.c|285| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> if (__this_cpu_read(apf_reason.enabled)) {
+ *   - arch/x86/kernel/kvm.c|286| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> token = __this_cpu_read(apf_reason.token);
+ *   - arch/x86/kernel/kvm.c|288| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> __this_cpu_write(apf_reason.token, 0);
+ *   - arch/x86/kernel/kvm.c|339| <<kvm_guest_cpu_init>> u64 pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
+ *   - arch/x86/kernel/kvm.c|343| <<kvm_guest_cpu_init>> pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
+ *   - arch/x86/kernel/kvm.c|352| <<kvm_guest_cpu_init>> __this_cpu_write(apf_reason.enabled, 1);
+ *   - arch/x86/kernel/kvm.c|373| <<kvm_pv_disable_apf>> if (!__this_cpu_read(apf_reason.enabled))
+ *   - arch/x86/kernel/kvm.c|377| <<kvm_pv_disable_apf>> __this_cpu_write(apf_reason.enabled, 0);
+ *   - arch/x86/kernel/kvm.c|428| <<sev_map_percpu_data>> __set_percpu_decrypted(&per_cpu(apf_reason, cpu), sizeof(apf_reason));
+ */
 static DEFINE_PER_CPU_DECRYPTED(struct kvm_vcpu_pv_apf_data, apf_reason) __aligned(64);
 DEFINE_PER_CPU_DECRYPTED(struct kvm_steal_time, steal_time) __aligned(64) __visible;
 static int has_steal_clock = 0;
@@ -91,6 +106,11 @@ static struct kvm_task_sleep_head {
 	struct hlist_head list;
 } async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|116| <<kvm_async_pf_queue_task>> e = _find_apf_task(b, token);
+ *   - arch/x86/kernel/kvm.c|202| <<kvm_async_pf_task_wake>> n = _find_apf_task(b, token);
+ */
 static struct kvm_task_sleep_node *_find_apf_task(struct kvm_task_sleep_head *b,
 						  u32 token)
 {
@@ -106,6 +126,37 @@ static struct kvm_task_sleep_node *_find_apf_task(struct kvm_task_sleep_head *b,
 	return NULL;
 }
 
+/*
+ * KVM uses #PF vector to deliver 'page not present' events to guests
+ * (asynchronous page fault mechanism). The event happens when a
+ * userspace task is trying to access some valid (from guest's point of
+ * view) memory which is not currently mapped by the host (e.g. the
+ * memory is swapped out). Note, the corresponding "page ready" event
+ * which is injected when the memory becomes available, is delivered via
+ * an interrupt mechanism and not a #PF exception
+ * (see arch/x86/kernel/kvm.c: sysvec_kvm_asyncpf_interrupt()).
+ *
+ * We are relying on the interrupted context being sane (valid RSP,
+ * relevant locks not held, etc.), which is fine as long as the
+ * interrupted context had IF=1.  We are also relying on the KVM
+ * async pf type field and CR2 being read consistently instead of
+ * getting values from real and async page faults mixed up.
+ *       
+ * Fingers crossed.
+ *               
+ * The async #PF handling code takes care of idtentry handling
+ * itself.
+ *
+ * 在vm中
+ * <DEFINE_IDTENTRY_RAW_ERRORCODE(exc)
+ * -> kvm_handle_async_pf()
+ *    -> __kvm_handle_async_pf()
+ *       -> kvm_async_pf_task_wait_schedule()
+ *          -> kvm_async_pf_queue_task()
+ *
+ * called by;
+ *   - arch/x86/kernel/kvm.c|147| <<kvm_async_pf_task_wait_schedule>> if (!kvm_async_pf_queue_task(token, &n))
+ */
 static bool kvm_async_pf_queue_task(u32 token, struct kvm_task_sleep_node *n)
 {
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
@@ -137,6 +188,11 @@ static bool kvm_async_pf_queue_task(u32 token, struct kvm_task_sleep_node *n)
  * Invoked from the async pagefault handling code or from the VM exit page
  * fault handler. In both cases RCU is watching.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|294| <<__kvm_handle_async_pf>> kvm_async_pf_task_wait_schedule(token);
+ *   - arch/x86/kvm/mmu/mmu.c|4661| <<kvm_handle_page_fault>> kvm_async_pf_task_wait_schedule(fault_address);
+ */
 void kvm_async_pf_task_wait_schedule(u32 token)
 {
 	struct kvm_task_sleep_node n;
@@ -160,6 +216,11 @@ void kvm_async_pf_task_wait_schedule(u32 token)
 }
 EXPORT_SYMBOL_GPL(kvm_async_pf_task_wait_schedule);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|207| <<apf_task_wake_all>> apf_task_wake_one(n);
+ *   - arch/x86/kernel/kvm.c|251| <<kvm_async_pf_task_wake>> apf_task_wake_one(n);
+ */
 static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 {
 	hlist_del_init(&n->link);
@@ -167,6 +228,11 @@ static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 		swake_up_one(&n->wq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|224| <<kvm_async_pf_task_wake>> apf_task_wake_all();
+ *   - arch/x86/kernel/kvm.c|471| <<kvm_guest_cpu_offline>> apf_task_wake_all();
+ */
 static void apf_task_wake_all(void)
 {
 	int i;
@@ -186,6 +252,10 @@ static void apf_task_wake_all(void)
 	}
 }
 
+/*
+ * called by;
+ *   - arch/x86/kernel/kvm.c|287| <<DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)>> kvm_async_pf_task_wake(token);
+ */
 void kvm_async_pf_task_wake(u32 token)
 {
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
@@ -240,6 +310,10 @@ noinstr u32 kvm_read_and_reset_apf_flags(void)
 }
 EXPORT_SYMBOL_GPL(kvm_read_and_reset_apf_flags);
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/kvm_para.h|114| <<kvm_handle_async_pf>> return __kvm_handle_async_pf(regs, token);
+ */
 noinstr bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token)
 {
 	u32 flags = kvm_read_and_reset_apf_flags();
@@ -282,6 +356,21 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)
 
 	inc_irq_stat(irq_hv_callback_count);
 
+	/*
+	 * #define KVM_PV_REASON_PAGE_NOT_PRESENT 1
+	 * #define KVM_PV_REASON_PAGE_READY 2
+	 *
+	 * struct kvm_vcpu_pv_apf_data {
+	 *     // Used for 'page not present' events delivered via #PF
+	 *     __u32 flags;
+	 *
+	 *     // Used for 'page ready' events delivered via interrupt notification
+	 *     __u32 token;
+	 *
+	 *     __u8 pad[56];
+	 *     __u32 enabled;
+	 * };
+	 */
 	if (__this_cpu_read(apf_reason.enabled)) {
 		token = __this_cpu_read(apf_reason.token);
 		kvm_async_pf_task_wake(token);
@@ -704,6 +793,19 @@ static int kvm_cpu_down_prepare(unsigned int cpu)
 
 #endif
 
+/*
+ * [0] kvm_suspend
+ * [0] syscore_suspend
+ * [0] suspend_devices_and_enter
+ * [0] pm_suspend
+ * [0] state_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int kvm_suspend(void)
 {
 	kvm_guest_cpu_offline(false);
@@ -711,6 +813,19 @@ static int kvm_suspend(void)
 	return 0;
 }
 
+/*
+ * [0] kvm_resume
+ * [0] syscore_resume
+ * [0] suspend_devices_and_enter
+ * [0] pm_suspend
+ * [0] state_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_resume(void)
 {
 	kvm_cpu_online(raw_smp_processor_id());
@@ -1108,6 +1223,15 @@ static void kvm_enable_host_haltpoll(void *i)
 	wrmsrl(MSR_KVM_POLL_CONTROL, 1);
 }
 
+/*
+ * [0] arch_haltpoll_enable
+ * [0] haltpoll_cpu_online
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void arch_haltpoll_enable(unsigned int cpu)
 {
 	if (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL)) {
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index c5caa7311bd8..39f3d6279aec 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -26,6 +26,13 @@ static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|91| <<kvm_sched_clock_read>> return kvm_clock_read() - kvm_sched_clock_offset;
+ *   - arch/x86/kernel/kvmclock.c|98| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+ *   - arch/x86/kernel/kvmclock.c|102| <<kvm_sched_clock_init>> kvm_sched_clock_offset);
+ *   - arch/x86/kernel/kvmclock.c|104| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
+ */
 static u64 kvm_sched_clock_offset __ro_after_init;
 
 static int __init parse_no_kvmclock(char *arg)
@@ -91,6 +98,10 @@ static u64 kvm_sched_clock_read(void)
 	return kvm_clock_read() - kvm_sched_clock_offset;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|334| <<kvmclock_init>> kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+ */
 static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable)
@@ -164,6 +175,12 @@ struct clocksource kvm_clock = {
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|186| <<kvm_restore_sched_clock_state>> kvm_register_clock("primary cpu clock, resume");
+ *   - arch/x86/kernel/kvmclock.c|192| <<kvm_setup_secondary_clock>> kvm_register_clock("secondary cpu clock");
+ *   - arch/x86/kernel/kvmclock.c|310| <<kvmclock_init>> kvm_register_clock("primary cpu clock");
+ */
 static void kvm_register_clock(char *txt)
 {
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
@@ -187,6 +204,10 @@ static void kvm_restore_sched_clock_state(void)
 }
 
 #ifdef CONFIG_X86_LOCAL_APIC
+/*
+ * 在以下使用kvm_setup_secondary_clock():
+ *   - arch/x86/kernel/kvmclock.c|324| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+ */
 static void kvm_setup_secondary_clock(void)
 {
 	kvm_register_clock("secondary cpu clock");
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index 4420499f7bb4..2060736fdd9b 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -143,6 +143,13 @@ static u64 native_steal_clock(int cpu)
 DEFINE_STATIC_CALL(pv_steal_clock, native_steal_clock);
 DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+ *   - arch/x86/kernel/kvmclock.c|106| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ *   - arch/x86/xen/time.c|527| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+ *   - drivers/clocksource/hyperv_timer.c|490| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+ */
 void paravirt_set_sched_clock(u64 (*func)(void))
 {
 	static_call_update(pv_sched_clock, func);
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index eda37df016f0..e1853f0cf555 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -16,9 +16,21 @@
 #include <asm/pvclock.h>
 #include <asm/vgtod.h>
 
+/*
+ * 在以下使用valid_flags:
+ *   - arch/x86/kernel/pvclock.c|24| <<pvclock_set_flags>> valid_flags = flags; 
+ *   - arch/x86/kernel/pvclock.c|64| <<pvclock_read_flags>> return flags & valid_flags;
+ *   - arch/x86/kernel/pvclock.c|85| <<pvclock_clocksource_read>> if ((valid_flags & PVCLOCK_TSC_STABLE_BIT) &&
+
+ */
 static u8 valid_flags __read_mostly = 0;
 static struct pvclock_vsyscall_time_info *pvti_cpu0_va __read_mostly;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|335| <<kvmclock_init>> pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+ *   - arch/x86/xen/time.c|509| <<xen_time_init>> pvclock_set_flags(PVCLOCK_TSC_STABLE_BIT);
+ */
 void pvclock_set_flags(u8 flags)
 {
 	valid_flags = flags;
@@ -51,6 +63,11 @@ void pvclock_resume(void)
 	atomic64_set(&last_value, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|272| <<kvm_setup_vsyscall_timeinfo>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+ *   - arch/x86/kernel/kvmclock.c|337| <<kvmclock_init>> flags = pvclock_read_flags(&hv_clock_boot[0].pvti);
+ */
 u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 617012f4619f..bbf523776e58 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -236,6 +236,11 @@ static void notrace start_secondary(void *unused)
 #endif
 	cpu_init_secondary();
 	rcu_cpu_starting(raw_smp_processor_id());
+	/*
+	 * 两处设置的地方:
+	 *   - arch/x86/kernel/x86_init.c|126| <<global>> .early_percpu_clock_init = x86_init_noop,
+	 *   - arch/x86/kernel/kvmclock.c|324| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+	 */
 	x86_cpuinit.early_percpu_clock_init();
 	smp_callin();
 
@@ -952,6 +957,10 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 }
 
 /* reduce the number of lines printed when booting a large cpu count system */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1100| <<do_boot_cpu>> announce_cpu(cpu, apicid);
+ */
 static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = NUMA_NO_NODE;
@@ -1079,6 +1088,10 @@ int common_cpu_up(unsigned int cpu, struct task_struct *idle)
  * Returns zero if CPU booted OK, else error code from
  * ->wakeup_secondary_cpu.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1239| <<native_cpu_up>> err = do_boot_cpu(apicid, cpu, tidle, &cpu0_nmi_registered);
+ */
 static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		       int *cpu0_nmi_registered)
 {
@@ -1646,6 +1659,11 @@ static void remove_cpu_from_maps(int cpu)
 	numa_remove_cpu(cpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1684| <<native_cpu_disable>> cpu_disable_common();
+ *   - arch/x86/xen/smp_pv.c|359| <<xen_pv_cpu_disable>> cpu_disable_common();
+ */
 void cpu_disable_common(void)
 {
 	int cpu = smp_processor_id();
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a698196377be..b32c2c904116 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1313,6 +1313,12 @@ EXPORT_SYMBOL(convert_art_ns_to_tsc);
 
 
 static void tsc_refine_calibration_work(struct work_struct *work);
+/*
+ * 在以下使用tsc_irqwork:
+ *   - arch/x86/kernel/tsc.c|1316| <<global>> static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
+ *   - arch/x86/kernel/tsc.c|1356| <<tsc_refine_calibration_work>> schedule_delayed_work(&tsc_irqwork, HZ);
+ *   - arch/x86/kernel/tsc.c|1429| <<init_tsc_clocksource>> schedule_delayed_work(&tsc_irqwork, 0);
+ */
 static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
 /**
  * tsc_refine_calibration_work - Further refine tsc freq calibration
@@ -1328,6 +1334,10 @@ static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
  * calibration, we throw out the new calibration and use the
  * early calibration.
  */
+/*
+ * 在以下使用tsc_refine_calibration_work():
+ *   - arch/x86/kernel/tsc.c|1316| <<global>> static DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);
+ */
 static void tsc_refine_calibration_work(struct work_struct *work)
 {
 	static u64 tsc_start = ULLONG_MAX, ref_start;
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
index 9452dc9664b5..e0e24d137cf5 100644
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@ -29,6 +29,15 @@ struct tsc_adjust {
 	bool		warned;
 };
 
+/*
+ * 在以下使用percpu的tsc_adjust:
+ *   - arch/x86/kernel/tsc_sync.c|32| <<global>> static DEFINE_PER_CPU(struct tsc_adjust, tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|58| <<tsc_verify_tsc_adjust>> struct tsc_adjust *adj = this_cpu_ptr(&tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|169| <<tsc_store_and_check_tsc_adjust>> struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|198| <<tsc_store_and_check_tsc_adjust>> struct tsc_adjust *ref, *cur = this_cpu_ptr(&tsc_adjust);
+ *   - arch/x86/kernel/tsc_sync.c|234| <<tsc_store_and_check_tsc_adjust>> ref = per_cpu_ptr(&tsc_adjust, refcpu);
+ *   - arch/x86/kernel/tsc_sync.c|463| <<check_tsc_sync_target>> struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
+ */
 static DEFINE_PER_CPU(struct tsc_adjust, tsc_adjust);
 static struct timer_list tsc_sync_check_timer;
 
@@ -46,6 +55,13 @@ void mark_tsc_async_resets(char *reason)
 	pr_info("tsc: Marking TSC async resets true due to %s\n", reason);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|711| <<arch_cpu_idle_enter>> tsc_verify_tsc_adjust(false);
+ *   - arch/x86/kernel/tsc.c|1074| <<tsc_resume>> tsc_verify_tsc_adjust(true);
+ *   - arch/x86/kernel/tsc_sync.c|97| <<tsc_sync_check_timer_fn>> tsc_verify_tsc_adjust(false);
+ *   - arch/x86/power/cpu.c|260| <<__restore_processor_state>> tsc_verify_tsc_adjust(true);
+ */
 void tsc_verify_tsc_adjust(bool resume)
 {
 	struct tsc_adjust *adj = this_cpu_ptr(&tsc_adjust);
@@ -152,6 +168,11 @@ static void tsc_sanitize_first_cpu(struct tsc_adjust *cur, s64 bootval,
 }
 
 #ifndef CONFIG_SMP
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1491| <<tsc_enable_sched_clock>> tsc_store_and_check_tsc_adjust(true);
+ *   - arch/x86/kernel/tsc_sync.c|460| <<check_tsc_sync_target>> if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {
+ */
 bool __init tsc_store_and_check_tsc_adjust(bool bootcpu)
 {
 	struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
@@ -176,6 +197,11 @@ bool __init tsc_store_and_check_tsc_adjust(bool bootcpu)
 /*
  * Store and check the TSC ADJUST MSR if available
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1491| <<tsc_enable_sched_clock>> tsc_store_and_check_tsc_adjust(true);
+ *   - arch/x86/kernel/tsc_sync.c|460| <<check_tsc_sync_target>> if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {
+ */
 bool tsc_store_and_check_tsc_adjust(bool bootcpu)
 {
 	struct tsc_adjust *ref, *cur = this_cpu_ptr(&tsc_adjust);
@@ -437,6 +463,10 @@ void check_tsc_sync_source(int cpu)
 /*
  * Freshly booted CPUs call into this:
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|254| <<start_secondary>> check_tsc_sync_target();
+ */
 void check_tsc_sync_target(void)
 {
 	struct tsc_adjust *cur = this_cpu_ptr(&tsc_adjust);
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index b8f8d268d058..1d261e77a204 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -29,6 +29,21 @@
  * Unlike "struct cpuinfo_x86.x86_capability", kvm_cpu_caps doesn't need to be
  * aligned to sizeof(unsigned long) because it's not accessed via bitops.
  */
+/*
+ * 在以下使用kvm_cpu_caps[NR_KVM_CPU_CAPS]:
+ *   -  arch/x86/kvm/cpuid.c|488| <<__kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);
+ *   - arch/x86/kvm/cpuid.c|497| <<kvm_cpu_cap_init_scattered>> kvm_cpu_caps[leaf] = mask;
+ *   - arch/x86/kvm/cpuid.c|507| <<kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= mask;
+ *   - arch/x86/kvm/cpuid.c|523| <<kvm_set_cpu_caps>> memset(kvm_cpu_caps, 0, sizeof(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.c|525| <<kvm_set_cpu_caps>> BUILD_BUG_ON(sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)) >
+ *   - arch/x86/kvm/cpuid.c|528| <<kvm_set_cpu_caps>> memcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,
+ *   - arch/x86/kvm/cpuid.c|529| <<kvm_set_cpu_caps>> sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)));
+ *   - arch/x86/kvm/cpuid.h|69| <<cpuid_entry_override>> BUILD_BUG_ON(leaf >= ARRAY_SIZE(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.h|70| <<cpuid_entry_override>> *reg = kvm_cpu_caps[leaf];
+ *   - arch/x86/kvm/cpuid.h|189| <<kvm_cpu_cap_clear>> kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|197| <<kvm_cpu_cap_set>> kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|205| <<kvm_cpu_cap_get>> return kvm_cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+ */
 u32 kvm_cpu_caps[NR_KVM_CPU_CAPS] __read_mostly;
 EXPORT_SYMBOL_GPL(kvm_cpu_caps);
 
@@ -84,6 +99,10 @@ static inline struct kvm_cpuid_entry2 *cpuid_entry2_find(
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|389| <<kvm_set_cpuid>> r = kvm_check_cpuid(vcpu, e2, nent);
+ */
 static int kvm_check_cpuid(struct kvm_vcpu *vcpu,
 			   struct kvm_cpuid_entry2 *entries,
 			   int nent)
@@ -278,6 +297,10 @@ void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_update_cpuid_runtime);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|407| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -344,6 +367,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|439| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
@@ -431,6 +459,10 @@ int kvm_vcpu_ioctl_set_cpuid(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * 处理KVM_SET_CPUID2:
+ *   - arch/x86/kvm/x86.c|5605| <<kvm_arch_vcpu_ioctl>> r = kvm_vcpu_ioctl_set_cpuid2(vcpu, &cpuid,
+ */
 int kvm_vcpu_ioctl_set_cpuid2(struct kvm_vcpu *vcpu,
 			      struct kvm_cpuid2 *cpuid,
 			      struct kvm_cpuid_entry2 __user *entries)
@@ -779,6 +811,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1210| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -879,6 +915,18 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 		eax.split.bit_width = cap.bit_width_gp;
 		eax.split.mask_length = cap.events_mask_len;
 
+		/*
+		 * union cpuid10_edx {
+		 *     struct {
+		 *         unsigned int num_counters_fixed:5;
+		 *         unsigned int bit_width_fixed:8;
+		 *         unsigned int reserved1:2;
+		 *         unsigned int anythread_deprecated:1;
+		 *         unsigned int reserved2:16;
+		 *     } split;
+		 *     unsigned int full;
+		 * };
+		 */
 		edx.split.num_counters_fixed = min(cap.num_counters_fixed, MAX_FIXED_COUNTERS);
 		edx.split.bit_width_fixed = cap.bit_width_fixed;
 		if (cap.version)
@@ -1157,6 +1205,10 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1251| <<kvm_dev_ioctl_get_cpuid>> r = get_cpuid_func(&array, funcs[i], type);
+ */
 static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			 unsigned int type)
 {
@@ -1168,6 +1220,10 @@ static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 
 #define CENTAUR_CPUID_SIGNATURE 0xC0000000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1299| <<kvm_dev_ioctl_get_cpuid>> r = get_cpuid_func(&array, funcs[i], type);
+ */
 static int get_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			  unsigned int type)
 {
@@ -1219,6 +1275,10 @@ static bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,
 	return false;
 }
 
+/*
+ * 处理KVM_GET_SUPPORTED_CPUID和KVM_GET_EMULATED_CPUID:
+ *   - arch/x86/kvm/x86.c|4442| <<kvm_arch_dev_ioctl>> r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,
+ */
 int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 			    struct kvm_cpuid_entry2 __user *entries,
 			    unsigned int type)
@@ -1339,9 +1399,22 @@ get_out_of_range_cpuid_entry(struct kvm_vcpu *vcpu, u32 *fn_ptr, u32 index)
 	return kvm_find_cpuid_entry(vcpu, basic->eax, index);
 }
 
+/*
+ * [0] kvm_cpuid
+ * [0] kvm_emulate_cpuid
+ * [0] vmx_handle_exit
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 	       u32 *ecx, u32 *edx, bool exact_only)
 {
+	/*
+	 * cpuid的function输入是eax ... index是什么??
+	 */
 	u32 orig_function = *eax, function = *eax, index = *ecx;
 	struct kvm_cpuid_entry2 *entry;
 	bool exact, used_max_basic = false;
@@ -1388,6 +1461,11 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 }
 EXPORT_SYMBOL_GPL(kvm_cpuid);
 
+/*
+ * 在以下使用kvm_emulate_cpuid():
+ *   - arch/x86/kvm/svm/svm.c|2993| <<global>> [SVM_EXIT_CPUID] = kvm_emulate_cpuid,
+ *   - arch/x86/kvm/vmx/vmx.c|5757| <<global>> [EXIT_REASON_CPUID] = kvm_emulate_cpuid,
+ */
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
 {
 	u32 eax, ebx, ecx, edx;
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index 8a770b481d9d..8b2cebb0054c 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -197,6 +197,11 @@ static __always_inline void kvm_cpu_cap_set(unsigned int x86_feature)
 	kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|210| <<kvm_cpu_cap_has>> return !!kvm_cpu_cap_get(x86_feature);
+ *   - arch/x86/kvm/vmx/evmcs.c|336| <<nested_get_evmcs_version>> if (kvm_cpu_cap_get(X86_FEATURE_VMX) &&
+ */
 static __always_inline u32 kvm_cpu_cap_get(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index e86d610dc6b7..919f4d413ec7 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -246,10 +246,27 @@ enum x86_transfer_type {
 
 static ulong reg_read(struct x86_emulate_ctxt *ctxt, unsigned nr)
 {
+	/*
+	 * u32 regs_valid;
+	 * u32 regs_dirty;
+	 *
+	 * 在以下使用x86_emulate_ctxt->regs_valid:
+	 *   - arch/x86/kvm/emulate.c|249| <<reg_read>> if (!(ctxt->regs_valid & (1 << nr))) {
+	 *   - arch/x86/kvm/emulate.c|250| <<reg_read>> ctxt->regs_valid |= 1 << nr;
+	 *   - arch/x86/kvm/emulate.c|258| <<reg_write>> ctxt->regs_valid |= 1 << nr;
+	 *   - arch/x86/kvm/emulate.c|284| <<invalidate_registers>> ctxt->regs_valid = 0;
+	 */
 	if (!(ctxt->regs_valid & (1 << nr))) {
 		ctxt->regs_valid |= 1 << nr;
 		ctxt->_regs[nr] = ctxt->ops->read_gpr(ctxt, nr);
 	}
+	/*
+	 * 在以下使用x86_emulate_ctxt->regs_valid:
+	 *   - arch/x86/kvm/emulate.c|251| <<reg_read>> ctxt->_regs[nr] = ctxt->ops->read_gpr(ctxt, nr);
+	 *   - arch/x86/kvm/emulate.c|253| <<reg_read>> return ctxt->_regs[nr];
+	 *   - arch/x86/kvm/emulate.c|260| <<reg_write>> return &ctxt->_regs[nr];
+	 *   - arch/x86/kvm/emulate.c|278| <<writeback_registers>> ctxt->ops->write_gpr(ctxt, reg, ctxt->_regs[reg]);
+	 */
 	return ctxt->_regs[nr];
 }
 
@@ -266,10 +283,22 @@ static ulong *reg_rmw(struct x86_emulate_ctxt *ctxt, unsigned nr)
 	return reg_write(ctxt, nr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|2045| <<emulate_int_real>> writeback_registers(ctxt);
+ *   - arch/x86/kvm/emulate.c|3330| <<emulator_task_switch>> writeback_registers(ctxt);
+ *   - arch/x86/kvm/emulate.c|5683| <<x86_emulate_insn>> writeback_registers(ctxt);
+ *   - arch/x86/kvm/emulate.c|5704| <<x86_emulate_insn>> writeback_registers(ctxt);
+ *   - arch/x86/kvm/emulate.c|5769| <<emulator_writeback_register_cache>> writeback_registers(ctxt);
+ */
 static void writeback_registers(struct x86_emulate_ctxt *ctxt)
 {
 	unsigned reg;
 
+	/*
+	 * struct x86_emulate_ctxt *ctxt:
+	 * -> u32 regs_dirty;
+	 */
 	for_each_set_bit(reg, (ulong *)&ctxt->regs_dirty, 16)
 		ctxt->ops->write_gpr(ctxt, reg, ctxt->_regs[reg]);
 }
@@ -587,6 +616,16 @@ register_address_increment(struct x86_emulate_ctxt *ctxt, int reg, int inc)
 	assign_register(preg, *preg + inc, ctxt->ad_bytes);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|1839| <<push>> rsp_increment(ctxt, -bytes);
+ *   - arch/x86/kvm/emulate.c|1865| <<emulate_pop>> rsp_increment(ctxt, len);
+ *   - arch/x86/kvm/emulate.c|1959| <<em_push_sreg>> rsp_increment(ctxt, -2);
+ *   - arch/x86/kvm/emulate.c|1979| <<em_pop_sreg>> rsp_increment(ctxt, ctxt->op_bytes - 2);
+ *   - arch/x86/kvm/emulate.c|2019| <<em_popa>> rsp_increment(ctxt, ctxt->op_bytes);
+ *   - arch/x86/kvm/emulate.c|2289| <<em_ret_far_imm>> rsp_increment(ctxt, ctxt->src.val);
+ *   - arch/x86/kvm/emulate.c|3533| <<em_ret_near_imm>> rsp_increment(ctxt, ctxt->src.val);
+ */
 static void rsp_increment(struct x86_emulate_ctxt *ctxt, int inc)
 {
 	masked_increment(reg_rmw(ctxt, VCPU_REGS_RSP), stack_mask(ctxt), inc);
@@ -607,6 +646,19 @@ static unsigned long seg_base(struct x86_emulate_ctxt *ctxt, int seg)
 	return ctxt->ops->get_cached_segment_base(ctxt, seg);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|648| <<emulate_db>> return emulate_exception(ctxt, DB_VECTOR, 0, false);
+ *   - arch/x86/kvm/emulate.c|653| <<emulate_gp>> return emulate_exception(ctxt, GP_VECTOR, err, true);
+ *   - arch/x86/kvm/emulate.c|658| <<emulate_ss>> return emulate_exception(ctxt, SS_VECTOR, err, true);
+ *   - arch/x86/kvm/emulate.c|663| <<emulate_ud>> return emulate_exception(ctxt, UD_VECTOR, 0, false);
+ *   - arch/x86/kvm/emulate.c|668| <<emulate_ts>> return emulate_exception(ctxt, TS_VECTOR, err, true);
+ *   - arch/x86/kvm/emulate.c|673| <<emulate_de>> return emulate_exception(ctxt, DE_VECTOR, 0, false);
+ *   - arch/x86/kvm/emulate.c|678| <<emulate_nm>> return emulate_exception(ctxt, NM_VECTOR, 0, false);
+ *   - arch/x86/kvm/emulate.c|1743| <<__load_segment_descriptor>> return emulate_exception(ctxt, err_vec, err_code, true);
+ *   - arch/x86/kvm/emulate.c|1763| <<load_segment_descriptor>> return emulate_exception(ctxt, GP_VECTOR, 0, true);
+ *   - arch/x86/kvm/emulate.c|5393| <<flush_pending_x87_faults>> return emulate_exception(ctxt, MF_VECTOR, 0, false);
+ */
 static int emulate_exception(struct x86_emulate_ctxt *ctxt, int vec,
 			     u32 error, bool valid)
 {
@@ -805,6 +857,14 @@ static inline int assign_eip(struct x86_emulate_ctxt *ctxt, ulong dst,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|867| <<jmp_rel>> return assign_eip_near(ctxt, ctxt->_eip + rel);
+ *   - arch/x86/kvm/emulate.c|2182| <<em_jmp_abs>> return assign_eip_near(ctxt, ctxt->src.val);
+ *   - arch/x86/kvm/emulate.c|2191| <<em_call_near_abs>> rc = assign_eip_near(ctxt, ctxt->src.val);
+ *   - arch/x86/kvm/emulate.c|2229| <<em_ret>> return assign_eip_near(ctxt, eip);
+ *   - arch/x86/kvm/emulate.c|3509| <<em_ret_near_imm>> rc = assign_eip_near(ctxt, eip);
+ */
 static inline int assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)
 {
 	return assign_eip(ctxt, dst, ctxt->mode);
@@ -1131,6 +1191,10 @@ static int em_fnstsw(struct x86_emulate_ctxt *ctxt)
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|4922| <<decode_operand>> decode_register_operand(ctxt, op);
+ */
 static void decode_register_operand(struct x86_emulate_ctxt *ctxt,
 				    struct operand *op)
 {
@@ -1386,6 +1450,12 @@ static int segmented_read(struct x86_emulate_ctxt *ctxt,
 	return read_emulated(ctxt, linear, data, size);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|1809| <<writeback>> return segmented_write(ctxt,
+ *   - arch/x86/kvm/emulate.c|1815| <<writeback>> return segmented_write(ctxt,
+ *   - arch/x86/kvm/emulate.c|1843| <<push>> return segmented_write(ctxt, addr, data, bytes);
+ */
 static int segmented_write(struct x86_emulate_ctxt *ctxt,
 			   struct segmented_address addr,
 			   const void *data,
@@ -1416,6 +1486,10 @@ static int segmented_cmpxchg(struct x86_emulate_ctxt *ctxt,
 					   size, &ctxt->exception);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|3953| <<em_in>> if (!pio_in_emulated(ctxt, ctxt->dst.bytes, ctxt->src.val,
+ */
 static int pio_in_emulated(struct x86_emulate_ctxt *ctxt,
 			   unsigned int size, unsigned short port,
 			   void *dest)
@@ -2153,6 +2227,13 @@ static int em_jmp_far(struct x86_emulate_ctxt *ctxt)
 
 static int em_jmp_abs(struct x86_emulate_ctxt *ctxt)
 {
+	/*
+	 * struct x86_emulate_ctxt *ctxt:
+	 * -> struct operand src;
+	 * -> struct operand src2;
+	 * -> struct operand dst;
+	 * -> struct operand memop;
+	 */
 	return assign_eip_near(ctxt, ctxt->src.val);
 }
 
@@ -2191,6 +2272,10 @@ static int em_cmpxchg8b(struct x86_emulate_ctxt *ctxt)
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * 在以下使用em_ret():
+ *   - arch/x86/kvm/emulate.c|4676| <<global>> I(ImplicitOps | NearBranch | IsBranch, em_ret),
+ */
 static int em_ret(struct x86_emulate_ctxt *ctxt)
 {
 	int rc;
@@ -3861,6 +3946,12 @@ static int em_jcxz(struct x86_emulate_ctxt *ctxt)
 	return rc;
 }
 
+/*
+ * 在以下使用em_in():
+ *   - arch/x86/kvm/emulate.c|4676| <<global>> I2bvIP(DstDI | SrcDX | Mov | String | Unaligned, em_in, ins, check_perm_in),
+ *   - arch/x86/kvm/emulate.c|4743| <<global>> I2bvIP(SrcImmUByte | DstAcc, em_in, in, check_perm_in),
+ *   - arch/x86/kvm/emulate.c|4750| <<global>> I2bvIP(SrcDX | DstAcc, em_in, in, check_perm_in),
+ */
 static int em_in(struct x86_emulate_ctxt *ctxt)
 {
 	if (!pio_in_emulated(ctxt, ctxt->dst.bytes, ctxt->src.val,
@@ -4209,6 +4300,11 @@ static int check_svme_pa(struct x86_emulate_ctxt *ctxt)
 	return check_svme(ctxt);
 }
 
+/*
+ * 在以下使用check_rdtsc():
+ *   - arch/x86/kvm/emulate.c|4373| <<global>> DIP(SrcNone, rdtscp, check_rdtsc),
+ *   - arch/x86/kvm/emulate.c|4750| <<global>> IIP(ImplicitOps, em_rdtsc, rdtsc, check_rdtsc),
+ */
 static int check_rdtsc(struct x86_emulate_ctxt *ctxt)
 {
 	u64 cr4 = ctxt->ops->get_cr(ctxt, 4);
@@ -4542,6 +4638,10 @@ static const struct mode_dual mode_dual_63 = {
 	N, I(DstReg | SrcMem32 | ModRM | Mov, em_movsxd)
 };
 
+/*
+ * 在以下使用opcode_table[256]:
+ *   - arch/x86/kvm/emulate.c|5202| <<x86_decode_insn>> opcode = opcode_table[ctxt->b];
+ */
 static const struct opcode opcode_table[256] = {
 	/* 0x00 - 0x07 */
 	F6ALU(Lock, em_add),
@@ -4865,6 +4965,12 @@ static int decode_imm(struct x86_emulate_ctxt *ctxt, struct operand *op,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|5355| <<x86_decode_insn>> rc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);
+ *   - arch/x86/kvm/emulate.c|5363| <<x86_decode_insn>> rc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);
+ *   - arch/x86/kvm/emulate.c|5368| <<x86_decode_insn>> rc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);
+ */
 static int decode_operand(struct x86_emulate_ctxt *ctxt, struct operand *op,
 			  unsigned d)
 {
@@ -5031,6 +5137,10 @@ static int decode_operand(struct x86_emulate_ctxt *ctxt, struct operand *op,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8573| <<x86_decode_emulated_instruction>> r = x86_decode_insn(ctxt, insn, insn_len, emulation_type);
+ */
 int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len, int emulation_type)
 {
 	int rc = X86EMUL_CONTINUE;
@@ -5375,6 +5485,17 @@ static void fetch_possible_mmx_operand(struct operand *op)
 		kvm_read_mmx_reg(op->addr.mm, &op->mm_val);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|1055| <<em_bsf_c>> return fastop(ctxt, em_bsf);
+ *   - arch/x86/kvm/emulate.c|1063| <<em_bsr_c>> return fastop(ctxt, em_bsr);
+ *   - arch/x86/kvm/emulate.c|2257| <<em_cmpxchg>> fastop(ctxt, em_cmp);
+ *   - arch/x86/kvm/emulate.c|3374| <<em_das>> fastop(ctxt, em_or);
+ *   - arch/x86/kvm/emulate.c|3400| <<em_aam>> fastop(ctxt, em_or);
+ *   - arch/x86/kvm/emulate.c|3418| <<em_aad>> fastop(ctxt, em_or);
+ *   - arch/x86/kvm/emulate.c|3509| <<em_imul_3op>> return fastop(ctxt, em_imul);
+ *   - arch/x86/kvm/emulate.c|5561| <<x86_emulate_insn>> rc = fastop(ctxt, ctxt->fop);
+ */
 static int fastop(struct x86_emulate_ctxt *ctxt, fastop_t fop)
 {
 	ulong flags = (ctxt->eflags & EFLAGS_MASK) | X86_EFLAGS_IF;
@@ -5403,10 +5524,24 @@ void init_decode_cache(struct x86_emulate_ctxt *ctxt)
 	ctxt->mem_read.end = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8691| <<x86_emulate_instruction>> r = x86_emulate_insn(ctxt);
+ */
 int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
 	int rc = X86EMUL_CONTINUE;
+	/*
+	 * struct x86_emulate_ctxt *ctxt:
+	 * -> struct operand src;
+	 * -> struct operand src2;
+	 * -> struct operand dst;
+	 * -> struct operand memop;
+	 *
+	 * type是enum, 包括以下:
+	 * OP_REG, OP_MEM, OP_MEM_STR, OP_IMM, OP_XMM, OP_MM, OP_NONE
+	 */
 	int saved_dst_type = ctxt->dst.type;
 	unsigned emul_flags;
 
diff --git a/arch/x86/kvm/ioapic.c b/arch/x86/kvm/ioapic.c
index decfa36b7891..490c7d4c899d 100644
--- a/arch/x86/kvm/ioapic.c
+++ b/arch/x86/kvm/ioapic.c
@@ -204,6 +204,11 @@ static void ioapic_lazy_update_eoi(struct kvm_ioapic *ioapic, int irq)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|273| <<kvm_ioapic_inject_all>> ioapic_set_irq(ioapic, idx, 1, true);
+ *   - arch/x86/kvm/ioapic.c|466| <<kvm_ioapic_set_irq>> ret = ioapic_set_irq(ioapic, irq, irq_level, line_status);
+ */
 static int ioapic_set_irq(struct kvm_ioapic *ioapic, unsigned int irq,
 		int irq_level, bool line_status)
 {
@@ -212,6 +217,24 @@ static int ioapic_set_irq(struct kvm_ioapic *ioapic, unsigned int irq,
 	u32 old_irr;
 	int edge, ret;
 
+	/*
+	 * union kvm_ioapic_redirect_entry {
+	 *     u64 bits;
+	 *     struct {
+	 *         u8 vector;
+	 *         u8 delivery_mode:3;
+	 *         u8 dest_mode:1;
+	 *         u8 delivery_status:1;
+	 *         u8 polarity:1;
+	 *         u8 remote_irr:1;
+	 *         u8 trig_mode:1;
+	 *         u8 mask:1;
+	 *         u8 reserve:7;
+	 *         u8 reserved[4];
+	 *         u8 dest_id;
+	 *     } fields;
+	 * };
+	 */
 	entry = ioapic->redirtbl[irq];
 	edge = (entry.fields.trig_mode == IOAPIC_EDGE_TRIG);
 
@@ -412,6 +435,24 @@ static void ioapic_write_indirect(struct kvm_ioapic *ioapic, u32 val)
 
 static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)
 {
+	/*
+	 * union kvm_ioapic_redirect_entry {
+	 *     u64 bits;
+	 *     struct {
+	 *         u8 vector;
+	 *         u8 delivery_mode:3;
+	 *         u8 dest_mode:1;
+	 *         u8 delivery_status:1;
+	 *         u8 polarity:1;
+	 *         u8 remote_irr:1;
+	 *         u8 trig_mode:1;
+	 *         u8 mask:1;
+	 *         u8 reserve:7;
+	 *         u8 reserved[4];
+	 *         u8 dest_id;
+	 *     } fields;
+	 * };
+	 */
 	union kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];
 	struct kvm_lapic_irq irqe;
 	int ret;
@@ -453,6 +494,10 @@ static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|42| <<kvm_set_ioapic_irq>> return kvm_ioapic_set_irq(ioapic, e->irqchip.pin, irq_source_id, level,
+ */
 int kvm_ioapic_set_irq(struct kvm_ioapic *ioapic, int irq, int irq_source_id,
 		       int level, bool line_status)
 {
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 172b05343cfd..9a91514dce48 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -129,6 +129,11 @@ static int kvm_cpu_get_extint(struct kvm_vcpu *v)
 /*
  * Read pending interrupt vector and intack.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4631| <<nested_vmx_vmexit>> int irq = kvm_cpu_get_interrupt(vcpu);
+ *   - arch/x86/kvm/x86.c|9885| <<inject_pending_event>> kvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu), false);
+ */
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 {
 	int vector = kvm_cpu_get_extint(v);
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 6e0dab04320e..5957968d039c 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -34,6 +34,10 @@ static int kvm_set_pic_irq(struct kvm_kernel_irq_routing_entry *e,
 	return kvm_pic_set_irq(pic, e->irqchip.pin, irq_source_id, level);
 }
 
+/*
+ * 在以下使用kvm_set_ioapic_irq():
+ *   - arch/x86/kvm/irq_comm.c|317| <<kvm_set_routing_entry>> e->set = kvm_set_ioapic_irq;
+ */
 static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -43,6 +47,15 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|463| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|444| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
+ *   - arch/x86/kvm/ioapic.c|448| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq_comm.c|143| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1323| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|9490| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -100,6 +113,14 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|141| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|173| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|426| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/svm/avic.c|754| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/vmx/posted_intr.c|406| <<pi_update_irte>> kvm_set_msi_irq(kvm, e, &irq);
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 3febc342360c..28f82be20223 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -50,6 +50,19 @@ BUILD_KVM_GPR_ACCESSORS(r15, R15)
  * 1	  0	  register in vcpu->arch
  * 1	  1	  register in vcpu->arch, needs to be stored back
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/kvm_cache_regs.h|88| <<kvm_register_read_raw>> if (!kvm_register_is_available(vcpu, reg))
+ *   - arch/x86/kvm/kvm_cache_regs.h|135| <<kvm_pdptr_read>> if (!kvm_register_is_available(vcpu, VCPU_EXREG_PDPTR))
+ *   - arch/x86/kvm/kvm_cache_regs.h|150| <<kvm_read_cr0_bits>> !kvm_register_is_available(vcpu, VCPU_EXREG_CR0))
+ *   - arch/x86/kvm/kvm_cache_regs.h|164| <<kvm_read_cr4_bits>> !kvm_register_is_available(vcpu, VCPU_EXREG_CR4))
+ *   - arch/x86/kvm/kvm_cache_regs.h|171| <<kvm_read_cr3>> if (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))
+ *   - arch/x86/kvm/vmx/vmx.c|674| <<vmx_segment_cache_test_set>> if (!kvm_register_is_available(&vmx->vcpu, VCPU_EXREG_SEGMENTS)) {
+ *   - arch/x86/kvm/vmx/vmx.c|1394| <<vmx_get_rflags>> if (!kvm_register_is_available(vcpu, VCPU_EXREG_RFLAGS)) {
+ *   - arch/x86/kvm/vmx/vmx.c|3144| <<vmx_set_cr0>> if (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))
+ *   - arch/x86/kvm/vmx/vmx.h|533| <<vmx_get_exit_qual>> if (!kvm_register_is_available(vcpu, VCPU_EXREG_EXIT_INFO_1)) {
+ *   - arch/x86/kvm/vmx/vmx.h|544| <<vmx_get_intr_info>> if (!kvm_register_is_available(vcpu, VCPU_EXREG_EXIT_INFO_2)) {
+ */
 static inline bool kvm_register_is_available(struct kvm_vcpu *vcpu,
 					     enum kvm_reg reg)
 {
@@ -88,6 +101,13 @@ static inline unsigned long kvm_register_read_raw(struct kvm_vcpu *vcpu, int reg
 	if (!kvm_register_is_available(vcpu, reg))
 		static_call(kvm_x86_cache_reg)(vcpu, reg);
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> unsigned long regs[NR_VCPU_REGS];
+	 *    -> u32 regs_avail;
+	 *    -> u32 regs_dirty;
+	 */
 	return vcpu->arch.regs[reg];
 }
 
@@ -177,12 +197,24 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|598| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|806| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|994| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3501| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4545| <<nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/kvm_emulate.h b/arch/x86/kvm/kvm_emulate.h
index 39eded2426ff..a6a0f0a178cf 100644
--- a/arch/x86/kvm/kvm_emulate.h
+++ b/arch/x86/kvm/kvm_emulate.h
@@ -346,6 +346,13 @@ struct x86_emulate_ctxt {
 	u8 lock_prefix;
 	u8 rep_prefix;
 	/* bitmaps of registers in _regs[] that can be read */
+	/*
+	 * 在以下使用x86_emulate_ctxt->regs_valid:
+	 *   - arch/x86/kvm/emulate.c|249| <<reg_read>> if (!(ctxt->regs_valid & (1 << nr))) {
+	 *   - arch/x86/kvm/emulate.c|250| <<reg_read>> ctxt->regs_valid |= 1 << nr;
+	 *   - arch/x86/kvm/emulate.c|258| <<reg_write>> ctxt->regs_valid |= 1 << nr;
+	 *   - arch/x86/kvm/emulate.c|284| <<invalidate_registers>> ctxt->regs_valid = 0;
+	 */
 	u32 regs_valid;
 	/* bitmaps of registers in _regs[] that have been written */
 	u32 regs_dirty;
@@ -364,6 +371,13 @@ struct x86_emulate_ctxt {
 	struct operand src2;
 	struct operand dst;
 	struct operand memop;
+	/*
+	 * 在以下使用x86_emulate_ctxt->regs_valid:
+	 *   - arch/x86/kvm/emulate.c|251| <<reg_read>> ctxt->_regs[nr] = ctxt->ops->read_gpr(ctxt, nr);
+	 *   - arch/x86/kvm/emulate.c|253| <<reg_read>> return ctxt->_regs[nr];
+	 *   - arch/x86/kvm/emulate.c|260| <<reg_write>> return &ctxt->_regs[nr];
+	 *   - arch/x86/kvm/emulate.c|278| <<writeback_registers>> ctxt->ops->write_gpr(ctxt, reg, ctxt->_regs[reg]);
+	 */
 	unsigned long _regs[NR_VCPU_REGS];
 	struct operand *memopp;
 	struct fetch_cache fetch;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9322e6340a74..9eb54c9d52e3 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -507,6 +507,10 @@ void kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_clear_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2738| <<kvm_get_apic_interrupt>> apic_set_isr(vector, apic);
+ */
 static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -554,6 +558,10 @@ static inline int apic_find_highest_isr(struct kvm_lapic *apic)
 	return result;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1281| <<apic_set_eoi>> apic_clear_isr(vector, apic);
+ */
 static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -579,6 +587,11 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6623| <<vmx_sync_pir_to_irr>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ *   - arch/x86/kvm/x86.c|9727| <<update_cr8_intercept>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ */
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
 	/* This may race with setting of irr in __apic_accept_irq() and
@@ -594,6 +607,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|807| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|1874| <<kvm_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|76| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|98| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|619| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|995| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1008| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|12947| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1059,6 +1083,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|602| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+ *   - arch/x86/kvm/lapic.c|2538| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1070,6 +1099,13 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 				  trig_mode, vector);
 	switch (delivery_mode) {
 	case APIC_DM_LOWEST:
+		/*
+		 * 在以下使用kvm_vcpu_arch->apic_arb_prio:
+		 *   - arch/x86/kvm/lapic.c|1089| <<__apic_accept_irq>> vcpu->arch.apic_arb_prio++;
+		 *   - arch/x86/kvm/lapic.c|1225| <<kvm_apic_compare_prio>> return vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;
+		 *   - arch/x86/kvm/lapic.c|2537| <<kvm_lapic_reset>> vcpu->arch.apic_arb_prio = 0;
+		 *   - arch/x86/kvm/lapic.c|2813| <<kvm_apic_set_state>> vcpu->arch.apic_arb_prio = 0;
+		 */
 		vcpu->arch.apic_arb_prio++;
 		fallthrough;
 	case APIC_DM_FIXED:
@@ -1096,6 +1132,11 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * 只在这里用kvm_x86_deliver_interrupt.
+		 *
+		 * vmx_deliver_interrupt()
+		 */
 		static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,
 						       trig_mode, vector);
 		break;
@@ -1199,6 +1240,11 @@ void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|81| <<kvm_irq_delivery_to_apic>> else if (kvm_apic_compare_prio(vcpu, lowest) < 0)
+ *   - arch/x86/kvm/lapic.c|971| <<kvm_apic_map_get_dest_lapic>> else if (kvm_apic_compare_prio((*dst)[i]->vcpu,
+ */
 int kvm_apic_compare_prio(struct kvm_vcpu *vcpu1, struct kvm_vcpu *vcpu2)
 {
 	return vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;
@@ -1261,6 +1307,10 @@ static int apic_set_eoi(struct kvm_lapic *apic)
  * this interface assumes a trap-like exit, which has already finished
  * desired side effect including vISR and vPPR update.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5436| <<handle_apic_eoi_induced>> kvm_apic_set_eoi_accelerated(vcpu, vector);
+ */
 void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -1272,6 +1322,11 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2057| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/x86.c|2034| <<handle_fastpath_set_x2apic_icr_irqoff>> kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
+ */
 void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 {
 	struct kvm_lapic_irq irq;
@@ -1496,6 +1551,12 @@ static void limit_periodic_timer_frequency(struct kvm_lapic *apic)
 
 static void cancel_hv_timer(struct kvm_lapic *apic);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1528| <<apic_update_lvtt>> cancel_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2197| <<kvm_lapic_reg_write(APIC_TMICT)>> cancel_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2732| <<kvm_apic_set_state>> cancel_apic_timer(apic);
+ */
 static void cancel_apic_timer(struct kvm_lapic *apic)
 {
 	hrtimer_cancel(&apic->lapic_timer.timer);
@@ -1505,6 +1566,13 @@ static void cancel_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2134| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2179| <<kvm_lapic_reg_write(APIC_LVTT)>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2436| <<kvm_lapic_reset>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2723| <<kvm_apic_set_state>> apic_update_lvtt(apic);
+ */
 static void apic_update_lvtt(struct kvm_lapic *apic)
 {
 	u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
@@ -1517,6 +1585,10 @@ static void apic_update_lvtt(struct kvm_lapic *apic)
 			kvm_lapic_set_reg(apic, APIC_TMICT, 0);
 			apic->lapic_timer.period = 0;
 			apic->lapic_timer.tscdeadline = 0;
+			/*
+			 * 这里缺少了
+			 * atomic_set(&apic->lapic_timer.pending, 0);
+			 */
 		}
 		apic->lapic_timer.timer_mode = timer_mode;
 		limit_periodic_timer_frequency(apic);
@@ -1546,8 +1618,18 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1625| <<__kvm_wait_lapic_expire>> __wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
+ */
 static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
+	/*
+	 * 在以下修改kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/lapic.c|1604| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2545| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2548| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 */
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
 
 	/*
@@ -1566,6 +1648,10 @@ static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1631| <<__kvm_wait_lapic_expire>> adjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);
+ */
 static inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,
 					      s64 advance_expire_delta)
 {
@@ -1595,14 +1681,37 @@ static inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,
 	apic->lapic_timer.timer_advance_ns = timer_advance_ns;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1638| <<kvm_wait_lapic_expire>> __kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/lapic.c|1690| <<apic_timer_expired>> __kvm_wait_lapic_expire(vcpu);
+ */
 static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u64 guest_tsc, tsc_deadline;
 
+	/*
+	 * 在以下设置kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1628| <<__kvm_wait_lapic_expire>> apic->lapic_timer.expired_tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1727| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_apic_set_state>> apic->lapic_timer.expired_tscdeadline = 0;
+	 * 在以下使用kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1627| <<__kvm_wait_lapic_expire>> tsc_deadline = apic->lapic_timer.expired_tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1690| <<kvm_wait_lapic_expire>> vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 *   - arch/x86/kvm/lapic.c|1743| <<apic_timer_expired>> if (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 */
 	tsc_deadline = apic->lapic_timer.expired_tscdeadline;
 	apic->lapic_timer.expired_tscdeadline = 0;
 	guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	/*
+	 * 在以下修改kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1615| <<__kvm_wait_lapic_expire>> apic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;
+	 *   - arch/x86/kvm/x86.c|10243| <<vcpu_enter_guest>> vcpu->arch.apic->lapic_timer.advance_expire_delta = S64_MIN;
+	 * 在以下使用kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1618| <<__kvm_wait_lapic_expire>> adjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);
+	 *   - arch/x86/kvm/x86.c|10240| <<vcpu_enter_guest>> s64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;
+	 */
 	apic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;
 
 	if (lapic_timer_advance_dynamic) {
@@ -1620,6 +1729,38 @@ static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 		__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
 }
 
+/*
+ * commit d0659d946be05e098883b6955d2764595997f6a4
+ * Author: Marcelo Tosatti <mtosatti@redhat.com>
+ * Date:   Tue Dec 16 09:08:15 2014 -0500
+ *
+ * KVM: x86: add option to advance tscdeadline hrtimer expiration
+ *
+ * For the hrtimer which emulates the tscdeadline timer in the guest,
+ * add an option to advance expiration, and busy spin on VM-entry waiting
+ * for the actual expiration time to elapse.
+ *
+ * This allows achieving low latencies in cyclictest (or any scenario
+ * which requires strict timing regarding timer expiration).
+ *
+ * Reduces average cyclictest latency from 12us to 8us
+ * on Core i5 desktop.
+ *
+ * Note: this option requires tuning to find the appropriate value
+ * for a particular hardware/guest combination. One method is to measure the
+ * average delay between apic_timer_fn and VM-entry.
+ * Another method is to start with 1000ns, and increase the value
+ * in say 500ns increments until avg cyclictest numbers stop decreasing.
+ *
+ * Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3769| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6899| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ */
 void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu) &&
@@ -1630,6 +1771,12 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1758| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|1773| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2649| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1643,6 +1790,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1721| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1829| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1891| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1940| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2454| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1919,6 +2074,10 @@ static void restart_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5665| <<handle_fastpath_preemption_timer>> kvm_lapic_expired_hv_timer(vcpu);
+ */
 void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -1993,6 +2152,20 @@ static void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2228| <<kvm_lapic_reg_write>> kvm_lapic_reg_write(apic, APIC_ICR,
+ *   - arch/x86/kvm/lapic.c|2272| <<apic_mmio_write>> kvm_lapic_reg_write(apic, offset & 0xff0, val);
+ *   - arch/x86/kvm/lapic.c|2279| <<kvm_lapic_set_eoi>> kvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);
+ *   - arch/x86/kvm/lapic.c|2298| <<kvm_apic_write_nodecode>> kvm_lapic_reg_write(vcpu->arch.apic, offset, val);
+ *   - arch/x86/kvm/lapic.c|2902| <<kvm_x2apic_msr_write>> kvm_lapic_reg_write(apic, APIC_ICR2, (u32)(data >> 32));
+ *   - arch/x86/kvm/lapic.c|2903| <<kvm_x2apic_msr_write>> return kvm_lapic_reg_write(apic, reg, (u32)data);
+ *   - arch/x86/kvm/lapic.c|2936| <<kvm_hv_vapic_msr_write>> kvm_lapic_reg_write(apic, APIC_ICR2, (u32)(data >> 32));
+ *   - arch/x86/kvm/lapic.c|2937| <<kvm_hv_vapic_msr_write>> return kvm_lapic_reg_write(apic, reg, (u32)data);
+ *   - arch/x86/kvm/svm/avic.c|337| <<avic_incomplete_ipi_interception>> kvm_lapic_reg_write(apic, APIC_ICR2, icrh);
+ *   - arch/x86/kvm/svm/avic.c|338| <<avic_incomplete_ipi_interception>> kvm_lapic_reg_write(apic, APIC_ICR, icrl);
+ *   - arch/x86/kvm/svm/avic.c|502| <<avic_unaccel_trap_write>> kvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));
+ */
 int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 {
 	int ret = 0;
@@ -2178,6 +2351,10 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
 /* emulate APIC access in a trap manner */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5316| <<handle_apic_write>> kvm_apic_write_nodecode(vcpu, offset);
+ */
 void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
@@ -2317,6 +2494,10 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_apicv);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11783| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2405,6 +2586,12 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1728| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2549| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|402| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2530,6 +2717,10 @@ void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|138| <<kvm_cpu_get_interrupt>> return kvm_get_apic_interrupt(v);
+ */
 int kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)
 {
 	int vector = kvm_apic_has_interrupt(vcpu);
@@ -2779,6 +2970,10 @@ int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3739| <<kvm_set_msr_common>> return kvm_x2apic_msr_write(vcpu, msr, data);
+ */
 int kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 2b44e533fc8d..3e0daca86dbc 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -36,8 +36,42 @@ struct kvm_timer {
 	u32 timer_mode;
 	u32 timer_mode_mask;
 	u64 tscdeadline;
+	/*
+	 * 在以下设置kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1628| <<__kvm_wait_lapic_expire>> apic->lapic_timer.expired_tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1727| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_apic_set_state>> apic->lapic_timer.expired_tscdeadline = 0;
+	 * 在以下使用kvm_timer->expired_tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1627| <<__kvm_wait_lapic_expire>> tsc_deadline = apic->lapic_timer.expired_tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1690| <<kvm_wait_lapic_expire>> vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 *   - arch/x86/kvm/lapic.c|1743| <<apic_timer_expired>> if (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
+	 */
 	u64 expired_tscdeadline;
+	/*
+	 * 在以下修改kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/lapic.c|1604| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2545| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2548| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|16| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|1560| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|1582| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|1681| <<kvm_wait_lapic_expire>> vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|1734| <<apic_timer_expired>> vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|1769| <<start_sw_tscdeadline>> likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|1771| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|2544| <<kvm_create_lapic>> if (timer_advance_ns == -1) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7592| <<vmx_set_hv_timer>> ktimer->timer_advance_ns);
+	 */
 	u32 timer_advance_ns;
+	/*
+	 * 在以下修改kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1615| <<__kvm_wait_lapic_expire>> apic->lapic_timer.advance_expire_delta = guest_tsc - tsc_deadline;
+	 *   - arch/x86/kvm/x86.c|10243| <<vcpu_enter_guest>> vcpu->arch.apic->lapic_timer.advance_expire_delta = S64_MIN;
+	 * 在以下使用kvm_timer->advance_expire_delta:
+	 *   - arch/x86/kvm/lapic.c|1618| <<__kvm_wait_lapic_expire>> adjust_lapic_timer_advance(vcpu, apic->lapic_timer.advance_expire_delta);
+	 *   - arch/x86/kvm/x86.c|10240| <<vcpu_enter_guest>> s64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;
+	 */
 	s64 advance_expire_delta;
 	atomic_t pending;			/* accumulated triggered timers */
 	bool hv_timer_in_use;
@@ -143,6 +177,11 @@ static inline void kvm_lapic_set_vector(int vec, void *bitmap)
 	set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3353| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/vmx/vmx.c|4168| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	kvm_lapic_set_vector(vec, apic->regs + APIC_IRR);
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index e9fbb2c8bbe2..9486cebbf6d1 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -82,6 +82,11 @@ void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10048| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|12271| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu->root_hpa != INVALID_PAGE))
@@ -130,12 +135,28 @@ struct kvm_page_fault {
 
 	/* Derived from mmu and global state.  */
 	const bool is_tdp;
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+	 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	const bool nx_huge_page_workaround_enabled;
 
 	/*
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
 	 */
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	bool huge_page_disallowed;
 
 	/*
@@ -171,14 +192,32 @@ struct kvm_page_fault {
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
 
 extern int nx_huge_pages;
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|221| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ *   - arch/x86/kvm/mmu/spte.c|119| <<make_spte>> is_nx_huge_page_enabled()) {
+ */
 static inline bool is_nx_huge_page_enabled(void)
 {
 	return READ_ONCE(nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5511| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
+ *   - arch/x86/kvm/x86.c|12199| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch)
 {
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+	 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	struct kvm_page_fault fault = {
 		.addr = cr2_or_gpa,
 		.error_code = err,
@@ -235,6 +274,22 @@ static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
  *
  * TODO: introduce APIs to split these two cases.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|586| <<spte_has_volatile_bits>> (is_writable_pte(spte) && (spte & shadow_dirty_mask) == 0))
+ *   - arch/x86/kvm/mmu/mmu.c|655| <<mmu_spte_update>> !is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|765| <<mmu_spte_age>> if (is_writable_pte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|1356| <<spte_write_protect>> if (!is_writable_pte(spte) &&
+ *   - arch/x86/kvm/mmu/mmu.c|3492| <<fast_pf_fix_direct_spte>> if (is_writable_pte(new_spte) && !is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|3504| <<is_access_allowed>> return is_writable_pte(spte);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|206| <<audit_write_protection>> if (is_writable_pte(*sptep))
+ *   - arch/x86/kvm/mmu/spte.c|184| <<make_spte>> if (is_writable_pte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|366| <<handle_changed_spte_dirty_log>> if ((!is_writable_pte(old_spte) || pfn_changed) &&
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|367| <<handle_changed_spte_dirty_log>> is_writable_pte(new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1523| <<age_gfn_range>> if (is_writable_pte(new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1691| <<clear_dirty_gfn_range>> if (is_writable_pte(iter.old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1766| <<clear_dirty_pt_masked>> if (is_writable_pte(iter.old_spte))
+ */
 static inline bool is_writable_pte(unsigned long pte)
 {
 	return pte & PT_WRITABLE_MASK;
@@ -248,6 +303,11 @@ static inline bool is_writable_pte(unsigned long pte)
  * Return zero if the access does not fault; return the page fault error code
  * if the access faults.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|459| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+ *   - arch/x86/kvm/x86.c|6952| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu,
+ */
 static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				  unsigned pte_access, unsigned pte_pkey,
 				  unsigned pfec)
@@ -322,6 +382,23 @@ static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mm
 static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return false; }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|98| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1409| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1442| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1511| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1676| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1689| <<kvm_set_spte_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1750| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1763| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3690| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|6062| <<__kvm_zap_rmaps>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|6137| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|6221| <<kvm_mmu_zap_collapsible_sptes>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|6260| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|11917| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !is_tdp_mmu_enabled(kvm) || kvm_shadow_root_allocated(kvm);
@@ -334,6 +411,10 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|119| <<kvm_mmu_rmaps_stat_show>> lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ */
 static inline unsigned long
 __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 		      int level)
@@ -348,14 +429,39 @@ kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|689| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|2958| <<mmu_set_spte>> kvm_update_page_stats(vcpu->kvm, level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|594| <<__handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
+	/*
+	 * struct kvm:
+	 * -> struct kvm_vm_stat stat;
+	 *    -> union {
+	 *           struct {
+	 *               atomic64_t pages_4k;
+	 *               atomic64_t pages_2m;
+	 *               atomic64_t pages_1g;
+	 *           };
+	 *           atomic64_t pages[KVM_NR_PAGE_SIZES];
+	 *       };
+	 */
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
 }
 
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4002| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|406| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|469| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|833| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u32 access,
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 5628d0ba637e..73dc7812bb61 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -57,6 +57,42 @@
 
 extern bool itlb_multihit_kvm_mitigation;
 
+/*
+ * commit 1aa9b9572b10529c2e64e2b8f44025d86e124308
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Mon Nov 4 20:26:00 2019 +0100
+ *
+ * kvm: x86: mmu: Recovery of shattered NX large pages
+ *
+ * The page table pages corresponding to broken down large pages are zapped in
+ * FIFO order, so that the large page can potentially be recovered, if it is
+ * not longer being used for execution.  This removes the performance penalty
+ * for walking deeper EPT page tables.
+ *
+ * By default, one large page will last about one hour once the guest
+ * reaches a steady state.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ *
+ * kvm.nx_huge_pages_recovery_ratio=
+ *     [KVM] Controls how many 4KiB pages are periodically zapped
+ *     back to huge pages.  0 disables the recovery, otherwise if
+ *     the value is N KVM will zap 1/Nth of the 4KiB pages every
+ *     minute.  The default is 60.
+ *
+ * 在以下使用nx_huge_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|60| <<global>> int __read_mostly nx_huge_pages = -1;
+ *   - arch/x86/kvm/mmu/mmu.c|82| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|83| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ *   - arch/x86/kvm/mmu.h|184| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6328| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ *   - arch/x86/kvm/mmu/mmu.c|6333| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+ *   - arch/x86/kvm/mmu/mmu.c|6374| <<kvm_mmu_module_init>> if (nx_huge_pages == -1)
+ *   - arch/x86/kvm/mmu/mmu.c|6441| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+ */
 int __read_mostly nx_huge_pages = -1;
 static uint __read_mostly nx_huge_pages_recovery_period_ms;
 #ifdef CONFIG_PREEMPT_RT
@@ -88,6 +124,26 @@ module_param_cb(nx_huge_pages_recovery_period_ms, &nx_huge_pages_recovery_param_
 		&nx_huge_pages_recovery_period_ms, 0644);
 __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
 
+/*
+ * commit 71fe70130d88729abc0e658d3202618c340d2e71
+ * Author: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Date:   Fri Mar 20 14:28:28 2020 -0700
+ *
+ * KVM: x86/mmu: Add module param to force TLB flush on root reuse
+ *
+ * Add a module param, flush_on_reuse, to override skip_tlb_flush and
+ * skip_mmu_sync when performing a so called "fast cr3 switch", i.e. when
+ * reusing a cached root.  The primary motiviation for the control is to
+ * provide a fallback mechanism in the event that TLB flushing and/or MMU
+ * sync bugs are exposed/introduced by upcoming changes to stop
+ * unconditionally flushing on nested VMX transitions.
+ *
+ * Suggested-by: Jim Mattson <jmattson@google.com>
+ * Suggested-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Message-Id: <20200320212833.3507-33-sean.j.christopherson@intel.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
 static bool __read_mostly force_flush_and_sync_on_reuse;
 module_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);
 
@@ -102,6 +158,12 @@ bool tdp_enabled = false;
 
 static int max_huge_page_level __read_mostly;
 static int tdp_root_level __read_mostly;
+/*
+ * 在以下使用max_tdp_level:
+ *   - arch/x86/kvm/mmu/mmu.c|4809| <<kvm_mmu_get_tdp_level>> if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)
+ *   - arch/x86/kvm/mmu/mmu.c|4812| <<kvm_mmu_get_tdp_level>> return max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|5496| <<kvm_configure_mmu>> max_tdp_level = tdp_max_root_level;
+ */
 static int max_tdp_level __read_mostly;
 
 enum {
@@ -168,17 +230,30 @@ struct kvm_shadow_walk_iterator {
 	unsigned index;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|954| <<FNAME(invlpg)>> for_each_shadow_entry_using_root(vcpu, root_hpa, gva, iterator) {
+ */
 #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
 					 (_root), (_addr));                \
 	     shadow_walk_okay(&(_walker));			           \
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3058| <<__direct_map>> for_each_shadow_entry(vcpu, fault->addr, it) {
+ */
 #define for_each_shadow_entry(_vcpu, _addr, _walker)            \
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);	\
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3233| <<fast_pf_get_last_sptep>> for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {
+ *   - arch/x86/kvm/mmu/mmu.c|4009| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -187,6 +262,13 @@ struct kvm_shadow_walk_iterator {
 
 static struct kmem_cache *pte_list_desc_cache;
 struct kmem_cache *mmu_page_header_cache;
+/*
+ * 在以下使用kvm_total_used_mmu_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|1831| <<kvm_mod_used_mmu_pages>> percpu_counter_add(&kvm_total_used_mmu_pages, nr);
+ *   - arch/x86/kvm/mmu/mmu.c|6557| <<mmu_shrink_count>> return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6700| <<kvm_mmu_module_init>> if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
+ *   - arch/x86/kvm/mmu/mmu.c|6725| <<kvm_mmu_module_exit>> percpu_counter_destroy(&kvm_total_used_mmu_pages);
+ */
 static struct percpu_counter kvm_total_used_mmu_pages;
 
 static void mmu_spte_set(u64 *sptep, u64 spte);
@@ -278,6 +360,9 @@ static void kvm_flush_remote_tlbs_with_range(struct kvm *kvm,
 {
 	int ret = -ENOTSUPP;
 
+	/*
+	 * 目前只有hv_remote_flush_tlb_with_range()支持
+	 */
 	if (range && kvm_x86_ops.tlb_remote_flush_with_range)
 		ret = static_call(kvm_x86_tlb_remote_flush_with_range)(kvm, range);
 
@@ -285,6 +370,23 @@ static void kvm_flush_remote_tlbs_with_range(struct kvm *kvm,
 		kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1371| <<drop_large_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|1662| <<kvm_set_pte_rmapp>> kvm_flush_remote_tlbs_with_address(kvm, gfn, 1);
+ *   - arch/x86/kvm/mmu/mmu.c|1824| <<rmap_add>> kvm_flush_remote_tlbs_with_address(
+ *   - arch/x86/kvm/mmu/mmu.c|2379| <<kvm_mmu_get_page>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, gfn, 1);
+ *   - arch/x86/kvm/mmu/mmu.c|2570| <<validate_direct_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, child->gfn, 1);
+ *   - arch/x86/kvm/mmu/mmu.c|3094| <<mmu_set_spte>> kvm_flush_remote_tlbs_with_address(vcpu->kvm, gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|6131| <<slot_handle_level_range>> kvm_flush_remote_tlbs_with_address(kvm,
+ *   - arch/x86/kvm/mmu/mmu.c|6474| <<kvm_zap_gfn_range>> kvm_flush_remote_tlbs_with_address(kvm, gfn_start,
+ *   - arch/x86/kvm/mmu/mmu.c|6564| <<kvm_mmu_zap_collapsible_spte>> kvm_flush_remote_tlbs_with_address(kvm, sp->gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|6613| <<kvm_arch_flush_remote_tlbs_memslot>> kvm_flush_remote_tlbs_with_address(kvm, memslot->base_gfn,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|978| <<FNAME(invlpg)>> kvm_flush_remote_tlbs_with_address(vcpu->kvm,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|517| <<handle_removed_tdp_mmu_page>> kvm_flush_remote_tlbs_with_address(kvm, base_gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|732| <<tdp_mmu_zap_spte_atomic>> kvm_flush_remote_tlbs_with_address(kvm, iter->gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1592| <<kvm_tdp_mmu_set_spte_gfn>> kvm_flush_remote_tlbs_with_address(kvm, range->start, 1);
+ */
 void kvm_flush_remote_tlbs_with_address(struct kvm *kvm,
 		u64 start_gfn, u64 pages)
 {
@@ -305,6 +407,11 @@ static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 	mmu_spte_set(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4309| <<handle_mmio_page_fault>> gfn_t gfn = get_mmio_spte_gfn(spte);
+ *   - arch/x86/kvm/mmu/mmu.c|4785| <<sync_mmio_spte>> if (gfn != get_mmio_spte_gfn(*sptep)) {
+ */
 static gfn_t get_mmio_spte_gfn(u64 spte)
 {
 	u64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;
@@ -315,11 +422,19 @@ static gfn_t get_mmio_spte_gfn(u64 spte)
 	return gpa >> PAGE_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4310| <<handle_mmio_page_fault>> unsigned int access = get_mmio_spte_access(spte);
+ */
 static unsigned get_mmio_spte_access(u64 spte)
 {
 	return spte & shadow_mmio_access_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4312| <<handle_mmio_page_fault>> if (!check_mmio_spte(vcpu, spte))
+ */
 static bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)
 {
 	u64 kvm_gen, spte_gen, gen;
@@ -484,6 +599,11 @@ static u64 __get_spte_lockless(u64 *sptep)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|620| <<mmu_spte_update_no_track>> if (!spte_has_volatile_bits(old_spte))
+ *   - arch/x86/kvm/mmu/mmu.c|688| <<mmu_spte_clear_track_bits>> if (!spte_has_volatile_bits(old_spte))
+ */
 static bool spte_has_volatile_bits(u64 spte)
 {
 	if (!is_shadow_present_pte(spte))
@@ -556,6 +676,13 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
  *
  * Returns true if the TLB needs to be flushed
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1388| <<spte_write_protect>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|1413| <<spte_clear_dirty>> return mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|3055| <<mmu_set_spte>> flush |= mmu_spte_update(sptep, spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1111| <<FNAME(sync_page)>> flush |= mmu_spte_update(sptep, spte);
+ */
 static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 {
 	bool flush = false;
@@ -647,6 +774,10 @@ static u64 mmu_spte_get_lockless(u64 *sptep)
 }
 
 /* Restore an acc-track PTE back to a regular PTE */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3618| <<fast_page_fault>> new_spte = restore_acc_track_spte(new_spte);
+ */
 static u64 restore_acc_track_spte(u64 spte)
 {
 	u64 new_spte = spte;
@@ -724,6 +855,14 @@ static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4578| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|5729| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->direct_map);
+ *   - arch/x86/kvm/mmu/mmu.c|5898| <<kvm_mmu_pte_write>> mmu_topup_memory_caches(vcpu, true);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|865| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|952| <<FNAME(invlpg)>> mmu_topup_memory_caches(vcpu, true);
+ */
 static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu, bool maybe_indirect)
 {
 	int r;
@@ -813,16 +952,30 @@ static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|917| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|131| <<kvm_slot_page_track_add_page>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|965| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|169| <<kvm_slot_page_track_remove_page>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2270| <<kvm_mmu_get_page>> account_shadowed(vcpu->kvm, sp);
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -842,17 +995,41 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+ */
 void account_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (sp->lpage_disallowed)
 		return;
 
 	++kvm->stat.nx_lpage_splits;
+	/*
+	 * 注释:
+	 * Use to track shadow pages that, if zapped, would allow KVM to create
+	 * an NX huge page.  A shadow page will have nx_huge_page_disallowed
+	 * set but not be on the list if a huge page is disallowed for other
+	 * reasons, e.g. because KVM is shadowing a PTE at the same gfn, the
+	 * memslot isn't properly aligned, etc...
+	 */
 	list_add_tail(&sp->lpage_disallowed_link,
 		      &kvm->arch.lpage_disallowed_mmu_pages);
 	sp->lpage_disallowed = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2550| <<__kvm_mmu_prepare_zap_page>> unaccount_shadowed(kvm, sp);
+ */
 static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -870,9 +1047,20 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2585| <<__kvm_mmu_prepare_zap_page>> unaccount_huge_nx_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|418| <<tdp_mmu_unlink_page>> unaccount_huge_nx_page(kvm, sp);
+ */
 void unaccount_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	--kvm->stat.nx_lpage_splits;
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->lpage_disallowed = false;
 	list_del(&sp->lpage_disallowed_link);
 }
@@ -1189,6 +1377,12 @@ static bool __drop_large_spte(struct kvm *kvm, u64 *sptep)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3249| <<__direct_map>> drop_large_spte(vcpu, it.sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|680| <<FNAME(fetch)>> drop_large_spte(vcpu, it.sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|740| <<FNAME(fetch)>> drop_large_spte(vcpu, it.sptep);
+ */
 static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	if (__drop_large_spte(vcpu->kvm, sptep)) {
@@ -1212,6 +1406,10 @@ static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
  *
  * Return true if tlb need be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1400| <<__rmap_write_protect>> flush |= spte_write_protect(sptep, pt_protect);
+ */
 static bool spte_write_protect(u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
@@ -1229,6 +1427,12 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 	return mmu_spte_update(sptep, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1473| <<kvm_mmu_write_protect_pt_masked>> __rmap_write_protect(kvm, rmap_head, false);
+ *   - arch/x86/kvm/mmu/mmu.c|1572| <<kvm_mmu_slot_gfn_write_protect>> write_protected |= __rmap_write_protect(kvm, rmap_head, true);
+ *   - arch/x86/kvm/mmu/mmu.c|6456| <<slot_rmap_write_protect>> return __rmap_write_protect(kvm, rmap_head, false);
+ */
 static bool __rmap_write_protect(struct kvm *kvm,
 				 struct kvm_rmap_head *rmap_head,
 				 bool pt_protect)
@@ -1690,6 +1894,11 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1779| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu/mmu.c|2480| <<__kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, long nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
@@ -1860,10 +2069,29 @@ static int __mmu_unsync_walk(struct kvm_mmu_page *sp,
 
 #define INVALID_INDEX (-1)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2229| <<mmu_sync_children>> while (mmu_unsync_walk(parent, &pages)) {
+ *   - arch/x86/kvm/mmu/mmu.c|2625| <<mmu_zap_unsync_children>> while (mmu_unsync_walk(parent, &pages)) {
+ */
 static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 			   struct kvm_mmu_pages *pvec)
 {
 	pvec->nr = 0;
+	/*
+	 * 在以下修改kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1942| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|1981| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 * 在以下使用kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1982| <<clear_unsync_child_bit>> WARN_ON((int )sp->unsync_children < 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|2002| <<__mmu_unsync_walk>> if (child->unsync_children) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2031| <<mmu_unsync_walk>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|2185| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+	 *   - arch/x86/kvm/mmu/mmu.c|2500| <<link_shadow_page>> if (sp->unsync_children || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|4166| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|704| <<FNAME(fetch)>> if (sp->unsync_children &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|991| <<FNAME(nvlpg)>> if (!sp->unsync_children)
+	 */
 	if (!sp->unsync_children)
 		return 0;
 
@@ -1933,6 +2161,17 @@ static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	if (sp->role.invalid)
 		return true;
 
+	/*
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|299| <<__field>> __entry->mmu_valid_gen = kvm->arch.mmu_valid_gen;
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmutrace.h|19| <<KVM_MMU_PAGE_ASSIGN>> __entry->mmu_valid_gen = sp->mmu_valid_gen; \
+	 */
 	/* TDP MMU pages due not use the MMU generation. */
 	return !sp->tdp_mmu_page &&
 	       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
@@ -2062,6 +2301,13 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sptep_to_sp(spte));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3096| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
+ *   - arch/x86/kvm/mmu/mmu.c|3487| <<mmu_alloc_root>> sp = kvm_mmu_get_page(vcpu, gfn, gva, level, direct, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|686| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, table_gfn, fault->addr,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, base_gfn, fault->addr,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -2161,10 +2407,29 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|172| <<for_each_shadow_entry_using_root>> for (shadow_walk_init_using_root(&(_walker), (_vcpu), \
+ *   - arch/x86/kvm/mmu/mmu.c|2202| <<shadow_walk_init>> shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
+ *
+ * 核心思想.
+ * iterator->addr = addr;
+ * iterator->shadow_addr = root;
+ * iterator->level = vcpu->arch.mmu->shadow_root_level;
+ */
 static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
 					struct kvm_vcpu *vcpu, hpa_t root,
 					u64 addr)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->addr = addr;
 	iterator->shadow_addr = root;
 	iterator->level = vcpu->arch.mmu->shadow_root_level;
@@ -2190,23 +2455,60 @@ static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterato
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|178| <<for_each_shadow_entry>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|183| <<for_each_shadow_entry_lockless>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|3815| <<get_walk>> for (shadow_walk_init(&iterator, vcpu, addr),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|674| <<FNAME(fetch)>> for (shadow_walk_init(&it, vcpu, fault->addr);
+ */
 static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 			     struct kvm_vcpu *vcpu, u64 addr)
 {
+	/*
+	 * 核心思想.
+	 * iterator->addr = addr;
+	 * iterator->shadow_addr = vcpu->arch.mmu->root_hpa;
+	 * iterator->level = vcpu->arch.mmu->shadow_root_level;
+	 */
 	shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
 				    addr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|174| <<for_each_shadow_entry_using_root>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|179| <<for_each_shadow_entry>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|184| <<for_each_shadow_entry_lockless>> shadow_walk_okay(&(_walker)) && \
+ *   - arch/x86/kvm/mmu/mmu.c|3817| <<get_walk>> shadow_walk_okay(&iterator);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|675| <<FNAME(fetch)>> shadow_walk_okay(&it) && it.level > gw->level;
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 {
 	if (iterator->level < PG_LEVEL_4K)
 		return false;
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址 
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
 	iterator->sptep	= ((u64 *)__va(iterator->shadow_addr)) + iterator->index;
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|186| <<for_each_shadow_entry_lockless>> __shadow_walk_next(&(_walker), spte))
+ *   - arch/x86/kvm/mmu/mmu.c|2230| <<shadow_walk_next>> __shadow_walk_next(iterator, *iterator->sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3818| <<get_walk>> __shadow_walk_next(&iterator, spte)) {
+ */
 static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 			       u64 spte)
 {
@@ -2215,15 +2517,37 @@ static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 		return;
 	}
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level (比方shadow_addr可以是hpa)
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->shadow_addr = spte & PT64_BASE_ADDR_MASK;
 	--iterator->level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|175| <<for_each_shadow_entry_using_root>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/mmu.c|180| <<for_each_shadow_entry>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|676| <<FNAME(fetch)>> shadow_walk_next(&it)) {
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 {
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3005| <<__direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|717| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|745| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2241,6 +2565,10 @@ static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|738| <<FNAME(fetch)>> validate_direct_spte(vcpu, it.sptep, direct_access);
+ */
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 				   unsigned direct_access)
 {
@@ -2340,6 +2668,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2671| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2727| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|6235| <<kvm_zap_obsolete_pages>> if (__kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|6597| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2398,6 +2733,18 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	return list_unstable;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1909| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2114| <<kvm_mmu_get_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|2294| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child,
+ *   - arch/x86/kvm/mmu/mmu.c|2340| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2543| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3276| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5313| <<kvm_mmu_pte_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|6327| <<kvm_recover_nx_lpages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmutrace.h|203| <<KVM_MMU_PAGE_ASSIGN>> DEFINE_EVENT(kvm_mmu_page_class, kvm_mmu_prepare_zap_page,
+ */
 static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				     struct list_head *invalid_list)
 {
@@ -2432,6 +2779,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2619| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2644| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|6261| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2469,6 +2822,10 @@ static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 	return total_zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2614| <<make_mmu_pages_available>> if (!kvm_mmu_available_pages(vcpu->kvm))
+ */
 static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 {
 	if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
@@ -2478,6 +2835,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3522| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3656| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4211| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|908| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2505,6 +2869,11 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
  * Changing the number of mmu pages allocated to the vm
  * Note: if goal_nr_mmu_pages is too small, you will get dead lock
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5719| <<kvm_vm_ioctl_set_nr_mmu_pages>> kvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);
+ *   - arch/x86/kvm/x86.c|12101| <<kvm_arch_commit_memory_region>> kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
+ */
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
 {
 	write_lock(&kvm->mmu_lock);
@@ -2673,6 +3042,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2985| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|3265| <<__direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|576| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|755| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2777,6 +3153,11 @@ static int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3042| <<direct_pte_prefetch>> __direct_pte_prefetch(vcpu, sp, sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|625| <<FNAME(pte_prefetch)>> return __direct_pte_prefetch(vcpu, sp, sptep);
+ */
 static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 				  struct kvm_mmu_page *sp, u64 *sptep)
 {
@@ -2802,6 +3183,10 @@ static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 		direct_pte_prefetch_many(vcpu, sp, start, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3270| <<__direct_map>> direct_pte_prefetch(vcpu, it.sptep);
+ */
 static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	struct kvm_mmu_page *sp;
@@ -2829,6 +3214,10 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2876| <<kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, pfn, slot);
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -2849,6 +3238,10 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	 */
 	hva = __gfn_to_hva_memslot(slot, gfn);
 
+	/*
+	 * Lookup the page table entry for a virtual address in a given mm. Return a
+	 * pointer to the entry and the level of the mapping.
+	 */
 	pte = lookup_address_in_mm(kvm->mm, hva, &level);
 	if (unlikely(!pte))
 		return PG_LEVEL_4K;
@@ -2856,6 +3249,12 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	return level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2900| <<kvm_mmu_hugepage_adjust>> fault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|5889| <<kvm_mmu_zap_collapsible_spte>> sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1390| <<zap_collapsible_spte_range>> iter.level >= kvm_mmu_max_mapping_level(kvm, slot, iter.gfn,
+ */
 int kvm_mmu_max_mapping_level(struct kvm *kvm,
 			      const struct kvm_memory_slot *slot, gfn_t gfn,
 			      kvm_pfn_t pfn, int max_level)
@@ -2877,11 +3276,25 @@ int kvm_mmu_max_mapping_level(struct kvm *kvm,
 	return min(host_level, max_level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3200| <<__direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ */
 void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -2913,6 +3326,15 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->pfn &= ~mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3180| <<__direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|732| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1253| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+ *
+ * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+ * 所以就没法huge page了, 只能让goal_level再下降一位
+ */
 void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
 {
 	if (cur_level > PG_LEVEL_4K &&
@@ -2933,21 +3355,90 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4103| <<direct_page_fault>> r = __direct_map(vcpu, fault);
+ *
+ * tdp mmu的版本是kvm_tdp_mmu_map()
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;
+	 *     hpa_t shadow_addr;
+	 *     u64 *sptep;
+	 *     int level;
+	 *     unsigned index;
+	 * };
+	 */
 	struct kvm_shadow_walk_iterator it;
 	struct kvm_mmu_page *sp;
 	int ret;
 	gfn_t base_gfn = fault->gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3200| <<__direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level (比方shadow_addr可以是hpa)
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
 		 */
+		/*
+		 * disallowed_hugepage_adjust():
+		 * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+		 * 所以就没法huge page了, 只能让goal_level再下降一位
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
 
@@ -2963,11 +3454,20 @@ static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 				      it.level - 1, true, ACC_ALL);
 
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下使用account_huge_nx_page():
+		 *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+		 */
 		if (fault->is_tdp && fault->huge_page_disallowed &&
 		    fault->req_level >= it.level)
 			account_huge_nx_page(vcpu->kvm, sp);
 	}
 
+	/*
+	 * WARN_ON_ONCE说明这里不会发生
+	 */
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
@@ -3004,6 +3504,11 @@ static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4316| <<direct_page_fault>> if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> if (handle_abnormal_pfn(vcpu, fault, walker.pte_access, &r))
+ */
 static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 				unsigned int access, int *ret_val)
 {
@@ -3032,6 +3537,10 @@ static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fa
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3589| <<fast_page_fault>> if (!page_fault_can_be_fast(fault))
+ */
 static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
 {
 	/*
@@ -3041,6 +3550,9 @@ static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
 	if (fault->rsvd)
 		return false;
 
+	/*
+	 * 如果non-present, 下面一定不返回, 会继续 ... present就是read
+	 */
 	/* See if the page fault is due to an NX violation */
 	if (unlikely(fault->exec && fault->present))
 		return false;
@@ -3112,6 +3624,10 @@ static bool is_access_allowed(struct kvm_page_fault *fault, u64 spte)
  *  - Must be called between walk_shadow_page_lockless_{begin,end}.
  *  - The returned sptep must not be used after walk_shadow_page_lockless_end.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3538| <<fast_page_fault>> sptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 {
 	struct kvm_shadow_walk_iterator iterator;
@@ -3129,6 +3645,38 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4426| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3174,6 +3722,11 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 		new_spte = spte;
 
+		/*
+		 * 这里应该是如果hardware的a/d bit不被支持的话,
+		 * 需要通过page fault来标记
+		 * 如果硬件支持就可以不用page fault
+		 */
 		if (is_access_track_spte(spte))
 			new_spte = restore_acc_track_spte(new_spte);
 
@@ -3230,6 +3783,12 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3287| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa,
+ *   - arch/x86/kvm/mmu/mmu.c|3293| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->root_hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3299| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->pae_root[i],
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3350,6 +3909,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5129| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3568,6 +4131,10 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5639| <<kvm_mmu_load>> r = mmu_alloc_special_roots(vcpu);
+ */
 static int mmu_alloc_special_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3825,6 +4392,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5863| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -3914,6 +4485,13 @@ static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				  kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4302| <<direct_page_fault>> if (kvm_faultin_pfn(vcpu, fault, &r))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|876| <<FNAME(page_fault)>> if (kvm_faultin_pfn(vcpu, fault, &r))
+ *
+ * 核心思想是填充fault->pfn
+ */
 static bool kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault, int *r)
 {
 	struct kvm_memory_slot *slot = fault->slot;
@@ -4003,6 +4581,11 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_notifier_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4121| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4169| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@ -4024,17 +4607,39 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r)
 		return r;
 
+	/*
+	 * arm和x86在以下使用kvm->mmu_notifier_seq:
+	 *   - arch/arm64/kvm/mmu.c|1167| <<user_mem_abort>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/mmu.c|4528| <<direct_page_fault>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - include/linux/kvm_host.h|1936| <<mmu_notifier_retry>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - include/linux/kvm_host.h|1956| <<mmu_notifier_retry_hva>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - virt/kvm/kvm_main.c|745| <<kvm_dec_notifier_count>> kvm->mmu_notifier_seq++;
+	 *   - virt/kvm/pfncache.c|151| <<hva_to_pfn_retry>> mmu_seq = kvm->mmu_notifier_seq;
+	 */
 	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 	smp_rmb();
 
+	/*
+	 * 核心思想是填充fault->pfn
+	 */
 	if (kvm_faultin_pfn(vcpu, fault, &r))
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4316| <<direct_page_fault>> if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> if (handle_abnormal_pfn(vcpu, fault, walker.pte_access, &r))
+	 */
 	if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
 		return r;
 
 	r = RET_PF_RETRY;
 
+	/*
+	 * 如果走上面的fast_page_fault()就不用任何lock了
+	 * 所以是fast path
+	 */
 	if (is_tdp_mmu_fault)
 		read_lock(&vcpu->kvm->mmu_lock);
 	else
@@ -4061,6 +4666,10 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return r;
 }
 
+/*
+ * 在以下使用nonpaging_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4174| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+ */
 static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4071,6 +4680,11 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1775| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address,
+ *   - arch/x86/kvm/vmx/vmx.c|4903| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4104,9 +4718,46 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu.h|199| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu.h|208| <<kvm_mmu_do_page_fault>> return kvm_tdp_page_fault(vcpu, &fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4840| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	while (fault->max_level > PG_LEVEL_4K) {
+		/*
+		 * kvm最大1G, 2M, 4K ...
+		 */
 		int page_num = KVM_PAGES_PER_HPAGE(fault->max_level);
 		gfn_t base = (fault->addr >> PAGE_SHIFT) & ~(page_num - 1);
 
@@ -4115,6 +4766,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 		--fault->max_level;
 	}
+	/*
+	 * 大部分情况可以假设, "--fault->max_level"不现实
+	 * 结果还是.max_level = KVM_MAX_HUGEPAGE_LEVEL
+	 */
 
 	return direct_page_fault(vcpu, fault);
 }
@@ -4236,6 +4891,10 @@ static unsigned long get_cr3(struct kvm_vcpu *vcpu)
 	return kvm_read_cr3(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1094| <<FNAME(sync_page)>> if (sync_mmio_spte(vcpu, &sp->spt[i], gfn, pte_access))
+ */
 static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 			   unsigned int access)
 {
@@ -4265,6 +4924,12 @@ static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 #include "paging_tmpl.h"
 #undef PTTYPE
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5035| <<reset_rsvds_bits_mask>> __reset_rsvds_bits_mask(&context->guest_rsvd_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5128| <<reset_shadow_zero_bits_mask>> __reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),
+ *   - arch/x86/kvm/mmu/mmu.c|5166| <<reset_tdp_shadow_zero_bits_mask>> __reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),
+ */
 static void
 __reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,
 			u64 pa_bits_rsvd, int level, bool nx, bool gbpages,
@@ -4381,6 +5046,12 @@ static void reset_rsvds_bits_mask(struct kvm_vcpu *vcpu,
 				guest_cpuid_is_amd_or_hygon(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5084| <<reset_rsvds_bits_mask_ept>> __reset_rsvds_bits_mask_ept(&context->guest_rsvd_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5161| <<reset_tdp_shadow_zero_bits_mask>> __reset_rsvds_bits_mask_ept(shadow_zero_check,
+ *   - arch/x86/kvm/mmu/mmu.c|5182| <<reset_ept_shadow_zero_bits_mask>> __reset_rsvds_bits_mask_ept(&context->shadow_zero_check,
+ */
 static void
 __reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,
 			    u64 pa_bits_rsvd, bool execonly, int huge_page_level)
@@ -4484,6 +5155,10 @@ static inline bool boot_cpu_is_amd(void)
  * the direct page table on host, use as much mmu features as
  * possible, however, kvm currently does not do execution-protection.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5467| <<init_kvm_tdp_mmu>> reset_tdp_shadow_zero_bits_mask(vcpu, context);
+ */
 static void
 reset_tdp_shadow_zero_bits_mask(struct kvm_vcpu *vcpu,
 				struct kvm_mmu *context)
@@ -5124,6 +5799,10 @@ void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 	WARN_ON(VALID_PAGE(vcpu->arch.guest_mmu.root_hpa));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5962| <<kvm_mmu_pte_write>> if (need_remote_flush(entry, *spte))
+ */
 static bool need_remote_flush(u64 old, u64 new)
 {
 	if (!is_shadow_present_pte(old))
@@ -5239,6 +5918,10 @@ static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6427| <<kvm_mmu_init_vm>> node->track_write = kvm_mmu_pte_write;
+ */
 static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 			      const u8 *new, int bytes,
 			      struct kvm_page_track_notifier_node *node)
@@ -5300,6 +5983,13 @@ static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4277| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|1789| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/vmx.c|5430| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5451| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5575,6 +6265,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11180| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -5605,6 +6299,10 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 }
 
 #define BATCH_ZAP_PAGES	10
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5899| <<kvm_mmu_zap_all_fast>> kvm_zap_obsolete_pages(kvm);
+ */
 static void kvm_zap_obsolete_pages(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -5664,6 +6362,12 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
  * not use any resource of the being-deleted slot or all slots
  * after calling the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6070| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6186| <<set_nx_huge_pages>> kvm_mmu_zap_all_fast(kvm);
+ */
 static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 {
 	lockdep_assert_held(&kvm->slots_lock);
@@ -5715,6 +6419,10 @@ static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
 	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
 }
 
+/*
+ * 在以下使用kvm_mmu_invalidate_zap_pages_in_memslot():
+ *   - arch/x86/kvm/mmu/mmu.c|6199| <<kvm_mmu_init_vm>> node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
+ */
 static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
 			struct kvm_memory_slot *slot,
 			struct kvm_page_track_notifier_node *node)
@@ -5780,6 +6488,12 @@ static bool __kvm_zap_rmaps(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
  * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
  * (not including it)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|332| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+ *   - arch/x86/kvm/x86.c|880| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+ *   - arch/x86/kvm/x86.c|9758| <<__kvm_request_apicv_update>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+ */
 void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 {
 	bool flush;
@@ -5903,6 +6617,10 @@ static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,
 	return need_tlb_flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12482| <<kvm_mmu_slot_apply_flags>> kvm_mmu_zap_collapsible_sptes(kvm, new);
+ */
 void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				   const struct kvm_memory_slot *slot)
 {
@@ -5971,6 +6689,10 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12113| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ */
 void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -6083,6 +6805,13 @@ mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 static unsigned long
 mmu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
 {
+	/*
+	 * 在以下使用kvm_total_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1831| <<kvm_mod_used_mmu_pages>> percpu_counter_add(&kvm_total_used_mmu_pages, nr);
+	 *   - arch/x86/kvm/mmu/mmu.c|6557| <<mmu_shrink_count>> return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6700| <<kvm_mmu_module_init>> if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
+	 *   - arch/x86/kvm/mmu/mmu.c|6725| <<kvm_mmu_module_exit>> percpu_counter_destroy(&kvm_total_used_mmu_pages);
+	 */
 	return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
 }
 
@@ -6092,6 +6821,11 @@ static struct shrinker mmu_shrinker = {
 	.seeks = DEFAULT_SEEKS * 10,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6369| <<kvm_mmu_module_init>> mmu_destroy_caches();
+ *   - arch/x86/kvm/mmu/mmu.c|6383| <<kvm_mmu_module_exit>> mmu_destroy_caches();
+ */
 static void mmu_destroy_caches(void)
 {
 	kmem_cache_destroy(pte_list_desc_cache);
@@ -6104,11 +6838,48 @@ static bool get_nx_auto_mode(void)
 	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();
 }
 
+/*
+ * commit b8e8c8303ff28c61046a4d0f6ea99aea609a7dc0
+ * Author: Paolo Bonzini <pbonzini@redhat.com>
+ * Date:   Mon Nov 4 12:22:02 2019 +0100
+ *
+ * kvm: mmu: ITLB_MULTIHIT mitigation
+ *
+ * With some Intel processors, putting the same virtual address in the TLB
+ * as both a 4 KiB and 2 MiB page can confuse the instruction fetch unit
+ * and cause the processor to issue a machine check resulting in a CPU lockup.
+ *
+ * Unfortunately when EPT page tables use huge pages, it is possible for a
+ * malicious guest to cause this situation.
+ *
+ * Add a knob to mark huge pages as non-executable. When the nx_huge_pages
+ * parameter is enabled (and we are using EPT), all huge pages are marked as
+ * NX. If the guest attempts to execute in one of those pages, the page is
+ * broken down into 4K pages, which are then marked executable.
+ *
+ * This is not an issue for shadow paging (except nested EPT), because then
+ * the host is in control of TLB flushes and the problematic situation cannot
+ * happen.  With nested EPT, again the nested guest can cause problems shadow
+ * and direct EPT is treated in the same way.
+ *
+ * [ tglx: Fixup default to auto and massage wording a bit ]
+ *
+ * Originally-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6346| <<set_nx_huge_pages>> __set_nx_huge_pages(new_val);
+ *   - arch/x86/kvm/mmu/mmu.c|6375| <<kvm_mmu_module_init>> __set_nx_huge_pages(get_nx_auto_mode());
+ */
 static void __set_nx_huge_pages(bool val)
 {
 	nx_huge_pages = itlb_multihit_kvm_mitigation = val;
 }
 
+/*
+ * struct kernel_param_ops nx_huge_pages_ops.set = set_nx_huge_pages()
+ */
 static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 {
 	bool old_val = nx_huge_pages;
@@ -6144,6 +6915,15 @@ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8844| <<kvm_arch_init>> r = kvm_mmu_module_init();
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ *       -> kvm_mmu_module_init()
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -6189,6 +6969,11 @@ int kvm_mmu_module_init(void)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11260| <<kvm_arch_vcpu_create>> kvm_mmu_destroy(vcpu);
+ *   - arch/x86/kvm/x86.c|11301| <<kvm_arch_vcpu_destroy>> kvm_mmu_destroy(vcpu);
+ */
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -6197,6 +6982,10 @@ void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 	mmu_free_memory_caches(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8920| <<kvm_arch_exit>> kvm_mmu_module_exit();
+ */
 void kvm_mmu_module_exit(void)
 {
 	mmu_destroy_caches();
@@ -6275,6 +7064,13 @@ static void kvm_recover_nx_lpages(struct kvm *kvm)
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|947| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|6641| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+		 *   - arch/x86/kvm/x86.c|11639| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+		 */
 		if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
 			break;
 
@@ -6342,6 +7138,10 @@ static int kvm_nx_lpage_recovery_worker(struct kvm *kvm, uintptr_t data)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11707| <<kvm_arch_post_init_vm>> return kvm_mmu_post_init_vm(kvm);
+ */
 int kvm_mmu_post_init_vm(struct kvm *kvm)
 {
 	int err;
@@ -6355,6 +7155,10 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11821| <<kvm_arch_pre_destroy_vm>> kvm_mmu_pre_destroy_vm(kvm);
+ */
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
 {
 	if (kvm->arch.nx_lpage_recovery_thread)
diff --git a/arch/x86/kvm/mmu/mmu_audit.c b/arch/x86/kvm/mmu/mmu_audit.c
index 9e7dcf999f08..b62833c457f0 100644
--- a/arch/x86/kvm/mmu/mmu_audit.c
+++ b/arch/x86/kvm/mmu/mmu_audit.c
@@ -233,6 +233,14 @@ static void audit_vcpu_spte(struct kvm_vcpu *vcpu)
 	mmu_spte_walk(vcpu, audit_spte);
 }
 
+/*
+ * 在以下使用mmu_audit:
+ *   - arch/x86/kvm/mmu/mmu_audit.c|303| <<global>> arch_param_cb(mmu_audit, &audit_param_ops, &mmu_audit, 0644);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|259| <<mmu_audit_enable>> if (mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|263| <<mmu_audit_enable>> mmu_audit = true;
+ *   - arch/x86/kvm/mmu/mmu_audit.c|268| <<mmu_audit_disable>> if (!mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|272| <<mmu_audit_disable>> mmu_audit = false;
+ */
 static bool mmu_audit;
 static DEFINE_STATIC_KEY_FALSE(mmu_audit_key);
 
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index da6166b5c377..407205ee242c 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -40,7 +40,35 @@ struct kvm_mmu_page {
 
 	bool tdp_mmu_page;
 	bool unsync;
+	/*
+	 * 在以下使用kvm_arch->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmu.c|6279| <<kvm_mmu_zap_all_fast>> kvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;
+	 *   - arch/x86/kvm/mmu/mmutrace.h|299| <<__field>> __entry->mmu_valid_gen = kvm->arch.mmu_valid_gen;
+	 * 在以下使用kvm_mmu_page->mmu_valid_gen:
+	 *   - arch/x86/kvm/mmu/mmu.c|1916| <<kvm_mmu_alloc_page>> sp->mmu_valid_gen = vcpu->kvm->arch.mmu_valid_gen;
+	 *   - arch/x86/kvm/mmu/mmu.c|2102| <<is_obsolete_sp>> unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/mmutrace.h|19| <<KVM_MMU_PAGE_ASSIGN>> __entry->mmu_valid_gen = sp->mmu_valid_gen; \
+	 */
 	u8 mmu_valid_gen;
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
+	/*
+	 * 在以下设置kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|934| <<account_huge_nx_page>> sp->lpage_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|957| <<unaccount_huge_nx_page>> sp->lpage_disallowed = false;
+	 * 在以下使用kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|928| <<account_huge_nx_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2558| <<__kvm_mmu_prepare_zap_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|6620| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(!sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|6625| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|407| <<tdp_mmu_unlink_page>> if (sp->lpage_disallowed)
+	 */
 	bool lpage_disallowed; /* Can't be replaced by an equiv large page */
 
 	/*
@@ -55,13 +83,60 @@ struct kvm_mmu_page {
 	gfn_t *gfns;
 	/* Currently serving as active root */
 	union {
+		/*
+		 * 在以下设置kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+		 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+		 * 在以下使用kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+		 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+		 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+		 */
 		int root_count;
+		/*
+		 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
+	/*
+	 * 在以下修改kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1942| <<mark_unsync>> if (sp->unsync_children++)
+	 *   - arch/x86/kvm/mmu/mmu.c|1981| <<clear_unsync_child_bit>> --sp->unsync_children;
+	 * 在以下使用kvm_mmu_page->unsync_children:
+	 *   - arch/x86/kvm/mmu/mmu.c|1982| <<clear_unsync_child_bit>> WARN_ON((int )sp->unsync_children < 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|2002| <<__mmu_unsync_walk>> if (child->unsync_children) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2031| <<mmu_unsync_walk>> if (!sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/mmu.c|2185| <<mmu_pages_clear_parents>> } while (!sp->unsync_children);
+	 *   - arch/x86/kvm/mmu/mmu.c|2500| <<link_shadow_page>> if (sp->unsync_children || sp->unsync)
+	 *   - arch/x86/kvm/mmu/mmu.c|4166| <<is_unsync_root>> if (sp->unsync || sp->unsync_children)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|704| <<FNAME(fetch)>> if (sp->unsync_children &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|991| <<FNAME(nvlpg)>> if (!sp->unsync_children)
+	 */
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
+	/*
+	 * 注释:
+	 * Use to track shadow pages that, if zapped, would allow KVM to create
+	 * an NX huge page.  A shadow page will have nx_huge_page_disallowed
+	 * set but not be on the list if a huge page is disallowed for other
+	 * reasons, e.g. because KVM is shadowing a PTE at the same gfn, the
+	 * memslot isn't properly aligned, etc...
+	 */
+	/*
+	 * 在kvm_mmu_page->lpage_disallowed_link:
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link,
+	 *   - arch/x86/kvm/mmu/mmu.c|958| <<unaccount_huge_nx_page>> list_del(&sp->lpage_disallowed_link);
+	 *   - arch/x86/kvm/mmu/mmu.c|6619| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages, struct kvm_mmu_page, lpage_disallowed_link);
+	 */
 	struct list_head lpage_disallowed_link;
 #ifdef CONFIG_X86_32
 	/*
@@ -82,6 +157,31 @@ struct kvm_mmu_page {
 
 extern struct kmem_cache *mmu_page_header_cache;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1836| <<__mmu_unsync_walk>> child = to_shadow_page(ent & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2257| <<validate_direct_spte>> child = to_shadow_page(*sptep & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2278| <<mmu_page_zap_pte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2710| <<mmu_set_spte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3251| <<mmu_free_root_page>> sp = to_shadow_page(*root_hpa & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3330| <<kvm_mmu_free_guest_mode_roots>> if (!to_shadow_page(root_hpa) ||
+ *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_free_guest_mode_roots>> to_shadow_page(root_hpa)->role.guest_mode)
+ *   - arch/x86/kvm/mmu/mmu.c|3672| <<is_unsync_root>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3694| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3717| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3995| <<is_page_fault_stale>> struct kvm_mmu_page *sp = to_shadow_page(vcpu->arch.mmu->root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4145| <<is_root_usable>> VALID_PAGE(root->hpa) && to_shadow_page(root->hpa) &&
+ *   - arch/x86/kvm/mmu/mmu.c|4146| <<is_root_usable>> role.word == to_shadow_page(root->hpa)->role.word;
+ *   - arch/x86/kvm/mmu/mmu.c|4235| <<__kvm_mmu_new_pgd>> to_shadow_page(vcpu->arch.mmu->root_hpa));
+ *   - arch/x86/kvm/mmu/mmu_audit.c|48| <<__mmu_spte_walk>> child = to_shadow_page(ent[i] & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|65| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|75| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|94| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|107| <<is_tdp_mmu>> sp = to_shadow_page(hpa);
+ *
+ * In some case, special roots are used in mmu.  It is often using
+ * to_shadow_page(mmu->root.hpa) to check if special roots are used.
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 5b5bdac97c7b..a683aed3f11b 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -719,6 +719,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 73cfe62fdad1..5bac27455b44 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -23,10 +23,27 @@ static bool __read_mostly enable_mmio_caching = true;
 module_param_named(mmio_caching, enable_mmio_caching, bool, 0444);
 
 u64 __read_mostly shadow_host_writable_mask;
+/*
+ * 在以下设置shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/spte.c|353| <<kvm_mmu_set_ept_masks>> shadow_mmu_writable_mask = EPT_SPTE_MMU_WRITABLE;
+ *   - arch/x86/kvm/mmu/spte.c|405| <<kvm_mmu_reset_all_pte_masks>> shadow_mmu_writable_mask = DEFAULT_SPTE_MMU_WRITEABLE;
+ * 在以下使用shadow_mmu_writable_mask:
+ *   - arch/x86/kvm/mmu/mmu.c|1363| <<spte_write_protect>> spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|164| <<make_spte>> spte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.c|186| <<make_spte>> spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ *   - arch/x86/kvm/mmu/spte.c|248| <<kvm_mmu_changed_pte_notifier_make_spte>> new_spte &= ~shadow_mmu_writable_mask;
+ *   - arch/x86/kvm/mmu/spte.h|391| <<spte_can_locklessly_be_made_writable>> if (spte & shadow_mmu_writable_mask) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1904| <<write_protect_gfn>> ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
+ */
 u64 __read_mostly shadow_mmu_writable_mask;
 u64 __read_mostly shadow_nx_mask;
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下设置shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|330| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|382| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ */
 u64 __read_mostly shadow_accessed_mask;
 u64 __read_mostly shadow_dirty_mask;
 u64 __read_mostly shadow_mmio_value;
@@ -34,6 +51,11 @@ u64 __read_mostly shadow_mmio_mask;
 u64 __read_mostly shadow_mmio_access_mask;
 u64 __read_mostly shadow_present_mask;
 u64 __read_mostly shadow_me_mask;
+/*
+ * 在以下设置shadow_acc_track_mask:
+ *   - arch/x86/kvm/mmu/spte.c|361| <<kvm_mmu_set_ept_masks>> shadow_acc_track_mask = VMX_EPT_RWX_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|413| <<kvm_mmu_reset_all_pte_masks>> shadow_acc_track_mask = 0;
+ */
 u64 __read_mostly shadow_acc_track_mask;
 
 u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
@@ -52,6 +74,11 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|377| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1240| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
@@ -90,6 +117,12 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 				     E820_TYPE_RAM);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2948| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1101| <<FNAME(sync_page)>> make_spte(vcpu, sp, slot, pte_access, gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1242| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn,
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
@@ -192,8 +225,21 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return wrprot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2420| <<link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1417| <<kvm_tdp_mmu_map>> new_spte = make_nonleaf_spte(child_pt,
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
@@ -270,6 +316,12 @@ u64 mark_spte_for_access_track(u64 spte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/spte.c|376| <<kvm_mmu_set_ept_masks>> kvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE,
+ *   - arch/x86/kvm/mmu/spte.c|436| <<kvm_mmu_reset_all_pte_masks>> kvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);
+ *   - arch/x86/kvm/svm/svm.c|4701| <<svm_adjust_mmio_mask>> kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
+ */
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 {
 	BUG_ON((u64)(unsigned)access_mask != access_mask);
@@ -305,6 +357,10 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7997| <<hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits,
+ */
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 {
 	shadow_user_mask	= VMX_EPT_READABLE_MASK;
@@ -328,6 +384,10 @@ void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_ept_masks);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6799| <<kvm_mmu_module_init>> kvm_mmu_reset_all_pte_masks();
+ */
 void kvm_mmu_reset_all_pte_masks(void)
 {
 	u8 low_phys_bits;
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index be6a007a4af3..4ecd2e6076eb 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -12,6 +12,14 @@
  * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
  * enough that the improved code generation is noticeable in KVM's footprint.
  */
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
 /*
@@ -53,11 +61,24 @@ static_assert(SPTE_TDP_AD_ENABLED_MASK == 0);
 
 #define PT64_LEVEL_BITS 9
 
+/*
+ * PT64_LEVEL_SHIFT(1) = 12 + 0 * 9 = 12
+ * PT64_LEVEL_SHIFT(2) = 12 + 1 * 9 = 21
+ * PT64_LEVEL_SHIFT(3) = 12 + 2 * 9 = 30
+ * PT64_LEVEL_SHIFT(4) = 12 + 3 * 9 = 39
+ * PT64_LEVEL_SHIFT(5) = 12 + 4 * 9 = 48
+ */
 #define PT64_LEVEL_SHIFT(level) \
 		(PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
 
 #define PT64_INDEX(address, level)\
 	(((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2211| <<shadow_walk_okay>> iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|26| <<tdp_iter_refresh_sptep>> SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|181| <<try_step_side>> if (SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level) ==
+ */
 #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
 
 /*
@@ -201,11 +222,29 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|215| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|299| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|219| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|453| <<handle_removed_tdp_mmu_page>> old_child_spte = xchg(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|480| <<handle_removed_tdp_mmu_page>> WRITE_ONCE(*sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|483| <<handle_removed_tdp_mmu_page>> old_child_spte, REMOVED_SPTE, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ */
 #define REMOVED_SPTE	0x5a0ULL
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|484| <<handle_removed_tdp_mmu_page>> if (!is_removed_spte(old_child_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|607| <<__handle_changed_spte>> !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|686| <<tdp_mmu_set_spte_atomic>> if (is_removed_spte(iter->old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|798| <<__tdp_mmu_set_spte>> WARN_ON(is_removed_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1417| <<kvm_tdp_mmu_map>> if (is_removed_spte(iter.old_spte))
+ */
 static inline bool is_removed_spte(u64 spte)
 {
 	return spte == REMOVED_SPTE;
@@ -235,6 +274,20 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * A MMU present SPTE is backed by actual memory and may or may not be present
+	 * in hardware.  E.g. MMIO SPTEs are not considered present.  Use bit 11, as it
+	 * is ignored by all flavors of SPTEs and checking a low bit often generates
+	 * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
+	 * enough that the improved code generation is noticeable in KVM's footprint.
+	 *
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
@@ -272,6 +325,14 @@ static inline u64 spte_shadow_dirty_mask(u64 spte)
 	return spte_ad_enabled(spte) ? shadow_dirty_mask : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|581| <<spte_has_volatile_bits>> is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/mmu.c|739| <<restore_acc_track_spte>> WARN_ON_ONCE(!is_access_track_spte(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|3617| <<fast_page_fault>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.c|291| <<mark_spte_for_access_track>> if (is_access_track_spte(spte))
+ *   - arch/x86/kvm/mmu/spte.h|358| <<is_accessed_spte>> : !is_access_track_spte(spte);
+ */
 static inline bool is_access_track_spte(u64 spte)
 {
 	return !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index caa96c270b95..a588b75c200b 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -8,15 +8,38 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|34| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|99| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|141| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
+	/*
+	 * tdp_iter->sptep:
+	 *    A pointer to the current SPTE
+	 * tdp_iter->pt_path[PT64_ROOT_MAX_LEVEL]:
+	 *    Pointers to the page tables traversed to reach the current SPTE
+	 */
 	iter->sptep = iter->pt_path[iter->level - 1] +
 		SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|33| <<tdp_iter_restart>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> iter->gfn = round_gfn_for_level(iter->gfn, iter->level);
+ */
 static gfn_t round_gfn_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) = KVM_HPAGE_SIZE(1) / 4096 = 4K / 4K = 1
+	 * KVM_PAGES_PER_HPAGE(2) = KVM_HPAGE_SIZE(2) / 4096 = 2M / 4K = 512
+	 * KVM_PAGES_PER_HPAGE(3) = KVM_HPAGE_SIZE(3) / 4096 = 1G / 4K = 262144
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -24,12 +47,30 @@ static gfn_t round_gfn_for_level(gfn_t gfn, int level)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|55| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|165| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
 	iter->yielded = false;
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 *
+	 * tdp_iter->gfn:
+	 *    The lowest GFN mapped by the current SPTE
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -40,6 +81,10 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|61| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, root_level, min_level, start); \
+ */
 void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -47,8 +92,17 @@ void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 	WARN_ON(root_level > PT64_ROOT_MAX_LEVEL);
 
 	iter->next_last_level_gfn = next_last_level_gfn;
+	/*
+	 * The level of the root page given to the iterator
+	 */
 	iter->root_level = root_level;
+	/*
+	 * The lowest level the iterator should traverse to
+	 */
 	iter->min_level = min_level;
+	/*
+	 * root_pt是虚拟地址(va)
+	 */
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root_pt;
 	iter->as_id = kvm_mmu_page_as_id(sptep_to_sp(root_pt));
 
@@ -76,6 +130,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|220| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -87,14 +145,28 @@ static bool try_step_down(struct tdp_iter *iter)
 	 * Reread the SPTE before stepping down to avoid traversing into page
 	 * tables that are no longer linked from this entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
+	/*
+	 * 这里得到的是当前pte所指向的下一个page table的page
+	 */
 	child_pt = spte_to_child_pt(iter->old_spte, iter->level);
 	if (!child_pt)
 		return false;
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -118,9 +190,27 @@ static bool try_step_side(struct tdp_iter *iter)
             (PT64_ENT_PER_PAGE - 1))
 		return false;
 
+	/*
+	 * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+	 * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+	 * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+	 * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+	 * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+	 */
 	iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->next_last_level_gfn = iter->gfn;
 	iter->sptep++;
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
 	return true;
@@ -131,6 +221,10 @@ static bool try_step_side(struct tdp_iter *iter)
  * can continue from the next entry in the parent page table. Returns true on a
  * successful step up, false if already in the root page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|272| <<tdp_iter_next>> } while (try_step_up(iter));
+ */
 static bool try_step_up(struct tdp_iter *iter)
 {
 	if (iter->level == iter->root_level)
@@ -159,8 +253,23 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|63| <<for_each_tdp_pte_min_level>> tdp_iter_next(&iter))
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	if (iter->yielded) {
 		tdp_iter_restart(iter);
 		return;
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index e19cabbcb65c..4aa6800c2650 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -17,12 +17,22 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
 	 * yielded. Only yielding when the next_last_level_gfn !=
 	 * yielded_gfn helps ensure forward progress.
 	 */
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	gfn_t yielded_gfn;
 	/* Pointers to the page tables traversed to reach the current SPTE */
 	tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL];
@@ -39,6 +49,12 @@ struct tdp_iter {
 	/* The address space ID, i.e. SMM vs. regular. */
 	int as_id;
 	/* A snapshot of the value at sptep */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	u64 old_spte;
 	/*
 	 * Whether the iterator has a valid state. This will be false if the
@@ -50,6 +66,17 @@ struct tdp_iter {
 	 * which case tdp_iter_next() needs to restart the walk at the root
 	 * level instead of advancing to the next entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	bool yielded;
 };
 
@@ -57,11 +84,23 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|727| <<zap_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1183| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1439| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ */
 #define for_each_tdp_pte_min_level(iter, root, root_level, min_level, start, end) \
 	for (tdp_iter_start(&iter, root, root_level, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|627| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|637| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, __va(_mmu->root_hpa), \
+ */
 #define for_each_tdp_pte(iter, root, root_level, start, end) \
 	for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bc9e3553fba2..22c894815c2f 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -10,6 +10,12 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用tdp_mmu_enabled:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|13| <<global>> static bool __read_mostly tdp_mmu_enabled = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|14| <<global>> module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|19| <<kvm_mmu_init_tdp_mmu>> if (!tdp_enabled || !READ_ONCE(tdp_mmu_enabled))
+ */
 static bool __read_mostly tdp_mmu_enabled = true;
 module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
 
@@ -29,15 +35,32 @@ bool kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|85| <<kvm_tdp_mmu_put_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|723| <<zap_gfn_range>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ */
 static __always_inline void kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 							     bool shared)
 {
+	/*
+	 * struct kvm *kvm:
+	 *     #ifdef KVM_HAVE_MMU_RWLOCK
+	 *     rwlock_t mmu_lock;
+	 *     #else
+	 *     spinlock_t mmu_lock;
+	 *     #endif
+	 */
 	if (shared)
 		lockdep_assert_held_read(&kvm->mmu_lock);
 	else
 		lockdep_assert_held_write(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5794| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	if (!kvm->arch.tdp_mmu_enabled)
@@ -50,6 +73,9 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 	 * Ensure that all the outstanding RCU callbacks to free shadow pages
 	 * can run before the VM is torn down.
 	 */
+	/*
+	 * 注释: Wait until all in-flight call_rcu() callbacks complete.
+	 */
 	rcu_barrier();
 }
 
@@ -57,6 +83,11 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|84| <<tdp_mmu_free_sp_rcu_callback>> tdp_mmu_free_sp(sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1080| <<kvm_tdp_mmu_map>> tdp_mmu_free_sp(sp);
+ */
 static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
 {
 	free_page((unsigned long)sp->spt);
@@ -71,6 +102,11 @@ static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
  * section, and freeing it after a grace period, lockless access to that
  * memory won't use it after it is freed.
  */
+/*
+ * 在以下使用tdp_mmu_free_sp_rcu_callback():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|109| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|417| <<handle_removed_tdp_mmu_page>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ */
 static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 {
 	struct kvm_mmu_page *sp = container_of(head, struct kvm_mmu_page,
@@ -79,6 +115,12 @@ static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 	tdp_mmu_free_sp(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3254| <<mmu_free_root_page>> kvm_tdp_mmu_put_root(kvm, sp, false);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<tdp_mmu_next_root>> kvm_tdp_mmu_put_root(kvm, prev_root, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|860| <<kvm_tdp_mmu_zap_invalidated_roots>> kvm_tdp_mmu_put_root(kvm, root, true);
+ */
 void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			  bool shared)
 {
@@ -93,6 +135,23 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	list_del_rcu(&root->link);
 	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
 
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
 
 	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
@@ -105,6 +164,11 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
  * that reference will be dropped. If no valid root is found, this
  * function will return NULL.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|152| <<for_each_tdp_mmu_root_yield_safe>> for (_root = tdp_mmu_next_root(_kvm, NULL, _shared); \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|154| <<for_each_tdp_mmu_root_yield_safe>> _root = tdp_mmu_next_root(_kvm, _root, _shared)) \
+ */
 static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 					      struct kvm_mmu_page *prev_root,
 					      bool shared)
@@ -113,6 +177,10 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 
 	rcu_read_lock();
 
+	/*
+	 * 关于list_next_or_null_rcu()注意:
+	 * Note that if the ptr is at the end of the list, NULL is returned.
+	 */
 	if (prev_root)
 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
 						  &prev_root->link,
@@ -143,6 +211,18 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
  * mode. In the unlikely event that this thread must free a root, the lock
  * will be temporarily dropped and reacquired in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|788| <<__kvm_tdp_mmu_zap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_unmap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1234| <<kvm_tdp_mmu_wrprot_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1304| <<kvm_tdp_mmu_clear_dirty_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1429| <<kvm_tdp_mmu_zap_collapsible_sptes>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)	\
 	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared);		\
 	     _root;							\
@@ -150,6 +230,17 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|209| <<kvm_tdp_mmu_get_vcpu_root_hpa>> for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1071| <<kvm_tdp_mmu_handle_gfn>> for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1370| <<kvm_tdp_mmu_clear_dirty_pt_masked>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1483| <<kvm_tdp_mmu_write_protect_gfn>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root(_kvm, _root, _as_id)				\
 	list_for_each_entry_rcu(_root, &_kvm->arch.tdp_mmu_roots, link,		\
 				lockdep_is_held_type(&kvm->mmu_lock, 0) ||	\
@@ -157,6 +248,11 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|275| <<alloc_tdp_mmu_page>> sp->role.word = page_role_for_level(vcpu, level).word;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|300| <<kvm_tdp_mmu_get_vcpu_root_hpa>> role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
+ */
 static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 						   int level)
 {
@@ -172,6 +268,11 @@ static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|313| <<kvm_tdp_mmu_get_vcpu_root_hpa>> root = alloc_tdp_mmu_page(vcpu, 0, vcpu->arch.mmu->shadow_root_level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1299| <<kvm_tdp_mmu_map>> sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
+ */
 static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 					       int level)
 {
@@ -190,6 +291,14 @@ static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3377| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ *
+ * kvm_mmu_load()
+ * -> mmu_alloc_direct_roots()
+ *    -> kvm_tdp_mmu_get_vcpu_root_hpa()
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role;
@@ -201,6 +310,10 @@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 	role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
 
 	/* Check for an existing root before allocating a new one. */
+	/*
+	 * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+	 * 前者不需要锁, 后者需要锁
+	 */
 	for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
 		if (root->role.word == role.word &&
 		    kvm_tdp_mmu_get_root(kvm, root))
@@ -222,6 +335,12 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|562| <<handle_changed_spte>> handle_changed_spte_acc_track(old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|603| <<tdp_mmu_set_spte_atomic>> handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|681| <<__tdp_mmu_set_spte>> handle_changed_spte_acc_track(iter->old_spte, new_spte,
+ */
 static void handle_changed_spte_acc_track(u64 old_spte, u64 new_spte, int level)
 {
 	if (!is_shadow_present_pte(old_spte) || !is_last_spte(old_spte, level))
@@ -259,6 +378,10 @@ static void handle_changed_spte_dirty_log(struct kvm *kvm, int as_id, gfn_t gfn,
  * @account_nx: This page replaces a NX large page and should be marked for
  *		eventual reclaim.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<kvm_tdp_mmu_map>> tdp_mmu_link_page(vcpu->kvm, sp,
+ */
 static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 			      bool account_nx)
 {
@@ -278,6 +401,10 @@ static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  *	    the MMU lock and the operation must synchronize with other
  *	    threads that might be adding or removing pages.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+ */
 static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				bool shared)
 {
@@ -311,6 +438,14 @@ static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|552| <<__handle_changed_spte>> handle_removed_tdp_mmu_page(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 核心思想是把一个sp->pt上的每一个entry标记为REMOVED_SPTE, 然后把sp给free
+ * Given a page table that has been removed from the TDP paging structure,
+ * iterates through the page table to clear SPTEs and free child page tables.
+ */
 static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 					bool shared)
 {
@@ -321,8 +456,15 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 
 	trace_kvm_mmu_prepare_zap_page(sp);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+	 */
 	tdp_mmu_unlink_page(kvm, sp, shared);
 
+	/*
+	 * 1 << 9 = 512
+	 */
 	for (i = 0; i < PT64_ENT_PER_PAGE; i++) {
 		u64 *sptep = rcu_dereference(pt) + i;
 		gfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);
@@ -393,6 +535,19 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
  * Handle bookkeeping that might result from the modification of a SPTE.
  * This function must be called for all TDP SPTE modifications.
  */
+/*
+ * 如果想要触发handle_removed_tdp_mmu_page()要满足这些条件:
+ * was_present && !was_leaf && (pfn_changed || !is_present)
+ *
+ * - was_present
+ * - !was_leaf
+ * - (pfn_changed || !is_present)
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<handle_changed_spte>> __handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|601| <<tdp_mmu_set_spte_atomic>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|678| <<__tdp_mmu_set_spte>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ */
 static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				  u64 old_spte, u64 new_spte, int level,
 				  bool shared)
@@ -471,11 +626,21 @@ static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 	 * Recursively handle child PTs if the change removed a subtree from
 	 * the paging structure.
 	 */
+	/*
+	 * handle_removed_tdp_mmu_page()
+	 * 核心思想是把一个sp->pt上的每一个entry标记为REMOVED_SPTE, 然后把sp给free
+	 * Given a page table that has been removed from the TDP paging structure,
+	 * iterates through the page table to clear SPTEs and free child page tables.
+	 */
 	if (was_present && !was_leaf && (pfn_changed || !is_present))
 		handle_removed_tdp_mmu_page(kvm,
 				spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|447| <<handle_removed_tdp_mmu_page>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_child_spte, REMOVED_SPTE, level, shared);
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -498,6 +663,14 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
  * Returns: true if the SPTE was set, false if it was not. If false is returned,
  *	    this function will have no side-effects.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1156| <<tdp_mmu_map_handle_target_level>> else if (!tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1256| <<kvm_tdp_mmu_map>> if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1444| <<wrprot_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1512| <<clear_dirty_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ */
 static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter,
 					   u64 new_spte)
@@ -517,10 +690,20 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	 * Note, fast_pf_fix_direct_spte() can also modify TDP MMU SPTEs and
 	 * does not hold the mmu_lock.
 	 */
+	/*
+	 * cmpxchg(void* ptr, int old, int new), 如果ptr和old的值一样,则把new写到ptr内存,
+	 * 否则返回ptr的值. 整个操作是原子的.
+	 * 在Intel平台下,会用lock cmpxchg来实现,这里的lock个人理解是锁住内存总线,
+	 * 这样如果有另一个线程想访问ptr的内存,就会被block住.
+	 */
 	if (cmpxchg64(rcu_dereference(iter->sptep), iter->old_spte,
 		      new_spte) != iter->old_spte)
 		return false;
 
+	/*
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, true);
 	handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
@@ -528,6 +711,12 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|810| <<zap_gfn_range>> } else if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1044| <<kvm_tdp_mmu_map>> if (!tdp_mmu_zap_spte_atomic(vcpu->kvm, &iter))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1460| <<zap_collapsible_spte_range>> if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ */
 static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter)
 {
@@ -573,10 +762,25 @@ static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  *		      Leaving record_dirty_log unset in that case prevents page
  *		      writes from being double counted.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|822| <<tdp_mmu_set_spte>> __tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|833| <<tdp_mmu_set_spte_no_acc_track>> __tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|844| <<tdp_mmu_set_spte_no_dirty_log>> __tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
+ */
 static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				      u64 new_spte, bool record_acc_track,
 				      bool record_dirty_log)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON_ONCE(iter->yielded);
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
@@ -588,10 +792,18 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 	 * should be used. If operating under the MMU lock in write mode, the
 	 * use of the removed SPTE should not be necessary.
 	 */
+	/*
+	 * return spte == REMOVED_SPTE
+	 */
 	WARN_ON(is_removed_spte(iter->old_spte));
 
 	WRITE_ONCE(*rcu_dereference(iter->sptep), new_spte);
 
+	/*
+	 * 应该就是记录stat吧
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, false);
 	if (record_acc_track)
@@ -603,12 +815,23 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					      iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|852| <<zap_gfn_range>> tdp_mmu_set_spte(kvm, &iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1258| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1565| <<write_protect_gfn>> tdp_mmu_set_spte(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				    u64 new_spte)
 {
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<age_gfn_range>> tdp_mmu_set_spte_no_acc_track(kvm, iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -616,6 +839,10 @@ static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1522| <<clear_dirty_pt_masked>> tdp_mmu_set_spte_no_dirty_log(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -623,6 +850,10 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
 }
 
+/*
+ * Iterates over every SPTE mapping the GFN range [start, end) in a
+ * preorder traversal.
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
 
@@ -633,6 +864,12 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1213| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1777| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1812| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, __va(_mmu->root_hpa),		\
 			 _mmu->shadow_root_level, _start, _end)
@@ -651,10 +888,26 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|736| <<zap_gfn_range>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, shared)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1196| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1259| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1391| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON(iter->yielded);
 
 	/* Ensure forward progress has been made before yielding. */
@@ -676,6 +929,17 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 在以下设置tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+		 * 在以下使用tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+		 */
 		iter->yielded = true;
 	}
 
@@ -699,6 +963,13 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
  * structures concurrently. If shared is false, this thread should hold the
  * MMU lock in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|145| <<kvm_tdp_mmu_put_root>> zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|954| <<__kvm_tdp_mmu_zap_gfn_range>> flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1019| <<kvm_tdp_mmu_zap_invalidated_roots>> flush = zap_gfn_range(kvm, root, 0, -1ull, true, flush, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1221| <<kvm_tdp_mmu_unmap_gfn_range>> flush = zap_gfn_range(kvm, root, range->start, range->end,
+ */
 static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared)
@@ -720,10 +991,17 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 	 */
 	end = min(end, max_gfn_host);
 
+	/*
+	 * kvm->mmu_lock
+	 */
 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
 				   min_level, start, end) {
 retry:
@@ -770,11 +1048,34 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * SPTEs have been cleared and a TLB flush is needed before releasing the
  * MMU lock.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|27| <<kvm_tdp_mmu_zap_gfn_range>> return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|43| <<kvm_tdp_mmu_zap_sp>> return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
+ */
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * zap_gfn_range()注释:
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
 		flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
 				      false);
@@ -782,6 +1083,10 @@ bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6028| <<kvm_mmu_zap_all>> kvm_tdp_mmu_zap_all(kvm);
+ */
 void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 {
 	bool flush = false;
@@ -794,6 +1099,11 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 		kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_zap_invalidated_roots>> root = next_invalidated_root(kvm, NULL);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1050| <<kvm_tdp_mmu_zap_invalidated_roots>> next_root = next_invalidated_root(kvm, root);
+ */
 static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
 						  struct kvm_mmu_page *prev_root)
 {
@@ -824,6 +1134,10 @@ static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
  * only has to do a trivial amount of work. Since the roots are invalid,
  * no new SPTEs should be created under them.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5742| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *next_root;
@@ -880,10 +1194,27 @@ void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
  * This has essentially the same effect for the TDP MMU
  * as updating mmu_valid_gen does for the shadow MMU.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5724| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+ */
 void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * List of struct kvm_mmu_pages being used as roots.
+	 * All struct kvm_mmu_pages in the list should have
+	 * tdp_mmu_page set.
+	 *
+	 * For reads, this list is protected by:
+	 *      the MMU lock in read mode + RCU or
+	 *      the MMU lock in write mode
+	 *
+	 * For writes, this list is protected by:
+	 *      the MMU lock in read mode + the tdp_mmu_pages_lock or
+	 *      the MMU lock in write mode
+	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
 	list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link)
 		if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
@@ -894,6 +1225,20 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()
+ *                -> tdp_mmu_map_handle_target_level()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1274| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -950,6 +1295,38 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4249| <<direct_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -961,14 +1338,39 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 * end是')'
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+		 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *
+		 * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+		 * 所以就没法huge page了, 只能让goal_level再下降一位
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
 
+		/*
+		 * 如果到了想要的level, 必然要退出
+		 */
 		if (iter.level == fault->goal_level)
 			break;
 
@@ -990,21 +1392,45 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
 		}
 
+		/*
+		 * iter.old_spte: snapshot of the value at sptep
+		 */
 		if (!is_shadow_present_pte(iter.old_spte)) {
 			/*
 			 * If SPTE has been frozen by another thread, just
 			 * give up and retry, avoiding unnecessary page table
 			 * allocation and free.
 			 */
+			/*
+			 * 关于REMOVED_SPTE:
+			 * If a thread running without exclusive control of the MMU lock must perform a
+			 * multi-part operation on an SPTE, it can set the SPTE to REMOVED_SPTE as a
+			 * non-present intermediate value. Other threads which encounter this value
+			 * should not modify the SPTE.
+			 *
+			 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+			 * bot AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+			 * vulnerability.  Use only low bits to avoid 64-bit immediates.
+			 *
+			 * Only used by the TDP MMU.
+			 */
 			if (is_removed_spte(iter.old_spte))
 				break;
 
 			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
 			child_pt = sp->spt;
 
+			/*
+			 * 在以下使用shadow_accessed_mask:
+			 *   - arch/x86/kvm/mmu/spte.c|330| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+			 *   - arch/x86/kvm/mmu/spte.c|382| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+			 */
 			new_spte = make_nonleaf_spte(child_pt,
 						     !shadow_accessed_mask);
 
+			/*
+			 * 走到这里说明iter.old_spte不present
+			 */
 			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
 				tdp_mmu_link_page(vcpu->kvm, sp,
 						  fault->huge_page_disallowed &&
@@ -1023,6 +1449,10 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		return RET_PF_RETRY;
 	}
 
+	/*
+	 * Installs a last-level SPTE to handle a TDP page fault.
+	 * (NPT/EPT violation/misconfiguration)
+	 */
 	ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
 	rcu_read_unlock();
 
@@ -1169,6 +1599,10 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  * [start, end). Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1648| <<kvm_tdp_mmu_wrprot_slot>> spte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn,
+ */
 static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			     gfn_t start, gfn_t end, int min_level)
 {
@@ -1213,6 +1647,10 @@ static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * only affect leaf SPTEs down to min_level.
  * Returns true if an SPTE has been changed and the TLBs need to be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6147| <<kvm_mmu_slot_remove_write_access>> flush |= kvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);
+ */
 bool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *slot, int min_level)
 {
@@ -1349,6 +1787,11 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
  * clearing the dirty status will involve clearing the dirty bit on each SPTE
  * or, if AD bits are not enabled, clearing the writable bit on each SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1311| <<kvm_mmu_write_protect_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<kvm_mmu_clear_dirty_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ */
 void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				       struct kvm_memory_slot *slot,
 				       gfn_t gfn, unsigned long mask,
@@ -1365,6 +1808,10 @@ void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * Clear leaf entries which could be replaced by large mappings, for
  * GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1641| <<kvm_tdp_mmu_zap_collapsible_sptes>> zap_collapsible_spte_range(kvm, root, slot);
+ */
 static void zap_collapsible_spte_range(struct kvm *kvm,
 				       struct kvm_mmu_page *root,
 				       const struct kvm_memory_slot *slot)
@@ -1376,6 +1823,9 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
 
 	rcu_read_lock();
 
+	/*
+	 * struct tdp_iter iter;
+	 */
 	tdp_root_for_each_pte(iter, root, start, end) {
 retry:
 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
@@ -1409,11 +1859,18 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
  * Clear non-leaf entries (and free associated page tables) which could
  * be replaced by large mappings, for GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5957| <<kvm_mmu_zap_collapsible_sptes>> kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);
+ */
 void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * read lock !!!!
+	 */
 	lockdep_assert_held_read(&kvm->mmu_lock);
 
 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
@@ -1425,6 +1882,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1657| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1462,6 +1923,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1425| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
@@ -1470,6 +1935,12 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 	bool spte_set = false;
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * write_protect_gfn():
+	 *   Removes write access on the last level SPTE mapping this GFN and unsets the
+	 *   MMU-writable bit to ensure future writes continue to be intercepted.
+	 *   Returns true if an SPTE was set and a TLB flush is needed.
+	 */
 	for_each_tdp_mmu_root(kvm, root, slot->as_id)
 		spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
 
@@ -1482,6 +1953,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
  *
  * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3823| <<get_mmio_spte>> leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);
+ */
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level)
 {
@@ -1494,6 +1969,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		leaf = iter.level;
+		/*
+		 * 从get_mmio_spte()来的时候:
+		 * u64 sptes[PT64_ROOT_MAX_LEVEL + 1];
+		 */
 		sptes[leaf] = iter.old_spte;
 	}
 
@@ -1511,6 +1990,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
  *
  * WARNING: This function is only intended to be called during fast_page_fault.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3173| <<fast_page_fault>> sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte)
 {
@@ -1520,6 +2003,10 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 	tdp_ptep_t sptep = NULL;
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+		/*
+		 * iter.old_spte : A snapshot of the value at sptep
+		 * iter.sptep    : A pointer to the current SPTE
+		 */
 		*spte = iter.old_spte;
 		sptep = iter.sptep;
 	}
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 3899004a5d91..8dd0d8311add 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -7,12 +7,29 @@
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|182| <<tdp_mmu_next_root>> while (next_root && !kvm_tdp_mmu_get_root(kvm, next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|297| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(kvm, root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm *kvm,
 						     struct kvm_mmu_page *root)
 {
 	if (root->role.invalid)
 		return false;
 
+	/*
+	 * increment a refcount unless it is 0
+	 * Return: true if the increment was successful, false otherwise
+	 */
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
 }
 
@@ -21,11 +38,20 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush);
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5809| <<kvm_zap_gfn_range>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, gfn_start,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|801| <<kvm_tdp_mmu_zap_all>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, 0, -1ull, flush);
+ */
 static inline bool kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id,
 					     gfn_t start, gfn_t end, bool flush)
 {
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
 }
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6301| <<kvm_recover_nx_lpages>> flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
+ */
 static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	gfn_t end = sp->gfn + KVM_PAGES_PER_HPAGE(sp->role.level + 1);
@@ -40,6 +66,12 @@ static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	 * of the shadow page's gfn range and stop iterating before yielding.
 	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 */
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
 					   sp->gfn, end, false, false);
 }
@@ -71,6 +103,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|696| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
@@ -105,6 +141,24 @@ static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
 	 * pae_root page, not a shadow page.
 	 */
 	sp = to_shadow_page(hpa);
+	/*
+	 * 在以下设置kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+	 *
+	 * 在以下使用tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return sp && is_tdp_mmu_page(sp) && sp->root_count;
 }
 #else
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index b1a02993782b..7494b0da04f1 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -49,6 +49,10 @@
  *        * AMD:   [0 .. AMD64_NUM_COUNTERS-1] <=> gp counters
  */
 
+/*
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|495| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
@@ -57,6 +61,11 @@ static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|94| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|539| <<kvm_pmu_incr_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -85,6 +94,10 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|151| <<pmc_reprogram_counter>> kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -94,6 +107,11 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	__kvm_perf_overflow(pmc, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|232| <<reprogram_gp_counter>> pmc_reprogram_counter(pmc, type, config,
+ *   - arch/x86/kvm/pmu.c|274| <<reprogram_fixed_counter>> pmc_reprogram_counter(pmc, PERF_TYPE_HARDWARE,
+ */
 static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 				  u64 config, bool exclude_user,
 				  bool exclude_kernel, bool intr,
@@ -128,6 +146,15 @@ static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 		attr.config |= HSW_IN_TX_CHECKPOINTED;
 	}
 
+	/*
+	 * perf_event_create_kernel_counter
+	 *
+	 * @attr: attributes of the counter to create
+	 * @cpu: cpu in which the counter is bound
+	 * @task: task to profile (NULL for percpu)
+	 * @overflow_handler: callback to trigger when we hit the event
+	 * @context: context data could be used in overflow_handler callback
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_perf_overflow, pmc);
 	if (IS_ERR(event)) {
@@ -143,6 +170,11 @@ static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 	pmc->intr = intr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|214| <<reprogram_gp_counter>> pmc_pause_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|272| <<reprogram_fixed_counter>> pmc_pause_counter(pmc);
+ */
 static void pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -179,6 +211,12 @@ static int cmp_u64(const void *a, const void *b)
 	return *(__u64 *)a - *(__u64 *)b;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|315| <<reprogram_counter>> reprogram_gp_counter(pmc, pmc->eventsel);
+ *   - arch/x86/kvm/svm/pmu.c|268| <<amd_pmu_set_msr>> reprogram_gp_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_set_msr>> reprogram_gp_counter(pmc, data);
+ */
 void reprogram_gp_counter(struct kvm_pmc *pmc, u64 eventsel)
 {
 	u64 config;
@@ -238,6 +276,11 @@ void reprogram_gp_counter(struct kvm_pmc *pmc, u64 eventsel)
 }
 EXPORT_SYMBOL_GPL(reprogram_gp_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|290| <<reprogram_counter>> reprogram_fixed_counter(pmc, ctrl, idx);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|53| <<reprogram_fixed_counters>> reprogram_fixed_counter(pmc, new_ctrl, i);
+ */
 void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 {
 	unsigned en_field = ctrl & 0x3;
@@ -274,6 +317,12 @@ void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 }
 EXPORT_SYMBOL_GPL(reprogram_fixed_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|308| <<kvm_pmu_handle_event>> reprogram_counter(pmu, bit);
+ *   - arch/x86/kvm/pmu.c|500| <<kvm_pmu_incr_counter>> reprogram_counter(pmu, pmc->idx);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|68| <<global_ctrl_changed>> reprogram_counter(pmu, bit);
+ */
 void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 {
 	struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, pmc_idx);
@@ -292,6 +341,10 @@ void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 }
 EXPORT_SYMBOL_GPL(reprogram_counter);
 
+/*
+ * 处理KVM_REQ_PMU:
+ *   - arch/x86/kvm/x86.c|10443| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -357,6 +410,11 @@ static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1392| <<kvm_emulate_rdpmc>> if (kvm_pmu_rdpmc(vcpu, ecx, &data)) {
+ *   - arch/x86/kvm/x86.c|8052| <<emulator_read_pmc>> return kvm_pmu_rdpmc(emul_to_vcpu(ctxt), pmc, pdata);
+ */
 int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 {
 	bool fast_mode = idx & (1u << 31);
@@ -422,11 +480,24 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|322| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|458| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * intel_pmu_refresh()
+	 */
 	kvm_x86_ops.pmu_ops->refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|526| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11914| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -435,6 +506,10 @@ void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 	kvm_x86_ops.pmu_ops->reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11764| <<kvm_arch_vcpu_create>> kvm_pmu_init(vcpu);
+ */
 void kvm_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -447,6 +522,11 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	kvm_pmu_refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|528| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/pmu.c|616| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -459,6 +539,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 }
 
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|351| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -484,11 +568,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11837| <<kvm_arch_vcpu_destroy>> kvm_pmu_destroy(vcpu);
+ */
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|591| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -502,6 +594,10 @@ static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 		__kvm_perf_overflow(pmc, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|590| <<kvm_pmu_trigger_event>> if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))
+ */
 static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
 	unsigned int perf_hw_id)
 {
@@ -514,6 +610,10 @@ static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
 	return config == perf_hw_id;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|590| <<kvm_pmu_trigger_event>> if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))
+ */
 static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 {
 	bool select_os, select_user;
@@ -530,6 +630,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3537| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8229| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8499| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8501| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -549,6 +656,10 @@ void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 }
 EXPORT_SYMBOL_GPL(kvm_pmu_trigger_event);
 
+/*
+ * 处理KVM_SET_PMU_EVENT_FILTER:
+ *   - arch/x86/kvm/x86.c|6871| <<kvm_arch_vm_ioctl>> r = kvm_vm_ioctl_set_pmu_event_filter(kvm, argp);
+ */
 int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_pmu_event_filter tmp, *filter;
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 7a7b8d5b775e..253809b0fdb6 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -41,6 +41,14 @@ struct kvm_pmu_ops {
 	void (*cleanup)(struct kvm_vcpu *vcpu);
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|174| <<pmc_pause_counter>> pmc->counter = counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.c|557| <<kvm_pmu_incr_counter>> pmc->counter = (pmc->counter + 1) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|60| <<pmc_read_counter>> return counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|141| <<get_sample_period>> u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|144| <<get_sample_period>> sample_period = pmc_bitmask(pmc) + 1;
+ */
 static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -48,6 +56,16 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.h|76| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|236| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|259| <<amd_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|415| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|420| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|489| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|495| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
@@ -60,6 +78,12 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|248| <<reprogram_gp_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.c|290| <<reprogram_fixed_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|77| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -70,6 +94,13 @@ static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|529| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|318| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|663| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -103,6 +134,21 @@ static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|130| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|196| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|251| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|269| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|270| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|413| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|414| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|424| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|475| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|476| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|500| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 					 u32 base)
 {
@@ -110,6 +156,11 @@ static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 		u32 index = array_index_nospec(msr - base,
 					       pmu->nr_arch_gp_counters);
 
+		/*
+		 * struct kvm_pmu *pmu:
+		 * -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+		 * -> struct kvm_pmc fixed_counters[INTEL_PMC_MAX_FIXED];
+		 */
 		return &pmu->gp_counters[index];
 	}
 
@@ -117,6 +168,15 @@ static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 }
 
 /* returns fixed PMC with the specified MSR */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|61| <<reprogram_fixed_counters>> pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|135| <<intel_pmc_idx_to_pmc>> return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|252| <<intel_is_valid_msr>> get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|268| <<intel_msr_idx_to_pmc>> pmc = get_fixed_pmc(pmu, msr);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|419| <<intel_pmu_get_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|494| <<intel_pmu_set_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ */
 static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 {
 	int base = MSR_CORE_PERF_FIXED_CTR0;
@@ -131,6 +191,13 @@ static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|127| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|185| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|492| <<intel_pmu_set_msr>> get_sample_period(pmc, data));
+ *   - arch/x86/kvm/vmx/pmu_intel.c|498| <<intel_pmu_set_msr>> get_sample_period(pmc, data));
+ */
 static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 17b53457d866..e86056157d89 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -234,6 +234,10 @@ static void sev_unbind_asid(struct kvm *kvm, unsigned int handle)
 	sev_decommission(handle);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/sev.c|1795| <<svm_mem_enc_op>> r = sev_guest_init(kvm, &sev_cmd);
+ */
 static int sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp)
 {
 	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index 3f430e218375..e7ccf440d142 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -60,6 +60,21 @@ struct vmcs_config {
 	u32 cpu_based_exec_ctrl;
 	u32 cpu_based_2nd_exec_ctrl;
 	u32 vmexit_ctrl;
+	/*
+	 * 在以下使用vmcs_config->vmentry_ctrl:
+	 *   - arch/x86/kvm/vmx/capabilities.h|97| <<cpu_has_load_ia32_efer>> return (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_EFER) &&
+	 *   - arch/x86/kvm/vmx/capabilities.h|103| <<cpu_has_load_perf_global_ctrl>> return (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL) &&
+	 *   - arch/x86/kvm/vmx/capabilities.h|114| <<cpu_has_vmx_mpx>> (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);
+	 *   - arch/x86/kvm/vmx/capabilities.h|371| <<cpu_has_vmx_intel_pt>> (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_RTIT_CTL);
+	 *   - arch/x86/kvm/vmx/evmcs.c|304| <<evmcs_sanitize_exec_ctrls>> vmcs_conf->vmentry_ctrl &= ~EVMCS1_UNSUPPORTED_VMENTRY_CTRL;
+	 *   - arch/x86/kvm/vmx/nested.c|2538| <<prepare_vmcs02>> } else if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|2149| <<vmx_set_msr>> if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|2631| <<setup_vmcs_config>> vmcs_conf->vmentry_ctrl = _vmentry_control;
+	 *   - arch/x86/kvm/vmx/vmx.c|4182| <<vmx_vmentry_ctrl>> u32 vmentry_ctrl = vmcs_config.vmentry_ctrl;
+	 *   - arch/x86/kvm/vmx/vmx.c|4185| <<vmx_vmentry_ctrl>> vmentry_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP |
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_vmentry_ctrl>> return vmentry_ctrl &
+	 *   - arch/x86/kvm/vmx/vmx.c|4451| <<init_vmcs>> if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)
+	 */
 	u32 vmentry_ctrl;
 	struct nested_vmx_msrs nested;
 };
@@ -92,18 +107,39 @@ static inline bool cpu_has_vmx_posted_intr(void)
 	return vmcs_config.pin_based_exec_ctrl & PIN_BASED_POSTED_INTR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2327| <<prepare_vmcs02_early>> if (cpu_has_load_ia32_efer()) {
+ *   - arch/x86/kvm/vmx/nested.c|2343| <<prepare_vmcs02_early>> if (cpu_has_load_ia32_efer() && guest_efer != host_efer)
+ *   - arch/x86/kvm/vmx/nested.c|4387| <<nested_vmx_get_vmcs01_guest_efer>> if (cpu_has_load_ia32_efer())
+ *   - arch/x86/kvm/vmx/vmx.c|815| <<clear_atomic_switch_msr>> if (cpu_has_load_ia32_efer()) {
+ *   - arch/x86/kvm/vmx/vmx.c|878| <<add_atomic_switch_msr>> if (cpu_has_load_ia32_efer()) {
+ *   - arch/x86/kvm/vmx/vmx.c|966| <<update_transition_efer>> if (cpu_has_load_ia32_efer() ||
+ *   - arch/x86/kvm/vmx/vmx.c|4163| <<vmx_set_constant_host_state>> if (cpu_has_load_ia32_efer())
+ */
 static inline bool cpu_has_load_ia32_efer(void)
 {
 	return (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_EFER) &&
 	       (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_EFER);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|823| <<clear_atomic_switch_msr>> if (cpu_has_load_perf_global_ctrl()) {
+ *   - arch/x86/kvm/vmx/vmx.c|879| <<add_atomic_switch_msr>> if (cpu_has_load_perf_global_ctrl()) {
+ *   - arch/x86/kvm/vmx/vmx.c|5964| <<dump_vmcs>> if (cpu_has_load_perf_global_ctrl() &&
+ *   - arch/x86/kvm/vmx/vmx.c|6005| <<dump_vmcs>> if (cpu_has_load_perf_global_ctrl() &&
+ */
 static inline bool cpu_has_load_perf_global_ctrl(void)
 {
 	return (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL) &&
 	       (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7976| <<hardware_setup>> if (!cpu_has_vmx_mpx())
+ */
 static inline bool cpu_has_vmx_mpx(void)
 {
 	return (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&
diff --git a/arch/x86/kvm/vmx/nested.h b/arch/x86/kvm/vmx/nested.h
index b69a80f43b37..f56f9b239732 100644
--- a/arch/x86/kvm/vmx/nested.h
+++ b/arch/x86/kvm/vmx/nested.h
@@ -39,6 +39,12 @@ bool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,
 
 static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 466d18fc0c5d..84d00300728d 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -20,6 +20,16 @@
 
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|81| <<intel_pmc_perf_hw_id>> for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|82| <<intel_pmc_perf_hw_id>> if (intel_arch_events[i].eventsel != event_select ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|83| <<intel_pmc_perf_hw_id>> intel_arch_events[i].unit_mask != unit_mask)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|93| <<intel_pmc_perf_hw_id>> if (i == ARRAY_SIZE(intel_arch_events))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|96| <<intel_pmc_perf_hw_id>> return intel_arch_events[i].event_type;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|510| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+ *   - arch/x86/kvm/vmx/pmu_intel.c|511| <<setup_fixed_pmc_eventsel>> intel_arch_events[event].eventsel;
+ */
 static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
 	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
@@ -35,6 +45,10 @@ static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 /* mapping between fixed pmc index and intel_arch_events array */
 static int fixed_pmc_events[] = {1, 0, 7};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|435| <<intel_pmu_set_msr>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	int i;
@@ -68,6 +82,9 @@ static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 		reprogram_counter(pmu, bit);
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.pmc_perf_hw_id = intel_pmc_perf_hw_id()
+ */
 static unsigned int intel_pmc_perf_hw_id(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -94,6 +111,9 @@ static unsigned int intel_pmc_perf_hw_id(struct kvm_pmc *pmc)
 }
 
 /* check if a PMC is enabled by comparing it with globl_ctrl bits. */
+/*
+ * kvm_pmu_ops intel_pmu_ops.pmc_is_enabled = intel_pmc_is_enabled()
+ */
 static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -101,6 +121,9 @@ static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.pmc_idx_to_pmc = intel_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	if (pmc_idx < INTEL_PMC_IDX_FIXED)
@@ -113,6 +136,9 @@ static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 	}
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -124,6 +150,9 @@ static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 		     : idx < pmu->nr_arch_gp_counters;
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 					    unsigned int idx, u64 *mask)
 {
@@ -202,6 +231,9 @@ static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
 	return ret;
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -225,6 +257,9 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return ret;
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -248,6 +283,11 @@ static inline void intel_pmu_release_guest_lbr_event(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|338| <<intel_pmu_handle_lbr_msrs_access>> if (!lbr_desc->event && intel_pmu_create_guest_lbr_event(vcpu) < 0)
+ *   - arch/x86/kvm/vmx/vmx.c|2106| <<vmx_set_msr>> intel_pmu_create_guest_lbr_event(vcpu);
+ */
 int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)
 {
 	struct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);
@@ -305,6 +345,11 @@ int intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)
  * been passthrough since the host would help restore or reset
  * the LBR msrs records when the guest LBR event is scheduled in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|403| <<intel_pmu_get_msr>> } else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, true))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|477| <<intel_pmu_set_msr>> } else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, false))
+ */
 static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
 				     struct msr_data *msr_info, bool read)
 {
@@ -342,6 +387,9 @@ static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.get_msr
+ */
 static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -383,6 +431,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.set_msr
+ */
 static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -423,6 +474,12 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	default:
 		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
 		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+			/*
+			 * struct kvm_pmc *pmc;
+			 * -> u64 counter;
+			 * -> u64 eventsel;  
+			 * -> struct perf_event *perf_event;
+			 */
 			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
 			    (data & ~pmu->counter_bitmask[KVM_PMC_GP]))
 				return 1;
@@ -469,6 +526,9 @@ static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 	}
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.refresh
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -552,6 +612,9 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 		bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static void intel_pmu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -578,6 +641,9 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 	lbr_desc->msr_passthrough = false;
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -621,6 +687,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
@@ -708,12 +777,18 @@ void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
 		vcpu->vcpu_id);
 }
 
+/*
+ * kvm_pmu_ops intel_pmu_ops.
+ */
 static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))
 		intel_pmu_release_guest_lbr_event(vcpu);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.pmu_ops = &intel_pmu_ops
+ */
 struct kvm_pmu_ops intel_pmu_ops = {
 	.pmc_perf_hw_id = intel_pmc_perf_hw_id,
 	.pmc_is_enabled = intel_pmc_is_enabled,
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index aa1fe9085d77..260512b75ab9 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -19,6 +19,13 @@
  * wake the target vCPUs.  vCPUs are removed from the list and the notification
  * vector is reset when the vCPU is scheduled in.
  */
+/*
+ * 在以下使用percpi的wakeup_vcpus_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|22| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|169| <<pi_enable_wakeup_handler>> &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|223| <<pi_wakeup_handler>> list_for_each_entry(vmx, &per_cpu(wakeup_vcpus_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/posted_intr.c|234| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
 /*
  * Protect the per-CPU list with a per-CPU spinlock to handle task migration.
@@ -27,10 +34,27 @@ static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
  * CPU.  IRQs must be disabled when taking this lock, otherwise deadlock will
  * occur if a wakeup IRQ arrives and attempts to acquire the lock.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu_lock:
+ *   - arch/x86/kvm/vmx/posted_intr.c|30| <<global>> static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
+ *   - arch/x86/kvm/vmx/posted_intr.c|90| <<vmx_vcpu_pi_load>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|92| <<vmx_vcpu_pi_load>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|152| <<pi_enable_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_enable_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|207| <<pi_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|214| <<pi_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|220| <<pi_init_cpu>> raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ */
 static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
 
 static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> struct pi_desc pi_desc;
+	 * -> struct list_head pi_wakeup_list;
+	 */
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
@@ -48,6 +72,10 @@ static int pi_try_set_control(struct pi_desc *pi_desc, u64 old, u64 new)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1372| <<vmx_vcpu_load>> vmx_vcpu_pi_load(vcpu, cpu);
+ */
 void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -69,6 +97,16 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 	 * full update can be skipped as neither the vector nor the destination
 	 * needs to be changed.
 	 */
+	/*
+	 * 在以下使用POSTED_INTR_WAKEUP_VECTOR:
+	 *   - arch/x86/include/asm/irq_vectors.h|96| <<global>> #define POSTED_INTR_WAKEUP_VECTOR 0xf1
+	 *   - arch/x86/kernel/idt.c|147| <<global>> INTG(POSTED_INTR_WAKEUP_VECTOR, asm_sysvec_kvm_posted_intr_wakeup_ipi),
+	 *   - arch/x86/include/asm/idtentry.h|670| <<SYM_CODE_END>> DECLARE_IDTENTRY_SYSVEC(POSTED_INTR_WAKEUP_VECTOR, sysvec_kvm_posted_intr_wakeup_ipi);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|100| <<vmx_vcpu_pi_load>> if (pi_desc->nv != POSTED_INTR_WAKEUP_VECTOR && vcpu->cpu == cpu) {
+	 *   - arch/x86/kvm/vmx/posted_intr.c|117| <<vmx_vcpu_pi_load>> if (pi_desc->nv == POSTED_INTR_WAKEUP_VECTOR) {
+	 *   - arch/x86/kvm/vmx/posted_intr.c|200| <<pi_enable_wakeup_handler>> new.nv = POSTED_INTR_WAKEUP_VECTOR;
+	 *   - arch/x86/kvm/vmx/posted_intr.c|212| <<pi_enable_wakeup_handler>> apic->send_IPI_self(POSTED_INTR_WAKEUP_VECTOR);
+	 */
 	if (pi_desc->nv != POSTED_INTR_WAKEUP_VECTOR && vcpu->cpu == cpu) {
 		/*
 		 * Clear SN if it was set due to being preempted.  Again, do
@@ -129,6 +167,11 @@ void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 		pi_set_on(pi_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|206| <<vmx_vcpu_pi_put>> if (!vmx_can_use_vtd_pi(vcpu->kvm))
+ *   - arch/x86/kvm/vmx/posted_intr.c|294| <<pi_update_irte>> if (!vmx_can_use_vtd_pi(kvm))
+ */
 static bool vmx_can_use_vtd_pi(struct kvm *kvm)
 {
 	return irqchip_in_kernel(kvm) && enable_apicv &&
@@ -140,6 +183,10 @@ static bool vmx_can_use_vtd_pi(struct kvm *kvm)
  * Put the vCPU on this pCPU's list of vCPUs that needs to be awakened and set
  * WAKEUP as the notification vector in the PI descriptor.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|199| <<vmx_vcpu_pi_put>> pi_enable_wakeup_handler(vcpu);
+ */
 static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -156,6 +203,17 @@ static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 
 	WARN(pi_desc->sn, "PI descriptor SN field set before blocking");
 
+	/*
+	 * 在以下使用POSTED_INTR_WAKEUP_VECTOR:
+	 *   - arch/x86/include/asm/irq_vectors.h|96| <<global>> #define POSTED_INTR_WAKEUP_VECTOR 0xf1
+	 *   - arch/x86/kernel/idt.c|147| <<global>> INTG(POSTED_INTR_WAKEUP_VECTOR, asm_sysvec_kvm_posted_intr_wakeup_ipi),
+	 *   - arch/x86/include/asm/idtentry.h|670| <<SYM_CODE_END>> DECLARE_IDTENTRY_SYSVEC(POSTED_INTR_WAKEUP_VECTOR, sysvec_kvm_posted_intr_wakeup_ipi);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|100| <<vmx_vcpu_pi_load>> if (pi_desc->nv != POSTED_INTR_WAKEUP_VECTOR && vcpu->cpu == cpu) {
+	 *   - arch/x86/kvm/vmx/posted_intr.c|117| <<vmx_vcpu_pi_load>> if (pi_desc->nv == POSTED_INTR_WAKEUP_VECTOR) {
+	 *   - arch/x86/kvm/vmx/posted_intr.c|200| <<pi_enable_wakeup_handler>> new.nv = POSTED_INTR_WAKEUP_VECTOR;
+	 *   - arch/x86/kvm/vmx/posted_intr.c|212| <<pi_enable_wakeup_handler>> apic->send_IPI_self(POSTED_INTR_WAKEUP_VECTOR);
+	 */
+
 	do {
 		old.control = new.control = READ_ONCE(pi_desc->control);
 
@@ -163,6 +221,11 @@ static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 		new.nv = POSTED_INTR_WAKEUP_VECTOR;
 	} while (pi_try_set_control(pi_desc, old.control, new.control));
 
+	/*
+	 * 上面的是atomic的操作, 说明过了上面, 来的必须是WAKEUP了
+	 * 不可能是普通的POSTED VECTOR
+	 */
+
 	/*
 	 * Send a wakeup IPI to this CPU if an interrupt may have been posted
 	 * before the notification vector was updated, in which case the IRQ
@@ -177,6 +240,10 @@ static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 	local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1379| <<vmx_vcpu_put>> vmx_vcpu_pi_put(vcpu);
+ */
 void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -199,12 +266,23 @@ void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 /*
  * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.
  */
+/*
+ * 在以下使用pi_wakeup_handler():
+ *   - arch/x86/kvm/vmx/vmx.c|8262| <<hardware_setup>> kvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);
+ */
 void pi_wakeup_handler(void)
 {
 	int cpu = smp_processor_id();
 	struct vcpu_vmx *vmx;
 
 	raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+	/*
+	 * 在以下使用percpi的wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|22| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|169| <<pi_enable_wakeup_handler>> &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|223| <<pi_wakeup_handler>> list_for_each_entry(vmx, &per_cpu(wakeup_vcpus_on_cpu, cpu),
+	 *   - arch/x86/kvm/vmx/posted_intr.c|234| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	list_for_each_entry(vmx, &per_cpu(wakeup_vcpus_on_cpu, cpu),
 			    pi_wakeup_list) {
 
@@ -214,12 +292,34 @@ void pi_wakeup_handler(void)
 	raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8384| <<vmx_init>> pi_init_cpu(cpu);
+ */
 void __init pi_init_cpu(int cpu)
 {
+	/*
+	 * Maintain a per-CPU list of vCPUs that need to be awakened by wakeup_handler()
+	 * when a WAKEUP_VECTOR interrupted is posted.  vCPUs are added to the list when
+	 * the vCPU is scheduled out and is blocking (e.g. in HLT) with IRQs enabled.
+	 * The vCPUs posted interrupt descriptor is updated at the same time to set its
+	 * notification vector to WAKEUP_VECTOR, so that posted interrupt from devices
+	 * wake the target vCPUs.  vCPUs are removed from the list and the notification
+	 * vector is reset when the vCPU is scheduled in.
+	 *
+	 * 在以下使用percpi的wakeup_vcpus_on_cpu:
+	 *   - arch/x86/kvm/vmx/posted_intr.c|22| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|169| <<pi_enable_wakeup_handler>> &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+	 *   - arch/x86/kvm/vmx/posted_intr.c|223| <<pi_wakeup_handler>> list_for_each_entry(vmx, &per_cpu(wakeup_vcpus_on_cpu, cpu),
+	 *   - arch/x86/kvm/vmx/posted_intr.c|234| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+	 */
 	INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
 	raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.dy_apicv_has_pending_interrupt = pi_has_pending_interrupt()
+ */
 bool pi_has_pending_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -235,6 +335,12 @@ bool pi_has_pending_interrupt(struct kvm_vcpu *vcpu)
  * PI.NV to the wakeup vector, i.e. the assigned device
  * came along after the initial check in vmx_vcpu_pi_put().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13065| <<kvm_arch_start_assignment>> static_call_cond(kvm_x86_start_assignment)(kvm);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.start_assignment = vmx_pi_start_assignment()
+ */
 void vmx_pi_start_assignment(struct kvm *kvm)
 {
 	if (!irq_remapping_cap(IRQ_POSTING_CAP))
@@ -252,6 +358,14 @@ void vmx_pi_start_assignment(struct kvm *kvm)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13108| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_update_pi_irte)(irqfd->kvm,
+ *   - arch/x86/kvm/x86.c|13133| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_update_pi_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|13144| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_update_pi_irte)(kvm, host_irq, guest_irq, set);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.update_pi_irte = pi_update_irte()
+ */
 int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 		   bool set)
 {
@@ -265,6 +379,13 @@ int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 	if (!vmx_can_use_vtd_pi(kvm))
 		return 0;
 
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_irq_routing_table __rcu *irq_routing;
+	 *    -> int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
+	 *    -> u32 nr_rt_entries;
+	 *    -> struct hlist_head map[];
+	 */
 	idx = srcu_read_lock(&kvm->irq_srcu);
 	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
 	if (guest_irq >= irq_rt->nr_rt_entries ||
@@ -317,6 +438,9 @@ int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 		trace_kvm_pi_irte_update(host_irq, vcpu->vcpu_id, e->gsi,
 				vcpu_info.vector, vcpu_info.pi_desc_addr, set);
 
+		/*
+		 * intel_ir_set_vcpu_affinity()
+		 */
 		if (set)
 			ret = irq_set_vcpu_affinity(host_irq, &vcpu_info);
 		else
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index b730d799c26e..50fef5ae3f36 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -845,11 +845,23 @@ static void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)
 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|874| <<add_atomic_switch_msr>> add_atomic_switch_msr_special(vmx,
+ *   - arch/x86/kvm/vmx/vmx.c|885| <<add_atomic_switch_msr>> add_atomic_switch_msr_special(vmx,
+ */
 static void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 		unsigned long entry, unsigned long exit,
 		unsigned long guest_val_vmcs, unsigned long host_val_vmcs,
 		u64 guest_val, u64 host_val)
 {
+	/*
+	 * 对于msr是MSR_EFER的时候
+	 * entry = VM_ENTRY_LOAD_IA32_EFER
+	 * eit = VM_EXIT_LOAD_IA32_EFER
+	 * guest_val_vmcs = GUEST_IA32_EFER
+	 * host_val_vmcs = HOST_IA32_PERF_GLOBAL_CTRL
+	 */
 	vmcs_write64(guest_val_vmcs, guest_val);
 	if (host_val_vmcs != HOST_IA32_EFER)
 		vmcs_write64(host_val_vmcs, host_val);
@@ -857,10 +869,29 @@ static void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,
 	vm_exit_controls_setbit(vmx, exit);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|957| <<update_transition_efer>> add_atomic_switch_msr(vmx, MSR_EFER,
+ *   - arch/x86/kvm/vmx/vmx.c|6766| <<atomic_switch_perf_msrs>> add_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,
+ */
 static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 				  u64 guest_val, u64 host_val, bool entry_only)
 {
 	int i, j = 0;
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * {
+	 *     ... ...
+	 *     struct msr_autoload {
+	 *         struct vmx_msrs guest;
+	 *         struct vmx_msrs host;
+	 *     } msr_autoload;
+	 *
+	 *     struct msr_autostore {
+	 *         struct vmx_msrs guest;
+	 *     } msr_autostore;
+	 * }
+	 */
 	struct msr_autoload *m = &vmx->msr_autoload;
 
 	switch (msr) {
@@ -909,6 +940,15 @@ static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 		i = m->guest.nr++;
 		vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);
 	}
+	/*
+	 * struct msr_autoload *m = &vmx->msr_autoload;
+	 * -> struct vmx_msrs guest;
+	 *    -> unsigned int            nr;
+	 *    -> struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+	 * -> struct vmx_msrs host;
+	 *    -> unsigned int            nr;
+	 *    -> struct vmx_msr_entry    val[MAX_NR_LOADSTORE_MSRS];
+	 */
 	m->guest.val[i].index = msr;
 	m->guest.val[i].value = guest_val;
 
@@ -923,6 +963,10 @@ static void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,
 	m->host.val[j].value = host_val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1704| <<vmx_setup_uret_msrs>> vmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));
+ */
 static bool update_transition_efer(struct vcpu_vmx *vmx)
 {
 	u64 guest_efer = vmx->vcpu.arch.efer;
@@ -1240,6 +1284,13 @@ static void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|269| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu, prev);
+ *   - arch/x86/kvm/vmx/nested.c|4100| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|4105| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/vmx.c|1316| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu, NULL);
+ */
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 			struct loaded_vmcs *buddy)
 {
@@ -1274,6 +1325,9 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 		 * the active VMCS within a guest, e.g. on nested VM-Enter.
 		 * The L1 VMM can protect itself with retpolines, IBPB or IBRS.
 		 */
+		/*
+		 * 只有nested的时候buddy才不为NULL
+		 */
 		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))
 			indirect_branch_prediction_barrier();
 	}
@@ -1320,6 +1374,12 @@ static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	vmx->host_debugctlmsr = get_debugctlmsr();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4936| <<kvm_arch_vcpu_put>> static_call(kvm_x86_vcpu_put)(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_put = vmx_vcpu_put()
+ */
 static void vmx_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	vmx_vcpu_pi_put(vcpu);
@@ -1350,6 +1410,19 @@ unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 	return vmx->rflags;
 }
 
+/*
+ * 可能被以下调用:
+ *   - arch/x86/kvm/x86.c|12606| <<__kvm_set_rflags>> static_call(kvm_x86_set_rflags)(vcpu, rflags);
+ *
+ * 在以下使用vmx_set_rflags():
+ *   - arch/x86/kvm/vmx/vmx.c|7797| <<global>> struct kvm_x86_ops vmx_x86_ops.set_rflags = vmx_set_rflags,
+ *   - arch/x86/kvm/vmx/nested.c|153| <<nested_vmx_succeed>> vmx_set_rflags(vcpu, vmx_get_rflags(vcpu)
+ *   - arch/x86/kvm/vmx/nested.c|161| <<nested_vmx_failInvalid>> vmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)
+ *   - arch/x86/kvm/vmx/nested.c|171| <<nested_vmx_failValid>> vmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)
+ *   - arch/x86/kvm/vmx/nested.c|2524| <<prepare_vmcs02>> vmx_set_rflags(vcpu, vmcs12->guest_rflags);
+ *   - arch/x86/kvm/vmx/nested.c|4271| <<load_vmcs12_host_state>> vmx_set_rflags(vcpu, X86_EFLAGS_FIXED);
+ *   - arch/x86/kvm/vmx/sgx.c|342| <<handle_encls_einit>> vmx_set_rflags(vcpu, rflags);
+ */
 void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -2139,6 +2212,11 @@ static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		vcpu->arch.mcg_ext_ctl = data;
 		break;
 	case MSR_IA32_FEAT_CTL:
+		/*
+		 * 这里满足下面的条件.
+		 * !vmx_feature_control_msr_valid(vcpu, data) 或者
+		 * (to_vmx(vcpu)->msr_ia32_feature_control & FEAT_CTL_LOCKED  &&  !msr_info->host_initiated)
+		 */
 		if (!vmx_feature_control_msr_valid(vcpu, data) ||
 		    (to_vmx(vcpu)->msr_ia32_feature_control &
 		     FEAT_CTL_LOCKED && !msr_info->host_initiated))
@@ -2396,6 +2474,14 @@ static bool cpu_has_sgx(void)
 	return cpuid_eax(0) >= 0x12 && (cpuid_eax(0x12) & BIT(0));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2520| <<setup_vmcs_config>> if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
+ *   - arch/x86/kvm/vmx/vmx.c|2555| <<setup_vmcs_config>> if (adjust_vmx_controls(min2, opt2,
+ *   - arch/x86/kvm/vmx/vmx.c|2603| <<setup_vmcs_config>> if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
+ *   - arch/x86/kvm/vmx/vmx.c|2610| <<setup_vmcs_config>> if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
+ *   - arch/x86/kvm/vmx/vmx.c|2627| <<setup_vmcs_config>> if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
+ */
 static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
 				      u32 msr, u32 *result)
 {
@@ -2415,6 +2501,11 @@ static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7203| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0)
+ *   - arch/x86/kvm/vmx/vmx.c|7978| <<hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+ */
 static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 				    struct vmx_capability *vmx_cap)
 {
@@ -3734,6 +3825,26 @@ static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 	vmx->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2054| <<vmx_set_msr>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|2147| <<vmx_set_msr>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|2183| <<vmx_set_msr>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|3949| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|3950| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7207| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_TSC, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7209| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_FS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7210| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7211| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7213| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7214| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7215| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7217| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C1_RES, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7218| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C3_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7219| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C6_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7220| <<vmx_create_vcpu>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C7_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.h|435| <<vmx_set_intercept_for_msr>> vmx_disable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3778,6 +3889,11 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 		vmx_clear_msr_bitmap_write(msr_bitmap, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3948| <<vmx_update_msr_bitmap_x2apic>> vmx_enable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.h|433| <<vmx_set_intercept_for_msr>> vmx_enable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3927,6 +4043,21 @@ static void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)
 static inline void kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,
 						     int pi_vec)
 {
+	/*
+	 * 在以下使用kvm_vcpu->mode:
+	 *   - arch/x86/kvm/lapic.c|129| <<kvm_use_posted_timer_interrupt>> return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
+	 *   - arch/x86/kvm/mmu/mmu.c|819| <<walk_shadow_page_lockless_begin>> smp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES);
+	 *   - arch/x86/kvm/mmu/mmu.c|833| <<walk_shadow_page_lockless_end>> smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);
+	 *   - arch/x86/kvm/svm/svm.c|3324| <<svm_complete_interrupt_delivery>> bool in_guest_mode = (smp_load_acquire(&vcpu->mode) == IN_GUEST_MODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|4011| <<kvm_vcpu_trigger_posted_interrupt>> if (vcpu->mode == IN_GUEST_MODE) {
+	 *   - arch/x86/kvm/x86.c|2080| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+	 *   - arch/x86/kvm/x86.c|10488| <<vcpu_enter_guest>> smp_store_release(&vcpu->mode, IN_GUEST_MODE);
+	 *   - arch/x86/kvm/x86.c|10517| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|10601| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - include/linux/kvm_host.h|544| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 *   - virt/kvm/kvm_main.c|3684| <<kvm_vcpu_kick>> if (vcpu->mode == IN_GUEST_MODE)
+	 *   - virt/kvm/kvm_main.c|3685| <<kvm_vcpu_kick>> WRITE_ONCE(vcpu->mode, EXITING_GUEST_MODE);
+	 */
 #ifdef CONFIG_SMP
 	if (vcpu->mode == IN_GUEST_MODE) {
 		/*
@@ -4055,6 +4186,11 @@ static void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
  * Note that host-state that does change is set elsewhere. E.g., host-state
  * that is set differently for each CPU is set in vmx_vcpu_load(), not here.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2188| <<prepare_vmcs02_constant_state>> vmx_set_constant_host_state(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|4487| <<init_vmcs>> vmx_set_constant_host_state(vmx);
+ */
 void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
 {
 	u32 low32, high32;
@@ -5297,6 +5433,14 @@ static int handle_apic_access(struct kvm_vcpu *vcpu)
 	return kvm_emulate_instruction(vcpu, 0);
 }
 
+/*
+ * 在以下使用handle_apic_eoi_induced():
+ *   - [EXIT_REASON_EOI_INDUCED] = handle_apic_eoi_induced,
+ *
+ * EXIT_REASON_EOI_INDUCED = 45
+ * Virtualized EOI. EOI virtualization was performed for a virtual interrupt
+ * whose vector indexed a bit set in the EOI-exit bitmap.
+ */
 static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5307,6 +5451,9 @@ static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_WRITE]
+ */
 static int handle_apic_write(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5463,14 +5610,37 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5496| <<handle_invalid_guest_state>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|5520| <<vmx_vcpu_pre_run>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ */
 static bool vmx_emulation_required_with_pending_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_queued_exception {
+	 *           bool pending;
+	 *           bool injected;
+	 *           bool has_error_code;
+	 *           u8 nr;
+	 *           u32 error_code;
+	 *           unsigned long payload;
+	 *           bool has_payload;
+	 *           u8 nested_apf;
+	 *       } exception;
+	 */
 	return vmx->emulation_required && !vmx->rmode.vm86_active &&
 	       vcpu->arch.exception.pending;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6116| <<__vmx_handle_exit>> return handle_invalid_guest_state(vcpu);
+ */
 static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5634,6 +5804,11 @@ static int handle_pml_full(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5674| <<handle_preemption_timer>> handle_fastpath_preemption_timer(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6778| <<vmx_exit_handlers_fastpath>> return handle_fastpath_preemption_timer(vcpu);
+ */
 static fastpath_t handle_fastpath_preemption_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6387,6 +6562,11 @@ static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6587| <<vmx_hwapic_irr_update>> vmx_set_rvi(max_irr);
+ *   - arch/x86/kvm/vmx/vmx.c|6629| <<vmx_sync_pir_to_irr>> vmx_set_rvi(max_irr);
+ */
 static void vmx_set_rvi(int vector)
 {
 	u16 status;
@@ -6404,6 +6584,9 @@ static void vmx_set_rvi(int vector)
 	}
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.hwapic_irr_update = vmx_hwapic_irr_update()
+ */
 static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 {
 	/*
@@ -6695,6 +6878,10 @@ static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6919| <<vmx_vcpu_run>> atomic_switch_perf_msrs(vmx);
+ */
 static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 {
 	int i, nr_msrs;
@@ -6773,6 +6960,13 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 	if (vcpu->arch.cr2 != native_read_cr2())
 		native_write_cr2(vcpu->arch.cr2);
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> unsigned long regs[NR_VCPU_REGS];
+	 *    -> u32 regs_avail;
+	 *    -> u32 regs_dirty;
+	 */
 	vmx->fail = __vmx_vcpu_run(vmx, (unsigned long *)&vcpu->arch.regs,
 				   vmx->loaded_vmcs->launched);
 
@@ -6927,6 +7121,16 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	loadsegment(es, __USER_DS);
 #endif
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_avail:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|56| <<kvm_register_is_available>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|68| <<kvm_register_mark_available>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|74| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/svm/svm.c|3848| <<svm_vcpu_run>> vcpu->arch.regs_avail &= ~SVM_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/nested.c|273| <<vmx_switch_vmcs>> vcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/vmx.c|7050| <<vmx_vcpu_run>> vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/x86.c|11526| <<kvm_arch_vcpu_create>> vcpu->arch.regs_avail = ~0;
+	 */
 	vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;
 
 	pt_guest_exit(vmx);
@@ -7398,6 +7602,10 @@ static void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	vmx_update_exception_bitmap(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8065| <<hardware_setup>> vmx_set_cpu_caps();
+ */
 static __init void vmx_set_cpu_caps(void)
 {
 	kvm_set_cpu_caps();
@@ -7848,6 +8056,10 @@ static struct kvm_x86_ops vmx_x86_ops __initdata = {
 	.vcpu_deliver_sipi_vector = kvm_vcpu_deliver_sipi_vector,
 };
 
+/*
+ * 在以下使用vmx_handle_intel_pt_intr():
+ *   - arch/x86/kvm/vmx/vmx.c|8255| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;
+ */
 static unsigned int vmx_handle_intel_pt_intr(void)
 {
 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
@@ -7890,6 +8102,9 @@ static __init void vmx_setup_user_return_msrs(void)
 
 static struct kvm_x86_init_ops vmx_init_ops __initdata;
 
+/*
+ * struct kvm_x86_init_ops vmx_init_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 9c6bfcd84008..91b7de4b4d66 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -310,6 +310,21 @@ struct vcpu_vmx {
 		} seg[8];
 	} segment_cache;
 	int vpid;
+	/*
+	 * 在以下设置vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<load_vmcs12_host_state>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1384| <<vmx_set_rflags>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3116| <<vmx_set_cr0>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3370| <<vmx_set_segment>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 * 在以下使用vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/vmx.c|1643| <<vmx_queue_exception>> WARN_ON_ONCE(vmx->emulation_required);
+	 *   - arch/x86/kvm/vmx/vmx.c|5502| <<vmx_emulation_required_with_pending_exception>> return vmx->emulation_required && !vmx->rmode.vm86_active &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5515| <<handle_invalid_guest_state>> while (vmx->emulation_required && count-- != 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6105| <<__vmx_handle_exit>> if (vmx->emulation_required) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6115| <<__vmx_handle_exit>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|6591| <<vmx_handle_exit_irqoff>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|6836| <<vmx_vcpu_run>> if (unlikely(vmx->emulation_required)) {
+	 */
 	bool emulation_required;
 
 	union vmx_exit_reason exit_reason;
@@ -351,6 +366,17 @@ struct vcpu_vmx {
 	struct pt_desc pt_desc;
 	struct lbr_desc lbr_desc;
 
+	/*
+	 * 在以下设置vcpu_vmx->shadow_msr_intercept:
+	 *   - arch/x86/kvm/vmx/vmx.c|3841| <<vmx_disable_intercept_for_msr>> clear_bit(idx, vmx->shadow_msr_intercept.read);
+	 *   - arch/x86/kvm/vmx/vmx.c|3843| <<vmx_disable_intercept_for_msr>> clear_bit(idx, vmx->shadow_msr_intercept.write);
+	 *   - arch/x86/kvm/vmx/vmx.c|3885| <<vmx_enable_intercept_for_msr>> set_bit(idx, vmx->shadow_msr_intercept.read);
+	 *   - arch/x86/kvm/vmx/vmx.c|3887| <<vmx_enable_intercept_for_msr>> set_bit(idx, vmx->shadow_msr_intercept.write);
+	 *   - arch/x86/kvm/vmx/vmx.c|4002| <<vmx_msr_filter_changed>> bool read = test_bit(i, vmx->shadow_msr_intercept.read);
+	 *   - arch/x86/kvm/vmx/vmx.c|4003| <<vmx_msr_filter_changed>> bool write = test_bit(i, vmx->shadow_msr_intercept.write);
+	 *   - arch/x86/kvm/vmx/vmx.c|7204| <<vmx_create_vcpu>> bitmap_fill(vmx->shadow_msr_intercept.read, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+	 *   - arch/x86/kvm/vmx/vmx.c|7205| <<vmx_create_vcpu>> bitmap_fill(vmx->shadow_msr_intercept.write, MAX_POSSIBLE_PASSTHROUGH_MSRS);
+	 */
 	/* Save desired MSR intercept (read: pass-through) state */
 #define MAX_POSSIBLE_PASSTHROUGH_MSRS	15
 	struct {
@@ -411,6 +437,24 @@ void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type);
 u64 vmx_get_l2_tsc_offset(struct kvm_vcpu *vcpu);
 u64 vmx_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|641| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|642| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|644| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|647| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|648| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/vmx.c|3944| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+ *   - arch/x86/kvm/vmx/vmx.c|3960| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|3961| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|3962| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|3963| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|3965| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|3966| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4005| <<vmx_msr_filter_changed>> vmx_set_intercept_for_msr(vcpu, msr, MSR_TYPE_R, read);
+ *   - arch/x86/kvm/vmx/vmx.c|4006| <<vmx_msr_filter_changed>> vmx_set_intercept_for_msr(vcpu, msr, MSR_TYPE_W, write);
+ *   - arch/x86/kvm/vmx/vmx.c|7535| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+ */
 static inline void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr,
 					     int type, bool value)
 {
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index eb4029660bd9..2d05029c248f 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -154,10 +154,32 @@ bool __read_mostly kvm_has_tsc_control;
 EXPORT_SYMBOL_GPL(kvm_has_tsc_control);
 u32  __read_mostly kvm_max_guest_tsc_khz;
 EXPORT_SYMBOL_GPL(kvm_max_guest_tsc_khz);
+/*
+ * 在以下设置kvm_tsc_scaling_ratio_frac_bits:
+ *   - arch/x86/kvm/svm/svm.c|4782| <<svm_hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 32;
+ *   - arch/x86/kvm/vmx/vmx.c|8017| <<hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 48;
+ */
 u8   __read_mostly kvm_tsc_scaling_ratio_frac_bits;
 EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
 u64  __read_mostly kvm_max_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_max_tsc_scaling_ratio);
+/*
+ * 在以下设置kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/x86.c|11825| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+ * 在以下使用kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/lapic.c|1591| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/svm/nested.c|581| <<nested_vmcb02_prepare_control>> if (svm->tsc_ratio_msr != kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/svm/nested.c|870| <<nested_svm_vmexit>> if (svm->tsc_ratio_msr != kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/svm/svm.c|1153| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/vmx/vmx.c|1720| <<vmx_get_l2_tsc_multiplier>> return kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/vmx/vmx.c|7600| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+ *   - arch/x86/kvm/x86.c|2299| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2342| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2439| <<kvm_scale_tsc>> if (ratio != kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|2477| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|2490| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|2701| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+ */
 u64 __read_mostly kvm_default_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_default_tsc_scaling_ratio);
 bool __read_mostly kvm_has_bus_lock_exit;
@@ -190,6 +212,17 @@ int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
 
 /* Enable/disable PMU virtualization */
+/*
+ * 在以下使用enable_pmu:
+ *   - arch/x86/kvm/x86.c|215| <<global>> bool __read_mostly enable_pmu = true;
+ *   - arch/x86/kvm/x86.c|217| <<global>> module_param(enable_pmu, bool, 0444);
+ *   - arch/x86/kvm/cpuid.c|889| <<__do_cpuid_func>> if (!cap.version || !enable_pmu)
+ *   - arch/x86/kvm/svm/pmu.c|104| <<get_gp_pmc_amd>> if (!enable_pmu)
+ *   - arch/x86/kvm/svm/svm.c|4734| <<svm_set_cpu_caps>> if (enable_pmu && boot_cpu_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/svm/svm.c|4873| <<svm_hardware_setup>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/capabilities.h|392| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|490| <<intel_pmu_refresh>> if (!entry || !enable_pmu)
+ */
 bool __read_mostly enable_pmu = true;
 EXPORT_SYMBOL_GPL(enable_pmu);
 module_param(enable_pmu, bool, 0444);
@@ -707,6 +740,20 @@ static void kvm_queue_exception_e_p(struct kvm_vcpu *vcpu, unsigned nr,
 			       true, payload, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7884| <<global>> .complete_emulated_msr = kvm_complete_insn_gp,
+ *   - arch/x86/kvm/svm/svm.c|2411| <<cr_interception>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/svm/svm.c|2446| <<cr_trap>> return kvm_complete_insn_gp(vcpu, ret);
+ *   - arch/x86/kvm/svm/svm.c|2481| <<dr_interception>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/svm/svm.c|2515| <<efer_trap>> return kvm_complete_insn_gp(vcpu, ret);
+ *   - arch/x86/kvm/svm/svm.c|2652| <<svm_complete_emulated_msr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5112| <<handle_cr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5117| <<handle_cr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5120| <<handle_cr>> return kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5125| <<handle_cr>> ret = kvm_complete_insn_gp(vcpu, err);
+ *   - arch/x86/kvm/vmx/vmx.c|5234| <<handle_dr>> return kvm_complete_insn_gp(vcpu, err);
+ */
 int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)
 {
 	if (err)
@@ -729,6 +776,16 @@ static int complete_emulated_insn_gp(struct kvm_vcpu *vcpu, int err)
 				       EMULTYPE_COMPLETE_USER_EXIT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5466| <<init_kvm_tdp_mmu>> context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5644| <<init_kvm_softmmu>> context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5676| <<init_kvm_nested_mmu>> g_context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/svm/nested.c|70| <<svm_inject_page_fault_nested>> kvm_inject_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/vmx/nested.c|495| <<vmx_inject_page_fault_nested>> kvm_inject_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/vmx/sgx.c|132| <<sgx_inject_fault>> kvm_inject_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/x86.c|13006| <<kvm_arch_async_page_not_present>> kvm_inject_page_fault(vcpu, &fault);
+ */
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 {
 	++vcpu->stat.pf_guest;
@@ -1669,6 +1726,13 @@ void kvm_enable_efer_bits(u64 mask)
 }
 EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|682| <<set_msr_interception_bitmap>> if (read && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/svm/svm.c|685| <<set_msr_interception_bitmap>> if (write && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE))
+ *   - arch/x86/kvm/vmx/vmx.c|3868| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {
+ *   - arch/x86/kvm/vmx/vmx.c|3874| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {
+ */
 bool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)
 {
 	struct kvm_x86_msr_filter *msr_filter;
@@ -1852,6 +1916,15 @@ int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data)
 }
 EXPORT_SYMBOL_GPL(kvm_get_msr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|955| <<nested_vmx_load_msr>> if (kvm_set_msr(vcpu, e.index, e.value)) {
+ *   - arch/x86/kvm/vmx/nested.c|2621| <<prepare_vmcs02>> WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,
+ *   - arch/x86/kvm/vmx/nested.c|4316| <<load_vmcs12_host_state>> WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,
+ *   - arch/x86/kvm/vmx/nested.c|4492| <<nested_vmx_restore_host_state>> if (kvm_set_msr(vcpu, h.index, h.value)) {
+ *   - arch/x86/kvm/x86.c|2003| <<kvm_emulate_wrmsr>> r = kvm_set_msr(vcpu, ecx, data);
+ *   - arch/x86/kvm/x86.c|7887| <<emulator_set_msr>> r = kvm_set_msr(vcpu, msr_index, data);
+ */
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data)
 {
 	return kvm_set_msr_ignored_check(vcpu, index, data, false);
@@ -1908,6 +1981,13 @@ static int kvm_msr_user_space(struct kvm_vcpu *vcpu, u32 index,
 	u64 msr_reason = kvm_msr_reason(r);
 
 	/* Check if the user wanted to know about this MSR fault */
+	/*
+	 * Deflect RDMSR and WRMSR to user space when they trigger a #GP
+	 *
+	 * 在以下使用kvm_arch->user_space_msr_mask:
+	 *   - arch/x86/kvm/x86.c|1974| <<kvm_msr_user_space>> if (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))
+	 *   - arch/x86/kvm/x86.c|6270| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X86_USER_SPACE_MSR)>> kvm->arch.user_space_msr_mask = cap->args[0];
+	 */
 	if (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))
 		return 0;
 
@@ -1947,6 +2027,12 @@ int kvm_emulate_rdmsr(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_rdmsr);
 
+/*
+ * 在以下使用kvm_emulate_wrmsr():
+ *   - arch/x86/kvm/vmx/vmx.c|5759| <<global>> [EXIT_REASON_MSR_WRITE] = kvm_emulate_wrmsr,
+ *   - arch/x86/kvm/svm/svm.c|2894| <<msr_interception>> return kvm_emulate_wrmsr(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6208| <<__vmx_handle_exit>> return kvm_emulate_wrmsr(vcpu);
+ */
 int kvm_emulate_wrmsr(struct kvm_vcpu *vcpu)
 {
 	u32 ecx = kvm_rcx_read(vcpu);
@@ -1972,6 +2058,12 @@ int kvm_emulate_wrmsr(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_wrmsr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2046| <<kvm_emulate_invd>> return kvm_emulate_as_nop(vcpu);
+ *   - arch/x86/kvm/x86.c|2053| <<kvm_emulate_mwait>> return kvm_emulate_as_nop(vcpu);
+ *   - arch/x86/kvm/x86.c|2067| <<kvm_emulate_monitor>> return kvm_emulate_as_nop(vcpu);
+ */
 int kvm_emulate_as_nop(struct kvm_vcpu *vcpu)
 {
 	return kvm_skip_emulated_instruction(vcpu);
@@ -2006,6 +2098,11 @@ int kvm_emulate_monitor(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_monitor);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10557| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10610| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+ */
 static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
 {
 	xfer_to_guest_mode_prepare();
@@ -2207,6 +2304,11 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3640| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3646| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
@@ -2273,6 +2375,16 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2303| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3022| <<__get_kvmclock>> if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
+ *   - arch/x86/kvm/x86.c|3036| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3170| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|8802| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|8833| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|8851| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
 static unsigned long max_tsc_khz;
 
@@ -2321,6 +2433,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5686| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|11438| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2370,6 +2487,10 @@ static inline int gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2586| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2377,6 +2498,11 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 
+	/*
+	 * x86在以下设置kvm->online_vcpus:
+	 *   - virt/kvm/kvm_main.c|477| <<kvm_destroy_vcpus>> atomic_set(&kvm->online_vcpus, 0);
+	 *   - virt/kvm/kvm_main.c|3914| <<kvm_vm_ioctl_create_vcpu>> atomic_inc(&kvm->online_vcpus);
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			 atomic_read(&vcpu->kvm->online_vcpus));
 
@@ -2388,6 +2514,10 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   kvm_update_masterclock(vcpu->kvm);
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2424,6 +2554,13 @@ u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc, u64 ratio)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2615| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2654| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3649| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4673| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2435,6 +2572,10 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2485| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 */
 	return vcpu->arch.l1_tsc_offset +
 		kvm_scale_tsc(vcpu, host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
 }
@@ -2465,6 +2606,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2545| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2635| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4578| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
@@ -2489,6 +2636,12 @@ static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 	static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2299| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2325| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+ *   - arch/x86/kvm/x86.c|2342| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_default_tsc_scaling_ratio);
+ */
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)
 {
 	vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
@@ -2524,6 +2677,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2647| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ *   - arch/x86/kvm/x86.c|5269| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched)
 {
@@ -2571,6 +2729,11 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	kvm_track_tsc_matching(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3601| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data); --> 要求msr_info->host_initiated=1
+ *   - arch/x86/kvm/x86.c|11403| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2580,8 +2743,17 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool synchronizing = false;
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * 0 - rdtsc()
+	 */
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2611| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2666| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|11802| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
 	if (vcpu->arch.virtual_tsc_khz) {
@@ -2593,6 +2765,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 */
 			synchronizing = true;
 		} else {
+			/*
+			 * 在以下使用kvm_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2612| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+			 *   - arch/x86/kvm/x86.c|2677| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|11803| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2615,6 +2793,11 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2596| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *   - arch/x86/kvm/x86.c|2662| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
@@ -2628,6 +2811,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2644| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|3533| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|3566| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
@@ -2666,6 +2856,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2890| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2910| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2702,6 +2897,10 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2927| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2743,6 +2942,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3006| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
@@ -2806,6 +3009,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2893| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6250| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|8692| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|11786| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2873,6 +3083,18 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 在以下触发KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1368| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2222| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2412| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8946| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11743| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|98| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10054| <<vcpu_enter_guest(KVM_REQ_MASTERCLOCK_UPDATE)>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_invalidate_tsc_page(kvm);
@@ -2882,6 +3104,19 @@ static void kvm_update_masterclock(struct kvm *kvm)
 }
 
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
+/*
+ * struct kvm_clock_data {
+ *     __u64 clock;
+ *     __u32 flags;
+ *     __u32 pad0;
+ *     __u64 realtime;
+ *     __u64 host_tsc;
+ *     __u32 pad[4];
+ * };
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2926| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2916,6 +3151,11 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2934| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6175| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2994,6 +3234,10 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				     sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * 处理KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|9944| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3014,8 +3258,15 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 */
 	do {
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|2828| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 */
 		use_master_clock = ka->use_master_clock;
 		if (use_master_clock) {
+			/*
+			 * 最早的时候live crash了好多次, 这两个都不变
+			 */
 			host_tsc = ka->master_cycle_now;
 			kernel_ns = ka->master_kernel_ns;
 		}
@@ -3031,6 +3282,9 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	}
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
+		/*
+		 * Returns monotonic time since boot in ns
+		 */
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
@@ -3046,6 +3300,12 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2306| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3099| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4635| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -3058,6 +3318,9 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* With all the info we got, fill in the values */
 
+	/*
+	 * TSC Scaling
+	 */
 	if (kvm_has_tsc_control)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz,
 					    v->arch.l1_tsc_scaling_ratio);
@@ -3069,8 +3332,20 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * 在以下设置pvclock_vcpu_time_info->tsc_timestamp:
+	 *   - arch/x86/kvm/x86.c|3072| <<kvm_guest_time_update>> vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2554| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3128| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10291| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4633| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	vcpu->last_guest_tsc = tsc_timestamp;
 
 	/* If the host uses TSC clocksource, then it is stable */
@@ -3128,6 +3403,13 @@ static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 	struct kvm *kvm = v->kvm;
 
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3185| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3201| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11839| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11885| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
@@ -3161,6 +3443,10 @@ static bool can_set_mci_status(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * 处理MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:
+ *   - arch/x86/kvm/x86.c|3871| <<kvm_set_msr_common>> return set_msr_mce(vcpu, msr_info);
+ */
 static int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	u64 mcg_cap = vcpu->arch.mcg_cap;
@@ -3326,6 +3612,10 @@ void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_service_local_tlb_flush_requests);
 
+/*
+ * 处理KVM_REQ_STEAL_UPDATE:
+ *   - arch/x86/kvm/x86.c|9907| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;
@@ -3499,8 +3789,21 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		kvm_set_lapic_tscdeadline_msr(vcpu, data);
 		break;
 	case MSR_IA32_TSC_ADJUST:
+		/*
+		 * 关于msr_tsc_adjust
+		 * 如果想要修改tsc的counter, 可以直接写入msr_tsc.
+		 * 然而这样很难保证所有的CPU是同步的 (因为每个CPU都要往msr_tsc写一个absolute value)
+		 * 使用msr_tsc_adjust就可以解决这个问题, 因为为每个CPU写入的是offset, 不是absolute value
+		 */
 		if (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {
 			if (!msr_info->host_initiated) {
+				/*
+				 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+				 *   - arch/x86/kvm/x86.c|3532| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+				 *   - arch/x86/kvm/x86.c|3539| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+				 *   - arch/x86/kvm/x86.c|3567| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+				 *   - arch/x86/kvm/x86.c|3932| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+				 */
 				s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
 				adjust_tsc_offset_guest(vcpu, adj);
 				/* Before back to guest, tsc_timestamp must be adjusted
@@ -4515,6 +4818,19 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * [0] kvm_arch_vcpu_load
+ * [0] vcpu_load
+ * [0] kvm_arch_vcpu_create
+ * [0] kvm_vm_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|214| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5792| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -4558,10 +4874,19 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		 * On a host with synchronized TSC, there is no need to update
 		 * kvmclock on vcpu->cpu migration
 		 */
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|2828| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 *
+		 * 触发kvm_gen_kvmclock_update()
+		 */
 		if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
 			kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 		if (vcpu->cpu != cpu)
 			kvm_make_request(KVM_REQ_MIGRATE_TIMER, vcpu);
+		/*
+		 * 这里设置了vcpu->cpu !!!
+		 */
 		vcpu->cpu = cpu;
 	}
 
@@ -4600,6 +4925,11 @@ static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 	mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|247| <<vcpu_put>> kvm_arch_vcpu_put(vcpu);
+ *   - virt/kvm/kvm_main.c|5909| <<kvm_sched_out>> kvm_arch_vcpu_put(vcpu);
+ */
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	int idx;
@@ -4625,6 +4955,9 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
+	/*
+	 * vmx_sync_pir_to_irr()
+	 */
 	static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
 
 	return kvm_apic_get_state(vcpu, s);
@@ -4851,6 +5184,15 @@ static void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,
 		vcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft;
 	events->interrupt.nr = vcpu->arch.interrupt.nr;
 	events->interrupt.soft = 0;
+	/*
+	 * struct kvm_vcpu_events *events:
+	 *   struct {
+	 *       __u8 injected;
+	 *       __u8 nr;
+	 *       __u8 soft;
+	 *       __u8 shadow;
+	 *   } interrupt;
+	 */
 	events->interrupt.shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
 
 	events->nmi.injected = vcpu->arch.nmi_injected;
@@ -5241,6 +5583,10 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4329| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl(filp, ioctl, arg);
+ */
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
@@ -5930,6 +6276,13 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		r = 0;
 		break;
 	case KVM_CAP_X86_DISABLE_EXITS:
+		/*
+		 * struct kvm_enable_cap *cap:
+		 * -> __u32 cap;
+		 * -> __u32 flags; 
+		 * -> __u64 args[4];
+		 * -> __u8  pad[64];
+		 */
 		r = -EINVAL;
 		if (cap->args[0] & ~KVM_X86_DISABLE_VALID_EXITS)
 			break;
@@ -6175,6 +6528,10 @@ static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
 	return 0;
 }
 
+/*
+ * 处理KVM_SET_CLOCK:
+ *   - arch/x86/kvm/x86.c|6625| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> r = kvm_vm_ioctl_set_clock(kvm, argp);
+ */
 static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -7096,6 +7453,22 @@ static int emulator_read_write_onepage(unsigned long addr, void *val,
 	return X86EMUL_CONTINUE;
 }
 
+/*
+ * [0] emulator_read_write
+ * [0] segmented_read
+ * [0] x86_emulate_insn
+ * [0] x86_emulate_instruction
+ * [0] vmx_handle_exit
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|7163| <<emulator_read_emulated>> return emulator_read_write(ctxt, addr, val, bytes,
+ *   - arch/x86/kvm/x86.c|7173| <<emulator_write_emulated>> return emulator_read_write(ctxt, addr, (void *)val, bytes,
+ */
 static int emulator_read_write(struct x86_emulate_ctxt *ctxt,
 			unsigned long addr,
 			void *val, unsigned int bytes,
@@ -7272,15 +7645,33 @@ static int kernel_pio(struct kvm_vcpu *vcpu, void *pd)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7599| <<__emulator_pio_in>> return emulator_pio_in_out(vcpu, size, port, count, true);
+ *   - arch/x86/kvm/x86.c|7650| <<emulator_pio_out>> ret = emulator_pio_in_out(vcpu, size, port, count, false);
+ */
 static int emulator_pio_in_out(struct kvm_vcpu *vcpu, int size,
 			       unsigned short port,
 			       unsigned int count, bool in)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pio_request pio;
+	 *       -> unsigned long linear_rip;
+	 *       -> unsigned long count;
+	 *       -> int in;
+	 *       -> int port;
+	 *       -> int size;
+	 */
 	vcpu->arch.pio.port = port;
 	vcpu->arch.pio.in = in;
 	vcpu->arch.pio.count  = count;
 	vcpu->arch.pio.size = size;
 
+	/*
+	 * 如果kernel不能emulate才要往下走去userspace
+	 */
 	if (!kernel_pio(vcpu, vcpu->arch.pio_data))
 		return 1;
 
@@ -7302,6 +7693,11 @@ static int __emulator_pio_in(struct kvm_vcpu *vcpu, int size,
 	return emulator_pio_in_out(vcpu, size, port, count, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7630| <<emulator_pio_in>> complete_emulator_pio_in(vcpu, val);
+ *   - arch/x86/kvm/x86.c|13330| <<advance_sev_es_emulated_ins>> complete_emulator_pio_in(vcpu, vcpu->arch.sev_pio_data);
+ */
 static void complete_emulator_pio_in(struct kvm_vcpu *vcpu, void *val)
 {
 	int size = vcpu->arch.pio.size;
@@ -7311,6 +7707,12 @@ static void complete_emulator_pio_in(struct kvm_vcpu *vcpu, void *val)
 	vcpu->arch.pio.count = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7638| <<emulator_pio_in_emulated>> return emulator_pio_in(emul_to_vcpu(ctxt), size, port, val, count);
+ *   - arch/x86/kvm/x86.c|8857| <<complete_fast_pio_in>> emulator_pio_in(vcpu, vcpu->arch.pio.size, vcpu->arch.pio.port, &val, 1);
+ *   - arch/x86/kvm/x86.c|8872| <<kvm_fast_pio_in>> ret = emulator_pio_in(vcpu, size, port, &val, 1);
+ */
 static int emulator_pio_in(struct kvm_vcpu *vcpu, int size,
 			   unsigned short port, void *val, unsigned int count)
 {
@@ -7330,10 +7732,19 @@ static int emulator_pio_in(struct kvm_vcpu *vcpu, int size,
 		/* Results already available, fall through.  */
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|7630| <<emulator_pio_in>> complete_emulator_pio_in(vcpu, val);
+	 *   - arch/x86/kvm/x86.c|13330| <<advance_sev_es_emulated_ins>> complete_emulator_pio_in(vcpu, vcpu->arch.sev_pio_data);
+	 */
 	complete_emulator_pio_in(vcpu, val);
 	return 1;
 }
 
+/*
+ * 在以下使用emulator_pio_in_emulated():
+ *    - arch/x86/kvm/x86.c|8069| <<global>> .pio_in_emulated = emulator_pio_in_emulated,
+ */
 static int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,
 				    int size, unsigned short port, void *val,
 				    unsigned int count)
@@ -7719,6 +8130,10 @@ static int emulator_set_xcr(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr)
 	return __kvm_set_xcr(emul_to_vcpu(ctxt), index, xcr);
 }
 
+/*
+ * 在以下使用emulate_ops:
+ *   - arch/x86/kvm/x86.c|8110| <<alloc_emulate_ctxt>> ctxt->ops = &emulate_ops;
+ */
 static const struct x86_emulate_ops emulate_ops = {
 	.read_gpr            = emulator_read_gpr,
 	.write_gpr           = emulator_write_gpr,
@@ -7799,6 +8214,10 @@ static bool inject_emulated_exception(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11556| <<kvm_arch_vcpu_create>> if (!alloc_emulate_ctxt(vcpu))
+ */
 static struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)
 {
 	struct x86_emulate_ctxt *ctxt;
@@ -7811,6 +8230,11 @@ static struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)
 
 	ctxt->vcpu = vcpu;
 	ctxt->ops = &emulate_ops;
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct x86_emulate_ctxt *emulate_ctxt;
+	 */
 	vcpu->arch.emulate_ctxt = ctxt;
 
 	return ctxt;
@@ -8094,6 +8518,12 @@ static bool retry_instruction(struct x86_emulate_ctxt *ctxt,
 static int complete_emulated_mmio(struct kvm_vcpu *vcpu);
 static int complete_emulated_pio(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5223| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_smm_changed(vcpu, events->smi.smm);
+ *   - arch/x86/kvm/x86.c|8044| <<emulator_exiting_smm>> kvm_smm_changed(vcpu, false);
+ *   - arch/x86/kvm/x86.c|10027| <<enter_smm>> kvm_smm_changed(vcpu, true);
+ */
 static void kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm)
 {
 	trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
@@ -8269,6 +8699,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5932| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|8454| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|8461| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -8445,6 +8881,26 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|556| <<avic_unaccelerated_access_interception>> ret = kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|373| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/svm/svm.c|1959| <<io_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2122| <<gp_interception>> return kvm_emulate_instruction(vcpu,
+ *   - arch/x86/kvm/svm/svm.c|2300| <<invlpg_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2308| <<emulate_on_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|1620| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP)) 
+ *   - arch/x86/kvm/vmx/vmx.c|4868| <<handle_rmode_exception>> if (kvm_emulate_instruction(vcpu, 0)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4957| <<handle_exception_nmi>> return kvm_emulate_instruction(vcpu, EMULTYPE_VMWARE_GP);
+ *   - arch/x86/kvm/vmx/vmx.c|5095| <<handle_io>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5169| <<handle_desc>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5382| <<handle_apic_access>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5516| <<handle_ept_violation>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5601| <<handle_invalid_guest_state>> if (!kvm_emulate_instruction(vcpu, 0))
+ *   - arch/x86/kvm/x86.c|775| <<complete_emulated_insn_gp>> return kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE | EMULTYPE_SKIP |
+ *   - arch/x86/kvm/x86.c|7196| <<handle_ud>> return kvm_emulate_instruction(vcpu, emul_type);
+ *   - arch/x86/kvm/x86.c|10708| <<complete_emulated_io>> r = kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);
+ */
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
 {
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
@@ -8563,6 +9019,22 @@ static int kvmclock_cpu_down_prep(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * "constant tsc" (或者"synchronized tsc")是指"The TSC is synchronized
+ * across all sockets/cores".
+ *
+ * 下面是"/proc/cpuinfo"中部分关于tsc的解释
+ *
+ * tsc          : The system has a TSC clock
+ * rdtscp       : The RDTSCP instruction is available
+ * constant_tsc : The TSC is synchronized across all sockets/cores
+ * nonstop_tsc  : The TSC is not affected by power management code
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8658| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|8685| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|8712| <<kvmclock_cpu_online>> tsc_khz_changed(NULL);
+ */
 static void tsc_khz_changed(void *data)
 {
 	struct cpufreq_freqs *freq = data;
@@ -8574,6 +9046,16 @@ static void tsc_khz_changed(void *data)
 		khz = cpufreq_quick_get(raw_smp_processor_id());
 	if (!khz)
 		khz = tsc_khz;
+	/*
+	 * 在以下使用percpu的cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2303| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|3022| <<__get_kvmclock>> if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3036| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3170| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|8802| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|8833| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|8851| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 */
 	__this_cpu_write(cpu_tsc_khz, khz);
 }
 
@@ -8709,6 +9191,10 @@ static int kvmclock_cpu_online(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9150| <<kvm_arch_init>> kvm_timer_init();
+ */
 static void kvm_timer_init(void)
 {
 	max_tsc_khz = tsc_khz;
@@ -8736,6 +9222,10 @@ static void kvm_timer_init(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|8909| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -8767,6 +9257,9 @@ static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 调用的很频繁
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
@@ -8791,6 +9284,14 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5664| <<kvm_init>> r = kvm_arch_init(opaque);
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ */
 int kvm_arch_init(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -8999,6 +9500,18 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, unsigned long flags, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|230| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4212| <<kvm_faultin_pfn>> !kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/avic.c|182| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|247| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|558| <<nested_vmcb02_prepare_control>> WARN_ON(kvm_apicv_activated(svm->vcpu.kvm));
+ *   - arch/x86/kvm/svm/svm.c|1393| <<svm_set_vintr>> WARN_ON(kvm_apicv_activated(svm->vcpu.kvm));
+ *   - arch/x86/kvm/x86.c|9701| <<kvm_vcpu_update_apicv>> activate = kvm_apicv_activated(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|10105| <<vcpu_enter_guest>> WARN_ON_ONCE(kvm_apicv_activated(vcpu->kvm) != kvm_vcpu_apicv_active(vcpu));
+ *   - arch/x86/kvm/x86.c|11159| <<kvm_arch_vcpu_create>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
@@ -9234,6 +9747,11 @@ int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops.nested_ops->check_events(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9260| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ *   - arch/x86/kvm/x86.c|9323| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ */
 static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.exception.error_code && !is_protmode(vcpu))
@@ -9656,6 +10174,10 @@ void kvm_make_scan_ioapic_request(struct kvm *kvm)
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * 处理KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/x86.c|10033| <<vcpu_enter_guest>> kvm_vcpu_update_apicv(vcpu);
+ */
 void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	bool activate;
@@ -9687,6 +10209,12 @@ void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|122| <<synic_update_vector>> __kvm_request_apicv_update(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|9768| <<kvm_request_apicv_update>> __kvm_request_apicv_update(kvm, activate, bit);
+ *   - arch/x86/kvm/x86.c|10952| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_request_apicv_update(kvm, !inhibit, APICV_INHIBIT_REASON_BLOCKIRQ);
+ */
 void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	unsigned long old, new;
@@ -9729,6 +10257,17 @@ void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 }
 EXPORT_SYMBOL_GPL(__kvm_request_apicv_update);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, false, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/i8254.c|315| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/svm/svm.c|2909| <<interrupt_window_interception>> kvm_request_apicv_update(vcpu->kvm, true, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3507| <<svm_enable_irq_window>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3980| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_X2APIC);
+ *   - arch/x86/kvm/svm/svm.c|3988| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_NESTED);
+ *   - arch/x86/kvm/x86.c|5918| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|6300| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_ABSENT);
+ */
 void kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	down_write(&kvm->arch.apicv_update_lock);
@@ -9814,6 +10353,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10789| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -9949,6 +10492,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_vcpu_update_apicv(vcpu);
 		if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
 			kvm_check_async_pf_completion(vcpu);
+		/*
+		 * vmx_msr_filter_changed()
+		 */
 		if (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))
 			static_call(kvm_x86_msr_filter_changed)(vcpu);
 
@@ -10000,6 +10546,13 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	local_irq_disable();
 
 	/* Store vcpu->apicv_active before vcpu->mode.  */
+	/*
+	 * 上面中断的注释, posted IPI被delay了
+	 *
+	 * 修改IN_GUEST_MODE的地方:
+	 *   - arch/x86/kvm/x86.c|10525| <<vcpu_enter_guest>> smp_store_release(&vcpu->mode, IN_GUEST_MODE);
+	 *   - include/linux/kvm_host.h|559| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 */
 	smp_store_release(&vcpu->mode, IN_GUEST_MODE);
 
 	srcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);
@@ -10071,6 +10624,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		 */
 		WARN_ON_ONCE(kvm_apicv_activated(vcpu->kvm) != kvm_vcpu_apicv_active(vcpu));
 
+		/*
+		 * vmx_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -10108,6 +10664,14 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		hw_breakpoint_restore();
 
 	vcpu->arch.last_vmentry_cpu = vcpu->cpu;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2554| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3128| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10291| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4633| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
 
 	vcpu->mode = OUTSIDE_GUEST_MODE;
@@ -10169,12 +10733,21 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		profile_hit(KVM_PROFILING, (void *)rip);
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2399| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|9443| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|10697| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	if (unlikely(vcpu->arch.tsc_always_catchup))
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/*
+	 * vmx_handle_exit()
+	 */
 	r = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -10290,6 +10863,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10340| <<complete_emulated_pio>> return complete_emulated_io(vcpu);
+ *   - arch/x86/kvm/x86.c|10393| <<complete_emulated_mmio>> return complete_emulated_io(vcpu);
+ */
 static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10300,6 +10878,10 @@ static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * 在以下使用complete_emulated_pio():
+ *   - arch/x86/kvm/x86.c|8749| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_pio;
+ */
 static int complete_emulated_pio(struct kvm_vcpu *vcpu)
 {
 	BUG_ON(!vcpu->arch.pio.count);
@@ -10325,6 +10907,11 @@ static int complete_emulated_pio(struct kvm_vcpu *vcpu)
  *       copy data
  *       exit
  */
+/*
+ * 在以下使用complete_emulated_mmio():
+ *   - arch/x86/kvm/x86.c|8417| <<x86_emulate_instruction>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+ *   - arch/x86/kvm/x86.c|10445| <<complete_emulated_mmio>> vcpu->arch.complete_userspace_io = complete_emulated_mmio;
+ */
 static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -10389,6 +10976,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4032| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *kvm_run = vcpu->run;
@@ -10450,6 +11041,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 比如complete_emulated_mmio()或complete_emulated_pio()或者更多
+	 */
 	if (unlikely(vcpu->arch.complete_userspace_io)) {
 		int (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;
 		vcpu->arch.complete_userspace_io = NULL;
@@ -10482,6 +11076,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10941| <<kvm_arch_vcpu_ioctl_get_regs>> __get_regs(vcpu, regs);
+ *   - arch/x86/kvm/x86.c|11472| <<store_regs>> __get_regs(vcpu, &vcpu->run->s.regs.regs);
+ */
 static void __get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)
 {
 	if (vcpu->arch.emulate_regs_need_sync_to_vcpu) {
@@ -11047,6 +11646,10 @@ int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)
 	return 0;
 }
 
+/*
+ * called by;
+ *   - arch/x86/kvm/x86.c|10893| <<kvm_arch_vcpu_ioctl_run>> store_regs(vcpu);
+ */
 static void store_regs(struct kvm_vcpu *vcpu)
 {
 	BUILD_BUG_ON(sizeof(struct kvm_sync_regs) > SYNC_REGS_SIZE_BYTES);
@@ -11062,6 +11665,10 @@ static void store_regs(struct kvm_vcpu *vcpu)
 				vcpu, &vcpu->run->s.regs.events);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10478| <<kvm_arch_vcpu_ioctl_run>> r = sync_regs(vcpu);
+ */
 static int sync_regs(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {
@@ -11092,6 +11699,10 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3873| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -11384,6 +11995,10 @@ void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5078| <<hardware_enable_nolock>> r = kvm_arch_hardware_enable();
+ */
 int kvm_arch_hardware_enable(void)
 {
 	struct kvm *kvm;
@@ -11481,6 +12096,10 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5855| <<kvm_init>> r = kvm_arch_hardware_setup(opaque);
+ */
 int kvm_arch_hardware_setup(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -11565,6 +12184,10 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
 	vcpu->arch.l1tf_flush_l1d = true;
+	/*
+	 * struct kvm_pmu *pmu:
+	 * -> u8 event_count;
+	 */
 	if (pmu->version && unlikely(pmu->event_count)) {
 		pmu->need_cleanup = true;
 		kvm_make_request(KVM_REQ_PMU, vcpu);
@@ -11643,6 +12266,10 @@ static void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)
 	vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11782| <<kvm_arch_destroy_vm>> kvm_free_vcpus(kvm);
+ */
 static void kvm_free_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -11688,6 +12315,16 @@ void kvm_arch_sync_events(struct kvm *kvm)
  * address, i.e. its accessibility is not guaranteed, and must be
  * accessed via __copy_{to,from}_user().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|220| <<avic_alloc_access_page>> ret = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3626| <<init_rmode_identity_map>> uaddr = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3675| <<alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx/vmx.c|4728| <<vmx_set_tss_addr>> ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|11840| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|11842| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 void __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
 				      u32 size)
 {
@@ -11746,6 +12383,11 @@ void kvm_arch_pre_destroy_vm(struct kvm *kvm)
 	kvm_mmu_pre_destroy_vm(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1150| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+ *   - virt/kvm/kvm_main.c|1217| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+ */
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	if (current->mm == kvm->mm) {
@@ -11902,6 +12544,10 @@ void kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1650| <<kvm_prepare_memory_region>> r = kvm_arch_prepare_memory_region(kvm, old, new, change);
+ */
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   const struct kvm_memory_slot *old,
 				   struct kvm_memory_slot *new,
@@ -11933,6 +12579,10 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 	WARN_ON_ONCE(ka->cpu_dirty_logging_count < 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12514| <<kvm_arch_commit_memory_region>> kvm_mmu_slot_apply_flags(kvm, old, new, change);
+ */
 static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new,
@@ -12007,6 +12657,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1710| <<kvm_commit_memory_region>> kvm_arch_commit_memory_region(kvm, old, new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
@@ -12046,6 +12700,10 @@ static inline bool kvm_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)
 			static_call(kvm_x86_guest_apic_has_interrupt)(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12726| <<kvm_arch_vcpu_runnable>> return kvm_vcpu_running(vcpu) || kvm_vcpu_has_events(vcpu);
+ */
 static inline bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 {
 	if (!list_empty_careful(&vcpu->async_pf.done))
@@ -12086,6 +12744,12 @@ static inline bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10754| <<vcpu_block>> if (!kvm_arch_vcpu_runnable(vcpu)) {
+ *   - virt/kvm/kvm_main.c|3519| <<kvm_vcpu_check_block>> if (kvm_arch_vcpu_runnable(vcpu)) {
+ *   - virt/kvm/kvm_main.c|3785| <<kvm_arch_dy_runnable>> return kvm_arch_vcpu_runnable(vcpu);
+ */
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)
 {
 	return kvm_vcpu_running(vcpu) || kvm_vcpu_has_events(vcpu);
@@ -12125,6 +12789,10 @@ unsigned long kvm_arch_vcpu_get_ip(struct kvm_vcpu *vcpu)
 	return kvm_rip_read(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3708| <<kvm_vcpu_kick>> if (kvm_arch_vcpu_should_kick(vcpu)) {
+ */
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
@@ -12165,6 +12833,12 @@ unsigned long kvm_get_rflags(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_get_rflags);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8719| <<x86_emulate_instruction>> __kvm_set_rflags(vcpu, ctxt->eflags);
+ *   - arch/x86/kvm/x86.c|9654| <<inject_pending_event>> __kvm_set_rflags(vcpu, kvm_get_rflags(vcpu) |
+ *   - arch/x86/kvm/x86.c|12611| <<kvm_set_rflags>> __kvm_set_rflags(vcpu, rflags);
+ */
 static void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 {
 	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP &&
@@ -12180,6 +12854,10 @@ void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 }
 EXPORT_SYMBOL_GPL(kvm_set_rflags);
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|149| <<kvm_check_async_pf_completion>> kvm_arch_async_page_ready(vcpu, work);
+ */
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
@@ -12395,6 +13073,11 @@ bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 		return kvm_lapic_enabled(vcpu) && apf_pageready_slot_free(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13112| <<kvm_arch_irq_bypass_add_producer>> kvm_arch_start_assignment(irqfd->kvm);
+ *   - virt/kvm/vfio.c|229| <<kvm_vfio_set_group>> kvm_arch_start_assignment(dev->kvm);
+ */
 void kvm_arch_start_assignment(struct kvm *kvm)
 {
 	if (atomic_inc_return(&kvm->arch.assigned_device_count) == 1)
@@ -12479,6 +13162,10 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	kvm_arch_end_assignment(irqfd->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|635| <<kvm_irq_routing_update>> int ret = kvm_arch_update_irqfd_routing(
+ */
 int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				   uint32_t guest_irq, bool set)
 {
@@ -12499,8 +13186,19 @@ bool kvm_vector_hashing_enabled(void)
 	return vector_hashing;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3383| <<kvm_vcpu_halt>> bool halt_poll_allowed = !kvm_arch_no_poll(vcpu);
+ */
 bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_kvm_poll_control:
+	 *   - arch/x86/kvm/x86.c|3647| <<kvm_set_msr_common>> vcpu->arch.msr_kvm_poll_control = data;
+	 *   - arch/x86/kvm/x86.c|3988| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_kvm_poll_control;
+	 *   - arch/x86/kvm/x86.c|11275| <<kvm_arch_vcpu_postcreate>> vcpu->arch.msr_kvm_poll_control = 1;
+	 *   - arch/x86/kvm/x86.c|12602| <<kvm_arch_no_poll>> return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
+	 */
 	return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
 }
 EXPORT_SYMBOL_GPL(kvm_arch_no_poll);
@@ -12532,6 +13230,10 @@ int kvm_spec_ctrl_test_value(u64 value)
 }
 EXPORT_SYMBOL_GPL(kvm_spec_ctrl_test_value);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5036| <<handle_exception_nmi>> kvm_fixup_and_inject_pf_error(vcpu, cr2, error_code);
+ */
 void kvm_fixup_and_inject_pf_error(struct kvm_vcpu *vcpu, gva_t gva, u16 error_code)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 767ec7f99516..8b6f94300789 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -353,9 +353,22 @@ enum kvm_intr_type {
 	KVM_HANDLING_NMI,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3816| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6520| <<handle_interrupt_nmi_irqoff>> kvm_before_interrupt(vcpu, is_nmi ? KVM_HANDLING_NMI : KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/x86.c|10448| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ */
 static inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,
 					enum kvm_intr_type intr)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1796| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|359| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|364| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|369| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
 }
 
diff --git a/block/blk-map.c b/block/blk-map.c
index c7f71d83eff1..777f7cc6b90a 100644
--- a/block/blk-map.c
+++ b/block/blk-map.c
@@ -230,6 +230,10 @@ static int bio_copy_user_iov(struct request *rq, struct rq_map_data *map_data,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|564| <<blk_rq_map_user_iov>> ret = bio_map_user_iov(rq, &i, gfp_mask);
+ */
 static int bio_map_user_iov(struct request *rq, struct iov_iter *iter,
 		gfp_t gfp_mask)
 {
@@ -296,6 +300,11 @@ static int bio_map_user_iov(struct request *rq, struct iov_iter *iter,
 			break;
 	}
 
+	/*
+	 * 注释
+	 * Append a bio to a passthrough request.  Only works if the bio can be merged
+	 * into the request based on the driver constraints.
+	 */
 	ret = blk_rq_append_bio(rq, bio);
 	if (ret)
 		goto out_unmap;
@@ -479,6 +488,16 @@ static struct bio *bio_copy_kern(struct request_queue *q, void *data,
  * Append a bio to a passthrough request.  Only works if the bio can be merged
  * into the request based on the driver constraints.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|504| <<global>> EXPORT_SYMBOL(blk_rq_append_bio);
+ *   - block/blk-map.c|220| <<bio_copy_user_iov>> ret = blk_rq_append_bio(rq, bio);
+ *   - block/blk-map.c|299| <<bio_map_user_iov>> ret = blk_rq_append_bio(rq, bio);
+ *   - block/blk-map.c|650| <<blk_rq_map_kern>> ret = blk_rq_append_bio(rq, bio);
+ *   - drivers/scsi/ufs/ufshpb.c|708| <<ufshpb_execute_map_req>> blk_rq_append_bio(req, map_req->bio);
+ *   - drivers/target/target_core_pscsi.c|912| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, bio);
+ *   - drivers/target/target_core_pscsi.c|931| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, bio);
+ */
 int blk_rq_append_bio(struct request *rq, struct bio *bio)
 {
 	struct bvec_iter iter;
@@ -518,6 +537,12 @@ EXPORT_SYMBOL(blk_rq_append_bio);
  *    A matching blk_rq_unmap_user() must be issued at the end of I/O, while
  *    still in process context.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|576| <<blk_rq_map_user>> return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
+ *   - drivers/scsi/scsi_ioctl.c|476| <<sg_io>> ret = blk_rq_map_user_iov(rq->q, rq, NULL, &i, GFP_KERNEL);
+ *   - drivers/scsi/sg.c|1833| <<sg_start_req>> res = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);
+ */
 int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 			struct rq_map_data *map_data,
 			const struct iov_iter *iter, gfp_t gfp_mask)
@@ -541,6 +566,9 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 		copy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);
 
 	i = *iter;
+	/*
+	 * 这里是个循环啊!!!
+	 */
 	do {
 		if (copy)
 			ret = bio_copy_user_iov(rq, map_data, &i, gfp_mask);
@@ -562,6 +590,19 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 }
 EXPORT_SYMBOL(blk_rq_map_user_iov);
 
+/*
+ * called by:
+ *   - block/bsg-lib.c|70| <<bsg_transport_sg_io_fn>> ret = blk_rq_map_user(rq->q, job->bidi_rq, NULL,
+ *   - block/bsg-lib.c|84| <<bsg_transport_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->dout_xferp),
+ *   - block/bsg-lib.c|87| <<bsg_transport_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->din_xferp),
+ *   - drivers/nvme/host/ioctl.c|78| <<nvme_submit_user_cmd>> ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
+ *   - drivers/scsi/scsi_bsg.c|52| <<scsi_bsg_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->dout_xferp),
+ *   - drivers/scsi/scsi_bsg.c|55| <<scsi_bsg_sg_io_fn>> ret = blk_rq_map_user(rq->q, rq, NULL, uptr64(hdr->din_xferp),
+ *   - drivers/scsi/scsi_ioctl.c|479| <<sg_io>> ret = blk_rq_map_user(rq->q, rq, NULL, hdr->dxferp,
+ *   - drivers/scsi/sg.c|1836| <<sg_start_req>> res = blk_rq_map_user(q, rq, md, hp->dxferp,
+ *   - drivers/scsi/sr.c|978| <<sr_read_cdda_bpc>> ret = blk_rq_map_user(disk->queue, rq, NULL, ubuf, len, GFP_KERNEL);
+ *   - drivers/scsi/st.c|557| <<st_scsi_execute>> err = blk_rq_map_user(req->q, req, mdata, NULL, bufflen,
+ */
 int blk_rq_map_user(struct request_queue *q, struct request *rq,
 		    struct rq_map_data *map_data, void __user *ubuf,
 		    unsigned long len, gfp_t gfp_mask)
@@ -573,6 +614,12 @@ int blk_rq_map_user(struct request_queue *q, struct request *rq,
 	if (unlikely(ret < 0))
 		return ret;
 
+	/*
+	 * called by:
+	 *   - block/blk-map.c|576| <<blk_rq_map_user>> return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
+	 *   - drivers/scsi/scsi_ioctl.c|476| <<sg_io>> ret = blk_rq_map_user_iov(rq->q, rq, NULL, &i, GFP_KERNEL);
+	 *   - drivers/scsi/sg.c|1833| <<sg_start_req>> res = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);
+	 */
 	return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
 }
 EXPORT_SYMBOL(blk_rq_map_user);
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 4de34a332c9f..47691ac97d04 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -597,6 +597,11 @@ static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 			blk_queue_get_max_sectors(q, req_op(rq)));
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|640| <<ll_back_merge_fn>> return ll_new_hw_segment(req, bio, nr_segs);
+ *   - block/blk-merge.c|659| <<ll_front_merge_fn>> return ll_new_hw_segment(req, bio, nr_segs);
+ */
 static inline int ll_new_hw_segment(struct request *req, struct bio *bio,
 		unsigned int nr_phys_segs)
 {
@@ -622,6 +627,11 @@ static inline int ll_new_hw_segment(struct request *req, struct bio *bio,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|494| <<blk_rq_append_bio>> if (!ll_back_merge_fn(rq, bio, nr_segs))
+ *   - block/blk-merge.c|965| <<bio_attempt_back_merge>> if (!ll_back_merge_fn(req, bio, nr_segs))
+ */
 int ll_back_merge_fn(struct request *req, struct bio *bio, unsigned int nr_segs)
 {
 	if (req_gap_back_merge(req, bio))
diff --git a/block/blk-settings.c b/block/blk-settings.c
index b880c70e22e4..a7e91f0669fd 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -239,6 +239,46 @@ EXPORT_SYMBOL_GPL(blk_queue_max_zone_append_sectors);
  *    Enables a low level driver to set an upper limit on the number of
  *    hw data segments in a request.
  **/
+/*
+ * [0] blk_queue_max_segments
+ * [0] __scsi_init_queue
+ * [0] scsi_alloc_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_add_device
+ * [0] scsi_add_device
+ * [0] tcm_loop_port_link
+ * [0] target_fabric_port_link
+ * [0] configfs_symlink
+ * [0] vfs_symlink
+ * [0] do_symlinkat
+ * [0] __x64_sys_symlink
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * blk_queue_max_segments
+ * [0] __scsi_init_queue
+ * [0] scsi_alloc_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
diff --git a/drivers/acpi/acpi_processor.c b/drivers/acpi/acpi_processor.c
index 6737b1cbf6d6..bccb679b036a 100644
--- a/drivers/acpi/acpi_processor.c
+++ b/drivers/acpi/acpi_processor.c
@@ -168,6 +168,10 @@ int __weak arch_register_cpu(int cpu)
 
 void __weak arch_unregister_cpu(int cpu) {}
 
+/*
+ * called by:
+ *   - drivers/acpi/acpi_processor.c|302| <<acpi_processor_get_info>> int ret = acpi_processor_hotadd_init(pr);
+ */
 static int acpi_processor_hotadd_init(struct acpi_processor *pr)
 {
 	unsigned long long sta;
@@ -214,6 +218,10 @@ static inline int acpi_processor_hotadd_init(struct acpi_processor *pr)
 }
 #endif /* CONFIG_ACPI_HOTPLUG_CPU */
 
+/*
+ * called by:
+ *   - drivers/acpi/acpi_processor.c|374| <<acpi_processor_add>> result = acpi_processor_get_info(device);
+ */
 static int acpi_processor_get_info(struct acpi_device *device)
 {
 	union acpi_object object = { 0 };
@@ -350,6 +358,9 @@ static int acpi_processor_get_info(struct acpi_device *device)
  */
 static DEFINE_PER_CPU(void *, processor_device_array);
 
+/*
+ * struct acpi_scan_handler processor_handler.attach = acpi_processor_add()
+ */
 static int acpi_processor_add(struct acpi_device *device,
 					const struct acpi_device_id *id)
 {
diff --git a/drivers/acpi/acpica/evmisc.c b/drivers/acpi/acpica/evmisc.c
index f14ebcd610ab..2b6c2537071e 100644
--- a/drivers/acpi/acpica/evmisc.c
+++ b/drivers/acpi/acpica/evmisc.c
@@ -158,6 +158,10 @@ acpi_ev_queue_notify_request(struct acpi_namespace_node *node, u32 notify_value)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/evmisc.c|140| <<acpi_ev_queue_notify_request>> acpi_ev_notify_dispatch, info);
+ */
 static void ACPI_SYSTEM_XFACE acpi_ev_notify_dispatch(void *context)
 {
 	union acpi_generic_state *info = (union acpi_generic_state *)context;
diff --git a/drivers/acpi/bus.c b/drivers/acpi/bus.c
index 07f604832fd6..5ff640a5e333 100644
--- a/drivers/acpi/bus.c
+++ b/drivers/acpi/bus.c
@@ -424,6 +424,10 @@ static void acpi_bus_osc_negotiate_usb_control(void)
  * ---------------
  * Callback for all 'system-level' device notifications (values 0x00-0x7F).
  */
+/*
+ * 在以下使用acpi_bus_notify():
+ *   - drivers/acpi/bus.c|1286| <<acpi_bus_init>> acpi_install_notify_handler(ACPI_ROOT_OBJECT, ACPI_SYSTEM_NOTIFY, &acpi_bus_notify, NULL);
+ */
 static void acpi_bus_notify(acpi_handle handle, u32 type, void *data)
 {
 	struct acpi_device *adev;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 45c5c0e45e33..881ca410e00f 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -1058,6 +1058,18 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/dbexec.c|695| <<acpi_db_create_execution_thread>> status = acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbexec.c|849| <<acpi_db_create_execution_threads>> acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbxface.c|448| <<acpi_initialize_debugger>> status = acpi_os_execute(OSL_DEBUGGER_MAIN_THREAD,
+ *   - drivers/acpi/acpica/evgpe.c|526| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/acpica/evgpe.c|823| <<acpi_ev_gpe_dispatch>> status = acpi_os_execute(OSL_GPE_HANDLER,
+ *   - drivers/acpi/acpica/evmisc.c|139| <<acpi_ev_queue_notify_request>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/bus.c|522| <<acpi_device_fixed_event>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_notify_device_fixed, data);
+ *   - drivers/acpi/sbshc.c|232| <<smbus_alarm>> acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/platform/x86/dell/dell-rbtn.c|278| <<rbtn_resume>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1161,6 +1173,11 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/bus.c|492| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|387| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 8b2e5ef15559..d473a8c263a2 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -133,6 +133,13 @@ bool acpi_scan_is_offline(struct acpi_device *adev, bool uevent)
 	return offline;
 }
 
+/*
+ * 在以下使用acpi_bus_offline():
+ *   - drivers/acpi/scan.c|219| <<acpi_scan_try_to_offline>> NULL, acpi_bus_offline, (void *)false,
+ *   - drivers/acpi/scan.c|227| <<acpi_scan_try_to_offline>> acpi_bus_offline(handle, 0, (void *)false, (void **)&errdev);
+ *   - drivers/acpi/scan.c|231| <<acpi_scan_try_to_offline>> NULL, acpi_bus_offline, (void *)true,
+ *   - drivers/acpi/scan.c|234| <<acpi_scan_try_to_offline>> acpi_bus_offline(handle, 0, (void *)true,
+ */
 static acpi_status acpi_bus_offline(acpi_handle handle, u32 lvl, void *data,
 				    void **ret_p)
 {
@@ -200,6 +207,10 @@ static acpi_status acpi_bus_online(acpi_handle handle, u32 lvl, void *data,
 	return AE_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|259| <<acpi_scan_hot_remove>> int error = acpi_scan_try_to_offline(device);
+ */
 static int acpi_scan_try_to_offline(struct acpi_device *device)
 {
 	acpi_handle handle = device->handle;
@@ -246,6 +257,10 @@ static int acpi_scan_try_to_offline(struct acpi_device *device)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|379| <<acpi_generic_hotplug_event>> return acpi_scan_hot_remove(adev);
+ */
 static int acpi_scan_hot_remove(struct acpi_device *device)
 {
 	acpi_handle handle = device->handle;
@@ -361,6 +376,10 @@ static int acpi_scan_bus_check(struct acpi_device *adev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|407| <<acpi_device_hotplug>> error = acpi_generic_hotplug_event(adev, src);
+ */
 static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 {
 	switch (type) {
@@ -381,6 +400,10 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1160| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
@@ -2161,6 +2184,10 @@ static struct acpi_scan_handler generic_device_handler = {
 	.attach = acpi_generic_device_attach,
 };
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|2228| <<acpi_bus_attach>> ret = acpi_scan_attach_handler(device);
+ */
 static int acpi_scan_attach_handler(struct acpi_device *device)
 {
 	struct acpi_hardware_id *hwid;
@@ -2190,6 +2217,13 @@ static int acpi_scan_attach_handler(struct acpi_device *device)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|2249| <<acpi_bus_attach>> acpi_bus_attach(child, first_pass);
+ *   - drivers/acpi/scan.c|2280| <<acpi_scan_clear_dep_fn>> acpi_bus_attach(cdw->adev, true);
+ *   - drivers/acpi/scan.c|2438| <<acpi_bus_scan>> acpi_bus_attach(device, true);
+ *   - drivers/acpi/scan.c|2452| <<acpi_bus_scan>> acpi_bus_attach(device, false);
+ */
 static void acpi_bus_attach(struct acpi_device *device, bool first_pass)
 {
 	struct acpi_device *child;
@@ -2419,6 +2453,17 @@ EXPORT_SYMBOL_GPL(acpi_dev_get_first_consumer_dev);
  *
  * Must be called under acpi_scan_lock.
  */
+/*
+ * called by:
+ *   - drivers/acpi/dock.c|273| <<hotplug_dock_devices>> int ret = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|322| <<acpi_scan_device_check>> error = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|351| <<acpi_scan_bus_check>> error = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|2603| <<acpi_scan_init>> if (acpi_bus_scan(ACPI_ROOT_OBJECT))
+ *   - drivers/acpi/scan.c|2664| <<acpi_table_events_fn>> acpi_bus_scan(ACPI_ROOT_OBJECT);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|428| <<acpiphp_rescan_slot>> acpi_bus_scan(adev->handle);
+ *   - drivers/platform/surface/surface3-wmi.c|232| <<s3_wmi_probe>> acpi_bus_scan(s3_wmi.pnp0c0d_adev->handle);
+ *   - drivers/platform/surface/surface3-wmi.c|242| <<s3_wmi_remove>> acpi_bus_scan(s3_wmi.pnp0c0d_adev->handle);
+ */
 int acpi_bus_scan(acpi_handle handle)
 {
 	struct acpi_device *device = NULL;
diff --git a/drivers/cpuidle/cpuidle-haltpoll.c b/drivers/cpuidle/cpuidle-haltpoll.c
index fcc53215bac8..83b36331c94a 100644
--- a/drivers/cpuidle/cpuidle-haltpoll.c
+++ b/drivers/cpuidle/cpuidle-haltpoll.c
@@ -22,6 +22,16 @@ static bool force __read_mostly;
 module_param(force, bool, 0444);
 MODULE_PARM_DESC(force, "Load unconditionally");
 
+/*
+ * 在以下使用percpu的haltpoll_cpuidle_devices:
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|25| <<global>> static struct cpuidle_device __percpu *haltpoll_cpuidle_devices;
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|61| <<haltpoll_cpu_online>> dev = per_cpu_ptr(haltpoll_cpuidle_devices, cpu);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|78| <<haltpoll_cpu_offline>> dev = per_cpu_ptr(haltpoll_cpuidle_devices, cpu);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|93| <<haltpoll_uninit>> free_percpu(haltpoll_cpuidle_devices);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|94| <<haltpoll_uninit>> haltpoll_cpuidle_devices = NULL;
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|149| <<haltpoll_init>> haltpoll_cpuidle_devices = alloc_percpu(struct cpuidle_device);
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|150| <<haltpoll_init>> if (haltpoll_cpuidle_devices == NULL) {
+ */
 static struct cpuidle_device __percpu *haltpoll_cpuidle_devices;
 static enum cpuhp_state haltpoll_hp_state;
 
@@ -36,6 +46,11 @@ static int default_enter_idle(struct cpuidle_device *dev,
 	return index;
 }
 
+/*
+ * struct cpuidle_driver haltpoll_driver:
+ * -> struct cpuidle_state    states[CPUIDLE_STATE_MAX];
+ * -> int                     state_count;
+ */
 static struct cpuidle_driver haltpoll_driver = {
 	.name = "haltpoll",
 	.governor = "haltpoll",
@@ -99,12 +114,41 @@ static bool haltpoll_want(void)
 	return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
 }
 
+/*
+ * The x86 architecture support code recognizes three kernel command line
+ * options related to CPU idle time management: idle=poll, idle=halt, and
+ * idle=nomwait. The first two of them disable the acpi_idle and intel_idle
+ * drivers altogether, which effectively causes the entire CPUIdle subsystem to
+ * be disabled and makes the idle loop invoke the architecture support code to
+ * deal with idle CPUs. How it does that depends on which of the two parameters
+ * is added to the kernel command line. In the idle=halt case, the architecture
+ * support code will use the HLT instruction of the CPUs (which, as a rule,
+ * suspends the execution of the program and causes the hardware to attempt to
+ * enter the shallowest available idle state) for this purpose, and if
+ * idle=poll is used, idle CPUs will execute a more or less lightweight''
+ * sequence of instructions in a tight loop.  [Note that using ``idle=poll is
+ * somewhat drastic in many cases, as preventing idle CPUs from saving almost
+ * any energy at all may not be the only effect of it. For example, on Intel
+ * hardware it effectively prevents CPUs from using P-states (see CPU
+ * Performance Scaling) that require any number of CPUs in a package to be
+ * idle, so it very well may hurt single-thread computations performance as
+ * well as energy-efficiency. Thus using it for performance reasons may not be
+ * a good idea at all.
+ *
+ * The idle=nomwait option disables the intel_idle driver and causes acpi_idle
+ * to be used (as long as all of the information needed by it is there in the
+ * system’s ACPI tables), but it is not allowed to use the MWAIT instruction of
+ * the CPUs to ask the hardware to enter idle states.
+ */
 static int __init haltpoll_init(void)
 {
 	int ret;
 	struct cpuidle_driver *drv = &haltpoll_driver;
 
 	/* Do not load haltpoll if idle= is passed */
+	/*
+	 * IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT, IDLE_POLL
+	 */
 	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
 		return -ENODEV;
 
diff --git a/drivers/cpuidle/cpuidle.c b/drivers/cpuidle/cpuidle.c
index ef2ea1b12cd8..fd70b8a8e331 100644
--- a/drivers/cpuidle/cpuidle.c
+++ b/drivers/cpuidle/cpuidle.c
@@ -197,6 +197,13 @@ int cpuidle_enter_s2idle(struct cpuidle_driver *drv, struct cpuidle_device *dev)
  * @drv: cpuidle driver for this cpu
  * @index: index into the states table in @drv of the state to enter
  */
+/*
+ * called by:
+ *   - drivers/cpuidle/coupled.c|486| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv,
+ *   - drivers/cpuidle/coupled.c|534| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv,
+ *   - drivers/cpuidle/coupled.c|595| <<cpuidle_enter_state_coupled>> entered_state = cpuidle_enter_state(dev, drv, next_state);
+ *   - drivers/cpuidle/cpuidle.c|351| <<cpuidle_enter>> ret = cpuidle_enter_state(dev, drv, index);
+ */
 int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 			int index)
 {
@@ -234,6 +241,27 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 	stop_critical_timings();
 	if (!(target_state->flags & CPUIDLE_FLAG_RCU_IDLE))
 		rcu_idle_enter();
+	/*
+	 * haltpoll_driver
+	 *
+	 *  44 static struct cpuidle_driver haltpoll_driver = {
+	 *  45         .name = "haltpoll",
+	 *  46         .governor = "haltpoll",
+	 *  47         .states = {
+	 *  48                 { // entry 0 is for polling  },
+	 *  49                 {
+	 *  50                         .enter                  = default_enter_idle,
+	 *  51                         .exit_latency           = 1,
+	 *  52                         .target_residency       = 1,
+	 *  53                         .power_usage            = -1,
+	 *  54                         .name                   = "haltpoll idle",
+	 *  55                         .desc                   = "default architecture idle",
+	 *  56                 },
+	 *  57         },
+	 *  58         .safe_state_index = 0,
+	 *  59         .state_count = 2,
+	 *  60 };
+	 */
 	entered_state = target_state->enter(dev, drv, index);
 	if (!(target_state->flags & CPUIDLE_FLAG_RCU_IDLE))
 		rcu_idle_exit();
@@ -257,6 +285,9 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 		local_irq_enable();
 
 	if (entered_state >= 0) {
+		/*
+		 * haltpoll_driver
+		 */
 		s64 diff, delay = drv->states[entered_state].exit_latency_ns;
 		int i;
 
@@ -316,9 +347,16 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
  * 'false' boolean value if the scheduler tick should not be stopped before
  * entering the returned state.
  */
+/*
+ * called by:
+ *   - kernel/sched/idle.c|232| <<cpuidle_idle_call>> next_state = cpuidle_select(drv, dev, &stop_tick);
+ */
 int cpuidle_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		   bool *stop_tick)
 {
+	/*
+	 * haltpoll_governor
+	 */
 	return cpuidle_curr_governor->select(drv, dev, stop_tick);
 }
 
@@ -389,6 +427,10 @@ void cpuidle_reflect(struct cpuidle_device *dev, int index)
  * @dev:   the cpuidle device
  *
  */
+/*
+ * called by:
+ *   - drivers/cpuidle/poll_state.c|25| <<poll_idle>> limit = cpuidle_poll_time(drv, dev);
+ */
 u64 cpuidle_poll_time(struct cpuidle_driver *drv,
 		      struct cpuidle_device *dev)
 {
diff --git a/drivers/cpuidle/governors/haltpoll.c b/drivers/cpuidle/governors/haltpoll.c
index cb2a96eafc02..9b0977b955d5 100644
--- a/drivers/cpuidle/governors/haltpoll.c
+++ b/drivers/cpuidle/governors/haltpoll.c
@@ -20,22 +20,56 @@
 #include <linux/module.h>
 #include <linux/kvm_para.h>
 
+/*
+ * 在以下使用guest_halt_poll_ns:
+ *   - drivers/cpuidle/governors/haltpoll.c|23| <<global>> static unsigned int guest_halt_poll_ns __read_mostly = 200000;
+ *   - drivers/cpuidle/governors/haltpoll.c|24| <<global>> module_param(guest_halt_poll_ns, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|85| <<adjust_poll_limit>> if (block_ns > dev->poll_limit_ns && block_ns <= guest_halt_poll_ns) {
+ *   - drivers/cpuidle/governors/haltpoll.c|90| <<adjust_poll_limit>> if (val > guest_halt_poll_ns)
+ *   - drivers/cpuidle/governors/haltpoll.c|91| <<adjust_poll_limit>> val = guest_halt_poll_ns;
+ *   - drivers/cpuidle/governors/haltpoll.c|94| <<adjust_poll_limit>> } else if (block_ns > guest_halt_poll_ns &&
+ */
 static unsigned int guest_halt_poll_ns __read_mostly = 200000;
 module_param(guest_halt_poll_ns, uint, 0644);
 
 /* division factor to shrink halt_poll_ns */
+/*
+ * 在以下使用guest_halt_poll_shrink:
+ *   - drivers/cpuidle/governors/haltpoll.c|27| <<global>> static unsigned int guest_halt_poll_shrink __read_mostly = 2;
+ *   - drivers/cpuidle/governors/haltpoll.c|28| <<global>> module_param(guest_halt_poll_shrink, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|96| <<adjust_poll_limit>> unsigned int shrink = guest_halt_poll_shrink;
+ */
 static unsigned int guest_halt_poll_shrink __read_mostly = 2;
 module_param(guest_halt_poll_shrink, uint, 0644);
 
 /* multiplication factor to grow per-cpu poll_limit_ns */
+/*
+ * 在以下使用guest_halt_poll_grow:
+ *   - drivers/cpuidle/governors/haltpoll.c|31| <<global>> static unsigned int guest_halt_poll_grow __read_mostly = 2;
+ *   - drivers/cpuidle/governors/haltpoll.c|32| <<global>> module_param(guest_halt_poll_grow, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|86| <<adjust_poll_limit>> val = dev->poll_limit_ns * guest_halt_poll_grow;
+ */
 static unsigned int guest_halt_poll_grow __read_mostly = 2;
 module_param(guest_halt_poll_grow, uint, 0644);
 
 /* value in us to start growing per-cpu halt_poll_ns */
+/*
+ * 在以下使用guest_halt_poll_grow_start:
+ *   - drivers/cpuidle/governors/haltpoll.c|35| <<global>> static unsigned int guest_halt_poll_grow_start __read_mostly = 50000;
+ *   - drivers/cpuidle/governors/haltpoll.c|36| <<global>> module_param(guest_halt_poll_grow_start, uint, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|88| <<adjust_poll_limit>> if (val < guest_halt_poll_grow_start)
+ *   - drivers/cpuidle/governors/haltpoll.c|89| <<adjust_poll_limit>> val = guest_halt_poll_grow_start;
+ */
 static unsigned int guest_halt_poll_grow_start __read_mostly = 50000;
 module_param(guest_halt_poll_grow_start, uint, 0644);
 
 /* allow shrinking guest halt poll */
+/*
+ * 在以下使用guest_halt_poll_allow_shrink:
+ *   - drivers/cpuidle/governors/haltpoll.c|39| <<global>> static bool guest_halt_poll_allow_shrink __read_mostly = true;
+ *   - drivers/cpuidle/governors/haltpoll.c|40| <<global>> module_param(guest_halt_poll_allow_shrink, bool, 0644);
+ *   - drivers/cpuidle/governors/haltpoll.c|95| <<adjust_poll_limit>> guest_halt_poll_allow_shrink) {
+ */
 static bool guest_halt_poll_allow_shrink __read_mostly = true;
 module_param(guest_halt_poll_allow_shrink, bool, 0644);
 
@@ -75,6 +109,16 @@ static int haltpoll_select(struct cpuidle_driver *drv,
 	return 0;
 }
 
+/*
+ * [0] adjust_poll_limit
+ * [0] haltpoll_reflect
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] secondary_startup_64_no_verify
+ *
+ * called by:
+ *   - drivers/cpuidle/governors/haltpoll.c|117| <<haltpoll_reflect>> adjust_poll_limit(dev, dev->last_residency_ns);
+ */
 static void adjust_poll_limit(struct cpuidle_device *dev, u64 block_ns)
 {
 	unsigned int val;
@@ -113,6 +157,18 @@ static void haltpoll_reflect(struct cpuidle_device *dev, int index)
 {
 	dev->last_state_idx = index;
 
+	/*
+	 * 在以下设置修改cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/cpuidle.c|277| <<cpuidle_enter_state>> dev->last_residency_ns = diff;
+	 *   - drivers/cpuidle/cpuidle.c|306| <<cpuidle_enter_state>> dev->last_residency_ns = 0;
+	 *   - kernel/sched/idle.c|148| <<call_cpuidle>> dev->last_residency_ns = 0;
+	 *   - drivers/cpuidle/cpuidle.c|597| <<__cpuidle_device_init>> dev->last_residency_ns = 0;
+	 * 在以下使用cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/governors/haltpoll.c|155| <<haltpoll_reflect>> adjust_poll_limit(dev, dev->last_residency_ns);
+	 *   - drivers/cpuidle/governors/ladder.c|84| <<ladder_select_state>> last_residency = dev->last_residency_ns - drv->states[last_idx].exit_latency_ns;
+	 *   - drivers/cpuidle/governors/menu.c|496| <<menu_update>> measured_ns = dev->last_residency_ns;
+	 *   - drivers/cpuidle/governors/teo.c|183| <<teo_update>> measured_ns = dev->last_residency_ns;
+	 */
 	if (index != 0)
 		adjust_poll_limit(dev, dev->last_residency_ns);
 }
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index f7e83613ae94..5e5b9f04df7b 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -10,6 +10,17 @@
 
 #define POLL_IDLE_RELAX_COUNT	200
 
+/*
+ * [0] poll_idle
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] secondary_startup_64_no_verify
+ *
+ * 在以下使用poll_idle():
+ *   - drivers/cpuidle/poll_state.c|55| <<cpuidle_poll_state_init>> state->enter = poll_idle;
+ */
 static int __cpuidle poll_idle(struct cpuidle_device *dev,
 			       struct cpuidle_driver *drv, int index)
 {
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index a801ea40908f..3825c1e5a18e 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -1915,6 +1915,12 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1949| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2224| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2584| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -1941,6 +1947,11 @@ static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3291| <<virtnet_probe>> virtnet_set_queues(vi, vi->curr_queue_pairs);
+ *   - drivers/net/virtio_net.c|3380| <<virtnet_restore>> virtnet_set_queues(vi, vi->curr_queue_pairs);
+ */
 static int virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	int err;
@@ -2197,6 +2208,9 @@ static void virtnet_get_drvinfo(struct net_device *dev,
 }
 
 /* TODO: Eliminate OOO packets during switching */
+/*
+ * struct ethtool_ops virtnet_ethtool_ops.set_channels = virtnet_set_channels()
+ */
 static int virtnet_set_channels(struct net_device *dev,
 				struct ethtool_channels *channels)
 {
@@ -2221,6 +2235,12 @@ static int virtnet_set_channels(struct net_device *dev,
 		return -EINVAL;
 
 	cpus_read_lock();
+	/*
+	 * called by:
+	 *   - drivers/net/virtio_net.c|1949| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2224| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+	 *   - drivers/net/virtio_net.c|2584| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+	 */
 	err = _virtnet_set_queues(vi, queue_pairs);
 	if (err) {
 		cpus_read_unlock();
@@ -2861,6 +2881,10 @@ static unsigned int mergeable_min_buf_len(struct virtnet_info *vi, struct virtqu
 		   (unsigned int)GOOD_PACKET_LEN);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3001| <<init_vqs>> ret = virtnet_find_vqs(vi);
+ */
 static int virtnet_find_vqs(struct virtnet_info *vi)
 {
 	vq_callback_t **callbacks;
@@ -2989,6 +3013,11 @@ static int virtnet_alloc_queues(struct virtnet_info *vi)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2459| <<virtnet_restore_up>> err = init_vqs(vi);
+ *   - drivers/net/virtio_net.c|3256| <<virtnet_probe>> err = init_vqs(vi);
+ */
 static int init_vqs(struct virtnet_info *vi)
 {
 	int ret;
diff --git a/drivers/pci/msi/msi.c b/drivers/pci/msi/msi.c
index 9037a7827eca..30fcd290e3c2 100644
--- a/drivers/pci/msi/msi.c
+++ b/drivers/pci/msi/msi.c
@@ -14,6 +14,14 @@
 #include "msi.h"
 
 static int pci_msi_enable = 1;
+/*
+ * 在以下使用pci_msi_ignore_mask:
+ *   - arch/x86/pci/xen.c|470| <<xen_setup_pci_msi>> pci_msi_ignore_mask = 1;
+ *   - drivers/pci/msi/msi.c|378| <<msi_setup_msi_desc>> if (pci_msi_ignore_mask)
+ *   - drivers/pci/msi/msi.c|526| <<msix_setup_msi_descs>> desc.pci.msi_attrib.can_mask = !pci_msi_ignore_mask &&
+ *   - drivers/pci/msi/msi.c|558| <<msix_mask_all>> if (pci_msi_ignore_mask)
+ *   - kernel/irq/msi.c|776| <<msi_check_reservation_mode>> if (IS_ENABLED(CONFIG_PCI_MSI) && pci_msi_ignore_mask)
+ */
 int pci_msi_ignore_mask;
 
 static noinline void pci_msi_update_mask(struct msi_desc *desc, u32 clear, u32 set)
diff --git a/drivers/pci/pci-driver.c b/drivers/pci/pci-driver.c
index 588588cfda48..3b6bca8ce731 100644
--- a/drivers/pci/pci-driver.c
+++ b/drivers/pci/pci-driver.c
@@ -301,6 +301,11 @@ struct drv_dev_and_id {
 	const struct pci_device_id *id;
 };
 
+/*
+ * called by:
+ *   - drivers/pci/pci-driver.c|378| <<pci_call_probe>> error = work_on_cpu(cpu, local_pci_probe, &ddi);
+ *   - drivers/pci/pci-driver.c|380| <<pci_call_probe>> error = local_pci_probe(&ddi);
+ */
 static long local_pci_probe(void *_ddi)
 {
 	struct drv_dev_and_id *ddi = _ddi;
diff --git a/drivers/scsi/scsi_ioctl.c b/drivers/scsi/scsi_ioctl.c
index e13fd380deb6..d11a840334a6 100644
--- a/drivers/scsi/scsi_ioctl.c
+++ b/drivers/scsi/scsi_ioctl.c
@@ -408,6 +408,11 @@ static int scsi_complete_sghdr_rq(struct request *rq, struct sg_io_hdr *hdr,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_ioctl.c|847| <<scsi_cdrom_send_packet>> err = sg_io(sdev, &hdr, mode);
+ *   - drivers/scsi/scsi_ioctl.c|871| <<scsi_ioctl_sg_io>> error = sg_io(sdev, &hdr, mode);
+ */
 static int sg_io(struct scsi_device *sdev, struct sg_io_hdr *hdr, fmode_t mode)
 {
 	unsigned long start_time;
@@ -445,6 +450,11 @@ static int sg_io(struct scsi_device *sdev, struct sg_io_hdr *hdr, fmode_t mode)
 		return PTR_ERR(rq);
 	req = scsi_req(rq);
 
+	/*
+	 * struct sg_io_hdr *hdr:
+	 * -> unsigned char cmd_len; // [i] SCSI command length
+	 * -> unsigned short iovec_count; // [i] 0 implies no scatter gather
+	 */
 	if (hdr->cmd_len > BLK_MAX_CDB) {
 		req->cmd = kzalloc(hdr->cmd_len, GFP_KERNEL);
 		if (!req->cmd)
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 9394aa9444c1..9a2b3a09282a 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -412,6 +412,16 @@ static int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|735| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|773| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|796| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|824| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|911| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1154| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|2371| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+ */
 static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 {
 	struct mm_struct *mm;
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 532e204f2b1b..7f770bd7e582 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -921,6 +921,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 1768362115c6..1d4113a51a76 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -1587,6 +1587,15 @@ static long vhost_vring_set_num_addr(struct vhost_dev *d,
 
 	return r;
 }
+
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1737| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/scsi.c|1903| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+ *   - drivers/vhost/test.c|358| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/vdpa.c|395| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+ *   - drivers/vhost/vsock.c|905| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+ */
 long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
 	struct file *eventfp, *filep = NULL;
@@ -2353,6 +2362,13 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|566| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2487| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|222| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|554| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2364,6 +2380,11 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2409| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2415| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2385,6 +2406,24 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|323| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2394| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2458| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2460| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 *
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2035| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|322| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|2401| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|976| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2387| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2512| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->last_used_idx;
 	new = (vq->last_used_idx += count);
 	/* If the driver never bothers to signal in a very long while,
@@ -2393,11 +2432,31 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	 * signals at least once in 2^16 and remove this. */
 	if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
 		vq->signalled_used_valid = false;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|324| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2023| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2395| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2459| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2461| <<vhost_notify>> vq->signalled_used_valid = true;
+	 *
+	 * old 是 vq->last_used_idx (u16)
+	 * new 是 vq->last_used_idx + count (u16)
+	 *
+	 * vq->signalled_used_valid = false的前提就是:
+	 *     vq->signalled_used比vq->last_used_idx大
+	 *     也就是vq->signalled_used比vq->last_used_idx增长的多
+	 */
 	return 0;
 }
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2363| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2497| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
@@ -2433,6 +2492,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2477| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2455,8 +2518,36 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		}
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
+	/*
+	 *
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|323| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2394| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2458| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2460| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->signalled_used;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|324| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2023| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2395| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2459| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2461| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2035| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|322| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|2401| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|976| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2387| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2512| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
@@ -2467,6 +2558,11 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 * 也就是
+	 * (__u16)(new - event - 1) < (__u16)(new - old)
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 638bb640d6b4..86575e7b8487 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -93,15 +93,42 @@ struct vhost_virtqueue {
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下设置vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2035| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|322| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|2401| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|976| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2387| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2512| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|323| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2394| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2458| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2460| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|324| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2023| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2395| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2459| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2461| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 962f1477b1fa..2ba3e086578f 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -92,6 +92,11 @@ struct vring_virtqueue {
 	bool packed_ring;
 
 	/* Is DMA API used? */
+	/*
+	 * 在以下设置vring_virtqueue->use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1715| <<vring_create_virtqueue_packed()>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2203| <<__vring_new_virtqueue()>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 */
 	bool use_dma_api;
 
 	/* Can we use weak barriers? */
@@ -331,6 +336,11 @@ static dma_addr_t vring_map_one_sg(const struct vring_virtqueue *vq,
 				   struct scatterlist *sg,
 				   enum dma_data_direction direction)
 {
+	/*
+	 * 在以下设置vring_virtqueue->use_dma_api:
+	 *   - drivers/virtio/virtio_ring.c|1715| <<vring_create_virtqueue_packed()>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 *   - drivers/virtio/virtio_ring.c|2203| <<__vring_new_virtqueue()>> vq->use_dma_api = vring_use_dma_api(vdev);
+	 */
 	if (!vq->use_dma_api)
 		return (dma_addr_t)sg_phys(sg);
 
@@ -915,6 +925,10 @@ static void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2311| <<vring_create_virtqueue>> return vring_create_virtqueue_split(index, num, vring_align,
+ */
 static struct virtqueue *vring_create_virtqueue_split(
 	unsigned int index,
 	unsigned int num,
@@ -1938,6 +1952,26 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|306| <<virtio_commit_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/block/virtio_blk.c|361| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|431| <<virtio_gpu_notify>> virtqueue_notify(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|475| <<virtio_gpu_queue_cursor>> virtqueue_notify(vq);
+ *   - drivers/net/virtio_net.c|623| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1349| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1561| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1762| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|994| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|472| <<virtscsi_kick_vq>> virtqueue_notify(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|500| <<virtscsi_add_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|1982| <<virtqueue_kick>> return virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|460| <<send_forget_request>> virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|1206| <<virtio_fs_enqueue_req>> virtqueue_notify(vq);
+ *   - sound/virtio/virtio_card.c|45| <<virtsnd_event_send>> virtqueue_notify(vqueue);
+ *   - sound/virtio/virtio_ctl_msg.c|174| <<virtsnd_ctl_msg_send>> virtqueue_notify(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|248| <<virtsnd_pcm_msg_send>> virtqueue_notify(vqueue);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2169,6 +2203,11 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 EXPORT_SYMBOL_GPL(vring_interrupt);
 
 /* Only available for split ring */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|977| <<vring_create_virtqueue_split>> vq = __vring_new_virtqueue(index, vring, vdev, weak_barriers, context,
+ *   - drivers/virtio/virtio_ring.c|2330| <<vring_new_virtqueue>> return __vring_new_virtqueue(index, vring, vdev, weak_barriers, context,
+ */
 struct virtqueue *__vring_new_virtqueue(unsigned int index,
 					struct vring vring,
 					struct virtio_device *vdev,
@@ -2255,6 +2294,15 @@ struct virtqueue *__vring_new_virtqueue(unsigned int index,
 }
 EXPORT_SYMBOL_GPL(__vring_new_virtqueue);
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|941| <<vu_setup_vq>> vq = vring_create_virtqueue(index, num, PAGE_SIZE, vdev, true, true,
+ *   - drivers/s390/virtio/virtio_ccw.c|522| <<virtio_ccw_setup_vq>> vq = vring_create_virtqueue(i, info->num, KVM_VIRTIO_CCW_RING_ALIGN,
+ *   - drivers/virtio/virtio_mmio.c|386| <<vm_setup_vq>> vq = vring_create_virtqueue(index, num, VIRTIO_MMIO_VRING_ALIGN, vdev,
+ *   - drivers/virtio/virtio_pci_legacy.c|131| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_pci_modern.c|214| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_vdpa.c|178| <<virtio_vdpa_setup_vq>> vq = vring_create_virtqueue(index, max_num, align, vdev,
+ */
 struct virtqueue *vring_create_virtqueue(
 	unsigned int index,
 	unsigned int num,
@@ -2280,6 +2328,12 @@ struct virtqueue *vring_create_virtqueue(
 EXPORT_SYMBOL_GPL(vring_create_virtqueue);
 
 /* Only available for split ring */
+/*
+ * called by:
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|952| <<mlxbf_tmfifo_virtio_find_vqs>> vq = vring_new_virtqueue(i, vring->num, vring->align, vdev,
+ *   - drivers/remoteproc/remoteproc_virtio.c|120| <<rp_find_vq>> vq = vring_new_virtqueue(id, len, rvring->align, vdev, false, ctx,
+ *   - 
+ */
 struct virtqueue *vring_new_virtqueue(unsigned int index,
 				      unsigned int num,
 				      unsigned int vring_align,
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 47aebd98f52f..aabc084ca0dc 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -152,6 +152,11 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	return "";
 }
 
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|146| <<xen_mm_init>> rc = xen_swiotlb_init();
+ *   - arch/x86/xen/pci-swiotlb-xen.c|79| <<pci_xen_swiotlb_init_late>> rc = xen_swiotlb_init();
+ */
 int xen_swiotlb_init(void)
 {
 	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 8e03b3d3f5fa..6f121d0f7c5d 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -365,6 +365,18 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
+/*
+ * called by:
+ *   - mm/huge_memory.c|643| <<__do_huge_pmd_anonymous_page>> ret = handle_userfault(vmf, VM_UFFD_MISSING);
+ *   - mm/huge_memory.c|762| <<do_huge_pmd_anonymous_page>> ret = handle_userfault(vmf, VM_UFFD_MISSING);
+ *   - mm/hugetlb.c|5370| <<hugetlb_handle_userfault>> ret = handle_userfault(&vmf, reason);
+ *   - mm/memory.c|3259| <<do_wp_page>> return handle_userfault(vmf, VM_UFFD_WP);
+ *   - mm/memory.c|3771| <<do_anonymous_page>> return handle_userfault(vmf, VM_UFFD_MISSING);
+ *   - mm/memory.c|3814| <<do_anonymous_page>> return handle_userfault(vmf, VM_UFFD_MISSING);
+ *   - mm/memory.c|4452| <<wp_huge_pmd>> return handle_userfault(vmf, VM_UFFD_WP);
+ *   - mm/shmem.c|1811| <<shmem_getpage_gfp>> *fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+ *   - mm/shmem.c|1853| <<shmem_getpage_gfp>> *fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+ */
 vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 {
 	struct mm_struct *mm = vmf->vma->vm_mm;
diff --git a/include/linux/cpuidle.h b/include/linux/cpuidle.h
index fce476275e16..4a5c84865357 100644
--- a/include/linux/cpuidle.h
+++ b/include/linux/cpuidle.h
@@ -97,7 +97,35 @@ struct cpuidle_device {
 	ktime_t			next_hrtimer;
 
 	int			last_state_idx;
+	/*
+	 * 在以下设置修改cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/cpuidle.c|277| <<cpuidle_enter_state>> dev->last_residency_ns = diff;
+	 *   - drivers/cpuidle/cpuidle.c|306| <<cpuidle_enter_state>> dev->last_residency_ns = 0;
+	 *   - kernel/sched/idle.c|148| <<call_cpuidle>> dev->last_residency_ns = 0;
+	 *   - drivers/cpuidle/cpuidle.c|597| <<__cpuidle_device_init>> dev->last_residency_ns = 0;
+	 * 在以下使用cpuidle_device->last_residency_ns:
+	 *   - drivers/cpuidle/governors/haltpoll.c|155| <<haltpoll_reflect>> adjust_poll_limit(dev, dev->last_residency_ns);
+	 *   - drivers/cpuidle/governors/ladder.c|84| <<ladder_select_state>> last_residency = dev->last_residency_ns - drv->states[last_idx].exit_latency_ns;
+	 *   - drivers/cpuidle/governors/menu.c|496| <<menu_update>> measured_ns = dev->last_residency_ns;
+	 *   - drivers/cpuidle/governors/teo.c|183| <<teo_update>> measured_ns = dev->last_residency_ns;
+	 */
 	u64			last_residency_ns;
+	/*
+	 * 在以下设置修改cpuidle_device->poll_limit_ns:
+	 *   - drivers/cpuidle/cpuidle.c|418| <<cpuidle_poll_time>> dev->poll_limit_ns = limit_ns;
+	 *   - drivers/cpuidle/governors/haltpoll.c|93| <<adjust_poll_limit>> dev->poll_limit_ns = val;
+	 *   - drivers/cpuidle/governors/haltpoll.c|103| <<adjust_poll_limit>> dev->poll_limit_ns = val;
+	 *   - drivers/cpuidle/governors/haltpoll.c|128| <<haltpoll_enable_device>> dev->poll_limit_ns = 0;
+	 *   - drivers/cpuidle/sysfs.c|433| <<cpuidle_state_store>> dev->poll_limit_ns = 0;
+	 * 在以下使用cpuidle_device->poll_limit_ns:
+	 *   - drivers/cpuidle/cpuidle.c|400| <<cpuidle_poll_time>> if (dev->poll_limit_ns)
+	 *   - drivers/cpuidle/cpuidle.c|401| <<cpuidle_poll_time>> return dev->poll_limit_ns;
+	 *   - drivers/cpuidle/cpuidle.c|420| <<cpuidle_poll_time>> return dev->poll_limit_ns;
+	 *   - drivers/cpuidle/governors/haltpoll.c|59| <<haltpoll_select>> if (dev->poll_limit_ns == 0)
+	 *   - drivers/cpuidle/governors/haltpoll.c|85| <<adjust_poll_limit>> if (block_ns > dev->poll_limit_ns && block_ns <= guest_halt_poll_ns) {
+	 *   - drivers/cpuidle/governors/haltpoll.c|86| <<adjust_poll_limit>> val = dev->poll_limit_ns * guest_halt_poll_grow;
+	 *   - drivers/cpuidle/governors/haltpoll.c|98| <<adjust_poll_limit>> val = dev->poll_limit_ns;
+	 */
 	u64			poll_limit_ns;
 	u64			forced_idle_latency_limit_ns;
 	struct cpuidle_state_usage	states_usage[CPUIDLE_STATE_MAX];
diff --git a/include/linux/kvm_dirty_ring.h b/include/linux/kvm_dirty_ring.h
index 906f899813dc..b80548c292b5 100644
--- a/include/linux/kvm_dirty_ring.h
+++ b/include/linux/kvm_dirty_ring.h
@@ -23,6 +23,16 @@ struct kvm_dirty_ring {
 	u32 reset_index;
 	u32 size;
 	u32 soft_limit;
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	struct kvm_dirty_gfn *dirty_gfns;
 	int index;
 };
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f11039944c08..d4b73ec0f7e7 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -251,9 +251,36 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range);
 #endif
 
 enum {
+	/*
+	 * 修改OUTSIDE_GUEST_MODE的地方:
+	 *   - arch/x86/kvm/mmu/mmu.c|853| <<walk_shadow_page_lockless_end>> smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);
+	 *   - arch/x86/kvm/x86.c|10554| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|10638| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 */
 	OUTSIDE_GUEST_MODE,
+	/*
+	 * x86修改IN_GUEST_MODE的地方:
+	 *   - arch/x86/kvm/x86.c|10525| <<vcpu_enter_guest>> smp_store_release(&vcpu->mode, IN_GUEST_MODE);
+	 *   - include/linux/kvm_host.h|559| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 * x86使用IN_GUEST_MODE的地方:
+	 *   - arch/x86/kvm/lapic.c|129| <<kvm_use_posted_timer_interrupt>> return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
+	 *   - arch/x86/kvm/svm/svm.c|3324| <<svm_complete_interrupt_delivery>> bool in_guest_mode = (smp_load_acquire(&vcpu->mode) == IN_GUEST_MODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|4056| <<kvm_vcpu_trigger_posted_interrupt>> if (vcpu->mode == IN_GUEST_MODE) {
+	 *   - arch/x86/kvm/x86.c|12753| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+	 *   - virt/kvm/kvm_main.c|269| <<kvm_request_needs_ipi>> return mode == IN_GUEST_MODE;
+	 *   - virt/kvm/kvm_main.c|3696| <<kvm_vcpu_kick>> if (vcpu->mode == IN_GUEST_MODE)
+	 */
 	IN_GUEST_MODE,
+	/*
+	 * 修改EXITING_GUEST_MODE的地方:
+	 *   - include/linux/kvm_host.h|559| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 *   - virt/kvm/kvm_main.c|3697| <<kvm_vcpu_kick>> WRITE_ONCE(vcpu->mode, EXITING_GUEST_MODE);
+	 */
 	EXITING_GUEST_MODE,
+	/*
+	 * 修改READING_SHADOW_PAGE_TABLES的地方:
+	 *   - arch/x86/kvm/mmu/mmu.c|839| <<walk_shadow_page_lockless_begin>> smp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES);
+	 */
 	READING_SHADOW_PAGE_TABLES,
 };
 
@@ -307,6 +334,21 @@ struct kvm_vcpu {
 	int vcpu_id; /* id given by userspace at creation */
 	int vcpu_idx; /* index in kvm->vcpus array */
 	int srcu_idx;
+	/*
+	 * 在以下使用kvm_vcpu->mode:
+	 *   - arch/x86/kvm/lapic.c|129| <<kvm_use_posted_timer_interrupt>> return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
+	 *   - arch/x86/kvm/mmu/mmu.c|819| <<walk_shadow_page_lockless_begin>> smp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES);
+	 *   - arch/x86/kvm/mmu/mmu.c|833| <<walk_shadow_page_lockless_end>> smp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);
+	 *   - arch/x86/kvm/svm/svm.c|3324| <<svm_complete_interrupt_delivery>> bool in_guest_mode = (smp_load_acquire(&vcpu->mode) == IN_GUEST_MODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|4011| <<kvm_vcpu_trigger_posted_interrupt>> if (vcpu->mode == IN_GUEST_MODE) {
+	 *   - arch/x86/kvm/x86.c|2080| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+	 *   - arch/x86/kvm/x86.c|10488| <<vcpu_enter_guest>> smp_store_release(&vcpu->mode, IN_GUEST_MODE);
+	 *   - arch/x86/kvm/x86.c|10517| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - arch/x86/kvm/x86.c|10601| <<vcpu_enter_guest>> vcpu->mode = OUTSIDE_GUEST_MODE;
+	 *   - include/linux/kvm_host.h|544| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 *   - virt/kvm/kvm_main.c|3684| <<kvm_vcpu_kick>> if (vcpu->mode == IN_GUEST_MODE)
+	 *   - virt/kvm/kvm_main.c|3685| <<kvm_vcpu_kick>> WRITE_ONCE(vcpu->mode, EXITING_GUEST_MODE);
+	 */
 	int mode;
 	u64 requests;
 	unsigned long guest_debug;
@@ -324,6 +366,20 @@ struct kvm_vcpu {
 	bool valid_wakeup;
 
 #ifdef CONFIG_HAS_IOMEM
+	/*
+	 * x86在以下使用kvm_vcpu->mmio_needed:
+	 *   - arch/x86/kvm/vmx/vmx.c|4994| <<handle_triple_fault>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|7146| <<emulator_read_write>> vcpu->mmio_needed = 1; 
+	 *   - arch/x86/kvm/x86.c|8411| <<x86_emulate_instruction>> } else if (vcpu->mmio_needed) {
+	 *   - arch/x86/kvm/x86.c|9950| <<vcpu_enter_guest>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|10407| <<complete_emulated_mmio>> BUG_ON(!vcpu->mmio_needed);
+	 *   - arch/x86/kvm/x86.c|10427| <<complete_emulated_mmio>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|10533| <<kvm_arch_vcpu_ioctl_run>> WARN_ON(vcpu->arch.pio.count || vcpu->mmio_needed);
+	 *   - arch/x86/kvm/x86.c|12766| <<complete_sev_es_emulated_mmio>> BUG_ON(!vcpu->mmio_needed);
+	 *   - arch/x86/kvm/x86.c|12786| <<complete_sev_es_emulated_mmio>> vcpu->mmio_needed = 0;
+	 *   - arch/x86/kvm/x86.c|12830| <<kvm_sev_es_mmio_write>> vcpu->mmio_needed = 1;
+	 *   - arch/x86/kvm/x86.c|12869| <<kvm_sev_es_mmio_read>> vcpu->mmio_needed = 1;
+	 */
 	int mmio_needed;
 	int mmio_read_completed;
 	int mmio_is_write;
@@ -519,6 +575,13 @@ static __always_inline void guest_state_exit_irqoff(void)
 	instrumentation_end();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|68| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - arch/riscv/kvm/vcpu.c|163| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - arch/x86/kvm/x86.c|12753| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - virt/kvm/kvm_main.c|257| <<kvm_request_needs_ipi>> int mode = kvm_vcpu_exiting_guest_mode(vcpu);
+ */
 static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -527,6 +590,10 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 	 * memory barrier following the write of vcpu->mode in VCPU RUN.
 	 */
 	smp_mb__before_atomic();
+	/*
+	 * 将old和ptr指向的内容比较,如果相等,则将new写入到ptr中,返回old,
+	 * 如果不相等,则返回ptr指向的内容
+	 */
 	return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
 }
 
@@ -558,6 +625,17 @@ struct kvm_memory_slot {
 	struct rb_node gfn_node[2];
 	gfn_t base_gfn;
 	unsigned long npages;
+	/*
+	 * 在以下设置分配系kvm_memory_slot->dirty_bitmap:
+	 *   - virt/kvm/kvm_main.c|948| <<kvm_destroy_dirty_bitmap>> kvfree(memslot->dirty_bitmap);
+	 *   - virt/kvm/kvm_main.c|949| <<kvm_destroy_dirty_bitmap>> memslot->dirty_bitmap = NULL;                                                                                                      
+	 *   - virt/kvm/kvm_main.c|1409| <<kvm_alloc_dirty_bitmap>> memslot->dirty_bitmap = kvzalloc(dirty_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1637| <<kvm_prepare_memory_region>> new->dirty_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1639| <<kvm_prepare_memory_region>> new->dirty_bitmap = old->dirty_bitmap;
+	 *   - virt/kvm/kvm_main.c|1646| <<kvm_prepare_memory_region>> bitmap_set(new->dirty_bitmap, 0, new->npages);
+	 *   - virt/kvm/kvm_main.c|1730| <<kvm_copy_memslot>> dest->dirty_bitmap = src->dirty_bitmap;
+	 *   - virt/kvm/kvm_main.c|3353| <<mark_page_dirty_in_slot>> set_bit_le(rel_gfn, memslot->dirty_bitmap);
+	 */
 	unsigned long *dirty_bitmap;
 	struct kvm_arch_memory_slot arch;
 	unsigned long userspace_addr;
@@ -566,6 +644,14 @@ struct kvm_memory_slot {
 	u16 as_id;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1042| <<gfn_to_memslot_dirty_bitmap>> if (no_dirty_log && kvm_slot_dirty_track_enabled(slot))
+ *   - arch/x86/kvm/mmu/mmu.c|3256| <<kvm_mmu_hugepage_adjust>> if (kvm_slot_dirty_track_enabled(slot))
+ *   - arch/x86/kvm/mmu/mmu.c|3703| <<fast_page_fault>> kvm_slot_dirty_track_enabled(fault->slot))
+ *   - arch/x86/kvm/mmu/spte.c|218| <<make_spte>> if ((spte & PT_WRITABLE_MASK) && kvm_slot_dirty_track_enabled(slot)) {
+ *   - virt/kvm/kvm_main.c|3375| <<mark_page_dirty_in_slot>> if (memslot && kvm_slot_dirty_track_enabled(memslot)) {
+ */
 static inline bool kvm_slot_dirty_track_enabled(const struct kvm_memory_slot *slot)
 {
 	return slot->flags & KVM_MEM_LOG_DIRTY_PAGES;
@@ -714,6 +800,20 @@ struct kvm {
 	 * incremented after storing the kvm_vcpu pointer in vcpus,
 	 * and is accessed atomically.
 	 */
+	/*
+	 * x86在以下设置kvm->online_vcpus:
+	 *   - virt/kvm/kvm_main.c|477| <<kvm_destroy_vcpus>> atomic_set(&kvm->online_vcpus, 0);
+	 *   - virt/kvm/kvm_main.c|3914| <<kvm_vm_ioctl_create_vcpu>> atomic_inc(&kvm->online_vcpus);
+	 * x86在以下使用kvm->online_vcpus:
+	 *   - arch/x86/kvm/svm/sev.c|1654| <<sev_es_migrate_from>> if (atomic_read(&src->online_vcpus) != atomic_read(&dst->online_vcpus))
+	 *   - arch/x86/kvm/x86.c|2381| <<kvm_track_tsc_matching>> atomic_read(&vcpu->kvm->online_vcpus));
+	 *   - arch/x86/kvm/x86.c|2396| <<kvm_track_tsc_matching>> atomic_read(&vcpu->kvm->online_vcpus),
+	 *   - arch/x86/kvm/x86.c|2818| <<pvclock_update_vm_gtod_copy>> atomic_read(&kvm->online_vcpus));
+	 *   - arch/x86/kvm/x86.c|11196| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+	 *   - include/linux/kvm_host.h|873| <<kvm_get_vcpu>> int num_vcpus = atomic_read(&kvm->online_vcpus);
+	 *   - include/linux/kvm_host.h|883| <<kvm_for_each_vcpu>> (atomic_read(&kvm->online_vcpus) - 1))
+	 *   - virt/kvm/kvm_main.c|3890| <<kvm_vm_ioctl_create_vcpu>> vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	 */
 	atomic_t online_vcpus;
 	int created_vcpus;
 	int last_boosted_vcpu;
@@ -731,6 +831,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|843| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|927| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1107| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1272| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1287| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1313| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1327| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
@@ -751,8 +862,25 @@ struct kvm {
 
 #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 	struct mmu_notifier mmu_notifier;
+	/*
+	 * arm和x86在以下使用kvm->mmu_notifier_seq:
+	 *   - arch/arm64/kvm/mmu.c|1167| <<user_mem_abort>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/mmu.c|4528| <<direct_page_fault>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME>> mmu_seq = vcpu->kvm->mmu_notifier_seq;
+	 *   - include/linux/kvm_host.h|1936| <<mmu_notifier_retry>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - include/linux/kvm_host.h|1956| <<mmu_notifier_retry_hva>> if (kvm->mmu_notifier_seq != mmu_seq)
+	 *   - virt/kvm/kvm_main.c|745| <<kvm_dec_notifier_count>> kvm->mmu_notifier_seq++;
+	 *   - virt/kvm/pfncache.c|151| <<hva_to_pfn_retry>> mmu_seq = kvm->mmu_notifier_seq;
+	 */
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
+	/*
+	 * 在以下使用kvm->mmu_notifier_range_start:
+	 *   -  include/linux/kvm_host.h|2002| <<mmu_notifier_retry_hva>> hva >= kvm->mmu_notifier_range_start &&
+	 *   - virt/kvm/kvm_main.c|717| <<kvm_inc_notifier_count>> kvm->mmu_notifier_range_start = start;
+	 *   - virt/kvm/kvm_main.c|729| <<kvm_inc_notifier_count>> kvm->mmu_notifier_range_start =
+	 *   - virt/kvm/kvm_main.c|730| <<kvm_inc_notifier_count>> min(kvm->mmu_notifier_range_start, start);
+	 */
 	unsigned long mmu_notifier_range_start;
 	unsigned long mmu_notifier_range_end;
 #endif
@@ -806,6 +934,11 @@ static inline void kvm_vm_dead(struct kvm *kvm)
 	kvm_make_all_cpus_request(kvm, KVM_REQ_VM_DEAD);
 }
 
+/*
+ * called by:
+ *   - include/linux/kvm_host.h|889| <<KVM_BUG>> kvm_vm_bugged(kvm); \
+ *   - include/linux/kvm_host.h|898| <<KVM_BUG_ON>> kvm_vm_bugged(kvm); \
+ */
 static inline void kvm_vm_bugged(struct kvm *kvm)
 {
 	kvm->vm_bugged = true;
@@ -1899,6 +2032,11 @@ static inline int mmu_notifier_retry(struct kvm *kvm, unsigned long mmu_seq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4581| <<is_page_fault_stale>> mmu_notifier_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
+ *   - virt/kvm/pfncache.c|160| <<hva_to_pfn_retry>> retry = mmu_notifier_retry_hva(kvm, mmu_seq, uhva);
+ */
 static inline int mmu_notifier_retry_hva(struct kvm *kvm,
 					 unsigned long mmu_seq,
 					 unsigned long hva)
@@ -2003,6 +2141,10 @@ static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> u64 requests;
+	 */
 	return test_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
diff --git a/include/linux/virtio_config.h b/include/linux/virtio_config.h
index dafdc7f48c01..531cd28c0b18 100644
--- a/include/linux/virtio_config.h
+++ b/include/linux/virtio_config.h
@@ -257,6 +257,15 @@ const char *virtio_bus_name(struct virtio_device *vdev)
  * due to config support, irq type and sharing.
  *
  */
+/*
+ * called by:
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|137| <<virtcrypto_clean_affinity>> virtqueue_set_affinity(vi->data_vq[i].vq, NULL);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|164| <<virtcrypto_set_affinity>> virtqueue_set_affinity(vcrypto->data_vq[i].vq, cpumask_of(cpu));
+ *   - drivers/net/virtio_net.c|2077| <<virtnet_clean_affinity>> virtqueue_set_affinity(vi->rq[i].vq, NULL);
+ *   - drivers/net/virtio_net.c|2078| <<virtnet_clean_affinity>> virtqueue_set_affinity(vi->sq[i].vq, NULL);
+ *   - drivers/net/virtio_net.c|2114| <<virtnet_set_affinity>> virtqueue_set_affinity(vi->rq[i].vq, mask);
+ *   - drivers/net/virtio_net.c|2115| <<virtnet_set_affinity>> virtqueue_set_affinity(vi->sq[i].vq, mask);
+ */
 static inline
 int virtqueue_set_affinity(struct virtqueue *vq, const struct cpumask *cpu_mask)
 {
diff --git a/include/linux/virtio_net.h b/include/linux/virtio_net.h
index a960de68ac69..d3a01ea878e5 100644
--- a/include/linux/virtio_net.h
+++ b/include/linux/virtio_net.h
@@ -43,6 +43,17 @@ static inline int virtio_net_hdr_set_proto(struct sk_buff *skb,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/vector_transports.c|212| <<raw_verify_header>> virtio_net_hdr_to_skb(skb, vheader, virtio_legacy_is_little_endian());
+ *   - drivers/net/tap.c|707| <<tap_get_user>> err = virtio_net_hdr_to_skb(skb, &vnet_hdr,
+ *   - drivers/net/tap.c|1159| <<tap_get_user_xdp>> err = virtio_net_hdr_to_skb(skb, gso, tap_is_little_endian(q));
+ *   - drivers/net/tun.c|1835| <<tun_get_user>> if (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {
+ *   - drivers/net/tun.c|2445| <<tun_xdp_one>> if (virtio_net_hdr_to_skb(skb, gso, tun_is_little_endian(tun))) {
+ *   - drivers/net/virtio_net.c|1164| <<receive_buf>> if (virtio_net_hdr_to_skb(skb, &hdr->hdr,
+ *   - net/packet/af_packet.c|2846| <<tpacket_snd>> if (virtio_net_hdr_to_skb(skb, vnet_hdr, vio_le())) {
+ *   - net/packet/af_packet.c|3048| <<packet_snd>> err = virtio_net_hdr_to_skb(skb, &vnet_hdr, vio_le());
+ */
 static inline int virtio_net_hdr_to_skb(struct sk_buff *skb,
 					const struct virtio_net_hdr *hdr,
 					bool little_endian)
diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
index bfe38869498d..f72e91dfb35c 100644
--- a/include/linux/vmstat.h
+++ b/include/linux/vmstat.h
@@ -559,6 +559,16 @@ static inline void mod_lruvec_state(struct lruvec *lruvec,
 void __mod_lruvec_page_state(struct page *page,
 			     enum node_stat_item idx, int val);
 
+/*
+ * called by:
+ *   - include/linux/vmstat.h|633| <<inc_lruvec_page_state>> mod_lruvec_page_state(page, idx, 1);
+ *   - include/linux/vmstat.h|639| <<dec_lruvec_page_state>> mod_lruvec_page_state(page, idx, -1);
+ *   - include/linux/vmstat.h|645| <<lruvec_stat_mod_folio>> mod_lruvec_page_state(&folio->page, idx, val);
+ *   - kernel/fork.c|389| <<account_kernel_stack>> mod_lruvec_page_state(vm->pages[i], NR_KERNEL_STACK_KB,
+ *   - mm/slab_common.c|947| <<kmalloc_order>> mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,
+ *   - mm/slub.c|3546| <<free_large_kmalloc>> mod_lruvec_page_state(folio_page(folio, 0), NR_SLAB_UNRECLAIMABLE_B,
+ *   - mm/slub.c|4441| <<kmalloc_large_node>> mod_lruvec_page_state(page, NR_SLAB_UNRECLAIMABLE_B,
+ */
 static inline void mod_lruvec_page_state(struct page *page,
 					 enum node_stat_item idx, int val)
 {
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 507ee1f2aa96..dec679870c6d 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -502,6 +502,14 @@ struct kvm_run {
 	 * struct kvm_sync_regs is architecture specific, as well as the
 	 * bits for kvm_valid_regs and kvm_dirty_regs
 	 */
+	/*
+	 * x86在以下使用kvm_run->kvm_valid_regs:
+	 *   - arch/x86/kvm/x86.c|10847| <<kvm_arch_vcpu_ioctl_run>> if ((kvm_run->kvm_valid_regs & ~KVM_SYNC_X86_VALID_FIELDS) ||
+	 *   - arch/x86/kvm/x86.c|10892| <<kvm_arch_vcpu_ioctl_run>> if (kvm_run->kvm_valid_regs)
+	 *   - arch/x86/kvm/x86.c|11471| <<store_regs>> if (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_REGS)
+	 *   - arch/x86/kvm/x86.c|11474| <<store_regs>> if (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_SREGS)
+	 *   - arch/x86/kvm/x86.c|11477| <<store_regs>> if (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_EVENTS)
+	 */
 	__u64 kvm_valid_regs;
 	__u64 kvm_dirty_regs;
 	union {
diff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c
index 5de18448016c..baa0624b9af6 100644
--- a/kernel/cgroup/cpuset.c
+++ b/kernel/cgroup/cpuset.c
@@ -375,6 +375,14 @@ static struct workqueue_struct *cpuset_migrate_mm_wq;
  * CPU / memory hotplug is handled asynchronously.
  */
 static void cpuset_hotplug_workfn(struct work_struct *work);
+/*
+ * 在以下使用cpuset_hotplug_work():
+ *   - kernel/cgroup/cpuset.c|378| <<global>> static DECLARE_WORK(cpuset_hotplug_work, cpuset_hotplug_workfn);
+ *   - kernel/cgroup/cpuset.c|2478| <<cpuset_write_resmask>> flush_work(&cpuset_hotplug_work);
+ *   - kernel/cgroup/cpuset.c|3361| <<cpuset_update_active_cpus>> schedule_work(&cpuset_hotplug_work);
+ *   - kernel/cgroup/cpuset.c|3366| <<cpuset_wait_for_hotplug>> flush_work(&cpuset_hotplug_work);
+ *   - kernel/cgroup/cpuset.c|3377| <<cpuset_track_online_nodes>> schedule_work(&cpuset_hotplug_work);
+ */
 static DECLARE_WORK(cpuset_hotplug_work, cpuset_hotplug_workfn);
 
 static DECLARE_WAIT_QUEUE_HEAD(cpuset_attach_wq);
@@ -3253,6 +3261,14 @@ static void cpuset_hotplug_update_tasks(struct cpuset *cs, struct tmpmasks *tmp)
  * Note that CPU offlining during suspend is ignored.  We don't modify
  * cpusets across suspend/resume cycles at all.
  */
+/*
+ * 在以下使用cpuset_hotplug_work():
+ *   - kernel/cgroup/cpuset.c|378| <<global>> static DECLARE_WORK(cpuset_hotplug_work, cpuset_hotplug_workfn);
+ *   - kernel/cgroup/cpuset.c|2478| <<cpuset_write_resmask>> flush_work(&cpuset_hotplug_work);
+ *   - kernel/cgroup/cpuset.c|3361| <<cpuset_update_active_cpus>> schedule_work(&cpuset_hotplug_work);
+ *   - kernel/cgroup/cpuset.c|3366| <<cpuset_wait_for_hotplug>> flush_work(&cpuset_hotplug_work);
+ *   - kernel/cgroup/cpuset.c|3377| <<cpuset_track_online_nodes>> schedule_work(&cpuset_hotplug_work);
+ */
 static void cpuset_hotplug_workfn(struct work_struct *work)
 {
 	static cpumask_t new_cpus;
@@ -3351,6 +3367,19 @@ static void cpuset_hotplug_workfn(struct work_struct *work)
 	free_cpumasks(NULL, ptmp);
 }
 
+/*
+ * [0] cpuset_update_active_cpus
+ * [0] sched_cpu_deactivate
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - kernel/sched/core.c|9019| <<cpuset_cpu_active>> cpuset_update_active_cpus();
+ *   - kernel/sched/core.c|9027| <<cpuset_cpu_inactive>> cpuset_update_active_cpus();
+ */
 void cpuset_update_active_cpus(void)
 {
 	/*
@@ -3361,8 +3390,31 @@ void cpuset_update_active_cpus(void)
 	schedule_work(&cpuset_hotplug_work);
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|941| <<cpu_up_down_serialize_trainwrecks>> cpuset_wait_for_hotplug();
+ *   - kernel/power/process.c|207| <<thaw_processes>> cpuset_wait_for_hotplug();
+ */
 void cpuset_wait_for_hotplug(void)
 {
+	/*
+	 * flush_work - wait for a work to finish executing the last queueing instance
+	 * @work: the work to flush
+	 *
+	 * Wait until @work has finished execution.  @work is guaranteed to be idle
+	 * on return if it hasn't been requeued since flush started.
+	 *
+	 * Return:
+	 * %true if flush_work() waited for the work to finish execution,
+	 * %false if it was already idle.
+	 *
+	 * 在以下使用cpuset_hotplug_work():
+	 *   - kernel/cgroup/cpuset.c|378| <<global>> static DECLARE_WORK(cpuset_hotplug_work, cpuset_hotplug_workfn);
+	 *   - kernel/cgroup/cpuset.c|2478| <<cpuset_write_resmask>> flush_work(&cpuset_hotplug_work);
+	 *   - kernel/cgroup/cpuset.c|3361| <<cpuset_update_active_cpus>> schedule_work(&cpuset_hotplug_work);
+	 *   - kernel/cgroup/cpuset.c|3366| <<cpuset_wait_for_hotplug>> flush_work(&cpuset_hotplug_work);
+	 *   - kernel/cgroup/cpuset.c|3377| <<cpuset_track_online_nodes>> schedule_work(&cpuset_hotplug_work);
+	 */
 	flush_work(&cpuset_hotplug_work);
 }
 
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 407a2568f35e..df970dfc5cb7 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1117,6 +1117,11 @@ static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 }
 
 /* Requires cpu_add_remove_lock to be held */
+/*
+ * called by:
+ *   - kernel/cpu.c|1190| <<cpu_down_maps_locked>> return _cpu_down(cpu, 0, target);
+ *   - kernel/cpu.c|1521| <<freeze_secondary_cpus>> error = _cpu_down(cpu, 1, CPUHP_OFFLINE);
+ */
 static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
 			   enum cpuhp_state target)
 {
@@ -1190,6 +1195,11 @@ static int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)
 	return _cpu_down(cpu, 0, target);
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|1215| <<cpu_device_down>> return cpu_down(dev->id, CPUHP_OFFLINE);
+ *   - kernel/cpu.c|2308| <<target_store>> ret = cpu_down(dev->id, target);
+ */
 static int cpu_down(unsigned int cpu, enum cpuhp_state target)
 {
 	int err;
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 6db1c475ec82..fd6c7c019c26 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -71,6 +71,20 @@
 
 enum swiotlb_force swiotlb_force;
 
+/*
+ * 在以下使用io_tlb_default_mem:
+ *   - drivers/base/core.c|2885| <<device_initialize>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ *   - drivers/xen/swiotlb-xen.c|164| <<xen_swiotlb_init>> if (io_tlb_default_mem.nslabs) {
+ *   - drivers/xen/swiotlb-xen.c|548| <<xen_swiotlb_dma_supported>> return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
+ *   - kernel/dma/swiotlb.c|107| <<swiotlb_max_segment>> return io_tlb_default_mem.nslabs ? max_segment : 0;
+ *   - kernel/dma/swiotlb.c|140| <<swiotlb_print_info>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|197| <<swiotlb_update_mem_attributes>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|250| <<swiotlb_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|352| <<swiotlb_late_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|377| <<swiotlb_exit>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|780| <<swiotlb_create_default_debugfs>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|882| <<rmem_swiotlb_device_release>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ */
 struct io_tlb_mem io_tlb_default_mem;
 
 phys_addr_t swiotlb_unencrypted_base;
@@ -81,6 +95,15 @@ phys_addr_t swiotlb_unencrypted_base;
  */
 static unsigned int max_segment;
 
+/*
+ * 在以下使用default_nslabs:
+ *   - kernel/dma/swiotlb.c|105| <<setup_io_tlb_npages>> default_nslabs =
+ *   - kernel/dma/swiotlb.c|135| <<swiotlb_size_or_default>> return default_nslabs << IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|145| <<swiotlb_adjust_size>> if (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)
+ *   - kernel/dma/swiotlb.c|148| <<swiotlb_adjust_size>> default_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|333| <<swiotlb_init>> size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|343| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 static unsigned long default_nslabs = IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT;
 
 static int __init
@@ -121,6 +144,10 @@ unsigned long swiotlb_size_or_default(void)
 	return default_nslabs << IO_TLB_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/mem_encrypt_amd.c|227| <<sev_setup_arch>> swiotlb_adjust_size(size);
+ */
 void __init swiotlb_adjust_size(unsigned long size)
 {
 	/*
@@ -135,6 +162,13 @@ void __init swiotlb_adjust_size(unsigned long size)
 	pr_info("SWIOTLB bounce buffer size adjusted to %luMB", size >> 20);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|23| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-swiotlb.c|75| <<pci_swiotlb_late_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|269| <<swiotlb_init_with_tbl>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> swiotlb_print_info();
+ */
 void swiotlb_print_info(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -211,6 +245,12 @@ void __init swiotlb_update_mem_attributes(void)
 	memset(mem->vaddr, 0, bytes);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+ *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+ *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ */
 static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 				    unsigned long nslabs, bool late_alloc)
 {
@@ -245,6 +285,13 @@ static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|248| <<plat_swiotlb_setup>> if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
+ *   - arch/powerpc/platforms/pseries/svm.c|56| <<svm_swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, false))
+ *   - drivers/xen/swiotlb-xen.c|255| <<xen_swiotlb_init_early>> if (swiotlb_init_with_tbl(start, nslabs, true))
+ *   - kernel/dma/swiotlb.c|291| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -263,6 +310,12 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 		      __func__, alloc_size, PAGE_SIZE);
 
+	/*
+	 * called by:
+	 *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+	 *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+	 *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+	 */
 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
 
 	if (verbose)
@@ -275,6 +328,18 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * called by:
+ *   - arch/arm/mm/init.c|317| <<mem_init>> swiotlb_init(1);
+ *   - arch/arm64/mm/init.c|378| <<mem_init>> swiotlb_init(1); 
+ *   - arch/ia64/mm/init.c|441| <<mem_init>> swiotlb_init(1);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/powerpc/mm/mem.c|254| <<mem_init>> swiotlb_init(0);
+ *   - arch/riscv/mm/init.c|124| <<mem_init>> swiotlb_init(1);
+ *   - arch/s390/mm/init.c|189| <<pv_init>> swiotlb_init(1);
+ *   - arch/x86/kernel/pci-swiotlb.c|64| <<pci_swiotlb_init>> swiotlb_init(0);
+ */
 void  __init
 swiotlb_init(int verbose)
 {
@@ -303,6 +368,10 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/x86/pci/sta2x11-fixup.c|60| <<sta2x11_new_instance>> if (swiotlb_late_init_with_default_size(size))
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -758,6 +827,13 @@ size_t swiotlb_max_mapping_size(struct device *dev)
 	return ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE;
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|45| <<i915_gem_object_get_pages_internal>> if (is_swiotlb_active(obj->base.dev->dev)) {
+ *   - drivers/gpu/drm/nouveau/nouveau_ttm.c|279| <<nouveau_ttm_init>> need_swiotlb = is_swiotlb_active(dev->dev);
+ *   - drivers/pci/xen-pcifront.c|684| <<pcifront_connect_and_init_dma>> if (!err && !is_swiotlb_active(&pdev->xdev->dev)) {
+ *   - kernel/dma/direct.c|580| <<dma_direct_max_mapping_size>> if (is_swiotlb_active(dev) &&
+ */
 bool is_swiotlb_active(struct device *dev)
 {
 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
@@ -769,6 +845,11 @@ EXPORT_SYMBOL_GPL(is_swiotlb_active);
 #ifdef CONFIG_DEBUG_FS
 static struct dentry *debugfs_dir;
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|785| <<swiotlb_create_default_debugfs>> swiotlb_create_debugfs_files(mem);
+ *   - kernel/dma/swiotlb.c|802| <<rmem_swiotlb_debugfs_init>> swiotlb_create_debugfs_files(mem);
+ */
 static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem)
 {
 	debugfs_create_ulong("io_tlb_nslabs", 0400, mem->debugfs, &mem->nslabs);
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 6859229497b1..b0701ae8f9e2 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -2729,6 +2729,12 @@ static void task_ctx_sched_out(struct perf_cpu_context *cpuctx,
 	ctx_sched_out(ctx, cpuctx, event_type);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2791| <<ctx_resched>> perf_event_sched_in(cpuctx, task_ctx, current);
+ *   - kernel/events/core.c|3959| <<perf_event_context_sched_in>> perf_event_sched_in(cpuctx, ctx, task);
+ *   - kernel/events/core.c|4270| <<perf_rotate_context>> perf_event_sched_in(cpuctx, task_ctx, current);
+ */
 static void perf_event_sched_in(struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
 				struct task_struct *task)
@@ -3917,6 +3923,10 @@ static void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,
 	ctx_sched_in(ctx, cpuctx, event_type, task);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4002| <<__perf_event_task_sched_in>> perf_event_context_sched_in(ctx, task);
+ */
 static void perf_event_context_sched_in(struct perf_event_context *ctx,
 					struct task_struct *task)
 {
@@ -11347,6 +11357,9 @@ static int perf_try_init_event(struct pmu *pmu, struct perf_event *event)
 	}
 
 	event->pmu = pmu;
+	/*
+	 * x86_pmu_event_init()
+	 */
 	ret = pmu->event_init(event);
 
 	if (ctx)
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index f23ffd30385b..dbc4f3f9e42c 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -654,6 +654,9 @@ int irq_set_vcpu_affinity(unsigned int irq, void *vcpu_info)
 	data = irq_desc_get_irq_data(desc);
 	do {
 		chip = irq_data_get_irq_chip(data);
+		/*
+		 * intel_ir_set_vcpu_affinity()
+		 */
 		if (chip && chip->irq_set_vcpu_affinity)
 			break;
 #ifdef CONFIG_IRQ_DOMAIN_HIERARCHY
diff --git a/kernel/power/process.c b/kernel/power/process.c
index 11b570fcf049..f82254bd69ef 100644
--- a/kernel/power/process.c
+++ b/kernel/power/process.c
@@ -186,6 +186,21 @@ int freeze_kernel_threads(void)
 	return error;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/manage.c|162| <<do_suspend>> thaw_processes();
+ *   - kernel/kexec_core.c|1198| <<kernel_kexec>> thaw_processes();
+ *   - kernel/power/hibernate.c|786| <<hibernate>> thaw_processes();
+ *   - kernel/power/hibernate.c|882| <<hibernate_quiet_exec>> thaw_processes();
+ *   - kernel/power/hibernate.c|1003| <<software_resume>> thaw_processes();
+ *   - kernel/power/hibernate.c|1008| <<software_resume>> thaw_processes();
+ *   - kernel/power/power.h|270| <<suspend_freeze_processes>> thaw_processes();
+ *   - kernel/power/power.h|277| <<suspend_thaw_processes>> thaw_processes();
+ *   - kernel/power/process.c|158| <<freeze_processes>> thaw_processes();
+ *   - kernel/power/user.c|118| <<snapshot_release>> thaw_processes();
+ *   - kernel/power/user.c|274| <<snapshot_ioctl>> thaw_processes();
+ *   - kernel/power/user.c|286| <<snapshot_ioctl>> thaw_processes();
+ */
 void thaw_processes(void)
 {
 	struct task_struct *g, *p;
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 6fcdee7e87a5..f67507ca7a39 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -415,6 +415,9 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
 	if (suspend_test(TEST_PLATFORM))
 		goto Platform_wake;
 
+	/*
+	 * echo "freeze" > /sys/power/state到这里就结束了
+	 */
 	if (state == PM_SUSPEND_TO_IDLE) {
 		s2idle_loop();
 		goto Platform_wake;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 9745613d531c..7c2e94104469 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -9076,6 +9076,10 @@ int sched_cpu_activate(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * 在以下使用sched_cpu_deactivate():
+ *   - kernel/cpu.c|1810| <<global>> .teardown.single = sched_cpu_deactivate,
+ */
 int sched_cpu_deactivate(unsigned int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
diff --git a/kernel/sched/core_sched.c b/kernel/sched/core_sched.c
index c8746a9a7ada..90401e6ca690 100644
--- a/kernel/sched/core_sched.c
+++ b/kernel/sched/core_sched.c
@@ -120,6 +120,12 @@ void sched_core_free(struct task_struct *p)
 	sched_core_put_cookie(p->core_cookie);
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core_sched.c|204| <<sched_core_share_pid>> __sched_core_set(current, cookie);
+ *   - kernel/sched/core_sched.c|213| <<sched_core_share_pid>> __sched_core_set(task, cookie);
+ *   - kernel/sched/core_sched.c|228| <<sched_core_share_pid>> __sched_core_set(p, cookie);
+ */
 static void __sched_core_set(struct task_struct *p, unsigned long cookie)
 {
 	cookie = sched_core_get_cookie(cookie);
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index d17b0a5ce6ac..47f825864cb7 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -167,6 +167,10 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
  * set, and it returns with polling set.  If it ever stops polling, it
  * must clear the polling bit.
  */
+/*
+ * called by:
+ *   - kernel/sched/idle.c|306| <<do_idle>> cpuidle_idle_call();
+ */
 static void cpuidle_idle_call(void)
 {
 	struct cpuidle_device *dev = cpuidle_get_device();
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 1cf73807b450..18a713ad29ad 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -113,6 +113,17 @@ static u64 suspend_start;
 static void clocksource_watchdog_work(struct work_struct *work);
 static void clocksource_select(void);
 
+/*
+ * 在以下使用watchdog_list:
+ *   - kernel/time/clocksource.c|116| <<global>> static LIST_HEAD(watchdog_list);
+ *   - kernel/time/clocksource.c|196| <<clocksource_mark_unstable>> list_add(&cs->wd_list, &watchdog_list);
+ *   - kernel/time/clocksource.c|400| <<clocksource_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list) {
+ *   - kernel/time/clocksource.c|522| <<clocksource_start_watchdog>> if (watchdog_running || !watchdog || list_empty(&watchdog_list))
+ *   - kernel/time/clocksource.c|532| <<clocksource_stop_watchdog>> if (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))
+ *   - kernel/time/clocksource.c|542| <<clocksource_reset_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list)
+ *   - kernel/time/clocksource.c|557| <<clocksource_enqueue_watchdog>> list_add(&cs->wd_list, &watchdog_list);
+ *   - kernel/time/clocksource.c|628| <<__clocksource_watchdog_kthread>> list_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {
+ */
 static LIST_HEAD(watchdog_list);
 static struct clocksource *watchdog;
 static struct timer_list watchdog_timer;
@@ -378,6 +389,10 @@ void clocksource_verify_percpu(struct clocksource *cs)
 }
 EXPORT_SYMBOL_GPL(clocksource_verify_percpu);
 
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|520| <<clocksource_start_watchdog>> timer_setup(&watchdog_timer, clocksource_watchdog, 0);
+ */
 static void clocksource_watchdog(struct timer_list *unused)
 {
 	u64 csnow, wdnow, cslast, wdlast, delta;
@@ -393,8 +408,22 @@ static void clocksource_watchdog(struct timer_list *unused)
 
 	reset_pending = atomic_read(&watchdog_reset_pending);
 
+	/*
+	 * 在以下使用watchdog_list:
+	 *   - kernel/time/clocksource.c|116| <<global>> static LIST_HEAD(watchdog_list);
+	 *   - kernel/time/clocksource.c|196| <<clocksource_mark_unstable>> list_add(&cs->wd_list, &watchdog_list);
+	 *   - kernel/time/clocksource.c|400| <<clocksource_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list) {
+	 *   - kernel/time/clocksource.c|522| <<clocksource_start_watchdog>> if (watchdog_running || !watchdog || list_empty(&watchdog_list))
+	 *   - kernel/time/clocksource.c|532| <<clocksource_stop_watchdog>> if (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))
+	 *   - kernel/time/clocksource.c|542| <<clocksource_reset_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list)
+	 *   - kernel/time/clocksource.c|557| <<clocksource_enqueue_watchdog>> list_add(&cs->wd_list, &watchdog_list);
+	 *   - kernel/time/clocksource.c|628| <<__clocksource_watchdog_kthread>> list_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {
+	 */
 	list_for_each_entry(cs, &watchdog_list, wd_list) {
 
+		/*
+		 * struct clocksource *cs;
+		 */
 		/* Clocksource already marked unstable? */
 		if (cs->flags & CLOCK_SOURCE_UNSTABLE) {
 			if (finished_booting)
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index 17a283ce2b20..6613e8834bd4 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -54,6 +54,12 @@ static ktime_t last_jiffies_update;
 /*
  * Must be called with interrupts disabled !
  */
+/*
+ * called by:
+ *   - kernel/time/tick-sched.c|197| <<tick_sched_do_timer>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|618| <<tick_nohz_update_jiffies>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|946| <<tick_nohz_restart_sched_tick>> tick_do_update_jiffies64(now);
+ */
 static void tick_do_update_jiffies64(ktime_t now)
 {
 	unsigned long ticks = 1;
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index dcdcb85121e4..421d5138ee2e 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -186,6 +186,20 @@ static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
  * a read of the fast-timekeeper tkrs (which is protected by its own locking
  * and update logic).
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|253| <<timekeeping_get_delta>> now = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|287| <<timekeeping_get_delta>> cycle_now = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|317| <<tk_setup_internals>> tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|445| <<__ktime_get_fast_ns>> tk_clock_read(tkr),
+ *   - kernel/time/timekeeping.c|548| <<__ktime_get_real_fast>> clocksource_delta(tk_clock_read(tkr),
+ *   - kernel/time/timekeeping.c|638| <<halt_fast_timekeeper>> cycles_at_suspend = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|779| <<timekeeping_forward_now>> cycle_now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1050| <<ktime_get_snapshot>> now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1228| <<ktime_get_snapshot>> now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1785| <<timekeeping_resume>> cycle_now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|2144| <<timekeeping_advance>> offset = clocksource_delta(tk_clock_read(&tk->tkr_mono),
+ */
 static inline u64 tk_clock_read(const struct tk_read_base *tkr)
 {
 	struct clocksource *clock = READ_ONCE(tkr->clock);
@@ -1608,6 +1622,10 @@ static bool persistent_clock_exists;
 /*
  * timekeeping_init - Initializes the clocksource and common timekeeping values
  */
+/*
+ * called by:
+ *   - init/main.c|1035| <<start_kernel>> timekeeping_init();
+ */
 void __init timekeeping_init(void)
 {
 	struct timespec64 wall_time, boot_offset, wall_to_mono;
@@ -2205,6 +2223,12 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
  * update_wall_time - Uses the current clocksource to increment the wall time
  *
  */
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|97| <<tick_periodic>> update_wall_time();
+ *   - kernel/time/tick-legacy.c|33| <<legacy_timer_tick>> update_wall_time();
+ *   - kernel/time/tick-sched.c|151| <<tick_do_update_jiffies64>> update_wall_time();
+ */
 void update_wall_time(void)
 {
 	if (timekeeping_advance(TK_ADV_TICK))
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index 6dd5330f7a99..080d0eb48e73 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -1640,6 +1640,16 @@ static ssize_t iter_xarray_get_pages_alloc(struct iov_iter *i,
 	return actual;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|259| <<bio_map_user_iov>> bytes = iov_iter_get_pages_alloc(iter, &pages, LONG_MAX, &offs);
+ *   - fs/ceph/addr.c|274| <<ceph_netfs_issue_op>> err = iov_iter_get_pages_alloc(&iter, &pages, len, &page_off);
+ *   - fs/cifs/file.c|3022| <<cifs_write_from_iter>> result = iov_iter_get_pages_alloc(
+ *   - fs/cifs/file.c|3753| <<cifs_send_async_read>> result = iov_iter_get_pages_alloc(
+ *   - fs/nfs/direct.c|370| <<nfs_direct_read_schedule_iovec>> result = iov_iter_get_pages_alloc(iter, &pagevec,
+ *   - fs/nfs/direct.c|814| <<nfs_direct_write_schedule_iovec>> result = iov_iter_get_pages_alloc(iter, &pagevec,
+ *   - net/9p/trans_virtio.c|334| <<p9_get_mapped_pages>> n = iov_iter_get_pages_alloc(data, pages, count, offs);
+ */
 ssize_t iov_iter_get_pages_alloc(struct iov_iter *i,
 		   struct page ***pages, size_t maxsize,
 		   size_t *start)
diff --git a/mm/gup.c b/mm/gup.c
index 7bc1ba9ce440..76542286a36a 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -939,6 +939,10 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,
  * does not include FOLL_NOWAIT, the mmap_lock may be released.  If it
  * is, *@locked will be set to 0 and -EBUSY returned.
  */
+/*
+ * called by:
+ *   - mm/gup.c|1190| <<__get_user_pages>> ret = faultin_page(vma, start, &foll_flags, locked);
+ */
 static int faultin_page(struct vm_area_struct *vma,
 		unsigned long address, unsigned int *flags, int *locked)
 {
diff --git a/net/core/dev.c b/net/core/dev.c
index 1baab07820f6..cc30d7084804 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -3464,6 +3464,9 @@ static int xmit_one(struct sk_buff *skb, struct net_device *dev,
 	unsigned int len;
 	int rc;
 
+	/*
+	 * dev_nit_active - return true if any network interface taps are in use
+	 */
 	if (dev_nit_active(dev))
 		dev_queue_xmit_nit(skb, dev);
 
diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
index a7273af2d900..b8905e804ee6 100644
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@ -2222,6 +2222,11 @@ static int packet_rcv(struct sk_buff *skb, struct net_device *dev,
 	return 0;
 }
 
+/*
+ * 在以下使用tpacket_rcv():
+ *   - net/packet/af_packet.c|1279| <<__packet_rcv_has_room>> if (po->prot_hook.func != tpacket_rcv) {
+ *   - net/packet/af_packet.c|4483| <<packet_set_ring>> tpacket_rcv : packet_rcv;
+ */
 static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,
 		       struct packet_type *pt, struct net_device *orig_dev)
 {
diff --git a/sound/core/timer.c b/sound/core/timer.c
index b3214baa8919..1ef23157d408 100644
--- a/sound/core/timer.c
+++ b/sound/core/timer.c
@@ -922,6 +922,10 @@ void snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left)
 	use_work = !list_empty(&timer->sack_list_head);
 	spin_unlock_irqrestore(&timer->lock, flags);
 
+	/*
+	 * struct snd_timer * timer:
+	 * -> struct work_struct task_work;
+	 */
 	if (use_work)
 		queue_work(system_highpri_wq, &timer->task_work);
 }
diff --git a/tools/testing/selftests/kvm/kvm_page_table_test.c b/tools/testing/selftests/kvm/kvm_page_table_test.c
index ba1fdc3dcf4a..cdb88794e4fb 100644
--- a/tools/testing/selftests/kvm/kvm_page_table_test.c
+++ b/tools/testing/selftests/kvm/kvm_page_table_test.c
@@ -67,6 +67,11 @@ struct test_args {
  * Guest variables. Use addr_gva2hva() if these variables need
  * to be changed in host.
  */
+/*
+ * 在以下使用guest_test_stage:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+ */
 static enum test_stage guest_test_stage;
 
 /* Host variables */
@@ -90,12 +95,20 @@ static uint64_t guest_test_phys_mem;
  * Guest virtual memory offset of the testing memory slot.
  * Must not conflict with identity mapped test code.
  */
+/*
+ * 0xc0000000
+ */
 static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 
 static void guest_code(int vcpu_id)
 {
 	struct test_args *p = &test_args;
 	struct vcpu_args *vcpu_args = &p->vcpu_args[vcpu_id];
+	/*
+	 * 在以下使用guest_test_stage:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+	 */
 	enum test_stage *current_stage = &guest_test_stage;
 	uint64_t addr;
 	int i, j;
@@ -336,6 +349,13 @@ static struct kvm_vm *pre_init_before_test(enum vm_guest_mode mode, void *arg)
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|400| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|407| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|420| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|432| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ */
 static void vcpus_complete_new_stage(enum test_stage stage)
 {
 	int ret;
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index 8784013b747c..981fc1a5a829 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -11,6 +11,16 @@ enum vm_guest_mode vm_mode_default;
 
 struct guest_mode guest_modes[NUM_VM_MODES];
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|355| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|408| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|344| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|864| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|485| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|501| <<init_guest_modes>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|154| <<main>> guest_modes_append_default();
+ */
 void guest_modes_append_default(void)
 {
 #ifndef __aarch64__
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index d8cf851ab119..1c71854e485b 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -1064,6 +1064,12 @@ memslot2region(struct kvm_vm *vm, uint32_t memslot)
  * Sets the flags of the memory region specified by the value of slot,
  * to the values given by flags.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|104| <<toggle_dirty_logging>> vm_mem_region_set_flags(vm, slot, flags);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|421| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX,
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|434| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
+ */
 void vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)
 {
 	int ret;
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index 9bfe1d6f6529..475c21c3de57 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -134,6 +134,12 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4003| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|3610| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|10013| <<vcpu_enter_guest(处理KVM_REQ_APF_READY)>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 222ecc81d7df..50be47c94c8f 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -147,6 +147,20 @@ int kvm_dirty_ring_reset(struct kvm *kvm, struct kvm_dirty_ring *ring)
 	return count;
 }
 
+/*
+ * 一个callstack的例子
+ * [0] kvm_dirty_ring_push
+ * [0] mark_page_dirty_in_slot
+ * [0] kvm_steal_time_set_preempted
+ * [0] kvm_arch_vcpu_put
+ * [0] vcpu_put
+ * [0] vmx_free_vcpu
+ * [0] kvm_arch_vcpu_destroy
+ * [0] kvm_vcpu_destroy
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|3171| <<mark_page_dirty_in_slot>> kvm_dirty_ring_push(&vcpu->dirty_ring,
+ */
 void kvm_dirty_ring_push(struct kvm_dirty_ring *ring, u32 slot, u64 offset)
 {
 	struct kvm_dirty_gfn *entry;
@@ -175,6 +189,16 @@ struct page *kvm_dirty_ring_get_page(struct kvm_dirty_ring *ring, u32 offset)
 
 void kvm_dirty_ring_free(struct kvm_dirty_ring *ring)
 {
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	vfree(ring->dirty_gfns);
 	ring->dirty_gfns = NULL;
 }
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 59b1dd4a549e..66e9a3542a57 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -256,6 +256,9 @@ static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
 	write_seqcount_begin(&irqfd->irq_entry_sc);
 
 	e = entries;
+	/*
+	 * 让两个struct kvm_kernel_irq_routing_entry相等
+	 */
 	if (n_entries == 1)
 		irqfd->irq_entry = *e;
 	else
@@ -615,6 +618,10 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
  */
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|222| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 0afc016cc54d..e4a8d3a7d60d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -109,7 +109,24 @@ static atomic_t hardware_enable_failed;
 
 static struct kmem_cache *kvm_vcpu_cache;
 
+/*
+ * 在以下使用kvm_preempt_ops:
+ *   - virt/kvm/kvm_main.c|468| <<kvm_vcpu_init>> preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
+ *   - virt/kvm/kvm_main.c|6034| <<kvm_init>> kvm_preempt_ops.sched_in = kvm_sched_in;
+ *   - virt/kvm/kvm_main.c|6035| <<kvm_init>> kvm_preempt_ops.sched_out = kvm_sched_out;
+ */
 static __read_mostly struct preempt_ops kvm_preempt_ops;
+/*
+ * 在以下使用percpu kvm_running_vcpu:
+ *   - virt/kvm/kvm_main.c|113| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|201| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|213| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|3606| <<kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+ *   - virt/kvm/kvm_main.c|5779| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|5794| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5811| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|5823| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+ */
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
 struct dentry *kvm_debugfs_dir;
@@ -194,6 +211,25 @@ bool kvm_is_reserved_pfn(kvm_pfn_t pfn)
 /*
  * Switches to specified vcpu, until a matching vcpu_put()
  */
+/*
+ * x86在以下调用:
+ *   - arch/x86/kvm/vmx/nested.c|336| <<nested_vmx_free_vcpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|5312| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10556| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10684| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10724| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10806| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10817| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|10845| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11055| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11088| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11150| <<kvm_arch_vcpu_ioctl_translate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11171| <<kvm_arch_vcpu_ioctl_get_fpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11194| <<kvm_arch_vcpu_ioctl_set_fpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11338| <<kvm_arch_vcpu_create>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11368| <<kvm_arch_vcpu_postcreate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11814| <<kvm_unload_vcpu_mmu>> vcpu_load(vcpu);
+ */
 void vcpu_load(struct kvm_vcpu *vcpu)
 {
 	int cpu = get_cpu();
@@ -205,6 +241,25 @@ void vcpu_load(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(vcpu_load);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|338| <<nested_vmx_free_vcpu>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|6013| <<kvm_arch_vcpu_ioctl>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11060| <<kvm_arch_vcpu_ioctl_run>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11109| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11149| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11231| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11259| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11293| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11480| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11559| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11583| <<kvm_arch_vcpu_ioctl_translate>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11606| <<kvm_arch_vcpu_ioctl_get_fpu>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11630| <<kvm_arch_vcpu_ioctl_set_fpu>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11769| <<kvm_arch_vcpu_create>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|11797| <<kvm_arch_vcpu_postcreate>> vcpu_put(vcpu);
+ *   - arch/x86/kvm/x86.c|12247| <<kvm_unload_vcpu_mmu>> vcpu_put(vcpu);
+ */
 void vcpu_put(struct kvm_vcpu *vcpu)
 {
 	preempt_disable();
@@ -415,6 +470,10 @@ void *kvm_mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3949| <<kvm_vm_ioctl_create_vcpu>> kvm_vcpu_init(vcpu, kvm, id);
+ */
 static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 {
 	mutex_init(&vcpu->mutex);
@@ -435,6 +494,10 @@ static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 	vcpu->last_used_slot = NULL;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|460| <<kvm_destroy_vcpus>> kvm_vcpu_destroy(vcpu);
+ */
 static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_dirty_ring_free(&vcpu->dirty_ring);
@@ -451,6 +514,15 @@ static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|182| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/mips/kvm/mips.c|183| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/powerpc/kvm/powerpc.c|476| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/riscv/kvm/vm.c|49| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/s390/kvm/kvm-s390.c|2798| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/x86/kvm/x86.c|11672| <<kvm_free_vcpus>> kvm_destroy_vcpus(kvm);
+ */
 void kvm_destroy_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -716,6 +788,11 @@ static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6405| <<kvm_zap_gfn_range>> kvm_dec_notifier_count(kvm, gfn_start, gfn_end);
+ *   - virt/kvm/kvm_main.c|759| <<kvm_mmu_notifier_invalidate_range_end>> .on_lock = kvm_dec_notifier_count,
+ */
 void kvm_dec_notifier_count(struct kvm *kvm, unsigned long start,
 				   unsigned long end)
 {
@@ -820,6 +897,10 @@ static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * 在以下使用kvm_mmu_notifier_ops:
+ *   - virt/kvm/kvm_main.c|849| <<kvm_init_mmu_notifier>> kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
+ */
 static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.invalidate_range	= kvm_mmu_notifier_invalidate_range,
 	.invalidate_range_start	= kvm_mmu_notifier_invalidate_range_start,
@@ -947,6 +1028,10 @@ static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4847| <<kvm_dev_ioctl_create_vm>> if (kvm_create_vm_debugfs(kvm, r) < 0) {
+ */
 static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 {
 	static DEFINE_MUTEX(kvm_debugfs_lock);
@@ -1049,6 +1134,10 @@ int __weak kvm_arch_create_vm_debugfs(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4831| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type);
+ */
 static struct kvm *kvm_create_vm(unsigned long type)
 {
 	struct kvm *kvm = kvm_arch_alloc_vm();
@@ -1074,6 +1163,13 @@ static struct kvm *kvm_create_vm(unsigned long type)
 	INIT_LIST_HEAD(&kvm->gpc_list);
 	spin_lock_init(&kvm->gpc_lock);
 
+	/*
+	 * 在以下使用kvm->devices:
+	 *   - virt/kvm/kvm_main.c|1147| <<kvm_create_vm>> INIT_LIST_HEAD(&kvm->devices);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_destroy_devices>> list_for_each_entry_safe(dev, tmp, &kvm->devices, vm_node) {
+	 *   - virt/kvm/kvm_main.c|4513| <<kvm_ioctl_create_device>> list_add(&dev->vm_node, &kvm->devices);
+	 *   - virt/kvm/vfio.c|401| <<kvm_vfio_create>> list_for_each_entry(tmp, &dev->kvm->devices, vm_node)
+	 */
 	INIT_LIST_HEAD(&kvm->devices);
 
 	BUILD_BUG_ON(KVM_MEM_SLOTS_NUM > SHRT_MAX);
@@ -1083,6 +1179,17 @@ static struct kvm *kvm_create_vm(unsigned long type)
 	if (init_srcu_struct(&kvm->irq_srcu))
 		goto out_err_no_irq_srcu;
 
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|843| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|927| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1107| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1272| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1287| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1313| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1327| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_set(&kvm->users_count, 1);
 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
 		for (j = 0; j < 2; j++) {
@@ -1170,12 +1277,23 @@ static void kvm_destroy_devices(struct kvm *kvm)
 	 * has a reference to the struct kvm at this point and therefore
 	 * cannot access the devices list anyhow.
 	 */
+	/*
+	 * 在以下使用kvm->devices:
+	 *   - virt/kvm/kvm_main.c|1147| <<kvm_create_vm>> INIT_LIST_HEAD(&kvm->devices);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_destroy_devices>> list_for_each_entry_safe(dev, tmp, &kvm->devices, vm_node) {
+	 *   - virt/kvm/kvm_main.c|4513| <<kvm_ioctl_create_device>> list_add(&dev->vm_node, &kvm->devices);
+	 *   - virt/kvm/vfio.c|401| <<kvm_vfio_create>> list_for_each_entry(tmp, &dev->kvm->devices, vm_node)
+	 */
 	list_for_each_entry_safe(dev, tmp, &kvm->devices, vm_node) {
 		list_del(&dev->vm_node);
 		dev->ops->destroy(dev);
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1250| <<kvm_put_kvm>> kvm_destroy_vm(kvm);
+ */
 static void kvm_destroy_vm(struct kvm *kvm)
 {
 	int i;
@@ -1228,6 +1346,20 @@ static void kvm_destroy_vm(struct kvm *kvm)
 	mmdrop(mm);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1956| <<kvm_vm_ioctl_get_htab_fd>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1999| <<debugfs_htab_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1271| <<debugfs_radix_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|321| <<kvm_vm_ioctl_create_spapr_tce>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2673| <<debugfs_timings_open>> kvm_get_kvm(vcpu->kvm);
+ *   - arch/x86/kvm/svm/sev.c|2014| <<svm_vm_copy_asid_from>> kvm_get_kvm(source_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1941| <<kvmgt_guest_init>> kvm_get_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1096| <<vfio_ap_mdev_set_kvm>> kvm_get_kvm(kvm);
+ *   - virt/kvm/async_pf.c|196| <<kvm_setup_async_pf>> kvm_get_kvm(work->vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|3862| <<kvm_vm_ioctl_create_vcpu>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4346| <<kvm_ioctl_create_device>> kvm_get_kvm(kvm);
+ */
 void kvm_get_kvm(struct kvm *kvm)
 {
 	refcount_inc(&kvm->users_count);
@@ -1238,12 +1370,37 @@ EXPORT_SYMBOL_GPL(kvm_get_kvm);
  * Make sure the vm is not during destruction, which is a safe version of
  * kvm_get_kvm().  Return true if kvm referenced successfully, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|162| <<kvm_mmu_rmaps_stat_open>> if (!kvm_get_kvm_safe(kvm))
+ *   - virt/kvm/kvm_main.c|5336| <<kvm_debugfs_open>> if (!kvm_get_kvm_safe(stat_data->kvm))
+ */
 bool kvm_get_kvm_safe(struct kvm *kvm)
 {
 	return refcount_inc_not_zero(&kvm->users_count);
 }
 EXPORT_SYMBOL_GPL(kvm_get_kvm_safe);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1932| <<kvm_htab_release>> kvm_put_kvm(ctx->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|2011| <<debugfs_htab_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1283| <<debugfs_radix_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|267| <<kvm_spapr_tce_release>> kvm_put_kvm(stt->kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2684| <<debugfs_timings_release>> kvm_put_kvm(p->vcpu->kvm);
+ *   - arch/x86/kvm/debugfs.c|172| <<kvm_mmu_rmaps_stat_release>> kvm_put_kvm(kvm);
+ *   - arch/x86/kvm/svm/sev.c|2058| <<sev_vm_destroy>> kvm_put_kvm(owner_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1961| <<kvmgt_guest_exit>> kvm_put_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1167| <<vfio_ap_mdev_unset_kvm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/async_pf.c|91| <<async_pf_execute>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/async_pf.c|118| <<kvm_clear_async_pf_completion_queue>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|1273| <<kvm_vm_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|3668| <<kvm_vcpu_release>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|4177| <<kvm_device_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4781| <<kvm_dev_ioctl_create_vm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|5254| <<kvm_debugfs_open>> kvm_put_kvm(stat_data->kvm);
+ *   - virt/kvm/kvm_main.c|5267| <<kvm_debugfs_release>> kvm_put_kvm(stat_data->kvm);
+ */
 void kvm_put_kvm(struct kvm *kvm)
 {
 	if (refcount_dec_and_test(&kvm->users_count))
@@ -1494,6 +1651,10 @@ static void kvm_swap_active_memslots(struct kvm *kvm, int as_id)
 	slots->generation = gen;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1871| <<kvm_set_memslot>> r = kvm_prepare_memory_region(kvm, old, new, change);
+ */
 static int kvm_prepare_memory_region(struct kvm *kvm,
 				     const struct kvm_memory_slot *old,
 				     struct kvm_memory_slot *new,
@@ -1508,6 +1669,14 @@ static int kvm_prepare_memory_region(struct kvm *kvm,
 	 * new and KVM isn't using a ring buffer, allocate and initialize a
 	 * new bitmap.
 	 */
+	/*
+	 * enum kvm_mr_change {
+	 *     KVM_MR_CREATE,
+	 *     KVM_MR_DELETE,
+	 *     KVM_MR_MOVE,
+	 *     KVM_MR_FLAGS_ONLY,
+	 * };
+	 */
 	if (change != KVM_MR_DELETE) {
 		if (!(new->flags & KVM_MEM_LOG_DIRTY_PAGES))
 			new->dirty_bitmap = NULL;
@@ -1526,6 +1695,24 @@ static int kvm_prepare_memory_region(struct kvm *kvm,
 	r = kvm_arch_prepare_memory_region(kvm, old, new, change);
 
 	/* Free the bitmap on failure if it was allocated above. */
+	/*
+	 * 在以下设置分配系kvm_memory_slot->dirty_bitmap:
+	 *   - virt/kvm/kvm_main.c|948| <<kvm_destroy_dirty_bitmap>> kvfree(memslot->dirty_bitmap);
+	 *   - virt/kvm/kvm_main.c|949| <<kvm_destroy_dirty_bitmap>> memslot->dirty_bitmap = NULL;                                                                                                      
+	 *   - virt/kvm/kvm_main.c|1409| <<kvm_alloc_dirty_bitmap>> memslot->dirty_bitmap = kvzalloc(dirty_bytes, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|1637| <<kvm_prepare_memory_region>> new->dirty_bitmap = NULL;
+	 *   - virt/kvm/kvm_main.c|1639| <<kvm_prepare_memory_region>> new->dirty_bitmap = old->dirty_bitmap;
+	 *   - virt/kvm/kvm_main.c|1646| <<kvm_prepare_memory_region>> bitmap_set(new->dirty_bitmap, 0, new->npages);
+	 *   - virt/kvm/kvm_main.c|1730| <<kvm_copy_memslot>> dest->dirty_bitmap = src->dirty_bitmap;
+	 *   - virt/kvm/kvm_main.c|3353| <<mark_page_dirty_in_slot>> set_bit_le(rel_gfn, memslot->dirty_bitmap);
+	 *
+	 * 几个条件:
+	 *   - r
+	 *   - new
+	 *   - new->dirty_bitmap
+	 *   - old
+	 *   - !old->dirty_bitmap
+	 */
 	if (r && new && new->dirty_bitmap && old && !old->dirty_bitmap)
 		kvm_destroy_dirty_bitmap(new);
 
@@ -1698,6 +1885,11 @@ static void kvm_update_flags_memslot(struct kvm *kvm,
 	kvm_activate_memslot(kvm, old, new);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2037| <<__kvm_set_memory_region>> return kvm_set_memslot(kvm, old, NULL, KVM_MR_DELETE);
+ *   - virt/kvm/kvm_main.c|2082| <<__kvm_set_memory_region>> r = kvm_set_memslot(kvm, old, new, change);
+ */
 static int kvm_set_memslot(struct kvm *kvm,
 			   struct kvm_memory_slot *old,
 			   struct kvm_memory_slot *new,
@@ -2461,6 +2653,10 @@ static int kvm_try_get_pfn(kvm_pfn_t pfn)
 	return get_page_unless_zero(pfn_to_page(pfn));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2768| <<hva_to_pfn>> r = hva_to_pfn_remapped(vma, addr, write_fault, writable, &pfn);
+ */
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool write_fault,
 			       bool *writable, kvm_pfn_t *p_pfn)
@@ -2470,6 +2666,13 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 	spinlock_t *ptl;
 	int r;
 
+	/*
+	 * follow_pte - look up PTE at a user virtual address
+	 * @mm: the mm_struct of the target address space
+	 * @address: user virtual address
+	 * @ptepp: location to store found PTE
+	 * @ptlp: location to store the lock for the PTE
+	 */
 	r = follow_pte(vma->vm_mm, addr, &ptep, &ptl);
 	if (r) {
 		/*
@@ -2540,6 +2743,11 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2810| <<__gfn_to_pfn_memslot>> return hva_to_pfn(addr, atomic, async, write_fault,
+ *   - virt/kvm/pfncache.c|155| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(uhva, false, NULL, true, NULL);
+ */
 kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 		     bool write_fault, bool *writable)
 {
@@ -2588,6 +2796,17 @@ kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 	return pfn;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1183| <<user_mem_abort>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, NULL,
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|593| <<kvmppc_book3s_hv_page_fault>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, NULL,
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|847| <<kvmppc_book3s_instantiate_page>> pfn = __gfn_to_pfn_memslot(memslot, gfn, false, NULL,
+ *   - arch/x86/kvm/mmu/mmu.c|4530| <<kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, &async,
+ *   - arch/x86/kvm/mmu/mmu.c|4546| <<kvm_faultin_pfn>> fault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, NULL,
+ *   - virt/kvm/kvm_main.c|2818| <<gfn_to_pfn_prot>> return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, NULL,
+ *   - virt/kvm/kvm_main.c|2825| <<gfn_to_pfn_memslot>> return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL, NULL);
+ *   - virt/kvm/kvm_main.c|2831| <<gfn_to_pfn_memslot_atomic>> return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL, NULL);
+ */
 kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable, hva_t *hva)
@@ -2698,6 +2917,11 @@ struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(gfn_to_page);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2839| <<kvm_vcpu_unmap>> kvm_release_pfn(map->pfn, dirty);
+ *   - virt/kvm/pfncache.c|134| <<__release_gpc>> kvm_release_pfn(pfn, dirty);
+ */
 void kvm_release_pfn(kvm_pfn_t pfn, bool dirty)
 {
 	if (pfn == 0)
@@ -3044,14 +3268,49 @@ static int __kvm_gfn_to_hva_cache_init(struct kvm_memslots *slots,
 	else
 		ghc->memslot = NULL;
 
+	/*
+	 * struct gfn_to_hva_cache {
+	 *     u64 generation;
+	 *     gpa_t gpa;
+	 *     unsigned long hva;
+	 *     unsigned long len;
+	 *     struct kvm_memory_slot *memslot;
+	 * };
+	 */
 	ghc->gpa = gpa;
 	ghc->len = len;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2839| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/lapic.c|2937| <<kvm_lapic_set_pv_eoi>> ret = kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);
+ *   - arch/x86/kvm/vmx/nested.c|706| <<nested_cache_shadow_vmcs12>> kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,
+ *   - arch/x86/kvm/vmx/nested.c|725| <<nested_flush_cached_shadow_vmcs12>> kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,
+ *   - arch/x86/kvm/vmx/nested.c|2966| <<nested_vmx_check_vmcs_link_ptr>> CC(kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,
+ *   - arch/x86/kvm/vmx/nested.c|5346| <<handle_vmptrld>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, vmptr, VMCS12_SIZE)) {
+ *   - arch/x86/kvm/x86.c|2230| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|3248| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,
+ *   - arch/x86/kvm/x86.c|3361| <<record_steal_time>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, gfn, sizeof(*st)) ||
+ *   - arch/x86/kvm/xen.c|149| <<kvm_xen_update_runstate_guest>> kvm_gfn_to_hva_cache_init(v->kvm, ghc, ghc->gpa, ghc->len))
+ *   - arch/x86/kvm/xen.c|304| <<__kvm_xen_has_interrupt>> err = kvm_gfn_to_hva_cache_init(v->kvm, ghc, ghc->gpa, ghc->len);
+ *   - arch/x86/kvm/xen.c|471| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/xen.c|494| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/xen.c|521| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ */
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len)
 {
+	/*
+	 * struct gfn_to_hva_cache {
+	 *     u64 generation;
+	 *     gpa_t gpa;
+	 *     unsigned long hva;
+	 *     unsigned long len;
+	 *     struct kvm_memory_slot *memslot;
+	 * };
+	 */
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	return __kvm_gfn_to_hva_cache_init(slots, ghc, gpa, len);
 }
@@ -3152,6 +3411,21 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1270| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3272| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|188| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|359| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3428| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|4604| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|244| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|366| <<__kvm_xen_has_interrupt>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2984| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3122| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3221| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3230| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3176,6 +3450,22 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty_in_slot);
 
+/*
+ * called by:
+ *   - arch/mips/kvm/mmu.c|547| <<_kvm_mips_map_page_fast>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|661| <<kvm_mips_map_page>> mark_page_dirty(kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|200| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|128| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_hv.c|884| <<kvmppc_copy_guest>> mark_page_dirty(kvm, to >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_xive_native.c|909| <<kvmppc_xive_native_vcpu_eq_sync>> mark_page_dirty(vcpu->kvm, gpa_to_gfn(q->guest_qaddr));
+ *   - arch/riscv/kvm/mmu.c|682| <<kvm_riscv_stage2_map>> mark_page_dirty(kvm, gfn);
+ *   - arch/s390/kvm/interrupt.c|2808| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->ind_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/interrupt.c|2814| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->summary_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/kvm-s390.c|636| <<kvm_arch_sync_dirty_log>> mark_page_dirty(kvm, cur_gfn + i);
+ *   - arch/s390/kvm/vsie.c|658| <<unpin_guest_page>> mark_page_dirty(kvm, gpa_to_gfn(gpa));
+ *   - include/linux/kvm_host.h|1175| <<__kvm_put_guest>> mark_page_dirty(kvm, gfn); \
+ *   - virt/kvm/pfncache.c|123| <<__release_gpc>> mark_page_dirty(kvm, gpa);
+ */
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -3185,6 +3475,15 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1492| <<kvm_hv_set_msr>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|291| <<FNAME>> kvm_vcpu_mark_page_dirty(vcpu, table_gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3752| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3757| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/vmx.c|5809| <<vmx_flush_pml_buffer>> kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2799| <<kvm_vcpu_unmap>> kvm_vcpu_mark_page_dirty(vcpu, map->gfn);
+ */
 void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -3409,6 +3708,19 @@ void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_halt);
 
+/*
+ * called by:
+ *   -  arch/arm64/kvm/arch_timer.c|283| <<kvm_bg_timer_expire>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/arm64/kvm/psci.c|116| <<kvm_psci_vcpu_on>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|581| <<kvm_riscv_vcpu_power_on>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1350| <<kvm_s390_vcpu_wakeup>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/x86/kvm/svm/avic.c|93| <<avic_ga_log_notifier>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|3346| <<svm_complete_interrupt_delivery>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|285| <<pi_wakeup_handler>> kvm_vcpu_wake_up(&vmx->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4097| <<kvm_vcpu_trigger_posted_interrupt>> kvm_vcpu_wake_up(vcpu);
+ *   - virt/kvm/kvm_main.c|312| <<kvm_make_vcpu_request>> if (!(req & KVM_REQUEST_NO_WAKEUP) && kvm_vcpu_wake_up(vcpu))
+ *   - virt/kvm/kvm_main.c|3704| <<kvm_vcpu_kick>> if (kvm_vcpu_wake_up(vcpu))
+ */
 bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu)
 {
 	if (__kvm_vcpu_wake_up(vcpu)) {
@@ -3688,6 +4000,10 @@ static int create_vcpu_fd(struct kvm_vcpu *vcpu)
 	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3874| <<kvm_vm_ioctl_create_vcpu>> kvm_create_vcpu_debugfs(vcpu);
+ */
 static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
@@ -3708,6 +4024,10 @@ static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 /*
  * Creates some virtual cpus.  Good luck creating more than one.
  */
+/*
+ * called by(KVM_CREATE_VCPU):
+ *   - virt/kvm/kvm_main.c|4605| <<kvm_vm_ioctl(KVM_CREATE_VCPU)>> r = kvm_vm_ioctl_create_vcpu(kvm, arg);
+ */
 static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 {
 	int r;
@@ -3744,6 +4064,9 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 	}
 	vcpu->run = page_address(page);
 
+	/*
+	 * 设置vcpu->cpu = -1;
+	 */
 	kvm_vcpu_init(vcpu, kvm, id);
 
 	r = kvm_arch_vcpu_create(vcpu);
@@ -4001,6 +4324,19 @@ static long kvm_vcpu_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_SET_GUEST_DEBUG: {
+		/*
+		 * KVM_SET_GUEST_DEBUG的例子:
+		 *
+		 * struct kvm_guest_debug debug;
+		 *
+		 * memset(&debug, 0, sizeof(debug));
+		 * debug.control = KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP;
+		 * ioctl(vcpu->fd, KVM_SET_GUEST_DEBUG, &debug);
+		 *
+		 * That will intercept #BP and exit to userspace with KVM_EXIT_DEBUG.  Note, it's
+		 * userspace's responsibility to re-inject the #BP if userspace wants to forward the
+		 * #BP to the guest.
+		 */
 		struct kvm_guest_debug dbg;
 
 		r = -EFAULT;
@@ -4200,6 +4536,17 @@ static const struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #endif
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2780| <<kvm_vgic_register_its_device>> return kvm_register_device_ops(&kvm_arm_vgic_its_ops,
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|275| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v2_ops,
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|279| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v3_ops,
+ *   - arch/powerpc/kvm/book3s.c|1075| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xive_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/powerpc/kvm/book3s.c|1077| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xive_native_ops,
+ *   - arch/powerpc/kvm/book3s.c|1081| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xics_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/s390/kvm/kvm-s390.c|500| <<kvm_arch_init>> rc = kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
+ *   - virt/kvm/vfio.c|419| <<kvm_vfio_ops_init>> return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
+ */
 int kvm_register_device_ops(const struct kvm_device_ops *ops, u32 type)
 {
 	if (type >= ARRAY_SIZE(kvm_device_ops_table))
@@ -4252,6 +4599,13 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 		kfree(dev);
 		return ret;
 	}
+	/*
+	 * 在以下使用kvm->devices:
+	 *   - virt/kvm/kvm_main.c|1147| <<kvm_create_vm>> INIT_LIST_HEAD(&kvm->devices);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_destroy_devices>> list_for_each_entry_safe(dev, tmp, &kvm->devices, vm_node) {
+	 *   - virt/kvm/kvm_main.c|4513| <<kvm_ioctl_create_device>> list_add(&dev->vm_node, &kvm->devices);
+	 *   - virt/kvm/vfio.c|401| <<kvm_vfio_create>> list_for_each_entry(tmp, &dev->kvm->devices, vm_node)
+	 */
 	list_add(&dev->vm_node, &kvm->devices);
 	mutex_unlock(&kvm->lock);
 
@@ -4273,6 +4627,11 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4724| <<kvm_vm_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(kvm, arg);
+ *   - virt/kvm/kvm_main.c|4889| <<kvm_dev_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
+ */
 static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 {
 	switch (arg) {
@@ -4459,6 +4818,9 @@ static int kvm_vm_ioctl_get_stats_fd(struct kvm *kvm)
 	return fd;
 }
 
+/*
+ * struct file_operations kvm_vm_fops.unlocked_ioctl = kvm_vm_ioctl()
+ */
 static long kvm_vm_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -4482,6 +4844,16 @@ static long kvm_vm_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_SET_USER_MEMORY_REGION: {
+		/*
+		 * // for KVM_SET_USER_MEMORY_REGION
+		 * struct kvm_userspace_memory_region {
+		 *     __u32 slot;
+		 *     __u32 flags;        // KVM_MEM_LOG_DIRTY_PAGES or KVM_MEM_READONLY
+		 *     __u64 guest_phys_addr;
+		 *     __u64 memory_size; // bytes
+		 *     __u64 userspace_addr; // start of the userspace allocated memory
+		 * };
+		 */
 		struct kvm_userspace_memory_region kvm_userspace_mem;
 
 		r = -EFAULT;
@@ -4720,6 +5092,12 @@ static long kvm_vm_compat_ioctl(struct file *filp,
 }
 #endif
 
+/*
+ * 在以下使用kvm_vm_fops:
+ *   - virt/kvm/kvm_main.c|4817| <<file_is_kvm>> return file && file->f_op == &kvm_vm_fops;
+ *   - virt/kvm/kvm_main.c|4846| <<kvm_dev_ioctl_create_vm>> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
+ *   - virt/kvm/kvm_main.c|5836| <<kvm_init>> kvm_vm_fops.owner = module;
+ */
 static struct file_operations kvm_vm_fops = {
 	.release        = kvm_vm_release,
 	.unlocked_ioctl = kvm_vm_ioctl,
@@ -4733,6 +5111,10 @@ bool file_is_kvm(struct file *file)
 }
 EXPORT_SYMBOL_GPL(file_is_kvm);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4874| <<kvm_dev_ioctl(KVM_CREATE_VM)>> r = kvm_dev_ioctl_create_vm(arg);
+ */
 static int kvm_dev_ioctl_create_vm(unsigned long type)
 {
 	int r;
@@ -4828,12 +5210,25 @@ static struct file_operations kvm_chardev_ops = {
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * 在以下使用kvm_dev:
+ *   - virt/kvm/kvm_main.c|5539| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|5580| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ *   - virt/kvm/kvm_main.c|5839| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|5885| <<kvm_exit>> misc_deregister(&kvm_dev);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
 	&kvm_chardev_ops,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5091| <<kvm_starting_cpu>> hardware_enable_nolock(NULL);
+ *   - virt/kvm/kvm_main.c|5140| <<hardware_enable_all>> on_each_cpu(hardware_enable_nolock, NULL, 1);
+ *   - virt/kvm/kvm_main.c|5768| <<kvm_resume>> hardware_enable_nolock(NULL);
+ */
 static void hardware_enable_nolock(void *junk)
 {
 	int cpu = raw_smp_processor_id();
@@ -5437,6 +5832,11 @@ DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, vcpu_stat_clear,
 			"%llu\n");
 DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_readonly_fops, vcpu_stat_get, NULL, "%llu\n");
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1185| <<kvm_destroy_vm>> kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
+ *   - virt/kvm/kvm_main.c|4775| <<kvm_dev_ioctl_create_vm>> kvm_uevent_notify_change(KVM_EVENT_CREATE_VM, kvm);
+ */
 static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 {
 	struct kobj_uevent_env *env;
@@ -5544,6 +5944,10 @@ struct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)
 	return container_of(pn, struct kvm_vcpu, preempt_notifier);
 }
 
+/*
+ * 在以下使用kvm_sched_in():
+ *   - virt/kvm/kvm_main.c|6126| <<kvm_init>> kvm_preempt_ops.sched_in = kvm_sched_in;
+ */
 static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 {
 	struct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);
@@ -5583,6 +5987,17 @@ struct kvm_vcpu *kvm_get_running_vcpu(void)
 	struct kvm_vcpu *vcpu;
 
 	preempt_disable();
+	/*
+	 * 在以下使用percpu kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|113| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|201| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|213| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|3606| <<kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+	 *   - virt/kvm/kvm_main.c|5779| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|5794| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|5811| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|5823| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	vcpu = __this_cpu_read(kvm_running_vcpu);
 	preempt_enable();
 
@@ -5654,6 +6069,20 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2240| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1649| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1065| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/riscv/kvm/main.c|124| <<riscv_kvm_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5075| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4914| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8166| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ *
+ * 在intel下被vmx_init()调用
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
@@ -5846,6 +6275,10 @@ static int kvm_vm_worker_thread(void *context)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7039| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_lpage_recovery_worker, 0, "kvm-nx-lpage-recovery", &kvm->arch.nx_lpage_recovery_thread);
+ */
 int kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,
 				uintptr_t data, const char *name,
 				struct task_struct **thread_ptr)
diff --git a/virt/kvm/pfncache.c b/virt/kvm/pfncache.c
index ce878f4be4da..c0a6f004644d 100644
--- a/virt/kvm/pfncache.c
+++ b/virt/kvm/pfncache.c
@@ -22,6 +22,10 @@
 /*
  * MMU notifier 'invalidate_range_start' hook.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|724| <<kvm_mmu_notifier_invalidate_range_start>> gfn_to_pfn_cache_invalidate_start(kvm, range->start, range->end,
+ */
 void gfn_to_pfn_cache_invalidate_start(struct kvm *kvm, unsigned long start,
 				       unsigned long end, bool may_block)
 {
@@ -85,6 +89,10 @@ void gfn_to_pfn_cache_invalidate_start(struct kvm *kvm, unsigned long start,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|925| <<kvm_xen_set_evtchn_fast>> if (!kvm_gfn_to_pfn_cache_check(kvm, gpc, gpc->gpa, PAGE_SIZE))
+ */
 bool kvm_gfn_to_pfn_cache_check(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 				gpa_t gpa, unsigned long len)
 {
@@ -104,6 +112,11 @@ bool kvm_gfn_to_pfn_cache_check(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_check);
 
+/*
+ * called by:
+ *   - virt/kvm/pfncache.c|265| <<kvm_gfn_to_pfn_cache_refresh>> __release_gpc(kvm, old_pfn, old_khva, old_gpa, old_dirty);
+ *   - virt/kvm/pfncache.c|296| <<kvm_gfn_to_pfn_cache_unmap>> __release_gpc(kvm, old_pfn, old_khva, old_gpa, old_dirty);
+ */
 static void __release_gpc(struct kvm *kvm, kvm_pfn_t pfn, void *khva,
 			  gpa_t gpa, bool dirty)
 {
@@ -124,6 +137,10 @@ static void __release_gpc(struct kvm *kvm, kvm_pfn_t pfn, void *khva,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/pfncache.c|216| <<kvm_gfn_to_pfn_cache_refresh>> new_pfn = hva_to_pfn_retry(kvm, uhva);
+ */
 static kvm_pfn_t hva_to_pfn_retry(struct kvm *kvm, unsigned long uhva)
 {
 	unsigned long mmu_seq;
@@ -151,6 +168,11 @@ static kvm_pfn_t hva_to_pfn_retry(struct kvm *kvm, unsigned long uhva)
 	return new_pfn;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|1028| <<evtchn_set_fn>> rc = kvm_gfn_to_pfn_cache_refresh(kvm, gpc, gpc->gpa,
+ *   - virt/kvm/pfncache.c|322| <<kvm_gfn_to_pfn_cache_init>> return kvm_gfn_to_pfn_cache_refresh(kvm, gpc, gpa, len, dirty);
+ */
 int kvm_gfn_to_pfn_cache_refresh(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 				 gpa_t gpa, unsigned long len, bool dirty)
 {
@@ -268,6 +290,10 @@ int kvm_gfn_to_pfn_cache_refresh(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_refresh);
 
+/*
+ * called by:
+ *   - virt/kvm/pfncache.c|333| <<kvm_gfn_to_pfn_cache_destroy>> kvm_gfn_to_pfn_cache_unmap(kvm, gpc);
+ */
 void kvm_gfn_to_pfn_cache_unmap(struct kvm *kvm, struct gfn_to_pfn_cache *gpc)
 {
 	void *old_khva;
@@ -298,6 +324,10 @@ void kvm_gfn_to_pfn_cache_unmap(struct kvm *kvm, struct gfn_to_pfn_cache *gpc)
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_unmap);
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|42| <<kvm_xen_shared_info_init>> ret = kvm_gfn_to_pfn_cache_init(kvm, gpc, NULL, false, true,
+ */
 int kvm_gfn_to_pfn_cache_init(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 			      struct kvm_vcpu *vcpu, bool guest_uses_pa,
 			      bool kernel_map, gpa_t gpa, unsigned long len,
@@ -323,6 +353,11 @@ int kvm_gfn_to_pfn_cache_init(struct kvm *kvm, struct gfn_to_pfn_cache *gpc,
 }
 EXPORT_SYMBOL_GPL(kvm_gfn_to_pfn_cache_init);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/xen.c|37| <<kvm_xen_shared_info_init>> kvm_gfn_to_pfn_cache_destroy(kvm, gpc);
+ *   - arch/x86/kvm/xen.c|811| <<kvm_xen_destroy_vm>> kvm_gfn_to_pfn_cache_destroy(kvm, &kvm->arch.xen.shinfo_cache);
+ */
 void kvm_gfn_to_pfn_cache_destroy(struct kvm *kvm, struct gfn_to_pfn_cache *gpc)
 {
 	if (gpc->active) {
-- 
2.17.1

