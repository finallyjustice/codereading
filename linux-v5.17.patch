From 50d98946a966d6f76c1161f398dad6a89c9eecfc Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 20 Apr 2022 08:28:50 -0700
Subject: [PATCH 1/1] linux v5.17

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/vgic/vgic-debug.c              |   4 +
 arch/arm64/kvm/vgic/vgic-init.c               |   7 +
 arch/x86/include/asm/kvm_host.h               |  83 +++
 arch/x86/kernel/kvm.c                         |  35 +
 arch/x86/kernel/smpboot.c                     |  13 +
 arch/x86/kvm/lapic.c                          |   9 +
 arch/x86/kvm/mmu.h                            |  90 +++
 arch/x86/kvm/mmu/mmu.c                        | 641 ++++++++++++++++++
 arch/x86/kvm/mmu/mmu_audit.c                  |   8 +
 arch/x86/kvm/mmu/mmu_internal.h               |  72 ++
 arch/x86/kvm/mmu/paging_tmpl.h                |   6 +
 arch/x86/kvm/mmu/spte.c                       |  33 +
 arch/x86/kvm/mmu/spte.h                       |  47 ++
 arch/x86/kvm/mmu/tdp_iter.c                   | 109 +++
 arch/x86/kvm/mmu/tdp_iter.h                   |  39 ++
 arch/x86/kvm/mmu/tdp_mmu.c                    | 484 +++++++++++++
 arch/x86/kvm/mmu/tdp_mmu.h                    |  54 ++
 arch/x86/kvm/svm/sev.c                        |   4 +
 arch/x86/kvm/vmx/posted_intr.c                |  11 +
 arch/x86/kvm/vmx/vmx.c                        |   6 +
 arch/x86/kvm/x86.c                            |  98 +++
 drivers/acpi/acpi_processor.c                 |  11 +
 drivers/acpi/bus.c                            |   4 +
 drivers/acpi/osl.c                            |   5 +
 drivers/acpi/scan.c                           |  26 +
 drivers/cpuidle/cpuidle-haltpoll.c            |  29 +
 drivers/vfio/vfio_iommu_type1.c               |  10 +
 drivers/xen/swiotlb-xen.c                     |   5 +
 include/linux/kvm_dirty_ring.h                |  10 +
 include/linux/kvm_host.h                      |  11 +
 include/linux/virtio_net.h                    |  11 +
 kernel/dma/swiotlb.c                          |  81 +++
 kernel/power/suspend.c                        |   3 +
 kernel/time/clocksource.c                     |   4 +
 .../selftests/kvm/kvm_page_table_test.c       |  20 +
 tools/testing/selftests/kvm/lib/guest_modes.c |  10 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |   6 +
 virt/kvm/async_pf.c                           |   6 +
 virt/kvm/dirty_ring.c                         |  24 +
 virt/kvm/kvm_main.c                           | 171 +++++
 40 files changed, 2300 insertions(+)

diff --git a/arch/arm64/kvm/vgic/vgic-debug.c b/arch/arm64/kvm/vgic/vgic-debug.c
index f38c40a76251..0ad6b9fa9591 100644
--- a/arch/arm64/kvm/vgic/vgic-debug.c
+++ b/arch/arm64/kvm/vgic/vgic-debug.c
@@ -269,6 +269,10 @@ static const struct seq_operations vgic_debug_sops = {
 
 DEFINE_SEQ_ATTRIBUTE(vgic_debug);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|320| <<vgic_init>> vgic_debug_init(kvm);
+ */
 void vgic_debug_init(struct kvm *kvm)
 {
 	debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index fc00304fe7d8..a73042109880 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -252,6 +252,13 @@ static void kvm_vgic_vcpu_enable(struct kvm_vcpu *vcpu)
  * vgic_initialized() returns true when this function has succeeded.
  * Must be called with kvm->lock held!
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|413| <<vgic_lazy_init>> ret = vgic_init(kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|214| <<vgic_set_common_attr>> r = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|372| <<vgic_v2_attr_regs_access>> ret = vgic_init(dev->kvm);
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|309| <<vgic_v2_map_resources>> ret = vgic_init(kvm);
+ */
 int vgic_init(struct kvm *kvm)
 {
 	struct vgic_dist *dist = &kvm->arch.vgic;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ec9830d2aabf..ac16a3f6a648 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -129,10 +129,36 @@
 /* KVM Hugepage definitions for x86 */
 #define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
 #define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)
+/*
+ * KVM_HPAGE_GFN_SHIFT(1) = (1 - 1) * 9 =  0
+ * KVM_HPAGE_GFN_SHIFT(2) = (2 - 1) * 9 =  9
+ * KVM_HPAGE_GFN_SHIFT(3) = (3 - 1) * 9 = 18
+ * KVM_HPAGE_GFN_SHIFT(4) = (4 - 1) * 9 = 27
+ * KVM_HPAGE_GFN_SHIFT(5) = (5 - 1) * 9 = 36
+ */
 #define KVM_HPAGE_GFN_SHIFT(x)	(((x) - 1) * 9)
+/*
+ * KVM_HPAGE_SHIFT(1) = 12 +  0 = 12
+ * KVM_HPAGE_SHIFT(2) = 12 +  9 = 21
+ * KVM_HPAGE_SHIFT(3) = 12 + 18 = 30
+ * KVM_HPAGE_SHIFT(4) = 12 + 27 = 38
+ * KVM_HPAGE_SHIFT(5) = 12 + 36 = 48
+ */
 #define KVM_HPAGE_SHIFT(x)	(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))
+/*
+ * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+ * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+ * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+ * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+ * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+ */
 #define KVM_HPAGE_SIZE(x)	(1UL << KVM_HPAGE_SHIFT(x))
 #define KVM_HPAGE_MASK(x)	(~(KVM_HPAGE_SIZE(x) - 1))
+/*
+ * KVM_PAGES_PER_HPAGE(1) = KVM_HPAGE_SIZE(1) / 4096 = 4K / 4K = 1
+ * KVM_PAGES_PER_HPAGE(2) = KVM_HPAGE_SIZE(2) / 4096 = 2M / 4K = 512
+ * KVM_PAGES_PER_HPAGE(3) = KVM_HPAGE_SIZE(3) / 4096 = 1G / 4K = 262144
+ */
 #define KVM_PAGES_PER_HPAGE(x)	(KVM_HPAGE_SIZE(x) / PAGE_SIZE)
 
 #define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50
@@ -1047,14 +1073,38 @@ struct kvm_x86_msr_filter {
 #define APICV_INHIBIT_REASON_ABSENT	7
 
 struct kvm_arch {
+	/*
+	 * 在以下使用kvm_arch->n_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1720| <<kvm_mod_used_mmu_pages>> kvm->arch.n_used_mmu_pages += nr;
+	 *   - arch/x86/kvm/mmu/mmu.c|2589| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2591| <<kvm_mmu_available_pages>> kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|2627| <<kvm_mmu_change_mmu_pages>> if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2628| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2631| <<kvm_mmu_change_mmu_pages>> goal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|6232| <<mmu_shrink_scan>> if (!kvm->arch.n_used_mmu_pages &&
+	 */
 	unsigned long n_used_mmu_pages;
 	unsigned long n_requested_mmu_pages;
+	/*
+	 * 在以下使用kvm_arch->n_max_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|2727| <<kvm_mmu_available_pages>> if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
+	 *   - arch/x86/kvm/mmu/mmu.c|2728| <<kvm_mmu_available_pages>> return kvm->arch.n_max_mmu_pages -
+	 *   - arch/x86/kvm/mmu/mmu.c|2779| <<kvm_mmu_change_mmu_pages>> kvm->arch.n_max_mmu_pages = goal_nr_mmu_pages;
+	 *   - arch/x86/kvm/x86.c|5728| <<kvm_vm_ioctl_get_nr_mmu_pages>> return kvm->arch.n_max_mmu_pages;
+	 */
 	unsigned long n_max_mmu_pages;
 	unsigned int indirect_shadow_pages;
 	u8 mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
+	/*
+	 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|947| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6641| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+	 *   - arch/x86/kvm/x86.c|11639| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+	 */
 	struct list_head lpage_disallowed_mmu_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1085,6 +1135,15 @@ struct kvm_arch {
 	struct rw_semaphore apicv_update_lock;
 
 	bool apic_access_memslot_enabled;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9032| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9041| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9044| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9733| <<__kvm_request_apicv_update>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|9755| <<__kvm_request_apicv_update>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|9761| <<__kvm_request_apicv_update>> kvm->arch.apicv_inhibit_reasons = new;
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
@@ -1177,6 +1236,12 @@ struct kvm_arch {
 	 * true, TDP MMU handler functions will run for various MMU
 	 * operations.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_enabled:
+	 *   - arch/x86/kvm/mmu.h|328| <<is_tdp_mmu_enabled>> static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mmu_enabled; }
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|23| <<kvm_mmu_init_tdp_mmu>> kvm->arch.tdp_mmu_enabled = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|56| <<kvm_mmu_uninit_tdp_mmu>> if (!kvm->arch.tdp_mmu_enabled)
+	 */
 	bool tdp_mmu_enabled;
 
 	/*
@@ -1218,6 +1283,19 @@ struct kvm_arch {
 	 * It is acceptable, but not necessary, to acquire this lock when
 	 * the thread holds the MMU lock in write mode.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_pages_lock:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|32| <<kvm_mmu_init_tdp_mmu>> spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|134| <<kvm_tdp_mmu_put_root>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<kvm_tdp_mmu_put_root>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<for_each_tdp_mmu_root>> lockdep_is_held(&kvm->arch.tdp_mmu_pages_lock)) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|318| <<kvm_tdp_mmu_get_vcpu_root_hpa>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|378| <<tdp_mmu_link_page>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|382| <<tdp_mmu_link_page>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|402| <<tdp_mmu_unlink_page>> spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|411| <<tdp_mmu_unlink_page>> spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+	 */
 	spinlock_t tdp_mmu_pages_lock;
 #endif /* CONFIG_X86_64 */
 
@@ -1226,6 +1304,11 @@ struct kvm_arch {
 	 * is used as one input when determining whether certain memslot
 	 * related allocations are necessary.
 	 */
+	/*
+	 * 在以下使用kvm_arch->shadow_root_allocated:
+	 *   - arch/x86/kvm/mmu.h|345| <<kvm_shadow_root_allocated>> return smp_load_acquire(&kvm->arch.shadow_root_allocated);
+	 *   - arch/x86/kvm/mmu/mmu.c|3721| <<mmu_first_shadow_root_alloc>> smp_store_release(&kvm->arch.shadow_root_allocated, true);
+	 */
 	bool shadow_root_allocated;
 
 #if IS_ENABLED(CONFIG_HYPERV)
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d77481ecb0d5..69e4122189e3 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -704,6 +704,19 @@ static int kvm_cpu_down_prepare(unsigned int cpu)
 
 #endif
 
+/*
+ * [0] kvm_suspend
+ * [0] syscore_suspend
+ * [0] suspend_devices_and_enter
+ * [0] pm_suspend
+ * [0] state_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int kvm_suspend(void)
 {
 	kvm_guest_cpu_offline(false);
@@ -711,6 +724,19 @@ static int kvm_suspend(void)
 	return 0;
 }
 
+/*
+ * [0] kvm_resume
+ * [0] syscore_resume
+ * [0] suspend_devices_and_enter
+ * [0] pm_suspend
+ * [0] state_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_resume(void)
 {
 	kvm_cpu_online(raw_smp_processor_id());
@@ -1108,6 +1134,15 @@ static void kvm_enable_host_haltpoll(void *i)
 	wrmsrl(MSR_KVM_POLL_CONTROL, 1);
 }
 
+/*
+ * [0] arch_haltpoll_enable
+ * [0] haltpoll_cpu_online
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void arch_haltpoll_enable(unsigned int cpu)
 {
 	if (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL)) {
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 617012f4619f..a0944b927088 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -236,6 +236,11 @@ static void notrace start_secondary(void *unused)
 #endif
 	cpu_init_secondary();
 	rcu_cpu_starting(raw_smp_processor_id());
+	/*
+	 * 两处设置的地方:
+	 *   - arch/x86/kernel/x86_init.c|126| <<global>> .early_percpu_clock_init = x86_init_noop,
+	 *   - arch/x86/kernel/kvmclock.c|324| <<kvmclock_init>> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock;
+	 */
 	x86_cpuinit.early_percpu_clock_init();
 	smp_callin();
 
@@ -952,6 +957,10 @@ wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)
 }
 
 /* reduce the number of lines printed when booting a large cpu count system */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1100| <<do_boot_cpu>> announce_cpu(cpu, apicid);
+ */
 static void announce_cpu(int cpu, int apicid)
 {
 	static int current_node = NUMA_NO_NODE;
@@ -1079,6 +1088,10 @@ int common_cpu_up(unsigned int cpu, struct task_struct *idle)
  * Returns zero if CPU booted OK, else error code from
  * ->wakeup_secondary_cpu.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|1239| <<native_cpu_up>> err = do_boot_cpu(apicid, cpu, tidle, &cpu0_nmi_registered);
+ */
 static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		       int *cpu0_nmi_registered)
 {
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9322e6340a74..58e602889d5a 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1272,6 +1272,11 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2057| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/x86.c|2034| <<handle_fastpath_set_x2apic_icr_irqoff>> kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
+ */
 void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 {
 	struct kvm_lapic_irq irq;
@@ -2178,6 +2183,10 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
 /* emulate APIC access in a trap manner */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5316| <<handle_apic_write>> kvm_apic_write_nodecode(vcpu, offset);
+ */
 void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index e9fbb2c8bbe2..4f7f63826a42 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -82,6 +82,11 @@ void kvm_mmu_unload(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10048| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu);
+ *   - arch/x86/kvm/x86.c|12271| <<kvm_arch_async_page_ready>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu->root_hpa != INVALID_PAGE))
@@ -130,12 +135,28 @@ struct kvm_page_fault {
 
 	/* Derived from mmu and global state.  */
 	const bool is_tdp;
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+	 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	const bool nx_huge_page_workaround_enabled;
 
 	/*
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
 	 */
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	bool huge_page_disallowed;
 
 	/*
@@ -171,14 +192,32 @@ struct kvm_page_fault {
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
 
 extern int nx_huge_pages;
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|221| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ *   - arch/x86/kvm/mmu/spte.c|119| <<make_spte>> is_nx_huge_page_enabled()) {
+ */
 static inline bool is_nx_huge_page_enabled(void)
 {
 	return READ_ONCE(nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5511| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
+ *   - arch/x86/kvm/x86.c|12199| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch)
 {
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+	 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	struct kvm_page_fault fault = {
 		.addr = cr2_or_gpa,
 		.error_code = err,
@@ -248,6 +287,11 @@ static inline bool is_writable_pte(unsigned long pte)
  * Return zero if the access does not fault; return the page fault error code
  * if the access faults.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|459| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+ *   - arch/x86/kvm/x86.c|6952| <<vcpu_mmio_gva_to_gpa>> !permission_fault(vcpu, vcpu->arch.walk_mmu,
+ */
 static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				  unsigned pte_access, unsigned pte_pkey,
 				  unsigned pfec)
@@ -322,6 +366,23 @@ static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mm
 static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return false; }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|98| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1409| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1442| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1511| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1676| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1689| <<kvm_set_spte_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1750| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1763| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|3690| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|6062| <<__kvm_zap_rmaps>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|6137| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|6221| <<kvm_mmu_zap_collapsible_sptes>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|6260| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|11917| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !is_tdp_mmu_enabled(kvm) || kvm_shadow_root_allocated(kvm);
@@ -334,6 +395,10 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|119| <<kvm_mmu_rmaps_stat_show>> lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ */
 static inline unsigned long
 __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 		      int level)
@@ -348,14 +413,39 @@ kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|689| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|2958| <<mmu_set_spte>> kvm_update_page_stats(vcpu->kvm, level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|594| <<__handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
+	/*
+	 * struct kvm:
+	 * -> struct kvm_vm_stat stat;
+	 *    -> union {
+	 *           struct {
+	 *               atomic64_t pages_4k;
+	 *               atomic64_t pages_2m;
+	 *               atomic64_t pages_1g;
+	 *           };
+	 *           atomic64_t pages[KVM_NR_PAGE_SIZES];
+	 *       };
+	 */
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
 }
 
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4002| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|406| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|469| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|833| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u32 access,
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 5628d0ba637e..6a4b4c7e36ef 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -57,6 +57,42 @@
 
 extern bool itlb_multihit_kvm_mitigation;
 
+/*
+ * commit 1aa9b9572b10529c2e64e2b8f44025d86e124308
+ * Author: Junaid Shahid <junaids@google.com>
+ * Date:   Mon Nov 4 20:26:00 2019 +0100
+ *
+ * kvm: x86: mmu: Recovery of shattered NX large pages
+ *
+ * The page table pages corresponding to broken down large pages are zapped in
+ * FIFO order, so that the large page can potentially be recovered, if it is
+ * not longer being used for execution.  This removes the performance penalty
+ * for walking deeper EPT page tables.
+ *
+ * By default, one large page will last about one hour once the guest
+ * reaches a steady state.
+ *
+ * Signed-off-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ *
+ * kvm.nx_huge_pages_recovery_ratio=
+ *     [KVM] Controls how many 4KiB pages are periodically zapped
+ *     back to huge pages.  0 disables the recovery, otherwise if
+ *     the value is N KVM will zap 1/Nth of the 4KiB pages every
+ *     minute.  The default is 60.
+ *
+ * 在以下使用nx_huge_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|60| <<global>> int __read_mostly nx_huge_pages = -1;
+ *   - arch/x86/kvm/mmu/mmu.c|82| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|83| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ *   - arch/x86/kvm/mmu.h|184| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6328| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ *   - arch/x86/kvm/mmu/mmu.c|6333| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+ *   - arch/x86/kvm/mmu/mmu.c|6374| <<kvm_mmu_module_init>> if (nx_huge_pages == -1)
+ *   - arch/x86/kvm/mmu/mmu.c|6441| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+ */
 int __read_mostly nx_huge_pages = -1;
 static uint __read_mostly nx_huge_pages_recovery_period_ms;
 #ifdef CONFIG_PREEMPT_RT
@@ -88,6 +124,26 @@ module_param_cb(nx_huge_pages_recovery_period_ms, &nx_huge_pages_recovery_param_
 		&nx_huge_pages_recovery_period_ms, 0644);
 __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
 
+/*
+ * commit 71fe70130d88729abc0e658d3202618c340d2e71
+ * Author: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Date:   Fri Mar 20 14:28:28 2020 -0700
+ *
+ * KVM: x86/mmu: Add module param to force TLB flush on root reuse
+ *
+ * Add a module param, flush_on_reuse, to override skip_tlb_flush and
+ * skip_mmu_sync when performing a so called "fast cr3 switch", i.e. when
+ * reusing a cached root.  The primary motiviation for the control is to
+ * provide a fallback mechanism in the event that TLB flushing and/or MMU
+ * sync bugs are exposed/introduced by upcoming changes to stop
+ * unconditionally flushing on nested VMX transitions.
+ *
+ * Suggested-by: Jim Mattson <jmattson@google.com>
+ * Suggested-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Message-Id: <20200320212833.3507-33-sean.j.christopherson@intel.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
 static bool __read_mostly force_flush_and_sync_on_reuse;
 module_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);
 
@@ -102,6 +158,12 @@ bool tdp_enabled = false;
 
 static int max_huge_page_level __read_mostly;
 static int tdp_root_level __read_mostly;
+/*
+ * 在以下使用max_tdp_level:
+ *   - arch/x86/kvm/mmu/mmu.c|4809| <<kvm_mmu_get_tdp_level>> if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)
+ *   - arch/x86/kvm/mmu/mmu.c|4812| <<kvm_mmu_get_tdp_level>> return max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|5496| <<kvm_configure_mmu>> max_tdp_level = tdp_max_root_level;
+ */
 static int max_tdp_level __read_mostly;
 
 enum {
@@ -168,17 +230,30 @@ struct kvm_shadow_walk_iterator {
 	unsigned index;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|954| <<FNAME(invlpg)>> for_each_shadow_entry_using_root(vcpu, root_hpa, gva, iterator) {
+ */
 #define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \
 	for (shadow_walk_init_using_root(&(_walker), (_vcpu),              \
 					 (_root), (_addr));                \
 	     shadow_walk_okay(&(_walker));			           \
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3058| <<__direct_map>> for_each_shadow_entry(vcpu, fault->addr, it) {
+ */
 #define for_each_shadow_entry(_vcpu, _addr, _walker)            \
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);	\
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3233| <<fast_pf_get_last_sptep>> for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {
+ *   - arch/x86/kvm/mmu/mmu.c|4009| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -187,6 +262,13 @@ struct kvm_shadow_walk_iterator {
 
 static struct kmem_cache *pte_list_desc_cache;
 struct kmem_cache *mmu_page_header_cache;
+/*
+ * 在以下使用kvm_total_used_mmu_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|1831| <<kvm_mod_used_mmu_pages>> percpu_counter_add(&kvm_total_used_mmu_pages, nr);
+ *   - arch/x86/kvm/mmu/mmu.c|6557| <<mmu_shrink_count>> return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
+ *   - arch/x86/kvm/mmu/mmu.c|6700| <<kvm_mmu_module_init>> if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
+ *   - arch/x86/kvm/mmu/mmu.c|6725| <<kvm_mmu_module_exit>> percpu_counter_destroy(&kvm_total_used_mmu_pages);
+ */
 static struct percpu_counter kvm_total_used_mmu_pages;
 
 static void mmu_spte_set(u64 *sptep, u64 spte);
@@ -813,16 +895,30 @@ static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|917| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|131| <<kvm_slot_page_track_add_page>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|965| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|169| <<kvm_slot_page_track_remove_page>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2270| <<kvm_mmu_get_page>> account_shadowed(vcpu->kvm, sp);
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -842,17 +938,41 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_disallow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+ */
 void account_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (sp->lpage_disallowed)
 		return;
 
 	++kvm->stat.nx_lpage_splits;
+	/*
+	 * 注释:
+	 * Use to track shadow pages that, if zapped, would allow KVM to create
+	 * an NX huge page.  A shadow page will have nx_huge_page_disallowed
+	 * set but not be on the list if a huge page is disallowed for other
+	 * reasons, e.g. because KVM is shadowing a PTE at the same gfn, the
+	 * memslot isn't properly aligned, etc...
+	 */
 	list_add_tail(&sp->lpage_disallowed_link,
 		      &kvm->arch.lpage_disallowed_mmu_pages);
 	sp->lpage_disallowed = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2550| <<__kvm_mmu_prepare_zap_page>> unaccount_shadowed(kvm, sp);
+ */
 static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -870,9 +990,20 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2585| <<__kvm_mmu_prepare_zap_page>> unaccount_huge_nx_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|418| <<tdp_mmu_unlink_page>> unaccount_huge_nx_page(kvm, sp);
+ */
 void unaccount_huge_nx_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	--kvm->stat.nx_lpage_splits;
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->lpage_disallowed = false;
 	list_del(&sp->lpage_disallowed_link);
 }
@@ -1189,6 +1320,12 @@ static bool __drop_large_spte(struct kvm *kvm, u64 *sptep)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3249| <<__direct_map>> drop_large_spte(vcpu, it.sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|680| <<FNAME(fetch)>> drop_large_spte(vcpu, it.sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|740| <<FNAME(fetch)>> drop_large_spte(vcpu, it.sptep);
+ */
 static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	if (__drop_large_spte(vcpu->kvm, sptep)) {
@@ -1690,6 +1827,11 @@ static int is_empty_shadow_page(u64 *spt)
  * aggregate version in order to make the slab shrinker
  * faster
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1779| <<kvm_mmu_alloc_page>> kvm_mod_used_mmu_pages(vcpu->kvm, +1);
+ *   - arch/x86/kvm/mmu/mmu.c|2480| <<__kvm_mmu_prepare_zap_page>> kvm_mod_used_mmu_pages(kvm, -1);
+ */
 static inline void kvm_mod_used_mmu_pages(struct kvm *kvm, long nr)
 {
 	kvm->arch.n_used_mmu_pages += nr;
@@ -2062,6 +2204,13 @@ static void clear_sp_write_flooding_count(u64 *spte)
 	__clear_sp_write_flooding_count(sptep_to_sp(spte));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3096| <<__direct_map>> sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
+ *   - arch/x86/kvm/mmu/mmu.c|3487| <<mmu_alloc_root>> sp = kvm_mmu_get_page(vcpu, gfn, gva, level, direct, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|686| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, table_gfn, fault->addr,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> sp = kvm_mmu_get_page(vcpu, base_gfn, fault->addr,
+ */
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 					     gfn_t gfn,
 					     gva_t gaddr,
@@ -2161,10 +2310,29 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|172| <<for_each_shadow_entry_using_root>> for (shadow_walk_init_using_root(&(_walker), (_vcpu), \
+ *   - arch/x86/kvm/mmu/mmu.c|2202| <<shadow_walk_init>> shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
+ *
+ * 核心思想.
+ * iterator->addr = addr;
+ * iterator->shadow_addr = root;
+ * iterator->level = vcpu->arch.mmu->shadow_root_level;
+ */
 static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,
 					struct kvm_vcpu *vcpu, hpa_t root,
 					u64 addr)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->addr = addr;
 	iterator->shadow_addr = root;
 	iterator->level = vcpu->arch.mmu->shadow_root_level;
@@ -2190,23 +2358,60 @@ static void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterato
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|178| <<for_each_shadow_entry>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|183| <<for_each_shadow_entry_lockless>> for (shadow_walk_init(&(_walker), _vcpu, _addr); \
+ *   - arch/x86/kvm/mmu/mmu.c|3815| <<get_walk>> for (shadow_walk_init(&iterator, vcpu, addr),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|674| <<FNAME(fetch)>> for (shadow_walk_init(&it, vcpu, fault->addr);
+ */
 static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 			     struct kvm_vcpu *vcpu, u64 addr)
 {
+	/*
+	 * 核心思想.
+	 * iterator->addr = addr;
+	 * iterator->shadow_addr = vcpu->arch.mmu->root_hpa;
+	 * iterator->level = vcpu->arch.mmu->shadow_root_level;
+	 */
 	shadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root_hpa,
 				    addr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|174| <<for_each_shadow_entry_using_root>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|179| <<for_each_shadow_entry>> shadow_walk_okay(&(_walker)); \
+ *   - arch/x86/kvm/mmu/mmu.c|184| <<for_each_shadow_entry_lockless>> shadow_walk_okay(&(_walker)) && \
+ *   - arch/x86/kvm/mmu/mmu.c|3817| <<get_walk>> shadow_walk_okay(&iterator);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|675| <<FNAME(fetch)>> shadow_walk_okay(&it) && it.level > gw->level;
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 {
 	if (iterator->level < PG_LEVEL_4K)
 		return false;
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址 
+	 *     int level;         // shadow_addr所在的level
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
 	iterator->sptep	= ((u64 *)__va(iterator->shadow_addr)) + iterator->index;
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|186| <<for_each_shadow_entry_lockless>> __shadow_walk_next(&(_walker), spte))
+ *   - arch/x86/kvm/mmu/mmu.c|2230| <<shadow_walk_next>> __shadow_walk_next(iterator, *iterator->sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|3818| <<get_walk>> __shadow_walk_next(&iterator, spte)) {
+ */
 static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 			       u64 spte)
 {
@@ -2215,15 +2420,37 @@ static void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,
 		return;
 	}
 
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level (比方shadow_addr可以是hpa)
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	iterator->shadow_addr = spte & PT64_BASE_ADDR_MASK;
 	--iterator->level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|175| <<for_each_shadow_entry_using_root>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/mmu.c|180| <<for_each_shadow_entry>> shadow_walk_next(&(_walker)))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|676| <<FNAME(fetch)>> shadow_walk_next(&it)) {
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|724| <<FNAME(fetch)>> for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+ */
 static void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)
 {
 	__shadow_walk_next(iterator, *iterator->sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3005| <<__direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|717| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|745| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2241,6 +2468,10 @@ static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|738| <<FNAME(fetch)>> validate_direct_spte(vcpu, it.sptep, direct_access);
+ */
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 				   unsigned direct_access)
 {
@@ -2398,6 +2629,18 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	return list_unstable;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1909| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2114| <<kvm_mmu_get_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|2294| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child,
+ *   - arch/x86/kvm/mmu/mmu.c|2340| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2543| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3276| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5313| <<kvm_mmu_pte_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|6327| <<kvm_recover_nx_lpages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmutrace.h|203| <<KVM_MMU_PAGE_ASSIGN>> DEFINE_EVENT(kvm_mmu_page_class, kvm_mmu_prepare_zap_page,
+ */
 static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				     struct list_head *invalid_list)
 {
@@ -2432,6 +2675,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2619| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2644| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|6261| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2469,6 +2718,10 @@ static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 	return total_zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2614| <<make_mmu_pages_available>> if (!kvm_mmu_available_pages(vcpu->kvm))
+ */
 static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 {
 	if (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)
@@ -2478,6 +2731,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3522| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3656| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4211| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|908| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2505,6 +2765,11 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
  * Changing the number of mmu pages allocated to the vm
  * Note: if goal_nr_mmu_pages is too small, you will get dead lock
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5719| <<kvm_vm_ioctl_set_nr_mmu_pages>> kvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);
+ *   - arch/x86/kvm/x86.c|12101| <<kvm_arch_commit_memory_region>> kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
+ */
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
 {
 	write_lock(&kvm->mmu_lock);
@@ -2673,6 +2938,13 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2985| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|3265| <<__direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|576| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|755| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2777,6 +3049,11 @@ static int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3042| <<direct_pte_prefetch>> __direct_pte_prefetch(vcpu, sp, sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|625| <<FNAME(pte_prefetch)>> return __direct_pte_prefetch(vcpu, sp, sptep);
+ */
 static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 				  struct kvm_mmu_page *sp, u64 *sptep)
 {
@@ -2802,6 +3079,10 @@ static void __direct_pte_prefetch(struct kvm_vcpu *vcpu,
 		direct_pte_prefetch_many(vcpu, sp, start, spte);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3270| <<__direct_map>> direct_pte_prefetch(vcpu, it.sptep);
+ */
 static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 {
 	struct kvm_mmu_page *sp;
@@ -2829,6 +3110,10 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
 	__direct_pte_prefetch(vcpu, sp, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2876| <<kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, pfn, slot);
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -2849,6 +3134,10 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	 */
 	hva = __gfn_to_hva_memslot(slot, gfn);
 
+	/*
+	 * Lookup the page table entry for a virtual address in a given mm. Return a
+	 * pointer to the entry and the level of the mapping.
+	 */
 	pte = lookup_address_in_mm(kvm->mm, hva, &level);
 	if (unlikely(!pte))
 		return PG_LEVEL_4K;
@@ -2856,6 +3145,12 @@ static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn, kvm_pfn_t pfn,
 	return level;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2900| <<kvm_mmu_hugepage_adjust>> fault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|5889| <<kvm_mmu_zap_collapsible_spte>> sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1390| <<zap_collapsible_spte_range>> iter.level >= kvm_mmu_max_mapping_level(kvm, slot, iter.gfn,
+ */
 int kvm_mmu_max_mapping_level(struct kvm *kvm,
 			      const struct kvm_memory_slot *slot, gfn_t gfn,
 			      kvm_pfn_t pfn, int max_level)
@@ -2877,11 +3172,25 @@ int kvm_mmu_max_mapping_level(struct kvm *kvm,
 	return min(host_level, max_level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3200| <<__direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ */
 void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2895| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|2913| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<__direct_map>> if (fault->is_tdp && fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|746| <<FNAME(fetch)>> if (fault->huge_page_disallowed &&
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1072| <<kvm_tdp_mmu_map>> fault->huge_page_disallowed &&
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -2913,6 +3222,15 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->pfn &= ~mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3180| <<__direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|732| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1253| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+ *
+ * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+ * 所以就没法huge page了, 只能让goal_level再下降一位
+ */
 void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
 {
 	if (cur_level > PG_LEVEL_4K &&
@@ -2933,21 +3251,90 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4103| <<direct_page_fault>> r = __direct_map(vcpu, fault);
+ *
+ * tdp mmu的版本是kvm_tdp_mmu_map()
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;
+	 *     hpa_t shadow_addr;
+	 *     u64 *sptep;
+	 *     int level;
+	 *     unsigned index;
+	 * };
+	 */
 	struct kvm_shadow_walk_iterator it;
 	struct kvm_mmu_page *sp;
 	int ret;
 	gfn_t base_gfn = fault->gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3200| <<__direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|720| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
+	/*
+	 * struct kvm_shadow_walk_iterator {
+	 *     u64 addr;          // 要解析的地址
+	 *     hpa_t shadow_addr; // 当前的某个页表/或者directory的hpa
+	 *     u64 *sptep;        // 根据当前的level和要解析的地址addr,获得当前level对应的pte的地址
+	 *     int level;         // shadow_addr所在的level (比方shadow_addr可以是hpa)
+	 *     unsigned index;    // 根据当前的level和要解析的地址addr,获得在当前页表(directory)的index
+	 * };
+	 */
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
 		 */
+		/*
+		 * disallowed_hugepage_adjust():
+		 * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+		 * 所以就没法huge page了, 只能让goal_level再下降一位
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
 
@@ -2963,11 +3350,20 @@ static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 				      it.level - 1, true, ACC_ALL);
 
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下使用account_huge_nx_page():
+		 *   - arch/x86/kvm/mmu/mmu.c|2986| <<__direct_map>> account_huge_nx_page(vcpu->kvm, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|748| <<FNAME(fetch)>> account_huge_nx_page(vcpu->kvm, sp);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_link_page>> account_huge_nx_page(kvm, sp);
+		 */
 		if (fault->is_tdp && fault->huge_page_disallowed &&
 		    fault->req_level >= it.level)
 			account_huge_nx_page(vcpu->kvm, sp);
 	}
 
+	/*
+	 * WARN_ON_ONCE说明这里不会发生
+	 */
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
@@ -3004,6 +3400,11 @@ static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4316| <<direct_page_fault>> if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> if (handle_abnormal_pfn(vcpu, fault, walker.pte_access, &r))
+ */
 static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 				unsigned int access, int *ret_val)
 {
@@ -3041,6 +3442,9 @@ static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
 	if (fault->rsvd)
 		return false;
 
+	/*
+	 * 如果non-present, 下面一定不返回, 会继续 ... present就是read
+	 */
 	/* See if the page fault is due to an NX violation */
 	if (unlikely(fault->exec && fault->present))
 		return false;
@@ -3112,6 +3516,10 @@ static bool is_access_allowed(struct kvm_page_fault *fault, u64 spte)
  *  - Must be called between walk_shadow_page_lockless_{begin,end}.
  *  - The returned sptep must not be used after walk_shadow_page_lockless_end.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3538| <<fast_page_fault>> sptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 {
 	struct kvm_shadow_walk_iterator iterator;
@@ -3129,6 +3537,38 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4426| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3230,6 +3670,12 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3287| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->prev_roots[i].hpa,
+ *   - arch/x86/kvm/mmu/mmu.c|3293| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->root_hpa, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3299| <<kvm_mmu_free_roots>> mmu_free_root_page(kvm, &mmu->pae_root[i],
+ */
 static void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,
 			       struct list_head *invalid_list)
 {
@@ -3350,6 +3796,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5129| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -3914,6 +4364,13 @@ static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				  kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4302| <<direct_page_fault>> if (kvm_faultin_pfn(vcpu, fault, &r))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|876| <<FNAME(page_fault)>> if (kvm_faultin_pfn(vcpu, fault, &r))
+ *
+ * 核心思想是填充fault->pfn
+ */
 static bool kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault, int *r)
 {
 	struct kvm_memory_slot *slot = fault->slot;
@@ -4003,6 +4460,11 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_notifier_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4121| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4169| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@ -4027,14 +4489,26 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 	smp_rmb();
 
+	/*
+	 * 核心思想是填充fault->pfn
+	 */
 	if (kvm_faultin_pfn(vcpu, fault, &r))
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4316| <<direct_page_fault>> if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|879| <<FNAME(page_fault)>> if (handle_abnormal_pfn(vcpu, fault, walker.pte_access, &r))
+	 */
 	if (handle_abnormal_pfn(vcpu, fault, ACC_ALL, &r))
 		return r;
 
 	r = RET_PF_RETRY;
 
+	/*
+	 * 如果走上面的fast_page_fault()就不用任何lock了
+	 * 所以是fast path
+	 */
 	if (is_tdp_mmu_fault)
 		read_lock(&vcpu->kvm->mmu_lock);
 	else
@@ -4061,6 +4535,10 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	return r;
 }
 
+/*
+ * 在以下使用nonpaging_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4174| <<nonpaging_init_context>> context->page_fault = nonpaging_page_fault;
+ */
 static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4071,6 +4549,11 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu,
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1775| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address,
+ *   - arch/x86/kvm/vmx/vmx.c|4903| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -4104,9 +4587,46 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu.h|199| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu.h|208| <<kvm_mmu_do_page_fault>> return kvm_tdp_page_fault(vcpu, &fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4840| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	while (fault->max_level > PG_LEVEL_4K) {
+		/*
+		 * kvm最大1G, 2M, 4K ...
+		 */
 		int page_num = KVM_PAGES_PER_HPAGE(fault->max_level);
 		gfn_t base = (fault->addr >> PAGE_SHIFT) & ~(page_num - 1);
 
@@ -4115,6 +4635,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 		--fault->max_level;
 	}
+	/*
+	 * 大部分情况可以假设, "--fault->max_level"不现实
+	 * 结果还是.max_level = KVM_MAX_HUGEPAGE_LEVEL
+	 */
 
 	return direct_page_fault(vcpu, fault);
 }
@@ -5300,6 +5824,13 @@ static void kvm_mmu_pte_write(struct kvm_vcpu *vcpu, gpa_t gpa,
 	write_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4277| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|1789| <<npf_interception>> return kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/vmx.c|5430| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5451| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -5575,6 +6106,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11180| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -5605,6 +6140,10 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 }
 
 #define BATCH_ZAP_PAGES	10
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5899| <<kvm_mmu_zap_all_fast>> kvm_zap_obsolete_pages(kvm);
+ */
 static void kvm_zap_obsolete_pages(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -5664,6 +6203,12 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
  * not use any resource of the being-deleted slot or all slots
  * after calling the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5772| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6070| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|6186| <<set_nx_huge_pages>> kvm_mmu_zap_all_fast(kvm);
+ */
 static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 {
 	lockdep_assert_held(&kvm->slots_lock);
@@ -5715,6 +6260,10 @@ static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
 	return unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));
 }
 
+/*
+ * 在以下使用kvm_mmu_invalidate_zap_pages_in_memslot():
+ *   - arch/x86/kvm/mmu/mmu.c|6199| <<kvm_mmu_init_vm>> node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
+ */
 static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
 			struct kvm_memory_slot *slot,
 			struct kvm_page_track_notifier_node *node)
@@ -5780,6 +6329,12 @@ static bool __kvm_zap_rmaps(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
  * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
  * (not including it)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mtrr.c|332| <<update_mtrr>> kvm_zap_gfn_range(vcpu->kvm, gpa_to_gfn(start), gpa_to_gfn(end));
+ *   - arch/x86/kvm/x86.c|880| <<kvm_post_set_cr0>> kvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);
+ *   - arch/x86/kvm/x86.c|9758| <<__kvm_request_apicv_update>> kvm_zap_gfn_range(kvm, gfn, gfn+1);
+ */
 void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 {
 	bool flush;
@@ -5971,6 +6526,10 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12113| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ */
 void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -6083,6 +6642,13 @@ mmu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)
 static unsigned long
 mmu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
 {
+	/*
+	 * 在以下使用kvm_total_used_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|1831| <<kvm_mod_used_mmu_pages>> percpu_counter_add(&kvm_total_used_mmu_pages, nr);
+	 *   - arch/x86/kvm/mmu/mmu.c|6557| <<mmu_shrink_count>> return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6700| <<kvm_mmu_module_init>> if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
+	 *   - arch/x86/kvm/mmu/mmu.c|6725| <<kvm_mmu_module_exit>> percpu_counter_destroy(&kvm_total_used_mmu_pages);
+	 */
 	return percpu_counter_read_positive(&kvm_total_used_mmu_pages);
 }
 
@@ -6092,6 +6658,11 @@ static struct shrinker mmu_shrinker = {
 	.seeks = DEFAULT_SEEKS * 10,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6369| <<kvm_mmu_module_init>> mmu_destroy_caches();
+ *   - arch/x86/kvm/mmu/mmu.c|6383| <<kvm_mmu_module_exit>> mmu_destroy_caches();
+ */
 static void mmu_destroy_caches(void)
 {
 	kmem_cache_destroy(pte_list_desc_cache);
@@ -6104,11 +6675,48 @@ static bool get_nx_auto_mode(void)
 	return boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();
 }
 
+/*
+ * commit b8e8c8303ff28c61046a4d0f6ea99aea609a7dc0
+ * Author: Paolo Bonzini <pbonzini@redhat.com>
+ * Date:   Mon Nov 4 12:22:02 2019 +0100
+ *
+ * kvm: mmu: ITLB_MULTIHIT mitigation
+ *
+ * With some Intel processors, putting the same virtual address in the TLB
+ * as both a 4 KiB and 2 MiB page can confuse the instruction fetch unit
+ * and cause the processor to issue a machine check resulting in a CPU lockup.
+ *
+ * Unfortunately when EPT page tables use huge pages, it is possible for a
+ * malicious guest to cause this situation.
+ *
+ * Add a knob to mark huge pages as non-executable. When the nx_huge_pages
+ * parameter is enabled (and we are using EPT), all huge pages are marked as
+ * NX. If the guest attempts to execute in one of those pages, the page is
+ * broken down into 4K pages, which are then marked executable.
+ *
+ * This is not an issue for shadow paging (except nested EPT), because then
+ * the host is in control of TLB flushes and the problematic situation cannot
+ * happen.  With nested EPT, again the nested guest can cause problems shadow
+ * and direct EPT is treated in the same way.
+ *
+ * [ tglx: Fixup default to auto and massage wording a bit ]
+ *
+ * Originally-by: Junaid Shahid <junaids@google.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ * Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6346| <<set_nx_huge_pages>> __set_nx_huge_pages(new_val);
+ *   - arch/x86/kvm/mmu/mmu.c|6375| <<kvm_mmu_module_init>> __set_nx_huge_pages(get_nx_auto_mode());
+ */
 static void __set_nx_huge_pages(bool val)
 {
 	nx_huge_pages = itlb_multihit_kvm_mitigation = val;
 }
 
+/*
+ * struct kernel_param_ops nx_huge_pages_ops.set = set_nx_huge_pages()
+ */
 static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 {
 	bool old_val = nx_huge_pages;
@@ -6144,6 +6752,15 @@ static int set_nx_huge_pages(const char *val, const struct kernel_param *kp)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8844| <<kvm_arch_init>> r = kvm_mmu_module_init();
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ *       -> kvm_mmu_module_init()
+ */
 int kvm_mmu_module_init(void)
 {
 	int ret = -ENOMEM;
@@ -6189,6 +6806,11 @@ int kvm_mmu_module_init(void)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11260| <<kvm_arch_vcpu_create>> kvm_mmu_destroy(vcpu);
+ *   - arch/x86/kvm/x86.c|11301| <<kvm_arch_vcpu_destroy>> kvm_mmu_destroy(vcpu);
+ */
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -6197,6 +6819,10 @@ void kvm_mmu_destroy(struct kvm_vcpu *vcpu)
 	mmu_free_memory_caches(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8920| <<kvm_arch_exit>> kvm_mmu_module_exit();
+ */
 void kvm_mmu_module_exit(void)
 {
 	mmu_destroy_caches();
@@ -6275,6 +6901,13 @@ static void kvm_recover_nx_lpages(struct kvm *kvm)
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * 在以下使用kvm_arch->lpage_disallowed_mmu_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|947| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link, &kvm->arch.lpage_disallowed_mmu_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|6641| <<kvm_recover_nx_lpages>> if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|6649| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages,
+		 *   - arch/x86/kvm/x86.c|11639| <<kvm_arch_init_vm>> INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+		 */
 		if (list_empty(&kvm->arch.lpage_disallowed_mmu_pages))
 			break;
 
@@ -6342,6 +6975,10 @@ static int kvm_nx_lpage_recovery_worker(struct kvm *kvm, uintptr_t data)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11707| <<kvm_arch_post_init_vm>> return kvm_mmu_post_init_vm(kvm);
+ */
 int kvm_mmu_post_init_vm(struct kvm *kvm)
 {
 	int err;
@@ -6355,6 +6992,10 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11821| <<kvm_arch_pre_destroy_vm>> kvm_mmu_pre_destroy_vm(kvm);
+ */
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
 {
 	if (kvm->arch.nx_lpage_recovery_thread)
diff --git a/arch/x86/kvm/mmu/mmu_audit.c b/arch/x86/kvm/mmu/mmu_audit.c
index 9e7dcf999f08..b62833c457f0 100644
--- a/arch/x86/kvm/mmu/mmu_audit.c
+++ b/arch/x86/kvm/mmu/mmu_audit.c
@@ -233,6 +233,14 @@ static void audit_vcpu_spte(struct kvm_vcpu *vcpu)
 	mmu_spte_walk(vcpu, audit_spte);
 }
 
+/*
+ * 在以下使用mmu_audit:
+ *   - arch/x86/kvm/mmu/mmu_audit.c|303| <<global>> arch_param_cb(mmu_audit, &audit_param_ops, &mmu_audit, 0644);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|259| <<mmu_audit_enable>> if (mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|263| <<mmu_audit_enable>> mmu_audit = true;
+ *   - arch/x86/kvm/mmu/mmu_audit.c|268| <<mmu_audit_disable>> if (!mmu_audit)
+ *   - arch/x86/kvm/mmu/mmu_audit.c|272| <<mmu_audit_disable>> mmu_audit = false;
+ */
 static bool mmu_audit;
 static DEFINE_STATIC_KEY_FALSE(mmu_audit_key);
 
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index da6166b5c377..b33cd67a4393 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -41,6 +41,23 @@ struct kvm_mmu_page {
 	bool tdp_mmu_page;
 	bool unsync;
 	u8 mmu_valid_gen;
+	/*
+	 * 注释:
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
+	/*
+	 * 在以下设置kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|934| <<account_huge_nx_page>> sp->lpage_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|957| <<unaccount_huge_nx_page>> sp->lpage_disallowed = false;
+	 * 在以下使用kvm_mmu_page->lpage_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|928| <<account_huge_nx_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|2558| <<__kvm_mmu_prepare_zap_page>> if (sp->lpage_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|6620| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(!sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|6625| <<kvm_recover_nx_lpages>> WARN_ON_ONCE(sp->lpage_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|407| <<tdp_mmu_unlink_page>> if (sp->lpage_disallowed)
+	 */
 	bool lpage_disallowed; /* Can't be replaced by an equiv large page */
 
 	/*
@@ -55,13 +72,46 @@ struct kvm_mmu_page {
 	gfn_t *gfns;
 	/* Currently serving as active root */
 	union {
+		/*
+		 * 在以下设置kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+		 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+		 * 在以下使用kvm_mmu_page->root_count:
+		 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+		 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+		 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+		 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+		 */
 		int root_count;
+		/*
+		 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+		 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+		 */
 		refcount_t tdp_mmu_root_count;
 	};
 	unsigned int unsync_children;
 	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
 	DECLARE_BITMAP(unsync_child_bitmap, 512);
 
+	/*
+	 * 注释:
+	 * Use to track shadow pages that, if zapped, would allow KVM to create
+	 * an NX huge page.  A shadow page will have nx_huge_page_disallowed
+	 * set but not be on the list if a huge page is disallowed for other
+	 * reasons, e.g. because KVM is shadowing a PTE at the same gfn, the
+	 * memslot isn't properly aligned, etc...
+	 */
+	/*
+	 * 在kvm_mmu_page->lpage_disallowed_link:
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<account_huge_nx_page>> list_add_tail(&sp->lpage_disallowed_link,
+	 *   - arch/x86/kvm/mmu/mmu.c|958| <<unaccount_huge_nx_page>> list_del(&sp->lpage_disallowed_link);
+	 *   - arch/x86/kvm/mmu/mmu.c|6619| <<kvm_recover_nx_lpages>> sp = list_first_entry(&kvm->arch.lpage_disallowed_mmu_pages, struct kvm_mmu_page, lpage_disallowed_link);
+	 */
 	struct list_head lpage_disallowed_link;
 #ifdef CONFIG_X86_32
 	/*
@@ -82,6 +132,28 @@ struct kvm_mmu_page {
 
 extern struct kmem_cache *mmu_page_header_cache;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1836| <<__mmu_unsync_walk>> child = to_shadow_page(ent & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2257| <<validate_direct_spte>> child = to_shadow_page(*sptep & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2278| <<mmu_page_zap_pte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|2710| <<mmu_set_spte>> child = to_shadow_page(pte & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3251| <<mmu_free_root_page>> sp = to_shadow_page(*root_hpa & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu.c|3330| <<kvm_mmu_free_guest_mode_roots>> if (!to_shadow_page(root_hpa) ||
+ *   - arch/x86/kvm/mmu/mmu.c|3331| <<kvm_mmu_free_guest_mode_roots>> to_shadow_page(root_hpa)->role.guest_mode)
+ *   - arch/x86/kvm/mmu/mmu.c|3672| <<is_unsync_root>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3694| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3717| <<kvm_mmu_sync_roots>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu.c|3995| <<is_page_fault_stale>> struct kvm_mmu_page *sp = to_shadow_page(vcpu->arch.mmu->root_hpa);
+ *   - arch/x86/kvm/mmu/mmu.c|4145| <<is_root_usable>> VALID_PAGE(root->hpa) && to_shadow_page(root->hpa) &&
+ *   - arch/x86/kvm/mmu/mmu.c|4146| <<is_root_usable>> role.word == to_shadow_page(root->hpa)->role.word;
+ *   - arch/x86/kvm/mmu/mmu.c|4235| <<__kvm_mmu_new_pgd>> to_shadow_page(vcpu->arch.mmu->root_hpa));
+ *   - arch/x86/kvm/mmu/mmu_audit.c|48| <<__mmu_spte_walk>> child = to_shadow_page(ent[i] & PT64_BASE_ADDR_MASK);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|65| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|75| <<mmu_spte_walk>> sp = to_shadow_page(root);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|94| <<sptep_to_sp>> return to_shadow_page(__pa(sptep));
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|107| <<is_tdp_mmu>> sp = to_shadow_page(hpa);
+ */
 static inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)
 {
 	struct page *page = pfn_to_page(shadow_page >> PAGE_SHIFT);
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 5b5bdac97c7b..a683aed3f11b 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -719,6 +719,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 73cfe62fdad1..b6730eae04b0 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -27,6 +27,11 @@ u64 __read_mostly shadow_mmu_writable_mask;
 u64 __read_mostly shadow_nx_mask;
 u64 __read_mostly shadow_x_mask; /* mutual exclusive with nx_mask */
 u64 __read_mostly shadow_user_mask;
+/*
+ * 在以下使用shadow_accessed_mask:
+ *   - arch/x86/kvm/mmu/spte.c|330| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+ *   - arch/x86/kvm/mmu/spte.c|382| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+ */
 u64 __read_mostly shadow_accessed_mask;
 u64 __read_mostly shadow_dirty_mask;
 u64 __read_mostly shadow_mmio_value;
@@ -52,6 +57,11 @@ static u64 generation_mmio_spte_mask(u64 gen)
 	return mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|377| <<mark_mmio_spte>> u64 spte = make_mmio_spte(vcpu, gfn, access);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1240| <<tdp_mmu_map_handle_target_level>> new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ */
 u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)
 {
 	u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
@@ -90,6 +100,12 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 				     E820_TYPE_RAM);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2948| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1101| <<FNAME(sync_page)>> make_spte(vcpu, sp, slot, pte_access, gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1242| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn,
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
@@ -192,8 +208,21 @@ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	return wrprot;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2420| <<link_shadow_page>> spte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1417| <<kvm_tdp_mmu_map>> new_spte = make_nonleaf_spte(child_pt,
+ */
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	u64 spte = SPTE_MMU_PRESENT_MASK;
 
 	spte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |
@@ -305,6 +334,10 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7997| <<hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits,
+ */
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)
 {
 	shadow_user_mask	= VMX_EPT_READABLE_MASK;
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index be6a007a4af3..28840f2b5806 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -12,6 +12,14 @@
  * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
  * enough that the improved code generation is noticeable in KVM's footprint.
  */
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
 /*
@@ -53,11 +61,24 @@ static_assert(SPTE_TDP_AD_ENABLED_MASK == 0);
 
 #define PT64_LEVEL_BITS 9
 
+/*
+ * PT64_LEVEL_SHIFT(1) = 12 + 0 * 9 = 12
+ * PT64_LEVEL_SHIFT(2) = 12 + 1 * 9 = 21
+ * PT64_LEVEL_SHIFT(3) = 12 + 2 * 9 = 30
+ * PT64_LEVEL_SHIFT(4) = 12 + 3 * 9 = 39
+ * PT64_LEVEL_SHIFT(5) = 12 + 4 * 9 = 48
+ */
 #define PT64_LEVEL_SHIFT(level) \
 		(PAGE_SHIFT + (level - 1) * PT64_LEVEL_BITS)
 
 #define PT64_INDEX(address, level)\
 	(((address) >> PT64_LEVEL_SHIFT(level)) & ((1 << PT64_LEVEL_BITS) - 1))
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2211| <<shadow_walk_okay>> iterator->index = SHADOW_PT_INDEX(iterator->addr, iterator->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|26| <<tdp_iter_refresh_sptep>> SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|181| <<try_step_side>> if (SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level) ==
+ */
 #define SHADOW_PT_INDEX(addr, level) PT64_INDEX(addr, level)
 
 /*
@@ -201,11 +222,29 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用REMOVED_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|215| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|299| <<kvm_mmu_set_mmio_spte_mask>> WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|219| <<is_removed_spte>> return spte == REMOVED_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|453| <<handle_removed_tdp_mmu_page>> old_child_spte = xchg(sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|480| <<handle_removed_tdp_mmu_page>> WRITE_ONCE(*sptep, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|483| <<handle_removed_tdp_mmu_page>> old_child_spte, REMOVED_SPTE, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ */
 #define REMOVED_SPTE	0x5a0ULL
 
 /* Removed SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|484| <<handle_removed_tdp_mmu_page>> if (!is_removed_spte(old_child_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|607| <<__handle_changed_spte>> !is_removed_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|686| <<tdp_mmu_set_spte_atomic>> if (is_removed_spte(iter->old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|798| <<__tdp_mmu_set_spte>> WARN_ON(is_removed_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1417| <<kvm_tdp_mmu_map>> if (is_removed_spte(iter.old_spte))
+ */
 static inline bool is_removed_spte(u64 spte)
 {
 	return spte == REMOVED_SPTE;
@@ -235,6 +274,14 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|147| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|207| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|100| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|197| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|238| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index caa96c270b95..a588b75c200b 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -8,15 +8,38 @@
  * Recalculates the pointer to the SPTE for the current GFN and level and
  * reread the SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|34| <<tdp_iter_restart>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|99| <<try_step_down>> tdp_iter_refresh_sptep(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|141| <<try_step_up>> tdp_iter_refresh_sptep(iter);
+ */
 static void tdp_iter_refresh_sptep(struct tdp_iter *iter)
 {
+	/*
+	 * tdp_iter->sptep:
+	 *    A pointer to the current SPTE
+	 * tdp_iter->pt_path[PT64_ROOT_MAX_LEVEL]:
+	 *    Pointers to the page tables traversed to reach the current SPTE
+	 */
 	iter->sptep = iter->pt_path[iter->level - 1] +
 		SHADOW_PT_INDEX(iter->gfn << PAGE_SHIFT, iter->level);
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|33| <<tdp_iter_restart>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|98| <<try_step_down>> iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|140| <<try_step_up>> iter->gfn = round_gfn_for_level(iter->gfn, iter->level);
+ */
 static gfn_t round_gfn_for_level(gfn_t gfn, int level)
 {
+	/*
+	 * KVM_PAGES_PER_HPAGE(1) = KVM_HPAGE_SIZE(1) / 4096 = 4K / 4K = 1
+	 * KVM_PAGES_PER_HPAGE(2) = KVM_HPAGE_SIZE(2) / 4096 = 2M / 4K = 512
+	 * KVM_PAGES_PER_HPAGE(3) = KVM_HPAGE_SIZE(3) / 4096 = 1G / 4K = 262144
+	 */
 	return gfn & -KVM_PAGES_PER_HPAGE(level);
 }
 
@@ -24,12 +47,30 @@ static gfn_t round_gfn_for_level(gfn_t gfn, int level)
  * Return the TDP iterator to the root PT and allow it to continue its
  * traversal over the paging structure from there.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|55| <<tdp_iter_start>> tdp_iter_restart(iter);
+ *   - arch/x86/kvm/mmu/tdp_iter.c|165| <<tdp_iter_next>> tdp_iter_restart(iter);
+ */
 void tdp_iter_restart(struct tdp_iter *iter)
 {
 	iter->yielded = false;
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	iter->yielded_gfn = iter->next_last_level_gfn;
 	iter->level = iter->root_level;
 
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 *
+	 * tdp_iter->gfn:
+	 *    The lowest GFN mapped by the current SPTE
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -40,6 +81,10 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|61| <<for_each_tdp_pte_min_level>> for (tdp_iter_start(&iter, root, root_level, min_level, start); \
+ */
 void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 		    int min_level, gfn_t next_last_level_gfn)
 {
@@ -47,8 +92,17 @@ void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
 	WARN_ON(root_level > PT64_ROOT_MAX_LEVEL);
 
 	iter->next_last_level_gfn = next_last_level_gfn;
+	/*
+	 * The level of the root page given to the iterator
+	 */
 	iter->root_level = root_level;
+	/*
+	 * The lowest level the iterator should traverse to
+	 */
 	iter->min_level = min_level;
+	/*
+	 * root_pt是虚拟地址(va)
+	 */
 	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root_pt;
 	iter->as_id = kvm_mmu_page_as_id(sptep_to_sp(root_pt));
 
@@ -76,6 +130,10 @@ tdp_ptep_t spte_to_child_pt(u64 spte, int level)
  * Steps down one level in the paging structure towards the goal GFN. Returns
  * true if the iterator was able to step down a level, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|220| <<tdp_iter_next>> if (try_step_down(iter))
+ */
 static bool try_step_down(struct tdp_iter *iter)
 {
 	tdp_ptep_t child_pt;
@@ -87,14 +145,28 @@ static bool try_step_down(struct tdp_iter *iter)
 	 * Reread the SPTE before stepping down to avoid traversing into page
 	 * tables that are no longer linked from this entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
+	/*
+	 * 这里得到的是当前pte所指向的下一个page table的page
+	 */
 	child_pt = spte_to_child_pt(iter->old_spte, iter->level);
 	if (!child_pt)
 		return false;
 
 	iter->level--;
 	iter->pt_path[iter->level - 1] = child_pt;
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->gfn = round_gfn_for_level(iter->next_last_level_gfn, iter->level);
 	tdp_iter_refresh_sptep(iter);
 
@@ -118,9 +190,27 @@ static bool try_step_side(struct tdp_iter *iter)
             (PT64_ENT_PER_PAGE - 1))
 		return false;
 
+	/*
+	 * KVM_HPAGE_SIZE(1) : 1 << 12 = 4K
+	 * KVM_HPAGE_SIZE(2) : 1 << 21 = 2M
+	 * KVM_HPAGE_SIZE(3) : 1 << 30 = 1G
+	 * KVM_HPAGE_SIZE(4) : 1 << 38 = 256G
+	 * KVM_HPAGE_SIZE(5) : 1 << 48 = 262144G
+	 */
 	iter->gfn += KVM_PAGES_PER_HPAGE(iter->level);
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	iter->next_last_level_gfn = iter->gfn;
 	iter->sptep++;
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
 
 	return true;
@@ -131,6 +221,10 @@ static bool try_step_side(struct tdp_iter *iter)
  * can continue from the next entry in the parent page table. Returns true on a
  * successful step up, false if already in the root page.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.c|272| <<tdp_iter_next>> } while (try_step_up(iter));
+ */
 static bool try_step_up(struct tdp_iter *iter)
 {
 	if (iter->level == iter->root_level)
@@ -159,8 +253,23 @@ static bool try_step_up(struct tdp_iter *iter)
  *    SPTE will have already been visited, and so the iterator must also step
  *    to the side again.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|63| <<for_each_tdp_pte_min_level>> tdp_iter_next(&iter))
+ */
 void tdp_iter_next(struct tdp_iter *iter)
 {
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	if (iter->yielded) {
 		tdp_iter_restart(iter);
 		return;
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index e19cabbcb65c..4aa6800c2650 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -17,12 +17,22 @@ struct tdp_iter {
 	 * The iterator will traverse the paging structure towards the mapping
 	 * for this GFN.
 	 */
+	/*
+	 * 在以下设置tdp_iter->next_last_level_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|49| <<tdp_iter_start>> iter->next_last_level_gfn = next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|122| <<try_step_side>> iter->next_last_level_gfn = iter->gfn;
+	 */
 	gfn_t next_last_level_gfn;
 	/*
 	 * The next_last_level_gfn at the time when the thread last
 	 * yielded. Only yielding when the next_last_level_gfn !=
 	 * yielded_gfn helps ensure forward progress.
 	 */
+	/*
+	 * 在以下使用tdp_iter->yielded_gfn:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|30| <<tdp_iter_restart>> iter->yielded_gfn = iter->next_last_level_gfn;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|661| <<tdp_mmu_iter_cond_resched>> if (iter->next_last_level_gfn == iter->yielded_gfn)
+	 */
 	gfn_t yielded_gfn;
 	/* Pointers to the page tables traversed to reach the current SPTE */
 	tdp_ptep_t pt_path[PT64_ROOT_MAX_LEVEL];
@@ -39,6 +49,12 @@ struct tdp_iter {
 	/* The address space ID, i.e. SMM vs. regular. */
 	int as_id;
 	/* A snapshot of the value at sptep */
+	/*
+	 * 在以下设置tdp_iter->old_spte:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|27| <<tdp_iter_refresh_sptep>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|121| <<try_step_down>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|160| <<try_step_side>> iter->old_spte = READ_ONCE(*rcu_dereference(iter->sptep));
+	 */
 	u64 old_spte;
 	/*
 	 * Whether the iterator has a valid state. This will be false if the
@@ -50,6 +66,17 @@ struct tdp_iter {
 	 * which case tdp_iter_next() needs to restart the walk at the root
 	 * level instead of advancing to the next entry.
 	 */
+	/*
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 * 在以下使用tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+	 */
 	bool yielded;
 };
 
@@ -57,11 +84,23 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_iter.h|66| <<for_each_tdp_pte>> for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|727| <<zap_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1183| <<wrprot_gfn_range>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1439| <<write_protect_gfn>> for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
+ */
 #define for_each_tdp_pte_min_level(iter, root, root_level, min_level, start, end) \
 	for (tdp_iter_start(&iter, root, root_level, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|627| <<tdp_root_for_each_pte>> for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|637| <<tdp_mmu_for_each_pte>> for_each_tdp_pte(_iter, __va(_mmu->root_hpa), \
+ */
 #define for_each_tdp_pte(iter, root, root_level, start, end) \
 	for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bc9e3553fba2..38c41deb651d 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -10,6 +10,12 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用tdp_mmu_enabled:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|13| <<global>> static bool __read_mostly tdp_mmu_enabled = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|14| <<global>> module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|19| <<kvm_mmu_init_tdp_mmu>> if (!tdp_enabled || !READ_ONCE(tdp_mmu_enabled))
+ */
 static bool __read_mostly tdp_mmu_enabled = true;
 module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
 
@@ -29,15 +35,32 @@ bool kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|85| <<kvm_tdp_mmu_put_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|723| <<zap_gfn_range>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ */
 static __always_inline void kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 							     bool shared)
 {
+	/*
+	 * struct kvm *kvm:
+	 *     #ifdef KVM_HAVE_MMU_RWLOCK
+	 *     rwlock_t mmu_lock;
+	 *     #else
+	 *     spinlock_t mmu_lock;
+	 *     #endif
+	 */
 	if (shared)
 		lockdep_assert_held_read(&kvm->mmu_lock);
 	else
 		lockdep_assert_held_write(&kvm->mmu_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5794| <<kvm_mmu_uninit_vm>> kvm_mmu_uninit_tdp_mmu(kvm);
+ */
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 {
 	if (!kvm->arch.tdp_mmu_enabled)
@@ -50,6 +73,9 @@ void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
 	 * Ensure that all the outstanding RCU callbacks to free shadow pages
 	 * can run before the VM is torn down.
 	 */
+	/*
+	 * 注释: Wait until all in-flight call_rcu() callbacks complete.
+	 */
 	rcu_barrier();
 }
 
@@ -57,6 +83,11 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|84| <<tdp_mmu_free_sp_rcu_callback>> tdp_mmu_free_sp(sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1080| <<kvm_tdp_mmu_map>> tdp_mmu_free_sp(sp);
+ */
 static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
 {
 	free_page((unsigned long)sp->spt);
@@ -71,6 +102,11 @@ static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
  * section, and freeing it after a grace period, lockless access to that
  * memory won't use it after it is freed.
  */
+/*
+ * 在以下使用tdp_mmu_free_sp_rcu_callback():
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|109| <<kvm_tdp_mmu_put_root>> call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|417| <<handle_removed_tdp_mmu_page>> call_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);
+ */
 static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 {
 	struct kvm_mmu_page *sp = container_of(head, struct kvm_mmu_page,
@@ -79,6 +115,12 @@ static void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)
 	tdp_mmu_free_sp(sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3254| <<mmu_free_root_page>> kvm_tdp_mmu_put_root(kvm, sp, false);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|136| <<tdp_mmu_next_root>> kvm_tdp_mmu_put_root(kvm, prev_root, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|860| <<kvm_tdp_mmu_zap_invalidated_roots>> kvm_tdp_mmu_put_root(kvm, root, true);
+ */
 void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 			  bool shared)
 {
@@ -93,6 +135,23 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	list_del_rcu(&root->link);
 	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
 
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
 
 	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
@@ -105,6 +164,11 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
  * that reference will be dropped. If no valid root is found, this
  * function will return NULL.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|152| <<for_each_tdp_mmu_root_yield_safe>> for (_root = tdp_mmu_next_root(_kvm, NULL, _shared); \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|154| <<for_each_tdp_mmu_root_yield_safe>> _root = tdp_mmu_next_root(_kvm, _root, _shared)) \
+ */
 static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 					      struct kvm_mmu_page *prev_root,
 					      bool shared)
@@ -113,6 +177,10 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 
 	rcu_read_lock();
 
+	/*
+	 * 关于list_next_or_null_rcu()注意:
+	 * Note that if the ptr is at the end of the list, NULL is returned.
+	 */
 	if (prev_root)
 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
 						  &prev_root->link,
@@ -143,6 +211,18 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
  * mode. In the unlikely event that this thread must free a root, the lock
  * will be temporarily dropped and reacquired in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|788| <<__kvm_tdp_mmu_zap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_unmap_gfn_range>> for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1234| <<kvm_tdp_mmu_wrprot_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1304| <<kvm_tdp_mmu_clear_dirty_slot>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1429| <<kvm_tdp_mmu_zap_collapsible_sptes>> for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)	\
 	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared);		\
 	     _root;							\
@@ -150,6 +230,17 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|209| <<kvm_tdp_mmu_get_vcpu_root_hpa>> for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1071| <<kvm_tdp_mmu_handle_gfn>> for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1370| <<kvm_tdp_mmu_clear_dirty_pt_masked>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1483| <<kvm_tdp_mmu_write_protect_gfn>> for_each_tdp_mmu_root(kvm, root, slot->as_id)
+ *
+ * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+ * 前者不需要锁, 后者需要锁
+ * 主要是用第二个参数_root "struct kvm_mmu_page *"
+ */
 #define for_each_tdp_mmu_root(_kvm, _root, _as_id)				\
 	list_for_each_entry_rcu(_root, &_kvm->arch.tdp_mmu_roots, link,		\
 				lockdep_is_held_type(&kvm->mmu_lock, 0) ||	\
@@ -157,6 +248,11 @@ static struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,
 		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
 		} else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|275| <<alloc_tdp_mmu_page>> sp->role.word = page_role_for_level(vcpu, level).word;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|300| <<kvm_tdp_mmu_get_vcpu_root_hpa>> role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
+ */
 static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 						   int level)
 {
@@ -172,6 +268,11 @@ static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|313| <<kvm_tdp_mmu_get_vcpu_root_hpa>> root = alloc_tdp_mmu_page(vcpu, 0, vcpu->arch.mmu->shadow_root_level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1299| <<kvm_tdp_mmu_map>> sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
+ */
 static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 					       int level)
 {
@@ -190,6 +291,14 @@ static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3377| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ *
+ * kvm_mmu_load()
+ * -> mmu_alloc_direct_roots()
+ *    -> kvm_tdp_mmu_get_vcpu_root_hpa()
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role;
@@ -201,6 +310,10 @@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 	role = page_role_for_level(vcpu, vcpu->arch.mmu->shadow_root_level);
 
 	/* Check for an existing root before allocating a new one. */
+	/*
+	 * for_each_tdp_mmu_root_yield_safe()和for_each_tdp_mmu_root()都是遍历所有root
+	 * 前者不需要锁, 后者需要锁
+	 */
 	for_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {
 		if (root->role.word == role.word &&
 		    kvm_tdp_mmu_get_root(kvm, root))
@@ -222,6 +335,12 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|562| <<handle_changed_spte>> handle_changed_spte_acc_track(old_spte, new_spte, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|603| <<tdp_mmu_set_spte_atomic>> handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|681| <<__tdp_mmu_set_spte>> handle_changed_spte_acc_track(iter->old_spte, new_spte,
+ */
 static void handle_changed_spte_acc_track(u64 old_spte, u64 new_spte, int level)
 {
 	if (!is_shadow_present_pte(old_spte) || !is_last_spte(old_spte, level))
@@ -259,6 +378,10 @@ static void handle_changed_spte_dirty_log(struct kvm *kvm, int as_id, gfn_t gfn,
  * @account_nx: This page replaces a NX large page and should be marked for
  *		eventual reclaim.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<kvm_tdp_mmu_map>> tdp_mmu_link_page(vcpu->kvm, sp,
+ */
 static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 			      bool account_nx)
 {
@@ -278,6 +401,10 @@ static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  *	    the MMU lock and the operation must synchronize with other
  *	    threads that might be adding or removing pages.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+ */
 static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				bool shared)
 {
@@ -311,6 +438,14 @@ static void tdp_mmu_unlink_page(struct kvm *kvm, struct kvm_mmu_page *sp,
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|552| <<__handle_changed_spte>> handle_removed_tdp_mmu_page(kvm, spte_to_child_pt(old_spte, level), shared);
+ *
+ * 核心思想是把一个sp->pt上的每一个entry标记为REMOVED_SPTE, 然后把sp给free
+ * Given a page table that has been removed from the TDP paging structure,
+ * iterates through the page table to clear SPTEs and free child page tables.
+ */
 static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 					bool shared)
 {
@@ -321,8 +456,15 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
 
 	trace_kvm_mmu_prepare_zap_page(sp);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|401| <<handle_removed_tdp_mmu_page>> tdp_mmu_unlink_page(kvm, sp, shared);
+	 */
 	tdp_mmu_unlink_page(kvm, sp, shared);
 
+	/*
+	 * 1 << 9 = 512
+	 */
 	for (i = 0; i < PT64_ENT_PER_PAGE; i++) {
 		u64 *sptep = rcu_dereference(pt) + i;
 		gfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);
@@ -393,6 +535,19 @@ static void handle_removed_tdp_mmu_page(struct kvm *kvm, tdp_ptep_t pt,
  * Handle bookkeeping that might result from the modification of a SPTE.
  * This function must be called for all TDP SPTE modifications.
  */
+/*
+ * 如果想要触发handle_removed_tdp_mmu_page()要满足这些条件:
+ * was_present && !was_leaf && (pfn_changed || !is_present)
+ *
+ * - was_present
+ * - !was_leaf
+ * - (pfn_changed || !is_present)
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|560| <<handle_changed_spte>> __handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|601| <<tdp_mmu_set_spte_atomic>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|678| <<__tdp_mmu_set_spte>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ */
 static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				  u64 old_spte, u64 new_spte, int level,
 				  bool shared)
@@ -471,11 +626,21 @@ static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 	 * Recursively handle child PTs if the change removed a subtree from
 	 * the paging structure.
 	 */
+	/*
+	 * handle_removed_tdp_mmu_page()
+	 * 核心思想是把一个sp->pt上的每一个entry标记为REMOVED_SPTE, 然后把sp给free
+	 * Given a page table that has been removed from the TDP paging structure,
+	 * iterates through the page table to clear SPTEs and free child page tables.
+	 */
 	if (was_present && !was_leaf && (pfn_changed || !is_present))
 		handle_removed_tdp_mmu_page(kvm,
 				spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|447| <<handle_removed_tdp_mmu_page>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_child_spte, REMOVED_SPTE, level, shared);
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -498,6 +663,14 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
  * Returns: true if the SPTE was set, false if it was not. If false is returned,
  *	    this function will have no side-effects.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|668| <<tdp_mmu_zap_spte_atomic>> if (!tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1156| <<tdp_mmu_map_handle_target_level>> else if (!tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1256| <<kvm_tdp_mmu_map>> if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1444| <<wrprot_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1512| <<clear_dirty_gfn_range>> if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ */
 static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter,
 					   u64 new_spte)
@@ -517,10 +690,20 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	 * Note, fast_pf_fix_direct_spte() can also modify TDP MMU SPTEs and
 	 * does not hold the mmu_lock.
 	 */
+	/*
+	 * cmpxchg(void* ptr, int old, int new), 如果ptr和old的值一样,则把new写到ptr内存,
+	 * 否则返回ptr的值. 整个操作是原子的.
+	 * 在Intel平台下,会用lock cmpxchg来实现,这里的lock个人理解是锁住内存总线,
+	 * 这样如果有另一个线程想访问ptr的内存,就会被block住.
+	 */
 	if (cmpxchg64(rcu_dereference(iter->sptep), iter->old_spte,
 		      new_spte) != iter->old_spte)
 		return false;
 
+	/*
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, true);
 	handle_changed_spte_acc_track(iter->old_spte, new_spte, iter->level);
@@ -528,6 +711,12 @@ static inline bool tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|810| <<zap_gfn_range>> } else if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1044| <<kvm_tdp_mmu_map>> if (!tdp_mmu_zap_spte_atomic(vcpu->kvm, &iter))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1460| <<zap_collapsible_spte_range>> if (!tdp_mmu_zap_spte_atomic(kvm, &iter)) {
+ */
 static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
 					   struct tdp_iter *iter)
 {
@@ -573,10 +762,25 @@ static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  *		      Leaving record_dirty_log unset in that case prevents page
  *		      writes from being double counted.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|822| <<tdp_mmu_set_spte>> __tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|833| <<tdp_mmu_set_spte_no_acc_track>> __tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|844| <<tdp_mmu_set_spte_no_dirty_log>> __tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
+ */
 static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				      u64 new_spte, bool record_acc_track,
 				      bool record_dirty_log)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON_ONCE(iter->yielded);
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
@@ -588,10 +792,18 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 	 * should be used. If operating under the MMU lock in write mode, the
 	 * use of the removed SPTE should not be necessary.
 	 */
+	/*
+	 * return spte == REMOVED_SPTE
+	 */
 	WARN_ON(is_removed_spte(iter->old_spte));
 
 	WRITE_ONCE(*rcu_dereference(iter->sptep), new_spte);
 
+	/*
+	 * 应该就是记录stat吧
+	 * Handle bookkeeping that might result from the modification of a SPTE.
+	 * This function must be called for all TDP SPTE modifications.
+	 */
 	__handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			      new_spte, iter->level, false);
 	if (record_acc_track)
@@ -603,12 +815,23 @@ static inline void __tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					      iter->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|852| <<zap_gfn_range>> tdp_mmu_set_spte(kvm, &iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, 0);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1258| <<set_spte_gfn>> tdp_mmu_set_spte(kvm, iter, new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1565| <<write_protect_gfn>> tdp_mmu_set_spte(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 				    u64 new_spte)
 {
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1282| <<age_gfn_range>> tdp_mmu_set_spte_no_acc_track(kvm, iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -616,6 +839,10 @@ static inline void tdp_mmu_set_spte_no_acc_track(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, false, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1522| <<clear_dirty_pt_masked>> tdp_mmu_set_spte_no_dirty_log(kvm, &iter, new_spte);
+ */
 static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 						 struct tdp_iter *iter,
 						 u64 new_spte)
@@ -623,6 +850,10 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 	__tdp_mmu_set_spte(kvm, iter, new_spte, true, false);
 }
 
+/*
+ * Iterates over every SPTE mapping the GFN range [start, end) in a
+ * preorder traversal.
+ */
 #define tdp_root_for_each_pte(_iter, _root, _start, _end) \
 	for_each_tdp_pte(_iter, _root->spt, _root->role.level, _start, _end)
 
@@ -633,6 +864,12 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
 			continue;					\
 		else
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1213| <<kvm_tdp_mmu_map>> tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1777| <<kvm_tdp_mmu_get_walk>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1812| <<kvm_tdp_mmu_fast_pf_get_last_sptep>> tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+ */
 #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
 	for_each_tdp_pte(_iter, __va(_mmu->root_hpa),		\
 			 _mmu->shadow_root_level, _start, _end)
@@ -651,10 +888,26 @@ static inline void tdp_mmu_set_spte_no_dirty_log(struct kvm *kvm,
  *
  * Returns true if this function yielded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|736| <<zap_gfn_range>> tdp_mmu_iter_cond_resched(kvm, &iter, flush, shared)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1196| <<wrprot_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1259| <<clear_dirty_gfn_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1391| <<zap_collapsible_spte_range>> if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
+ */
 static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 							  struct tdp_iter *iter,
 							  bool flush, bool shared)
 {
+	/*
+	 * True if KVM dropped mmu_lock and yielded in the middle of a walk, in
+	 * which case tdp_iter_next() needs to restart the walk at the root
+	 * level instead of advancing to the next entry.
+	 *
+	 * 在以下设置tdp_iter->yielded:
+	 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+	 */
 	WARN_ON(iter->yielded);
 
 	/* Ensure forward progress has been made before yielding. */
@@ -676,6 +929,17 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
 
 		WARN_ON(iter->gfn > iter->next_last_level_gfn);
 
+		/*
+		 * 在以下设置tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|29| <<tdp_iter_restart>> iter->yielded = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|679| <<tdp_mmu_iter_cond_resched>> iter->yielded = true;
+		 * 在以下使用tdp_iter->yielded:
+		 *   - arch/x86/kvm/mmu/tdp_iter.c|164| <<tdp_iter_next>> if (iter->yielded) {
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|505| <<tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|580| <<__tdp_mmu_set_spte>> WARN_ON_ONCE(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_iter_cond_resched>> WARN_ON(iter->yielded);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|682| <<tdp_mmu_iter_cond_resched>> return iter->yielded;
+		 */
 		iter->yielded = true;
 	}
 
@@ -699,6 +963,13 @@ static inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,
  * structures concurrently. If shared is false, this thread should hold the
  * MMU lock in write mode.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|145| <<kvm_tdp_mmu_put_root>> zap_gfn_range(kvm, root, 0, -1ull, false, false, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|954| <<__kvm_tdp_mmu_zap_gfn_range>> flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1019| <<kvm_tdp_mmu_zap_invalidated_roots>> flush = zap_gfn_range(kvm, root, 0, -1ull, true, flush, true);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1221| <<kvm_tdp_mmu_unmap_gfn_range>> flush = zap_gfn_range(kvm, root, range->start, range->end,
+ */
 static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 			  bool shared)
@@ -720,10 +991,17 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 	 */
 	end = min(end, max_gfn_host);
 
+	/*
+	 * kvm->mmu_lock
+	 */
 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 */
 	for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
 				   min_level, start, end) {
 retry:
@@ -770,11 +1048,34 @@ static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * SPTEs have been cleared and a TLB flush is needed before releasing the
  * MMU lock.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|27| <<kvm_tdp_mmu_zap_gfn_range>> return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
+ *   - arch/x86/kvm/mmu/tdp_mmu.h|43| <<kvm_tdp_mmu_zap_sp>> return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
+ */
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * zap_gfn_range()注释:
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 *
+	 * If can_yield is true, will release the MMU lock and reschedule if the
+	 * scheduler needs the CPU or there is contention on the MMU lock. If this
+	 * function cannot yield, it will not release the MMU lock or reschedule and
+	 * the caller must ensure it does not supply too large a GFN range, or the
+	 * operation can cause a soft lockup.
+	 *
+	 * If shared is true, this thread holds the MMU lock in read mode and must
+	 * account for the possibility that other threads are modifying the paging
+	 * structures concurrently. If shared is false, this thread should hold the
+	 * MMU lock in write mode.
+	 */
 	for_each_tdp_mmu_root_yield_safe(kvm, root, as_id, false)
 		flush = zap_gfn_range(kvm, root, start, end, can_yield, flush,
 				      false);
@@ -782,6 +1083,10 @@ bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 	return flush;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6028| <<kvm_mmu_zap_all>> kvm_tdp_mmu_zap_all(kvm);
+ */
 void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 {
 	bool flush = false;
@@ -794,6 +1099,11 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
 		kvm_flush_remote_tlbs(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1047| <<kvm_tdp_mmu_zap_invalidated_roots>> root = next_invalidated_root(kvm, NULL);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1050| <<kvm_tdp_mmu_zap_invalidated_roots>> next_root = next_invalidated_root(kvm, root);
+ */
 static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
 						  struct kvm_mmu_page *prev_root)
 {
@@ -824,6 +1134,10 @@ static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
  * only has to do a trivial amount of work. Since the roots are invalid,
  * no new SPTEs should be created under them.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5742| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *next_root;
@@ -880,10 +1194,27 @@ void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
  * This has essentially the same effect for the TDP MMU
  * as updating mmu_valid_gen does for the shadow MMU.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5724| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_invalidate_all_roots(kvm);
+ */
 void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * List of struct kvm_mmu_pages being used as roots.
+	 * All struct kvm_mmu_pages in the list should have
+	 * tdp_mmu_page set.
+	 *
+	 * For reads, this list is protected by:
+	 *      the MMU lock in read mode + RCU or
+	 *      the MMU lock in write mode
+	 *
+	 * For writes, this list is protected by:
+	 *      the MMU lock in read mode + the tdp_mmu_pages_lock or
+	 *      the MMU lock in write mode
+	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
 	list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link)
 		if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
@@ -894,6 +1225,20 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()
+ *                -> tdp_mmu_map_handle_target_level()
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1274| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -950,6 +1295,38 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * 从ept的handle_ept_violation()过来的fault的例子.
+ * handle_ept_violation()
+ * -> kvm_mmu_page_fault()
+ *    -> kvm_mmu_do_page_fault()
+ *       -> kvm_tdp_page_fault()
+ *          -> direct_page_fault()
+ *             -> fast_page_fault()
+ *             -> kvm_tdp_mmu_map()或者__direct_map()
+ *
+ * 在kvm_mmu_do_page_fault()
+ * 221         struct kvm_page_fault fault = {
+ * 222                 .addr = cr2_or_gpa,
+ * 223                 .error_code = err,
+ * 224                 .exec = err & PFERR_FETCH_MASK,
+ * 225                 .write = err & PFERR_WRITE_MASK,
+ * 226                 .present = err & PFERR_PRESENT_MASK,
+ * 227                 .rsvd = err & PFERR_RSVD_MASK,
+ * 228                 .user = err & PFERR_USER_MASK,
+ * 229                 .prefetch = prefetch,
+ * 230                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 231                 .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+ * 232 
+ * 233                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 234                 .req_level = PG_LEVEL_4K,
+ * 235                 .goal_level = PG_LEVEL_4K,
+ * 236         };
+ * 此外,在kvm_tdp_page_fault()修改过"--fault->max_level"
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4249| <<direct_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -961,14 +1338,39 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在以下调用trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3266| <<__direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1335| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	rcu_read_lock();
 
+	/*
+	 * Iterates over every SPTE mapping the GFN range [start, end) in a
+	 * preorder traversal.
+	 * end是')'
+	 */
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu.h|205| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
+		 *   - arch/x86/kvm/mmu/mmu.c|3100| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3179| <<__direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|731| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1252| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *
+		 * 核心思想: 本来是想用huge page的,但是已经存在一个present的pte但是不是large pte,
+		 * 所以就没法huge page了, 只能让goal_level再下降一位
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
 
+		/*
+		 * 如果到了想要的level, 必然要退出
+		 */
 		if (iter.level == fault->goal_level)
 			break;
 
@@ -990,21 +1392,45 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
 		}
 
+		/*
+		 * iter.old_spte: snapshot of the value at sptep
+		 */
 		if (!is_shadow_present_pte(iter.old_spte)) {
 			/*
 			 * If SPTE has been frozen by another thread, just
 			 * give up and retry, avoiding unnecessary page table
 			 * allocation and free.
 			 */
+			/*
+			 * 关于REMOVED_SPTE:
+			 * If a thread running without exclusive control of the MMU lock must perform a
+			 * multi-part operation on an SPTE, it can set the SPTE to REMOVED_SPTE as a
+			 * non-present intermediate value. Other threads which encounter this value
+			 * should not modify the SPTE.
+			 *
+			 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+			 * bot AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+			 * vulnerability.  Use only low bits to avoid 64-bit immediates.
+			 *
+			 * Only used by the TDP MMU.
+			 */
 			if (is_removed_spte(iter.old_spte))
 				break;
 
 			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
 			child_pt = sp->spt;
 
+			/*
+			 * 在以下使用shadow_accessed_mask:
+			 *   - arch/x86/kvm/mmu/spte.c|330| <<kvm_mmu_set_ept_masks>> shadow_accessed_mask = has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;
+			 *   - arch/x86/kvm/mmu/spte.c|382| <<kvm_mmu_reset_all_pte_masks>> shadow_accessed_mask = PT_ACCESSED_MASK;
+			 */
 			new_spte = make_nonleaf_spte(child_pt,
 						     !shadow_accessed_mask);
 
+			/*
+			 * 走到这里说明iter.old_spte不present
+			 */
 			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
 				tdp_mmu_link_page(vcpu->kvm, sp,
 						  fault->huge_page_disallowed &&
@@ -1023,6 +1449,10 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		return RET_PF_RETRY;
 	}
 
+	/*
+	 * Installs a last-level SPTE to handle a TDP page fault.
+	 * (NPT/EPT violation/misconfiguration)
+	 */
 	ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
 	rcu_read_unlock();
 
@@ -1169,6 +1599,10 @@ bool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  * [start, end). Returns true if an SPTE has been changed and the TLBs need to
  * be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1648| <<kvm_tdp_mmu_wrprot_slot>> spte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn,
+ */
 static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 			     gfn_t start, gfn_t end, int min_level)
 {
@@ -1213,6 +1647,10 @@ static bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  * only affect leaf SPTEs down to min_level.
  * Returns true if an SPTE has been changed and the TLBs need to be flushed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6147| <<kvm_mmu_slot_remove_write_access>> flush |= kvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);
+ */
 bool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *slot, int min_level)
 {
@@ -1349,6 +1787,11 @@ static void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,
  * clearing the dirty status will involve clearing the dirty bit on each SPTE
  * or, if AD bits are not enabled, clearing the writable bit on each SPTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1311| <<kvm_mmu_write_protect_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ *   - arch/x86/kvm/mmu/mmu.c|1344| <<kvm_mmu_clear_dirty_pt_masked>> kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
+ */
 void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				       struct kvm_memory_slot *slot,
 				       gfn_t gfn, unsigned long mask,
@@ -1365,6 +1808,10 @@ void kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,
  * Clear leaf entries which could be replaced by large mappings, for
  * GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1641| <<kvm_tdp_mmu_zap_collapsible_sptes>> zap_collapsible_spte_range(kvm, root, slot);
+ */
 static void zap_collapsible_spte_range(struct kvm *kvm,
 				       struct kvm_mmu_page *root,
 				       const struct kvm_memory_slot *slot)
@@ -1409,11 +1856,18 @@ static void zap_collapsible_spte_range(struct kvm *kvm,
  * Clear non-leaf entries (and free associated page tables) which could
  * be replaced by large mappings, for GFNs within the slot.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5957| <<kvm_mmu_zap_collapsible_sptes>> kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);
+ */
 void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot)
 {
 	struct kvm_mmu_page *root;
 
+	/*
+	 * read lock !!!!
+	 */
 	lockdep_assert_held_read(&kvm->mmu_lock);
 
 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
@@ -1425,6 +1879,10 @@ void kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1657| <<kvm_tdp_mmu_write_protect_gfn>> spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
+ */
 static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
 			      gfn_t gfn, int min_level)
 {
@@ -1462,6 +1920,10 @@ static bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,
  * MMU-writable bit to ensure future writes continue to be intercepted.
  * Returns true if an SPTE was set and a TLB flush is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1425| <<kvm_mmu_slot_gfn_write_protect>> kvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);
+ */
 bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level)
@@ -1470,6 +1932,12 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 	bool spte_set = false;
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * write_protect_gfn():
+	 *   Removes write access on the last level SPTE mapping this GFN and unsets the
+	 *   MMU-writable bit to ensure future writes continue to be intercepted.
+	 *   Returns true if an SPTE was set and a TLB flush is needed.
+	 */
 	for_each_tdp_mmu_root(kvm, root, slot->as_id)
 		spte_set |= write_protect_gfn(kvm, root, gfn, min_level);
 
@@ -1482,6 +1950,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
  *
  * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3823| <<get_mmio_spte>> leaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);
+ */
 int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 			 int *root_level)
 {
@@ -1494,6 +1966,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 		leaf = iter.level;
+		/*
+		 * 从get_mmio_spte()来的时候:
+		 * u64 sptes[PT64_ROOT_MAX_LEVEL + 1];
+		 */
 		sptes[leaf] = iter.old_spte;
 	}
 
@@ -1511,6 +1987,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
  *
  * WARNING: This function is only intended to be called during fast_page_fault.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3173| <<fast_page_fault>> sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte)
 {
@@ -1520,6 +2000,10 @@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 	tdp_ptep_t sptep = NULL;
 
 	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
+		/*
+		 * iter.old_spte : A snapshot of the value at sptep
+		 * iter.sptep    : A pointer to the current SPTE
+		 */
 		*spte = iter.old_spte;
 		sptep = iter.sptep;
 	}
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index 3899004a5d91..8dd0d8311add 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -7,12 +7,29 @@
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|182| <<tdp_mmu_next_root>> while (next_root && !kvm_tdp_mmu_get_root(kvm, next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|297| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(kvm, root))
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm *kvm,
 						     struct kvm_mmu_page *root)
 {
 	if (root->role.invalid)
 		return false;
 
+	/*
+	 * increment a refcount unless it is 0
+	 * Return: true if the increment was successful, false otherwise
+	 */
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
 }
 
@@ -21,11 +38,20 @@ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 
 bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
 				 gfn_t end, bool can_yield, bool flush);
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5809| <<kvm_zap_gfn_range>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, gfn_start,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|801| <<kvm_tdp_mmu_zap_all>> flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, 0, -1ull, flush);
+ */
 static inline bool kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id,
 					     gfn_t start, gfn_t end, bool flush)
 {
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
 }
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6301| <<kvm_recover_nx_lpages>> flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
+ */
 static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	gfn_t end = sp->gfn + KVM_PAGES_PER_HPAGE(sp->role.level + 1);
@@ -40,6 +66,12 @@ static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	 * of the shadow page's gfn range and stop iterating before yielding.
 	 */
 	lockdep_assert_held_write(&kvm->mmu_lock);
+	/*
+	 * Tears down the mappings for the range of gfns, [start, end), and frees the
+	 * non-root pages mapping GFNs strictly within that range. Returns true if
+	 * SPTEs have been cleared and a TLB flush is needed before releasing the
+	 * MMU lock.
+	 */
 	return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
 					   sp->gfn, end, false, false);
 }
@@ -71,6 +103,10 @@ bool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,
 				   struct kvm_memory_slot *slot, gfn_t gfn,
 				   int min_level);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|696| <<walk_shadow_page_lockless_begin>> kvm_tdp_mmu_walk_lockless_begin();
+ */
 static inline void kvm_tdp_mmu_walk_lockless_begin(void)
 {
 	rcu_read_lock();
@@ -105,6 +141,24 @@ static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
 	 * pae_root page, not a shadow page.
 	 */
 	sp = to_shadow_page(hpa);
+	/*
+	 * 在以下设置kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|3358| <<mmu_alloc_root>> ++sp->root_count;
+	 *   - arch/x86/kvm/mmu/mmu.c|3255| <<mmu_free_root_page>> else if (!--sp->root_count && sp->role.invalid)
+	 * 在以下使用kvm_mmu_page->root_count:
+	 *   - arch/x86/kvm/mmu/mmu.c|2364| <<__kvm_mmu_prepare_zap_page>> if (!sp->root_count) {
+	 *   - arch/x86/kvm/mmu/mmu.c|2430| <<kvm_mmu_commit_zap_page>> WARN_ON(!sp->role.invalid || sp->root_count);
+	 *   - arch/x86/kvm/mmu/mmu.c|2453| <<kvm_mmu_zap_oldest_mmu_pages>> if (sp->root_count)
+	 *   - arch/x86/kvm/mmu/mmutrace.h|22| <<KVM_MMU_PAGE_ASSIGN>> __entry->root_count = sp->root_count; \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|108| <<is_tdp_mmu>> return sp && is_tdp_mmu_page(sp) && sp->root_count;
+	 *
+	 * 在以下使用tdp_mmu_root_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|119| <<kvm_tdp_mmu_put_root>> if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|302| <<kvm_tdp_mmu_get_vcpu_root_hpa>> refcount_set(&root->tdp_mmu_root_count, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1048| <<next_invalidated_root>> refcount_read(&next_root->tdp_mmu_root_count)))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1147| <<kvm_tdp_mmu_invalidate_all_roots>> if (refcount_inc_not_zero(&root->tdp_mmu_root_count))
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|20| <<kvm_tdp_mmu_get_root>> return refcount_inc_not_zero(&root->tdp_mmu_root_count);
+	 */
 	return sp && is_tdp_mmu_page(sp) && sp->root_count;
 }
 #else
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 17b53457d866..e86056157d89 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -234,6 +234,10 @@ static void sev_unbind_asid(struct kvm *kvm, unsigned int handle)
 	sev_decommission(handle);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/sev.c|1795| <<svm_mem_enc_op>> r = sev_guest_init(kvm, &sev_cmd);
+ */
 static int sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp)
 {
 	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index aa1fe9085d77..2199288bf023 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -27,6 +27,17 @@ static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
  * CPU.  IRQs must be disabled when taking this lock, otherwise deadlock will
  * occur if a wakeup IRQ arrives and attempts to acquire the lock.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu_lock:
+ *   - arch/x86/kvm/vmx/posted_intr.c|30| <<global>> static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
+ *   - arch/x86/kvm/vmx/posted_intr.c|90| <<vmx_vcpu_pi_load>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|92| <<vmx_vcpu_pi_load>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|152| <<pi_enable_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_enable_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|207| <<pi_wakeup_handler>> raw_spin_lock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|214| <<pi_wakeup_handler>> raw_spin_unlock(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|220| <<pi_init_cpu>> raw_spin_lock_init(&per_cpu(wakeup_vcpus_on_cpu_lock, cpu));
+ */
 static DEFINE_PER_CPU(raw_spinlock_t, wakeup_vcpus_on_cpu_lock);
 
 static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index b730d799c26e..6b03300e5c27 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -5307,6 +5307,9 @@ static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * kvm_vmx_exit_handlers[EXIT_REASON_APIC_WRITE]
+ */
 static int handle_apic_write(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -7890,6 +7893,9 @@ static __init void vmx_setup_user_return_msrs(void)
 
 static struct kvm_x86_init_ops vmx_init_ops __initdata;
 
+/*
+ * struct kvm_x86_init_ops vmx_init_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index eb4029660bd9..e28c47cff6b0 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -3326,6 +3326,10 @@ void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_service_local_tlb_flush_requests);
 
+/*
+ * 处理KVM_REQ_STEAL_UPDATE:
+ *   - arch/x86/kvm/x86.c|9907| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct gfn_to_hva_cache *ghc = &vcpu->arch.st.cache;
@@ -8563,6 +8567,22 @@ static int kvmclock_cpu_down_prep(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * "constant tsc" (或者"synchronized tsc")是指"The TSC is synchronized
+ * across all sockets/cores".
+ *
+ * 下面是"/proc/cpuinfo"中部分关于tsc的解释
+ *
+ * tsc          : The system has a TSC clock
+ * rdtscp       : The RDTSCP instruction is available
+ * constant_tsc : The TSC is synchronized across all sockets/cores
+ * nonstop_tsc  : The TSC is not affected by power management code
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8658| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|8685| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|8712| <<kvmclock_cpu_online>> tsc_khz_changed(NULL);
+ */
 static void tsc_khz_changed(void *data)
 {
 	struct cpufreq_freqs *freq = data;
@@ -8791,6 +8811,14 @@ static struct notifier_block pvclock_gtod_notifier = {
 };
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5664| <<kvm_init>> r = kvm_arch_init(opaque);
+ *
+ * vmx_init()
+ * -> kvm_init()
+ *    -> kvm_arch_init()
+ */
 int kvm_arch_init(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -8999,6 +9027,18 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, unsigned long flags, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|230| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4212| <<kvm_faultin_pfn>> !kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/avic.c|182| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|247| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|558| <<nested_vmcb02_prepare_control>> WARN_ON(kvm_apicv_activated(svm->vcpu.kvm));
+ *   - arch/x86/kvm/svm/svm.c|1393| <<svm_set_vintr>> WARN_ON(kvm_apicv_activated(svm->vcpu.kvm));
+ *   - arch/x86/kvm/x86.c|9701| <<kvm_vcpu_update_apicv>> activate = kvm_apicv_activated(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|10105| <<vcpu_enter_guest>> WARN_ON_ONCE(kvm_apicv_activated(vcpu->kvm) != kvm_vcpu_apicv_active(vcpu));
+ *   - arch/x86/kvm/x86.c|11159| <<kvm_arch_vcpu_create>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
@@ -9234,6 +9274,11 @@ int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 	return kvm_x86_ops.nested_ops->check_events(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9260| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ *   - arch/x86/kvm/x86.c|9323| <<inject_pending_event>> kvm_inject_exception(vcpu);
+ */
 static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.exception.error_code && !is_protmode(vcpu))
@@ -9687,6 +9732,12 @@ void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|122| <<synic_update_vector>> __kvm_request_apicv_update(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|9768| <<kvm_request_apicv_update>> __kvm_request_apicv_update(kvm, activate, bit);
+ *   - arch/x86/kvm/x86.c|10952| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_request_apicv_update(kvm, !inhibit, APICV_INHIBIT_REASON_BLOCKIRQ);
+ */
 void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	unsigned long old, new;
@@ -9729,6 +9780,17 @@ void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 }
 EXPORT_SYMBOL_GPL(__kvm_request_apicv_update);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, false,
+ *   - arch/x86/kvm/i8254.c|315| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, true,
+ *   - arch/x86/kvm/svm/svm.c|2909| <<interrupt_window_interception>> kvm_request_apicv_update(vcpu->kvm, true, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3507| <<svm_enable_irq_window>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3980| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false,
+ *   - arch/x86/kvm/svm/svm.c|3988| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false,
+ *   - arch/x86/kvm/x86.c|5918| <<kvm_vm_ioctl_enable_cap>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|6300| <<kvm_arch_vm_ioctl>> kvm_request_apicv_update(kvm, true, APICV_INHIBIT_REASON_ABSENT);
+ */
 void kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	down_write(&kvm->arch.apicv_update_lock);
@@ -10290,6 +10352,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10340| <<complete_emulated_pio>> return complete_emulated_io(vcpu);
+ *   - arch/x86/kvm/x86.c|10393| <<complete_emulated_mmio>> return complete_emulated_io(vcpu);
+ */
 static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -11062,6 +11129,10 @@ static void store_regs(struct kvm_vcpu *vcpu)
 				vcpu, &vcpu->run->s.regs.events);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10478| <<kvm_arch_vcpu_ioctl_run>> r = sync_regs(vcpu);
+ */
 static int sync_regs(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {
@@ -11643,6 +11714,10 @@ static void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)
 	vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11782| <<kvm_arch_destroy_vm>> kvm_free_vcpus(kvm);
+ */
 static void kvm_free_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -11688,6 +11763,16 @@ void kvm_arch_sync_events(struct kvm *kvm)
  * address, i.e. its accessibility is not guaranteed, and must be
  * accessed via __copy_{to,from}_user().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|220| <<avic_alloc_access_page>> ret = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3626| <<init_rmode_identity_map>> uaddr = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3675| <<alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx/vmx.c|4728| <<vmx_set_tss_addr>> ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|11838| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|11840| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|11842| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 void __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
 				      u32 size)
 {
@@ -11746,6 +11831,11 @@ void kvm_arch_pre_destroy_vm(struct kvm *kvm)
 	kvm_mmu_pre_destroy_vm(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1150| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+ *   - virt/kvm/kvm_main.c|1217| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+ */
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	if (current->mm == kvm->mm) {
@@ -12180,6 +12270,10 @@ void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 }
 EXPORT_SYMBOL_GPL(kvm_set_rflags);
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|149| <<kvm_check_async_pf_completion>> kvm_arch_async_page_ready(vcpu, work);
+ */
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
@@ -12499,6 +12593,10 @@ bool kvm_vector_hashing_enabled(void)
 	return vector_hashing;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3383| <<kvm_vcpu_halt>> bool halt_poll_allowed = !kvm_arch_no_poll(vcpu);
+ */
 bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 {
 	return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
diff --git a/drivers/acpi/acpi_processor.c b/drivers/acpi/acpi_processor.c
index 6737b1cbf6d6..bccb679b036a 100644
--- a/drivers/acpi/acpi_processor.c
+++ b/drivers/acpi/acpi_processor.c
@@ -168,6 +168,10 @@ int __weak arch_register_cpu(int cpu)
 
 void __weak arch_unregister_cpu(int cpu) {}
 
+/*
+ * called by:
+ *   - drivers/acpi/acpi_processor.c|302| <<acpi_processor_get_info>> int ret = acpi_processor_hotadd_init(pr);
+ */
 static int acpi_processor_hotadd_init(struct acpi_processor *pr)
 {
 	unsigned long long sta;
@@ -214,6 +218,10 @@ static inline int acpi_processor_hotadd_init(struct acpi_processor *pr)
 }
 #endif /* CONFIG_ACPI_HOTPLUG_CPU */
 
+/*
+ * called by:
+ *   - drivers/acpi/acpi_processor.c|374| <<acpi_processor_add>> result = acpi_processor_get_info(device);
+ */
 static int acpi_processor_get_info(struct acpi_device *device)
 {
 	union acpi_object object = { 0 };
@@ -350,6 +358,9 @@ static int acpi_processor_get_info(struct acpi_device *device)
  */
 static DEFINE_PER_CPU(void *, processor_device_array);
 
+/*
+ * struct acpi_scan_handler processor_handler.attach = acpi_processor_add()
+ */
 static int acpi_processor_add(struct acpi_device *device,
 					const struct acpi_device_id *id)
 {
diff --git a/drivers/acpi/bus.c b/drivers/acpi/bus.c
index 07f604832fd6..5ff640a5e333 100644
--- a/drivers/acpi/bus.c
+++ b/drivers/acpi/bus.c
@@ -424,6 +424,10 @@ static void acpi_bus_osc_negotiate_usb_control(void)
  * ---------------
  * Callback for all 'system-level' device notifications (values 0x00-0x7F).
  */
+/*
+ * 在以下使用acpi_bus_notify():
+ *   - drivers/acpi/bus.c|1286| <<acpi_bus_init>> acpi_install_notify_handler(ACPI_ROOT_OBJECT, ACPI_SYSTEM_NOTIFY, &acpi_bus_notify, NULL);
+ */
 static void acpi_bus_notify(acpi_handle handle, u32 type, void *data)
 {
 	struct acpi_device *adev;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 45c5c0e45e33..149a0ee34f90 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -1161,6 +1161,11 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/bus.c|492| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|387| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index 8b2e5ef15559..825de5774305 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -381,6 +381,10 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1160| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
@@ -2161,6 +2165,10 @@ static struct acpi_scan_handler generic_device_handler = {
 	.attach = acpi_generic_device_attach,
 };
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|2228| <<acpi_bus_attach>> ret = acpi_scan_attach_handler(device);
+ */
 static int acpi_scan_attach_handler(struct acpi_device *device)
 {
 	struct acpi_hardware_id *hwid;
@@ -2190,6 +2198,13 @@ static int acpi_scan_attach_handler(struct acpi_device *device)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|2249| <<acpi_bus_attach>> acpi_bus_attach(child, first_pass);
+ *   - drivers/acpi/scan.c|2280| <<acpi_scan_clear_dep_fn>> acpi_bus_attach(cdw->adev, true);
+ *   - drivers/acpi/scan.c|2438| <<acpi_bus_scan>> acpi_bus_attach(device, true);
+ *   - drivers/acpi/scan.c|2452| <<acpi_bus_scan>> acpi_bus_attach(device, false);
+ */
 static void acpi_bus_attach(struct acpi_device *device, bool first_pass)
 {
 	struct acpi_device *child;
@@ -2419,6 +2434,17 @@ EXPORT_SYMBOL_GPL(acpi_dev_get_first_consumer_dev);
  *
  * Must be called under acpi_scan_lock.
  */
+/*
+ * called by:
+ *   - drivers/acpi/dock.c|273| <<hotplug_dock_devices>> int ret = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|322| <<acpi_scan_device_check>> error = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|351| <<acpi_scan_bus_check>> error = acpi_bus_scan(adev->handle);
+ *   - drivers/acpi/scan.c|2603| <<acpi_scan_init>> if (acpi_bus_scan(ACPI_ROOT_OBJECT))
+ *   - drivers/acpi/scan.c|2664| <<acpi_table_events_fn>> acpi_bus_scan(ACPI_ROOT_OBJECT);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|428| <<acpiphp_rescan_slot>> acpi_bus_scan(adev->handle);
+ *   - drivers/platform/surface/surface3-wmi.c|232| <<s3_wmi_probe>> acpi_bus_scan(s3_wmi.pnp0c0d_adev->handle);
+ *   - drivers/platform/surface/surface3-wmi.c|242| <<s3_wmi_remove>> acpi_bus_scan(s3_wmi.pnp0c0d_adev->handle);
+ */
 int acpi_bus_scan(acpi_handle handle)
 {
 	struct acpi_device *device = NULL;
diff --git a/drivers/cpuidle/cpuidle-haltpoll.c b/drivers/cpuidle/cpuidle-haltpoll.c
index fcc53215bac8..8bd37881913e 100644
--- a/drivers/cpuidle/cpuidle-haltpoll.c
+++ b/drivers/cpuidle/cpuidle-haltpoll.c
@@ -99,12 +99,41 @@ static bool haltpoll_want(void)
 	return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
 }
 
+/*
+ * The x86 architecture support code recognizes three kernel command line
+ * options related to CPU idle time management: idle=poll, idle=halt, and
+ * idle=nomwait. The first two of them disable the acpi_idle and intel_idle
+ * drivers altogether, which effectively causes the entire CPUIdle subsystem to
+ * be disabled and makes the idle loop invoke the architecture support code to
+ * deal with idle CPUs. How it does that depends on which of the two parameters
+ * is added to the kernel command line. In the idle=halt case, the architecture
+ * support code will use the HLT instruction of the CPUs (which, as a rule,
+ * suspends the execution of the program and causes the hardware to attempt to
+ * enter the shallowest available idle state) for this purpose, and if
+ * idle=poll is used, idle CPUs will execute a more or less lightweight''
+ * sequence of instructions in a tight loop.  [Note that using ``idle=poll is
+ * somewhat drastic in many cases, as preventing idle CPUs from saving almost
+ * any energy at all may not be the only effect of it. For example, on Intel
+ * hardware it effectively prevents CPUs from using P-states (see CPU
+ * Performance Scaling) that require any number of CPUs in a package to be
+ * idle, so it very well may hurt single-thread computations performance as
+ * well as energy-efficiency. Thus using it for performance reasons may not be
+ * a good idea at all.
+ *
+ * The idle=nomwait option disables the intel_idle driver and causes acpi_idle
+ * to be used (as long as all of the information needed by it is there in the
+ * system’s ACPI tables), but it is not allowed to use the MWAIT instruction of
+ * the CPUs to ask the hardware to enter idle states.
+ */
 static int __init haltpoll_init(void)
 {
 	int ret;
 	struct cpuidle_driver *drv = &haltpoll_driver;
 
 	/* Do not load haltpoll if idle= is passed */
+	/*
+	 * IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT, IDLE_POLL
+	 */
 	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
 		return -ENODEV;
 
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 9394aa9444c1..9a2b3a09282a 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -412,6 +412,16 @@ static int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|735| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|773| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|796| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|824| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|911| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1154| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|2371| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+ */
 static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 {
 	struct mm_struct *mm;
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 47aebd98f52f..aabc084ca0dc 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -152,6 +152,11 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	return "";
 }
 
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|146| <<xen_mm_init>> rc = xen_swiotlb_init();
+ *   - arch/x86/xen/pci-swiotlb-xen.c|79| <<pci_xen_swiotlb_init_late>> rc = xen_swiotlb_init();
+ */
 int xen_swiotlb_init(void)
 {
 	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
diff --git a/include/linux/kvm_dirty_ring.h b/include/linux/kvm_dirty_ring.h
index 906f899813dc..b80548c292b5 100644
--- a/include/linux/kvm_dirty_ring.h
+++ b/include/linux/kvm_dirty_ring.h
@@ -23,6 +23,16 @@ struct kvm_dirty_ring {
 	u32 reset_index;
 	u32 size;
 	u32 soft_limit;
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	struct kvm_dirty_gfn *dirty_gfns;
 	int index;
 };
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f11039944c08..ead6bb9c4655 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -731,6 +731,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|843| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|927| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1107| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1272| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1287| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1313| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1327| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
diff --git a/include/linux/virtio_net.h b/include/linux/virtio_net.h
index a960de68ac69..d3a01ea878e5 100644
--- a/include/linux/virtio_net.h
+++ b/include/linux/virtio_net.h
@@ -43,6 +43,17 @@ static inline int virtio_net_hdr_set_proto(struct sk_buff *skb,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/vector_transports.c|212| <<raw_verify_header>> virtio_net_hdr_to_skb(skb, vheader, virtio_legacy_is_little_endian());
+ *   - drivers/net/tap.c|707| <<tap_get_user>> err = virtio_net_hdr_to_skb(skb, &vnet_hdr,
+ *   - drivers/net/tap.c|1159| <<tap_get_user_xdp>> err = virtio_net_hdr_to_skb(skb, gso, tap_is_little_endian(q));
+ *   - drivers/net/tun.c|1835| <<tun_get_user>> if (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {
+ *   - drivers/net/tun.c|2445| <<tun_xdp_one>> if (virtio_net_hdr_to_skb(skb, gso, tun_is_little_endian(tun))) {
+ *   - drivers/net/virtio_net.c|1164| <<receive_buf>> if (virtio_net_hdr_to_skb(skb, &hdr->hdr,
+ *   - net/packet/af_packet.c|2846| <<tpacket_snd>> if (virtio_net_hdr_to_skb(skb, vnet_hdr, vio_le())) {
+ *   - net/packet/af_packet.c|3048| <<packet_snd>> err = virtio_net_hdr_to_skb(skb, &vnet_hdr, vio_le());
+ */
 static inline int virtio_net_hdr_to_skb(struct sk_buff *skb,
 					const struct virtio_net_hdr *hdr,
 					bool little_endian)
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 6db1c475ec82..fd6c7c019c26 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -71,6 +71,20 @@
 
 enum swiotlb_force swiotlb_force;
 
+/*
+ * 在以下使用io_tlb_default_mem:
+ *   - drivers/base/core.c|2885| <<device_initialize>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ *   - drivers/xen/swiotlb-xen.c|164| <<xen_swiotlb_init>> if (io_tlb_default_mem.nslabs) {
+ *   - drivers/xen/swiotlb-xen.c|548| <<xen_swiotlb_dma_supported>> return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
+ *   - kernel/dma/swiotlb.c|107| <<swiotlb_max_segment>> return io_tlb_default_mem.nslabs ? max_segment : 0;
+ *   - kernel/dma/swiotlb.c|140| <<swiotlb_print_info>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|197| <<swiotlb_update_mem_attributes>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|250| <<swiotlb_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|352| <<swiotlb_late_init_with_tbl>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|377| <<swiotlb_exit>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|780| <<swiotlb_create_default_debugfs>> struct io_tlb_mem *mem = &io_tlb_default_mem;
+ *   - kernel/dma/swiotlb.c|882| <<rmem_swiotlb_device_release>> dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ */
 struct io_tlb_mem io_tlb_default_mem;
 
 phys_addr_t swiotlb_unencrypted_base;
@@ -81,6 +95,15 @@ phys_addr_t swiotlb_unencrypted_base;
  */
 static unsigned int max_segment;
 
+/*
+ * 在以下使用default_nslabs:
+ *   - kernel/dma/swiotlb.c|105| <<setup_io_tlb_npages>> default_nslabs =
+ *   - kernel/dma/swiotlb.c|135| <<swiotlb_size_or_default>> return default_nslabs << IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|145| <<swiotlb_adjust_size>> if (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)
+ *   - kernel/dma/swiotlb.c|148| <<swiotlb_adjust_size>> default_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|333| <<swiotlb_init>> size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|343| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 static unsigned long default_nslabs = IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT;
 
 static int __init
@@ -121,6 +144,10 @@ unsigned long swiotlb_size_or_default(void)
 	return default_nslabs << IO_TLB_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/mem_encrypt_amd.c|227| <<sev_setup_arch>> swiotlb_adjust_size(size);
+ */
 void __init swiotlb_adjust_size(unsigned long size)
 {
 	/*
@@ -135,6 +162,13 @@ void __init swiotlb_adjust_size(unsigned long size)
 	pr_info("SWIOTLB bounce buffer size adjusted to %luMB", size >> 20);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|23| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-swiotlb.c|75| <<pci_swiotlb_late_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|269| <<swiotlb_init_with_tbl>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> swiotlb_print_info();
+ */
 void swiotlb_print_info(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -211,6 +245,12 @@ void __init swiotlb_update_mem_attributes(void)
 	memset(mem->vaddr, 0, bytes);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+ *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+ *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ */
 static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 				    unsigned long nslabs, bool late_alloc)
 {
@@ -245,6 +285,13 @@ static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|248| <<plat_swiotlb_setup>> if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
+ *   - arch/powerpc/platforms/pseries/svm.c|56| <<svm_swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, false))
+ *   - drivers/xen/swiotlb-xen.c|255| <<xen_swiotlb_init_early>> if (swiotlb_init_with_tbl(start, nslabs, true))
+ *   - kernel/dma/swiotlb.c|291| <<swiotlb_init>> if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -263,6 +310,12 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 		      __func__, alloc_size, PAGE_SIZE);
 
+	/*
+	 * called by:
+	 *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+	 *   - kernel/dma/swiotlb.c|368| <<swiotlb_late_init_with_tbl>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
+	 *   - kernel/dma/swiotlb.c|865| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+	 */
 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
 
 	if (verbose)
@@ -275,6 +328,18 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * called by:
+ *   - arch/arm/mm/init.c|317| <<mem_init>> swiotlb_init(1);
+ *   - arch/arm64/mm/init.c|378| <<mem_init>> swiotlb_init(1); 
+ *   - arch/ia64/mm/init.c|441| <<mem_init>> swiotlb_init(1);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/powerpc/mm/mem.c|254| <<mem_init>> swiotlb_init(0);
+ *   - arch/riscv/mm/init.c|124| <<mem_init>> swiotlb_init(1);
+ *   - arch/s390/mm/init.c|189| <<pv_init>> swiotlb_init(1);
+ *   - arch/x86/kernel/pci-swiotlb.c|64| <<pci_swiotlb_init>> swiotlb_init(0);
+ */
 void  __init
 swiotlb_init(int verbose)
 {
@@ -303,6 +368,10 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/x86/pci/sta2x11-fixup.c|60| <<sta2x11_new_instance>> if (swiotlb_late_init_with_default_size(size))
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -758,6 +827,13 @@ size_t swiotlb_max_mapping_size(struct device *dev)
 	return ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE;
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|45| <<i915_gem_object_get_pages_internal>> if (is_swiotlb_active(obj->base.dev->dev)) {
+ *   - drivers/gpu/drm/nouveau/nouveau_ttm.c|279| <<nouveau_ttm_init>> need_swiotlb = is_swiotlb_active(dev->dev);
+ *   - drivers/pci/xen-pcifront.c|684| <<pcifront_connect_and_init_dma>> if (!err && !is_swiotlb_active(&pdev->xdev->dev)) {
+ *   - kernel/dma/direct.c|580| <<dma_direct_max_mapping_size>> if (is_swiotlb_active(dev) &&
+ */
 bool is_swiotlb_active(struct device *dev)
 {
 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
@@ -769,6 +845,11 @@ EXPORT_SYMBOL_GPL(is_swiotlb_active);
 #ifdef CONFIG_DEBUG_FS
 static struct dentry *debugfs_dir;
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|785| <<swiotlb_create_default_debugfs>> swiotlb_create_debugfs_files(mem);
+ *   - kernel/dma/swiotlb.c|802| <<rmem_swiotlb_debugfs_init>> swiotlb_create_debugfs_files(mem);
+ */
 static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem)
 {
 	debugfs_create_ulong("io_tlb_nslabs", 0400, mem->debugfs, &mem->nslabs);
diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
index 6fcdee7e87a5..f67507ca7a39 100644
--- a/kernel/power/suspend.c
+++ b/kernel/power/suspend.c
@@ -415,6 +415,9 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
 	if (suspend_test(TEST_PLATFORM))
 		goto Platform_wake;
 
+	/*
+	 * echo "freeze" > /sys/power/state到这里就结束了
+	 */
 	if (state == PM_SUSPEND_TO_IDLE) {
 		s2idle_loop();
 		goto Platform_wake;
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 1cf73807b450..0b7c96f7c713 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -378,6 +378,10 @@ void clocksource_verify_percpu(struct clocksource *cs)
 }
 EXPORT_SYMBOL_GPL(clocksource_verify_percpu);
 
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|520| <<clocksource_start_watchdog>> timer_setup(&watchdog_timer, clocksource_watchdog, 0);
+ */
 static void clocksource_watchdog(struct timer_list *unused)
 {
 	u64 csnow, wdnow, cslast, wdlast, delta;
diff --git a/tools/testing/selftests/kvm/kvm_page_table_test.c b/tools/testing/selftests/kvm/kvm_page_table_test.c
index ba1fdc3dcf4a..cdb88794e4fb 100644
--- a/tools/testing/selftests/kvm/kvm_page_table_test.c
+++ b/tools/testing/selftests/kvm/kvm_page_table_test.c
@@ -67,6 +67,11 @@ struct test_args {
  * Guest variables. Use addr_gva2hva() if these variables need
  * to be changed in host.
  */
+/*
+ * 在以下使用guest_test_stage:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+ */
 static enum test_stage guest_test_stage;
 
 /* Host variables */
@@ -90,12 +95,20 @@ static uint64_t guest_test_phys_mem;
  * Guest virtual memory offset of the testing memory slot.
  * Must not conflict with identity mapped test code.
  */
+/*
+ * 0xc0000000
+ */
 static uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;
 
 static void guest_code(int vcpu_id)
 {
 	struct test_args *p = &test_args;
 	struct vcpu_args *vcpu_args = &p->vcpu_args[vcpu_id];
+	/*
+	 * 在以下使用guest_test_stage:
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|99| <<guest_code>> enum test_stage *current_stage = &guest_test_stage;
+	 *   - tools/testing/selftests/kvm/kvm_page_table_test.c|321| <<pre_init_before_test>> current_stage = addr_gva2hva(vm, (vm_vaddr_t)(&guest_test_stage));
+	 */
 	enum test_stage *current_stage = &guest_test_stage;
 	uint64_t addr;
 	int i, j;
@@ -336,6 +349,13 @@ static struct kvm_vm *pre_init_before_test(enum vm_guest_mode mode, void *arg)
 	return vm;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|400| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|407| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|420| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|432| <<run_test>> vcpus_complete_new_stage(*current_stage);
+ */
 static void vcpus_complete_new_stage(enum test_stage stage)
 {
 	int ret;
diff --git a/tools/testing/selftests/kvm/lib/guest_modes.c b/tools/testing/selftests/kvm/lib/guest_modes.c
index 8784013b747c..981fc1a5a829 100644
--- a/tools/testing/selftests/kvm/lib/guest_modes.c
+++ b/tools/testing/selftests/kvm/lib/guest_modes.c
@@ -11,6 +11,16 @@ enum vm_guest_mode vm_mode_default;
 
 struct guest_mode guest_modes[NUM_VM_MODES];
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/access_tracking_perf_test.c|355| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|408| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|344| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|864| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|485| <<main>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|501| <<init_guest_modes>> guest_modes_append_default();
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|154| <<main>> guest_modes_append_default();
+ */
 void guest_modes_append_default(void)
 {
 #ifndef __aarch64__
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index d8cf851ab119..1c71854e485b 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -1064,6 +1064,12 @@ memslot2region(struct kvm_vm *vm, uint32_t memslot)
  * Sets the flags of the memory region specified by the value of slot,
  * to the values given by flags.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|104| <<toggle_dirty_logging>> vm_mem_region_set_flags(vm, slot, flags);
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|421| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX,
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|434| <<run_test>> vm_mem_region_set_flags(vm, TEST_MEM_SLOT_INDEX, 0);
+ */
 void vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)
 {
 	int ret;
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index 9bfe1d6f6529..475c21c3de57 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -134,6 +134,12 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4003| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|3610| <<kvm_set_msr_common(MSR_KVM_ASYNC_PF_ACK)>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|10013| <<vcpu_enter_guest(处理KVM_REQ_APF_READY)>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 222ecc81d7df..50be47c94c8f 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -147,6 +147,20 @@ int kvm_dirty_ring_reset(struct kvm *kvm, struct kvm_dirty_ring *ring)
 	return count;
 }
 
+/*
+ * 一个callstack的例子
+ * [0] kvm_dirty_ring_push
+ * [0] mark_page_dirty_in_slot
+ * [0] kvm_steal_time_set_preempted
+ * [0] kvm_arch_vcpu_put
+ * [0] vcpu_put
+ * [0] vmx_free_vcpu
+ * [0] kvm_arch_vcpu_destroy
+ * [0] kvm_vcpu_destroy
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|3171| <<mark_page_dirty_in_slot>> kvm_dirty_ring_push(&vcpu->dirty_ring,
+ */
 void kvm_dirty_ring_push(struct kvm_dirty_ring *ring, u32 slot, u64 offset)
 {
 	struct kvm_dirty_gfn *entry;
@@ -175,6 +189,16 @@ struct page *kvm_dirty_ring_get_page(struct kvm_dirty_ring *ring, u32 offset)
 
 void kvm_dirty_ring_free(struct kvm_dirty_ring *ring)
 {
+	/*
+	 * 在以下使用dirty_ring->dirty_gfns:
+	 *   - virt/kvm/dirty_ring.c|62| <<kvm_dirty_ring_alloc>> ring->dirty_gfns = vzalloc(size);
+	 *   - virt/kvm/dirty_ring.c|63| <<kvm_dirty_ring_alloc>> if (!ring->dirty_gfns)
+	 *   - virt/kvm/dirty_ring.c|103| <<kvm_dirty_ring_reset>> entry = &ring->dirty_gfns[ring->reset_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|157| <<kvm_dirty_ring_push>> entry = &ring->dirty_gfns[ring->dirty_index & (ring->size - 1)];
+	 *   - virt/kvm/dirty_ring.c|173| <<kvm_dirty_ring_get_page>> return vmalloc_to_page((void *)ring->dirty_gfns + offset * PAGE_SIZE);
+	 *   - virt/kvm/dirty_ring.c|178| <<kvm_dirty_ring_free>> vfree(ring->dirty_gfns);
+	 *   - virt/kvm/dirty_ring.c|179| <<kvm_dirty_ring_free>> ring->dirty_gfns = NULL;
+	 */
 	vfree(ring->dirty_gfns);
 	ring->dirty_gfns = NULL;
 }
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 0afc016cc54d..2496d06fa74a 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -435,6 +435,10 @@ static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 	vcpu->last_used_slot = NULL;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|460| <<kvm_destroy_vcpus>> kvm_vcpu_destroy(vcpu);
+ */
 static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_dirty_ring_free(&vcpu->dirty_ring);
@@ -451,6 +455,15 @@ static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|182| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/mips/kvm/mips.c|183| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/powerpc/kvm/powerpc.c|476| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/riscv/kvm/vm.c|49| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/s390/kvm/kvm-s390.c|2798| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/x86/kvm/x86.c|11672| <<kvm_free_vcpus>> kvm_destroy_vcpus(kvm);
+ */
 void kvm_destroy_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -947,6 +960,10 @@ static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4847| <<kvm_dev_ioctl_create_vm>> if (kvm_create_vm_debugfs(kvm, r) < 0) {
+ */
 static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 {
 	static DEFINE_MUTEX(kvm_debugfs_lock);
@@ -1049,6 +1066,10 @@ int __weak kvm_arch_create_vm_debugfs(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4831| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type);
+ */
 static struct kvm *kvm_create_vm(unsigned long type)
 {
 	struct kvm *kvm = kvm_arch_alloc_vm();
@@ -1083,6 +1104,17 @@ static struct kvm *kvm_create_vm(unsigned long type)
 	if (init_srcu_struct(&kvm->irq_srcu))
 		goto out_err_no_irq_srcu;
 
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|843| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|927| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1107| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1272| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1287| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1313| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1327| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_set(&kvm->users_count, 1);
 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
 		for (j = 0; j < 2; j++) {
@@ -1176,6 +1208,10 @@ static void kvm_destroy_devices(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1250| <<kvm_put_kvm>> kvm_destroy_vm(kvm);
+ */
 static void kvm_destroy_vm(struct kvm *kvm)
 {
 	int i;
@@ -1228,6 +1264,20 @@ static void kvm_destroy_vm(struct kvm *kvm)
 	mmdrop(mm);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1956| <<kvm_vm_ioctl_get_htab_fd>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1999| <<debugfs_htab_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1271| <<debugfs_radix_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|321| <<kvm_vm_ioctl_create_spapr_tce>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2673| <<debugfs_timings_open>> kvm_get_kvm(vcpu->kvm);
+ *   - arch/x86/kvm/svm/sev.c|2014| <<svm_vm_copy_asid_from>> kvm_get_kvm(source_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1941| <<kvmgt_guest_init>> kvm_get_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1096| <<vfio_ap_mdev_set_kvm>> kvm_get_kvm(kvm);
+ *   - virt/kvm/async_pf.c|196| <<kvm_setup_async_pf>> kvm_get_kvm(work->vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|3862| <<kvm_vm_ioctl_create_vcpu>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4346| <<kvm_ioctl_create_device>> kvm_get_kvm(kvm);
+ */
 void kvm_get_kvm(struct kvm *kvm)
 {
 	refcount_inc(&kvm->users_count);
@@ -1238,12 +1288,37 @@ EXPORT_SYMBOL_GPL(kvm_get_kvm);
  * Make sure the vm is not during destruction, which is a safe version of
  * kvm_get_kvm().  Return true if kvm referenced successfully, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|162| <<kvm_mmu_rmaps_stat_open>> if (!kvm_get_kvm_safe(kvm))
+ *   - virt/kvm/kvm_main.c|5336| <<kvm_debugfs_open>> if (!kvm_get_kvm_safe(stat_data->kvm))
+ */
 bool kvm_get_kvm_safe(struct kvm *kvm)
 {
 	return refcount_inc_not_zero(&kvm->users_count);
 }
 EXPORT_SYMBOL_GPL(kvm_get_kvm_safe);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1932| <<kvm_htab_release>> kvm_put_kvm(ctx->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|2011| <<debugfs_htab_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1283| <<debugfs_radix_release>> kvm_put_kvm(p->kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|267| <<kvm_spapr_tce_release>> kvm_put_kvm(stt->kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2684| <<debugfs_timings_release>> kvm_put_kvm(p->vcpu->kvm);
+ *   - arch/x86/kvm/debugfs.c|172| <<kvm_mmu_rmaps_stat_release>> kvm_put_kvm(kvm);
+ *   - arch/x86/kvm/svm/sev.c|2058| <<sev_vm_destroy>> kvm_put_kvm(owner_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1961| <<kvmgt_guest_exit>> kvm_put_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1167| <<vfio_ap_mdev_unset_kvm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/async_pf.c|91| <<async_pf_execute>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/async_pf.c|118| <<kvm_clear_async_pf_completion_queue>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|1273| <<kvm_vm_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|3668| <<kvm_vcpu_release>> kvm_put_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|4177| <<kvm_device_release>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4781| <<kvm_dev_ioctl_create_vm>> kvm_put_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|5254| <<kvm_debugfs_open>> kvm_put_kvm(stat_data->kvm);
+ *   - virt/kvm/kvm_main.c|5267| <<kvm_debugfs_release>> kvm_put_kvm(stat_data->kvm);
+ */
 void kvm_put_kvm(struct kvm *kvm)
 {
 	if (refcount_dec_and_test(&kvm->users_count))
@@ -3152,6 +3227,21 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1270| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3272| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|188| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|359| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3428| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|4604| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|244| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|366| <<__kvm_xen_has_interrupt>> mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2984| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3122| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3221| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3230| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3176,6 +3266,22 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty_in_slot);
 
+/*
+ * called by:
+ *   - arch/mips/kvm/mmu.c|547| <<_kvm_mips_map_page_fast>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|661| <<kvm_mips_map_page>> mark_page_dirty(kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|200| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|128| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_hv.c|884| <<kvmppc_copy_guest>> mark_page_dirty(kvm, to >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_xive_native.c|909| <<kvmppc_xive_native_vcpu_eq_sync>> mark_page_dirty(vcpu->kvm, gpa_to_gfn(q->guest_qaddr));
+ *   - arch/riscv/kvm/mmu.c|682| <<kvm_riscv_stage2_map>> mark_page_dirty(kvm, gfn);
+ *   - arch/s390/kvm/interrupt.c|2808| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->ind_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/interrupt.c|2814| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->summary_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/kvm-s390.c|636| <<kvm_arch_sync_dirty_log>> mark_page_dirty(kvm, cur_gfn + i);
+ *   - arch/s390/kvm/vsie.c|658| <<unpin_guest_page>> mark_page_dirty(kvm, gpa_to_gfn(gpa));
+ *   - include/linux/kvm_host.h|1175| <<__kvm_put_guest>> mark_page_dirty(kvm, gfn); \
+ *   - virt/kvm/pfncache.c|123| <<__release_gpc>> mark_page_dirty(kvm, gpa);
+ */
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -3185,6 +3291,15 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1492| <<kvm_hv_set_msr>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|291| <<FNAME>> kvm_vcpu_mark_page_dirty(vcpu, table_gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3752| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/nested.c|3757| <<nested_mark_vmcs12_pages_dirty>> kvm_vcpu_mark_page_dirty(vcpu, gfn);
+ *   - arch/x86/kvm/vmx/vmx.c|5809| <<vmx_flush_pml_buffer>> kvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|2799| <<kvm_vcpu_unmap>> kvm_vcpu_mark_page_dirty(vcpu, map->gfn);
+ */
 void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -3688,6 +3803,10 @@ static int create_vcpu_fd(struct kvm_vcpu *vcpu)
 	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3874| <<kvm_vm_ioctl_create_vcpu>> kvm_create_vcpu_debugfs(vcpu);
+ */
 static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
@@ -4273,6 +4392,11 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4724| <<kvm_vm_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(kvm, arg);
+ *   - virt/kvm/kvm_main.c|4889| <<kvm_dev_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
+ */
 static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 {
 	switch (arg) {
@@ -4459,6 +4583,9 @@ static int kvm_vm_ioctl_get_stats_fd(struct kvm *kvm)
 	return fd;
 }
 
+/*
+ * struct file_operations kvm_vm_fops.unlocked_ioctl = kvm_vm_ioctl()
+ */
 static long kvm_vm_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -4482,6 +4609,16 @@ static long kvm_vm_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_SET_USER_MEMORY_REGION: {
+		/*
+		 * // for KVM_SET_USER_MEMORY_REGION
+		 * struct kvm_userspace_memory_region {
+		 *     __u32 slot;
+		 *     __u32 flags;        // KVM_MEM_LOG_DIRTY_PAGES or KVM_MEM_READONLY
+		 *     __u64 guest_phys_addr;
+		 *     __u64 memory_size; // bytes
+		 *     __u64 userspace_addr; // start of the userspace allocated memory
+		 * };
+		 */
 		struct kvm_userspace_memory_region kvm_userspace_mem;
 
 		r = -EFAULT;
@@ -4720,6 +4857,12 @@ static long kvm_vm_compat_ioctl(struct file *filp,
 }
 #endif
 
+/*
+ * 在以下使用kvm_vm_fops:
+ *   - virt/kvm/kvm_main.c|4817| <<file_is_kvm>> return file && file->f_op == &kvm_vm_fops;
+ *   - virt/kvm/kvm_main.c|4846| <<kvm_dev_ioctl_create_vm>> file = anon_inode_getfile("kvm-vm", &kvm_vm_fops, kvm, O_RDWR);
+ *   - virt/kvm/kvm_main.c|5836| <<kvm_init>> kvm_vm_fops.owner = module;
+ */
 static struct file_operations kvm_vm_fops = {
 	.release        = kvm_vm_release,
 	.unlocked_ioctl = kvm_vm_ioctl,
@@ -4733,6 +4876,10 @@ bool file_is_kvm(struct file *file)
 }
 EXPORT_SYMBOL_GPL(file_is_kvm);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4874| <<kvm_dev_ioctl(KVM_CREATE_VM)>> r = kvm_dev_ioctl_create_vm(arg);
+ */
 static int kvm_dev_ioctl_create_vm(unsigned long type)
 {
 	int r;
@@ -4828,6 +4975,13 @@ static struct file_operations kvm_chardev_ops = {
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * 在以下使用kvm_dev:
+ *   - virt/kvm/kvm_main.c|5539| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|5580| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ *   - virt/kvm/kvm_main.c|5839| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|5885| <<kvm_exit>> misc_deregister(&kvm_dev);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
@@ -5437,6 +5591,11 @@ DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, vcpu_stat_clear,
 			"%llu\n");
 DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_readonly_fops, vcpu_stat_get, NULL, "%llu\n");
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1185| <<kvm_destroy_vm>> kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
+ *   - virt/kvm/kvm_main.c|4775| <<kvm_dev_ioctl_create_vm>> kvm_uevent_notify_change(KVM_EVENT_CREATE_VM, kvm);
+ */
 static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 {
 	struct kobj_uevent_env *env;
@@ -5654,6 +5813,18 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2240| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1649| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1065| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/riscv/kvm/main.c|124| <<riscv_kvm_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5075| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4914| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8166| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
-- 
2.17.1

