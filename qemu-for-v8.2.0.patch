From 02f0627890d7980811a3db231d0d916556ed394a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 24 Apr 2024 09:14:48 -0700
Subject: [PATCH 1/1] qemu for v8.2.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 accel/kvm/kvm-accel-ops.c         |   3 +
 accel/kvm/kvm-all.c               | 341 ++++++++++++++++++++
 backends/hostmem-file.c           |  64 ++++
 backends/hostmem-memfd.c          |   4 +
 backends/hostmem-ram.c            |  20 ++
 backends/hostmem.c                |  76 +++++
 block/block-backend.c             |  72 +++++
 block/file-posix.c                |  14 +
 block/io.c                        |   9 +
 cpu-common.c                      |  10 +
 hw/core/cpu-common.c              |   8 +
 hw/core/qdev.c                    |  29 ++
 hw/core/vm-change-state-handler.c |  21 ++
 hw/i386/intel_iommu.c             |   5 +
 hw/i386/kvm/apic.c                |  11 +
 hw/i386/x86.c                     |  27 ++
 hw/input/pckbd.c                  |  44 +++
 hw/intc/apic_common.c             |  28 ++
 hw/net/virtio-net.c               |  21 ++
 hw/scsi/scsi-bus.c                |  73 +++++
 hw/scsi/scsi-disk.c               |  15 +
 hw/vfio/common.c                  |  26 ++
 hw/vfio/container.c               |   6 +
 hw/vfio/helpers.c                 |   6 +
 hw/vfio/pci.c                     |   4 +
 hw/virtio/vhost.c                 | 259 ++++++++++++++++
 hw/virtio/virtio.c                |  67 ++++
 include/exec/ram_addr.h           |  46 +++
 include/exec/ramblock.h           |  34 ++
 include/exec/ramlist.h            |  14 +
 include/hw/core/cpu.h             |  29 ++
 include/hw/scsi/scsi.h            |  12 +
 include/hw/virtio/vhost.h         |  19 ++
 include/hw/virtio/virtio.h        |  22 ++
 include/migration/register.h      |  48 +++
 include/migration/vmstate.h       |  55 ++++
 include/qemu/thread-context.h     |   8 +
 include/sysemu/hostmem.h          |  18 ++
 include/sysemu/kvm_int.h          |  39 +++
 io/channel-socket.c               |  12 +
 io/task.c                         |  16 +
 migration/channel.c               |  21 ++
 migration/dirtyrate.c             |  10 +
 migration/file.c                  |  12 +
 migration/migration-hmp-cmds.c    | 108 +++++++
 migration/migration-stats.c       |  12 +
 migration/migration-stats.h       |  44 +++
 migration/migration.c             | 496 ++++++++++++++++++++++++++++++
 migration/migration.h             |  94 ++++++
 migration/multifd.c               | 203 ++++++++++++
 migration/multifd.h               |   9 +
 migration/options.c               |  72 +++++
 migration/qemu-file.c             | 134 ++++++++
 migration/ram.c                   | 464 ++++++++++++++++++++++++++++
 migration/ram.h                   |  32 ++
 migration/savevm.c                | 338 ++++++++++++++++++++
 migration/savevm.h                |   7 +
 migration/socket.c                |  63 ++++
 migration/vmstate.c               | 138 +++++++++
 monitor/hmp.c                     |  21 ++
 monitor/qmp.c                     |  15 +
 qapi/qmp-dispatch.c               |  29 ++
 qom/qom-qmp-cmds.c                |  46 +++
 system/cpus.c                     |  40 +++
 system/main.c                     |   4 +
 system/memory.c                   | 230 ++++++++++++++
 system/physmem.c                  |  93 ++++++
 system/runstate.c                 |  33 ++
 system/vl.c                       |   4 +
 target/i386/cpu-dump.c            |  41 +++
 target/i386/cpu.h                 |  12 +
 target/i386/kvm/kvm.c             |  88 ++++++
 util/bitmap.c                     |  24 ++
 util/main-loop.c                  |  56 ++++
 util/mmap-alloc.c                 |  56 ++++
 util/oslib-posix.c                | 139 +++++++++
 util/qemu-config.c                |  43 ++-
 util/qemu-timer.c                 |   4 +
 util/thread-context.c             |  68 ++++
 79 files changed, 4937 insertions(+), 1 deletion(-)

diff --git a/accel/kvm/kvm-accel-ops.c b/accel/kvm/kvm-accel-ops.c
index 6195150a0..54b4fbf99 100644
--- a/accel/kvm/kvm-accel-ops.c
+++ b/accel/kvm/kvm-accel-ops.c
@@ -48,6 +48,9 @@ static void *kvm_vcpu_thread_fn(void *arg)
 
     do {
         if (cpu_can_run(cpu)) {
+            /*
+	     * 调用主函数
+	     */
             r = kvm_cpu_exec(cpu);
             if (r == EXCP_DEBUG) {
                 cpu_handle_guest_debug(cpu);
diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index e39a810a4..5140e994b 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -198,6 +198,10 @@ unsigned int kvm_get_free_memslots(void)
     return s->nr_slots - used_slots;
 }
 
+/*
+ * caled by:
+ *   - accel/kvm/kvm-all.c|219| <<kvm_alloc_slot>> KVMSlot *slot = kvm_get_free_slot(kml);
+ */
 /* Called with KVMMemoryListener.slots_lock held */
 static KVMSlot *kvm_get_free_slot(KVMMemoryListener *kml)
 {
@@ -213,6 +217,54 @@ static KVMSlot *kvm_get_free_slot(KVMMemoryListener *kml)
     return NULL;
 }
 
+/*
+ * 一些例子:
+ *
+ * (gdb) bt
+ * #0  kvm_alloc_slot (kml=0x55555709c730) at ../accel/kvm/kvm-all.c:219
+ * #1  0x0000555555ddebab in kvm_set_phys_mem (kml=0x55555709c730, section=0x555557827ad0, add=true) at ../accel/kvm/kvm-all.c:1358
+ * #2  0x0000555555ddf567 in kvm_region_commit (listener=0x55555709c730) at ../accel/kvm/kvm-all.c:1575
+ * #3  0x0000555555d8095a in memory_region_transaction_commit () at ../system/memory.c:1137
+ * #4  0x0000555555d847e1 in memory_region_update_container_subregions (subregion=0x555557087200) at ../system/memory.c:2630
+ * #5  0x0000555555d8488c in memory_region_add_subregion_common (mr=0x555557083ea0, offset=786432, subregion=0x555557087200) at ../system/memory.c:2645
+ * #6  0x0000555555d84906 in memory_region_add_subregion_overlap (mr=0x555557083ea0, offset=786432, subregion=0x555557087200, priority=1) at ../system/memory.c:2662
+ * #7  0x0000555555c53f61 in pc_memory_init (pcms=0x5555572f85a0, system_memory=0x55555728a400, rom_memory=0x555557083ea0, pci_hole64_size=2147483648) at ../hw/i386/pc.c:1045
+ * #8  0x0000555555c3580f in pc_init1 (machine=0x5555572f85a0, host_type=0x55555620c198 "i440FX-pcihost", pci_type=0x55555620c191 "i440FX") at ../hw/i386/pc_piix.c:246
+ * #9  0x0000555555c363ff in pc_init_v8_2 (machine=0x5555572f85a0) at ../hw/i386/pc_piix.c:555
+ * #10 0x0000555555922bad in machine_run_board_init (machine=0x5555572f85a0, mem_path=0x0, errp=0x7fffffffd9f0) at ../hw/core/machine.c:1509
+ * #11 0x0000555555b73b0d in qemu_init_board () at ../system/vl.c:2613
+ * #12 0x0000555555b73d92 in qmp_x_exit_preconfig (errp=0x555556fc9800 <error_fatal>) at ../system/vl.c:2704
+ * #13 0x0000555555b76771 in qemu_init (argc=16, argv=0x7fffffffdcd8) at ../system/vl.c:3753
+ * #14 0x0000555555df1d6f in main (argc=16, argv=0x7fffffffdcd8) at ../system/main.c:47
+ *
+ * (gdb) bt
+ * #0  kvm_alloc_slot (kml=0x555556fb3f60 <smram_listener>) at ../accel/kvm/kvm-all.c:219
+ * #1  0x0000555555ddebab in kvm_set_phys_mem (kml=0x555556fb3f60 <smram_listener>, section=0x7fffe4030e00, add=true) at ../accel/kvm/kvm-all.c:1358
+ * #2  0x0000555555ddf567 in kvm_region_commit (listener=0x555556fb3f60 <smram_listener>) at ../accel/kvm/kvm-all.c:1575
+ * #3  0x0000555555d8095a in memory_region_transaction_commit () at ../system/memory.c:1137
+ * #4  0x0000555555a63cc4 in i440fx_update_memory_mappings (d=0x5555573fa3b0) at ../hw/pci-host/i440fx.c:98
+ * #5  0x0000555555a63d49 in i440fx_write_config (dev=0x5555573fa3b0, address=88, val=858992640, len=4) at ../hw/pci-host/i440fx.c:111
+ * #6  0x0000555555a4e04b in pci_host_config_write_common (pci_dev=0x5555573fa3b0, addr=88, limit=256, val=858992640, len=4) at ../hw/pci/pci_host.c:96
+ * #7  0x0000555555a4e271 in pci_data_write (s=0x5555573f9760, addr=2147483736, val=858992640, len=4) at ../hw/pci/pci_host.c:138
+ * #8  0x0000555555a4e408 in pci_host_data_write (opaque=0x5555573c2730, addr=0, val=858992640, len=4) at ../hw/pci/pci_host.c:188
+ * #9  0x0000555555d7e439 in memory_region_write_accessor (mr=0x5555573c2b70, addr=0, value=0x7fffedde73c8, size=4, shift=0, mask=4294967295, attrs=...) at ../system/memory.c:497
+ * #10 0x0000555555d7e74e in access_with_adjusted_size (addr=0, value=0x7fffedde73c8, size=4, access_size_min=1, access_size_max=4, access_fn=
+ *     0x555555d7e343 <memory_region_write_accessor>, mr=0x5555573c2b70, attrs=...) at ../system/memory.c:573
+ * #11 0x0000555555d81832 in memory_region_dispatch_write (mr=0x5555573c2b70, addr=0, data=858992640, op=MO_32, attrs=...) at ../system/memory.c:1521
+ * #12 0x0000555555d8f16b in flatview_write_continue (fv=0x555558125cb0, addr=3324, attrs=..., ptr=0x7ffff7fba000, len=4, addr1=0, l=4, mr=0x5555573c2b70) at ../system/physmem.c:2714
+ * #13 0x0000555555d8f2ce in flatview_write (fv=0x555558125cb0, addr=3324, attrs=..., buf=0x7ffff7fba000, len=4) at ../system/physmem.c:2756
+ * #14 0x0000555555d8f67e in address_space_write (as=0x555556fb5640 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff7fba000, len=4) at ../system/physmem.c:2863
+ * #15 0x0000555555d8f6eb in address_space_rw (as=0x555556fb5640 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff7fba000, len=4, is_write=true) at ../system/physmem.c:2873
+ * #16 0x0000555555de1d41 in kvm_handle_io (port=3324, attrs=..., data=0x7ffff7fba000, direction=1, size=4, count=1) at ../accel/kvm/kvm-all.c:2632
+ * #17 0x0000555555de25e4 in kvm_cpu_exec (cpu=0x555557370b30) at ../accel/kvm/kvm-all.c:2905
+ * #18 0x0000555555de5323 in kvm_vcpu_thread_fn (arg=0x555557370b30) at ../accel/kvm/kvm-accel-ops.c:51
+ * #19 0x0000555555fefb70 in qemu_thread_start (args=0x55555737a8d0) at ../util/qemu-thread-posix.c:541
+ * #20 0x00007ffff790e1da in start_thread () at /lib/../lib64/libpthread.so.0
+ * #21 0x00007ffff52e7e73 in clone () at /lib/../lib64/libc.so.6
+ *
+ * called by:
+ *   - accel/kvm/kvm-all.c|1584| <<kvm_set_phys_mem>> mem = kvm_alloc_slot(kml);
+ */
 /* Called with KVMMemoryListener.slots_lock held */
 static KVMSlot *kvm_alloc_slot(KVMMemoryListener *kml)
 {
@@ -289,6 +341,12 @@ int kvm_physical_memory_addr_from_host(KVMState *s, void *ram,
     return ret;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|493| <<kvm_slot_update_flags>> return kvm_set_user_memory_region(kml, mem, false);
+ *   - accel/kvm/kvm-all.c|1342| <<kvm_set_phys_mem>> err = kvm_set_user_memory_region(kml, mem, false);
+ *   - accel/kvm/kvm-all.c|1366| <<kvm_set_phys_mem>> err = kvm_set_user_memory_region(kml, mem, true);
+ */
 static int kvm_set_user_memory_region(KVMMemoryListener *kml, KVMSlot *slot, bool new)
 {
     KVMState *s = kvm_state;
@@ -464,6 +522,29 @@ err:
  * dirty pages logging control
  */
 
+/*
+ * (gdb) bt
+ * #0  kvm_mem_flags (mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:469
+ * #1  0x0000555555dc270f in kvm_slot_update_flags (kml=0x555557242d10, mem=0x5555572646c0, mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:485
+ * #2  0x0000555555dc282b in kvm_section_update_flags (kml=0x555557242d10, section=0x7ffdcb8b75f0) at ../accel/kvm/kvm-all.c:518
+ * #3  0x0000555555dc28cc in kvm_log_start (listener=0x555557242d10, section=0x7ffdcb8b75f0, old=0, new=4) at ../accel/kvm/kvm-all.c:539
+ * #4  0x0000555555d66ba7 in address_space_update_topology_pass (as=0x555556f49b40 <address_space_memory>, old_view=0x7ffde8144c90, new_view=0x7ffdb80418f0, adding=true) at ../system/memory.c:987
+ * #5  0x0000555555d6700d in address_space_set_flatview (as=0x555556f49b40 <address_space_memory>) at ../system/memory.c:1080
+ * #6  0x0000555555d671b5 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #7  0x0000555555d6be07 in memory_global_dirty_log_start (flags=1) at ../system/memory.c:2926
+ * #8  0x0000555555d81a9a in ram_init_bitmaps (rs=0x7ffdb8001320) at ../migration/ram.c:2810
+ * #9  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #10 0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #11 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #12 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #13 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #14 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #15 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - accel/kvm/kvm-all.c|491| <<kvm_slot_update_flags>> mem->flags = kvm_mem_flags(mr);
+ *   - accel/kvm/kvm-all.c|1375| <<kvm_set_phys_mem>> mem->flags = kvm_mem_flags(mr);
+ */
 static int kvm_mem_flags(MemoryRegion *mr)
 {
     bool readonly = mr->readonly || memory_region_is_romd(mr);
@@ -525,6 +606,10 @@ out:
     return ret;
 }
 
+/*
+ * 在以下使用kvm_log_start():
+ *   - accel/kvm/kvm-all.c|1750| <<kvm_memory_listener_register>> kml->listener.log_start = kvm_log_start;
+ */
 static void kvm_log_start(MemoryListener *listener,
                           MemoryRegionSection *section,
                           int old, int new)
@@ -565,16 +650,56 @@ static void kvm_slot_sync_dirty_pages(KVMSlot *slot)
     ram_addr_t start = slot->ram_start_offset;
     ram_addr_t pages = slot->memory_size / qemu_real_host_page_size();
 
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
 }
 
 static void kvm_slot_reset_dirty_pages(KVMSlot *slot)
 {
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
 }
 
 #define ALIGN(x, y)  (((x)+(y)-1) & ~((y)-1))
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|498| <<kvm_slot_update_flags>> kvm_slot_init_dirty_bitmap(mem);
+ *   - accel/kvm/kvm-all.c|1376| <<kvm_set_phys_mem>> kvm_slot_init_dirty_bitmap(mem);
+ *
+ * KVMSlot *mem:
+ * -> ram_addr_t memory_size;
+ * -> unsigned long *dirty_bmap;
+ * -> unsigned long dirty_bmap_size;
+ */
 /* Allocate the dirty bitmap for a slot  */
 static void kvm_slot_init_dirty_bitmap(KVMSlot *mem)
 {
@@ -582,6 +707,21 @@ static void kvm_slot_init_dirty_bitmap(KVMSlot *mem)
         return;
     }
 
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     /*
      * XXX bad kernel interface alert
      * For dirty bitmap, kernel allocates array of size aligned to
@@ -605,6 +745,67 @@ static void kvm_slot_init_dirty_bitmap(KVMSlot *mem)
     mem->dirty_bmap_size = bitmap_size;
 }
 
+/*
+ * (gdb) bt
+ * #0  kvm_mem_flags (mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:469
+ * #1  0x0000555555dc270f in kvm_slot_update_flags (kml=0x555557242d10, mem=0x5555572646c0, mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:485
+ * #2  0x0000555555dc282b in kvm_section_update_flags (kml=0x555557242d10, section=0x7ffdcb8b75f0) at ../accel/kvm/kvm-all.c:518
+ * #3  0x0000555555dc28cc in kvm_log_start (listener=0x555557242d10, section=0x7ffdcb8b75f0, old=0, new=4) at ../accel/kvm/kvm-all.c:539
+ * #4  0x0000555555d66ba7 in address_space_update_topology_pass (as=0x555556f49b40 <address_space_memory>, old_view=0x7ffde8144c90, new_view=0x7ffdb80418f0, adding=true) at ../system/memory.c:987
+ * #5  0x0000555555d6700d in address_space_set_flatview (as=0x555556f49b40 <address_space_memory>) at ../system/memory.c:1080
+ * #6  0x0000555555d671b5 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #7  0x0000555555d6be07 in memory_global_dirty_log_start (flags=1) at ../system/memory.c:2926
+ * #8  0x0000555555d81a9a in ram_init_bitmaps (rs=0x7ffdb8001320) at ../migration/ram.c:2810
+ * #9  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #10 0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #11 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #12 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #13 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #14 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #15 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ *
+ * 这是setup的时候的
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x555557264750) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d81aab in ram_init_bitmaps (rs=0x7ffdc0001410) at ../migration/ram.c:2811
+ * #8  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #9  0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #10 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #11 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #12 0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #14 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 这是iteration时候的
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x5555572646c0) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7fffeebee640) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7fffeebee640) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc4001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc4001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d827c3 in ram_state_pending_exact (opaque=0x555556f49e60 <ram_state>, must_precopy=0x7fffeebee7f8, can_postcopy=0x7fffeebee800) at ../migration/ram.c:3226
+ * #8  0x0000555555ba1817 in qemu_savevm_state_pending_exact (must_precopy=0x7fffeebee7f8, can_postcopy=0x7fffeebee800) at ../migration/savevm.c:1680
+ * #9  0x0000555555b8aac0 in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3094
+ * #10 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #11 0x0000555555fd1784 in qemu_thread_start (args=0x555557243440) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - accel/kvm/kvm-all.c|863| <<kvm_physical_sync_dirty_bitmap>> if (kvm_slot_get_dirty_log(s, mem)) {
+ *   - accel/kvm/kvm-all.c|1340| <<kvm_set_phys_mem>> kvm_slot_get_dirty_log(kvm_state, mem);
+ *   - accel/kvm/kvm-all.c|1343| <<kvm_set_phys_mem>> kvm_slot_get_dirty_log(kvm_state, mem);
+ *   - accel/kvm/kvm-all.c|1628| <<kvm_log_sync_global>> kvm_slot_get_dirty_log(s, mem)) {
+ */
 /*
  * Sync dirty bitmap from kernel to KVMSlot.dirty_bmap, return true if
  * succeeded, false otherwise
@@ -614,6 +815,21 @@ static bool kvm_slot_get_dirty_log(KVMState *s, KVMSlot *slot)
     struct kvm_dirty_log d = {};
     int ret;
 
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     d.dirty_bitmap = slot->dirty_bmap;
     d.slot = slot->slot | (slot->as_id << 16);
     ret = kvm_vm_ioctl(s, KVM_GET_DIRTY_LOG, &d);
@@ -648,6 +864,21 @@ static void kvm_dirty_ring_mark_page(KVMState *s, uint32_t as_id,
         return;
     }
 
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     set_bit(offset, mem->dirty_bmap);
 }
 
@@ -838,6 +1069,10 @@ static void kvm_dirty_ring_flush(void)
  * @kml: the KVM memory listener object
  * @section: the memory section to sync the dirty bitmap with
  */
+/*
+ * called by;
+ *   - accel/kvm/kvm-all.c|1603| <<kvm_log_sync>> kvm_physical_sync_dirty_bitmap(kml, section);
+ */
 static void kvm_physical_sync_dirty_bitmap(KVMMemoryListener *kml,
                                            MemoryRegionSection *section)
 {
@@ -867,6 +1102,10 @@ static void kvm_physical_sync_dirty_bitmap(KVMMemoryListener *kml,
 #define KVM_CLEAR_LOG_ALIGN  (qemu_real_host_page_size() << KVM_CLEAR_LOG_SHIFT)
 #define KVM_CLEAR_LOG_MASK   (-KVM_CLEAR_LOG_ALIGN)
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1065| <<kvm_physical_log_clear>> ret = kvm_log_clear_one_slot(mem, kml->as_id, offset, count);
+ */
 static int kvm_log_clear_one_slot(KVMSlot *mem, int as_id, uint64_t start,
                                   uint64_t size)
 {
@@ -922,6 +1161,21 @@ static int kvm_log_clear_one_slot(KVMSlot *mem, int as_id, uint64_t start,
      */
 
     assert(bmap_start % BITS_PER_LONG == 0);
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     /* We should never do log_clear before log_sync */
     assert(mem->dirty_bmap);
     if (start_delta || bmap_npages - size / psize) {
@@ -985,6 +1239,10 @@ static int kvm_log_clear_one_slot(KVMSlot *mem, int as_id, uint64_t start,
  * @kml:     the kvm memory listener
  * @section: the memory range to clear dirty bitmap
  */
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1683| <<kvm_log_clear>> r = kvm_physical_log_clear(kml, section);
+ */
 static int kvm_physical_log_clear(KVMMemoryListener *kml,
                                   MemoryRegionSection *section)
 {
@@ -1266,6 +1524,11 @@ void kvm_set_max_memslot_size(hwaddr max_slot_size)
     kvm_max_slot_size = max_slot_size;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1565| <<kvm_region_commit>> kvm_set_phys_mem(kml, &u1->section, false);
+ *   - accel/kvm/kvm-all.c|1575| <<kvm_region_commit>> kvm_set_phys_mem(kml, &u1->section, true);
+ */
 /* Called with KVMMemoryListener.slots_lock held */
 static void kvm_set_phys_mem(KVMMemoryListener *kml,
                              MemoryRegionSection *section, bool add)
@@ -1334,6 +1597,21 @@ static void kvm_set_phys_mem(KVMMemoryListener *kml,
                 kvm_slot_sync_dirty_pages(mem);
             }
 
+	    /*
+	     * 在以下使用KVMSlot->dirty_bmap:
+	     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+	     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+	     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+	     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+	     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+	     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+	     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+	     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+	     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+	     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+	     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+	     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+	     */
             /* unregister the slot */
             g_free(mem->dirty_bmap);
             mem->dirty_bmap = NULL;
@@ -1425,6 +1703,10 @@ static void kvm_dirty_ring_reaper_init(KVMState *s)
                        s, QEMU_THREAD_JOINABLE);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2532| <<kvm_init>> ret = kvm_dirty_ring_init(s);
+ */
 static int kvm_dirty_ring_init(KVMState *s)
 {
     uint32_t ring_size = s->kvm_dirty_ring_size;
@@ -1583,6 +1865,10 @@ static void kvm_region_commit(MemoryListener *listener)
     kvm_slots_unlock();
 }
 
+/*
+ * 在以下使用kvm_log_sync():
+ *   - accel/kvm/kvm-all.c|1758| <<kvm_memory_listener_register>> kml->listener.log_sync = kvm_log_sync;
+ */
 static void kvm_log_sync(MemoryListener *listener,
                          MemoryRegionSection *section)
 {
@@ -1629,6 +1915,10 @@ static void kvm_log_sync_global(MemoryListener *l, bool last_stage)
     kvm_slots_unlock();
 }
 
+/*
+ * 在以下使用kvm_log_clear():
+ *   - accel/kvm/kvm-all.c|1793| <<kvm_memory_listener_register>> kml->listener.log_clear = kvm_log_clear;
+ */
 static void kvm_log_clear(MemoryListener *listener,
                           MemoryRegionSection *section)
 {
@@ -1718,6 +2008,11 @@ static void kvm_io_ioeventfd_del(MemoryListener *listener,
     }
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2881| <<kvm_init>> kvm_memory_listener_register(s, &s->memory_listener, &address_space_memory, 0, "kvm-memory");
+ *   - target/i386/kvm/kvm.c|2508| <<register_smram_listener>> kvm_memory_listener_register(kvm_state, &smram_listener, &smram_address_space, 1, "kvm-smram");
+ */
 void kvm_memory_listener_register(KVMState *s, KVMMemoryListener *kml,
                                   AddressSpace *as, int as_id, const char *name)
 {
@@ -1741,6 +2036,13 @@ void kvm_memory_listener_register(KVMState *s, KVMMemoryListener *kml,
     kml->listener.priority = MEMORY_LISTENER_PRIORITY_ACCEL;
     kml->listener.name = name;
 
+    /*
+     * 在以下设置KVMState->kvm_dirty_ring_size:
+     *   - accel/kvm/kvm-all.c|1476| <<kvm_dirty_ring_init>> s->kvm_dirty_ring_size = 0;
+     *   - accel/kvm/kvm-all.c|1526| <<kvm_dirty_ring_init>> s->kvm_dirty_ring_size = ring_size;
+     *   - accel/kvm/kvm-all.c|3685| <<kvm_set_dirty_ring_size>> s->kvm_dirty_ring_size = value;
+     *   - accel/kvm/kvm-all.c|3698| <<kvm_accel_instance_init>> s->kvm_dirty_ring_size = 0;
+     */
     if (s->kvm_dirty_ring_size) {
         kml->listener.log_sync_global = kvm_log_sync_global;
     } else {
@@ -2697,6 +2999,10 @@ bool kvm_cpu_check_are_resettable(void)
     return kvm_arch_cpu_check_are_resettable();
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2728| <<kvm_cpu_synchronize_state>> run_on_cpu(cpu, do_kvm_cpu_synchronize_state, RUN_ON_CPU_NULL);
+ */
 static void do_kvm_cpu_synchronize_state(CPUState *cpu, run_on_cpu_data arg)
 {
     if (!cpu->vcpu_dirty) {
@@ -2711,6 +3017,18 @@ static void do_kvm_cpu_synchronize_state(CPUState *cpu, run_on_cpu_data arg)
     }
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2995| <<kvm_cpu_exec>> kvm_cpu_synchronize_state(cpu);
+ *   - target/arm/kvm64.c|1117| <<kvm_arch_on_sigbus_vcpu>> kvm_cpu_synchronize_state(c);
+ *   - target/arm/kvm64.c|1187| <<kvm_arm_handle_debug>> kvm_cpu_synchronize_state(cs);
+ *   - target/i386/kvm/kvm.c|4872| <<kvm_arch_process_async_events>> kvm_cpu_synchronize_state(cs);
+ *   - target/i386/kvm/kvm.c|4891| <<kvm_arch_process_async_events>> kvm_cpu_synchronize_state(cs);
+ *   - target/i386/kvm/kvm.c|4909| <<kvm_arch_process_async_events>> kvm_cpu_synchronize_state(cs);
+ *   - target/i386/kvm/kvm.c|4914| <<kvm_arch_process_async_events>> kvm_cpu_synchronize_state(cs);
+ *   - target/i386/kvm/kvm.c|5385| <<kvm_arch_stop_on_emulation_error>> kvm_cpu_synchronize_state(cs);
+ *   - target/s390x/kvm/kvm.c|1928| <<kvm_arch_handle_exit>> kvm_cpu_synchronize_state(cs);
+ */
 void kvm_cpu_synchronize_state(CPUState *cpu)
 {
     if (!cpu->vcpu_dirty) {
@@ -2718,6 +3036,17 @@ void kvm_cpu_synchronize_state(CPUState *cpu)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  do_kvm_cpu_synchronize_post_reset (cpu=0x55555735de90, arg=...) at ../accel/kvm/kvm-all.c:2723
+ * #1  0x000055555586a857 in process_queued_cpu_work (cpu=0x55555735de90) at ../cpu-common.c:360
+ * #2  0x0000555555b6047f in qemu_wait_io_event_common (cpu=0x55555735de90) at ../system/cpus.c:419
+ * #3  0x0000555555b60510 in qemu_wait_io_event (cpu=0x55555735de90) at ../system/cpus.c:437
+ * #4  0x0000555555df631e in kvm_vcpu_thread_fn (arg=0x55555735de90) at ../accel/kvm/kvm-accel-ops.c:56
+ * #5  0x00005555560017cc in qemu_thread_start (args=0x555557367a30) at ../util/qemu-thread-posix.c:541
+ * #6  0x00007ffff749f812 in start_thread () at /lib64/libc.so.6
+ * #7  0x00007ffff743f450 in clone3 () at /lib64/libc.so.6
+ */
 static void do_kvm_cpu_synchronize_post_reset(CPUState *cpu, run_on_cpu_data arg)
 {
     int ret = kvm_arch_put_registers(cpu, KVM_PUT_RESET_STATE);
@@ -2735,6 +3064,10 @@ void kvm_cpu_synchronize_post_reset(CPUState *cpu)
     run_on_cpu(cpu, do_kvm_cpu_synchronize_post_reset, RUN_ON_CPU_NULL);
 }
 
+/*
+ * 在以下使用do_kvm_cpu_synchronize_post_init():
+ *   - accel/kvm/kvm-all.c|2762| <<kvm_cpu_synchronize_post_init>> run_on_cpu(cpu, do_kvm_cpu_synchronize_post_init, RUN_ON_CPU_NULL);
+ */
 static void do_kvm_cpu_synchronize_post_init(CPUState *cpu, run_on_cpu_data arg)
 {
     int ret = kvm_arch_put_registers(cpu, KVM_PUT_FULL_STATE);
@@ -2746,6 +3079,10 @@ static void do_kvm_cpu_synchronize_post_init(CPUState *cpu, run_on_cpu_data arg)
     cpu->vcpu_dirty = false;
 }
 
+/*
+ * 在以下使用kvm_cpu_synchronize_post_init():
+ *   - accel/kvm/kvm-accel-ops.c|104| <<kvm_accel_ops_class_init>> ops->synchronize_post_init = kvm_cpu_synchronize_post_init;
+ */
 void kvm_cpu_synchronize_post_init(CPUState *cpu)
 {
     run_on_cpu(cpu, do_kvm_cpu_synchronize_post_init, RUN_ON_CPU_NULL);
@@ -2816,6 +3153,10 @@ static void kvm_eat_signals(CPUState *cpu)
     } while (sigismember(&chkset, SIG_IPI));
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-accel-ops.c|51| <<kvm_vcpu_thread_fn>> r = kvm_cpu_exec(cpu);
+ */
 int kvm_cpu_exec(CPUState *cpu)
 {
     struct kvm_run *run = cpu->kvm_run;
diff --git a/backends/hostmem-file.c b/backends/hostmem-file.c
index 361d4a810..5ad3aa626 100644
--- a/backends/hostmem-file.c
+++ b/backends/hostmem-file.c
@@ -36,6 +36,28 @@ struct HostMemoryBackendFile {
     OnOffAuto rom;
 };
 
+/*
+ * 非常一部分的例子:
+ *   - backends/hostmem-epc.c|60| <<sgx_epc_backend_class_init>> bc->alloc = sgx_epc_backend_memory_alloc;
+ *   - backends/hostmem-file.c|275| <<file_backend_class_init>> bc->alloc = file_backend_memory_alloc;
+ *   - backends/hostmem-memfd.c|139| <<memfd_backend_class_init>> bc->alloc = memfd_backend_memory_alloc;
+ *   - backends/hostmem-ram.c|43| <<ram_backend_class_init>> bc->alloc = ram_backend_memory_alloc;
+ *
+ * 一个例子:
+ * host_memory_backend_memory_complete()
+ * -> bc->alloc = file_backend_memory_alloc()
+ *    -> memory_region_init_ram_from_file()
+ *       -> qemu_ram_alloc_from_file()
+ *          -> qemu_ram_alloc_from_fd()
+ *             -> file_ram_alloc() 
+ *                -> qemu_ram_mmap()
+ *                   -> mmap_activate()
+ *                      -> mmap()
+ *                -> "unable to map backing store for guest RAM"
+ *
+ * 在以下使用file_backend_memory_alloc():
+ *   - backends/hostmem-file.c|275| <<file_backend_class_init>> bc->alloc = file_backend_memory_alloc;
+ */
 static void
 file_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
 {
@@ -43,6 +65,19 @@ file_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
     error_setg(errp, "backend '%s' not supported on this host",
                object_get_typename(OBJECT(backend)));
 #else
+    /*
+     * struct HostMemoryBackendFile {
+     *     HostMemoryBackend parent_obj;
+     *
+     *     char *mem_path;
+     *     uint64_t align;
+     *     uint64_t offset;
+     *     bool discard_data;
+     *     bool is_pmem;
+     *     bool readonly;
+     *     OnOffAuto rom;
+     * };
+     */
     HostMemoryBackendFile *fb = MEMORY_BACKEND_FILE(backend);
     uint32_t ram_flags;
     gchar *name;
@@ -79,6 +114,12 @@ file_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
         assert(false);
     }
 
+    /*
+     * called by:
+     *   - backends/hostmem-file.c|117| <<file_backend_memory_alloc>> name = host_memory_backend_get_name(backend);
+     *   - backends/hostmem-memfd.c|59| <<memfd_backend_memory_alloc>> name = host_memory_backend_get_name(backend);
+     *   - backends/hostmem-ram.c|30| <<ram_backend_memory_alloc>> name = host_memory_backend_get_name(backend);
+     */
     name = host_memory_backend_get_name(backend);
     ram_flags = backend->share ? RAM_SHARED : 0;
     ram_flags |= fb->readonly ? RAM_READONLY_FD : 0;
@@ -86,6 +127,9 @@ file_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
     ram_flags |= backend->reserve ? 0 : RAM_NORESERVE;
     ram_flags |= fb->is_pmem ? RAM_PMEM : 0;
     ram_flags |= RAM_NAMED_FILE;
+    /*
+     * 只在此处调用
+     */
     memory_region_init_ram_from_file(&backend->mr, OBJECT(backend), name,
                                      backend->size, fb->align, ram_flags,
                                      fb->mem_path, fb->offset, errp);
@@ -228,6 +272,10 @@ static void file_memory_backend_set_readonly(Object *obj, bool value,
     fb->readonly = value;
 }
 
+/*
+ * 347     object_class_property_add(oc, "rom", "OnOffAuto",
+ * 348         file_memory_backend_get_rom, file_memory_backend_set_rom, NULL, NULL);
+ */
 static void file_memory_backend_get_rom(Object *obj, Visitor *v,
                                         const char *name, void *opaque,
                                         Error **errp)
@@ -238,6 +286,10 @@ static void file_memory_backend_get_rom(Object *obj, Visitor *v,
     visit_type_OnOffAuto(v, name, &rom, errp);
 }
 
+/*
+ * 347     object_class_property_add(oc, "rom", "OnOffAuto",
+ * 348         file_memory_backend_get_rom, file_memory_backend_set_rom, NULL, NULL);
+ */
 static void file_memory_backend_set_rom(Object *obj, Visitor *v,
                                         const char *name, void *opaque,
                                         Error **errp)
@@ -254,6 +306,10 @@ static void file_memory_backend_set_rom(Object *obj, Visitor *v,
     visit_type_OnOffAuto(v, name, &fb->rom, errp);
 }
 
+/*
+ * 在以下使用file_backend_unparent():
+ *   - backends/hostmem-file.c|320| <<file_backend_class_init>> oc->unparent = file_backend_unparent;
+ */
 static void file_backend_unparent(Object *obj)
 {
     HostMemoryBackend *backend = MEMORY_BACKEND(obj);
@@ -309,6 +365,14 @@ static void file_backend_instance_finalize(Object *o)
     g_free(fb->mem_path);
 }
 
+/*
+ * -object thread-context,id=tc01,node-affinity=0 \
+ * -object memory-backend-file,id=ram01,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=0,policy=bind,prealloc-context=tc01 \
+ * -numa node,nodeid=0,cpus=0-1,memdev=ram01 \
+ * -object thread-context,id=tc02,node-affinity=1 \
+ * -object memory-backend-file,id=ram02,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=1,policy=bind,prealloc-context=tc02 \
+ * -numa node,nodeid=1,cpus=2-3,memdev=ram02 \
+ */
 static const TypeInfo file_backend_info = {
     .name = TYPE_MEMORY_BACKEND_FILE,
     .parent = TYPE_MEMORY_BACKEND,
diff --git a/backends/hostmem-memfd.c b/backends/hostmem-memfd.c
index 3fc85c3db..5dfd594e0 100644
--- a/backends/hostmem-memfd.c
+++ b/backends/hostmem-memfd.c
@@ -31,6 +31,10 @@ struct HostMemoryBackendMemfd {
     bool seal;
 };
 
+/*
+ * 在以下使用memfd_backend_memory_alloc():
+ *   - backends/hostmem-memfd.c|135| <<memfd_backend_class_init>> bc->alloc = memfd_backend_memory_alloc;
+ */
 static void
 memfd_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
 {
diff --git a/backends/hostmem-ram.c b/backends/hostmem-ram.c
index b8e55cdbd..511faa5f6 100644
--- a/backends/hostmem-ram.c
+++ b/backends/hostmem-ram.c
@@ -16,6 +16,26 @@
 #include "qemu/module.h"
 #include "qom/object_interfaces.h"
 
+/*
+ * 如果只是普通的"-m xxxx"
+ * (gdb) bt
+ * #0  qemu_ram_mmap (fd=-1, size=137438953472, align=2097152, qemu_map_flags=0, map_offset=0) at ../util/mmap-alloc.c:253
+ * #1  0x0000555555ffae09 in qemu_anon_ram_alloc (size=137438953472, alignment=0x55555723f100, shared=false, noreserve=false) at ../util/oslib-posix.c:193
+ * #2  0x0000555555d9d25e in ram_block_add (new_block=0x55555732f2a0, errp=0x7fffffffdb28) at ../system/physmem.c:1830
+ * #3  0x0000555555d9dbf9 in qemu_ram_alloc_internal (size=137438953472, max_size=137438953472, resized=0x0, host=0x0, ram_flags=0, mr=0x55555723f070, errp=0x7fffffffdbc0) at ../system/physmem.c:2034
+ * #4  0x0000555555d9dd06 in qemu_ram_alloc (size=137438953472, ram_flags=0, mr=0x55555723f070, errp=0x7fffffffdbc0) at ../system/physmem.c:2054
+ * #5  0x0000555555d91807 in memory_region_init_ram_flags_nomigrate (mr=0x55555723f070, owner=0x55555723f000, name=0x5555570507d0 "pc.ram", size=137438953472, ram_flags=0, errp=0x7fffffffdc30)
+ *     at ../system/memory.c:1570
+ * #6  0x0000555555b7c406 in ram_backend_memory_alloc (backend=0x55555723f000, errp=0x7fffffffdc30) at ../backends/hostmem-ram.c:33
+ * #7  0x0000555555b7d0de in host_memory_backend_memory_complete (uc=0x55555723f000, errp=0x7fffffffdca8) at ../backends/hostmem.c:332
+ * #8  0x0000555555e121a2 in user_creatable_complete (uc=0x55555723f000, errp=0x7fffffffdd80) at ../qom/object_interfaces.c:28
+ * #9  0x0000555555922d73 in create_default_memdev (ms=0x5555572b7580, path=0x0, errp=0x7fffffffdd80) at ../hw/core/machine.c:1380
+ * #10 0x0000555555922f9d in machine_run_board_init (machine=0x5555572b7580, mem_path=0x0, errp=0x7fffffffdd80) at ../hw/core/machine.c:1432
+ * #11 0x0000555555b74388 in qemu_init_board () at ../system/vl.c:2613
+ * #12 0x0000555555b7460d in qmp_x_exit_preconfig (errp=0x555556fc8340 <error_fatal>) at ../system/vl.c:2704
+ * #13 0x0000555555b76fec in qemu_init (argc=19, argv=0x7fffffffe0a8) at ../system/vl.c:3753
+ * #14 0x0000555555e01cbf in main (argc=19, argv=0x7fffffffe0a8) at ../system/main.c:47
+ */
 static void
 ram_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
 {
diff --git a/backends/hostmem.c b/backends/hostmem.c
index 747e7838c..393a83e2d 100644
--- a/backends/hostmem.c
+++ b/backends/hostmem.c
@@ -34,6 +34,12 @@ QEMU_BUILD_BUG_ON(HOST_MEM_POLICY_BIND != MPOL_BIND);
 QEMU_BUILD_BUG_ON(HOST_MEM_POLICY_INTERLEAVE != MPOL_INTERLEAVE);
 #endif
 
+/*
+ * called by:
+ *   - backends/hostmem-file.c|117| <<file_backend_memory_alloc>> name = host_memory_backend_get_name(backend);
+ *   - backends/hostmem-memfd.c|59| <<memfd_backend_memory_alloc>> name = host_memory_backend_get_name(backend);
+ *   - backends/hostmem-ram.c|30| <<ram_backend_memory_alloc>> name = host_memory_backend_get_name(backend);
+ */
 char *
 host_memory_backend_get_name(HostMemoryBackend *backend)
 {
@@ -44,6 +50,12 @@ host_memory_backend_get_name(HostMemoryBackend *backend)
     return object_get_canonical_path(OBJECT(backend));
 }
 
+/*
+ * 562     object_class_property_add(oc, "size", "int",
+ * 563         host_memory_backend_get_size,
+ * 564         host_memory_backend_set_size,
+ * 565         NULL, NULL);
+ */
 static void
 host_memory_backend_get_size(Object *obj, Visitor *v, const char *name,
                              void *opaque, Error **errp)
@@ -54,6 +66,12 @@ host_memory_backend_get_size(Object *obj, Visitor *v, const char *name,
     visit_type_size(v, name, &value, errp);
 }
 
+/*
+ * 562     object_class_property_add(oc, "size", "int",
+ * 563         host_memory_backend_get_size,
+ * 564         host_memory_backend_set_size,
+ * 565         NULL, NULL);
+ */
 static void
 host_memory_backend_set_size(Object *obj, Visitor *v, const char *name,
                              void *opaque, Error **errp)
@@ -271,6 +289,9 @@ static void host_memory_backend_set_prealloc_threads(Object *obj, Visitor *v,
     backend->prealloc_threads = value;
 }
 
+/*
+ * TypeInfo host_memory_backend_info.instance_init = host_memory_backend_init()
+ */
 static void host_memory_backend_init(Object *obj)
 {
     HostMemoryBackend *backend = MEMORY_BACKEND(obj);
@@ -283,6 +304,9 @@ static void host_memory_backend_init(Object *obj)
     backend->prealloc_threads = machine->smp.cpus;
 }
 
+/*
+ * TypeInfo host_memory_backend_info.instance_post_init = host_memory_backend_post_init()
+ */
 static void host_memory_backend_post_init(Object *obj)
 {
     object_apply_compat_props(obj);
@@ -319,6 +343,42 @@ size_t host_memory_backend_pagesize(HostMemoryBackend *memdev)
     return pagesize;
 }
 
+/*
+ * 在下面的例子调用2次:
+ *
+ * -object thread-context,id=tc01,node-affinity=0 \
+ * -object memory-backend-file,id=ram01,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=0,policy=bind,prealloc-context=tc01 \
+ * -numa node,nodeid=0,cpus=0-1,memdev=ram01 \
+ * -object thread-context,id=tc02,node-affinity=1 \
+ * -object memory-backend-file,id=ram02,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=1,policy=bind,prealloc-context=tc02 \
+ * -numa node,nodeid=1,cpus=2-3,memdev=ram02 \
+ *
+ * (gdb) bt
+ * #0  host_memory_backend_memory_complete (uc=0x555557245e50, errp=0x7fffffffda08) at ../backends/hostmem.c:324
+ * #1  0x0000555555e121a2 in user_creatable_complete (uc=0x555557245e50, errp=0x7fffffffda68) at ../qom/object_interfaces.c:28
+ * #2  0x0000555555e1262e in user_creatable_add_type (type=0x5555562af104 "memory-backend-file", id=0x555557053f10 "ram-node0", qdict=0x5555572d6b70,
+ *                                     v=0x5555572d0d90, errp=0x7fffffffda80) at ../qom/object_interfaces.c:125
+ * #3  0x0000555555e127d4 in user_creatable_add_qapi (options=0x555557053bc0, errp=0x555556fc8340 <error_fatal>) at ../qom/object_interfaces.c:157
+ * #4  0x0000555555b7231c in object_option_foreach_add (type_opt_predicate=0x555555b7295d <object_create_late>) at ../system/vl.c:1792
+ * #5  0x0000555555b729e4 in qemu_create_late_backends () at ../system/vl.c:2011
+ * #6  0x0000555555b76f4c in qemu_init (argc=31, argv=0x7fffffffde38) at ../system/vl.c:3727
+ * #7  0x0000555555e01cbf in main (argc=31, argv=0x7fffffffde38) at ../system/main.c:47
+ *
+ * 在以下使用host_memory_backend_memory_complete():
+ *   - backends/hostmem.c|488| <<host_memory_backend_class_init>> ucc->complete = host_memory_backend_memory_complete;
+ *
+ * 一个例子:
+ * host_memory_backend_memory_complete()
+ * -> bc->alloc = file_backend_memory_alloc()
+ *    -> memory_region_init_ram_from_file()
+ *       -> qemu_ram_alloc_from_file()
+ *          -> qemu_ram_alloc_from_fd()
+ *             -> file_ram_alloc() 
+ *                -> qemu_ram_mmap()
+ *                   -> mmap_activate()
+ *                      -> mmap()
+ *                -> "unable to map backing store for guest RAM"
+ */
 static void
 host_memory_backend_memory_complete(UserCreatable *uc, Error **errp)
 {
@@ -328,6 +388,13 @@ host_memory_backend_memory_complete(UserCreatable *uc, Error **errp)
     void *ptr;
     uint64_t sz;
 
+    /*
+     * 非常一部分的例子:
+     *   - backends/hostmem-epc.c|60| <<sgx_epc_backend_class_init>> bc->alloc = sgx_epc_backend_memory_alloc;
+     *   - backends/hostmem-file.c|275| <<file_backend_class_init>> bc->alloc = file_backend_memory_alloc;
+     *   - backends/hostmem-memfd.c|139| <<memfd_backend_class_init>> bc->alloc = memfd_backend_memory_alloc;
+     *   - backends/hostmem-ram.c|43| <<ram_backend_class_init>> bc->alloc = ram_backend_memory_alloc;
+     */
     if (bc->alloc) {
         bc->alloc(backend, &local_err);
         if (local_err) {
@@ -557,6 +624,15 @@ host_memory_backend_class_init(ObjectClass *oc, void *data)
         host_memory_backend_set_use_canonical_path);
 }
 
+/*
+ * -object thread-context,id=tc01,node-affinity=0 \
+ * -object memory-backend-file,id=ram01,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=0,policy=bind,prealloc-context=tc01 \
+ * -numa node,nodeid=0,cpus=0-1,memdev=ram01 \
+ * -object thread-context,id=tc02,node-affinity=1 \
+ * -object memory-backend-file,id=ram02,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=1,policy=bind,prealloc-context=tc02 \
+ * -numa node,nodeid=1,cpus=2-3,memdev=ram02 \
+ */
+
 static const TypeInfo host_memory_backend_info = {
     .name = TYPE_MEMORY_BACKEND,
     .parent = TYPE_OBJECT,
diff --git a/block/block-backend.c b/block/block-backend.c
index ec2114880..1f808eef7 100644
--- a/block/block-backend.c
+++ b/block/block-backend.c
@@ -63,6 +63,20 @@ struct BlockBackend {
     /* I/O stats (display with "info blockstats"). */
     BlockAcctStats stats;
 
+    /*
+     * 在以下使用on_read_error:
+     *   - block/block-backend.c|368| <<blk_new>> blk->on_read_error = BLOCKDEV_ON_ERROR_REPORT;
+     *   - block/block-backend.c|1233| <<blk_iostatus_is_enabled>> blk->on_read_error == BLOCKDEV_ON_ERROR_STOP));
+     *   - block/block-backend.c|2157| <<blk_set_on_error>> blk->on_read_error = on_read_error;
+     *   - block/block-backend.c|2164| <<blk_get_on_error>> return is_read ? blk->on_read_error : blk->on_write_error;
+     *
+     * 在以下使用BlockBackend->on_write_error:
+     *   - block/block-backend.c|369| <<blk_new>> blk->on_write_error = BLOCKDEV_ON_ERROR_ENOSPC;
+     *   - block/block-backend.c|1231| <<blk_iostatus_is_enabled>> (blk->on_write_error == BLOCKDEV_ON_ERROR_ENOSPC ||
+     *   - block/block-backend.c|1232| <<blk_iostatus_is_enabled>> blk->on_write_error == BLOCKDEV_ON_ERROR_STOP ||
+     *   - block/block-backend.c|2158| <<blk_set_on_error>> blk->on_write_error = on_write_error;
+     *   - block/block-backend.c|2164| <<blk_get_on_error>> return is_read ? blk->on_read_error : blk->on_write_error;
+     */
     BlockdevOnError on_read_error, on_write_error;
     bool iostatus_enabled;
     BlockDeviceIoStatus iostatus;
@@ -1590,6 +1604,15 @@ static void blk_aio_complete_bh(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * called by:
+ *   - block/block-backend.c|1655| <<blk_aio_pwrite_zeroes>> return blk_aio_prwv(blk, offset, bytes, NULL, blk_aio_write_entry,
+ *   - block/block-backend.c|1726| <<blk_aio_preadv>> return blk_aio_prwv(blk, offset, qiov->size, qiov, blk_aio_read_entry, flags, cb, opaque);
+ *   - block/block-backend.c|1736| <<blk_aio_pwritev>> return blk_aio_prwv(blk, offset, qiov->size, qiov, blk_aio_write_entry, flags, cb, opaque);
+ *   - block/block-backend.c|1795| <<blk_aio_ioctl>> return blk_aio_prwv(blk, req, 0, buf, blk_aio_ioctl_entry, 0, cb, opaque);
+ *   - block/block-backend.c|1830| <<blk_aio_pdiscard>> return blk_aio_prwv(blk, offset, bytes, NULL, blk_aio_pdiscard_entry, 0,
+ *   - block/block-backend.c|1874| <<blk_aio_flush>> return blk_aio_prwv(blk, 0, 0, NULL, blk_aio_flush_entry, 0, cb, opaque);
+ */
 static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset,
                                 int64_t bytes,
                                 void *iobuf, CoroutineEntry co_entry,
@@ -1727,6 +1750,27 @@ BlockAIOCB *blk_aio_preadv(BlockBackend *blk, int64_t offset,
                         blk_aio_read_entry, flags, cb, opaque);
 }
 
+/*
+ * called by:
+ *   - hw/block/dataplane/xen-block.c|398| <<xen_block_do_aio>> blk_aio_pwritev(dataplane->blk, request->start, &request->v, 0,
+ *   - hw/block/m25p80.c|562| <<flash_sync_page>> blk_aio_pwritev(s->blk, page * s->pi->page_size, iov, 0,
+ *   - hw/block/m25p80.c|578| <<flash_sync_area>> blk_aio_pwritev(s->blk, off, iov, 0, blk_sync_complete, iov);
+ *   - hw/block/virtio-blk.c|411| <<submit_requests>> blk_aio_pwritev(blk, sector_num << BDRV_SECTOR_BITS, qiov,
+ *   - hw/ide/core.c|1101| <<ide_sector_write>> s->pio_aiocb = blk_aio_pwritev(s->blk, sector_num << BDRV_SECTOR_BITS,
+ *   - hw/nvme/ctrl.c|1465| <<nvme_blk_write>> req->aiocb = blk_aio_pwritev(blk, offset, &req->sg.iov, 0, cb, req);
+ *   - hw/nvme/ctrl.c|2917| <<nvme_copy_out_cb>> iocb->aiocb = blk_aio_pwritev(ns->blkconf.blk, nvme_moff(ns, iocb->slba),
+ *   - hw/nvme/ctrl.c|3011| <<nvme_copy_in_completed_cb>> iocb->aiocb = blk_aio_pwritev(ns->blkconf.blk, nvme_l2b(ns, iocb->slba),
+ *   - hw/nvme/dif.c|530| <<nvme_dif_rw_mdata_out_cb>> req->aiocb = blk_aio_pwritev(blk, offset, &ctx->mdata.iov, 0,
+ *   - hw/nvme/dif.c|701| <<nvme_dif_rw>> req->aiocb = blk_aio_pwritev(ns->blkconf.blk, offset, &ctx->data.iov, 0,
+ *   - hw/scsi/scsi-disk.c|1846| <<scsi_write_same_complete>> r->req.aiocb = blk_aio_pwritev(s->qdev.conf.blk,
+ *   - hw/scsi/scsi-disk.c|1920| <<scsi_disk_emulate_write_same>> r->req.aiocb = blk_aio_pwritev(s->qdev.conf.blk,
+ *   - hw/scsi/scsi-disk.c|3109| <<scsi_dma_writev>> return blk_aio_pwritev(s->qdev.conf.blk, offset, iov, 0, cb, cb_opaque);
+ *   - qemu-img.c|4497| <<bench_cb>> acb = blk_aio_pwritev(b->blk, offset, b->qiov, 0, bench_cb, b);
+ *   - qemu-io-cmds.c|663| <<do_aio_writev>> blk_aio_pwritev(blk, offset, qiov, flags, aio_rw_done, &async_ret);
+ *   - qemu-io-cmds.c|1699| <<aio_write_f>> blk_aio_pwritev(blk, ctx->offset, &ctx->qiov, ctx->flags,
+ *   - system/dma-helpers.c|265| <<dma_blk_write_io_func>> return blk_aio_pwritev(blk, offset, iov, 0, cb, cb_opaque);
+ *   - tests/unit/test-replication.c|113| <<test_blk_write>> blk_aio_pwritev(blk, offset, &qiov, 0, blk_rw_done, &async_ret);
+ */
 BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,
                             QEMUIOVector *qiov, BdrvRequestFlags flags,
                             BlockCompletionFunc *cb, void *opaque)
@@ -2120,6 +2164,12 @@ void blk_drain_all(void)
     bdrv_drain_all_end();
 }
 
+/*
+ * called by:
+ *   - block/monitor/block-hmp-cmds.c|194| <<hmp_drive_del>> blk_set_on_error(blk, BLOCKDEV_ON_ERROR_REPORT, BLOCKDEV_ON_ERROR_REPORT);
+ *   - blockdev.c|639| <<blockdev_init>> blk_set_on_error(blk, on_read_error, on_write_error);
+ *   - hw/block/block.c|240| <<blkconf_apply_backend_options>> blk_set_on_error(blk, rerror, werror);
+ */
 void blk_set_on_error(BlockBackend *blk, BlockdevOnError on_read_error,
                       BlockdevOnError on_write_error)
 {
@@ -2128,12 +2178,34 @@ void blk_set_on_error(BlockBackend *blk, BlockdevOnError on_read_error,
     blk->on_write_error = on_write_error;
 }
 
+/*
+ * called by:
+ *   - block/block-backend.c|2170| <<blk_get_error_action>> BlockdevOnError on_err = blk_get_on_error(blk, is_read);
+ *   - hw/block/block.c|231| <<blkconf_apply_backend_options>> rerror = blk_get_on_error(blk, true);
+ *   - hw/block/block.c|236| <<blkconf_apply_backend_options>> werror = blk_get_on_error(blk, false);
+ *   - hw/block/fdc.c|532| <<floppy_drive_realize>> if (blk_get_on_error(dev->conf.blk, 0) != BLOCKDEV_ON_ERROR_ENOSPC &&
+ *   - hw/block/fdc.c|533| <<floppy_drive_realize>> blk_get_on_error(dev->conf.blk, 0) != BLOCKDEV_ON_ERROR_REPORT) {
+ *   - hw/block/fdc.c|537| <<floppy_drive_realize>> if (blk_get_on_error(dev->conf.blk, 1) != BLOCKDEV_ON_ERROR_REPORT) {
+ *   - hw/block/swim.c|240| <<swim_drive_realize>> if (blk_get_on_error(dev->conf.blk, 0) != BLOCKDEV_ON_ERROR_ENOSPC &&
+ *   - hw/block/swim.c|241| <<swim_drive_realize>> blk_get_on_error(dev->conf.blk, 0) != BLOCKDEV_ON_ERROR_REPORT) {
+ *   - hw/block/swim.c|245| <<swim_drive_realize>> if (blk_get_on_error(dev->conf.blk, 1) != BLOCKDEV_ON_ERROR_REPORT) {
+ *   - hw/scsi/scsi-generic.c|705| <<scsi_generic_realize>> if (blk_get_on_error(s->conf.blk, 0) != BLOCKDEV_ON_ERROR_ENOSPC &&
+ *   - hw/scsi/scsi-generic.c|706| <<scsi_generic_realize>> blk_get_on_error(s->conf.blk, 0) != BLOCKDEV_ON_ERROR_REPORT) {
+ *   - hw/scsi/scsi-generic.c|710| <<scsi_generic_realize>> if (blk_get_on_error(s->conf.blk, 1) != BLOCKDEV_ON_ERROR_REPORT) {
+ */
 BlockdevOnError blk_get_on_error(BlockBackend *blk, bool is_read)
 {
     IO_CODE();
     return is_read ? blk->on_read_error : blk->on_write_error;
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|79| <<virtio_blk_handle_rw_error>> BlockErrorAction action = blk_get_error_action(s->blk, is_read, error);
+ *   - hw/ide/ahci.c|1052| <<ncq_cb>> BlockErrorAction action = blk_get_error_action(ide_state->blk,
+ *   - hw/ide/core.c|860| <<ide_handle_rw_error>> BlockErrorAction action = blk_get_error_action(s->blk, is_read, error);
+ *   - hw/scsi/scsi-disk.c|228| <<scsi_handle_rw_error>> action = blk_get_error_action(s->qdev.conf.blk, is_read, error);
+ */
 BlockErrorAction blk_get_error_action(BlockBackend *blk, bool is_read,
                                       int error)
 {
diff --git a/block/file-posix.c b/block/file-posix.c
index b862406c7..cd366e4ba 100644
--- a/block/file-posix.c
+++ b/block/file-posix.c
@@ -1762,6 +1762,14 @@ static ssize_t handle_aiocb_rw_linear(RawPosixAIOData *aiocb, char *buf)
     return offset;
 }
 
+/*
+ * (gdb) bt
+ * #0  handle_aiocb_rw (opaque=0x7f25a45ea810) at ../block/file-posix.c:1767
+ * #1  0x00005579c6e1f097 in worker_thread (opaque=0x5579c95d2ec0) at ../util/thread-pool.c:111
+ * #2  0x00005579c6e027f2 in qemu_thread_start (args=0x5579c9ff0bb0) at ../util/qemu-thread-posix.c:541
+ * #3  0x00007f27bddc3ea5 in start_thread () at /lib64/libpthread.so.0
+ * #4  0x00007f27bdaec9fd in clone () at /lib64/libc.so.6
+ */
 static int handle_aiocb_rw(void *opaque)
 {
     RawPosixAIOData *aiocb = opaque;
@@ -2444,6 +2452,12 @@ static bool bdrv_qiov_is_aligned(BlockDriverState *bs, QEMUIOVector *qiov)
     return true;
 }
 
+/*
+ * called by:
+ *   - block/file-posix.c|2542| <<raw_co_preadv>> return raw_co_prw(bs, &offset, bytes, qiov, QEMU_AIO_READ);
+ *   - block/file-posix.c|2549| <<raw_co_pwritev>> return raw_co_prw(bs, &offset, bytes, qiov, QEMU_AIO_WRITE);
+ *   - block/file-posix.c|3532| <<raw_co_zone_append>> return raw_co_prw(bs, offset, len, qiov, QEMU_AIO_ZONE_APPEND);
+ */
 static int coroutine_fn raw_co_prw(BlockDriverState *bs, int64_t *offset_ptr,
                                    uint64_t bytes, QEMUIOVector *qiov, int type)
 {
diff --git a/block/io.c b/block/io.c
index 7e62fabbf..df2882c83 100644
--- a/block/io.c
+++ b/block/io.c
@@ -2239,6 +2239,15 @@ int coroutine_fn bdrv_co_pwritev(BdrvChild *child,
     return bdrv_co_pwritev_part(child, offset, bytes, qiov, 0, flags);
 }
 
+/*
+ * called by:
+ *   - block/block-backend.c|1450| <<blk_co_do_pwritev_part>> ret = bdrv_co_pwritev_part(blk->root, offset, bytes, qiov, qiov_offset,
+ *   - block/copy-on-read.c|191| <<cor_co_pwritev_part>> return bdrv_co_pwritev_part(bs->file, offset, bytes, qiov, qiov_offset,
+ *   - block/filter-compress.c|82| <<compress_co_pwritev_part>> return bdrv_co_pwritev_part(bs->file, offset, bytes, qiov, qiov_offset,
+ *   - block/io.c|2239| <<bdrv_co_pwritev>> return bdrv_co_pwritev_part(child, offset, bytes, qiov, 0, flags);
+ *   - block/preallocate.c|418| <<preallocate_co_pwritev_part>> return bdrv_co_pwritev_part(bs->file, offset, bytes, qiov, qiov_offset,
+ *   - block/qcow2.c|2623| <<qcow2_co_pwritev_task>> ret = bdrv_co_pwritev_part(s->data_file, host_offset,
+ */
 int coroutine_fn bdrv_co_pwritev_part(BdrvChild *child,
     int64_t offset, int64_t bytes, QEMUIOVector *qiov, size_t qiov_offset,
     BdrvRequestFlags flags)
diff --git a/cpu-common.c b/cpu-common.c
index c81fd72d1..204ac8fe1 100644
--- a/cpu-common.c
+++ b/cpu-common.c
@@ -130,6 +130,12 @@ struct qemu_work_item {
     bool free, exclusive, done;
 };
 
+/*
+ * called by:
+ *   - cpu-common.c|159| <<do_run_on_cpu>> queue_work_on_cpu(cpu, &wi);
+ *   - cpu-common.c|177| <<async_run_on_cpu>> queue_work_on_cpu(cpu, wi);
+ *   - cpu-common.c|331| <<async_safe_run_on_cpu>> queue_work_on_cpu(cpu, wi);
+ */
 static void queue_work_on_cpu(CPUState *cpu, struct qemu_work_item *wi)
 {
     qemu_mutex_lock(&cpu->work_mutex);
@@ -140,6 +146,10 @@ static void queue_work_on_cpu(CPUState *cpu, struct qemu_work_item *wi)
     qemu_cpu_kick(cpu);
 }
 
+/*
+ * called by:
+ *   - system/cpus.c|399| <<run_on_cpu>> do_run_on_cpu(cpu, func, data, &qemu_global_mutex);
+ */
 void do_run_on_cpu(CPUState *cpu, run_on_cpu_func func, run_on_cpu_data data,
                    QemuMutex *mutex)
 {
diff --git a/hw/core/cpu-common.c b/hw/core/cpu-common.c
index 82dae51a5..3b4ff031b 100644
--- a/hw/core/cpu-common.c
+++ b/hw/core/cpu-common.c
@@ -105,6 +105,9 @@ void cpu_dump_state(CPUState *cpu, FILE *f, int flags)
 
     if (cc->dump_state) {
         cpu_synchronize_state(cpu);
+	/*
+	 * x86_cpu_dump_state()
+	 */
         cc->dump_state(cpu, f, flags);
     }
 }
@@ -210,6 +213,11 @@ static void cpu_common_realizefn(DeviceState *dev, Error **errp)
     }
 
     if (dev->hotplugged) {
+        /*
+	 * x86_64的调用
+	 *   - hw/core/cpu-common.c|213| <<cpu_common_realizefn>> cpu_synchronize_post_init(cpu);
+	 *   - system/cpus.c|155| <<cpu_synchronize_all_post_init>> cpu_synchronize_post_init(cpu);
+	 */
         cpu_synchronize_post_init(cpu);
         cpu_resume(cpu);
     }
diff --git a/hw/core/qdev.c b/hw/core/qdev.c
index 43d863b0c..4df826fab 100644
--- a/hw/core/qdev.c
+++ b/hw/core/qdev.c
@@ -467,6 +467,10 @@ static bool check_only_migratable(Object *obj, Error **errp)
     return true;
 }
 
+/*
+ * 在以下使用device_set_realized():
+ *   - hw/core/qdev.c|838| <<device_class_init>> object_class_property_add_bool(class, "realized", device_get_realized, device_set_realized);
+ */
 static void device_set_realized(Object *obj, bool value, Error **errp)
 {
     DeviceState *dev = DEVICE(obj);
@@ -881,11 +885,36 @@ Object *qdev_get_machine(void)
 
 static MachineInitPhase machine_phase;
 
+/*
+ * called by:
+ *   - hw/core/machine-qmp-cmds.c|141| <<qmp_set_numa_node>> if (phase_check(PHASE_MACHINE_INITIALIZED)) {
+ *   - hw/core/machine.c|1519| <<qemu_add_machine_init_done_notifier>> if (phase_check(PHASE_MACHINE_READY)) {
+ *   - hw/core/qdev.c|656| <<device_initfn>> if (phase_check(PHASE_MACHINE_READY)) {
+ *   - hw/pci/pci.c|1205| <<do_pci_register_device>> if (phase_check(PHASE_MACHINE_READY)) {
+ *   - hw/remote/machine.c|84| <<remote_machine_set_vfio_user>> if (phase_check(PHASE_MACHINE_CREATED)) {
+ *   - hw/remote/vfio-user-obj.c|735| <<vfu_object_init_ctx>> !phase_check(PHASE_MACHINE_READY)) {
+ *   - hw/remote/vfio-user-obj.c|864| <<vfu_object_init>> if (!phase_check(PHASE_MACHINE_READY)) {
+ *   - hw/usb/core.c|100| <<usb_wakeup>> if (!phase_check(PHASE_MACHINE_READY)) {
+ *   - monitor/hmp.c|218| <<cmd_available>> return phase_check(PHASE_MACHINE_READY) || cmd_can_preconfig(cmd);
+ *   - system/qdev-monitor.c|261| <<qdev_get_device_class>> (phase_check(PHASE_MACHINE_READY) && !dc->hotpluggable)) {
+ *   - system/qdev-monitor.c|671| <<qdev_device_add_from_qdict>> if (phase_check(PHASE_MACHINE_READY) && bus && !qbus_is_hotpluggable(bus)) {
+ *   - system/qdev-monitor.c|685| <<qdev_device_add_from_qdict>> if (phase_check(PHASE_MACHINE_READY)) {
+ *   - system/qdev-monitor.c|1146| <<qmp_command_available>> if (!phase_check(PHASE_MACHINE_READY) &&
+ *   - system/vl.c|2699| <<qmp_x_exit_preconfig>> if (phase_check(PHASE_MACHINE_INITIALIZED)) {
+ *   - ui/console.c|403| <<qemu_console_register>> } else if (!QEMU_IS_GRAPHIC_CONSOLE(c) || phase_check(PHASE_MACHINE_READY)) {
+ */
 bool phase_check(MachineInitPhase phase)
 {
     return machine_phase >= phase;
 }
 
+/*
+ * called by:
+ *   - hw/core/machine.c|1510| <<machine_run_board_init>> phase_advance(PHASE_MACHINE_INITIALIZED);
+ *   - hw/core/machine.c|1542| <<qdev_machine_creation_done>> phase_advance(PHASE_MACHINE_READY);
+ *   - system/vl.c|3691| <<qemu_init>> phase_advance(PHASE_MACHINE_CREATED);
+ *   - system/vl.c|3698| <<qemu_init>> phase_advance(PHASE_ACCEL_CREATED);
+ */
 void phase_advance(MachineInitPhase phase)
 {
     assert(machine_phase == phase - 1);
diff --git a/hw/core/vm-change-state-handler.c b/hw/core/vm-change-state-handler.c
index 8e2639224..f18246074 100644
--- a/hw/core/vm-change-state-handler.c
+++ b/hw/core/vm-change-state-handler.c
@@ -52,10 +52,21 @@ static int qdev_get_dev_tree_depth(DeviceState *dev)
  *
  * Returns: an entry to be freed with qemu_del_vm_change_state_handler()
  */
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1654| <<virtio_blk_device_realize>> qdev_add_vm_change_state_handler(dev, virtio_blk_dma_restart_cb, s);
+ *   - hw/scsi/scsi-bus.c|306| <<scsi_qdev_realize>> dev->vmsentry = qdev_add_vm_change_state_handler(DEVICE(dev), scsi_dma_restart_cb, dev);
+ *   - hw/virtio/virtio.c|3254| <<virtio_init>> vdev->vmstate = qdev_add_vm_change_state_handler(DEVICE(vdev), virtio_vmstate_change, vdev);
+ */
 VMChangeStateEntry *qdev_add_vm_change_state_handler(DeviceState *dev,
                                                      VMChangeStateHandler *cb,
                                                      void *opaque)
 {
+    /*
+     * called by:
+     *   - hw/core/vm-change-state-handler.c|65| <<qdev_add_vm_change_state_handler>> return qdev_add_vm_change_state_handler_full(dev, cb, NULL, opaque);
+     *   - hw/vfio/migration.c|873| <<vfio_migration_init>> migration->vm_state = qdev_add_vm_change_state_handler_full(vbasedev->dev, vfio_vmstate_change, prepare_cb, vbasedev);
+     */
     return qdev_add_vm_change_state_handler_full(dev, cb, NULL, opaque);
 }
 
@@ -63,12 +74,22 @@ VMChangeStateEntry *qdev_add_vm_change_state_handler(DeviceState *dev,
  * Exactly like qdev_add_vm_change_state_handler() but passes a prepare_cb
  * argument too.
  */
+/*
+ * called by:
+ *   - hw/core/vm-change-state-handler.c|65| <<qdev_add_vm_change_state_handler>> return qdev_add_vm_change_state_handler_full(dev, cb, NULL, opaque);
+ *   - hw/vfio/migration.c|873| <<vfio_migration_init>> migration->vm_state = qdev_add_vm_change_state_handler_full(vbasedev->dev, vfio_vmstate_change, prepare_cb, vbasedev);
+ */
 VMChangeStateEntry *qdev_add_vm_change_state_handler_full(
     DeviceState *dev, VMChangeStateHandler *cb,
     VMChangeStateHandler *prepare_cb, void *opaque)
 {
     int depth = qdev_get_dev_tree_depth(dev);
 
+    /*
+     * called by:
+     *   - hw/core/vm-change-state-handler.c|78| <<qdev_add_vm_change_state_handler_full>> return qemu_add_vm_change_state_handler_prio_full(cb, prepare_cb, opaque,
+     *   - system/runstate.c|298| <<qemu_add_vm_change_state_handler_prio>> return qemu_add_vm_change_state_handler_prio_full(cb, NULL, opaque, priority);
+     */
     return qemu_add_vm_change_state_handler_prio_full(cb, prepare_cb, opaque,
                                                       depth);
 }
diff --git a/hw/i386/intel_iommu.c b/hw/i386/intel_iommu.c
index 5085a6fee..7c79aaaa1 100644
--- a/hw/i386/intel_iommu.c
+++ b/hw/i386/intel_iommu.c
@@ -1554,6 +1554,11 @@ static uint16_t vtd_get_domain_id(IntelIOMMUState *s,
     return VTD_CONTEXT_ENTRY_DID(ce->hi);
 }
 
+/*
+ * called by:
+ *   - hw/i386/intel_iommu.c|1610| <<vtd_address_space_sync>> return vtd_sync_shadow_page_table_range(vtd_as, &ce, 0, UINT64_MAX);
+ *   - hw/i386/intel_iommu.c|2215| <<vtd_iotlb_page_invalidate_notify>> vtd_sync_shadow_page_table_range(vtd_as, &ce, addr, size);
+ */
 static int vtd_sync_shadow_page_table_range(VTDAddressSpace *vtd_as,
                                             VTDContextEntry *ce,
                                             hwaddr addr, hwaddr size)
diff --git a/hw/i386/kvm/apic.c b/hw/i386/kvm/apic.c
index 1e89ca089..2e8be09c4 100644
--- a/hw/i386/kvm/apic.c
+++ b/hw/i386/kvm/apic.c
@@ -155,6 +155,17 @@ static void kvm_apic_post_load(APICCommonState *s)
     run_on_cpu(CPU(s->cpu), kvm_apic_put, RUN_ON_CPU_HOST_PTR(s));
 }
 
+/*
+ * (gdb) bt
+ * #0  do_inject_external_nmi (cpu=0x5555573a4d30, data=...) at ../hw/i386/kvm/apic.c:160
+ * #1  0x000055555586a857 in process_queued_cpu_work (cpu=0x5555573a4d30) at ../cpu-common.c:360
+ * #2  0x0000555555b6047f in qemu_wait_io_event_common (cpu=0x5555573a4d30) at ../system/cpus.c:419
+ * #3  0x0000555555b60510 in qemu_wait_io_event (cpu=0x5555573a4d30) at ../system/cpus.c:437
+ * #4  0x0000555555df631e in kvm_vcpu_thread_fn (arg=0x5555573a4d30) at ../accel/kvm/kvm-accel-ops.c:56
+ * #5  0x00005555560017cc in qemu_thread_start (args=0x5555573aed40) at ../util/qemu-thread-posix.c:541
+ * #6  0x00007ffff749f812 in start_thread () at /lib64/libc.so.6
+ * #7  0x00007ffff743f450 in clone3 () at /lib64/libc.so.6
+ */
 static void do_inject_external_nmi(CPUState *cpu, run_on_cpu_data data)
 {
     APICCommonState *s = data.host_ptr;
diff --git a/hw/i386/x86.c b/hw/i386/x86.c
index 2b6291ad8..ca372ca89 100644
--- a/hw/i386/x86.c
+++ b/hw/i386/x86.c
@@ -508,6 +508,33 @@ const CPUArchIdList *x86_possible_cpu_arch_ids(MachineState *ms)
     return ms->possible_cpus;
 }
 
+/*
+ * (gdb) bt
+ * #0  x86_nmi (n=0x5555572bf2d0, cpu_index=2, errp=0x7fffffffce88) at ../hw/i386/x86.c:512
+ * #1  0x0000555555924607 in do_nmi (o=0x5555572bf2d0, opaque=0x7fffffffce80) at ../hw/core/nmi.c:45
+ * #2  0x0000555555e0f14e in do_object_child_foreach (obj=0x5555572bf7f0, fn=0x555555924594 <do_nmi>, opaque=0x7fffffffce80, recurse=false) at ../qom/object.c:1135
+ * #3  0x0000555555e0f1e4 in object_child_foreach (obj=0x5555572bf7f0, fn=0x555555924594 <do_nmi>, opaque=0x7fffffffce80) at ../qom/object.c:1153
+ * #4  0x000055555592465f in nmi_children (o=0x5555572bf7f0, ns=0x7fffffffce80) at ../hw/core/nmi.c:57
+ * #5  0x00005555559246a9 in nmi_monitor_handle (cpu_index=2, errp=0x7fffffffcee0) at ../hw/core/nmi.c:68
+ * #6  0x0000555555b61397 in qmp_inject_nmi (errp=0x7fffffffcee0) at ../system/cpus.c:827
+ * #7  0x000055555591eeea in hmp_nmi (mon=0x5555572d5bd0, qdict=0x55555813ce70) at ../hw/core/machine-hmp-cmds.c:233
+ * #8  0x0000555555bc5c9a in handle_hmp_command_exec (mon=0x5555572d5bd0, cmd=0x555556ee43e0 <hmp_cmds+5440>, qdict=0x55555813ce70) at ../monitor/hmp.c:1106
+ * #9  0x0000555555bc5ec7 in handle_hmp_command (mon=0x5555572d5bd0, cmdline=0x55555733c2a3 "") at ../monitor/hmp.c:1158
+ * #10 0x0000555555bc32df in monitor_command_cb (opaque=0x5555572d5bd0, cmdline=0x55555733c2a0 "nmi", readline_opaque=0x0) at ../monitor/hmp.c:47
+ * #11 0x0000555556032b80 in readline_handle_byte (rs=0x55555733c2a0, ch=13) at ../util/readline.c:419
+ * #12 0x0000555555bc698a in monitor_read (opaque=0x5555572d5bd0, buf=0x7fffffffd140 "\rЯd\376\177", size=1) at ../monitor/hmp.c:1390
+ * #13 0x0000555555f32a60 in qemu_chr_be_write_impl (s=0x555557100740, buf=0x7fffffffd140 "\rЯd\376\177", len=1) at ../chardev/char.c:202
+ * #14 0x0000555555f32ac4 in qemu_chr_be_write (s=0x555557100740, buf=0x7fffffffd140 "\rЯd\376\177", len=1) at ../chardev/char.c:214
+ * #15 0x0000555555f355da in fd_chr_read (chan=0x5555572a0670, cond=G_IO_IN, opaque=0x555557100740) at ../chardev/char-fd.c:72
+ * #16 0x0000555555e21ca2 in qio_channel_fd_source_dispatch (source=0x55555743df90, callback=0x555555f354b0 <fd_chr_read>, user_data=0x555557100740) at ../io/channel-watch.c:84
+ * #17 0x00007ffff7b33e2f in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #18 0x000055555601b933 in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #19 0x000055555601b9b0 in os_host_main_loop_wait (timeout=142674550) at ../util/main-loop.c:313
+ * #20 0x000055555601babe in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #21 0x0000555555b6ea18 in qemu_main_loop () at ../system/runstate.c:782
+ * #22 0x0000555555e02d7d in qemu_default_main () at ../system/main.c:37
+ * #23 0x0000555555e02db8 in main (argc=23, argv=0x7fffffffe448) at ../system/main.c:48
+ */
 static void x86_nmi(NMIState *n, int cpu_index, Error **errp)
 {
     /* cpu index isn't used */
diff --git a/hw/input/pckbd.c b/hw/input/pckbd.c
index b92b63bed..326fc8472 100644
--- a/hw/input/pckbd.c
+++ b/hw/input/pckbd.c
@@ -612,6 +612,50 @@ static int kbd_post_load(void *opaque, int version_id)
     return 0;
 }
 
+/*
+ * typedef struct KBDState {
+ *     uint8_t write_cmd; // if non zero, write data to port 60 is expected
+ *     uint8_t status;
+ *     uint8_t mode;
+ *     uint8_t outport;
+ *     uint32_t migration_flags;
+ *     uint32_t obsrc;
+ *     bool outport_present;
+ *     bool extended_state;
+ *     bool extended_state_loaded;
+ *     // Bitmask of devices with data available.
+ *     uint8_t pending;
+ *     uint8_t obdata;
+ *     uint8_t cbdata;
+ *     uint8_t pending_tmp;
+ *     PS2KbdState ps2kbd;
+ *     PS2MouseState ps2mouse;
+ *     QEMUTimer *throttle_timer;
+ *
+ *     qemu_irq irqs[2];
+ *     qemu_irq a20_out;
+ *     hwaddr mask; 
+ * } KBDState;
+ *
+ * struct VMStateField {
+ *     const char *name;
+ *     const char *err_hint;
+ *     size_t offset;
+ *     size_t size;
+ *     size_t start;
+ *     int num;
+ *     size_t num_offset;
+ *     size_t size_offset;
+ *     const VMStateInfo *info;
+ *     enum VMStateFlags flags;
+ *     const VMStateDescription *vmsd;
+ *     int version_id;
+ *     int struct_version_id;
+ *     bool (*field_exists)(void *opaque, int version_id);
+ * };
+ *
+ * 为什么弄subsection
+ */
 static const VMStateDescription vmstate_kbd = {
     .name = "pckbd",
     .version_id = 3,
diff --git a/hw/intc/apic_common.c b/hw/intc/apic_common.c
index bccb4241c..0b7b4ea1a 100644
--- a/hw/intc/apic_common.c
+++ b/hw/intc/apic_common.c
@@ -121,6 +121,34 @@ void apic_handle_tpr_access_report(DeviceState *dev, target_ulong ip,
     vapic_report_tpr_access(s->vapic, CPU(s->cpu), ip, access);
 }
 
+/*
+ * (gdb) bt
+ * #0  apic_deliver_nmi (dev=0x5555570cb940) at ../hw/intc/apic_common.c:126
+ * #1  0x0000555555c34f91 in x86_nmi (n=0x5555572bf2d0, cpu_index=2, errp=0x7fffffffce88) at ../hw/i386/x86.c:522
+ * #2  0x0000555555924607 in do_nmi (o=0x5555572bf2d0, opaque=0x7fffffffce80) at ../hw/core/nmi.c:45
+ * #3  0x0000555555e0f14e in do_object_child_foreach (obj=0x5555572bf7f0, fn=0x555555924594 <do_nmi>, opaque=0x7fffffffce80, recurse=false) at ../qom/object.c:1135
+ * #4  0x0000555555e0f1e4 in object_child_foreach (obj=0x5555572bf7f0, fn=0x555555924594 <do_nmi>, opaque=0x7fffffffce80) at ../qom/object.c:1153
+ * #5  0x000055555592465f in nmi_children (o=0x5555572bf7f0, ns=0x7fffffffce80) at ../hw/core/nmi.c:57
+ * #6  0x00005555559246a9 in nmi_monitor_handle (cpu_index=2, errp=0x7fffffffcee0) at ../hw/core/nmi.c:68
+ * #7  0x0000555555b61397 in qmp_inject_nmi (errp=0x7fffffffcee0) at ../system/cpus.c:827
+ * #8  0x000055555591eeea in hmp_nmi (mon=0x5555572d5bd0, qdict=0x55555813ce70) at ../hw/core/machine-hmp-cmds.c:233
+ * #9  0x0000555555bc5c9a in handle_hmp_command_exec (mon=0x5555572d5bd0, cmd=0x555556ee43e0 <hmp_cmds+5440>, qdict=0x55555813ce70) at ../monitor/hmp.c:1106
+ * #10 0x0000555555bc5ec7 in handle_hmp_command (mon=0x5555572d5bd0, cmdline=0x55555733c2a3 "") at ../monitor/hmp.c:1158
+ * #11 0x0000555555bc32df in monitor_command_cb (opaque=0x5555572d5bd0, cmdline=0x55555733c2a0 "nmi", readline_opaque=0x0) at ../monitor/hmp.c:47
+ * #12 0x0000555556032b80 in readline_handle_byte (rs=0x55555733c2a0, ch=13) at ../util/readline.c:419
+ * #13 0x0000555555bc698a in monitor_read (opaque=0x5555572d5bd0, buf=0x7fffffffd140 "\rЯd\376\177", size=1) at ../monitor/hmp.c:1390
+ * #14 0x0000555555f32a60 in qemu_chr_be_write_impl (s=0x555557100740, buf=0x7fffffffd140 "\rЯd\376\177", len=1) at ../chardev/char.c:202
+ * #15 0x0000555555f32ac4 in qemu_chr_be_write (s=0x555557100740, buf=0x7fffffffd140 "\rЯd\376\177", len=1) at ../chardev/char.c:214
+ * #16 0x0000555555f355da in fd_chr_read (chan=0x5555572a0670, cond=G_IO_IN, opaque=0x555557100740) at ../chardev/char-fd.c:72
+ * #17 0x0000555555e21ca2 in qio_channel_fd_source_dispatch (source=0x55555743df90, callback=0x555555f354b0 <fd_chr_read>, user_data=0x555557100740) at ../io/channel-watch.c:84
+ * #18 0x00007ffff7b33e2f in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #19 0x000055555601b933 in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #20 0x000055555601b9b0 in os_host_main_loop_wait (timeout=142674550) at ../util/main-loop.c:313
+ * #21 0x000055555601babe in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #22 0x0000555555b6ea18 in qemu_main_loop () at ../system/runstate.c:782
+ * #23 0x0000555555e02d7d in qemu_default_main () at ../system/main.c:37
+ * #24 0x0000555555e02db8 in main (argc=23, argv=0x7fffffffe448) at ../system/main.c:48
+ */
 void apic_deliver_nmi(DeviceState *dev)
 {
     APICCommonState *s = APIC_COMMON(dev);
diff --git a/hw/net/virtio-net.c b/hw/net/virtio-net.c
index 80c56f0cf..1bc64488c 100644
--- a/hw/net/virtio-net.c
+++ b/hw/net/virtio-net.c
@@ -3867,6 +3867,27 @@ static struct vhost_dev *virtio_net_get_vhost(VirtIODevice *vdev)
     return &net->dev;
 }
 
+/*
+ * struct VMStateField {
+ *     const char *name;
+ *     const char *err_hint;
+ *     size_t offset;
+ *     size_t size;
+ *     size_t start;
+ *     int num;
+ *     size_t num_offset;
+ *     size_t size_offset;
+ *     const VMStateInfo *info; 
+ *     enum VMStateFlags flags;
+ *     const VMStateDescription *vmsd;
+ *     int version_id;
+ *     int struct_version_id;
+ *     bool (*field_exists)(void *opaque, int version_id);
+ * };
+ *
+ * 在以下使用vmstate_virtio_net:
+ *   - hw/net/virtio-net.c|3961| <<virtio_net_class_init>> dc->vmsd = &vmstate_virtio_net;
+ */
 static const VMStateDescription vmstate_virtio_net = {
     .name = "virtio-net",
     .minimum_version_id = VIRTIO_NET_VM_VERSION,
diff --git a/hw/scsi/scsi-bus.c b/hw/scsi/scsi-bus.c
index fc4b77fdb..2fbb6692d 100644
--- a/hw/scsi/scsi-bus.c
+++ b/hw/scsi/scsi-bus.c
@@ -144,6 +144,10 @@ void scsi_bus_init_named(SCSIBus *bus, size_t bus_size, DeviceState *host,
     qbus_set_bus_hotplug_handler(BUS(bus));
 }
 
+/*
+ * 在以下使用scsi_dma_restart_bh():
+ *   - hw/scsi/scsi-bus.c|213| <<scsi_dma_restart_cb>> s->bh = aio_bh_new_guarded(ctx, scsi_dma_restart_bh, s, &DEVICE(s)->mem_reentrancy_guard);
+ */
 static void scsi_dma_restart_bh(void *opaque)
 {
     SCSIDevice *s = opaque;
@@ -175,13 +179,35 @@ static void scsi_dma_restart_bh(void *opaque)
     object_unref(OBJECT(s));
 }
 
+/*
+ * called by:
+ *   - hw/scsi/scsi-disk.c|249| <<scsi_handle_rw_error>> scsi_req_retry(&r->req);
+ */
 void scsi_req_retry(SCSIRequest *req)
 {
+    /*
+     * 在以下设置SCSIRequest->retry:
+     *   - hw/scsi/scsi-bus.c|159| <<scsi_dma_restart_bh>> req->retry = false;
+     *   - hw/scsi/scsi-bus.c|182| <<scsi_req_retry>> req->retry = true;
+     *   - hw/scsi/scsi-bus.c|914| <<scsi_req_dequeue>> req->retry = false;
+     *   - hw/scsi/scsi-bus.c|1789| <<get_scsi_requests>> req->retry = (sbyte == 1);
+     * 在以下使用SCSIRequest->retry:
+     *   - hw/scsi/scsi-bus.c|158| <<scsi_dma_restart_bh>> if (req->retry) {
+     *   - hw/scsi/scsi-bus.c|903| <<scsi_req_enqueue>> assert(!req->retry);
+     *   - hw/scsi/scsi-bus.c|1752| <<put_scsi_requests>> qemu_put_sbyte(f, req->retry ? 1 : 2);
+     *   - hw/scsi/scsi-disk.c|154| <<scsi_disk_save_request>> } else if (!req->retry) {
+     *   - hw/scsi/scsi-disk.c|173| <<scsi_disk_load_request>> } else if (!r->req.retry) {
+     */
+
     /* No need to save a reference, because scsi_dma_restart_bh just
      * looks at the request list.  */
     req->retry = true;
 }
 
+/*
+ * 在以下使用scsi_dma_restart_cb():
+ *   - hw/scsi/scsi-bus.c|307| <<scsi_qdev_realize>> dev->vmsentry = qdev_add_vm_change_state_handler(DEVICE(dev), scsi_dma_restart_cb, dev);
+ */
 static void scsi_dma_restart_cb(void *opaque, bool running, RunState state)
 {
     SCSIDevice *s = opaque;
@@ -908,6 +934,53 @@ int32_t scsi_req_enqueue(SCSIRequest *req)
     return rc;
 }
 
+/*
+ * (gdb) bt
+ * #0  scsi_req_dequeue (req=0x555557b18760) at ../hw/scsi/scsi-bus.c:913
+ * #1  0x0000555555a645a1 in scsi_req_complete (req=0x555557b18760, status=0) at ../hw/scsi/scsi-bus.c:1527
+ * #2  0x0000555555a6819e in scsi_dma_complete_noio (r=0x555557b18760, ret=0) at ../hw/scsi/scsi-disk.c:352
+ * #3  0x0000555555a6829c in scsi_dma_complete (opaque=0x555557b18760, ret=0) at ../hw/scsi/scsi-disk.c:373
+ * #4  0x0000555555b5a830 in dma_complete (dbs=0x5555570a4a50, ret=0) at ../system/dma-helpers.c:107
+ * #5  0x0000555555b5a8f5 in dma_blk_cb (opaque=0x5555570a4a50, ret=0) at ../system/dma-helpers.c:127
+ * #6  0x0000555555e6ec98 in blk_aio_complete (acb=0x555557b8b3d0) at ../block/block-backend.c:1580
+ * #7  0x0000555555e6eec3 in blk_aio_read_entry (opaque=0x555557b8b3d0) at ../block/block-backend.c:1635
+ * #8  0x0000555555ffeba5 in coroutine_trampoline (i0=-466575504, i1=32767) at ../util/coroutine-ucontext.c:177
+ * #9  0x00007ffff4424150 in __start_context () at /lib/../lib64/libc.so.6
+ * #10 0x00007fffed5f9830 in  ()
+ * #11 0x0000000000000000 in  ()
+ *
+ * (gdb) bt
+ * #0  scsi_req_dequeue (req=0x555557b18760) at ../hw/scsi/scsi-bus.c:913
+ * #1  0x0000555555a645a1 in scsi_req_complete (req=0x555557b18760, status=0) at ../hw/scsi/scsi-bus.c:1527
+ * #2  0x0000555555a6b0f0 in scsi_disk_emulate_read_data (req=0x555557b18760) at ../hw/scsi/scsi-disk.c:1475
+ * #3  0x0000555555a63f83 in scsi_req_continue (req=0x555557b18760) at ../hw/scsi/scsi-bus.c:1412
+ * #4  0x0000555555a64155 in scsi_req_data (req=0x555557b18760, len=254) at ../hw/scsi/scsi-bus.c:1448
+ * #5  0x0000555555a6b0dd in scsi_disk_emulate_read_data (req=0x555557b18760) at ../hw/scsi/scsi-disk.c:1470
+ * #6  0x0000555555a63f83 in scsi_req_continue (req=0x555557b18760) at ../hw/scsi/scsi-bus.c:1412
+ * #7  0x0000555555d18e78 in virtio_scsi_handle_cmd_req_submit (s=0x55555806f8a0, req=0x7ffe540091d0) at ../hw/scsi/virtio-scsi.c:812
+ * #8  0x0000555555d190a1 in virtio_scsi_handle_cmd_vq (s=0x55555806f8a0, vq=0x5555580ac9f8) at ../hw/scsi/virtio-scsi.c:854
+ * #9  0x0000555555d1911f in virtio_scsi_handle_cmd (vdev=0x55555806f8a0, vq=0x5555580ac9f8) at ../hw/scsi/virtio-scsi.c:868
+ * #10 0x0000555555d47131 in virtio_queue_notify_vq (vq=0x5555580ac9f8) at ../hw/virtio/virtio.c:2268
+ * #11 0x0000555555d4a4ec in virtio_queue_host_notifier_read (n=0x5555580aca6c) at ../hw/virtio/virtio.c:3590
+ * #12 0x0000555555fdda41 in aio_dispatch_handler (ctx=0x555557008060, node=0x555558004560) at ../util/aio-posix.c:372
+ * #13 0x0000555555fddbd9 in aio_dispatch_handlers (ctx=0x555557008060) at ../util/aio-posix.c:414
+ * #14 0x0000555555fddc2f in aio_dispatch (ctx=0x555557008060) at ../util/aio-posix.c:424
+ * #15 0x0000555555ffb2c5 in aio_ctx_dispatch (source=0x555557008060, callback=0x0, user_data=0x0) at ../util/async.c:358
+ * #16 0x00007ffff644daed in g_main_context_dispatch () at /lib/../lib64/libglib-2.0.so.0
+ * #17 0x0000555555ffc83a in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #18 0x0000555555ffc8b7 in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:313
+ * #19 0x0000555555ffc9c5 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #20 0x0000555555b6498b in qemu_main_loop () at ../system/runstate.c:782
+ * #21 0x0000555555de8fbc in qemu_default_main () at ../system/main.c:37
+ * #22 0x0000555555de8ff7 in main (argc=30, argv=0x7fffffffdb78) at ../system/main.c:48
+ *
+ * called by:
+ *   - hw/scsi/scsi-bus.c|170| <<scsi_dma_restart_bh>> scsi_req_dequeue(req);
+ *   - hw/scsi/scsi-bus.c|1524| <<scsi_req_complete_failed>> scsi_req_dequeue(req);
+ *   - hw/scsi/scsi-bus.c|1553| <<scsi_req_complete>> scsi_req_dequeue(req);
+ *   - hw/scsi/scsi-bus.c|1592| <<scsi_req_cancel_async>> scsi_req_dequeue(req);
+ *   - hw/scsi/scsi-bus.c|1610| <<scsi_req_cancel>> scsi_req_dequeue(req);
+ */
 static void scsi_req_dequeue(SCSIRequest *req)
 {
     trace_scsi_req_dequeue(req->dev->id, req->lun, req->tag);
diff --git a/hw/scsi/scsi-disk.c b/hw/scsi/scsi-disk.c
index 6691f5edb..2fb1c1466 100644
--- a/hw/scsi/scsi-disk.c
+++ b/hw/scsi/scsi-disk.c
@@ -189,6 +189,11 @@ static void scsi_disk_load_request(QEMUFile *f, SCSIRequest *req)
  * scsi_handle_rw_error always manages its reference counts, independent
  * of the return value.
  */
+/*
+ * called by:
+ *   - hw/scsi/scsi-disk.c|265| <<scsi_disk_req_check_error>> return scsi_handle_rw_error(r, ret, acct_failed);
+ *   - hw/scsi/scsi-disk.c|2831| <<scsi_block_sgio_complete>> if (scsi_handle_rw_error(r, ret, true)) {
+ */
 static bool scsi_handle_rw_error(SCSIDiskReq *r, int ret, bool acct_failed)
 {
     bool is_read = (r->req.cmd.mode == SCSI_XFER_FROM_DEV);
@@ -254,6 +259,16 @@ static bool scsi_handle_rw_error(SCSIDiskReq *r, int ret, bool acct_failed)
     }
 }
 
+/*
+ * called by:
+ *   - hw/scsi/scsi-disk.c|281| <<scsi_aio_complete>> if (scsi_disk_req_check_error(r, ret, true)) {
+ *   - hw/scsi/scsi-disk.c|340| <<scsi_dma_complete_noio>> if (scsi_disk_req_check_error(r, ret, false)) {
+ *   - hw/scsi/scsi-disk.c|379| <<scsi_read_complete_noio>> if (scsi_disk_req_check_error(r, ret, false)) {
+ *   - hw/scsi/scsi-disk.c|419| <<scsi_do_read>> if (scsi_disk_req_check_error(r, ret, false)) {
+ *   - hw/scsi/scsi-disk.c|511| <<scsi_write_complete_noio>> if (scsi_disk_req_check_error(r, ret, false)) {
+ *   - hw/scsi/scsi-disk.c|1750| <<scsi_unmap_complete>> if (scsi_disk_req_check_error(r, ret, true)) {
+ *   - hw/scsi/scsi-disk.c|1830| <<scsi_write_same_complete>> if (scsi_disk_req_check_error(r, ret, true)) {
+ */
 static bool scsi_disk_req_check_error(SCSIDiskReq *r, int ret, bool acct_failed)
 {
     if (r->req.io_canceled) {
diff --git a/hw/vfio/common.c b/hw/vfio/common.c
index e70fdf5e0..2435c0afa 100644
--- a/hw/vfio/common.c
+++ b/hw/vfio/common.c
@@ -289,6 +289,32 @@ static bool vfio_get_xlat_addr(IOMMUTLBEntry *iotlb, void **vaddr,
     return ret;
 }
 
+/*
+ * (gdb) bt
+ * #0  vfio_iommu_map_notify (n=0x7fefd80c9388, iotlb=0x7fffed379ff0) at ../hw/vfio/common.c:293
+ * #1  0x0000555555d79e23 in memory_region_notify_iommu_one (notifier=0x7fefd80c9388, event=0x7fffed37a070) at ../system/memory.c:2015
+ * #2  0x0000555555d79f3e in memory_region_notify_iommu (iommu_mr=0x55555803a9d0, iommu_idx=0, event=...) at ../system/memory.c:2042
+ * #3  0x0000555555b1673e in virtio_iommu_notify_map_unmap (mr=0x55555803a9d0, event=0x7fffed37a130, virt_start=4294422528, virt_end=4294426623) at ../hw/virtio/virtio-iommu.c:222
+ * #4  0x0000555555b16847 in virtio_iommu_notify_map (mr=0x55555803a9d0, virt_start=4294422528, virt_end=4294426623, paddr=4653244416, flags=2) at ../hw/virtio/virtio-iommu.c:251
+ * #5  0x0000555555b1742f in virtio_iommu_map (s=0x555557f600e0, req=0x7fffed37a210) at ../hw/virtio/virtio-iommu.c:595
+ * #6  0x0000555555b179db in virtio_iommu_handle_map (s=0x555557f600e0, iov=0x7fffe4034a68, iov_cnt=1) at ../hw/virtio/virtio-iommu.c:739
+ * #7  0x0000555555b17ccb in virtio_iommu_handle_command (vdev=0x555557f600e0, vq=0x555557f68b80) at ../hw/virtio/virtio-iommu.c:796
+ * #8  0x0000555555d47215 in virtio_queue_notify (vdev=0x555557f600e0, n=0) at ../hw/virtio/virtio.c:2288
+ * #9  0x0000555555b0b09f in virtio_pci_notify_write (opaque=0x555557f57d10, addr=0, val=0, size=2) at ../hw/virtio/virtio-pci.c:1688
+ * #10 0x0000555555d75521 in memory_region_write_accessor (mr=0x555557f58bb0, addr=0, value=0x7fffed37a448, size=2, shift=0, mask=65535, attrs=...) at ../system/memory.c:497
+ * #11 0x0000555555d75844 in access_with_adjusted_size (addr=0, value=0x7fffed37a448, size=2, access_size_min=1, access_size_max=4, access_fn=0x555555d7542b <memory_region_write_accessor>,
+ *                                       mr=0x555557f58bb0, attrs=...) at ../system/memory.c:573
+ * #12 0x0000555555d78ad8 in memory_region_dispatch_write (mr=0x555557f58bb0, addr=0, data=0, op=MO_16, attrs=...) at ../system/memory.c:1521
+ * #13 0x0000555555d864a1 in flatview_write_continue (fv=0x7fefd81e04b0, addr=61573724909568, attrs=..., ptr=0x7ffff7fc4028, len=2, addr1=0, l=2, mr=0x555557f58bb0) at ../system/physmem.c:2714
+ * #14 0x0000555555d86604 in flatview_write (fv=0x7fefd81e04b0, addr=61573724909568, attrs=..., buf=0x7ffff7fc4028, len=2) at ../system/physmem.c:2756
+ * #15 0x0000555555d869b4 in address_space_write (as=0x555556f51780 <address_space_memory>, addr=61573724909568, attrs=..., buf=0x7ffff7fc4028, len=2) at ../system/physmem.c:2863
+ * #16 0x0000555555d86a21 in address_space_rw (as=0x555556f51780 <address_space_memory>, addr=61573724909568, attrs=..., buf=0x7ffff7fc4028, len=2, is_write=true) at ../system/physmem.c:2873
+ * #17 0x0000555555dd98ac in kvm_cpu_exec (cpu=0x5555572f6fd0) at ../accel/kvm/kvm-all.c:2915
+ * #18 0x0000555555ddc588 in kvm_vcpu_thread_fn (arg=0x5555572f6fd0) at ../accel/kvm/kvm-accel-ops.c:51
+ * #19 0x0000555555fe280b in qemu_thread_start (args=0x555557300f40) at ../util/qemu-thread-posix.c:541
+ * #20 0x00007ffff48081da in start_thread () at /lib/../lib64/libpthread.so.0
+ * #21 0x00007ffff4439e73 in clone () at /lib/../lib64/libc.so.6
+ */
 static void vfio_iommu_map_notify(IOMMUNotifier *n, IOMMUTLBEntry *iotlb)
 {
     VFIOGuestIOMMU *giommu = container_of(n, VFIOGuestIOMMU, n);
diff --git a/hw/vfio/container.c b/hw/vfio/container.c
index 242010036..60c512ae0 100644
--- a/hw/vfio/container.c
+++ b/hw/vfio/container.c
@@ -170,6 +170,12 @@ int vfio_dma_unmap(VFIOContainer *container, hwaddr iova,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/vfio/common.c|325| <<vfio_iommu_map_notify>> ret = vfio_dma_map(container, iova, iotlb->addr_mask + 1, vaddr, read_only);
+ *   - hw/vfio/common.c|388| <<vfio_ram_discard_notify_populate>> ret = vfio_dma_map(vrdl->container, iova, next - start, vaddr, section->readonly);
+ *   - hw/vfio/common.c|687| <<vfio_listener_region_add>> ret = vfio_dma_map(container, iova, int128_get64(llsize), vaddr, section->readonly);
+ */
 int vfio_dma_map(VFIOContainer *container, hwaddr iova,
                  ram_addr_t size, void *vaddr, bool readonly)
 {
diff --git a/hw/vfio/helpers.c b/hw/vfio/helpers.c
index 168847e7c..0cdc735cc 100644
--- a/hw/vfio/helpers.c
+++ b/hw/vfio/helpers.c
@@ -398,6 +398,12 @@ static void vfio_subregion_unmap(VFIORegion *region, int index)
     region->mmaps[index].mmap = NULL;
 }
 
+/*
+ * called by:
+ *   - hw/vfio/display.c|449| <<vfio_display_region_update>> ret = vfio_region_mmap(&dpy->region.buffer);
+ *   - hw/vfio/pci.c|1775| <<vfio_bar_register>> if (vfio_region_mmap(&bar->region)) {
+ *   - hw/vfio/platform.c|622| <<vfio_platform_realize>> if (vfio_region_mmap(vdev->regions[i])) {
+ */
 int vfio_region_mmap(VFIORegion *region)
 {
     int i, prot = 0;
diff --git a/hw/vfio/pci.c b/hw/vfio/pci.c
index c62c02f7b..a14e3b7f9 100644
--- a/hw/vfio/pci.c
+++ b/hw/vfio/pci.c
@@ -1755,6 +1755,10 @@ static void vfio_bars_prepare(VFIOPCIDevice *vdev)
     }
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|1789| <<vfio_bars_register>> vfio_bar_register(vdev, i);
+ */
 static void vfio_bar_register(VFIOPCIDevice *vdev, int nr)
 {
     VFIOBAR *bar = &vdev->bars[nr];
diff --git a/hw/virtio/vhost.c b/hw/virtio/vhost.c
index 2c9ac7946..bde064570 100644
--- a/hw/virtio/vhost.c
+++ b/hw/virtio/vhost.c
@@ -43,6 +43,13 @@
     do { } while (0)
 #endif
 
+/*
+ * 在以下使用static的vhost_log:
+ *   - hw/virtio/vhost.c|406| <<vhost_log_get>> struct vhost_log *log = share ? vhost_log_shm : vhost_log;
+ *   - hw/virtio/vhost.c|413| <<vhost_log_get>> vhost_log = log;
+ *   - hw/virtio/vhost.c|444| <<vhost_log_put>> if (vhost_log == log) {
+ *   - hw/virtio/vhost.c|446| <<vhost_log_put>> vhost_log = NULL;
+ */
 static struct vhost_log *vhost_log;
 static struct vhost_log *vhost_log_shm;
 
@@ -86,6 +93,12 @@ unsigned int vhost_get_free_memslots(void)
     return free;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|171| <<vhost_sync_dirty_bitmap>> vhost_dev_sync_region(dev, section, start_addr, end_addr, reg->guest_phys_addr, range_get_last(reg->guest_phys_addr, reg->memory_size));
+ *   - hw/virtio/vhost.c|218| <<vhost_sync_dirty_bitmap>> vhost_dev_sync_region(dev, section, start_addr, end_addr, phys, range_get_last(phys, s));
+ *   - hw/virtio/vhost.c|224| <<vhost_sync_dirty_bitmap>> vhost_dev_sync_region(dev, section, start_addr, end_addr, vq->used_phys, range_get_last(vq->used_phys, vq->used_size));
+ */
 static void vhost_dev_sync_region(struct vhost_dev *dev,
                                   MemoryRegionSection *section,
                                   uint64_t mfirst, uint64_t mlast,
@@ -149,6 +162,47 @@ bool vhost_dev_has_iommu(struct vhost_dev *dev)
     }
 }
 
+/*
+ * 连vnc会
+ * (gdb) bt
+ * #0  vhost_sync_dirty_bitmap (dev=0x555557056990, section=0x7fffffffdf50, first=0, last=18446744073709551615) at ../hw/virtio/vhost.c:156
+ * #1  0x0000555555b262c8 in vhost_log_sync (listener=0x555557056998, section=0x7fffffffdf50) at ../hw/virtio/vhost.c:237
+ * #2  0x0000555555d9457b in memory_region_sync_dirty_bitmap (mr=0x555557c553d0, last_stage=false) at ../system/memory.c:2279
+ * #3  0x0000555555d94994 in memory_region_snapshot_and_clear_dirty (mr=0x555557c553d0, addr=0, size=4096000, client=0) at ../system/memory.c:2354
+ * #4  0x0000555555969a55 in vga_draw_graphic (s=0x555557c553c0, full_update=0) at ../hw/display/vga.c:1668
+ * #5  0x000055555596a015 in vga_update_display (opaque=0x555557c553c0) at ../hw/display/vga.c:1791
+ * #6  0x000055555586de25 in graphic_hw_update (con=0x555557d71040) at ../ui/console.c:143
+ * #7  0x000055555588a147 in vnc_refresh (dcl=0x5555580a4250) at ../ui/vnc.c:3204
+ * #8  0x000055555586fd7f in dpy_refresh (s=0x5555572dbb20) at ../ui/console.c:959
+ * #9  0x000055555586dbb1 in gui_update (opaque=0x5555572dbb20) at ../ui/console.c:89
+ * #10 0x00005555560204da in timerlist_run_timers (timer_list=0x55555705ad00) at ../util/qemu-timer.c:576
+ * #11 0x0000555556020584 in qemu_clock_run_timers (type=QEMU_CLOCK_REALTIME) at ../util/qemu-timer.c:590
+ * #12 0x0000555556020845 in qemu_clock_run_all_timers () at ../util/qemu-timer.c:672
+ * #13 0x000055555601bb00 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:603
+ * #14 0x0000555555b6ea18 in qemu_main_loop () at ../system/runstate.c:782
+ * #15 0x0000555555e02d7d in qemu_default_main () at ../system/main.c:37
+ * #16 0x0000555555e02db8 in main (argc=23, argv=0x7fffffffe448) at ../system/main.c:48
+ *
+ * (gdb) bt
+ * #0  vhost_sync_dirty_bitmap (dev=0x555557056990, section=0x7ffe42b2a5b0, first=0, last=18446744073709551615) at ../hw/virtio/vhost.c:156
+ * #1  0x0000555555b262c8 in vhost_log_sync (listener=0x555557056998, section=0x7ffe42b2a5b0) at ../hw/virtio/vhost.c:237
+ * #2  0x0000555555d9457b in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #3  0x0000555555d96484 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #4  0x0000555555da947e in migration_bitmap_sync (rs=0x7ffe38002270, last_stage=false) at ../migration/ram.c:1046
+ * #5  0x0000555555da96b6 in migration_bitmap_sync_precopy (rs=0x7ffe38002270, last_stage=false) at ../migration/ram.c:1094
+ * #6  0x0000555555dac854 in ram_init_bitmaps (rs=0x7ffe38002270) at ../migration/ram.c:2811
+ * #7  0x0000555555dac8f3 in ram_init_all (rsp=0x555556fba420 <ram_state>) at ../migration/ram.c:2834
+ * #8  0x0000555555dacc28 in ram_save_setup (f=0x5555572c9a30, opaque=0x555556fba420 <ram_state>) at ../migration/ram.c:2947
+ * #9  0x0000555555bb55e0 in qemu_savevm_state_setup (f=0x5555572c9a30) at ../migration/savevm.c:1345
+ * #10 0x0000555555b9f3d4 in migration_thread (opaque=0x555557058db0) at ../migration/migration.c:3340
+ * #11 0x00005555560017cc in qemu_thread_start (args=0x5555578610b0) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff749f812 in start_thread () at /lib64/libc.so.6
+ * #13 0x00007ffff743f450 in clone3 () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - hw/virtio/vhost.c|237| <<vhost_log_sync>> vhost_sync_dirty_bitmap(dev, section, 0x0, ~0x0ULL);
+ *   - hw/virtio/vhost.c|247| <<vhost_log_sync_range>> vhost_sync_dirty_bitmap(dev, section, first, last);
+ */
 static int vhost_sync_dirty_bitmap(struct vhost_dev *dev,
                                    MemoryRegionSection *section,
                                    hwaddr first,
@@ -166,6 +220,17 @@ static int vhost_sync_dirty_bitmap(struct vhost_dev *dev,
     start_addr = MAX(first, start_addr);
     end_addr = MIN(last, end_addr);
 
+    /*
+     * struct vhost_dev *dev:
+     * -> struct vhost_memory *mem;
+     *    -> uint32_t nregions;
+     *    -> uint32_t padding;
+     *    -> struct vhost_memory_region regions[];
+     *       -> uint64_t guest_phys_addr;
+     *       -> uint64_t memory_size; // bytes
+     *       -> uint64_t userspace_addr;
+     *       -> uint64_t flags_padding; // No flags are currently specified.
+     */
     for (i = 0; i < dev->mem->nregions; ++i) {
         struct vhost_memory_region *reg = dev->mem->regions + i;
         vhost_dev_sync_region(dev, section, start_addr, end_addr,
@@ -229,6 +294,37 @@ static int vhost_sync_dirty_bitmap(struct vhost_dev *dev,
     return 0;
 }
 
+/*
+ * 在以下使用log_sync:
+ *   - accel/hvf/hvf-accel-ops.c|312| <<global>> .log_sync = hvf_log_sync,
+ *   - hw/i386/xen/xen-hvm.c|467| <<global>> .log_sync = xen_log_sync,
+ *   - hw/vfio/common.c|1348| <<global>> .log_sync = vfio_listener_log_sync,
+ *   - target/i386/nvmm/nvmm-all.c|1129| <<global>> .log_sync = nvmm_log_sync,
+ *   - target/i386/whpx/whpx-all.c|2413| <<global>> .log_sync = whpx_log_sync,
+ *   - accel/kvm/kvm-all.c|1758| <<kvm_memory_listener_register>> kml->listener.log_sync = kvm_log_sync;
+ *   - hw/arm/xen_arm.c|46| <<OBJECT_DECLARE_SIMPLE_TYPE>> .log_sync = NULL,
+ *   - hw/virtio/vhost.c|1507| <<vhost_dev_init>> .log_sync = vhost_log_sync,
+ *   - system/memory.c|2307| <<memory_region_sync_dirty_bitmap>> if (listener->log_sync) {
+ *   - system/memory.c|2313| <<memory_region_sync_dirty_bitmap>> listener->log_sync(listener, &mrs);
+ *   - system/memory.c|3088| <<memory_listener_register>> assert(!(listener->log_sync && listener->log_sync_global));
+ *
+ * 1514     hdev->memory_listener = (MemoryListener) {
+ * 1515         .name = "vhost",          
+ * 1516         .begin = vhost_begin,
+ * 1517         .commit = vhost_commit,
+ * 1518         .region_add = vhost_region_addnop,
+ * 1519         .region_nop = vhost_region_addnop,
+ * 1520         .log_start = vhost_log_start,
+ * 1521         .log_stop = vhost_log_stop,
+ * 1522         .log_sync = vhost_log_sync,
+ * 1523         .log_global_start = vhost_log_global_start,
+ * 1524         .log_global_stop = vhost_log_global_stop,
+ * 1525         .priority = MEMORY_LISTENER_PRIORITY_DEV_BACKEND
+ * 1526     };
+ *
+ * 在以下使用vhost_log_sync():
+ *   - hw/virtio/vhost.c|1507| <<vhost_dev_init>> .log_sync = vhost_log_sync,
+ */
 static void vhost_log_sync(MemoryListener *listener,
                           MemoryRegionSection *section)
 {
@@ -237,6 +333,10 @@ static void vhost_log_sync(MemoryListener *listener,
     vhost_sync_dirty_bitmap(dev, section, 0x0, ~0x0ULL);
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|352| <<vhost_log_put>> vhost_log_sync_range(dev, 0, dev->log_size * VHOST_LOG_CHUNK - 1);
+ */
 static void vhost_log_sync_range(struct vhost_dev *dev,
                                  hwaddr first, hwaddr last)
 {
@@ -337,6 +437,13 @@ static struct vhost_log *vhost_log_get(uint64_t size, bool share)
     return log;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|390| <<vhost_dev_log_resize>> vhost_log_put(dev, true);
+ *   - hw/virtio/vhost.c|1016| <<vhost_migration_log>> vhost_log_put(dev, false);
+ *   - hw/virtio/vhost.c|2081| <<vhost_dev_start>> vhost_log_put(hdev, false);
+ *   - hw/virtio/vhost.c|2138| <<vhost_dev_stop>> vhost_log_put(hdev, true);
+ */
 static void vhost_log_put(struct vhost_dev *dev, bool sync)
 {
     struct vhost_log *log = dev->log;
@@ -556,6 +663,48 @@ static void vhost_begin(MemoryListener *listener)
     dev->n_tmp_sections = 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  vhost_commit (listener=0x555557056998) at ../hw/virtio/vhost.c:561
+ * #1  0x0000555555d9189e in memory_region_transaction_commit () at ../system/memory.c:1137
+ * #2  0x0000555555d959ec in memory_region_del_subregion (mr=0x55555713c800, subregion=0x55555785c340) at ../system/memory.c:2680
+ * #3  0x0000555555a4a8b7 in pci_update_mappings (d=0x55555785a190) at ../hw/pci/pci.c:1539
+ * #4  0x0000555555a4ad39 in pci_default_write_config (d=0x55555785a190, addr=4, val_in=256, l=2) at ../hw/pci/pci.c:1607
+ * #5  0x0000555555a4f838 in pci_host_config_write_common (pci_dev=0x55555785a190, addr=4, limit=256, val=256, len=2) at ../hw/pci/pci_host.c:96
+ * #6  0x0000555555a4fa5e in pci_data_write (s=0x5555573e55f0, addr=2147485956, val=256, len=2) at ../hw/pci/pci_host.c:138
+ * #7  0x0000555555a4fbf5 in pci_host_data_write (opaque=0x5555573af0a0, addr=0, val=256, len=2) at ../hw/pci/pci_host.c:188
+ * #8  0x0000555555d8f37d in memory_region_write_accessor (mr=0x5555573af4e0, addr=0, value=0x7ffe67dfe5a8, size=2, shift=0, mask=65535, attrs=...) at ../system/memory.c:497
+ * #9  0x0000555555d8f692 in access_with_adjusted_size (addr=0, value=0x7ffe67dfe5a8, size=2, access_size_min=1, access_size_max=4, access_fn=
+ *                             0x555555d8f287 <memory_region_write_accessor>, mr=0x5555573af4e0, attrs=...) at ../system/memory.c:573
+ * #10 0x0000555555d92776 in memory_region_dispatch_write (mr=0x5555573af4e0, addr=0, data=256, op=MO_16, attrs=...) at ../system/memory.c:1521
+ * #11 0x0000555555da00c4 in flatview_write_continue (fv=0x7fffe8032220, addr=3324, attrs=..., ptr=0x7ffff42a3000, len=2, addr1=0, l=2, mr=0x5555573af4e0) at ../system/physmem.c:2714
+ * #12 0x0000555555da0227 in flatview_write (fv=0x7fffe8032220, addr=3324, attrs=..., buf=0x7ffff42a3000, len=2) at ../system/physmem.c:2756
+ * #13 0x0000555555da05d7 in address_space_write (as=0x555556fba0a0 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff42a3000, len=2) at ../system/physmem.c:2863
+ * #14 0x0000555555da0644 in address_space_rw (as=0x555556fba0a0 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff42a3000, len=2, is_write=true) at ../system/physmem.c:2873
+ * #15 0x0000555555df2d18 in kvm_handle_io (port=3324, attrs=..., data=0x7ffff42a3000, direction=1, size=2, count=1) at ../accel/kvm/kvm-all.c:2632
+ * #16 0x0000555555df35bb in kvm_cpu_exec (cpu=0x55555739b350) at ../accel/kvm/kvm-all.c:2905
+ * #17 0x0000555555df62fa in kvm_vcpu_thread_fn (arg=0x55555739b350) at ../accel/kvm/kvm-accel-ops.c:51
+ * #18 0x00005555560017cc in qemu_thread_start (args=0x5555573a42b0) at ../util/qemu-thread-posix.c:541
+ * #19 0x00007ffff749f812 in start_thread () at /lib64/libc.so.6
+ * #20 0x00007ffff743f450 in clone3 () at /lib64/libc.so.6
+ *
+ * 1606     hdev->memory_listener = (MemoryListener) {
+ * 1607         .name = "vhost",
+ * 1608         .begin = vhost_begin,
+ * 1609         .commit = vhost_commit,
+ * 1610         .region_add = vhost_region_addnop,
+ * 1611         .region_nop = vhost_region_addnop,
+ * 1612         .log_start = vhost_log_start,
+ * 1613         .log_stop = vhost_log_stop,
+ * 1614         .log_sync = vhost_log_sync,
+ * 1615         .log_global_start = vhost_log_global_start,
+ * 1616         .log_global_stop = vhost_log_global_stop,
+ * 1617         .priority = MEMORY_LISTENER_PRIORITY_DEV_BACKEND
+ * 1618     };
+ *
+ * 在以下使用vhost_commit():
+ *   - hw/virtio/vhost.c|1591| <<vhost_dev_init>> .commit = vhost_commit,
+ */
 static void vhost_commit(MemoryListener *listener)
 {
     struct vhost_dev *dev = container_of(listener, struct vhost_dev,
@@ -598,6 +747,13 @@ static void vhost_commit(MemoryListener *listener)
     /* Rebuild the regions list from the new sections list */
     regions_size = offsetof(struct vhost_memory, regions) +
                        dev->n_mem_sections * sizeof dev->mem->regions[0];
+    /*
+     * struct vhost_dev *dev:
+     * -> struct vhost_memory *mem;
+     *    -> uint32_t nregions;
+     *    -> uint32_t padding;
+     *    -> struct vhost_memory_region regions[];
+     */
     dev->mem = g_realloc(dev->mem, regions_size);
     dev->mem->nregions = dev->n_mem_sections;
 
@@ -609,6 +765,14 @@ static void vhost_commit(MemoryListener *listener)
     }
 
     for (i = 0; i < dev->n_mem_sections; i++) {
+        /*
+	 * struct vhost_memory_region {
+	 *     uint64_t guest_phys_addr;
+	 *     uint64_t memory_size; // bytes
+	 *     uint64_t userspace_addr;
+	 *     uint64_t flags_padding; // No flags are currently specified.
+	 * };
+	 */
         struct vhost_memory_region *cur_vmr = dev->mem->regions + i;
         struct MemoryRegionSection *mrs = dev->mem_sections + i;
 
@@ -675,6 +839,28 @@ out:
  * and for each region (via the _add and _nop methods) to
  * join neighbours.
  */
+/*
+ * (gdb) bt
+ * #0  vhost_region_add_section (dev=0x5555572db030, section=0x7fffffffdcc0) at ../hw/virtio/vhost.c:681
+ * #1  0x0000555555b27839 in vhost_region_addnop (listener=0x5555572db038, section=0x7fffffffdcc0) at ../hw/virtio/vhost.c:794
+ * #2  0x0000555555d9138d in address_space_update_topology_pass (as=0x555556fba100 <address_space_memory>, old_view=0x555557711610, new_view=0x55555772cf90, adding=true) at ../system/memory.c:1004
+ * #3  0x0000555555d9168d in address_space_set_flatview (as=0x555556fba100 <address_space_memory>) at ../system/memory.c:1080
+ * #4  0x0000555555d91846 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #5  0x0000555555d95725 in memory_region_update_container_subregions (subregion=0x5555570a8c00) at ../system/memory.c:2630
+ * #6  0x0000555555d957d0 in memory_region_add_subregion_common (mr=0x5555572c8400, offset=0, subregion=0x5555570a8c00) at ../system/memory.c:2645
+ * #7  0x0000555555d9580c in memory_region_add_subregion (mr=0x5555572c8400, offset=0, subregion=0x5555570a8c00) at ../system/memory.c:2653
+ * #8  0x0000555555c5dc59 in pc_memory_init (pcms=0x5555572bf2d0, system_memory=0x5555572c8400, rom_memory=0x5555570fc6e0, pci_hole64_size=2147483648) at ../hw/i386/pc.c:954
+ * #9  0x0000555555c3f9e7 in pc_init1 (machine=0x5555572bf2d0, host_type=0x555556219858 "i440FX-pcihost", pci_type=0x555556219851 "i440FX") at ../hw/i386/pc_piix.c:246
+ * #10 0x0000555555c405d7 in pc_init_v8_2 (machine=0x5555572bf2d0) at ../hw/i386/pc_piix.c:555
+ * #11 0x00005555559243b5 in machine_run_board_init (machine=0x5555572bf2d0, mem_path=0x0, errp=0x7fffffffe120) at ../hw/core/machine.c:1509
+ * #12 0x0000555555b75478 in qemu_init_board () at ../system/vl.c:2613
+ * #13 0x0000555555b756fd in qmp_x_exit_preconfig (errp=0x555556fce260 <error_fatal>) at ../system/vl.c:2704
+ * #14 0x0000555555b780dc in qemu_init (argc=23, argv=0x7fffffffe448) at ../system/vl.c:3753
+ * #15 0x0000555555e02daf in main (argc=23, argv=0x7fffffffe448) at ../system/main.c:47
+ *
+ * called by:
+ *   - hw/virtio/vhost.c|934| <<vhost_region_addnop>> vhost_region_add_section(dev, section);
+ */
 static void vhost_region_add_section(struct vhost_dev *dev,
                                      MemoryRegionSection *section)
 {
@@ -770,6 +956,15 @@ static void vhost_region_add_section(struct vhost_dev *dev,
 
     if (need_add) {
         ++dev->n_tmp_sections;
+	/*
+	 * 在以下使用vhost_dev->tmp_sections:
+	 *   - hw/virtio/vhost.c|662| <<vhost_begin>> dev->tmp_sections = NULL;
+	 *   - hw/virtio/vhost.c|702| <<vhost_commit>> dev->mem_sections = dev->tmp_sections;
+	 *   - hw/virtio/vhost.c|856| <<vhost_region_add_section>> MemoryRegionSection *prev_sec = dev->tmp_sections +
+	 *   - hw/virtio/vhost.c|913| <<vhost_region_add_section>> dev->tmp_sections = g_renew(MemoryRegionSection, dev->tmp_sections,
+	 *   - hw/virtio/vhost.c|915| <<vhost_region_add_section>> dev->tmp_sections[dev->n_tmp_sections - 1] = *section;
+	 *   - hw/virtio/vhost.c|919| <<vhost_region_add_section>> dev->tmp_sections[dev->n_tmp_sections - 1].fv = NULL;
+	 */
         dev->tmp_sections = g_renew(MemoryRegionSection, dev->tmp_sections,
                                     dev->n_tmp_sections);
         dev->tmp_sections[dev->n_tmp_sections - 1] = *section;
@@ -781,6 +976,70 @@ static void vhost_region_add_section(struct vhost_dev *dev,
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  vhost_region_add_section (dev=0x5555572db3d0, section=0x7ffe66dfc1d0) at ../hw/virtio/vhost.c:681
+ * #1  0x0000555555b27839 in vhost_region_addnop (listener=0x5555572db3d8, section=0x7ffe66dfc1d0) at ../hw/virtio/vhost.c:794
+ * #2  0x0000555555d91169 in address_space_update_topology_pass (as=0x555556fba100 <address_space_memory>, old_view=0x7fffec2ab0a0, new_view=0x7ffe5c054f00, adding=true) at ../system/memory.c:985
+ * #3  0x0000555555d9168d in address_space_set_flatview (as=0x555556fba100 <address_space_memory>) at ../system/memory.c:1080
+ * #4  0x0000555555d91846 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #5  0x0000555555d959ec in memory_region_del_subregion (mr=0x55555713c800, subregion=0x55555785cce0) at ../system/memory.c:2680
+ * #6  0x0000555555a4a8b7 in pci_update_mappings (d=0x55555785ab30) at ../hw/pci/pci.c:1539
+ * #7  0x0000555555a4ad39 in pci_default_write_config (d=0x55555785ab30, addr=4, val_in=256, l=2) at ../hw/pci/pci.c:1607
+ * #8  0x0000555555a4f838 in pci_host_config_write_common (pci_dev=0x55555785ab30, addr=4, limit=256, val=256, len=2) at ../hw/pci/pci_host.c:96
+ * #9  0x0000555555a4fa5e in pci_data_write (s=0x5555573e6060, addr=2147485956, val=256, len=2) at ../hw/pci/pci_host.c:138
+ * #10 0x0000555555a4fbf5 in pci_host_data_write (opaque=0x5555573afb10, addr=0, val=256, len=2) at ../hw/pci/pci_host.c:188
+ * #11 0x0000555555d8f37d in memory_region_write_accessor (mr=0x5555573aff50, addr=0, value=0x7ffe66dfc5a8, size=2, shift=0, mask=65535, attrs=...) at ../system/memory.c:497
+ * #12 0x0000555555d8f692 in access_with_adjusted_size (addr=0, value=0x7ffe66dfc5a8, size=2, access_size_min=1, access_size_max=4, access_fn=
+ *                                    0x555555d8f287 <memory_region_write_accessor>, mr=0x5555573aff50, attrs=...) at ../system/memory.c:573
+ * #13 0x0000555555d92776 in memory_region_dispatch_write (mr=0x5555573aff50, addr=0, data=256, op=MO_16, attrs=...) at ../system/memory.c:1521
+ * #14 0x0000555555da00c4 in flatview_write_continue (fv=0x7fffec281390, addr=3324, attrs=..., ptr=0x7ffff42a0000, len=2, addr1=0, l=2, mr=0x5555573aff50) at ../system/physmem.c:2714
+ * #15 0x0000555555da0227 in flatview_write (fv=0x7fffec281390, addr=3324, attrs=..., buf=0x7ffff42a0000, len=2) at ../system/physmem.c:2756
+ * #16 0x0000555555da05d7 in address_space_write (as=0x555556fba0a0 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff42a0000, len=2) at ../system/physmem.c:2863
+ * #17 0x0000555555da0644 in address_space_rw (as=0x555556fba0a0 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff42a0000, len=2, is_write=true) at ../system/physmem.c:2873
+ * #18 0x0000555555df2d18 in kvm_handle_io (port=3324, attrs=..., data=0x7ffff42a0000, direction=1, size=2, count=1) at ../accel/kvm/kvm-all.c:2632
+ * #19 0x0000555555df35bb in kvm_cpu_exec (cpu=0x5555573a4d30) at ../accel/kvm/kvm-all.c:2905
+ * #20 0x0000555555df62fa in kvm_vcpu_thread_fn (arg=0x5555573a4d30) at ../accel/kvm/kvm-accel-ops.c:51
+ * #21 0x00005555560017cc in qemu_thread_start (args=0x5555573aed40) at ../util/qemu-thread-posix.c:541
+ * #22 0x00007ffff749f812 in start_thread () at /lib64/libc.so.6
+ * #23 0x00007ffff743f450 in clone3 () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vhost_region_add_section (dev=0x5555572db030, section=0x7fffffffdcc0) at ../hw/virtio/vhost.c:681
+ * #1  0x0000555555b27839 in vhost_region_addnop (listener=0x5555572db038, section=0x7fffffffdcc0) at ../hw/virtio/vhost.c:794
+ * #2  0x0000555555d9138d in address_space_update_topology_pass (as=0x555556fba100 <address_space_memory>, old_view=0x555557711610, new_view=0x55555772cf90, adding=true) at ../system/memory.c:1004
+ * #3  0x0000555555d9168d in address_space_set_flatview (as=0x555556fba100 <address_space_memory>) at ../system/memory.c:1080
+ * #4  0x0000555555d91846 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #5  0x0000555555d95725 in memory_region_update_container_subregions (subregion=0x5555570a8c00) at ../system/memory.c:2630
+ * #6  0x0000555555d957d0 in memory_region_add_subregion_common (mr=0x5555572c8400, offset=0, subregion=0x5555570a8c00) at ../system/memory.c:2645
+ * #7  0x0000555555d9580c in memory_region_add_subregion (mr=0x5555572c8400, offset=0, subregion=0x5555570a8c00) at ../system/memory.c:2653
+ * #8  0x0000555555c5dc59 in pc_memory_init (pcms=0x5555572bf2d0, system_memory=0x5555572c8400, rom_memory=0x5555570fc6e0, pci_hole64_size=2147483648) at ../hw/i386/pc.c:954
+ * #9  0x0000555555c3f9e7 in pc_init1 (machine=0x5555572bf2d0, host_type=0x555556219858 "i440FX-pcihost", pci_type=0x555556219851 "i440FX") at ../hw/i386/pc_piix.c:246
+ * #10 0x0000555555c405d7 in pc_init_v8_2 (machine=0x5555572bf2d0) at ../hw/i386/pc_piix.c:555
+ * #11 0x00005555559243b5 in machine_run_board_init (machine=0x5555572bf2d0, mem_path=0x0, errp=0x7fffffffe120) at ../hw/core/machine.c:1509
+ * #12 0x0000555555b75478 in qemu_init_board () at ../system/vl.c:2613
+ * #13 0x0000555555b756fd in qmp_x_exit_preconfig (errp=0x555556fce260 <error_fatal>) at ../system/vl.c:2704
+ * #14 0x0000555555b780dc in qemu_init (argc=23, argv=0x7fffffffe448) at ../system/vl.c:3753
+ * #15 0x0000555555e02daf in main (argc=23, argv=0x7fffffffe448) at ../system/main.c:47
+ *
+ * 在以下使用vhost_region_addnop():
+ *   - hw/virtio/vhost.c|1684| <<vhost_dev_init>> .region_add = vhost_region_addnop,
+ *   - hw/virtio/vhost.c|1685| <<vhost_dev_init>> .region_nop = vhost_region_addnop,
+ *
+ * 1680     hdev->memory_listener = (MemoryListener) {
+ * 1681         .name = "vhost",
+ * 1682         .begin = vhost_begin,
+ * 1683         .commit = vhost_commit,
+ * 1684         .region_add = vhost_region_addnop,
+ * 1685         .region_nop = vhost_region_addnop,
+ * 1686         .log_start = vhost_log_start,
+ * 1687         .log_stop = vhost_log_stop,
+ * 1688         .log_sync = vhost_log_sync,
+ * 1689         .log_global_start = vhost_log_global_start,
+ * 1690         .log_global_stop = vhost_log_global_stop,
+ * 1691         .priority = MEMORY_LISTENER_PRIORITY_DEV_BACKEND
+ * 1692     };
+ */
 /* Used for both add and nop callbacks */
 static void vhost_region_addnop(MemoryListener *listener,
                                 MemoryRegionSection *section)
diff --git a/hw/virtio/virtio.c b/hw/virtio/virtio.c
index 3a160f86e..5975d9139 100644
--- a/hw/virtio/virtio.c
+++ b/hw/virtio/virtio.c
@@ -2590,6 +2590,9 @@ static bool virtio_disabled_needed(void *opaque)
     return vdev->disabled;
 }
 
+/*
+ * 被下面的vmstate_virtio_virtqueues引用
+ */
 static const VMStateDescription vmstate_virtqueue = {
     .name = "virtqueue_state",
     .version_id = 1,
@@ -2766,6 +2769,11 @@ static const VMStateDescription vmstate_virtio_disabled = {
     }
 };
 
+/*
+ * 在以下使用vmstate_virtio:
+ *   - hw/virtio/virtio.c|2899| <<virtio_save>> return vmstate_save_state(f, &vmstate_virtio, vdev, NULL);
+ *   - hw/virtio/virtio.c|3134| <<virtio_load>> ret = vmstate_load_state(f, &vmstate_virtio, vdev, 1);
+ */
 static const VMStateDescription vmstate_virtio = {
     .name = "virtio",
     .version_id = 1,
@@ -2787,6 +2795,58 @@ static const VMStateDescription vmstate_virtio = {
     }
 };
 
+/*
+ * (gdb) bt
+ * #0  virtio_save (vdev=0x555557fd7dd0, f=0x5555575a4000) at ../hw/virtio/virtio.c:2791
+ * #1  0x0000555555d3853b in virtio_device_put (f=0x5555575a4000, opaque=0x555557fd7dd0, size=0, field=0x555556e6ad20 <__compound_literal.7>, vmdesc=0x555557b8c630) at ../hw/virtio/virtio.c:2854
+ * #2  0x00005555560616c6 in vmstate_save_state_v (f=0x5555575a4000, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7dd0, vmdesc=0x555557b8c630, version_id=11, errp=0x7fffffffc4f8)
+ *                                     at ../migration/vmstate.c:408
+ * #3  0x0000555556061356 in vmstate_save_state_with_err (f=0x5555575a4000, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7dd0, vmdesc_id=0x555557b8c630, errp=0x7fffffffc4f8)
+ *                                     at ../migration/vmstate.c:347
+ * #4  0x0000555555ba0060 in vmstate_save (f=0x5555575a4000, se=0x5555580fea10, vmdesc=0x555557b8c630) at ../migration/savevm.c:1037
+ * #5  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x5555575a4000, in_postcopy=false, inactivate_disks=false) at ../migration/savevm.c:1553
+ * #6  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x5555575a4000, iterable_only=false, inactivate_disks=false) at ../migration/savevm.c:1628
+ * #7  0x0000555555ba1a36 in qemu_savevm_state (f=0x5555575a4000, errp=0x7fffffffc860) at ../migration/savevm.c:1734
+ * #8  0x0000555555ba42a6 in save_snapshot (name=0x555556fe3110 "001", overwrite=true, vmstate=0x0, has_devices=false, devices=0x0, errp=0x7fffffffc860) at ../migration/savevm.c:3131
+ * #9  0x0000555555b80b57 in hmp_savevm (mon=0x555556fe3c90, qdict=0x555557de7000) at ../migration/migration-hmp-cmds.c:418
+ * #10 0x0000555555bb0241 in handle_hmp_command_exec (mon=0x555556fe3c90, cmd=0x555556e74640 <hmp_cmds+6880>, qdict=0x555557de7000) at ../monitor/hmp.c:1106
+ * #11 0x0000555555bb046e in handle_hmp_command (mon=0x555556fe3c90, cmdline=0x55555727c5a7 "001") at ../monitor/hmp.c:1158
+ * #12 0x0000555555bad95a in monitor_command_cb (opaque=0x555556fe3c90, cmdline=0x55555727c5a0 "savevm 001", readline_opaque=0x0) at ../monitor/hmp.c:47
+ * #13 0x0000555556001e9f in readline_handle_byte (rs=0x55555727c5a0, ch=13) at ../util/readline.c:419
+ * #14 0x0000555555bb0f19 in monitor_read (opaque=0x555556fe3c90, buf=0x7fffffffcac0 "\r\320\377\377\377\177", size=1) at ../monitor/hmp.c:1390
+ * #15 0x0000555555f06291 in qemu_chr_be_write_impl (s=0x5555572404d0, buf=0x7fffffffcac0 "\r\320\377\377\377\177", len=1) at ../chardev/char.c:202
+ * #16 0x0000555555f062f5 in qemu_chr_be_write (s=0x5555572404d0, buf=0x7fffffffcac0 "\r\320\377\377\377\177", len=1) at ../chardev/char.c:214
+ * #17 0x0000555555f08ca2 in fd_chr_read (chan=0x555557240440, cond=G_IO_IN, opaque=0x5555572404d0) at ../chardev/char-fd.c:72
+ * #18 0x0000555555df4fa8 in qio_channel_fd_source_dispatch (source=0x5555575a2be0, callback=0x555555f08b78 <fd_chr_read>, user_data=0x5555572404d0) at ../io/channel-watch.c:84
+ * #19 0x00007ffff69ef119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #20 0x0000555555feb300 in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #21 0x0000555555feb37a in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:313
+ * #22 0x0000555555feb47f in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #23 0x0000555555b5be98 in qemu_main_loop () at ../system/runstate.c:782
+ * #24 0x0000555555dd6e12 in qemu_default_main () at ../system/main.c:37
+ * #25 0x0000555555dd6e4d in main (argc=18, argv=0x7fffffffdd88) at ../system/main.c:48
+ *
+ * (gdb) bt
+ * #0  virtio_save (vdev=0x555557fd7ef0, f=0x555557236b40) at ../hw/virtio/virtio.c:2791
+ * #1  0x0000555555d3853b in virtio_device_put (f=0x555557236b40, opaque=0x555557fd7ef0, size=0, field=0x555556e6ad20 <__compound_literal.7>, vmdesc=0x7ffdc8000cc0) at ../hw/virtio/virtio.c:2854
+ * #2  0x00005555560616c6 in vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7ef0, vmdesc=0x7ffdc8000cc0, version_id=11, errp=0x7fffeebee6a8)
+ *                                    at ../migration/vmstate.c:408
+ * #3  0x0000555556061356 in vmstate_save_state_with_err (f=0x555557236b40, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7ef0, vmdesc_id=0x7ffdc8000cc0, errp=0x7fffeebee6a8)
+ *                                    at ../migration/vmstate.c:347
+ * #4  0x0000555555ba0060 in vmstate_save (f=0x555557236b40, se=0x5555580feb30, vmdesc=0x7ffdc8000cc0) at ../migration/savevm.c:1037
+ * #5  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x555557236b40, in_postcopy=false, inactivate_disks=true) at ../migration/savevm.c:1553
+ * #6  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1628
+ * #7  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7fffeebee7c0) at ../migration/migration.c:2641
+ * #8  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #9  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #10 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #11 0x0000555555fd1784 in qemu_thread_start (args=0x555557234bc0) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - hw/virtio/virtio.c|2885| <<virtio_device_put>> return virtio_save(VIRTIO_DEVICE(opaque), f);
+ */
 int virtio_save(VirtIODevice *vdev, QEMUFile *f)
 {
     BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
@@ -2847,6 +2907,13 @@ int virtio_save(VirtIODevice *vdev, QEMUFile *f)
     return vmstate_save_state(f, &vmstate_virtio, vdev, NULL);
 }
 
+/*
+ * 2920 const VMStateInfo  virtio_vmstate_info = {
+ * 2921     .name = "virtio",
+ * 2922     .get = virtio_device_get,
+ * 2923     .put = virtio_device_put,
+ * 2924 };
+ */
 /* A wrapper for use as a VMState .put function */
 static int virtio_device_put(QEMUFile *f, void *opaque, size_t size,
                               const VMStateField *field, JSONWriter *vmdesc)
diff --git a/include/exec/ram_addr.h b/include/exec/ram_addr.h
index 90676093f..211721780 100644
--- a/include/exec/ram_addr.h
+++ b/include/exec/ram_addr.h
@@ -149,6 +149,10 @@ static inline void qemu_ram_block_writeback(RAMBlock *block)
 #define DIRTY_CLIENTS_ALL     ((1 << DIRTY_MEMORY_NUM) - 1)
 #define DIRTY_CLIENTS_NOCODE  (DIRTY_CLIENTS_ALL & ~(1 << DIRTY_MEMORY_CODE))
 
+/*
+ * called by:
+ *   - include/exec/ram_addr.h|234| <<cpu_physical_memory_get_dirty_flag>> return cpu_physical_memory_get_dirty(addr, 1, client);
+ */
 static inline bool cpu_physical_memory_get_dirty(ram_addr_t start,
                                                  ram_addr_t length,
                                                  unsigned client)
@@ -228,6 +232,13 @@ static inline bool cpu_physical_memory_all_dirty(ram_addr_t start,
     return dirty;
 }
 
+/*
+ * called by:
+ *   - accel/tcg/cputlb.c|1399| <<notdirty_write>> if (!cpu_physical_memory_get_dirty_flag(ram_addr, DIRTY_MEMORY_CODE)) {
+ *   - include/exec/ram_addr.h|239| <<cpu_physical_memory_is_clean>> bool vga = cpu_physical_memory_get_dirty_flag(addr, DIRTY_MEMORY_VGA);
+ *   - include/exec/ram_addr.h|240| <<cpu_physical_memory_is_clean>> bool code = cpu_physical_memory_get_dirty_flag(addr, DIRTY_MEMORY_CODE);
+ *   - include/exec/ram_addr.h|242| <<cpu_physical_memory_is_clean>> cpu_physical_memory_get_dirty_flag(addr, DIRTY_MEMORY_MIGRATION);
+ */
 static inline bool cpu_physical_memory_get_dirty_flag(ram_addr_t addr,
                                                       unsigned client)
 {
@@ -335,6 +346,12 @@ static inline void cpu_physical_memory_set_dirty_range(ram_addr_t start,
 
 #if !defined(_WIN32)
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+ *   - hw/vfio/common.c|1209| <<vfio_get_dirty_bitmap>> dirty_pages = cpu_physical_memory_set_dirty_lebitmap(vbmap.bitmap, ram_addr,
+ *   - hw/vfio/container.c|102| <<vfio_dma_unmap_bitmap>> cpu_physical_memory_set_dirty_lebitmap(vbmap.bitmap,
+ */
 /*
  * Contrary to cpu_physical_memory_sync_dirty_bitmap() this function returns
  * the number of dirty pages in @bitmap passed as argument. On the other hand,
@@ -369,6 +386,23 @@ uint64_t cpu_physical_memory_set_dirty_lebitmap(unsigned long *bitmap,
                           DIRTY_MEMORY_BLOCK_SIZE);
 
         WITH_RCU_READ_LOCK_GUARD() {
+            /*
+	     * typedef struct {
+	     *     struct rcu_head rcu;
+	     *     unsigned long *blocks[];
+	     * } DirtyMemoryBlocks;
+	     *
+	     * typedef struct RAMList {
+	     *     QemuMutex mutex;
+	     *     RAMBlock *mru_block;        
+	     *     QLIST_HEAD(, RAMBlock) blocks;
+	     *     DirtyMemoryBlocks *dirty_memory[DIRTY_MEMORY_NUM];
+	     *     uint32_t version;
+	     *     QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
+	     * } RAMList;
+	     *
+	     * RAMList ram_list:
+	     */
             for (i = 0; i < DIRTY_MEMORY_NUM; i++) {
                 blocks[i] =
                     qatomic_rcu_read(&ram_list.dirty_memory[i])->blocks;
@@ -379,6 +413,9 @@ uint64_t cpu_physical_memory_set_dirty_lebitmap(unsigned long *bitmap,
                     unsigned long temp = leul_to_cpu(bitmap[k]);
 
                     nbits = ctpopl(temp);
+                    /*
+		     * 在这里设置!!!
+		     */
                     qatomic_or(&blocks[DIRTY_MEMORY_VGA][idx][offset], temp);
 
                     if (global_dirty_tracking) {
@@ -463,6 +500,10 @@ static inline void cpu_physical_memory_clear_dirty_range(ram_addr_t start,
 }
 
 
+/*
+ * called by:
+ *   - migration/ram.c|901| <<ramblock_sync_dirty_bitmap>> cpu_physical_memory_sync_dirty_bitmap(rb, 0, rb->used_length);
+ */
 /* Called with RCU critical section */
 static inline
 uint64_t cpu_physical_memory_sync_dirty_bitmap(RAMBlock *rb,
@@ -486,6 +527,11 @@ uint64_t cpu_physical_memory_sync_dirty_bitmap(RAMBlock *rb,
                                         DIRTY_MEMORY_BLOCK_SIZE);
         unsigned long page = BIT_WORD(start >> TARGET_PAGE_BITS);
 
+        /*
+	 * RAMList ram_list:
+	 * -> DirtyMemoryBlocks *dirty_memory[DIRTY_MEMORY_NUM];
+	 *    -> unsigned long *blocks[];
+	 */
         src = qatomic_rcu_read(
                 &ram_list.dirty_memory[DIRTY_MEMORY_MIGRATION])->blocks;
 
diff --git a/include/exec/ramblock.h b/include/exec/ramblock.h
index 69c6a5390..7ad5b8e99 100644
--- a/include/exec/ramblock.h
+++ b/include/exec/ramblock.h
@@ -30,6 +30,12 @@ struct RAMBlock {
     uint8_t *host;
     uint8_t *colo_cache; /* For colo, VM's ram cache */
     ram_addr_t offset;
+    /*
+     * 在以下设置RAMBlock->used_length:
+     *   - system/physmem.c|1767| <<qemu_ram_resize>> block->used_length = newsize;
+     *   - system/physmem.c|1998| <<qemu_ram_alloc_from_fd>> new_block->used_length = size;
+     *   - system/physmem.c|2120| <<qemu_ram_alloc_internal>> new_block->used_length = size;
+     */
     ram_addr_t used_length;
     ram_addr_t max_length;
     void (*resized)(const char*, uint64_t length, void *host);
@@ -42,6 +48,34 @@ struct RAMBlock {
     int fd;
     uint64_t fd_offset;
     size_t page_size;
+    /*
+     * 在以下使用RAMBlock->bmap:
+     *   - include/exec/ram_addr.h|485| <<cpu_physical_memory_sync_dirty_bitmap>> unsigned long *dest = rb->bmap;
+     *   - migration/ram.c|725| <<pss_find_next_dirty>> unsigned long *bitmap = rb->bmap;
+     *   - migration/ram.c|806| <<colo_bitmap_find_dirty>> unsigned long *bitmap = rb->bmap;
+     *   - migration/ram.c|841| <<migration_bitmap_clear_dirty>> ret = test_and_clear_bit(page, rb->bmap);
+     *   - migration/ram.c|867| <<dirty_bitmap_clear_section>> *cleared_bits += bitmap_count_one_with_offset(rb->bmap, start, npages);
+     *   - migration/ram.c|868| <<dirty_bitmap_clear_section>> bitmap_clear(rb->bmap, start, npages);
+     *   - migration/ram.c|888| <<ramblock_dirty_bitmap_clear_discarded_pages>> if (rb->mr && rb->bmap && memory_region_has_ram_discard_manager(rb->mr)) {
+     *   - migration/ram.c|1943| <<get_queued_page>> dirty = test_bit(page, block->bmap);
+     *   - migration/ram.c|2554| <<ram_save_cleanup>> g_free(block->bmap);
+     *   - migration/ram.c|2555| <<ram_save_cleanup>> block->bmap = NULL;
+     *   - migration/ram.c|2588| <<ram_postcopy_migrated_memory_release>> unsigned long *bitmap = block->bmap;
+     *   - migration/ram.c|2615| <<postcopy_send_discard_bm_ram>> unsigned long *bitmap = block->bmap;
+     *   - migration/ram.c|2691| <<postcopy_chunk_hostpages_pass>> unsigned long *bitmap = block->bmap;
+     *   - migration/ram.c|2927| <<ram_list_init_bitmaps>> block->bmap = bitmap_new(pages);
+     *   - migration/ram.c|2931| <<ram_list_init_bitmaps>> bitmap_set(block->bmap, 0, pages);
+     *   - migration/ram.c|3029| <<ram_state_resume_prepare>> pages += bitmap_count_one(block->bmap,
+     *   - migration/ram.c|3092| <<qemu_guest_free_page_hint>> bitmap_count_one_with_offset(block->bmap, start, npages);
+     *   - migration/ram.c|3093| <<qemu_guest_free_page_hint>> bitmap_clear(block->bmap, start, npages);
+     *   - migration/ram.c|3714| <<colo_record_bitmap>> block->bmap);
+     *   - migration/ram.c|3804| <<colo_init_ram_cache>> block->bmap = bitmap_new(pages);
+     *   - migration/ram.c|3825| <<colo_incoming_start_dirty_log>> bitmap_zero(block->bmap, block->max_length >> TARGET_PAGE_BITS);
+     *   - migration/ram.c|3841| <<colo_release_ram_cache>> g_free(block->bmap);
+     *   - migration/ram.c|3842| <<colo_release_ram_cache>> block->bmap = NULL;
+     *   - migration/ram.c|4536| <<ram_dirty_bitmap_reload>> bitmap_from_le(block->bmap, le_bitmap, nbits);
+     *   - migration/ram.c|4542| <<ram_dirty_bitmap_reload>> bitmap_complement(block->bmap, block->bmap, nbits);
+     */
     /* dirty bitmap used during migration */
     unsigned long *bmap;
     /* bitmap of already received pages in postcopy */
diff --git a/include/exec/ramlist.h b/include/exec/ramlist.h
index 2ad2a81ac..04fb2d70f 100644
--- a/include/exec/ramlist.h
+++ b/include/exec/ramlist.h
@@ -49,6 +49,20 @@ typedef struct RAMList {
     RAMBlock *mru_block;
     /* RCU-enabled, writes protected by the ramlist lock. */
     QLIST_HEAD(, RAMBlock) blocks;
+    /*
+     * 在以下使用RAMList->dirty_memory[DIRTY_MEMORY_NUM]:
+     *   - include/exec/ramlist.h|52| <<global>> DirtyMemoryBlocks *dirty_memory[DIRTY_MEMORY_NUM];
+     *   - system/physmem.c|912| <<cpu_physical_memory_snapshot_and_clear_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+     *   - include/exec/ram_addr.h|167| <<cpu_physical_memory_get_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+     *   - include/exec/ram_addr.h|208| <<cpu_physical_memory_all_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+     *   - include/exec/ram_addr.h|281| <<cpu_physical_memory_set_dirty_flag>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+     *   - include/exec/ram_addr.h|304| <<cpu_physical_memory_set_dirty_range>> blocks[i] = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+     *   - include/exec/ram_addr.h|380| <<cpu_physical_memory_set_dirty_lebitmap>> qatomic_rcu_read(&ram_list.dirty_memory[i])->blocks;
+     *   - include/exec/ram_addr.h|500| <<cpu_physical_memory_sync_dirty_bitmap>> &ram_list.dirty_memory[DIRTY_MEMORY_MIGRATION])->blocks;
+     *   - system/physmem.c|862| <<cpu_physical_memory_test_and_clear_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+     *   - system/physmem.c|1835| <<dirty_memory_extend>> old_blocks = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+     *   - system/physmem.c|1848| <<dirty_memory_extend>> qatomic_rcu_set(&ram_list.dirty_memory[i], new_blocks);
+     */
     DirtyMemoryBlocks *dirty_memory[DIRTY_MEMORY_NUM];
     uint32_t version;
     QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
diff --git a/include/hw/core/cpu.h b/include/hw/core/cpu.h
index c0c832041..44fccfc4d 100644
--- a/include/hw/core/cpu.h
+++ b/include/hw/core/cpu.h
@@ -546,6 +546,35 @@ struct CPUState {
     int32_t exception_index;
 
     AccelCPUState *accel;
+    /*
+     * 在以下设置CPUState->vcpu_dirty:
+     *   - accel/hvf/hvf-accel-ops.c|209| <<do_hvf_cpu_synchronize_state>> cpu->vcpu_dirty = true;
+     *   - accel/hvf/hvf-accel-ops.c|224| <<do_hvf_cpu_synchronize_set_dirty>> cpu->vcpu_dirty = true;
+     *   - accel/hvf/hvf-accel-ops.c|405| <<hvf_init_vcpu>> cpu->vcpu_dirty = 1;
+     *   - accel/kvm/kvm-all.c|417| <<kvm_init_vcpu>> cpu->vcpu_dirty = true;
+     *   - accel/kvm/kvm-all.c|2721| <<do_kvm_cpu_synchronize_state>> cpu->vcpu_dirty = true;
+     *   - accel/kvm/kvm-all.c|2741| <<do_kvm_cpu_synchronize_post_reset>> cpu->vcpu_dirty = false;
+     *   - accel/kvm/kvm-all.c|2761| <<do_kvm_cpu_synchronize_post_init>> cpu->vcpu_dirty = false;
+     *   - accel/kvm/kvm-all.c|2775| <<do_kvm_cpu_synchronize_pre_loadvm>> cpu->vcpu_dirty = true;
+     *   - accel/kvm/kvm-all.c|2869| <<kvm_cpu_exec>> cpu->vcpu_dirty = false;
+     *   - target/arm/hvf/hvf.c|806| <<flush_cpu_state>> cpu->vcpu_dirty = false;
+     *   - target/i386/hvf/hvf.c|424| <<hvf_vcpu_exec>> cpu->vcpu_dirty = false;
+     *   - target/i386/nvmm/nvmm-all.c|511| <<nvmm_io_callback>> current_cpu->vcpu_dirty = false;
+     *   - target/i386/nvmm/nvmm-all.c|520| <<nvmm_mem_callback>> current_cpu->vcpu_dirty = false;
+     *   - target/i386/nvmm/nvmm-all.c|733| <<nvmm_vcpu_loop>> cpu->vcpu_dirty = false;
+     *   - target/i386/nvmm/nvmm-all.c|831| <<do_nvmm_cpu_synchronize_state>> cpu->vcpu_dirty = true;
+     *   - target/i386/nvmm/nvmm-all.c|838| <<do_nvmm_cpu_synchronize_post_reset>> cpu->vcpu_dirty = false;
+     *   - target/i386/nvmm/nvmm-all.c|845| <<do_nvmm_cpu_synchronize_post_init>> cpu->vcpu_dirty = false;
+     *   - target/i386/nvmm/nvmm-all.c|851| <<do_nvmm_cpu_synchronize_pre_loadvm>> cpu->vcpu_dirty = true;
+     *   - target/i386/nvmm/nvmm-all.c|986| <<nvmm_init_vcpu>> cpu->vcpu_dirty = true;
+     *   - target/i386/whpx/whpx-all.c|846| <<whpx_emu_setreg_callback>> cpu->vcpu_dirty = false;
+     *   - target/i386/whpx/whpx-all.c|1724| <<whpx_vcpu_run>> cpu->vcpu_dirty = false;
+     *   - target/i386/whpx/whpx-all.c|2074| <<do_whpx_cpu_synchronize_state>> cpu->vcpu_dirty = true;
+     *   - target/i386/whpx/whpx-all.c|2082| <<do_whpx_cpu_synchronize_post_reset>> cpu->vcpu_dirty = false;
+     *   - target/i386/whpx/whpx-all.c|2089| <<do_whpx_cpu_synchronize_post_init>> cpu->vcpu_dirty = false;
+     *   - target/i386/whpx/whpx-all.c|2095| <<do_whpx_cpu_synchronize_pre_loadvm>> cpu->vcpu_dirty = true;
+     *   - target/i386/whpx/whpx-all.c|2244| <<whpx_init_vcpu>> cpu->vcpu_dirty = true;
+     */
     /* shared by kvm and hvf */
     bool vcpu_dirty;
 
diff --git a/include/hw/scsi/scsi.h b/include/hw/scsi/scsi.h
index 3692ca82f..0ec947fff 100644
--- a/include/hw/scsi/scsi.h
+++ b/include/hw/scsi/scsi.h
@@ -44,6 +44,18 @@ struct SCSIRequest {
     uint32_t          sense_len;
     bool              enqueued;
     bool              io_canceled;
+    /*
+     * 在以下使用SCSIRequest->retry:
+     *   - hw/scsi/scsi-bus.c|158| <<scsi_dma_restart_bh>> if (req->retry) {
+     *   - hw/scsi/scsi-bus.c|159| <<scsi_dma_restart_bh>> req->retry = false;
+     *   - hw/scsi/scsi-bus.c|182| <<scsi_req_retry>> req->retry = true;
+     *   - hw/scsi/scsi-bus.c|903| <<scsi_req_enqueue>> assert(!req->retry);
+     *   - hw/scsi/scsi-bus.c|914| <<scsi_req_dequeue>> req->retry = false;
+     *   - hw/scsi/scsi-bus.c|1752| <<put_scsi_requests>> qemu_put_sbyte(f, req->retry ? 1 : 2);
+     *   - hw/scsi/scsi-bus.c|1789| <<get_scsi_requests>> req->retry = (sbyte == 1);
+     *   - hw/scsi/scsi-disk.c|154| <<scsi_disk_save_request>> } else if (!req->retry) {
+     *   - hw/scsi/scsi-disk.c|173| <<scsi_disk_load_request>> } else if (!r->req.retry) {
+     */
     bool              retry;
     bool              dma_started;
     BlockAIOCB        *aiocb;
diff --git a/include/hw/virtio/vhost.h b/include/hw/virtio/vhost.h
index 02477788d..be194fe0d 100644
--- a/include/hw/virtio/vhost.h
+++ b/include/hw/virtio/vhost.h
@@ -81,8 +81,27 @@ struct vhost_dev {
     MemoryListener iommu_listener;
     struct vhost_memory *mem;
     int n_mem_sections;
+    /*
+     * 在以下使用vhost_dev->mem_sections:
+     *   - hw/virtio/vhost.c|346| <<vhost_log_sync_range>> MemoryRegionSection *section = &dev->mem_sections[i];
+     *   - hw/virtio/vhost.c|700| <<vhost_commit>> old_sections = dev->mem_sections;
+     *   - hw/virtio/vhost.c|702| <<vhost_commit>> dev->mem_sections = dev->tmp_sections;
+     *   - hw/virtio/vhost.c|711| <<vhost_commit>> if (!MemoryRegionSection_eq(&old_sections[i], &dev->mem_sections[i])) {
+     *   - hw/virtio/vhost.c|753| <<vhost_commit>> struct MemoryRegionSection *mrs = dev->mem_sections + i;
+     *   - hw/virtio/vhost.c|1678| <<vhost_dev_init>> hdev->mem_sections = NULL;
+     *   - hw/virtio/vhost.c|1741| <<vhost_dev_cleanup>> g_free(hdev->mem_sections);
+     */
     MemoryRegionSection *mem_sections;
     int n_tmp_sections;
+    /*
+     * 在以下使用vhost_dev->tmp_sections:
+     *   - hw/virtio/vhost.c|662| <<vhost_begin>> dev->tmp_sections = NULL;
+     *   - hw/virtio/vhost.c|702| <<vhost_commit>> dev->mem_sections = dev->tmp_sections;
+     *   - hw/virtio/vhost.c|856| <<vhost_region_add_section>> MemoryRegionSection *prev_sec = dev->tmp_sections +
+     *   - hw/virtio/vhost.c|913| <<vhost_region_add_section>> dev->tmp_sections = g_renew(MemoryRegionSection, dev->tmp_sections,
+     *   - hw/virtio/vhost.c|915| <<vhost_region_add_section>> dev->tmp_sections[dev->n_tmp_sections - 1] = *section;
+     *   - hw/virtio/vhost.c|919| <<vhost_region_add_section>> dev->tmp_sections[dev->n_tmp_sections - 1].fv = NULL;
+     */
     MemoryRegionSection *tmp_sections;
     struct vhost_virtqueue *vqs;
     unsigned int nvqs;
diff --git a/include/hw/virtio/virtio.h b/include/hw/virtio/virtio.h
index c8f72850b..f5fa02919 100644
--- a/include/hw/virtio/virtio.h
+++ b/include/hw/virtio/virtio.h
@@ -281,6 +281,28 @@ int virtio_save(VirtIODevice *vdev, QEMUFile *f);
 
 extern const VMStateInfo virtio_vmstate_info;
 
+/*
+ * 在以下使用VMSTATE_VIRTIO_DEVICE:
+ *   - hw/9pfs/virtio-9p-device.c|241| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/audio/virtio-snd.c|76| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/block/vhost-user-blk.c|558| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/block/virtio-blk.c|1702| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/char/virtio-serial-bus.c|1152| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/display/virtio-gpu.c|1596| <<global>> VMSTATE_VIRTIO_DEVICE ,
+ *   - hw/input/virtio-input.c|297| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/net/virtio-net.c|3875| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/scsi/vhost-scsi.c|162| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/scsi/vhost-user-scsi.c|382| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/scsi/virtio-scsi.c|1307| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/vdpa-dev.c|345| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/vhost-user-fs.c|377| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/vhost-vsock.c|115| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/virtio-balloon.c|1000| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/virtio-crypto.c|1126| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/virtio-iommu.c|1516| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/virtio-mem.c|1440| <<global>> VMSTATE_VIRTIO_DEVICE,
+ *   - hw/virtio/virtio-rng.c|246| <<global>> VMSTATE_VIRTIO_DEVICE,
+ */
 #define VMSTATE_VIRTIO_DEVICE \
     {                                         \
         .name = "virtio",                     \
diff --git a/include/migration/register.h b/include/migration/register.h
index fed1d04a3..8ab63ac7f 100644
--- a/include/migration/register.h
+++ b/include/migration/register.h
@@ -25,9 +25,34 @@ typedef struct SaveVMHandlers {
      * used to perform early checks.
      */
     int (*save_prepare)(void *opaque, Error **errp);
+    /*
+     * 在以下使用SaveVMHandlers->save_setup:
+     *   - hw/ppc/spapr.c|2489| <<global>> .save_setup = htab_save_setup,
+     *   - hw/s390x/s390-stattrib.c|335| <<global>> .save_setup = cmma_save_setup,
+     *   - hw/vfio/migration.c|674| <<global>> .save_setup = vfio_save_setup,
+     *   - migration/block-dirty-bitmap.c|1250| <<global>> .save_setup = dirty_bitmap_save_setup,
+     *   - migration/block.c|1009| <<global>> .save_setup = block_save_setup,
+     *   - migration/ram.c|4498| <<global>> .save_setup = ram_save_setup,
+     *   - migration/savevm.c|844| <<register_savevm_live>> if (ops->save_setup != NULL) {
+     *   - migration/savevm.c|1476| <<qemu_savevm_state_setup>> if (!se->ops || !se->ops->save_setup) {
+     *   - migration/savevm.c|1489| <<qemu_savevm_state_setup>> ret = se->ops->save_setup(f, se->opaque);
+     */
     int (*save_setup)(QEMUFile *f, void *opaque);
     void (*save_cleanup)(void *opaque);
     int (*save_live_complete_postcopy)(QEMUFile *f, void *opaque);
+    /*
+     * 在以下使用SaveVMHandlers->save_live_complete_precopy:
+     *   - hw/ppc/spapr.c|2491| <<global>> .save_live_complete_precopy = htab_save_complete,
+     *   - hw/s390x/s390-stattrib.c|337| <<global>> .save_live_complete_precopy = cmma_save_complete,
+     *   - hw/vfio/migration.c|680| <<global>> .save_live_complete_precopy = vfio_save_complete_precopy,
+     *   - migration/block-dirty-bitmap.c|1252| <<global>> .save_live_complete_precopy = dirty_bitmap_save_complete,
+     *   - migration/block.c|1011| <<global>> .save_live_complete_precopy = block_save_complete,
+     *   - migration/ram.c|4457| <<global>> .save_live_complete_precopy = ram_save_complete,
+     *   - migration/savevm.c|1655| <<qemu_savevm_state_complete_precopy_iterable>> !se->ops->save_live_complete_precopy) {
+     *   - migration/savevm.c|1670| <<qemu_savevm_state_complete_precopy_iterable>> ret = se->ops->save_live_complete_precopy(f, se->opaque);
+     *
+     * 剩余可以一次性迁移的最后一部分数据
+     */
     int (*save_live_complete_precopy)(QEMUFile *f, void *opaque);
 
     /* This runs both outside and inside the iothread lock.  */
@@ -43,6 +68,17 @@ typedef struct SaveVMHandlers {
      */
     bool (*is_active_iterate)(void *opaque);
 
+    /*
+     * 在以下使用SaveVMHandlers->save_live_iterate:
+     *   - hw/ppc/spapr.c|2490| <<global>> .save_live_iterate = htab_save_iterate,
+     *   - hw/s390x/s390-stattrib.c|336| <<global>> .save_live_iterate = cmma_save_iterate,
+     *   - hw/vfio/migration.c|679| <<global>> .save_live_iterate = vfio_save_iterate,
+     *   - migration/block-dirty-bitmap.c|1256| <<global>> .save_live_iterate = dirty_bitmap_save_iterate,
+     *   - migration/block.c|1010| <<global>> .save_live_iterate = block_save_iterate,
+     *   - migration/ram.c|4437| <<global>> .save_live_iterate = ram_save_iterate,
+     *   - migration/savevm.c|1532| <<qemu_savevm_state_iterate>> if (!se->ops || !se->ops->save_live_iterate) {
+     *   - migration/savevm.c|1560| <<qemu_savevm_state_iterate>> ret = se->ops->save_live_iterate(f, se->opaque);
+     */
     /* This runs outside the iothread lock in the migration case, and
      * within the lock in the savevm case.  The callback had better only
      * use data that is local to the migration thread or protected
@@ -68,6 +104,18 @@ typedef struct SaveVMHandlers {
     /* This estimates the remaining data to transfer */
     void (*state_pending_estimate)(void *opaque, uint64_t *must_precopy,
                                    uint64_t *can_postcopy);
+    /*
+     * 在以下使用SaveVMHandlers->state_pending_exact:
+     *   - hw/s390x/s390-stattrib.c|338| <<global>> .state_pending_exact = cmma_state_pending,
+     *   - hw/vfio/migration.c|677| <<global>> .state_pending_exact = vfio_state_pending_exact,
+     *   - migration/block-dirty-bitmap.c|1254| <<global>> .state_pending_exact = dirty_bitmap_state_pending,
+     *   - migration/block.c|1012| <<global>> .state_pending_exact = block_state_pending,
+     *   - migration/ram.c|4375| <<global>> .state_pending_exact = ram_state_pending_exact,
+     *   - migration/savevm.c|1827| <<qemu_savevm_state_pending_exact>> if (!se->ops || !se->ops->state_pending_exact) {
+     *   - migration/savevm.c|1835| <<qemu_savevm_state_pending_exact>> se->ops->state_pending_exact(se->opaque, must_precopy, can_postcopy);
+     *
+     *  This calculate the exact remaining data to transfer
+     */
     /* This calculate the exact remaining data to transfer */
     void (*state_pending_exact)(void *opaque, uint64_t *must_precopy,
                                 uint64_t *can_postcopy);
diff --git a/include/migration/vmstate.h b/include/migration/vmstate.h
index 982191863..2aca6f6f5 100644
--- a/include/migration/vmstate.h
+++ b/include/migration/vmstate.h
@@ -1213,6 +1213,44 @@ int vmstate_register_with_alias_id(VMStateIf *obj, uint32_t instance_id,
                                    int required_for_version,
                                    Error **errp);
 
+/*
+ * called by:
+ *   - cpu-target.c|150| <<cpu_exec_realizefn>> vmstate_register(NULL, cpu->cpu_index, &vmstate_cpu_common, cpu);
+ *   - cpu-target.c|153| <<cpu_exec_realizefn>> vmstate_register(NULL, cpu->cpu_index, cpu->cc->sysemu_ops->legacy_vmsd, cpu);
+ *   - hw/arm/pxa2xx.c|1783| <<pxa2xx_i2s_init>> vmstate_register(NULL, base, &vmstate_pxa2xx_i2s, s);
+ *   - hw/arm/pxa2xx.c|2180| <<pxa270_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_cm, s);
+ *   - hw/arm/pxa2xx.c|2190| <<pxa270_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_mm, s);
+ *   - hw/arm/pxa2xx.c|2195| <<pxa270_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_pm, s);
+ *   - hw/arm/pxa2xx.c|2318| <<pxa255_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_cm, s);
+ *   - hw/arm/pxa2xx.c|2328| <<pxa255_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_mm, s);
+ *   - hw/arm/pxa2xx.c|2333| <<pxa255_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_pm, s);
+ *   - hw/arm/virt-acpi-build.c|1157| <<virt_acpi_setup>> vmstate_register(NULL, 0, &vmstate_virt_acpi_build, build_state);
+ *   - hw/block/onenand.c|827| <<onenand_realize>> vmstate_register(VMSTATE_IF(dev),
+ *   - hw/display/pxa2xx_lcd.c|1443| <<pxa2xx_lcdc_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_lcdc, s);
+ *   - hw/i386/acpi-build.c|2866| <<acpi_setup>> vmstate_register(NULL, 0, &vmstate_acpi_build, build_state);
+ *   - hw/input/pxa2xx_keypad.c|316| <<pxa27x_keypad_init>> vmstate_register(NULL, 0, &vmstate_pxa2xx_keypad, s);
+ *   - hw/input/tsc2005.c|516| <<tsc2005_init>> vmstate_register(NULL, 0, &vmstate_tsc2005, s);
+ *   - hw/input/tsc210x.c|1108| <<tsc210x_init>> vmstate_register(NULL, 0, vmsd, s);
+ *   - hw/ipmi/ipmi_bmc_extern.c|501| <<ipmi_bmc_extern_realize>> vmstate_register(NULL, 0, &vmstate_ipmi_bmc_extern, ibe);
+ *   - hw/ipmi/ipmi_bmc_sim.c|2191| <<ipmi_sim_realize>> vmstate_register(NULL, 0, &vmstate_ipmi_sim, ibs);
+ *   - hw/ipmi/isa_ipmi_bt.c|121| <<isa_ipmi_bt_realize>> vmstate_register(NULL, 0, &vmstate_ISAIPMIBTDevice, dev);
+ *   - hw/ipmi/isa_ipmi_kcs.c|128| <<ipmi_isa_realize>> vmstate_register(NULL, 0, &vmstate_ISAIPMIKCSDevice, iik);
+ *   - hw/loongarch/acpi-build.c|609| <<loongarch_acpi_setup>> vmstate_register(NULL, 0, &vmstate_acpi_build, build_state);
+ *   - hw/openrisc/cputimer.c|163| <<cpu_openrisc_clock_init>> vmstate_register(NULL, 0, &vmstate_or1k_timer, or1k_timer);
+ *   - hw/ppc/spapr.c|171| <<pre_2_10_vmstate_register_dummy_icp>> vmstate_register(NULL, i, &pre_2_10_vmstate_dummy_icp,
+ *   - hw/ppc/spapr.c|3091| <<spapr_machine_init>> vmstate_register(NULL, 0, &vmstate_spapr, spapr);
+ *   - hw/ppc/spapr_cpu_core.c|286| <<spapr_realize_vcpu>> vmstate_register(NULL, cs->cpu_index, &vmstate_spapr_cpu_state,
+ *   - hw/ppc/spapr_drc.c|537| <<drc_realize>> vmstate_register(VMSTATE_IF(drc), spapr_drc_index(drc), &vmstate_spapr_drc,
+ *   - hw/ppc/spapr_drc.c|643| <<realize_physical>> vmstate_register(VMSTATE_IF(drcp),
+ *   - hw/ppc/spapr_iommu.c|325| <<spapr_tce_table_realize>> vmstate_register(VMSTATE_IF(tcet), tcet->liobn, &vmstate_spapr_tce_table,
+ *   - hw/riscv/virt-acpi-build.c|410| <<virt_acpi_setup>> vmstate_register(NULL, 0, &vmstate_virt_acpi_build, build_state);
+ *   - hw/s390x/css.c|415| <<css_register_vmstate>> vmstate_register(NULL, 0, &vmstate_css, &channel_subsys);
+ *   - migration/global_state.c|146| <<register_global_state>> vmstate_register(NULL, 0, &vmstate_globalstate, &global_state);
+ *   - migration/savevm.c|935| <<vmstate_replace_hack_for_ppc>> return vmstate_register(obj, instance_id, vmsd, opaque);
+ *   - replay/replay-snapshot.c|69| <<replay_vmstate_register>> vmstate_register(NULL, 0, &vmstate_replay, &replay_state);
+ *   - system/cpu-timers.c|274| <<cpu_timers_init>> vmstate_register(NULL, 0, &vmstate_timers, &timers_state);
+ *   - target/arm/hvf/hvf.c|2036| <<hvf_arch_init>> vmstate_register(NULL, 0, &vmstate_hvf_vtimer, &vtimer);
+ */
 /**
  * vmstate_register() - legacy function to register state
  * serialisation description
@@ -1250,6 +1288,23 @@ int vmstate_replace_hack_for_ppc(VMStateIf *obj, int instance_id,
  *
  * Returns: 0 on success, -1 on failure
  */
+/*
+ * called by:
+ *   - audio/audio.c|1787| <<audio_init>> vmstate_register_any(NULL, &vmstate_audio, s);
+ *   - backends/dbus-vmstate.c|429| <<dbus_vmstate_complete>> if (vmstate_register_any(VMSTATE_IF(self), &dbus_vmstate, self) < 0) {
+ *   - backends/tpm/tpm_emulator.c|978| <<tpm_emulator_inst_init>> vmstate_register_any(NULL, &vmstate_tpm_emulator, obj);
+ *   - hw/display/vmware_vga.c|1267| <<vmsvga_init>> vmstate_register_any(NULL, &vmstate_vga_common, &s->vga);
+ *   - hw/i2c/core.c|67| <<i2c_init_bus>> vmstate_register_any(NULL, &vmstate_i2c_bus, bus);
+ *   - hw/ide/isa.c|76| <<isa_ide_realizefn>> vmstate_register_any(VMSTATE_IF(dev), &vmstate_ide_isa, s);
+ *   - hw/input/adb.c|250| <<adb_bus_realize>> vmstate_register_any(NULL, &vmstate_adb_bus, adb_bus);
+ *   - hw/input/ads7846.c|161| <<ads7846_realize>> vmstate_register_any(NULL, &vmstate_ads7846, s);
+ *   - hw/net/eepro100.c|1888| <<e100_nic_realize>> vmstate_register_any(VMSTATE_IF(&pci_dev->qdev), s->vmstate, s);
+ *   - hw/nvram/eeprom93xx.c|324| <<eeprom93xx_new>> vmstate_register_any(VMSTATE_IF(dev), &vmstate_eeprom, eeprom);
+ *   - hw/pci/pci.c|150| <<pci_bus_realize>> vmstate_register_any(NULL, &vmstate_pcibus, bus);
+ *   - hw/ppc/spapr_nvdimm.c|879| <<spapr_nvdimm_realize>> vmstate_register_any(NULL, &vmstate_spapr_nvdimm_states, dimm);
+ *   - hw/timer/arm_timer.c|184| <<arm_timer_init>> vmstate_register_any(NULL, &vmstate_arm_timer, s);
+ *   - hw/virtio/virtio-mem.c|1124| <<virtio_mem_device_realize>> vmstate_register_any(VMSTATE_IF(vmem), &vmstate_virtio_mem_device_early, vmem);
+ */
 static inline int vmstate_register_any(VMStateIf *obj,
                                        const VMStateDescription *vmsd,
                                        void *opaque)
diff --git a/include/qemu/thread-context.h b/include/qemu/thread-context.h
index 2ebd6b7fe..b459c99a0 100644
--- a/include/qemu/thread-context.h
+++ b/include/qemu/thread-context.h
@@ -35,6 +35,14 @@ struct ThreadContext {
 
     /* Semaphore to wait for context thread action. */
     QemuSemaphore sem;
+    /*
+     * 在以下使用ThreadContext->sem_thread:
+     *   - util/thread-context.c|117| <<thread_context_run>> qemu_sem_wait(&tc->sem_thread);
+     *   - util/thread-context.c|345| <<thread_context_instance_init>> qemu_sem_init(&tc->sem_thread, 0);
+     *   - util/thread-context.c|355| <<thread_context_instance_finalize>> qemu_sem_post(&tc->sem_thread);
+     *   - util/thread-context.c|359| <<thread_context_instance_finalize>> qemu_sem_destroy(&tc->sem_thread);
+     *   - util/thread-context.c|402| <<thread_context_create_thread>> qemu_sem_post(&tc->sem_thread);
+     */
     /* Semaphore to wait for action in context thread. */
     QemuSemaphore sem_thread;
     /* Mutex to synchronize requests. */
diff --git a/include/sysemu/hostmem.h b/include/sysemu/hostmem.h
index 39326f1d4..abd9bdbb6 100644
--- a/include/sysemu/hostmem.h
+++ b/include/sysemu/hostmem.h
@@ -66,7 +66,25 @@ struct HostMemoryBackend {
     uint64_t size;
     bool merge, dump, use_canonical_path;
     bool prealloc, is_mapped, share, reserve;
+    /*
+     * 在以下使用HostMemoryBackend->prealloc_threads:
+     *   - backends/hostmem.c|258| <<host_memory_backend_set_prealloc>> qemu_prealloc_mem(fd, ptr, sz, backend->prealloc_threads, backend->prealloc_context, &local_err);
+     *   - backends/hostmem.c|272| <<host_memory_backend_get_prealloc_threads>> visit_type_uint32(v, name, &backend->prealloc_threads, errp);
+     *   - backends/hostmem.c|289| <<host_memory_backend_set_prealloc_threads>> backend->prealloc_threads = value;
+     *   - backends/hostmem.c|304| <<host_memory_backend_init>> backend->prealloc_threads = machine->smp.cpus;
+     *   - backends/hostmem.c|470| <<host_memory_backend_memory_complete>> qemu_prealloc_mem(memory_region_get_fd(&backend->mr), ptr, sz, backend->prealloc_threads, backend->prealloc_context, &local_err);
+     */
     uint32_t prealloc_threads;
+    /*
+     * 在以下使用HostMemoryBackend->prealloc_context:
+     *   - backends/hostmem.c|259| <<host_memory_backend_set_prealloc>> backend->prealloc_context, &local_err);
+     *   - backends/hostmem.c|471| <<host_memory_backend_memory_complete>> backend->prealloc_context, &local_err);
+     *   - backends/hostmem.c|580| <<host_memory_backend_class_init>> TYPE_THREAD_CONTEXT, offsetof(HostMemoryBackend, prealloc_context),
+     *
+     * 579     object_class_property_add_link(oc, "prealloc-context",
+     * 580         TYPE_THREAD_CONTEXT, offsetof(HostMemoryBackend, prealloc_context),
+     * 581         object_property_allow_set_link, OBJ_PROP_LINK_STRONG);
+     */
     ThreadContext *prealloc_context;
     DECLARE_BITMAP(host_nodes, MAX_NODES + 1);
     HostMemPolicy policy;
diff --git a/include/sysemu/kvm_int.h b/include/sysemu/kvm_int.h
index fd846394b..2ce2a2d8f 100644
--- a/include/sysemu/kvm_int.h
+++ b/include/sysemu/kvm_int.h
@@ -23,6 +23,21 @@ typedef struct KVMSlot
     int slot;
     int flags;
     int old_flags;
+    /*
+     * 在以下使用KVMSlot->dirty_bmap:
+     *   - accel/kvm/kvm-all.c|601| <<kvm_slot_sync_dirty_pages>> cpu_physical_memory_set_dirty_lebitmap(slot->dirty_bmap, start, pages);
+     *   - accel/kvm/kvm-all.c|606| <<kvm_slot_reset_dirty_pages>> memset(slot->dirty_bmap, 0, slot->dirty_bmap_size);
+     *   - accel/kvm/kvm-all.c|624| <<kvm_slot_init_dirty_bitmap>> if (!(mem->flags & KVM_MEM_LOG_DIRTY_PAGES) || mem->dirty_bmap) {
+     *   - accel/kvm/kvm-all.c|647| <<kvm_slot_init_dirty_bitmap>> mem->dirty_bmap = g_malloc0(bitmap_size);
+     *   - accel/kvm/kvm-all.c|721| <<kvm_slot_get_dirty_log>> d.dirty_bitmap = slot->dirty_bmap;
+     *   - accel/kvm/kvm-all.c|755| <<kvm_dirty_ring_mark_page>> set_bit(offset, mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1038| <<kvm_log_clear_one_slot>> assert(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1042| <<kvm_log_clear_one_slot>> bitmap_copy_with_src_offset(bmap_clear, mem->dirty_bmap,
+     *   - accel/kvm/kvm-all.c|1056| <<kvm_log_clear_one_slot>> d.dirty_bitmap = mem->dirty_bmap + BIT_WORD(bmap_start);
+     *   - accel/kvm/kvm-all.c|1082| <<kvm_log_clear_one_slot>> bitmap_clear(mem->dirty_bmap, bmap_start + start_delta,
+     *   - accel/kvm/kvm-all.c|1459| <<kvm_set_phys_mem>> g_free(mem->dirty_bmap);
+     *   - accel/kvm/kvm-all.c|1460| <<kvm_set_phys_mem>> mem->dirty_bmap = NULL;
+     */
     /* Dirty bitmap cache for the slot */
     unsigned long *dirty_bmap;
     unsigned long dirty_bmap_size;
@@ -87,6 +102,12 @@ struct KVMState
     bool kernel_irqchip_required;
     OnOffAuto kernel_irqchip_split;
     bool sync_mmu;
+    /*
+     * 在以下使用KVMState->manual_dirty_log_protect:
+     *   - accel/kvm/kvm-all.c|1032| <<kvm_physical_log_clear>> if (!s->manual_dirty_log_protect) {
+     *   - accel/kvm/kvm-all.c|2555| <<kvm_init>> s->manual_dirty_log_protect = dirty_log_manual_caps;
+     *   - accel/kvm/kvm-all.c|2564| <<kvm_init>> s->manual_dirty_log_protect = 0;
+     */
     uint64_t manual_dirty_log_protect;
     /* The man page (and posix) say ioctl numbers are signed int, but
      * they're not.  Linux, glibc and *BSD all treat ioctl numbers as
@@ -110,6 +131,24 @@ struct KVMState
         AddressSpace *as;
     } *as;
     uint64_t kvm_dirty_ring_bytes;  /* Size of the per-vcpu dirty ring */
+    /*
+     * 在以下设置KVMState->kvm_dirty_ring_size:
+     *   - accel/kvm/kvm-all.c|1476| <<kvm_dirty_ring_init>> s->kvm_dirty_ring_size = 0;
+     *   - accel/kvm/kvm-all.c|1526| <<kvm_dirty_ring_init>> s->kvm_dirty_ring_size = ring_size;
+     *   - accel/kvm/kvm-all.c|3685| <<kvm_set_dirty_ring_size>> s->kvm_dirty_ring_size = value;
+     *   - accel/kvm/kvm-all.c|3698| <<kvm_accel_instance_init>> s->kvm_dirty_ring_size = 0;
+     * 在以下使用KVMState->kvm_dirty_ring_size:
+     *   - accel/kvm/kvm-all.c|444| <<kvm_init_vcpu>> if (s->kvm_dirty_ring_size) {
+     *   - accel/kvm/kvm-all.c|723| <<kvm_dirty_ring_reap_one>> uint32_t ring_size = s->kvm_dirty_ring_size;
+     *   - accel/kvm/kvm-all.c|1366| <<kvm_set_phys_mem>> if (kvm_state->kvm_dirty_ring_size) {
+     *   - accel/kvm/kvm-all.c|1471| <<kvm_dirty_ring_init>> uint32_t ring_size = s->kvm_dirty_ring_size;
+     *   - accel/kvm/kvm-all.c|1789| <<kvm_memory_listener_register>> if (s->kvm_dirty_ring_size) {
+     *   - accel/kvm/kvm-all.c|2356| <<kvm_dirty_ring_enabled>> return kvm_state->kvm_dirty_ring_size ? true : false;
+     *   - accel/kvm/kvm-all.c|2365| <<kvm_dirty_ring_size>> return kvm_state->kvm_dirty_ring_size;
+     *   - accel/kvm/kvm-all.c|2550| <<kvm_init>> if (!s->kvm_dirty_ring_size) {
+     *   - accel/kvm/kvm-all.c|2640| <<kvm_init>> if (s->kvm_dirty_ring_size) {
+     *   - accel/kvm/kvm-all.c|3660| <<kvm_get_dirty_ring_size>> uint32_t value = s->kvm_dirty_ring_size;
+     */
     uint32_t kvm_dirty_ring_size;   /* Number of dirty GFNs per ring */
     bool kvm_dirty_ring_with_bitmap;
     uint64_t kvm_eager_split_size;  /* Eager Page Splitting chunk size */
diff --git a/io/channel-socket.c b/io/channel-socket.c
index 3a899b060..8e97a9632 100644
--- a/io/channel-socket.c
+++ b/io/channel-socket.c
@@ -181,6 +181,10 @@ int qio_channel_socket_connect_sync(QIOChannelSocket *ioc,
 }
 
 
+/*
+ * 在以下使用qio_channel_socket_connect_worker():
+ *   - io/channel-socket.c|222| <<qio_channel_socket_connect_async>> qio_task_run_in_thread(task, qio_channel_socket_connect_worker, addrCopy, (GDestroyNotify)qapi_free_SocketAddress, context);
+ */
 static void qio_channel_socket_connect_worker(QIOTask *task,
                                               gpointer opaque)
 {
@@ -194,6 +198,14 @@ static void qio_channel_socket_connect_worker(QIOTask *task,
 }
 
 
+/*
+ * called by:
+ *   - migration/socket.c|46| <<socket_send_channel_create>> qio_channel_socket_connect_async(sioc, outgoing_args.saddr, f, data, NULL, NULL);
+ *   - migration/socket.c|148| <<socket_start_outgoing_migration>> qio_channel_socket_connect_async(sioc, saddr, socket_outgoing_migration, data, socket_connect_data_free, NULL);
+ *   - net/stream.c|376| <<net_stream_reconnect>> qio_channel_socket_connect_async(sioc, s->addr, net_stream_client_connected, s, NULL, NULL);
+ *   - net/stream.c|411| <<net_stream_client_init>> qio_channel_socket_connect_async(sioc, addr, net_stream_client_connected, s, NULL, NULL);
+ *   - tests/unit/test-io-channel-socket.c|137| <<test_io_channel_setup_async>> qio_channel_socket_connect_async(QIO_CHANNEL_SOCKET(*src), connect_addr, test_io_channel_complete, &data, NULL, NULL);
+ */
 void qio_channel_socket_connect_async(QIOChannelSocket *ioc,
                                       SocketAddress *addr,
                                       QIOTaskFunc callback,
diff --git a/io/task.c b/io/task.c
index 451f26f8b..73addff23 100644
--- a/io/task.c
+++ b/io/task.c
@@ -104,6 +104,11 @@ static void qio_task_free(QIOTask *task)
 }
 
 
+/*
+ * 在以下使用qio_task_thread_result():
+ *   - io/task.c|137| <<qio_task_thread_worker>> g_source_set_callback(task->thread->completion, qio_task_thread_result, task, NULL);
+ *   - io/task.c|191| <<qio_task_wait_thread>> qio_task_thread_result(task);
+ */
 static gboolean qio_task_thread_result(gpointer opaque)
 {
     QIOTask *task = opaque;
@@ -115,6 +120,17 @@ static gboolean qio_task_thread_result(gpointer opaque)
 }
 
 
+/*
+ * 调用了8次???
+ * (gdb) bt
+ * #0  qio_task_thread_worker (opaque=0x555557e3da90) at ../io/task.c:120
+ * #1  0x0000555555fd1784 in qemu_thread_start (args=0x555557de7a10) at ../util/qemu-thread-posix.c:541
+ * #2  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #3  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 在以下使用qio_task_thread_worker():
+ *   - io/task.c|173| <<qio_task_run_in_thread>> qemu_thread_create(&thread, "io-task-worker", qio_task_thread_worker, task, QEMU_THREAD_DETACHED);
+ */
 static gpointer qio_task_thread_worker(gpointer opaque)
 {
     QIOTask *task = opaque;
diff --git a/migration/channel.c b/migration/channel.c
index ca3319a30..afb201c65 100644
--- a/migration/channel.c
+++ b/migration/channel.c
@@ -30,6 +30,14 @@
  *
  * @ioc: Channel to which we are connecting
  */
+/*
+ * called by:
+ *   - migration/exec.c|100| <<exec_accept_incoming_migration>> migration_channel_process_incoming(ioc);
+ *   - migration/fd.c|50| <<fd_accept_incoming_migration>> migration_channel_process_incoming(ioc);
+ *   - migration/file.c|79| <<file_accept_incoming_migration>> migration_channel_process_incoming(ioc);
+ *   - migration/socket.c|212| <<socket_accept_incoming_migration>> migration_channel_process_incoming(QIO_CHANNEL(cioc));
+ *   - migration/tls.c|69| <<migration_tls_incoming_handshake>> migration_channel_process_incoming(ioc);
+ */
 void migration_channel_process_incoming(QIOChannel *ioc)
 {
     MigrationState *s = migrate_get_current();
@@ -59,6 +67,14 @@ void migration_channel_process_incoming(QIOChannel *ioc)
  * @hostname: Where we want to connect
  * @error: Error indicating failure to connect, free'd here
  */
+/*
+ * called by:
+ *   - migration/exec.c|92| <<exec_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+ *   - migration/fd.c|42| <<fd_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+ *   - migration/file.c|60| <<file_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+ *   - migration/socket.c|121| <<socket_outgoing_migration>> migration_channel_connect(data->s, sioc, data->hostname, err);
+ *   - migration/tls.c|113| <<migration_tls_outgoing_handshake>> migration_channel_connect(s, ioc, NULL, err);
+ */
 void migration_channel_connect(MigrationState *s,
                                QIOChannel *ioc,
                                const char *hostname,
@@ -89,6 +105,11 @@ void migration_channel_connect(MigrationState *s,
             qemu_mutex_unlock(&s->qemu_file_lock);
         }
     }
+    /*
+     * called by:
+     *   - migration/channel.c|92| <<migration_channel_connect>> migrate_fd_connect(s, error);
+     *   - migration/rdma.c|4177| <<rdma_start_outgoing_migration>> migrate_fd_connect(s, NULL);
+     */
     migrate_fd_connect(s, error);
     error_free(error);
 }
diff --git a/migration/dirtyrate.c b/migration/dirtyrate.c
index 036ac017f..e5455db0e 100644
--- a/migration/dirtyrate.c
+++ b/migration/dirtyrate.c
@@ -104,6 +104,11 @@ void global_dirty_log_change(unsigned int flag, bool start)
  * 1. sync dirty log from kvm
  * 2. stop dirty tracking if needed.
  */
+/*
+ * called by:
+ *   - migration/dirtyrate.c|166| <<vcpu_calculate_dirtyrate>> global_dirty_log_sync(flag, one_shot);
+ *   - migration/dirtyrate.c|644| <<calculate_dirtyrate_dirty_bitmap>> global_dirty_log_sync(GLOBAL_DIRTY_DIRTY_RATE, true);
+ */
 static void global_dirty_log_sync(unsigned int flag, bool one_shot)
 {
     qemu_mutex_lock_iothread();
@@ -246,6 +251,11 @@ static int64_t convert_time_unit(int64_t value, TimeUnit unit_from,
 }
 
 
+/*
+ * called by:
+ *   - migration/dirtyrate.c|850| <<qmp_query_dirty_rate>> return query_dirty_rate_info(has_calc_time_unit ? calc_time_unit : TIME_UNIT_SECOND);
+ *   - migration/dirtyrate.c|856| <<hmp_info_dirty_rate>> DirtyRateInfo *info = query_dirty_rate_info(TIME_UNIT_SECOND);
+ */
 static struct DirtyRateInfo *
 query_dirty_rate_info(TimeUnit calc_time_unit)
 {
diff --git a/migration/file.c b/migration/file.c
index 5d4975f43..62d28ac77 100644
--- a/migration/file.c
+++ b/migration/file.c
@@ -36,6 +36,10 @@ int file_parse_offset(char *filespec, uint64_t *offsetp, Error **errp)
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2104| <<qmp_migrate>> file_start_outgoing_migration(s, &addr->u.file, &local_err);
+ */
 void file_start_outgoing_migration(MigrationState *s,
                                    FileMigrationArgs *file_args, Error **errp)
 {
@@ -57,6 +61,14 @@ void file_start_outgoing_migration(MigrationState *s,
         return;
     }
     qio_channel_set_name(ioc, "migration-file-outgoing");
+    /*
+     * called by:
+     *   - migration/exec.c|92| <<exec_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+     *   - migration/fd.c|42| <<fd_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+     *   - migration/file.c|60| <<file_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+     *   - migration/socket.c|121| <<socket_outgoing_migration>> migration_channel_connect(data->s, sioc, data->hostname, err);
+     *   - migration/tls.c|113| <<migration_tls_outgoing_handshake>> migration_channel_connect(s, ioc, NULL, err);
+     */
     migration_channel_connect(s, ioc, NULL, NULL);
 }
 
diff --git a/migration/migration-hmp-cmds.c b/migration/migration-hmp-cmds.c
index 86ae83217..ea1c67af4 100644
--- a/migration/migration-hmp-cmds.c
+++ b/migration/migration-hmp-cmds.c
@@ -33,6 +33,35 @@
 #include "options.h"
 #include "migration.h"
 
+/*
+ * (qemu) info migrate
+ * globals:
+ * store-global-state: on
+ * only-migratable: off
+ * send-configuration: on
+ * send-section-footer: on
+ * decompress-error-check: on
+ * clear-bitmap-shift: 18
+ * Migration status: active
+ * total time: 175139 ms
+ * expected downtime: 69162 ms
+ * setup: 34 ms
+ * transferred ram: 23047141 kbytes
+ * throughput: 1135.37 mbps
+ * remaining ram: 2567352 kbytes
+ * total ram: 104874824 kbytes
+ * duplicate: 27520241 pages
+ * skipped: 0 pages
+ * normal: 5686721 pages
+ * normal bytes: 22746884 kbytes
+ * dirty sync count: 7
+ * page size: 4 kbytes
+ * multifd bytes: 22805230 kbytes
+ * pages-per-second: 34670
+ * dirty pages rate: 215095 pages
+ * precopy ram: 241892 kbytes
+ */
+
 static void migration_global_dump(Monitor *mon)
 {
     MigrationState *ms = migrate_get_current();
@@ -48,6 +77,18 @@ static void migration_global_dump(Monitor *mon)
                    ms->send_section_footer ? "on" : "off");
     monitor_printf(mon, "decompress-error-check: %s\n",
                    ms->decompress_error_check ? "on" : "off");
+    /*
+     * 在以下使用clear_bitmap_shift:
+     *   - migration/options.c|124| <<global>> clear_bitmap_shift, CLEAR_BITMAP_SHIFT_DEFAULT),
+     *   - migration/migration-hmp-cmds.c|52| <<migration_global_dump>> ms->clear_bitmap_shift);
+     *   - migration/ram.c|2941| <<ram_list_init_bitmaps>> shift = ms->clear_bitmap_shift;
+     *
+     * 默认的
+     *  1<<18=256K pages -> 1G chunk when page size is 4K.  This is the
+     *  default value to use if no one specified.
+     *
+     * #define CLEAR_BITMAP_SHIFT_DEFAULT        18
+     */
     monitor_printf(mon, "clear-bitmap-shift: %u\n",
                    ms->clear_bitmap_shift);
 }
@@ -97,6 +138,15 @@ void hmp_info_migrate(Monitor *mon, const QDict *qdict)
     if (info->ram) {
         monitor_printf(mon, "transferred ram: %" PRIu64 " kbytes\n",
                        info->ram->transferred >> 10);
+	/*
+	 * 在以下使用MigrationState->mbps:
+	 *   - migration/migration-hmp-cmds.c|142| <<hmp_info_migrate>> monitor_printf(mon, "throughput: %0.2f mbps\n", info->ram->mbps);
+	 *   - migration/migration.c|1157| <<populate_ram_info>> info->ram->mbps = s->mbps;
+	 *   - migration/migration.c|1683| <<migrate_init>> s->mbps = 0.0;
+	 *   - migration/migration.c|3146| <<migration_calculate_complete>> s->mbps = ((double ) bytes * 8.0) / transfer_time / 1000;
+	 *   - migration/migration.c|3209| <<migration_update_counters>> s->mbps = (((double ) transferred * 8.0) /
+	 *   - migration/migration.c|4132| <<migration_instance_init>> ms->mbps = -1;
+	 */
         monitor_printf(mon, "throughput: %0.2f mbps\n",
                        info->ram->mbps);
         monitor_printf(mon, "remaining ram: %" PRIu64 " kbytes\n",
@@ -111,15 +161,43 @@ void hmp_info_migrate(Monitor *mon, const QDict *qdict)
                        info->ram->normal);
         monitor_printf(mon, "normal bytes: %" PRIu64 " kbytes\n",
                        info->ram->normal_bytes >> 10);
+	/*
+	 * 在以下使用MigrationAtomicStats->dirty_sync_count:
+	 *   - migration/migration-hmp-cmds.c|156| <<hmp_info_migrate>> monitor_printf(mon, "dirty sync count: %" PRIu64 "\n", info->ram->dirty_sync_count);
+	 *   - migration/migration.c|1158| <<populate_ram_info>> info->ram->dirty_sync_count =
+	 *   - migration/migration.c|1159| <<populate_ram_info>> stat64_get(&mig_stats.dirty_sync_count);
+	 *   - migration/ram.c|603| <<xbzrle_cache_zero_page>> stat64_get(&mig_stats.dirty_sync_count));
+	 *   - migration/ram.c|629| <<save_xbzrle_page>> uint64_t generation = stat64_get(&mig_stats.dirty_sync_count);
+	 *   - migration/ram.c|1099| <<migration_bitmap_sync>> stat64_add(&mig_stats.dirty_sync_count, 1);
+	 *   - migration/ram.c|1150| <<migration_bitmap_sync>> uint64_t generation = stat64_get(&mig_stats.dirty_sync_count);
+	 */
         monitor_printf(mon, "dirty sync count: %" PRIu64 "\n",
                        info->ram->dirty_sync_count);
         monitor_printf(mon, "page size: %" PRIu64 " kbytes\n",
                        info->ram->page_size >> 10);
         monitor_printf(mon, "multifd bytes: %" PRIu64 " kbytes\n",
                        info->ram->multifd_bytes >> 10);
+	/*
+	 * 在以下使用MigrationState->pages_per_second:
+	 *   - migration/migration-hmp-cmds.c|162| <<hmp_info_migrate>> monitor_printf(mon, "pages-per-second: %" PRIu64 "\n", info->ram->pages_per_second);
+	 *   - migration/migration.c|1166| <<populate_ram_info>> info->ram->pages_per_second = s->pages_per_second;
+	 *   - migration/migration.c|1684| <<migrate_init>> s->pages_per_second = 0.0;
+	 *   - migration/migration.c|3209| <<migration_update_counters>> s->pages_per_second = (double ) transferred_pages /
+	 *   - migration/migration.c|4115| <<migration_instance_init>> ms->pages_per_second = -1;
+	 */
         monitor_printf(mon, "pages-per-second: %" PRIu64 "\n",
                        info->ram->pages_per_second);
 
+	/*
+	 * 在以下使用dirty_pages_rate:
+	 *   - migration/migration-hmp-cmds.c|123| <<hmp_info_migrate>> if (info->ram->dirty_pages_rate) {
+	 *   - migration/migration-hmp-cmds.c|125| <<hmp_info_migrate>> info->ram->dirty_pages_rate);
+	 *   - migration/migration.c|1191| <<populate_ram_info>> info->ram->dirty_pages_rate = stat64_get(&mig_stats.dirty_pages_rate);
+	 *   - migration/migration.c|1192| <<populate_ram_info>> stat64_get(&mig_stats.dirty_pages_rate);
+	 *   - migration/migration.c|3216| <<migration_update_counters>> if (stat64_get(&mig_stats.dirty_pages_rate) && transferred > 10000) {
+	 *   - migration/ram.c|973| <<migration_update_rates>> stat64_set(&mig_stats.dirty_pages_rate, rs->num_dirty_pages_period * 1000 / (end_time - rs->time_last_bitmap_sync));
+	 * Number of pages dirtied per second.
+	 */
         if (info->ram->dirty_pages_rate) {
             monitor_printf(mon, "dirty pages rate: %" PRIu64 " pages\n",
                            info->ram->dirty_pages_rate);
@@ -415,6 +493,24 @@ void hmp_savevm(Monitor *mon, const QDict *qdict)
 {
     Error *err = NULL;
 
+    /*
+     * 注释:
+     * save_snapshot: Save an internal snapshot.
+     * @name: name of internal snapshot
+     * @overwrite: replace existing snapshot with @name
+     * @vmstate: blockdev node name to store VM state in
+     * @has_devices: whether to use explicit device list
+     * @devices: explicit device list to snapshot
+     * @errp: pointer to error object
+     * On success, return %true.
+     * On failure, store an error through @errp and return %false.
+     *
+     * called by:
+     *   - migration/migration-hmp-cmds.c|418| <<hmp_savevm>> save_snapshot(qdict_get_try_str(qdict, "name"),
+     *   - migration/savevm.c|3731| <<snapshot_save_job_bh>> s->ret = save_snapshot(s->tag, false, s->vmstate, true, s->devices, s->errp);
+     *   - replay/replay-debugging.c|329| <<replay_gdb_attached>> if (!save_snapshot("start_debugging", true, NULL, false, NULL, NULL)) {
+     *   - replay/replay-snapshot.c|78| <<replay_vmstate_init>> if (!save_snapshot(replay_snapshot, true, NULL, false, NULL, &err)) {
+     */
     save_snapshot(qdict_get_try_str(qdict, "name"),
                   true, NULL, false, NULL, &err);
     hmp_handle_error(mon, err);
@@ -717,6 +813,10 @@ typedef struct HMPMigrationStatus {
     Monitor *mon;
 } HMPMigrationStatus;
 
+/*
+ * 在以下使用hmp_migrate_status_cb():
+ *   - migration/migration-hmp-cmds.c|803| <<hmp_migrate>> status->timer = timer_new_ms(QEMU_CLOCK_REALTIME, hmp_migrate_status_cb, status);
+ */
 static void hmp_migrate_status_cb(void *opaque)
 {
     HMPMigrationStatus *status = opaque;
@@ -839,6 +939,11 @@ void migrate_set_parameter_completion(ReadLineState *rs, int nb_args,
     }
 }
 
+/*
+ * called by:
+ *   - migration/migration-hmp-cmds.c|881| <<delvm_completion>> vm_completion(rs, str);
+ *   - migration/migration-hmp-cmds.c|888| <<loadvm_completion>> vm_completion(rs, str);
+ */
 static void vm_completion(ReadLineState *rs, const char *str)
 {
     size_t len;
@@ -882,6 +987,9 @@ void delvm_completion(ReadLineState *rs, int nb_args, const char *str)
     }
 }
 
+/*
+ * 不知道什么调用
+ */
 void loadvm_completion(ReadLineState *rs, int nb_args, const char *str)
 {
     if (nb_args == 2) {
diff --git a/migration/migration-stats.c b/migration/migration-stats.c
index f690b98a0..e2baed5c4 100644
--- a/migration/migration-stats.c
+++ b/migration/migration-stats.c
@@ -59,6 +59,18 @@ void migration_rate_reset(void)
     stat64_set(&mig_stats.rate_limit_start, migration_transferred_bytes());
 }
 
+/*
+ * called by:
+ *   - migration/migration-stats.c|33| <<migration_rate_exceeded>> uint64_t rate_limit_current = migration_transferred_bytes();
+ *   - migration/migration-stats.c|59| <<migration_rate_reset>> stat64_set(&mig_stats.rate_limit_start, migration_transferred_bytes());
+ *   - migration/migration.c|1150| <<populate_ram_info>> info->ram->transferred = migration_transferred_bytes();
+ *   - migration/migration.c|3138| <<migration_calculate_complete>> uint64_t bytes = migration_transferred_bytes();
+ *   - migration/migration.c|3175| <<update_iteration_initial_status>> s->iteration_initial_bytes = migration_transferred_bytes();
+ *   - migration/migration.c|3199| <<migration_update_counters>> current_bytes = migration_transferred_bytes();
+ *   - migration/ram.c|593| <<mig_throttle_counter_reset>> rs->bytes_xfer_prev = migration_transferred_bytes();
+ *   - migration/ram.c|1065| <<migration_trigger_throttle>> migration_transferred_bytes() - rs->bytes_xfer_prev;
+ *   - migration/ram.c|1190| <<migration_bitmap_sync>> rs->bytes_xfer_prev = migration_transferred_bytes();
+ */
 uint64_t migration_transferred_bytes(void)
 {
     uint64_t multifd = stat64_get(&mig_stats.multifd_bytes);
diff --git a/migration/migration-stats.h b/migration/migration-stats.h
index 05290ade7..6014d9c20 100644
--- a/migration/migration-stats.h
+++ b/migration/migration-stats.h
@@ -42,10 +42,29 @@ typedef struct {
      * since we synchronized bitmaps.
      */
     Stat64 dirty_bytes_last_sync;
+    /*
+     * 在以下使用dirty_pages_rate:
+     *   - migration/migration-hmp-cmds.c|123| <<hmp_info_migrate>> if (info->ram->dirty_pages_rate) {
+     *   - migration/migration-hmp-cmds.c|125| <<hmp_info_migrate>> info->ram->dirty_pages_rate);
+     *   - migration/migration.c|1191| <<populate_ram_info>> info->ram->dirty_pages_rate = stat64_get(&mig_stats.dirty_pages_rate);
+     *   - migration/migration.c|1192| <<populate_ram_info>> stat64_get(&mig_stats.dirty_pages_rate);
+     *   - migration/migration.c|3216| <<migration_update_counters>> if (stat64_get(&mig_stats.dirty_pages_rate) && transferred > 10000) {
+     *   - migration/ram.c|973| <<migration_update_rates>> stat64_set(&mig_stats.dirty_pages_rate, rs->num_dirty_pages_period * 1000 / (end_time - rs->time_last_bitmap_sync));
+     */
     /*
      * Number of pages dirtied per second.
      */
     Stat64 dirty_pages_rate;
+    /*
+     * 在以下使用MigrationAtomicStats->dirty_sync_count:
+     *   - migration/migration-hmp-cmds.c|156| <<hmp_info_migrate>> monitor_printf(mon, "dirty sync count: %" PRIu64 "\n", info->ram->dirty_sync_count);
+     *   - migration/migration.c|1158| <<populate_ram_info>> info->ram->dirty_sync_count =
+     *   - migration/migration.c|1159| <<populate_ram_info>> stat64_get(&mig_stats.dirty_sync_count);
+     *   - migration/ram.c|603| <<xbzrle_cache_zero_page>> stat64_get(&mig_stats.dirty_sync_count));
+     *   - migration/ram.c|629| <<save_xbzrle_page>> uint64_t generation = stat64_get(&mig_stats.dirty_sync_count);
+     *   - migration/ram.c|1099| <<migration_bitmap_sync>> stat64_add(&mig_stats.dirty_sync_count, 1);
+     *   - migration/ram.c|1150| <<migration_bitmap_sync>> uint64_t generation = stat64_get(&mig_stats.dirty_sync_count);
+     */
     /*
      * Number of times we have synchronized guest bitmaps.
      */
@@ -60,10 +79,26 @@ typedef struct {
      * guest is stopped.
      */
     Stat64 downtime_bytes;
+    /*
+     * 在以下使用MigrationAtomicStats->multifd_bytes:
+     *   - migration/migration-hmp-cmds.c|119| <<hmp_info_migrate>> monitor_printf(mon, "multifd bytes: %" PRIu64 " kbytes\n", info->ram->multifd_bytes >> 10);
+     *   - migration/migration-stats.c|64| <<migration_transferred_bytes>> uint64_t multifd = stat64_get(&mig_stats.multifd_bytes);
+     *   - migration/migration.c|1097| <<populate_ram_info>> info->ram->multifd_bytes = stat64_get(&mig_stats.multifd_bytes);
+     *   - migration/multifd.c|190| <<multifd_send_initial_packet>> stat64_add(&mig_stats.multifd_bytes, size);
+     *   - migration/multifd.c|798| <<multifd_send_thread>> stat64_add(&mig_stats.multifd_bytes, p->next_packet_size + p->packet_len);
+     */
     /*
      * Number of bytes sent through multifd channels.
      */
     Stat64 multifd_bytes;
+    /*
+     * 在以下使用MigrationAtomicStats->normal_pages:
+     *   - migration/migration.c|1155| <<populate_ram_info>> info->ram->normal = stat64_get(&mig_stats.normal_pages);
+     *   - migration/ram.c|963| <<ram_get_total_transferred_pages>> return stat64_get(&mig_stats.normal_pages) +
+     *   - migration/ram.c|1305| <<save_normal_page>> stat64_add(&mig_stats.normal_pages, 1);
+     *   - migration/ram.c|1369| <<ram_save_multifd_page>> stat64_add(&mig_stats.normal_pages, 1);
+     *   - migration/rdma.c|2204| <<qemu_rdma_write_one>> stat64_add(&mig_stats.normal_pages, sge.length / qemu_target_page_size());
+     */
     /*
      * Number of pages transferred that were not full of zeros.
      */
@@ -97,6 +132,15 @@ typedef struct {
      * Number of bytes sent through RDMA.
      */
     Stat64 rdma_bytes;
+    /*
+     * 在以下使用zero_pages:
+     *   - migration/migration.c|1152| <<populate_ram_info>> info->ram->duplicate = stat64_get(&mig_stats.zero_pages);
+     *   - migration/ram-compress.c|529| <<update_compress_thread_counts>> stat64_add(&mig_stats.zero_pages, 1);
+     *   - migration/ram.c|964| <<ram_get_total_transferred_pages>> return stat64_get(&mig_stats.normal_pages) + stat64_get(&mig_stats.zero_pages) +
+     *                                    compress_ram_pages() + xbzrle_counters.pages;
+     *   - migration/ram.c|1234| <<save_zero_page>> stat64_add(&mig_stats.zero_pages, 1);
+     *   - migration/rdma.c|2095| <<qemu_rdma_write_one>> stat64_add(&mig_stats.zero_pages, sge.length / qemu_target_page_size());
+     */
     /*
      * Number of pages transferred that were full of zeros.
      */
diff --git a/migration/migration.c b/migration/migration.c
index 3ce04b2aa..16152a1f3 100644
--- a/migration/migration.c
+++ b/migration/migration.c
@@ -68,6 +68,57 @@
 #include "sysemu/dirtylimit.h"
 #include "qemu/sockets.h"
 
+/*
+ * Migration Status的定义.
+ *
+ * ##
+ * # @MigrationStatus:
+ * #
+ * # An enumeration of migration status.
+ * #
+ * # @none: no migration has ever happened.
+ * #
+ * # @setup: migration process has been initiated.
+ * #
+ * # @cancelling: in the process of cancelling migration.
+ * #
+ * # @cancelled: cancelling migration is finished.
+ * #
+ * # @active: in the process of doing migration.
+ * #
+ * # @postcopy-active: like active, but now in postcopy mode.  (since
+ * #     2.5)
+ * #
+ * # @postcopy-paused: during postcopy but paused.  (since 3.0)
+ * #
+ * # @postcopy-recover: trying to recover from a paused postcopy.  (since
+ * #     3.0)
+ * #
+ * # @completed: migration is finished.
+ * #
+ * # @failed: some error occurred during migration process.
+ * #
+ * # @colo: VM is in the process of fault tolerance, VM can not get into
+ * #     this state unless colo capability is enabled for migration.
+ * #     (since 2.8)
+ * #
+ * # @pre-switchover: Paused before device serialisation.  (since 2.11)
+ * #
+ * # @device: During device serialisation when pause-before-switchover is
+ * #     enabled (since 2.11)
+ * #
+ * # @wait-unplug: wait for device unplug request by guest OS to be
+ * #     completed.  (since 4.2)
+ * #
+ * # Since: 2.3
+ * ##
+ * { 'enum': 'MigrationStatus',
+ *   'data': [ 'none', 'setup', 'cancelling', 'cancelled',
+ *             'active', 'postcopy-active', 'postcopy-paused',
+ *             'postcopy-recover', 'completed', 'failed', 'colo',
+ *             'pre-switchover', 'device', 'wait-unplug' ] }
+ */
+
 static NotifierList migration_state_notifiers =
     NOTIFIER_LIST_INITIALIZER(migration_state_notifiers);
 
@@ -93,6 +144,14 @@ enum mig_rp_message_type {
 static MigrationState *current_migration;
 static MigrationIncomingState *current_incoming;
 
+/*
+ * 在以下使用migration_blockers[]:
+ *   - migration/migration.c|96| <<global>> static GSList *migration_blockers[MIG_MODE__MAX];
+ *   - migration/migration.c|1143| <<fill_source_migration_info>> GSList *cur_blocker = migration_blockers[migrate_mode()];
+ *   - migration/migration.c|1686| <<add_blockers>> migration_blockers[mode] = g_slist_prepend(migration_blockers[mode],
+ *   - migration/migration.c|1734| <<migrate_del_blocker>> migration_blockers[mode] = g_slist_remove(migration_blockers[mode],
+ *   - migration/migration.c|1847| <<migration_is_blocked>> GSList *blockers = migration_blockers[migrate_mode()];
+ */
 static GSList *migration_blockers[MIG_MODE__MAX];
 
 static bool migration_object_check(MigrationState *ms, Error **errp);
@@ -512,6 +571,11 @@ bool migrate_uri_parse(const char *uri, MigrationChannel **channel,
     return true;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1735| <<qmp_migrate_incoming>> qemu_start_incoming_migration(uri, has_channels, channels, &local_err);
+ *   - migration/migration.c|1771| <<qmp_migrate_recover>> qemu_start_incoming_migration(uri, false, NULL, errp);
+ */
 static void qemu_start_incoming_migration(const char *uri, bool has_channels,
                                           MigrationChannelList *channels,
                                           Error **errp)
@@ -652,6 +716,10 @@ static void process_incoming_migration_bh(void *opaque)
     migration_incoming_state_destroy();
 }
 
+/*
+ * 在以下使用process_incoming_migration_co():
+ *   - migration/migration.c|808| <<migration_incoming_process>> Coroutine *co = qemu_coroutine_create(process_incoming_migration_co, NULL);
+ */
 static void coroutine_fn
 process_incoming_migration_co(void *opaque)
 {
@@ -739,6 +807,11 @@ static bool migration_incoming_setup(QEMUFile *f, Error **errp)
     return true;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|853| <<migration_fd_process_incoming>> migration_incoming_process();
+ *   - migration/migration.c|947| <<migration_ioc_process_incoming>> migration_incoming_process();
+ */
 void migration_incoming_process(void)
 {
     Coroutine *co = qemu_coroutine_create(process_incoming_migration_co, NULL);
@@ -778,6 +851,10 @@ static bool postcopy_try_recover(void)
     return false;
 }
 
+/*
+ * called by:
+ *   - migration/rdma.c|4060| <<rdma_accept_incoming_migration>> migration_fd_process_incoming(f, &local_err);
+ */
 void migration_fd_process_incoming(QEMUFile *f, Error **errp)
 {
     if (!migration_incoming_setup(f, errp)) {
@@ -814,6 +891,10 @@ static bool migration_should_start_incoming(bool main_channel)
     return true;
 }
 
+/*
+ * called by:
+ *   - migration/channel.c|45| <<migration_channel_process_incoming>> migration_ioc_process_incoming(ioc, &local_err);
+ */
 void migration_ioc_process_incoming(QIOChannel *ioc, Error **errp)
 {
     MigrationIncomingState *mis = migration_incoming_get_current();
@@ -1245,6 +1326,11 @@ MigrationInfo *qmp_query_migrate(Error **errp)
     return info;
 }
 
+/*
+ * called by:
+ *   - qmp
+ *   - migration/migration-hmp-cmds.c|701| <<hmp_migrate_start_postcopy>> qmp_migrate_start_postcopy(&err);
+ */
 void qmp_migrate_start_postcopy(Error **errp)
 {
     MigrationState *s = migrate_get_current();
@@ -1260,6 +1346,12 @@ void qmp_migrate_start_postcopy(Error **errp)
                          " started");
         return;
     }
+    /*
+     * 在以下使用MigrationState->start_postcopy:
+     *   - migration/migration.c|1272| <<qmp_migrate_start_postcopy>> qatomic_set(&s->start_postcopy, true);
+     *   - migration/migration.c|1601| <<migrate_init>> s->start_postcopy = false;
+     *   - migration/migration.c|3243| <<migration_iteration_run>> qatomic_read(&s->start_postcopy)) {
+     */
     /*
      * we don't error if migration has finished since that would be racy
      * with issuing this command.
@@ -1336,6 +1428,11 @@ static void migrate_fd_cleanup(MigrationState *s)
     yank_unregister_instance(MIGRATION_YANK_INSTANCE);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3269| <<migration_iteration_finish>> migrate_fd_cleanup_schedule(s);
+ *   - migration/migration.c|3300| <<bg_migration_iteration_finish>> migrate_fd_cleanup_schedule(s);
+ */
 static void migrate_fd_cleanup_schedule(MigrationState *s)
 {
     /*
@@ -1559,6 +1656,11 @@ bool migration_is_active(MigrationState *s)
             s->state == MIGRATION_STATUS_POSTCOPY_ACTIVE);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1930| <<migrate_prepare>> if (migrate_init(s, errp)) {
+ *   - migration/savevm.c|1790| <<qemu_savevm_state>> ret = migrate_init(ms, errp);
+ */
 int migrate_init(MigrationState *s, Error **errp)
 {
     int ret;
@@ -1816,10 +1918,19 @@ void qmp_migrate_pause(Error **errp)
                "during postcopy-active or postcopy-recover state");
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1930| <<migrate_prepare>> if (migration_is_blocked(errp)) {
+ *   - migration/savevm.c|3230| <<save_snapshot>> if (migration_is_blocked(errp)) {
+ */
 bool migration_is_blocked(Error **errp)
 {
     GSList *blockers = migration_blockers[migrate_mode()];
 
+    /*
+     * 遍历savevm_state.handlers
+     * 检验是否有不能迁移的设备,也就是se->vmsd->unmigratable
+     */
     if (qemu_savevm_state_blocked(errp)) {
         return true;
     }
@@ -1832,6 +1943,10 @@ bool migration_is_blocked(Error **errp)
     return false;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1972| <<qmp_migrate>> if (!migrate_prepare(s, has_blk && blk, has_inc && inc, resume_requested, errp)) {
+ */
 /* Returns true if continue to migrate, or false if error detected */
 static bool migrate_prepare(MigrationState *s, bool blk, bool blk_inc,
                             bool resume, Error **errp)
@@ -1969,6 +2084,9 @@ void qmp_migrate(const char *uri, bool has_channels,
     }
 
     resume_requested = has_resume && resume;
+    /*
+     * 检查能不能迁移, 最后migrate_init()
+     */
     if (!migrate_prepare(s, has_blk && blk, has_inc && inc,
                          resume_requested, errp)) {
         /* Error detected, put into errp */
@@ -1986,6 +2104,9 @@ void qmp_migrate(const char *uri, bool has_channels,
         if (saddr->type == SOCKET_ADDRESS_TYPE_INET ||
             saddr->type == SOCKET_ADDRESS_TYPE_UNIX ||
             saddr->type == SOCKET_ADDRESS_TYPE_VSOCK) {
+            /*
+	     * tcp socket是这里
+	     */
             socket_start_outgoing_migration(s, saddr, &local_err);
         } else if (saddr->type == SOCKET_ADDRESS_TYPE_FD) {
             fd_start_outgoing_migration(s, saddr->u.fd.str, &local_err);
@@ -2167,6 +2288,10 @@ static void migration_release_dst_files(MigrationState *ms)
  * Handles messages sent on the return path towards the source VM
  *
  */
+/*
+ * 在以下使用source_return_path_thread():
+ *   - migration/migration.c|2445| <<open_return_path_on_source>> qemu_thread_create(&ms->rp_state.rp_thread, "return path", source_return_path_thread, ms, QEMU_THREAD_JOINABLE);
+ */
 static void *source_return_path_thread(void *opaque)
 {
     MigrationState *ms = opaque;
@@ -2328,6 +2453,12 @@ out:
     return NULL;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3943| <<migrate_fd_connect>> if (open_return_path_on_source(s)) {
+ *
+ * 创建source_return_path_thread()
+ */
 static int open_return_path_on_source(MigrationState *ms)
 {
     ms->rp_state.from_dst_file = qemu_file_get_return_path(ms->to_dst_file);
@@ -2570,6 +2701,11 @@ fail:
  * migrate_pause_before_switchover called with the iothread locked
  * Returns: 0 on success
  */
+/*
+ * called by:
+ *   - migration/migration.c|2530| <<postcopy_start>> ret = migration_maybe_pause(ms, &cur_state, MIGRATION_STATUS_POSTCOPY_ACTIVE);
+ *   - migration/migration.c|2737| <<migration_completion_precopy>> ret = migration_maybe_pause(s, current_active_state, MIGRATION_STATUS_DEVICE);
+ */
 static int migration_maybe_pause(MigrationState *s,
                                  int *current_active_state,
                                  int new_state)
@@ -2608,6 +2744,10 @@ static int migration_maybe_pause(MigrationState *s,
     return s->state == new_state ? 0 : -EINVAL;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2734| <<migration_completion>> ret = migration_completion_precopy(s, &current_active_state);
+ */
 static int migration_completion_precopy(MigrationState *s,
                                         int *current_active_state)
 {
@@ -2626,6 +2766,11 @@ static int migration_completion_precopy(MigrationState *s,
         goto out_unlock;
     }
 
+    /*
+     * called by:
+     *   - migration/migration.c|2530| <<postcopy_start>> ret = migration_maybe_pause(ms, &cur_state, MIGRATION_STATUS_POSTCOPY_ACTIVE);
+     *   - migration/migration.c|2737| <<migration_completion_precopy>> ret = migration_maybe_pause(s, current_active_state, MIGRATION_STATUS_DEVICE);
+     */
     ret = migration_maybe_pause(s, current_active_state,
                                 MIGRATION_STATUS_DEVICE);
     if (ret < 0) {
@@ -2638,6 +2783,14 @@ static int migration_completion_precopy(MigrationState *s,
      */
     s->block_inactive = !migrate_colo();
     migration_rate_set(RATE_LIMIT_DISABLED);
+    /*
+     * called by:
+     *   - migration/migration.c|2451| <<postcopy_start>> qemu_savevm_state_complete_precopy(ms->to_dst_file, true, false);
+     *   - migration/migration.c|2496| <<postcopy_start>> qemu_savevm_state_complete_precopy(fb, false, false);
+     *   - migration/migration.c|2650| <<migration_completion_precopy>> ret = qemu_savevm_state_complete_precopy(s->to_dst_file, false, s->block_inactive);
+     *   - migration/savevm.c|1807| <<qemu_savevm_state>> qemu_savevm_state_complete_precopy(f, false, false);
+     *   - migration/savevm.c|1832| <<qemu_savevm_live_state>> qemu_savevm_state_complete_precopy(f, true, false);
+     */
     ret = qemu_savevm_state_complete_precopy(s->to_dst_file, false,
                                              s->block_inactive);
 out_unlock:
@@ -2645,6 +2798,10 @@ out_unlock:
     return ret;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2715| <<migration_completion>> migration_completion_postcopy(s);
+ */
 static void migration_completion_postcopy(MigrationState *s)
 {
     trace_migration_completion_postcopy_end();
@@ -2695,6 +2852,10 @@ static void migration_completion_failed(MigrationState *s,
  *
  * @s: Current migration state
  */
+/*
+ * called by:
+ *   - migration/migration.c|3119| <<migration_iteration_run>> migration_completion(s);
+ */
 static void migration_completion(MigrationState *s)
 {
     int ret = 0;
@@ -2967,6 +3128,11 @@ static MigThrError migration_detect_error(MigrationState *s)
     }
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3128| <<migration_iteration_finish>> migration_calculate_complete(s);
+ *   - migration/migration.c|3171| <<bg_migration_iteration_finish>> migration_calculate_complete(s);
+ */
 static void migration_calculate_complete(MigrationState *s)
 {
     uint64_t bytes = migration_transferred_bytes();
@@ -2981,6 +3147,16 @@ static void migration_calculate_complete(MigrationState *s)
     }
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3237| <<migration_update_counters>> update_iteration_initial_status(s);
+ *   - migration/migration.c|3713| <<migration_thread>> update_iteration_initial_status(s);
+ *   - migration/migration.c|3796| <<migration_thread>> update_iteration_initial_status(s);
+ *   - migration/migration.c|3864| <<bg_migration_thread>> update_iteration_initial_status(s);
+ *
+ * iteration_start_time和iteration_initial_bytes记录这个iteration
+ * 这样在iteration结束的时候可以计算比如throughput
+ */
 static void update_iteration_initial_status(MigrationState *s)
 {
     /*
@@ -2988,10 +3164,23 @@ static void update_iteration_initial_status(MigrationState *s)
      * wrong speed calculation.
      */
     s->iteration_start_time = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+    /*
+     * 在以下使用iteration_initial_bytes:
+     *   - migration/migration.c|1701| <<migrate_init>> s->iteration_initial_bytes = 0;
+     *   - migration/migration.c|3157| <<update_iteration_initial_status>> s->iteration_initial_bytes = migration_transferred_bytes();
+     *   - migration/migration.c|3182| <<migration_update_counters>> transferred = current_bytes - s->iteration_initial_bytes;
+     *
+     * bytes already send at the beginning of current iteration
+     */
     s->iteration_initial_bytes = migration_transferred_bytes();
     s->iteration_initial_pages = ram_get_total_transferred_pages();
 }
 
+/*
+ * called by;
+ *   - migration/migration.c|3533| <<migration_rate_limit>> migration_update_counters(s, now);
+ *   - migration/migration.c|3932| <<bg_migration_thread>> migration_update_counters(s, qemu_clock_get_ms(QEMU_CLOCK_REALTIME));
+ */
 static void migration_update_counters(MigrationState *s,
                                       int64_t current_time)
 {
@@ -3008,7 +3197,22 @@ static void migration_update_counters(MigrationState *s,
 
     switchover_bw = migrate_avail_switchover_bandwidth();
     current_bytes = migration_transferred_bytes();
+    /*
+     * 在以下使用iteration_initial_bytes:
+     *   - migration/migration.c|1701| <<migrate_init>> s->iteration_initial_bytes = 0;
+     *   - migration/migration.c|3157| <<update_iteration_initial_status>> s->iteration_initial_bytes = migration_transferred_bytes();
+     *   - migration/migration.c|3182| <<migration_update_counters>> transferred = current_bytes - s->iteration_initial_bytes;
+     *
+     * bytes already send at the beginning of current iteration
+     */
     transferred = current_bytes - s->iteration_initial_bytes;
+    /*
+     * 在以下使用MigrationState->iteration_start_time:
+     *   - migration/migration.c|3156| <<update_iteration_initial_status>> s->iteration_start_time = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+     *   - migration/migration.c|3176| <<migration_update_counters>> if (current_time < s->iteration_start_time + BUFFER_DELAY) {
+     *   - migration/migration.c|3183| <<migration_update_counters>> time_spent = current_time - s->iteration_start_time;
+     *   - migration/migration.c|3561| <<migration_rate_limit>> int ms = s->iteration_start_time + BUFFER_DELAY - now;
+     */
     time_spent = current_time - s->iteration_start_time;
     bandwidth = (double)transferred / time_spent;
 
@@ -3023,6 +3227,16 @@ static void migration_update_counters(MigrationState *s,
         expected_bw_per_ms = bandwidth;
     }
 
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1610| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|3056| <<migration_update_counters>> s->threshold_size = expected_bw_per_ms * migrate_downtime_limit();
+     *   - migration/migration.c|3083| <<migration_update_counters>> trace_migrate_transferred(transferred, time_spent, bandwidth, switchover_bw / 1000, s->threshold_size);
+     *   - migration/migration.c|3200| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|3206| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|3216| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover && qatomic_read(&s->start_postcopy)) {
+     *   - migration/ram.c|3342| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     */
     s->threshold_size = expected_bw_per_ms * migrate_downtime_limit();
 
     s->mbps = (((double) transferred * 8.0) /
@@ -3030,6 +3244,14 @@ static void migration_update_counters(MigrationState *s,
 
     transferred_pages = ram_get_total_transferred_pages() -
                             s->iteration_initial_pages;
+    /*
+     * 在以下使用MigrationSate->pages_per_second:
+     *   - migration/migration-hmp-cmds.c|162| <<hmp_info_migrate>> monitor_printf(mon, "pages-per-second: %" PRIu64 "\n", info->ram->pages_per_second);
+     *   - migration/migration.c|1166| <<populate_ram_info>> info->ram->pages_per_second = s->pages_per_second;
+     *   - migration/migration.c|1684| <<migrate_init>> s->pages_per_second = 0.0;
+     *   - migration/migration.c|3209| <<migration_update_counters>> s->pages_per_second = (double ) transferred_pages /
+     *   - migration/migration.c|4115| <<migration_instance_init>> ms->pages_per_second = -1;
+     */
     s->pages_per_second = (double) transferred_pages /
                              (((double) time_spent / 1000.0));
 
@@ -3064,6 +3286,12 @@ static bool migration_can_switchover(MigrationState *s)
         return true;
     }
 
+    /*
+     * 在以下使用MigrationState->switchover_acked:
+     *   - migration/migration.c|1686| <<migrate_init>> s->switchover_acked = false;
+     *   - migration/migration.c|2399| <<source_return_path_thread>> ms->switchover_acked = true;
+     *   - migration/migration.c|3216| <<migration_can_switchover>> return s->switchover_acked;
+     */
     return s->switchover_acked;
 }
 
@@ -3074,10 +3302,87 @@ typedef enum {
     MIG_ITERATE_BREAK,          /* Break the loop */
 } MigIterateState;
 
+/*
+ * 这是setup的时候的
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x555557264750) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d81aab in ram_init_bitmaps (rs=0x7ffdc0001410) at ../migration/ram.c:2811
+ * #8  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #9  0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #10 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #11 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #12 0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #14 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 这是iteration时候的
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x5555572646c0) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7fffeebee640) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7fffeebee640) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc4001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc4001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d827c3 in ram_state_pending_exact (opaque=0x555556f49e60 <ram_state>, must_precopy=0x7fffeebee7f8, can_postcopy=0x7fffeebee800) at ../migration/ram.c:3226
+ * #8  0x0000555555ba1817 in qemu_savevm_state_pending_exact (must_precopy=0x7fffeebee7f8, can_postcopy=0x7fffeebee800) at ../migration/savevm.c:1680
+ * #9  0x0000555555b8aac0 in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3094
+ * #10 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #11 0x0000555555fd1784 in qemu_thread_start (args=0x555557243440) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ *
+ * (gdb) bt
+ * #0  ram_save_complete (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:3140
+ * #1  0x0000555555ba124a in qemu_savevm_state_complete_precopy_iterable (f=0x555557236b40, in_postcopy=false) at ../migration/savevm.c:1517
+ * #2  0x0000555555ba161b in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1618
+ * #3  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7ffdc9baf7c0) at ../migration/migration.c:2641
+ * #4  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #5  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #6  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #7  0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #8  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #9  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  ram_save_iterate (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:3011
+ * #1  0x0000555555ba0e9f in qemu_savevm_state_iterate (f=0x555557236b40, postcopy=false) at ../migration/savevm.c:1424
+ * #2  0x0000555555b8abbb in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3116
+ * #3  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #4  0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #5  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #6  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 4243 static SaveVMHandlers savevm_ram_handlers = {
+ * 4244     .save_setup = ram_save_setup,
+ * 4245     .save_live_iterate = ram_save_iterate,
+ * 4246     .save_live_complete_postcopy = ram_save_complete,
+ * 4247     .save_live_complete_precopy = ram_save_complete,
+ * 4248     .has_postcopy = ram_has_postcopy,
+ * 4249     .state_pending_exact = ram_state_pending_exact,
+ * 4250     .state_pending_estimate = ram_state_pending_estimate,
+ * 4251     .load_state = ram_load,
+ * 4252     .save_cleanup = ram_save_cleanup,
+ * 4253     .load_setup = ram_load_setup,
+ * 4254     .load_cleanup = ram_load_cleanup,
+ * 4255     .resume_prepare = ram_resume_prepare,
+ * 4256 };
+ */
 /*
  * Return true if continue to the next iteration directly, false
  * otherwise.
  */
+/*
+ * called by:
+ *   - migration/migration.c|3370| <<migration_thread>> MigIterateState iter_state = migration_iteration_run(s);
+ */
 static MigIterateState migration_iteration_run(MigrationState *s)
 {
     uint64_t must_precopy, can_postcopy;
@@ -3090,7 +3395,29 @@ static MigIterateState migration_iteration_run(MigrationState *s)
 
     trace_migrate_pending_estimate(pending_size, must_precopy, can_postcopy);
 
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1610| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|3056| <<migration_update_counters>> s->threshold_size = expected_bw_per_ms * migrate_downtime_limit();
+     *   - migration/migration.c|3083| <<migration_update_counters>> trace_migrate_transferred(transferred, time_spent, bandwidth, switchover_bw / 1000, s->threshold_size);
+     *   - migration/migration.c|3200| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|3206| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|3216| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover && qatomic_read(&s->start_postcopy)) {
+     *   - migration/ram.c|3342| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     *
+     * 注释:
+     * The final stage happens when the remaining data is smaller than
+     * this threshold; it's calculated from the requested downtime and
+     * measured bandwidth, or avail-switchover-bandwidth if specified.
+     */
     if (must_precopy <= s->threshold_size) {
+        /*
+	 * 遍历&savevm_state.handlers调用.state_pending_exact
+	 * 比如ram是ram_state_pending_exact, vfio是vfio_state_pending_exact
+	 * 把结果的和通过两个参数返回
+	 *
+	 * 统计剩余的脏页的数量
+	 */
         qemu_savevm_state_pending_exact(&must_precopy, &can_postcopy);
         pending_size = must_precopy + can_postcopy;
         trace_migrate_pending_exact(pending_size, must_precopy, can_postcopy);
@@ -3098,10 +3425,22 @@ static MigIterateState migration_iteration_run(MigrationState *s)
 
     if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
         trace_migration_thread_low_pending(pending_size);
+	/*
+	 * 只在此处调用
+	 * 这里是completion!!!!!!!
+	 */
         migration_completion(s);
         return MIG_ITERATE_BREAK;
     }
 
+    /*
+     * 在以下使用MigrationState->start_postcopy:
+     *   - migration/migration.c|1272| <<qmp_migrate_start_postcopy>> qatomic_set(&s->start_postcopy, true);
+     *   - migration/migration.c|1601| <<migrate_init>> s->start_postcopy = false;
+     *   - migration/migration.c|3243| <<migration_iteration_run>> qatomic_read(&s->start_postcopy)) {
+     *
+     * 只在qmp_migrate_start_postcopy()设置成true
+     */
     /* Still a significant amount to transfer */
     if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover &&
         qatomic_read(&s->start_postcopy)) {
@@ -3112,11 +3451,23 @@ static MigIterateState migration_iteration_run(MigrationState *s)
         return MIG_ITERATE_SKIP;
     }
 
+    /*
+     * called by:
+     *   - migration/migration.c|3147| <<migration_iteration_run>> qemu_savevm_state_iterate(s->to_dst_file, in_postcopy);
+     *   - migration/migration.c|3233| <<bg_migration_iteration_run>> res = qemu_savevm_state_iterate(s->to_dst_file, false);
+     *   - migration/savevm.c|1823| <<qemu_savevm_state>> if (qemu_savevm_state_iterate(f, false) > 0) {
+     *
+     * 这里是真正迭代的地方
+     */
     /* Just another iteration step */
     qemu_savevm_state_iterate(s->to_dst_file, in_postcopy);
     return MIG_ITERATE_RESUME;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3381| <<migration_thread>> migration_iteration_finish(s);
+ */
 static void migration_iteration_finish(MigrationState *s)
 {
     /* If we enabled cpu throttling for auto-converge, turn it off. */
@@ -3187,6 +3538,10 @@ static void bg_migration_iteration_finish(MigrationState *s)
     qemu_mutex_unlock_iothread();
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3546| <<bg_migration_thread>> MigIterateState iter_state = bg_migration_iteration_run(s);
+ */
 /*
  * Return true if continue to the next iteration directly, false
  * otherwise.
@@ -3214,6 +3569,11 @@ void migration_consume_urgent_request(void)
     qemu_sem_wait(&migrate_get_current()->rate_limit_sem);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3781| <<migration_thread>> urgent = migration_rate_limit();
+ *   - migration/ram.c|2405| <<ram_save_host_page>> migration_rate_limit();
+ */
 /* Returns true if the rate limiting was broken by an urgent request */
 bool migration_rate_limit(void)
 {
@@ -3221,6 +3581,11 @@ bool migration_rate_limit(void)
     MigrationState *s = migrate_get_current();
 
     bool urgent = false;
+    /*
+     * called by;
+     *   - migration/migration.c|3533| <<migration_rate_limit>> migration_update_counters(s, now);
+     *   - migration/migration.c|3932| <<bg_migration_thread>> migration_update_counters(s, qemu_clock_get_ms(QEMU_CLOCK_REALTIME));
+     */
     migration_update_counters(s, now);
     if (migration_rate_exceeded(s->to_dst_file)) {
 
@@ -3287,6 +3652,85 @@ static void qemu_savevm_wait_unplug(MigrationState *s, int old_state,
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  kvm_mem_flags (mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:469
+ * #1  0x0000555555dc270f in kvm_slot_update_flags (kml=0x555557242d10, mem=0x5555572646c0, mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:485
+ * #2  0x0000555555dc282b in kvm_section_update_flags (kml=0x555557242d10, section=0x7ffdcb8b75f0) at ../accel/kvm/kvm-all.c:518
+ * #3  0x0000555555dc28cc in kvm_log_start (listener=0x555557242d10, section=0x7ffdcb8b75f0, old=0, new=4) at ../accel/kvm/kvm-all.c:539
+ * #4  0x0000555555d66ba7 in address_space_update_topology_pass (as=0x555556f49b40 <address_space_memory>, old_view=0x7ffde8144c90, new_view=0x7ffdb80418f0, adding=true) at ../system/memory.c:987
+ * #5  0x0000555555d6700d in address_space_set_flatview (as=0x555556f49b40 <address_space_memory>) at ../system/memory.c:1080
+ * #6  0x0000555555d671b5 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #7  0x0000555555d6be07 in memory_global_dirty_log_start (flags=1) at ../system/memory.c:2926
+ * #8  0x0000555555d81a9a in ram_init_bitmaps (rs=0x7ffdb8001320) at ../migration/ram.c:2810
+ * #9  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #10 0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #11 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #12 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #13 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #14 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #15 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  ram_save_iterate (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:3011
+ * #1  0x0000555555ba0e9f in qemu_savevm_state_iterate (f=0x555557236b40, postcopy=false) at ../migration/savevm.c:1424
+ * #2  0x0000555555b8abbb in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3116
+ * #3  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #4  0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #5  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #6  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x555557264750) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d81aab in ram_init_bitmaps (rs=0x7ffdc0001410) at ../migration/ram.c:2811
+ * #8  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #9  0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #10 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #11 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #12 0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #14 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 8个channel只调用一次
+ * (gdb) bt
+ * #0  migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3296
+ * #1  0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #2  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #3  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (qemu) info migrate
+ * globals:
+ * store-global-state: on
+ * only-migratable: off
+ * send-configuration: on
+ * send-section-footer: on
+ * decompress-error-check: on
+ * clear-bitmap-shift: 18
+ * Migration status: active
+ * total time: 175139 ms
+ * expected downtime: 69162 ms
+ * setup: 34 ms
+ * transferred ram: 23047141 kbytes
+ * throughput: 1135.37 mbps
+ * remaining ram: 2567352 kbytes
+ * total ram: 104874824 kbytes
+ * duplicate: 27520241 pages
+ * skipped: 0 pages
+ * normal: 5686721 pages
+ * normal bytes: 22746884 kbytes
+ * dirty sync count: 7
+ * page size: 4 kbytes
+ * multifd bytes: 22805230 kbytes
+ * pages-per-second: 34670
+ * dirty pages rate: 215095 pages
+ * precopy ram: 241892 kbytes
+ */
 /*
  * Master migration thread on the source VM.
  * It drives the migration and pumps the data down the outgoing channel.
@@ -3307,6 +3751,14 @@ static void *migration_thread(void *opaque)
     update_iteration_initial_status(s);
 
     qemu_mutex_lock_iothread();
+    /*
+     * called by:
+     *   - gration/migration.c|3652| <<migration_thread>> qemu_savevm_state_header(s->to_dst_file);
+     *   - migration/migration.c|3803| <<bg_migration_thread>> qemu_savevm_state_header(s->to_dst_file);
+     *   - migration/savevm.c|1983| <<qemu_savevm_state>> qemu_savevm_state_header(f);
+     *
+     * 发送头部header --> 通过to_dst_file
+     */
     qemu_savevm_state_header(s->to_dst_file);
     qemu_mutex_unlock_iothread();
 
@@ -3337,6 +3789,11 @@ static void *migration_thread(void *opaque)
     }
 
     qemu_mutex_lock_iothread();
+    /*
+     * 应该是第一阶段!
+     * 核心是遍历&savevm_state.handlers调用各种save_setup, 比如ram_save_setup()
+     * 为内存分配脏页标记的bitmap, 开启脏页记录, 把所有ramblock写入stream
+     */
     qemu_savevm_state_setup(s->to_dst_file);
     qemu_mutex_unlock_iothread();
 
@@ -3347,6 +3804,9 @@ static void *migration_thread(void *opaque)
 
     trace_migration_thread_setup_complete();
 
+    /*
+     * 这里是主循环
+     */
     while (migration_is_active(s)) {
         if (urgent || !migration_rate_exceeded(s->to_dst_file)) {
             MigIterateState iter_state = migration_iteration_run(s);
@@ -3371,6 +3831,13 @@ static void *migration_thread(void *opaque)
              * the local variables. This is important to avoid
              * breaking transferred_bytes and bandwidth calculation
              */
+            /*
+	     * called by:
+	     *   - migration/migration.c|3237| <<migration_update_counters>> update_iteration_initial_status(s);
+	     *   - migration/migration.c|3713| <<migration_thread>> update_iteration_initial_status(s);
+	     *   - migration/migration.c|3796| <<migration_thread>> update_iteration_initial_status(s);
+	     *   - migration/migration.c|3864| <<bg_migration_thread>> update_iteration_initial_status(s);
+	     */
             update_iteration_initial_status(s);
         }
 
@@ -3546,6 +4013,11 @@ fail:
     return NULL;
 }
 
+/*
+ * called by:
+ *   - migration/channel.c|92| <<migration_channel_connect>> migrate_fd_connect(s, error);
+ *   - migration/rdma.c|4177| <<rdma_start_outgoing_migration>> migrate_fd_connect(s, NULL);
+ */
 void migrate_fd_connect(MigrationState *s, Error *error_in)
 {
     Error *local_err = NULL;
@@ -3564,6 +4036,18 @@ void migrate_fd_connect(MigrationState *s, Error *error_in)
         assert(s->cleanup_bh);
     } else {
         assert(!s->cleanup_bh);
+        /*
+	 * 在以下使用MigrationState->cleanup_bh:
+	 *   - migration/migration.c|1358| <<migrate_fd_cleanup>> qemu_bh_delete(s->cleanup_bh);
+	 *   - migration/migration.c|1359| <<migrate_fd_cleanup>> s->cleanup_bh = NULL;
+	 *   - migration/migration.c|1426| <<migrate_fd_cleanup_schedule>> qemu_bh_schedule(s->cleanup_bh);
+	 *   - migration/migration.c|1661| <<migrate_init>> s->cleanup_bh = 0;
+	 *   - migration/migration.c|3929| <<migrate_fd_connect>> assert(s->cleanup_bh);
+	 *   - migration/migration.c|3931| <<migrate_fd_connect>> assert(!s->cleanup_bh)
+	 *   - migration/migration.c|3932| <<migrate_fd_connect>> s->cleanup_bh = qemu_bh_new(migrate_fd_cleanup_bh, s);
+	 *
+	 * 结束的时候触发
+	 */
         s->cleanup_bh = qemu_bh_new(migrate_fd_cleanup_bh, s);
     }
     if (error_in) {
@@ -3603,6 +4087,9 @@ void migrate_fd_connect(MigrationState *s, Error *error_in)
      * QEMU uses the return path.
      */
     if (migrate_postcopy_ram() || migrate_return_path()) {
+        /*
+	 * 创建source_return_path_thread()
+	 */
         if (open_return_path_on_source(s)) {
             error_setg(&local_err, "Unable to open return-path for postcopy");
             migrate_set_state(&s->state, s->state, MIGRATION_STATUS_FAILED);
@@ -3630,6 +4117,12 @@ void migrate_fd_connect(MigrationState *s, Error *error_in)
         return;
     }
 
+    /*
+     * 这里是multifd
+     * 只在此处调用
+     *
+     * 会创建N个(比如8个)multifd_send_thread()
+     */
     if (multifd_save_setup(&local_err) != 0) {
         migrate_set_error(s, local_err);
         error_report_err(local_err);
@@ -3643,6 +4136,9 @@ void migrate_fd_connect(MigrationState *s, Error *error_in)
         qemu_thread_create(&s->thread, "bg_snapshot",
                 bg_migration_thread, s, QEMU_THREAD_JOINABLE);
     } else {
+        /*
+	 * 这里是创建迁移的进程
+	 */
         qemu_thread_create(&s->thread, "live_migration",
                 migration_thread, s, QEMU_THREAD_JOINABLE);
     }
diff --git a/migration/migration.h b/migration/migration.h
index cf2c9c88e..dbe5bcbbc 100644
--- a/migration/migration.h
+++ b/migration/migration.h
@@ -256,6 +256,18 @@ struct MigrationState {
     /*< public >*/
     QemuThread thread;
     QEMUBH *vm_start_bh;
+    /*
+     * 在以下使用MigrationState->cleanup_bh:
+     *   - migration/migration.c|1358| <<migrate_fd_cleanup>> qemu_bh_delete(s->cleanup_bh);
+     *   - migration/migration.c|1359| <<migrate_fd_cleanup>> s->cleanup_bh = NULL;
+     *   - migration/migration.c|1426| <<migrate_fd_cleanup_schedule>> qemu_bh_schedule(s->cleanup_bh);
+     *   - migration/migration.c|1661| <<migrate_init>> s->cleanup_bh = 0;
+     *   - migration/migration.c|3929| <<migrate_fd_connect>> assert(s->cleanup_bh);
+     *   - migration/migration.c|3931| <<migrate_fd_connect>> assert(!s->cleanup_bh)
+     *   - migration/migration.c|3932| <<migrate_fd_connect>> s->cleanup_bh = qemu_bh_new(migrate_fd_cleanup_bh, s);
+     *
+     * 结束的时候触发
+     */
     QEMUBH *cleanup_bh;
     /* Protected by qemu_file_lock */
     QEMUFile *to_dst_file;
@@ -284,13 +296,46 @@ struct MigrationState {
     /* pages already send at the beginning of current iteration */
     uint64_t iteration_initial_pages;
 
+    /*
+     * 在以下使用MigrationSate->pages_per_second:
+     *   - migration/migration-hmp-cmds.c|162| <<hmp_info_migrate>> monitor_printf(mon, "pages-per-second: %" PRIu64 "\n", info->ram->pages_per_second);
+     *   - migration/migration.c|1166| <<populate_ram_info>> info->ram->pages_per_second = s->pages_per_second;
+     *   - migration/migration.c|1684| <<migrate_init>> s->pages_per_second = 0.0;
+     *   - migration/migration.c|3209| <<migration_update_counters>> s->pages_per_second = (double ) transferred_pages /
+     *   - migration/migration.c|4115| <<migration_instance_init>> ms->pages_per_second = -1;
+     */
     /* pages transferred per second */
     double pages_per_second;
 
+    /*
+     * 在以下使用iteration_initial_bytes:
+     *   - migration/migration.c|1701| <<migrate_init>> s->iteration_initial_bytes = 0;
+     *   - migration/migration.c|3157| <<update_iteration_initial_status>> s->iteration_initial_bytes = migration_transferred_bytes();
+     *   - migration/migration.c|3182| <<migration_update_counters>> transferred = current_bytes - s->iteration_initial_bytes;
+     *
+     * bytes already send at the beginning of current iteration
+     */
     /* bytes already send at the beginning of current iteration */
     uint64_t iteration_initial_bytes;
+    /*
+     * 在以下使用MigrationState->iteration_start_time:
+     *   - migration/migration.c|3156| <<update_iteration_initial_status>> s->iteration_start_time = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+     *   - migration/migration.c|3176| <<migration_update_counters>> if (current_time < s->iteration_start_time + BUFFER_DELAY) {
+     *   - migration/migration.c|3183| <<migration_update_counters>> time_spent = current_time - s->iteration_start_time;
+     *   - migration/migration.c|3561| <<migration_rate_limit>> int ms = s->iteration_start_time + BUFFER_DELAY - now;
+     */
     /* time at the start of current iteration */
     int64_t iteration_start_time;
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1610| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|3056| <<migration_update_counters>> s->threshold_size = expected_bw_per_ms * migrate_downtime_limit();
+     *   - migration/migration.c|3083| <<migration_update_counters>> trace_migrate_transferred(transferred, time_spent, bandwidth, switchover_bw / 1000, s->threshold_size);
+     *   - migration/migration.c|3200| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|3206| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|3216| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover && qatomic_read(&s->start_postcopy)) {
+     *   - migration/ram.c|3342| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     */
     /*
      * The final stage happens when the remaining data is smaller than
      * this threshold; it's calculated from the requested downtime and
@@ -330,9 +375,27 @@ struct MigrationState {
         QemuSemaphore rp_pong_acks;
     } rp_state;
 
+    /*
+     * 在以下使用MigrationState->mbps:
+     *   - migration/migration-hmp-cmds.c|142| <<hmp_info_migrate>> monitor_printf(mon, "throughput: %0.2f mbps\n", info->ram->mbps);
+     *   - migration/migration.c|1157| <<populate_ram_info>> info->ram->mbps = s->mbps;
+     *   - migration/migration.c|1683| <<migrate_init>> s->mbps = 0.0;
+     *   - migration/migration.c|3146| <<migration_calculate_complete>> s->mbps = ((double ) bytes * 8.0) / transfer_time / 1000;
+     *   - migration/migration.c|3209| <<migration_update_counters>> s->mbps = (((double ) transferred * 8.0) /
+     *   - migration/migration.c|4132| <<migration_instance_init>> ms->mbps = -1;
+     */
     double mbps;
     /* Timestamp when recent migration starts (ms) */
     int64_t start_time;
+    /*
+     * 在以下使用MigrationState->total_time:
+     *   - migration/migration-hmp-cmds.c|82| <<hmp_info_migrate>> info->total_time);
+     *   - migration/migration.c|1048| <<populate_time_info>> info->total_time = s->total_time;
+     *   - migration/migration.c|1051| <<populate_time_info>> info->total_time = qemu_clock_get_ms(QEMU_CLOCK_REALTIME) -
+     *   - migration/migration.c|1597| <<migrate_init>> s->total_time = 0;
+     *   - migration/migration.c|2977| <<migration_calculate_complete>> s->total_time = end_time - s->start_time;
+     *   - migration/migration.c|2978| <<migration_calculate_complete>> transfer_time = s->total_time - s->setup_time;
+     */
     /* Total time used by latest migration (ms) */
     int64_t total_time;
     /* Timestamp when VM is down (ms) to migrate the last stuff */
@@ -350,6 +413,12 @@ struct MigrationState {
      */
     RunState vm_old_state;
 
+    /*
+     * 在以下使用MigrationState->start_postcopy:
+     *   - migration/migration.c|1272| <<qmp_migrate_start_postcopy>> qatomic_set(&s->start_postcopy, true);
+     *   - migration/migration.c|1601| <<migrate_init>> s->start_postcopy = false;
+     *   - migration/migration.c|3243| <<migration_iteration_run>> qatomic_read(&s->start_postcopy)) {
+     */
     /* Flag set once the migration has been asked to enter postcopy */
     bool start_postcopy;
     /* Flag set after postcopy has sent the device state */
@@ -391,6 +460,13 @@ struct MigrationState {
      */
     bool store_global_state;
 
+    /*
+     * 在以下使用MigrationState->send_configuration:
+     *   - migration/options.c|116| <<global>> DEFINE_PROP_BOOL("send-configuration", MigrationState, send_configuration, true),
+     *   - migration/migration-hmp-cmds.c|46| <<migration_global_dump>> monitor_printf(mon, "send-configuration: %s\n", ms->send_configuration ? "on" : "off");
+     *   - migration/savevm.c|1352| <<qemu_savevm_state_header>> if (s->send_configuration) {
+     *   - migration/savevm.c|2988| <<qemu_loadvm_state_header>> if (migrate_get_current()->send_configuration) {
+     */
     /* Whether we send QEMU_VM_CONFIGURATION during migration */
     bool send_configuration;
     /* Whether we send section footer during migration */
@@ -453,6 +529,18 @@ struct MigrationState {
      * dirty bitmap only once for 1<<10=1K continuous guest pages
      * (which is in 4M chunk).
      */
+    /*
+     * 在以下使用clear_bitmap_shift:
+     *   - migration/options.c|124| <<global>> clear_bitmap_shift, CLEAR_BITMAP_SHIFT_DEFAULT),
+     *   - migration/migration-hmp-cmds.c|52| <<migration_global_dump>> ms->clear_bitmap_shift);
+     *   - migration/ram.c|2941| <<ram_list_init_bitmaps>> shift = ms->clear_bitmap_shift;
+     *
+     * 默认的
+     *  1<<18=256K pages -> 1G chunk when page size is 4K.  This is the
+     *  default value to use if no one specified.
+     *
+     * #define CLEAR_BITMAP_SHIFT_DEFAULT        18
+     */
     uint8_t clear_bitmap_shift;
 
     /*
@@ -463,6 +551,12 @@ struct MigrationState {
     /* QEMU_VM_VMDESCRIPTION content filled for all non-iterable devices. */
     JSONWriter *vmdesc;
 
+    /*
+     * 在以下使用MigrationState->switchover_acked:
+     *   - migration/migration.c|1686| <<migrate_init>> s->switchover_acked = false;
+     *   - migration/migration.c|2399| <<source_return_path_thread>> ms->switchover_acked = true;
+     *   - migration/migration.c|3216| <<migration_can_switchover>> return s->switchover_acked;
+     */
     /*
      * Indicates whether an ACK from the destination that it's OK to do
      * switchover has been received.
diff --git a/migration/multifd.c b/migration/multifd.c
index 409460684..f4ff7c219 100644
--- a/migration/multifd.c
+++ b/migration/multifd.c
@@ -172,6 +172,10 @@ void multifd_register_ops(int method, MultiFDMethods *ops)
     multifd_ops[method] = ops;
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|730| <<multifd_send_thread>> if (multifd_send_initial_packet(p, &local_err) < 0) {
+ */
 static int multifd_send_initial_packet(MultiFDSendParams *p, Error **errp)
 {
     MultiFDInit_t msg = {};
@@ -355,12 +359,35 @@ static int multifd_recv_unfill_packet(MultiFDRecvParams *p, Error **errp)
     return 0;
 }
 
+/*
+ * multifd_send_state被下面的函数使用:
+ *  - multifd_send_pages()
+ *  - multifd_queue_page()
+ *  - multifd_send_terminate_threads()
+ *  - multifd_save_cleanup()
+ *  - multifd_send_sync_main()
+ *  - multifd_send_thread()
+ *  - multifd_tls_outgoing_handshake()
+ *  - multifd_new_send_channel_cleanup()
+ *  - multifd_save_setup()
+ */
 struct {
     MultiFDSendParams *params;
     /* array of pages to sent */
     MultiFDPages_t *pages;
     /* global number of generated multifd packets */
     uint64_t packet_num;
+    /*
+     * 在以下使用multifd_send_state->channels_ready:
+     *   - migration/multifd.c|405| <<multifd_send_pages>> qemu_sem_wait(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|561| <<multifd_save_cleanup>> qemu_sem_destroy(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|637| <<multifd_send_sync_main>> qemu_sem_wait(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|675| <<multifd_send_thread>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|759| <<multifd_send_thread>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|799| <<multifd_tls_outgoing_handshake>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|875| <<multifd_new_send_channel_cleanup>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|937| <<multifd_save_setup>> qemu_sem_init(&multifd_send_state->channels_ready, 0);
+     */
     /* send channels ready */
     QemuSemaphore channels_ready;
     /*
@@ -391,17 +418,70 @@ struct {
  * false.
  */
 
+/*
+ * (gdb) bt
+ * #0  multifd_send_pages (f=0x555557236b40) at ../migration/multifd.c:398
+ * #1  0x0000555555b8e325 in multifd_queue_page (f=0x555557236b40, block=0x5555572bb6c0, offset=2703360) at ../migration/multifd.c:460
+ * #2  0x0000555555d7edc1 in ram_save_multifd_page (file=0x555557236b40, block=0x5555572bb6c0, offset=2703360) at ../migration/ram.c:1256
+ * #3  0x0000555555d8059d in ram_save_target_page_legacy (rs=0x7ffdb8001180, pss=0x7ffdb8001180) at ../migration/ram.c:2070
+ * #4  0x0000555555d80955 in ram_save_host_page (rs=0x7ffdb8001180, pss=0x7ffdb8001180) at ../migration/ram.c:2231
+ * #5  0x0000555555d80b1f in ram_find_and_save_block (rs=0x7ffdb8001180) at ../migration/ram.c:2315
+ * #6  0x0000555555d82239 in ram_save_iterate (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:3058
+ * #7  0x0000555555ba0e9f in qemu_savevm_state_iterate (f=0x555557236b40, postcopy=false) at ../migration/savevm.c:1424
+ * #8  0x0000555555b8abbb in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3116
+ * #9  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #10 0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #11 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #12 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - migration/multifd.c|460| <<multifd_queue_page>> if (multifd_send_pages(f) < 0) {
+ *   - migration/multifd.c|596| <<multifd_send_sync_main>> if (multifd_send_pages(f) < 0) {
+ */
 static int multifd_send_pages(QEMUFile *f)
 {
     int i;
+    /*
+     * 静态的!!!
+     */
     static int next_channel;
     MultiFDSendParams *p = NULL; /* make happy gcc */
+    /*
+     * typedef struct {
+     *     // number of used pages
+     *     uint32_t num;
+     *     // number of allocated pages
+     *     uint32_t allocated;
+     *     // global number of generated multifd packets
+     *     uint64_t packet_num;
+     *     // offset of each page
+     *     ram_addr_t *offset;
+     *     RAMBlock *block;
+     * } MultiFDPages_t;
+     */
     MultiFDPages_t *pages = multifd_send_state->pages;
 
+    /*
+     * 注释:
+     * Have we already run terminate threads.  There is a race when it
+     * happens that we got one error while we are exiting.
+     * We will use atomic operations.  Only valid values are 0 and 1.
+     */
     if (qatomic_read(&multifd_send_state->exiting)) {
         return -1;
     }
 
+    /*
+     * 在以下使用multifd_send_state->channels_ready:
+     *   - migration/multifd.c|405| <<multifd_send_pages>> qemu_sem_wait(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|561| <<multifd_save_cleanup>> qemu_sem_destroy(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|637| <<multifd_send_sync_main>> qemu_sem_wait(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|675| <<multifd_send_thread>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|759| <<multifd_send_thread>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|799| <<multifd_tls_outgoing_handshake>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|875| <<multifd_new_send_channel_cleanup>> qemu_sem_post(&multifd_send_state->channels_ready);
+     *   - migration/multifd.c|937| <<multifd_save_setup>> qemu_sem_init(&multifd_send_state->channels_ready, 0);
+     */
     qemu_sem_wait(&multifd_send_state->channels_ready);
     /*
      * next_channel can remain from a previous migration that was
@@ -418,6 +498,15 @@ static int multifd_send_pages(QEMUFile *f)
             qemu_mutex_unlock(&p->mutex);
             return -1;
         }
+        /*
+	 * 在以下使用MultiFDSendParams->pending_job:
+	 *   - migration/multifd.c|467| <<multifd_send_pages>> if (!p->pending_job) {
+	 *   - migration/multifd.c|468| <<multifd_send_pages>> p->pending_job++;
+	 *   - migration/multifd.c|693| <<multifd_send_sync_main>> p->pending_job++;
+	 *   - migration/multifd.c|757| <<multifd_send_thread>> if (p->pending_job) {
+	 *   - migration/multifd.c|823| <<multifd_send_thread>> p->pending_job--;
+	 *   - migration/multifd.c|1052| <<multifd_save_setup>> p->pending_job = 0;
+	 */
         if (!p->pending_job) {
             p->pending_job++;
             next_channel = (i + 1) % migrate_multifd_channels();
@@ -432,13 +521,34 @@ static int multifd_send_pages(QEMUFile *f)
     multifd_send_state->pages = p->pages;
     p->pages = pages;
     qemu_mutex_unlock(&p->mutex);
+    /*
+     * 唤醒multifd_send_thread()
+     */
     qemu_sem_post(&p->sem);
 
     return 1;
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|511| <<multifd_queue_page>> return multifd_queue_page(f, block, offset);
+ *   - migration/ram.c|1300| <<ram_save_multifd_page>> if (multifd_queue_page(file, block, offset) < 0) {
+ */
 int multifd_queue_page(QEMUFile *f, RAMBlock *block, ram_addr_t offset)
 {
+    /*
+     * typedef struct {
+     *     // number of used pages
+     *     uint32_t num;
+     *     // number of allocated pages
+     *     uint32_t allocated;
+     *     // global number of generated multifd packets
+     *     uint64_t packet_num;
+     *     // offset of each page
+     *     ram_addr_t *offset;
+     *     RAMBlock *block;
+     * } MultiFDPages_t;
+     */
     MultiFDPages_t *pages = multifd_send_state->pages;
     bool changed = false;
 
@@ -457,6 +567,11 @@ int multifd_queue_page(QEMUFile *f, RAMBlock *block, ram_addr_t offset)
         changed = true;
     }
 
+    /*
+     * called by:
+     *   - migration/multifd.c|460| <<multifd_queue_page>> if (multifd_send_pages(f) < 0) {
+     *   - migration/multifd.c|596| <<multifd_send_sync_main>> if (multifd_send_pages(f) < 0) {
+     */
     if (multifd_send_pages(f) < 0) {
         return -1;
     }
@@ -468,6 +583,11 @@ int multifd_queue_page(QEMUFile *f, RAMBlock *block, ram_addr_t offset)
     return 1;
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|580| <<multifd_save_cleanup>> multifd_send_terminate_threads(NULL);
+ *   - migration/multifd.c|839| <<multifd_send_thread>> multifd_send_terminate_threads(local_err);
+ */
 static void multifd_send_terminate_threads(Error *err)
 {
     int i;
@@ -584,6 +704,13 @@ static int multifd_zero_copy_flush(QIOChannel *c)
     return ret;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1356| <<find_dirty_block>> int ret = multifd_send_sync_main(f);
+ *   - migration/ram.c|3060| <<ram_save_setup>> ret = multifd_send_sync_main(f);
+ *   - migration/ram.c|3221| <<ram_save_iterate>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+ *   - migration/ram.c|3311| <<ram_save_complete>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+ */
 int multifd_send_sync_main(QEMUFile *f)
 {
     int i;
@@ -647,6 +774,10 @@ int multifd_send_sync_main(QEMUFile *f)
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|855| <<multifd_channel_connect>> qemu_thread_create(&p->thread, p->name, multifd_send_thread, p, QEMU_THREAD_JOINABLE);
+ */
 static void *multifd_send_thread(void *opaque)
 {
     MultiFDSendParams *p = opaque;
@@ -668,12 +799,26 @@ static void *multifd_send_thread(void *opaque)
     p->num_packets = 1;
 
     while (true) {
+        /*
+	 * 在以下使用multifd_send_state->channels_ready:
+	 *   - migration/multifd.c|405| <<multifd_send_pages>> qemu_sem_wait(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|561| <<multifd_save_cleanup>> qemu_sem_destroy(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|637| <<multifd_send_sync_main>> qemu_sem_wait(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|675| <<multifd_send_thread>> qemu_sem_post(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|759| <<multifd_send_thread>> qemu_sem_post(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|799| <<multifd_tls_outgoing_handshake>> qemu_sem_post(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|875| <<multifd_new_send_channel_cleanup>> qemu_sem_post(&multifd_send_state->channels_ready);
+	 *   - migration/multifd.c|937| <<multifd_save_setup>> qemu_sem_init(&multifd_send_state->channels_ready, 0);
+	 */
         qemu_sem_post(&multifd_send_state->channels_ready);
         qemu_sem_wait(&p->sem);
 
         if (qatomic_read(&multifd_send_state->exiting)) {
             break;
         }
+	/*
+	 * p是MultiFDSendParams
+	 */
         qemu_mutex_lock(&p->mutex);
 
         if (p->pending_job) {
@@ -730,6 +875,14 @@ static void *multifd_send_thread(void *opaque)
                 break;
             }
 
+            /*
+	     * 在以下使用multifd_bytes:
+	     *   - migration/migration-hmp-cmds.c|119| <<hmp_info_migrate>> monitor_printf(mon, "multifd bytes: %" PRIu64 " kbytes\n", info->ram->multifd_bytes >> 10);
+	     *   - migration/migration-stats.c|64| <<migration_transferred_bytes>> uint64_t multifd = stat64_get(&mig_stats.multifd_bytes);
+	     *   - migration/migration.c|1097| <<populate_ram_info>> info->ram->multifd_bytes = stat64_get(&mig_stats.multifd_bytes);
+	     *   - migration/multifd.c|190| <<multifd_send_initial_packet>> stat64_add(&mig_stats.multifd_bytes, size);
+	     *   - migration/multifd.c|798| <<multifd_send_thread>> stat64_add(&mig_stats.multifd_bytes, p->next_packet_size + p->packet_len);
+	     */
             stat64_add(&mig_stats.multifd_bytes,
                        p->next_packet_size + p->packet_len);
             p->next_packet_size = 0;
@@ -832,6 +985,11 @@ static bool multifd_tls_channel_connect(MultiFDSendParams *p,
     return true;
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|783| <<multifd_tls_outgoing_handshake>> if (multifd_channel_connect(p, ioc, &err)) {
+ *   - migration/multifd.c|888| <<multifd_new_send_channel_async>> if (multifd_channel_connect(p, ioc, &local_err)) {
+ */
 static bool multifd_channel_connect(MultiFDSendParams *p,
                                     QIOChannel *ioc,
                                     Error **errp)
@@ -875,6 +1033,10 @@ static void multifd_new_send_channel_cleanup(MultiFDSendParams *p,
      error_free(err);
 }
 
+/*
+ * 在以下使用multifd_new_send_channel_async():
+ *   - migration/multifd.c|899| <<multifd_new_send_channel_create>> socket_send_channel_create(multifd_new_send_channel_async, opaque);
+ */
 static void multifd_new_send_channel_async(QIOTask *task, gpointer opaque)
 {
     MultiFDSendParams *p = opaque;
@@ -885,6 +1047,9 @@ static void multifd_new_send_channel_async(QIOTask *task, gpointer opaque)
     if (!qio_task_propagate_error(task, &local_err)) {
         qio_channel_set_delay(ioc, false);
         p->running = true;
+	/*
+	 * 创建multifd_send_thread()
+	 */
         if (multifd_channel_connect(p, ioc, &local_err)) {
             return;
         }
@@ -894,11 +1059,38 @@ static void multifd_new_send_channel_async(QIOTask *task, gpointer opaque)
     multifd_new_send_channel_cleanup(p, ioc, local_err);
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|948| <<multifd_save_setup>> multifd_new_send_channel_create(p);
+ */
 static void multifd_new_send_channel_create(gpointer opaque)
 {
+    /*
+     * 创建multifd_send_thread()
+     */
     socket_send_channel_create(multifd_new_send_channel_async, opaque);
 }
 
+/*
+ * (gdb) bt
+ * #0  multifd_save_setup (errp=0x7fffffffd9f8) at ../migration/multifd.c:903
+ * #1  0x0000555555b8b7cc in migrate_fd_connect (s=0x5555572488a0, error_in=0x0) at ../migration/migration.c:3633
+ * #2  0x0000555555b7a9a9 in migration_channel_connect (s=0x5555572488a0, ioc=0x555557de95f0, hostname=0x55555799ce60 "127.0.0.1", error=0x0) at ../migration/channel.c:92
+ * #3  0x0000555555ba56dd in socket_outgoing_migration (task=0x555557e3dbb0, opaque=0x555557de7a10) at ../migration/socket.c:109
+ * #4  0x0000555555dfc430 in qio_task_complete (task=0x555557e3dbb0) at ../io/task.c:197
+ * #5  0x0000555555dfc0c3 in qio_task_thread_result (opaque=0x555557e3dbb0) at ../io/task.c:112
+ * #6  0x00007ffff69ebd47 in g_idle_dispatch () at /lib64/libglib-2.0.so.0
+ * #7  0x00007ffff69ef119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #8  0x0000555555feb300 in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #9  0x0000555555feb37a in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:313
+ * #10 0x0000555555feb47f in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #11 0x0000555555b5be98 in qemu_main_loop () at ../system/runstate.c:782
+ * #12 0x0000555555dd6e12 in qemu_default_main () at ../system/main.c:37
+ * #13 0x0000555555dd6e4d in main (argc=18, argv=0x7fffffffdd88) at ../system/main.c:48
+ *
+ * called by:
+ *   - migration/migration.c|3717| <<migrate_fd_connect>> if (multifd_save_setup(&local_err) != 0) {
+ */
 int multifd_save_setup(Error **errp)
 {
     int thread_count;
@@ -945,6 +1137,9 @@ int multifd_save_setup(Error **errp)
             p->write_flags = 0;
         }
 
+	/*
+	 * 创建multifd_send_thread()
+	 */
         multifd_new_send_channel_create(p);
     }
 
@@ -953,6 +1148,14 @@ int multifd_save_setup(Error **errp)
         Error *local_err = NULL;
         int ret;
 
+	/*
+	 * 在以下使用send_setup:
+	 *   - migration/multifd-zlib.c|312| <<global>> .send_setup = zlib_send_setup,
+	 *   - migration/multifd-zstd.c|303| <<global>> .send_setup = zstd_send_setup,
+	 *   - migration/multifd.c|157| <<global>> .send_setup = nocomp_send_setup,
+	 *   - migration/multifd.h|196| <<global>> int (*send_setup)(MultiFDSendParams *p, Error **errp);
+	 *   - migration/multifd.c|1047| <<multifd_save_setup>> ret = multifd_send_state->ops->send_setup(p, &local_err);
+	 */
         ret = multifd_send_state->ops->send_setup(p, &local_err);
         if (ret) {
             error_propagate(errp, local_err);
diff --git a/migration/multifd.h b/migration/multifd.h
index a835643b4..bcc4d4a4a 100644
--- a/migration/multifd.h
+++ b/migration/multifd.h
@@ -103,6 +103,15 @@ typedef struct {
     uint32_t flags;
     /* global number of generated multifd packets */
     uint64_t packet_num;
+    /*
+     * 在以下使用MultiFDSendParams->pending_job:
+     *   - migration/multifd.c|467| <<multifd_send_pages>> if (!p->pending_job) {
+     *   - migration/multifd.c|468| <<multifd_send_pages>> p->pending_job++;
+     *   - migration/multifd.c|693| <<multifd_send_sync_main>> p->pending_job++;
+     *   - migration/multifd.c|757| <<multifd_send_thread>> if (p->pending_job) {
+     *   - migration/multifd.c|823| <<multifd_send_thread>> p->pending_job--;
+     *   - migration/multifd.c|1052| <<multifd_save_setup>> p->pending_job = 0;
+     */
     /* thread has work to do */
     int pending_job;
     /* array of pages to sent.
diff --git a/migration/options.c b/migration/options.c
index 8d8ec73ad..2d855f9fa 100644
--- a/migration/options.c
+++ b/migration/options.c
@@ -79,12 +79,36 @@
 #define DEFAULT_MIGRATE_ANNOUNCE_ROUNDS    5
 #define DEFAULT_MIGRATE_ANNOUNCE_STEP    100
 
+/*
+ * called by:
+ *   - migration/options.c|184| <<global>> DEFINE_PROP_MIG_CAP("x-xbzrle", MIGRATION_CAPABILITY_XBZRLE),
+ *   - migration/options.c|185| <<global>> DEFINE_PROP_MIG_CAP("x-rdma-pin-all", MIGRATION_CAPABILITY_RDMA_PIN_ALL),
+ *   - migration/options.c|186| <<global>> DEFINE_PROP_MIG_CAP("x-auto-converge", MIGRATION_CAPABILITY_AUTO_CONVERGE),
+ *   - migration/options.c|187| <<global>> DEFINE_PROP_MIG_CAP("x-zero-blocks", MIGRATION_CAPABILITY_ZERO_BLOCKS),
+ *   - migration/options.c|188| <<global>> DEFINE_PROP_MIG_CAP("x-compress", MIGRATION_CAPABILITY_COMPRESS),
+ *   - migration/options.c|189| <<global>> DEFINE_PROP_MIG_CAP("x-events", MIGRATION_CAPABILITY_EVENTS),
+ *   - migration/options.c|190| <<global>> DEFINE_PROP_MIG_CAP("x-postcopy-ram", MIGRATION_CAPABILITY_POSTCOPY_RAM),
+ *   - migration/options.c|191| <<global>> DEFINE_PROP_MIG_CAP("x-postcopy-preempt",
+ *   - migration/options.c|193| <<global>> DEFINE_PROP_MIG_CAP("x-colo", MIGRATION_CAPABILITY_X_COLO),
+ *   - migration/options.c|194| <<global>> DEFINE_PROP_MIG_CAP("x-release-ram", MIGRATION_CAPABILITY_RELEASE_RAM),
+ *   - migration/options.c|195| <<global>> DEFINE_PROP_MIG_CAP("x-block", MIGRATION_CAPABILITY_BLOCK),
+ *   - migration/options.c|196| <<global>> DEFINE_PROP_MIG_CAP("x-return-path", MIGRATION_CAPABILITY_RETURN_PATH),
+ *   - migration/options.c|197| <<global>> DEFINE_PROP_MIG_CAP("x-multifd", MIGRATION_CAPABILITY_MULTIFD),
+ *   - migration/options.c|198| <<global>> DEFINE_PROP_MIG_CAP("x-background-snapshot",
+ *   - migration/options.c|201| <<global>> DEFINE_PROP_MIG_CAP("x-zero-copy-send",
+ *   - migration/options.c|204| <<global>> DEFINE_PROP_MIG_CAP("x-switchover-ack",
+ *   - migration/options.c|206| <<global>> DEFINE_PROP_MIG_CAP("x-dirty-limit", MIGRATION_CAPABILITY_DIRTY_LIMIT),
+ */
 #define DEFINE_PROP_MIG_CAP(name, x)             \
     DEFINE_PROP_BOOL(name, MigrationState, capabilities[x], false)
 
 #define DEFAULT_MIGRATE_VCPU_DIRTY_LIMIT_PERIOD     1000    /* milliseconds */
 #define DEFAULT_MIGRATE_VCPU_DIRTY_LIMIT            1       /* MB/s */
 
+/*
+ * 在以下使用migration_properties[]:
+ *   - migration/migration.c|3697| <<migration_class_init>> device_class_set_props(dc, migration_properties);
+ */
 Property migration_properties[] = {
     DEFINE_PROP_BOOL("store-global-state", MigrationState,
                      store_global_state, true),
@@ -218,6 +242,12 @@ bool migrate_background_snapshot(void)
 {
     MigrationState *s = migrate_get_current();
 
+    /*
+     * 在以下使用MIGRATION_CAPABILITY_BACKGROUND_SNAPSHOT:
+     *   - migration/options.c|199| <<global>> DEFINE_PROP_MIG_CAP("x-background-snapshot", MIGRATION_CAPABILITY_BACKGROUND_SNAPSHOT),
+     *   - migration/options.c|221| <<migrate_background_snapshot>> return s->capabilities[MIGRATION_CAPABILITY_BACKGROUND_SNAPSHOT];
+     *   - migration/options.c|526| <<migrate_caps_check>> if (new_caps[MIGRATION_CAPABILITY_BACKGROUND_SNAPSHOT]) {
+     */
     return s->capabilities[MIGRATION_CAPABILITY_BACKGROUND_SNAPSHOT];
 }
 
@@ -277,6 +307,31 @@ bool migrate_late_block_activate(void)
     return s->capabilities[MIGRATION_CAPABILITY_LATE_BLOCK_ACTIVATE];
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|187| <<migration_needs_multiple_sockets>> return migrate_multifd() || migrate_postcopy_preempt();
+ *   - migration/migration.c|641| <<qemu_start_incoming_migration>> if (migrate_multifd()) {
+ *   - migration/migration.c|863| <<migration_should_start_incoming>> if (migrate_multifd()) {
+ *   - migration/migration.c|890| <<migration_ioc_process_incoming>> if (migrate_multifd() && !migrate_postcopy_ram() &&
+ *   - migration/migration.c|929| <<migration_ioc_process_incoming>> if (migrate_multifd()) {
+ *   - migration/migration.c|965| <<migration_has_all_channels>> if (migrate_multifd()) {
+ *   - migration/multifd.c|567| <<multifd_save_cleanup>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|645| <<multifd_send_sync_main>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1023| <<multifd_save_setup>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1139| <<multifd_load_shutdown>> if (migrate_multifd()) {
+ *   - migration/multifd.c|1148| <<multifd_load_cleanup>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1195| <<multifd_recv_sync_main>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1293| <<multifd_load_setup>> if (multifd_recv_state || !migrate_multifd()) {
+ *   - migration/multifd.c|1339| <<multifd_recv_all_channels_created>> if (!migrate_multifd()) {
+ *   - migration/ram.c|1380| <<find_dirty_block>> if (migrate_multifd() &&
+ *   - migration/ram.c|2121| <<ram_save_target_page_legacy>> if (migrate_multifd() && !migration_in_postcopy()) {
+ *   - migration/ram.c|3118| <<ram_save_setup>> if (migrate_multifd() && !migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3272| <<ram_save_iterate>> if (migrate_multifd() && migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3382| <<ram_save_complete>> if (migrate_multifd() && !migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3905| <<ram_load_postcopy>> if (migrate_multifd() &&
+ *   - migration/ram.c|4210| <<ram_load_precopy>> if (migrate_multifd() &&
+ *   - migration/socket.c|238| <<socket_start_incoming_migration>> if (migrate_multifd()) {
+ */
 bool migrate_multifd(void)
 {
     MigrationState *s = migrate_get_current();
@@ -361,6 +416,14 @@ bool migrate_zero_blocks(void)
     return s->capabilities[MIGRATION_CAPABILITY_ZERO_BLOCKS];
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|676| <<multifd_send_sync_main>> flush_zero_copy = migrate_zero_copy_send();
+ *   - migration/multifd.c|723| <<multifd_send_thread>> bool use_zero_copy_send = migrate_zero_copy_send();
+ *   - migration/multifd.c|1067| <<multifd_save_setup>> if (migrate_zero_copy_send()) {
+ *   - migration/options.c|1274| <<migrate_params_check>> if (migrate_zero_copy_send() &&
+ *   - migration/socket.c|139| <<socket_outgoing_migration>> if (migrate_zero_copy_send() &&
+ */
 bool migrate_zero_copy_send(void)
 {
     MigrationState *s = migrate_get_current();
@@ -370,6 +433,15 @@ bool migrate_zero_copy_send(void)
 
 /* pseudo capabilities */
 
+/*
+ * called by:
+ *   - migration/ram.c|1381| <<find_dirty_block>> if (migrate_multifd() && !migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3118| <<ram_save_setup>> if (migrate_multifd() && !migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3272| <<ram_save_iterate>> if (migrate_multifd() && migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3382| <<ram_save_complete>> if (migrate_multifd() && !migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|3906| <<ram_load_postcopy>> if (migrate_multifd() && migrate_multifd_flush_after_each_section()) {
+ *   - migration/ram.c|4211| <<ram_load_precopy>> if (migrate_multifd() && migrate_multifd_flush_after_each_section()) {
+ */
 bool migrate_multifd_flush_after_each_section(void)
 {
     MigrationState *s = migrate_get_current();
diff --git a/migration/qemu-file.c b/migration/qemu-file.c
index 94231ff29..a722c32b3 100644
--- a/migration/qemu-file.c
+++ b/migration/qemu-file.c
@@ -37,6 +37,21 @@
 #define IO_BUF_SIZE 32768
 #define MAX_IOV_SIZE MIN_CONST(IOV_MAX, 64)
 
+/*
+ * 80 struct QIOChannel {
+ * 81     Object parent;
+ * 82     unsigned int features; // bitmask of QIOChannelFeatures
+ * 83     char *name;
+ * 84     AioContext *read_ctx;
+ * 85     Coroutine *read_coroutine;
+ * 86     AioContext *write_ctx;
+ * 87     Coroutine *write_coroutine;
+ * 88     bool follow_coroutine_ctx;
+ * 89 #ifdef _WIN32
+ * 90     HANDLE event; // For use with GSource on Win32
+ * 91 #endif
+ * 92 };
+ */
 struct QEMUFile {
     QIOChannel *ioc;
     bool is_writable;
@@ -97,6 +112,12 @@ int qemu_file_shutdown(QEMUFile *f)
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/qemu-file.c|119| <<qemu_file_get_return_path>> return qemu_file_new_impl(f->ioc, !f->is_writable);
+ *   - migration/qemu-file.c|124| <<qemu_file_new_output>> return qemu_file_new_impl(ioc, true);
+ *   - migration/qemu-file.c|129| <<qemu_file_new_input>> return qemu_file_new_impl(ioc, false);
+ */
 static QEMUFile *qemu_file_new_impl(QIOChannel *ioc, bool is_writable)
 {
     QEMUFile *f;
@@ -119,11 +140,35 @@ QEMUFile *qemu_file_get_return_path(QEMUFile *f)
     return qemu_file_new_impl(f->ioc, !f->is_writable);
 }
 
+/*
+ * called by:
+ *   - migration/channel.c|99| <<migration_channel_connect>> QEMUFile *f = qemu_file_new_output(ioc);
+ *   - migration/colo.c|557| <<colo_process_checkpoint>> fb = qemu_file_new_output(QIO_CHANNEL(bioc));
+ *   - migration/migration.c|2609| <<postcopy_start>> fb = qemu_file_new_output(QIO_CHANNEL(bioc));
+ *   - migration/migration.c|3816| <<bg_migration_thread>> fb = qemu_file_new_output(QIO_CHANNEL(s->bioc));
+ *   - migration/postcopy-ram.c|1636| <<postcopy_preempt_send_channel_done>> s->postcopy_qemufile_src = qemu_file_new_output(ioc);
+ *   - migration/ram-compress.c|190| <<compress_threads_save_setup>> comp_param[i].file = qemu_file_new_output(
+ *   - migration/rdma.c|4027| <<rdma_new_output>> rioc->file = qemu_file_new_output(QIO_CHANNEL(rioc));
+ *   - migration/savevm.c|139| <<qemu_fopen_bdrv>> return qemu_file_new_output(QIO_CHANNEL(qio_channel_block_new(bs)));
+ *   - migration/savevm.c|3497| <<qmp_xen_save_devices_state>> f = qemu_file_new_output(QIO_CHANNEL(ioc));
+ *   - tests/unit/test-vmstate.c|53| <<open_test_file>> f = qemu_file_new_output(ioc);
+ */
 QEMUFile *qemu_file_new_output(QIOChannel *ioc)
 {
     return qemu_file_new_impl(ioc, true);
 }
 
+/*
+ * called by:
+ *   - migration/colo.c|851| <<colo_process_incoming_thread>> fb = qemu_file_new_input(QIO_CHANNEL(bioc));
+ *   - migration/migration.c|938| <<migration_ioc_process_incoming>> f = qemu_file_new_input(ioc);
+ *   - migration/migration.c|950| <<migration_ioc_process_incoming>> f = qemu_file_new_input(ioc);
+ *   - migration/rdma.c|4016| <<rdma_new_input>> rioc->file = qemu_file_new_input(QIO_CHANNEL(rioc));
+ *   - migration/savevm.c|141| <<qemu_fopen_bdrv>> return qemu_file_new_input(QIO_CHANNEL(qio_channel_block_new(bs)));
+ *   - migration/savevm.c|2633| <<loadvm_handle_cmd_packaged>> QEMUFile *packf = qemu_file_new_input(QIO_CHANNEL(bioc));
+ *   - migration/savevm.c|3544| <<qmp_xen_load_devices_state>> f = qemu_file_new_input(QIO_CHANNEL(ioc));
+ *   - tests/unit/test-vmstate.c|55| <<open_test_file>> f = qemu_file_new_input(ioc);
+ */
 QEMUFile *qemu_file_new_input(QIOChannel *ioc)
 {
     return qemu_file_new_impl(ioc, false);
@@ -185,6 +230,13 @@ int qemu_file_get_error_obj_any(QEMUFile *f1, QEMUFile *f2, Error **errp)
 /*
  * Set the last error for stream f with optional Error*
  */
+/*
+ * called by:
+ *   - migration/qemu-file.c|215| <<qemu_file_set_error>> qemu_file_set_error_obj(f, ret, NULL);
+ *   - migration/qemu-file.c|279| <<qemu_fflush>> qemu_file_set_error_obj(f, -EIO, local_error);
+ *   - migration/qemu-file.c|339| <<qemu_fill_buffer>> qemu_file_set_error_obj(f, -EIO, local_error);
+ *   - migration/qemu-file.c|341| <<qemu_fill_buffer>> qemu_file_set_error_obj(f, len, local_error);
+ */
 void qemu_file_set_error_obj(QEMUFile *f, int ret, Error *err)
 {
     if (f->last_error == 0 && ret) {
@@ -207,6 +259,38 @@ int qemu_file_get_error(QEMUFile *f)
     return f->last_error;
 }
 
+/*
+ * called by:
+ *   - hw/vfio/common.c|158| <<vfio_set_migration_error>> qemu_file_set_error(ms->to_dst_file, err);
+ *   - hw/vfio/migration.c|571| <<vfio_save_state>> qemu_file_set_error(f, ret);
+ *   - hw/vfio/migration.c|714| <<vfio_vmstate_change_prepare>> qemu_file_set_error(migrate_get_current()->to_dst_file, ret);
+ *   - hw/vfio/migration.c|751| <<vfio_vmstate_change>> qemu_file_set_error(migrate_get_current()->to_dst_file, ret);
+ *   - migration/qemu-file.c|85| <<qemu_file_shutdown>> qemu_file_set_error(f, -EIO);
+ *   - migration/ram-compress.c|229| <<do_compress_ram_page>> qemu_file_set_error(migrate_get_current()->to_dst_file, ret);
+ *   - migration/ram-compress.c|369| <<do_data_decompress>> qemu_file_set_error(decomp_file, ret);
+ *   - migration/ram.c|3234| <<ram_save_setup>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|3240| <<ram_save_setup>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|3348| <<ram_save_iterate>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|3377| <<ram_save_iterate>> qemu_file_set_error(f, pages);
+ *   - migration/ram.c|3416| <<ram_save_iterate>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|3507| <<ram_save_complete>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|3534| <<ram_save_complete>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|4206| <<parse_ramblock>> qemu_file_set_error(f, ret);
+ *   - migration/ram.c|4380| <<ram_load_precopy>> qemu_file_set_error(f, ret);
+ *   - migration/rdma.c|3292| <<rdma_control_save_page>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1470| <<qemu_savevm_state_setup>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1503| <<qemu_savevm_state_setup>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1607| <<qemu_savevm_state_iterate>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1661| <<qemu_savevm_state_complete_postcopy>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1720| <<qemu_savevm_state_complete_precopy_iterable>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1785| <<qemu_savevm_state_complete_precopy_non_iterable>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|1804| <<qemu_savevm_state_complete_precopy_non_iterable>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|2315| <<postcopy_ram_listen_thread>> qemu_file_set_error(f, load_res);
+ *   - migration/savevm.c|3050| <<qemu_loadvm_state_setup>> qemu_file_set_error(f, ret);
+ *   - migration/savevm.c|3190| <<qemu_loadvm_state_main>> qemu_file_set_error(f, ret);
+ *   - migration/vmstate.c|196| <<vmstate_load_state>> qemu_file_set_error(f, ret);
+ *   - migration/vmstate.c|216| <<vmstate_load_state>> qemu_file_set_error(f, ret);
+ */
 /*
  * Set the last error for stream f
  */
@@ -256,6 +340,36 @@ static void qemu_iovec_release_ram(QEMUFile *f)
 }
 
 
+/*
+ * called by:
+ *   - migration/block-dirty-bitmap.c|458| <<send_bitmap_bits>> qemu_fflush(f);
+ *   - migration/block.c|147| <<blk_send>> qemu_fflush(f);
+ *   - migration/colo.c|317| <<colo_send_message>> ret = qemu_fflush(f);
+ *   - migration/colo.c|336| <<colo_send_message_value>> ret = qemu_fflush(f);
+ *   - migration/colo.c|469| <<colo_do_checkpoint_transaction>> qemu_fflush(fb);
+ *   - migration/colo.c|482| <<colo_do_checkpoint_transaction>> ret = qemu_fflush(s->to_dst_file);
+ *   - migration/migration.c|401| <<migrate_send_rp_message>> return qemu_fflush(mis->to_src_file);
+ *   - migration/migration.c|2918| <<bg_migration_completion>> qemu_fflush(s->to_dst_file);
+ *   - migration/migration.c|3868| <<bg_migration_thread>> qemu_fflush(fb);
+ *   - migration/qemu-file.c|441| <<qemu_fclose>> int ret = qemu_fflush(f);
+ *   - migration/qemu-file.c|484| <<add_to_iovec>> qemu_fflush(f);
+ *   - migration/qemu-file.c|496| <<add_buf_to_iovec>> qemu_fflush(f);
+ *   - migration/ram-compress.c|231| <<do_compress_ram_page>> qemu_fflush(f);
+ *   - migration/ram.c|325| <<ramblock_recv_bitmap_send>> int ret = qemu_fflush(file);
+ *   - migration/ram.c|1454| <<find_dirty_block>> qemu_fflush(f);
+ *   - migration/ram.c|1582| <<ram_save_release_protection>> qemu_fflush(pss->pss_channel);
+ *   - migration/ram.c|2293| <<ram_save_host_page_urgent>> qemu_fflush(pss->pss_channel);
+ *   - migration/ram.c|3266| <<ram_save_setup>> return qemu_fflush(f);
+ *   - migration/ram.c|3451| <<ram_save_iterate>> ret = qemu_fflush(f);
+ *   - migration/ram.c|3548| <<ram_save_complete>> return qemu_fflush(f);
+ *   - migration/ram.c|4590| <<postcopy_preempt_shutdown_file>> qemu_fflush(s->postcopy_qemufile_src);
+ *   - migration/rdma.c|3221| <<qemu_rdma_save_page>> qemu_fflush(f);
+ *   - migration/rdma.c|3845| <<rdma_registration_start>> return qemu_fflush(f);
+ *   - migration/rdma.c|3875| <<rdma_registration_stop>> qemu_fflush(f);
+ *   - migration/savevm.c|1142| <<qemu_savevm_command_send>> qemu_fflush(f);
+ *   - migration/savevm.c|1667| <<qemu_savevm_state_complete_postcopy>> qemu_fflush(f);
+ *   - migration/savevm.c|1895| <<qemu_savevm_state_complete_precopy>> return qemu_fflush(f);
+ */
 /**
  * Flushes QEMUFile buffer
  *
@@ -298,6 +412,12 @@ int qemu_fflush(QEMUFile *f)
  * case if the underlying file descriptor gives a short read, and that can
  * happen even on a blocking fd.
  */
+/*
+ * called by:
+ *   - migration/qemu-file.c|578| <<qemu_peek_buffer>> int received = qemu_fill_buffer(f);
+ *   - migration/qemu-file.c|678| <<qemu_peek_byte>> qemu_fill_buffer(f);
+ *   - migration/qemu-file.c|898| <<qemu_file_get_to_fd>> rc = qemu_fill_buffer(f);
+ */
 static ssize_t coroutine_mixed_fn qemu_fill_buffer(QEMUFile *f)
 {
     int len;
@@ -404,6 +524,12 @@ static int add_to_iovec(QEMUFile *f, const uint8_t *buf, size_t size,
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/qemu-file.c|561| <<qemu_put_buffer>> add_buf_to_iovec(f, l);
+ *   - migration/qemu-file.c|577| <<qemu_put_byte>> add_buf_to_iovec(f, 1);
+ *   - migration/qemu-file.c|838| <<qemu_put_compression_data>> add_buf_to_iovec(f, blen);
+ */
 static void add_buf_to_iovec(QEMUFile *f, size_t len)
 {
     if (!add_to_iovec(f, f->buf + f->buf_index, len, false)) {
@@ -414,6 +540,10 @@ static void add_buf_to_iovec(QEMUFile *f, size_t len)
     }
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1298| <<save_normal_page>> qemu_put_buffer_async(file, buf, TARGET_PAGE_SIZE, migrate_release_ram() && migration_in_postcopy());
+ */
 void qemu_put_buffer_async(QEMUFile *f, const uint8_t *buf, size_t size,
                            bool may_free)
 {
@@ -804,6 +934,10 @@ QIOChannel *qemu_file_get_ioc(QEMUFile *file)
 /*
  * Read size bytes from QEMUFile f and write them to fd.
  */
+/*
+ * called by:
+ *   - hw/vfio/migration.c|172| <<vfio_load_buffer>> ret = qemu_file_get_to_fd(f, migration->data_fd, data_size);
+ */
 int qemu_file_get_to_fd(QEMUFile *f, int fd, size_t size)
 {
     while (size) {
diff --git a/migration/ram.c b/migration/ram.c
index 8c7886ab7..e46c08d45 100644
--- a/migration/ram.c
+++ b/migration/ram.c
@@ -86,6 +86,17 @@
 #define RAM_SAVE_FLAG_ZERO     0x02
 #define RAM_SAVE_FLAG_MEM_SIZE 0x04
 #define RAM_SAVE_FLAG_PAGE     0x08
+/*
+ * 在以下使用RAM_SAVE_FLAG_EOS:
+ *   - migration/ram.c|3107| <<ram_save_setup>> qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+ *   - migration/ram.c|3264| <<ram_save_iterate>> qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+ *   - migration/ram.c|3356| <<ram_save_complete>> qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+ *   - migration/ram.c|3742| <<ram_load_postcopy>> while (!ret && !(flags & RAM_SAVE_FLAG_EOS)) {
+ *   - migration/ram.c|3874| <<ram_load_postcopy>> case RAM_SAVE_FLAG_EOS:
+ *   - migration/ram.c|4069| <<ram_load_precopy>> while (!ret && !(flags & RAM_SAVE_FLAG_EOS)) {
+ *   - migration/ram.c|4179| <<ram_load_precopy>> case RAM_SAVE_FLAG_EOS:
+ *   - migration/ram.c|4398| <<postcopy_preempt_shutdown_file>> qemu_put_be64(s->postcopy_qemufile_src, RAM_SAVE_FLAG_EOS);
+ */
 #define RAM_SAVE_FLAG_EOS      0x10
 #define RAM_SAVE_FLAG_CONTINUE 0x20
 #define RAM_SAVE_FLAG_XBZRLE   0x40
@@ -351,6 +362,15 @@ struct RAMState {
     /* How many times we have dirty too many pages */
     int dirty_rate_high_cnt;
     /* these variables are used for bitmap sync */
+    /*
+     * 在以下使用time_last_bitmap_sync:
+     *   - migration/ram.c|582| <<mig_throttle_counter_reset>> rs->time_last_bitmap_sync = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+     *   - migration/ram.c|980| <<migration_update_rates>> stat64_set(&mig_stats.dirty_pages_rate, rs->num_dirty_pages_period * 1000 / (end_time - rs->time_last_bitmap_sync));
+     *   - migration/ram.c|1101| <<migration_bitmap_sync>> if (!rs->time_last_bitmap_sync) {
+     *   - migration/ram.c|1102| <<migration_bitmap_sync>> rs->time_last_bitmap_sync = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+     *   - migration/ram.c|1137| <<migration_bitmap_sync>> if (end_time > rs->time_last_bitmap_sync + 1000) {
+     *   - migration/ram.c|1145| <<migration_bitmap_sync>> rs->time_last_bitmap_sync = end_time;
+     */
     /* last time we did a full bitmap_sync */
     int64_t time_last_bitmap_sync;
     /* bytes transferred at start_time */
@@ -437,6 +457,16 @@ uint64_t ram_bytes_remaining(void)
                        0;
 }
 
+/*
+ * called by:
+ *   - migration/ram-compress.c|526| <<update_compress_thread_counts>> ram_transferred_add(bytes_xmit);
+ *   - migration/ram.c|678| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+ *   - migration/ram.c|1153| <<save_zero_page>> ram_transferred_add(len);
+ *   - migration/ram.c|1210| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+ *   - migration/ram.c|1219| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+ *   - migration/ram.c|3228| <<ram_save_iterate>> ram_transferred_add(8);
+ *   - migration/rdma.c|2215| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+ */
 void ram_transferred_add(uint64_t bytes)
 {
     if (runstate_is_running()) {
@@ -448,6 +478,12 @@ void ram_transferred_add(uint64_t bytes)
     }
 }
 
+/*
+ * 在以下使用MigrationOps->ram_save_target_page:
+ *   - migration/ram.c|2214| <<ram_save_host_page_urgent>> if (migration_ops->ram_save_target_page(rs, pss) != 1) {
+ *   - migration/ram.c|2287| <<ram_save_host_page>> tmppages = migration_ops->ram_save_target_page(rs, pss);
+ *   - migration/ram.c|3109| <<ram_save_setup>> migration_ops->ram_save_target_page = ram_save_target_page_legacy;
+ */
 struct MigrationOps {
     int (*ram_save_target_page)(RAMState *rs, PageSearchStatus *pss);
 };
@@ -715,6 +751,11 @@ static void pss_find_next_dirty(PageSearchStatus *pss)
     pss->page = find_next_bit(bitmap, size, pss->page);
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|786| <<migration_clear_memory_region_dirty_bitmap_range>> migration_clear_memory_region_dirty_bitmap(rb, i);
+ *   - migration/ram.c|839| <<migration_bitmap_clear_dirty>> migration_clear_memory_region_dirty_bitmap(rb, page);
+ */
 static void migration_clear_memory_region_dirty_bitmap(RAMBlock *rb,
                                                        unsigned long page)
 {
@@ -894,6 +935,12 @@ bool ramblock_page_is_discarded(RAMBlock *rb, ram_addr_t start)
     return false;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1056| <<migration_bitmap_sync>> ramblock_sync_dirty_bitmap(rs, block);
+ *   - migration/ram.c|3581| <<colo_incoming_start_dirty_log>> ramblock_sync_dirty_bitmap(ram_state, block);
+ *   - migration/ram.c|3875| <<colo_flush_ram_cache>> ramblock_sync_dirty_bitmap(ram_state, block);
+ */
 /* Called with RCU critical section */
 static void ramblock_sync_dirty_bitmap(RAMState *rs, RAMBlock *rb)
 {
@@ -932,10 +979,25 @@ uint64_t ram_get_total_transferred_pages(void)
         compress_ram_pages() + xbzrle_counters.pages;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1159| <<migration_bitmap_sync>> migration_update_rates(rs, end_time);
+ */
 static void migration_update_rates(RAMState *rs, int64_t end_time)
 {
     uint64_t page_count = rs->target_page_count - rs->target_page_count_prev;
 
+    /*
+     * 在以下使用dirty_pages_rate:
+     *   - migration/migration-hmp-cmds.c|123| <<hmp_info_migrate>> if (info->ram->dirty_pages_rate) {
+     *   - migration/migration-hmp-cmds.c|125| <<hmp_info_migrate>> info->ram->dirty_pages_rate);
+     *   - migration/migration.c|1191| <<populate_ram_info>> info->ram->dirty_pages_rate = stat64_get(&mig_stats.dirty_pages_rate);
+     *   - migration/migration.c|1192| <<populate_ram_info>> stat64_get(&mig_stats.dirty_pages_rate);
+     *   - migration/migration.c|3216| <<migration_update_counters>> if (stat64_get(&mig_stats.dirty_pages_rate) && transferred > 10000) {
+     *   - migration/ram.c|973| <<migration_update_rates>> stat64_set(&mig_stats.dirty_pages_rate, rs->num_dirty_pages_period * 1000 / (end_time - rs->time_last_bitmap_sync));
+     *
+     * Number of pages dirtied per second.
+     */
     /* calculate period counters */
     stat64_set(&mig_stats.dirty_pages_rate,
                rs->num_dirty_pages_period * 1000 /
@@ -1031,23 +1093,78 @@ static void migration_trigger_throttle(RAMState *rs)
     }
 }
 
+/*
+ * ram_init_bitmaps()
+ * -> migration_bitmap_sync_precopy()
+ *    -> migration_bitmap_sync()
+ *
+ * ram_state_pending_exact()
+ * -> migration_bitmap_sync_precopy()
+ *    -> migration_bitmap_sync()
+ *
+ * ram_save_complete()
+ * -> migration_bitmap_sync_precopy()
+ *    -> migration_bitmap_sync()
+ *
+ * called by:
+ *   - migration/ram.c|1094| <<migration_bitmap_sync_precopy>> migration_bitmap_sync(rs, last_stage);
+ *   - migration/ram.c|2622| <<ram_postcopy_send_discard_bitmap>> migration_bitmap_sync(rs, false);
+ *
+ * 网上评论: migration_bitmap_sync进行脏页同步,该函数最终会调用到
+ * ioctl(KVM_GET_DIRTY_LOG)将KVM的脏页记录获取保存到ram_list.dirty_memory中
+ * (通过函数kvm_log_sync完成),然后将ram_list.dirty_memory的脏页信息复制到
+ * RAMBlock的bmap成员中.
+ */
 static void migration_bitmap_sync(RAMState *rs, bool last_stage)
 {
     RAMBlock *block;
     int64_t end_time;
 
+    /*
+     * 在以下使用MigrationAtomicStats->dirty_sync_count:
+     *   - migration/migration-hmp-cmds.c|156| <<hmp_info_migrate>> monitor_printf(mon, "dirty sync count: %" PRIu64 "\n", info->ram->dirty_sync_count);
+     *   - migration/migration.c|1158| <<populate_ram_info>> info->ram->dirty_sync_count =
+     *   - migration/migration.c|1159| <<populate_ram_info>> stat64_get(&mig_stats.dirty_sync_count);
+     *   - migration/ram.c|603| <<xbzrle_cache_zero_page>> stat64_get(&mig_stats.dirty_sync_count));
+     *   - migration/ram.c|629| <<save_xbzrle_page>> uint64_t generation = stat64_get(&mig_stats.dirty_sync_count);
+     *   - migration/ram.c|1099| <<migration_bitmap_sync>> stat64_add(&mig_stats.dirty_sync_count, 1);
+     *   - migration/ram.c|1150| <<migration_bitmap_sync>> uint64_t generation = stat64_get(&mig_stats.dirty_sync_count);
+     */
     stat64_add(&mig_stats.dirty_sync_count, 1);
 
+    /*
+     * 在以下使用time_last_bitmap_sync:
+     *   - migration/ram.c|582| <<mig_throttle_counter_reset>> rs->time_last_bitmap_sync = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+     *   - migration/ram.c|980| <<migration_update_rates>> stat64_set(&mig_stats.dirty_pages_rate, rs->num_dirty_pages_period * 1000 / (end_time - rs->time_last_bitmap_sync));
+     *   - migration/ram.c|1101| <<migration_bitmap_sync>> if (!rs->time_last_bitmap_sync) {
+     *   - migration/ram.c|1102| <<migration_bitmap_sync>> rs->time_last_bitmap_sync = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
+     *   - migration/ram.c|1137| <<migration_bitmap_sync>> if (end_time > rs->time_last_bitmap_sync + 1000) {
+     *   - migration/ram.c|1145| <<migration_bitmap_sync>> rs->time_last_bitmap_sync = end_time;
+     */
     if (!rs->time_last_bitmap_sync) {
         rs->time_last_bitmap_sync = qemu_clock_get_ms(QEMU_CLOCK_REALTIME);
     }
 
     trace_migration_bitmap_sync_start();
+    /*
+     * called by:
+     *   - migration/dirtyrate.c|110| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+     *   - migration/dirtyrate.c|622| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|1046| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+     *   - migration/ram.c|3459| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+     *   - migration/ram.c|3752| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+     */
     memory_global_dirty_log_sync(last_stage);
 
     qemu_mutex_lock(&rs->bitmap_mutex);
     WITH_RCU_READ_LOCK_GUARD() {
         RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+            /*
+	     * called by:
+	     *   - migration/ram.c|1056| <<migration_bitmap_sync>> ramblock_sync_dirty_bitmap(rs, block);
+	     *   - migration/ram.c|3581| <<colo_incoming_start_dirty_log>> ramblock_sync_dirty_bitmap(ram_state, block);
+	     *   - migration/ram.c|3875| <<colo_flush_ram_cache>> ramblock_sync_dirty_bitmap(ram_state, block);
+	     */
             ramblock_sync_dirty_bitmap(rs, block);
         }
         stat64_set(&mig_stats.dirty_bytes_last_sync, ram_bytes_remaining());
@@ -1078,6 +1195,26 @@ static void migration_bitmap_sync(RAMState *rs, bool last_stage)
     }
 }
 
+/*
+ * ram_init_bitmaps()
+ * -> migration_bitmap_sync_precopy()
+ *    -> migration_bitmap_sync()
+ *
+ * ram_state_pending_exact()
+ * -> migration_bitmap_sync_precopy()
+ *    -> migration_bitmap_sync()
+ *
+ * ram_save_complete()
+ * -> migration_bitmap_sync_precopy()
+ *    -> migration_bitmap_sync()
+ *
+ * called by:
+ *   - migration/ram.c|2816| <<ram_init_bitmaps>> migration_bitmap_sync_precopy(rs, false);
+ *   - migration/ram.c|3154| <<ram_save_complete>> migration_bitmap_sync_precopy(rs, true);
+ *   - migration/ram.c|3231| <<ram_state_pending_exact>> migration_bitmap_sync_precopy(rs, false);
+ *
+ * 这个应该是用来在标记统计脏页
+ */
 static void migration_bitmap_sync_precopy(RAMState *rs, bool last_stage)
 {
     Error *local_err = NULL;
@@ -1091,6 +1228,16 @@ static void migration_bitmap_sync_precopy(RAMState *rs, bool last_stage)
         local_err = NULL;
     }
 
+    /*
+     * called by:
+     *   - migration/ram.c|1094| <<migration_bitmap_sync_precopy>> migration_bitmap_sync(rs, last_stage);
+     *   - migration/ram.c|2622| <<ram_postcopy_send_discard_bitmap>> migration_bitmap_sync(rs, false);
+     *
+     * 网上评论: migration_bitmap_sync进行脏页同步,该函数最终会调用到
+     * ioctl(KVM_GET_DIRTY_LOG)将KVM的脏页记录获取保存到ram_list.dirty_memory中
+     * (通过函数kvm_log_sync完成),然后将ram_list.dirty_memory的脏页信息复制到
+     * RAMBlock的bmap成员中.
+     */
     migration_bitmap_sync(rs, last_stage);
 
     if (precopy_notify(PRECOPY_NOTIFY_AFTER_BITMAP_SYNC, &local_err)) {
@@ -1190,6 +1337,9 @@ static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
 {
     QEMUFile *file = pss->pss_channel;
 
+    /*
+     * save_page_header()会发布头部的信息
+     */
     ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
                                          offset | RAM_SAVE_FLAG_PAGE));
     if (async) {
@@ -1200,6 +1350,14 @@ static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
         qemu_put_buffer(file, buf, TARGET_PAGE_SIZE);
     }
     ram_transferred_add(TARGET_PAGE_SIZE);
+    /*
+     * 在以下使用MigrationAtomicStats->normal_pages:
+     *   - migration/migration.c|1155| <<populate_ram_info>> info->ram->normal = stat64_get(&mig_stats.normal_pages);
+     *   - migration/ram.c|963| <<ram_get_total_transferred_pages>> return stat64_get(&mig_stats.normal_pages) +
+     *   - migration/ram.c|1305| <<save_normal_page>> stat64_add(&mig_stats.normal_pages, 1);
+     *   - migration/ram.c|1369| <<ram_save_multifd_page>> stat64_add(&mig_stats.normal_pages, 1);
+     *   - migration/rdma.c|2204| <<qemu_rdma_write_one>> stat64_add(&mig_stats.normal_pages, sge.length / qemu_target_page_size());
+     */
     stat64_add(&mig_stats.normal_pages, 1);
     return 1;
 }
@@ -1216,6 +1374,10 @@ static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
  * @block: block that contains the page we want to send
  * @offset: offset inside the block for the page
  */
+/*
+ * called by:
+ *   - migration/ram.c|2090| <<ram_save_target_page_legacy>> return ram_save_page(rs, pss);
+ */
 static int ram_save_page(RAMState *rs, PageSearchStatus *pss)
 {
     int pages = -1;
@@ -1225,6 +1387,11 @@ static int ram_save_page(RAMState *rs, PageSearchStatus *pss)
     ram_addr_t offset = ((ram_addr_t)pss->page) << TARGET_PAGE_BITS;
     ram_addr_t current_addr = block->offset + offset;
 
+    /*
+     * RAMBlock *block:
+     * -> uint8_t *host;
+     * -> unsigned long *bmap;
+     */
     p = block->host + offset;
     trace_ram_save_page(block->idstr, (uint64_t)offset, p);
 
@@ -1250,12 +1417,29 @@ static int ram_save_page(RAMState *rs, PageSearchStatus *pss)
     return pages;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|2122| <<ram_save_target_page_legacy>> return ram_save_multifd_page(pss->pss_channel, block, offset);
+ */
 static int ram_save_multifd_page(QEMUFile *file, RAMBlock *block,
                                  ram_addr_t offset)
 {
+    /*
+     * called by:
+     *   - migration/multifd.c|511| <<multifd_queue_page>> return multifd_queue_page(f, block, offset);
+     *   - migration/ram.c|1300| <<ram_save_multifd_page>> if (multifd_queue_page(file, block, offset) < 0) {
+     */
     if (multifd_queue_page(file, block, offset) < 0) {
         return -1;
     }
+    /*
+     * 在以下使用MigrationAtomicStats->normal_pages:
+     *   - migration/migration.c|1155| <<populate_ram_info>> info->ram->normal = stat64_get(&mig_stats.normal_pages);
+     *   - migration/ram.c|963| <<ram_get_total_transferred_pages>> return stat64_get(&mig_stats.normal_pages) +
+     *   - migration/ram.c|1305| <<save_normal_page>> stat64_add(&mig_stats.normal_pages, 1);
+     *   - migration/ram.c|1369| <<ram_save_multifd_page>> stat64_add(&mig_stats.normal_pages, 1);
+     *   - migration/rdma.c|2204| <<qemu_rdma_write_one>> stat64_add(&mig_stats.normal_pages, sge.length / qemu_target_page_size());
+     */
     stat64_add(&mig_stats.normal_pages, 1);
 
     return 1;
@@ -1808,6 +1992,10 @@ void ram_write_tracking_stop(void)
  * @rs: current RAM state
  * @pss: data about the state of the current dirty page scan
  */
+/*
+ * called by:
+ *   - migration/ram.c|2347| <<ram_find_and_save_block>> if (!get_queued_page(rs, pss)) {
+ */
 static bool get_queued_page(RAMState *rs, PageSearchStatus *pss)
 {
     RAMBlock  *block;
@@ -2034,6 +2222,15 @@ static bool save_compress_page(RAMState *rs, PageSearchStatus *pss,
                                            compress_send_queued_data);
 }
 
+/*
+ * 在以下使用MigrationOps->ram_save_target_page:
+ *   - migration/ram.c|2214| <<ram_save_host_page_urgent>> if (migration_ops->ram_save_target_page(rs, pss) != 1) {
+ *   - migration/ram.c|2287| <<ram_save_host_page>> tmppages = migration_ops->ram_save_target_page(rs, pss);
+ *   - migration/ram.c|3109| <<ram_save_setup>> migration_ops->ram_save_target_page = ram_save_target_page_legacy;
+ *
+ * 在以下使用ram_save_target_page_legacy():
+ *   - migration/ram.c|3057| <<ram_save_setup>> migration_ops->ram_save_target_page = ram_save_target_page_legacy;
+ */
 /**
  * ram_save_target_page_legacy: save one target page
  *
@@ -2198,6 +2395,10 @@ out:
  * @rs: current RAM state
  * @pss: data about the page we want to send
  */
+/*
+ * called by:
+ *   - migration/ram.c|2332| <<ram_find_and_save_block>> pages = ram_save_host_page(rs, pss);
+ */
 static int ram_save_host_page(RAMState *rs, PageSearchStatus *pss)
 {
     bool page_dirty, preempt_active = postcopy_preempt_active();
@@ -2228,6 +2429,14 @@ static int ram_save_host_page(RAMState *rs, PageSearchStatus *pss)
             if (preempt_active) {
                 qemu_mutex_unlock(&rs->bitmap_mutex);
             }
+            /*
+	     * 在以下使用MigrationOps->ram_save_target_page:
+	     *   - migration/ram.c|2214| <<ram_save_host_page_urgent>> if (migration_ops->ram_save_target_page(rs, pss) != 1) {
+	     *   - migration/ram.c|2287| <<ram_save_host_page>> tmppages = migration_ops->ram_save_target_page(rs, pss);
+	     *   - migration/ram.c|3109| <<ram_save_setup>> migration_ops->ram_save_target_page = ram_save_target_page_legacy;
+	     *
+	     * ram_save_target_page_legacy()
+	     */
             tmppages = migration_ops->ram_save_target_page(rs, pss);
             if (tmppages >= 0) {
                 pages += tmppages;
@@ -2260,6 +2469,13 @@ static int ram_save_host_page(RAMState *rs, PageSearchStatus *pss)
     return (res < 0 ? res : pages);
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|3167| <<ram_save_iterate>> pages = ram_find_and_save_block(rs);
+ *   - migration/ram.c|3290| <<ram_save_complete>> pages = ram_find_and_save_block(rs);
+ *
+ * 真正的传输函数
+ */
 /**
  * ram_find_and_save_block: finds a dirty page and sends it to f
  *
@@ -2299,6 +2515,9 @@ static int ram_find_and_save_block(RAMState *rs)
 
     while (true){
         if (!get_queued_page(rs, pss)) {
+            /*
+	     * 寻找上次发送过后的新的脏页
+	     */
             /* priority queue empty, so just search for something dirty */
             int res = find_dirty_block(rs, pss);
             if (res != PAGE_DIRTY_FOUND) {
@@ -2722,6 +2941,11 @@ err_out:
     return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|3006| <<ram_init_all>> if (ram_state_init(rsp)) {
+ *   - migration/ram.c|3764| <<colo_init_ram_state>> ram_state_init(&ram_state);
+ */
 static int ram_state_init(RAMState **rsp)
 {
     *rsp = g_try_new0(RAMState, 1);
@@ -2747,6 +2971,12 @@ static int ram_state_init(RAMState **rsp)
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|2892| <<ram_init_bitmaps>> ram_list_init_bitmaps();
+ *
+ * 遍历所有的RAMBlock, 分配bmap, 把所有page的bit都设置成1
+ */
 static void ram_list_init_bitmaps(void)
 {
     MigrationState *ms = migrate_get_current();
@@ -2756,6 +2986,18 @@ static void ram_list_init_bitmaps(void)
 
     /* Skip setting bitmap if there is no RAM */
     if (ram_bytes_total()) {
+        /*
+	 * 在以下使用clear_bitmap_shift:
+	 *   - migration/options.c|124| <<global>> clear_bitmap_shift, CLEAR_BITMAP_SHIFT_DEFAULT),
+	 *   - migration/migration-hmp-cmds.c|52| <<migration_global_dump>> ms->clear_bitmap_shift);
+	 *   - migration/ram.c|2941| <<ram_list_init_bitmaps>> shift = ms->clear_bitmap_shift;
+	 *
+	 * 默认的
+	 *  1<<18=256K pages -> 1G chunk when page size is 4K.  This is the
+	 *  default value to use if no one specified.
+	 *
+	 * #define CLEAR_BITMAP_SHIFT_DEFAULT        18
+	 */
         shift = ms->clear_bitmap_shift;
         if (shift > CLEAR_BITMAP_SHIFT_MAX) {
             error_report("clear_bitmap_shift (%u) too big, using "
@@ -2767,6 +3009,9 @@ static void ram_list_init_bitmaps(void)
             shift = CLEAR_BITMAP_SHIFT_MIN;
         }
 
+        /*
+	 * 遍历&ram_list.blocks (RAMBlock)
+	 */
         RAMBLOCK_FOREACH_NOT_IGNORED(block) {
             pages = block->max_length >> TARGET_PAGE_BITS;
             /*
@@ -2779,6 +3024,9 @@ static void ram_list_init_bitmaps(void)
              * guest memory.
              */
             block->bmap = bitmap_new(pages);
+	    /*
+	     * 把0-pages全设置成1
+	     */
             bitmap_set(block->bmap, 0, pages);
             block->clear_bmap_shift = shift;
             block->clear_bmap = bitmap_new(clear_bmap_size(pages, shift));
@@ -2799,14 +3047,39 @@ static void migration_bitmap_clear_discarded_pages(RAMState *rs)
     }
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|2839| <<ram_init_all>> ram_init_bitmaps(*rsp);
+ *
+ * migration_thread()
+ * -> qemu_savevm_state_setup()
+ *    -> ram_save_setup()
+ *       -> ram_init_all()
+ *          -> ram_init_bitmaps()
+ * 遍历所有的RAMBlock, 分配bmap, 把所有page的bit都设置成1
+ * 激活dirty log的memslot
+ */
 static void ram_init_bitmaps(RAMState *rs)
 {
     qemu_mutex_lock_ramlist();
 
     WITH_RCU_READ_LOCK_GUARD() {
+        /*
+	 * 遍历所有的RAMBlock, 分配bmap, 把所有page的bit都设置成1
+	 */
         ram_list_init_bitmaps();
         /* We don't use dirty log with background snapshots */
         if (!migrate_background_snapshot()) {
+            /*
+	     * called by:
+	     *   - hw/i386/xen/xen-hvm.c|656| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
+	     *   - migration/dirtyrate.c|95| <<global_dirty_log_change>> memory_global_dirty_log_start(flag);
+	     *   - migration/dirtyrate.c|619| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_start(GLOBAL_DIRTY_DIRTY_RATE);
+	     *   - migration/ram.c|2831| <<ram_init_bitmaps>> memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
+	     *   - migration/ram.c|3607| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
+	     *
+	     * 激活dirty log!
+	     */
             memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
             migration_bitmap_sync_precopy(rs, false);
         }
@@ -2820,6 +3093,10 @@ static void ram_init_bitmaps(RAMState *rs)
     migration_bitmap_clear_discarded_pages(rs);
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|3091| <<ram_save_setup>> if (ram_init_all(rsp) != 0) {
+ */
 static int ram_init_all(RAMState **rsp)
 {
     if (ram_state_init(rsp)) {
@@ -2924,6 +3201,68 @@ void qemu_guest_free_page_hint(void *addr, size_t len)
  * granularity of these critical sections.
  */
 
+/*
+ * (gdb) bt
+ * #0  kvm_mem_flags (mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:469
+ * #1  0x0000555555dc270f in kvm_slot_update_flags (kml=0x555557242d10, mem=0x5555572646c0, mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:485
+ * #2  0x0000555555dc282b in kvm_section_update_flags (kml=0x555557242d10, section=0x7ffdcb8b75f0) at ../accel/kvm/kvm-all.c:518
+ * #3  0x0000555555dc28cc in kvm_log_start (listener=0x555557242d10, section=0x7ffdcb8b75f0, old=0, new=4) at ../accel/kvm/kvm-all.c:539
+ * #4  0x0000555555d66ba7 in address_space_update_topology_pass (as=0x555556f49b40 <address_space_memory>, old_view=0x7ffde8144c90, new_view=0x7ffdb80418f0, adding=true) at ../system/memory.c:987
+ * #5  0x0000555555d6700d in address_space_set_flatview (as=0x555556f49b40 <address_space_memory>) at ../system/memory.c:1080
+ * #6  0x0000555555d671b5 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #7  0x0000555555d6be07 in memory_global_dirty_log_start (flags=1) at ../system/memory.c:2926
+ * #8  0x0000555555d81a9a in ram_init_bitmaps (rs=0x7ffdb8001320) at ../migration/ram.c:2810
+ * #9  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #10 0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #11 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #12 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #13 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #14 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #15 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x555557264750) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d81aab in ram_init_bitmaps (rs=0x7ffdc0001410) at ../migration/ram.c:2811
+ * #8  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #9  0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #10 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #11 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #12 0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #14 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 4296 static SaveVMHandlers savevm_ram_handlers = {
+ * 4297     .save_setup = ram_save_setup,
+ * 4298     .save_live_iterate = ram_save_iterate,
+ * 4299     .save_live_complete_postcopy = ram_save_complete,
+ * 4300     .save_live_complete_precopy = ram_save_complete,
+ * 4301     .has_postcopy = ram_has_postcopy,
+ * 4302     .state_pending_exact = ram_state_pending_exact,
+ * 4303     .state_pending_estimate = ram_state_pending_estimate,
+ * 4304     .load_state = ram_load,
+ * 4305     .save_cleanup = ram_save_cleanup,
+ * 4306     .load_setup = ram_load_setup,
+ * 4307     .load_cleanup = ram_load_cleanup,
+ * 4308     .resume_prepare = ram_resume_prepare,
+ * 4309 };
+ *
+ * 在以下使用SaveVMHandlers->save_setup:
+ *   - hw/ppc/spapr.c|2489| <<global>> .save_setup = htab_save_setup,
+ *   - hw/s390x/s390-stattrib.c|335| <<global>> .save_setup = cmma_save_setup,
+ *   - hw/vfio/migration.c|674| <<global>> .save_setup = vfio_save_setup,
+ *   - migration/block-dirty-bitmap.c|1250| <<global>> .save_setup = dirty_bitmap_save_setup,
+ *   - migration/block.c|1009| <<global>> .save_setup = block_save_setup,
+ *   - migration/ram.c|4498| <<global>> .save_setup = ram_save_setup,
+ *   - migration/savevm.c|844| <<register_savevm_live>> if (ops->save_setup != NULL) {
+ *   - migration/savevm.c|1476| <<qemu_savevm_state_setup>> if (!se->ops || !se->ops->save_setup) {
+ *   - migration/savevm.c|1489| <<qemu_savevm_state_setup>> ret = se->ops->save_setup(f, se->opaque);
+ */
 /**
  * ram_save_setup: Setup RAM for migration
  *
@@ -2944,6 +3283,15 @@ static int ram_save_setup(QEMUFile *f, void *opaque)
 
     /* migration has already setup the bitmap, reuse it. */
     if (!migration_in_colo_state()) {
+        /*
+	 * migration_thread()
+	 * -> qemu_savevm_state_setup()
+	 *    -> ram_save_setup()
+	 *       -> ram_init_all()
+	 *          -> ram_init_bitmaps()
+	 * 遍历所有的RAMBlock, 分配bmap, 把所有page的bit都设置成1
+	 * 激活dirty log的memslot
+	 */
         if (ram_init_all(rsp) != 0) {
             compress_threads_save_cleanup();
             return -1;
@@ -2951,6 +3299,9 @@ static int ram_save_setup(QEMUFile *f, void *opaque)
     }
     (*rsp)->pss[RAM_CHANNEL_PRECOPY].pss_channel = f;
 
+    /*
+     * 发送每个RAMBlock的信息
+     */
     WITH_RCU_READ_LOCK_GUARD() {
         qemu_put_be64(f, ram_bytes_total_with_ignored()
                          | RAM_SAVE_FLAG_MEM_SIZE);
@@ -2985,6 +3336,13 @@ static int ram_save_setup(QEMUFile *f, void *opaque)
     migration_ops->ram_save_target_page = ram_save_target_page_legacy;
 
     qemu_mutex_unlock_iothread();
+    /*
+     * called by:
+     *   - migration/ram.c|1356| <<find_dirty_block>> int ret = multifd_send_sync_main(f);
+     *   - migration/ram.c|3060| <<ram_save_setup>> ret = multifd_send_sync_main(f);
+     *   - migration/ram.c|3221| <<ram_save_iterate>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+     *   - migration/ram.c|3311| <<ram_save_complete>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+     */
     ret = multifd_send_sync_main(f);
     qemu_mutex_lock_iothread();
     if (ret < 0) {
@@ -2999,6 +3357,43 @@ static int ram_save_setup(QEMUFile *f, void *opaque)
     return qemu_fflush(f);
 }
 
+/*
+ * (gdb) bt
+ * #0  ram_save_complete (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:3140
+ * #1  0x0000555555ba124a in qemu_savevm_state_complete_precopy_iterable (f=0x555557236b40, in_postcopy=false) at ../migration/savevm.c:1517
+ * #2  0x0000555555ba161b in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1618
+ * #3  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7ffdc9baf7c0) at ../migration/migration.c:2641
+ * #4  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #5  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #6  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #7  0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #8  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #9  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  ram_save_iterate (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:3011
+ * #1  0x0000555555ba0e9f in qemu_savevm_state_iterate (f=0x555557236b40, postcopy=false) at ../migration/savevm.c:1424
+ * #2  0x0000555555b8abbb in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3116
+ * #3  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #4  0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #5  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #6  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * 4243 static SaveVMHandlers savevm_ram_handlers = {
+ * 4244     .save_setup = ram_save_setup,
+ * 4245     .save_live_iterate = ram_save_iterate,
+ * 4246     .save_live_complete_postcopy = ram_save_complete,
+ * 4247     .save_live_complete_precopy = ram_save_complete,
+ * 4248     .has_postcopy = ram_has_postcopy,
+ * 4249     .state_pending_exact = ram_state_pending_exact,
+ * 4250     .state_pending_estimate = ram_state_pending_estimate,
+ * 4251     .load_state = ram_load,
+ * 4252     .save_cleanup = ram_save_cleanup,
+ * 4253     .load_setup = ram_load_setup,
+ * 4254     .load_cleanup = ram_load_cleanup,
+ * 4255     .resume_prepare = ram_resume_prepare,
+ * 4256 };
+ */
 /**
  * ram_save_iterate: iterative stage for migration
  *
@@ -3055,6 +3450,13 @@ static int ram_save_iterate(QEMUFile *f, void *opaque)
                     break;
                 }
 
+                /*
+		 * called by:
+		 *   - migration/ram.c|3167| <<ram_save_iterate>> pages = ram_find_and_save_block(rs);
+		 *   - migration/ram.c|3290| <<ram_save_complete>> pages = ram_find_and_save_block(rs);
+		 *
+		 * 真正的传输函数
+		 */
                 pages = ram_find_and_save_block(rs);
                 /* no more pages to sent */
                 if (pages == 0) {
@@ -3109,12 +3511,32 @@ out:
     if (ret >= 0
         && migration_is_setup_or_active(migrate_get_current()->state)) {
         if (migrate_multifd() && migrate_multifd_flush_after_each_section()) {
+            /*
+	     * called by:
+	     *   - migration/ram.c|1356| <<find_dirty_block>> int ret = multifd_send_sync_main(f);
+	     *   - migration/ram.c|3060| <<ram_save_setup>> ret = multifd_send_sync_main(f);
+	     *   - migration/ram.c|3221| <<ram_save_iterate>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+	     *   - migration/ram.c|3311| <<ram_save_complete>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+	     *
+	     * 重要的发送的函数, flush??
+	     */
             ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
             if (ret < 0) {
                 return ret;
             }
         }
 
+        /*
+	 * 在以下使用RAM_SAVE_FLAG_EOS:
+	 *   - migration/ram.c|3107| <<ram_save_setup>> qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+	 *   - migration/ram.c|3264| <<ram_save_iterate>> qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+	 *   - migration/ram.c|3356| <<ram_save_complete>> qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+	 *   - migration/ram.c|3742| <<ram_load_postcopy>> while (!ret && !(flags & RAM_SAVE_FLAG_EOS)) {
+	 *   - migration/ram.c|3874| <<ram_load_postcopy>> case RAM_SAVE_FLAG_EOS:
+	 *   - migration/ram.c|4069| <<ram_load_precopy>> while (!ret && !(flags & RAM_SAVE_FLAG_EOS)) {
+	 *   - migration/ram.c|4179| <<ram_load_precopy>> case RAM_SAVE_FLAG_EOS:
+	 *   - migration/ram.c|4398| <<postcopy_preempt_shutdown_file>> qemu_put_be64(s->postcopy_qemufile_src, RAM_SAVE_FLAG_EOS);
+	 */
         qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
         ram_transferred_add(8);
         ret = qemu_fflush(f);
@@ -3126,6 +3548,22 @@ out:
     return done;
 }
 
+/*
+ * 4353 static SaveVMHandlers savevm_ram_handlers = {
+ * 4354     .save_setup = ram_save_setup,
+ * 4355     .save_live_iterate = ram_save_iterate,
+ * 4356     .save_live_complete_postcopy = ram_save_complete,
+ * 4357     .save_live_complete_precopy = ram_save_complete,
+ * 4358     .has_postcopy = ram_has_postcopy,
+ * 4359     .state_pending_exact = ram_state_pending_exact,
+ * 4360     .state_pending_estimate = ram_state_pending_estimate,
+ * 4361     .load_state = ram_load,
+ * 4362     .save_cleanup = ram_save_cleanup,
+ * 4363     .load_setup = ram_load_setup,
+ * 4364     .load_cleanup = ram_load_cleanup,
+ * 4365     .resume_prepare = ram_resume_prepare,
+ * 4366 };
+ */
 /**
  * ram_save_complete: function called to send the remaining amount of ram
  *
@@ -3146,6 +3584,12 @@ static int ram_save_complete(QEMUFile *f, void *opaque)
 
     WITH_RCU_READ_LOCK_GUARD() {
         if (!migration_in_postcopy()) {
+            /*
+	     * called by:
+	     *   - migration/ram.c|2816| <<ram_init_bitmaps>> migration_bitmap_sync_precopy(rs, false);
+	     *   - migration/ram.c|3154| <<ram_save_complete>> migration_bitmap_sync_precopy(rs, true);
+	     *   - migration/ram.c|3231| <<ram_state_pending_exact>> migration_bitmap_sync_precopy(rs, false);
+	     */
             migration_bitmap_sync_precopy(rs, true);
         }
 
@@ -3211,6 +3655,22 @@ static void ram_state_pending_estimate(void *opaque, uint64_t *must_precopy,
     }
 }
 
+/*
+ * 4283 static SaveVMHandlers savevm_ram_handlers = {
+ * 4284     .save_setup = ram_save_setup,
+ * 4285     .save_live_iterate = ram_save_iterate,
+ * 4286     .save_live_complete_postcopy = ram_save_complete,
+ * 4287     .save_live_complete_precopy = ram_save_complete,
+ * 4288     .has_postcopy = ram_has_postcopy,
+ * 4289     .state_pending_exact = ram_state_pending_exact,
+ * 4290     .state_pending_estimate = ram_state_pending_estimate,
+ * 4291     .load_state = ram_load,
+ * 4292     .save_cleanup = ram_save_cleanup,
+ * 4293     .load_setup = ram_load_setup,
+ * 4294     .load_cleanup = ram_load_cleanup,
+ * 4295     .resume_prepare = ram_resume_prepare,
+ * 4296 };
+ */
 static void ram_state_pending_exact(void *opaque, uint64_t *must_precopy,
                                     uint64_t *can_postcopy)
 {
@@ -4221,6 +4681,10 @@ void postcopy_preempt_shutdown_file(MigrationState *s)
     qemu_fflush(s->postcopy_qemufile_src);
 }
 
+/*
+ * 在以下使用savevm_ram_handlers:
+ *   - migration/ram.c|4321| <<ram_mig_init>> register_savevm_live("ram", 0, 4, &savevm_ram_handlers, &ram_state);
+ */
 static SaveVMHandlers savevm_ram_handlers = {
     .save_setup = ram_save_setup,
     .save_live_iterate = ram_save_iterate,
diff --git a/migration/ram.h b/migration/ram.h
index 9b937a446..581f17368 100644
--- a/migration/ram.h
+++ b/migration/ram.h
@@ -35,6 +35,38 @@
 
 extern XBZRLECacheStats xbzrle_counters;
 
+/*
+ * called by:
+ *   - migration/ram.c|228| <<foreach_not_ignored_block>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|241| <<ramblock_recv_map_init>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|954| <<ram_pagesize_summary>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1113| <<migration_bitmap_sync>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1625| <<ram_write_tracking_compatible>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1724| <<ram_write_tracking_prepare>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1795| <<ram_write_tracking_start>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1823| <<ram_write_tracking_start>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|1848| <<ram_write_tracking_stop>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2487| <<ram_bytes_total>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2551| <<ram_save_cleanup>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2587| <<ram_postcopy_migrated_memory_release>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2654| <<postcopy_each_ram_send_discard>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2919| <<ram_list_init_bitmaps>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|2948| <<migration_bitmap_clear_discarded_pages>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|3031| <<ram_state_resume_prepare>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3777| <<colo_init_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3784| <<colo_init_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3805| <<colo_init_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3825| <<colo_incoming_start_dirty_log>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3843| <<colo_release_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3849| <<colo_release_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|3879| <<ram_load_cleanup>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|3885| <<ram_load_cleanup>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|4119| <<colo_flush_ram_cache>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *   - migration/ram.c|4436| <<ram_has_postcopy>> RAMBLOCK_FOREACH_NOT_IGNORED(rb) {
+ *   - migration/ram.c|4456| <<ram_dirty_bitmap_sync_all>> RAMBLOCK_FOREACH_NOT_IGNORED(block) {
+ *
+ * 遍历&ram_list.blocks (RAMBlock)
+ */
 /* Should be holding either ram_list.mutex, or the RCU lock. */
 #define RAMBLOCK_FOREACH_NOT_IGNORED(block)            \
     INTERNAL_RAMBLOCK_FOREACH(block)                   \
diff --git a/migration/savevm.c b/migration/savevm.c
index eec5503a4..d2e60b51c 100644
--- a/migration/savevm.c
+++ b/migration/savevm.c
@@ -231,6 +231,46 @@ typedef struct SaveState {
     QemuUUID uuid;
 } SaveState;
 
+/*
+ * 在以下使用savevm_state:
+ *   - migration/savevm.c|234| <<global>> static SaveState savevm_state = {
+ *   - migration/savevm.c|235| <<global>> .handlers = QTAILQ_HEAD_INITIALIZER(savevm_state.handlers),
+ *   - migration/savevm.c|676| <<calculate_new_instance_id>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|692| <<calculate_compat_instance_id>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|734| <<savevm_state_handler_insert>> se = savevm_state.handler_pri_head[i];
+ *   - migration/savevm.c|744| <<savevm_state_handler_insert>> QTAILQ_INSERT_TAIL(&savevm_state.handlers, nse, entry);
+ *   - migration/savevm.c|747| <<savevm_state_handler_insert>> if (savevm_state.handler_pri_head[priority] == NULL) {
+ *   - migration/savevm.c|748| <<savevm_state_handler_insert>> savevm_state.handler_pri_head[priority] = nse;
+ *   - migration/savevm.c|757| <<savevm_state_handler_remove>> if (se == savevm_state.handler_pri_head[priority]) {
+ *   - migration/savevm.c|760| <<savevm_state_handler_remove>> savevm_state.handler_pri_head[priority] = next;
+ *   - migration/savevm.c|762| <<savevm_state_handler_remove>> savevm_state.handler_pri_head[priority] = NULL;
+ *   - migration/savevm.c|765| <<savevm_state_handler_remove>> QTAILQ_REMOVE(&savevm_state.handlers, se, entry);
+ *   - migration/savevm.c|782| <<register_savevm_live>> se->section_id = savevm_state.global_section_id++;
+ *   - migration/savevm.c|818| <<unregister_savevm>> QTAILQ_FOREACH_SAFE(se, &savevm_state.handlers, entry, new_se) {
+ *   - migration/savevm.c|894| <<vmstate_register_with_alias_id>> se->section_id = savevm_state.global_section_id++;
+ *   - migration/savevm.c|941| <<vmstate_unregister>> QTAILQ_FOREACH_SAFE(se, &savevm_state.handlers, entry, new_se) {
+ *   - migration/savevm.c|1227| <<qemu_savevm_state_blocked>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1241| <<qemu_savevm_non_migratable_list>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1271| <<qemu_savevm_state_header>> vmstate_save_state(f, &vmstate_configuration, &savevm_state, s->vmdesc);
+ *   - migration/savevm.c|1280| <<qemu_savevm_state_guest_unplug_pending>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1295| <<qemu_savevm_state_prepare>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1325| <<qemu_savevm_state_setup>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1365| <<qemu_savevm_state_resume_prepare>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1395| <<qemu_savevm_state_iterate>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1464| <<qemu_savevm_state_complete_postcopy>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1498| <<qemu_savevm_state_complete_precopy_iterable>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1545| <<qemu_savevm_state_complete_precopy_non_iterable>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1650| <<qemu_savevm_state_pending_estimate>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1671| <<qemu_savevm_state_pending_exact>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1694| <<qemu_savevm_state_cleanup>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1773| <<qemu_save_device_state>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|1794| <<find_se>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|2655| <<qemu_loadvm_section_part_end>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|2716| <<qemu_loadvm_state_header>> ret = vmstate_load_state(f, &vmstate_configuration, &savevm_state, 0);
+ *   - migration/savevm.c|2730| <<qemu_loadvm_state_switchover_ack_needed>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|2749| <<qemu_loadvm_state_setup>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ *   - migration/savevm.c|2774| <<qemu_loadvm_state_cleanup>> QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
+ */
 static SaveState savevm_state = {
     .handlers = QTAILQ_HEAD_INITIALIZER(savevm_state.handlers),
     .handler_pri_head = { [MIG_PRI_DEFAULT ... MIG_PRI_MAX] = NULL },
@@ -710,6 +750,11 @@ static inline MigrationPriority save_state_priority(SaveStateEntry *se)
     return MIG_PRI_DEFAULT;
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|799| <<register_savevm_live>> savevm_state_handler_insert(se);
+ *   - migration/savevm.c|932| <<vmstate_register_with_alias_id>> savevm_state_handler_insert(se);
+ */
 static void savevm_state_handler_insert(SaveStateEntry *nse)
 {
     MigrationPriority priority = save_state_priority(nse);
@@ -769,6 +814,18 @@ static void savevm_state_handler_remove(SaveStateEntry *se)
    of the system, so instance_id should be removed/replaced.
    Meanwhile pass -1 as instance_id if you do not already have a clearly
    distinguishing id for all instances of your device class. */
+/*
+ * called by:
+ *   - hw/ppc/spapr.c|3092| <<spapr_machine_init>> register_savevm_live("spapr/htab", VMSTATE_INSTANCE_ID_ANY, 1, &savevm_htab_handlers, spapr);
+ *   - hw/s390x/s390-skeys.c|446| <<s390_skeys_realize>> register_savevm_live(TYPE_S390_SKEYS, 0, 1, &savevm_s390_storage_keys, ss);
+ *   - hw/s390x/s390-stattrib.c|355| <<s390_stattrib_realize>> register_savevm_live(TYPE_S390_STATTRIB, 0, 0, &savevm_s390_stattrib_handlers, dev);
+ *   - hw/s390x/tod.c|111| <<s390_tod_realize>> register_savevm_live("todclock", 0, 1, &savevm_tod, td);
+ *   - hw/vfio/migration.c|867| <<vfio_migration_init>> register_savevm_live(id, VMSTATE_INSTANCE_ID_ANY, 1, &savevm_vfio_handlers, vbasedev);
+ *   - migration/block-dirty-bitmap.c|1268| <<dirty_bitmap_mig_init>> register_savevm_live("dirty-bitmap", 0, 1, &savevm_dirty_bitmap_handlers, &dbm_state);
+ *   - migration/block.c|1025| <<blk_mig_init>> register_savevm_live("block", 0, 1, &savevm_block_handlers, &block_mig_state);
+ *   - migration/ram.c|4321| <<ram_mig_init>> register_savevm_live("ram", 0, 4, &savevm_ram_handlers, &ram_state);
+ *   - net/slirp.c|663| <<net_slirp_init>> register_savevm_live("slirp", VMSTATE_INSTANCE_ID_ANY, slirp_state_version(), &savevm_slirp_state, s->slirp);
+ */
 int register_savevm_live(const char *idstr,
                          uint32_t instance_id,
                          int version_id,
@@ -878,6 +935,14 @@ int vmstate_replace_hack_for_ppc(VMStateIf *obj, int instance_id,
     return vmstate_register(obj, instance_id, vmsd, opaque);
 }
 
+/*
+ * called by:
+ *   - hw/core/qdev.c|533| <<device_set_realized>> if (vmstate_register_with_alias_id(VMSTATE_IF(dev), VMSTATE_INSTANCE_ID_ANY,
+ *                                                 qdev_get_vmsd(dev), dev, dev->instance_id_alias, dev->alias_required_for_version, &local_err) < 0) {
+ *   - hw/intc/apic_common.c|316| <<apic_common_realize>> vmstate_register_with_alias_id(NULL, instance_id, &vmstate_apic_common, s, -1, 0, NULL);
+ *   - include/migration/vmstate.h|1229| <<vmstate_register>> return vmstate_register_with_alias_id(obj, instance_id, vmsd, opaque, -1, 0, NULL);
+ *   - include/migration/vmstate.h|1257| <<vmstate_register_any>> return vmstate_register_with_alias_id(obj, VMSTATE_INSTANCE_ID_ANY, vmsd, opaque, -1, 0, NULL);
+ */
 int vmstate_register_with_alias_id(VMStateIf *obj, uint32_t instance_id,
                                    const VMStateDescription *vmsd,
                                    void *opaque, int alias_id,
@@ -975,6 +1040,13 @@ static void vmstate_save_old_style(QEMUFile *f, SaveStateEntry *se,
     }
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|1097| <<vmstate_save>> save_section_header(f, se, QEMU_VM_SECTION_FULL);
+ *   - migration/savevm.c|1484| <<qemu_savevm_state_setup>> save_section_header(f, se, QEMU_VM_SECTION_START);
+ *   - migration/savevm.c|1586| <<qemu_savevm_state_iterate>> save_section_header(f, se, QEMU_VM_SECTION_PART);
+ *   - migration/savevm.c|1701| <<qemu_savevm_state_complete_precopy_iterable>> save_section_header(f, se, QEMU_VM_SECTION_END);
+ */
 /*
  * Write the header for device section (QEMU_VM_SECTION START/END/PART/FULL)
  */
@@ -1000,6 +1072,14 @@ static void save_section_header(QEMUFile *f, SaveStateEntry *se,
  * Write a footer onto device sections that catches cases misformatted device
  * sections.
  */
+/*
+ * called by:
+ *   - migration/savevm.c|1124| <<vmstate_save>> save_section_footer(f, se);
+ *   - migration/savevm.c|1508| <<qemu_savevm_state_setup>> save_section_footer(f, se);
+ *   - migration/savevm.c|1608| <<qemu_savevm_state_iterate>> save_section_footer(f, se);
+ *   - migration/savevm.c|1666| <<qemu_savevm_state_complete_postcopy>> save_section_footer(f, se);
+ *   - migration/savevm.c|1725| <<qemu_savevm_state_complete_precopy_iterable>> save_section_footer(f, se);
+ */
 static void save_section_footer(QEMUFile *f, SaveStateEntry *se)
 {
     if (migrate_get_current()->send_section_footer) {
@@ -1008,6 +1088,12 @@ static void save_section_footer(QEMUFile *f, SaveStateEntry *se)
     }
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|1392| <<qemu_savevm_state_setup>> ret = vmstate_save(f, se, ms->vmdesc);
+ *   - migration/savevm.c|1618| <<qemu_savevm_state_complete_precopy_non_iterable>> ret = vmstate_save(f, se, vmdesc);
+ *   - migration/savevm.c|1844| <<qemu_save_device_state>> ret = vmstate_save(f, se, NULL);
+ */
 static int vmstate_save(QEMUFile *f, SaveStateEntry *se, JSONWriter *vmdesc)
 {
     int ret;
@@ -1220,6 +1306,14 @@ void qemu_savevm_send_recv_bitmap(QEMUFile *f, char *block_name)
     qemu_savevm_command_send(f, MIG_CMD_RECV_BITMAP, len + 1, (uint8_t *)buf);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1849| <<migration_is_blocked>> if (qemu_savevm_state_blocked(errp)) {
+ *   - migration/savevm.c|3089| <<qemu_loadvm_state>> if (qemu_savevm_state_blocked(&local_err)) {
+ *
+ * 遍历savevm_state.handlers
+ * 检验是否有不能迁移的设备,也就是se->vmsd->unmigratable
+ */
 bool qemu_savevm_state_blocked(Error **errp)
 {
     SaveStateEntry *se;
@@ -1247,6 +1341,12 @@ void qemu_savevm_non_migratable_list(strList **reasons)
     }
 }
 
+/*
+ * called by:
+ *   - gration/migration.c|3652| <<migration_thread>> qemu_savevm_state_header(s->to_dst_file);
+ *   - migration/migration.c|3803| <<bg_migration_thread>> qemu_savevm_state_header(s->to_dst_file);
+ *   - migration/savevm.c|1983| <<qemu_savevm_state>> qemu_savevm_state_header(f);
+ */
 void qemu_savevm_state_header(QEMUFile *f)
 {
     MigrationState *s = migrate_get_current();
@@ -1254,6 +1354,13 @@ void qemu_savevm_state_header(QEMUFile *f)
     s->vmdesc = json_writer_new(false);
 
     trace_savevm_state_header();
+    /*
+     * 在以下使用QEMU_VM_FILE_MAGIC:
+     *   - migration/migration.c|910| <<migration_ioc_process_incoming>> default_channel = (channel_magic == cpu_to_be32(QEMU_VM_FILE_MAGIC));
+     *   - migration/savevm.c|1336| <<qemu_savevm_state_header>> qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
+     *   - migration/savevm.c|2033| <<qemu_save_device_state>> qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
+     *   - migration/savevm.c|2960| <<qemu_loadvm_state_header>> if (v != QEMU_VM_FILE_MAGIC) {
+     */
     qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
     qemu_put_be32(f, QEMU_VM_FILE_VERSION);
 
@@ -1311,6 +1418,49 @@ int qemu_savevm_state_prepare(Error **errp)
     return 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  kvm_mem_flags (mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:469
+ * #1  0x0000555555dc270f in kvm_slot_update_flags (kml=0x555557242d10, mem=0x5555572646c0, mr=0x5555572bb1a0) at ../accel/kvm/kvm-all.c:485
+ * #2  0x0000555555dc282b in kvm_section_update_flags (kml=0x555557242d10, section=0x7ffdcb8b75f0) at ../accel/kvm/kvm-all.c:518
+ * #3  0x0000555555dc28cc in kvm_log_start (listener=0x555557242d10, section=0x7ffdcb8b75f0, old=0, new=4) at ../accel/kvm/kvm-all.c:539
+ * #4  0x0000555555d66ba7 in address_space_update_topology_pass (as=0x555556f49b40 <address_space_memory>, old_view=0x7ffde8144c90, new_view=0x7ffdb80418f0, adding=true) at ../system/memory.c:987
+ * #5  0x0000555555d6700d in address_space_set_flatview (as=0x555556f49b40 <address_space_memory>) at ../system/memory.c:1080
+ * #6  0x0000555555d671b5 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #7  0x0000555555d6be07 in memory_global_dirty_log_start (flags=1) at ../system/memory.c:2926
+ * #8  0x0000555555d81a9a in ram_init_bitmaps (rs=0x7ffdb8001320) at ../migration/ram.c:2810
+ * #9  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #10 0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #11 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #12 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #13 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #14 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #15 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  kvm_slot_get_dirty_log (s=0x555557242c60, slot=0x555557264750) at ../accel/kvm/kvm-all.c:613
+ * #1  0x0000555555dc30a8 in kvm_physical_sync_dirty_bitmap (kml=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:857
+ * #2  0x0000555555dc4aaf in kvm_log_sync (listener=0x555557242d10, section=0x7ffdcdbe8630) at ../accel/kvm/kvm-all.c:1592
+ * #3  0x0000555555d69e1c in memory_region_sync_dirty_bitmap (mr=0x0, last_stage=false) at ../system/memory.c:2279
+ * #4  0x0000555555d6bcc4 in memory_global_dirty_log_sync (last_stage=false) at ../system/memory.c:2885
+ * #5  0x0000555555d7e763 in migration_bitmap_sync (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1046
+ * #6  0x0000555555d7e995 in migration_bitmap_sync_precopy (rs=0x7ffdc0001410, last_stage=false) at ../migration/ram.c:1094
+ * #7  0x0000555555d81aab in ram_init_bitmaps (rs=0x7ffdc0001410) at ../migration/ram.c:2811
+ * #8  0x0000555555d81b4a in ram_init_all (rsp=0x555556f49e60 <ram_state>) at ../migration/ram.c:2834
+ * #9  0x0000555555d81e76 in ram_save_setup (f=0x555557236b40, opaque=0x555556f49e60 <ram_state>) at ../migration/ram.c:2947
+ * #10 0x0000555555ba0bb0 in qemu_savevm_state_setup (f=0x555557236b40) at ../migration/savevm.c:1345
+ * #11 0x0000555555b8b130 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3340
+ * #12 0x0000555555fd1784 in qemu_thread_start (args=0x555557a4d6b0) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #14 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - migration/migration.c|3375| <<migration_thread>> qemu_savevm_state_setup(s->to_dst_file);
+ *   - migration/migration.c|3489| <<bg_migration_thread>> qemu_savevm_state_setup(s->to_dst_file);
+ *   - migration/savevm.c|1809| <<qemu_savevm_state>> qemu_savevm_state_setup(f);
+ *
+ * 核心是遍历&savevm_state.handlers调用各种save_setup, 比如ram_save_setup()
+ */
 void qemu_savevm_state_setup(QEMUFile *f)
 {
     MigrationState *ms = migrate_get_current();
@@ -1324,6 +1474,12 @@ void qemu_savevm_state_setup(QEMUFile *f)
     trace_savevm_state_setup();
     QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
         if (se->vmsd && se->vmsd->early_setup) {
+            /*
+	     * called by:
+	     *   - migration/savevm.c|1392| <<qemu_savevm_state_setup>> ret = vmstate_save(f, se, ms->vmdesc);
+	     *   - migration/savevm.c|1618| <<qemu_savevm_state_complete_precopy_non_iterable>> ret = vmstate_save(f, se, vmdesc);
+	     *   - migration/savevm.c|1844| <<qemu_save_device_state>> ret = vmstate_save(f, se, NULL);
+	     */
             ret = vmstate_save(f, se, ms->vmdesc);
             if (ret) {
                 qemu_file_set_error(f, ret);
@@ -1342,6 +1498,20 @@ void qemu_savevm_state_setup(QEMUFile *f)
         }
         save_section_header(f, se, QEMU_VM_SECTION_START);
 
+	/*
+	 * 在以下使用SaveVMHandlers->save_setup:
+	 *   - hw/ppc/spapr.c|2489| <<global>> .save_setup = htab_save_setup,
+	 *   - hw/s390x/s390-stattrib.c|335| <<global>> .save_setup = cmma_save_setup,
+	 *   - hw/vfio/migration.c|674| <<global>> .save_setup = vfio_save_setup,
+	 *   - migration/block-dirty-bitmap.c|1250| <<global>> .save_setup = dirty_bitmap_save_setup,
+	 *   - migration/block.c|1009| <<global>> .save_setup = block_save_setup,
+	 *   - migration/ram.c|4498| <<global>> .save_setup = ram_save_setup,
+	 *   - migration/savevm.c|844| <<register_savevm_live>> if (ops->save_setup != NULL) {
+	 *   - migration/savevm.c|1476| <<qemu_savevm_state_setup>> if (!se->ops || !se->ops->save_setup) {
+	 *   - migration/savevm.c|1489| <<qemu_savevm_state_setup>> ret = se->ops->save_setup(f, se->opaque);
+	 *
+	 * 比如ram_save_setup()
+	 */
         ret = se->ops->save_setup(f, se->opaque);
         save_section_footer(f, se);
         if (ret < 0) {
@@ -1380,6 +1550,12 @@ int qemu_savevm_state_resume_prepare(MigrationState *s)
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3147| <<migration_iteration_run>> qemu_savevm_state_iterate(s->to_dst_file, in_postcopy);
+ *   - migration/migration.c|3233| <<bg_migration_iteration_run>> res = qemu_savevm_state_iterate(s->to_dst_file, false);
+ *   - migration/savevm.c|1823| <<qemu_savevm_state>> if (qemu_savevm_state_iterate(f, false) > 0) {
+ */
 /*
  * this function has three return values:
  *   negative: there was one error, and we have -errno.
@@ -1392,6 +1568,9 @@ int qemu_savevm_state_iterate(QEMUFile *f, bool postcopy)
     int ret = 1;
 
     trace_savevm_state_iterate();
+    /*
+     * 所有的handler!!!
+     */
     QTAILQ_FOREACH(se, &savevm_state.handlers, entry) {
         if (!se->ops || !se->ops->save_live_iterate) {
             continue;
@@ -1421,6 +1600,17 @@ int qemu_savevm_state_iterate(QEMUFile *f, bool postcopy)
 
         save_section_header(f, se, QEMU_VM_SECTION_PART);
 
+	/*
+	 * 在以下使用SaveVMHandlers->save_live_iterate:
+	 *   - hw/ppc/spapr.c|2490| <<global>> .save_live_iterate = htab_save_iterate,
+	 *   - hw/s390x/s390-stattrib.c|336| <<global>> .save_live_iterate = cmma_save_iterate,
+	 *   - hw/vfio/migration.c|679| <<global>> .save_live_iterate = vfio_save_iterate,
+	 *   - migration/block-dirty-bitmap.c|1256| <<global>> .save_live_iterate = dirty_bitmap_save_iterate,
+	 *   - migration/block.c|1010| <<global>> .save_live_iterate = block_save_iterate,
+	 *   - migration/ram.c|4437| <<global>> .save_live_iterate = ram_save_iterate,
+	 *   - migration/savevm.c|1532| <<qemu_savevm_state_iterate>> if (!se->ops || !se->ops->save_live_iterate) {
+	 *   - migration/savevm.c|1560| <<qemu_savevm_state_iterate>> ret = se->ops->save_live_iterate(f, se->opaque);
+	 */
         ret = se->ops->save_live_iterate(f, se->opaque);
         trace_savevm_section_end(se->idstr, se->section_id, ret);
         save_section_footer(f, se);
@@ -1449,6 +1639,10 @@ static bool should_send_vmdesc(void)
     return !machine->suppress_vmdesc && !in_postcopy;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2674| <<migration_completion_postcopy>> qemu_savevm_state_complete_postcopy(s->to_dst_file);
+ */
 /*
  * Calls the save_live_complete_postcopy methods
  * causing the last few pages to be sent immediately and doing any associated
@@ -1488,6 +1682,13 @@ void qemu_savevm_state_complete_postcopy(QEMUFile *f)
     qemu_fflush(f);
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|1761| <<qemu_savevm_state_complete_precopy>> ret = qemu_savevm_state_complete_precopy_iterable(f, in_postcopy);
+ *
+ * 遍历&savevm_state.handlers, 调用se->ops->save_live_complete_precopy
+ * 剩余可以一次性迁移的最后一部分数据
+ */
 static
 int qemu_savevm_state_complete_precopy_iterable(QEMUFile *f, bool in_postcopy)
 {
@@ -1514,6 +1715,19 @@ int qemu_savevm_state_complete_precopy_iterable(QEMUFile *f, bool in_postcopy)
 
         save_section_header(f, se, QEMU_VM_SECTION_END);
 
+        /*
+	 * 在以下使用SaveVMHandlers->save_live_complete_precopy:
+	 *   - hw/ppc/spapr.c|2491| <<global>> .save_live_complete_precopy = htab_save_complete,
+	 *   - hw/s390x/s390-stattrib.c|337| <<global>> .save_live_complete_precopy = cmma_save_complete,
+	 *   - hw/vfio/migration.c|680| <<global>> .save_live_complete_precopy = vfio_save_complete_precopy,
+	 *   - migration/block-dirty-bitmap.c|1252| <<global>> .save_live_complete_precopy = dirty_bitmap_save_complete,
+	 *   - migration/block.c|1011| <<global>> .save_live_complete_precopy = block_save_complete,
+	 *   - migration/ram.c|4457| <<global>> .save_live_complete_precopy = ram_save_complete,
+	 *   - migration/savevm.c|1655| <<qemu_savevm_state_complete_precopy_iterable>> !se->ops->save_live_complete_precopy) {
+	 *   - migration/savevm.c|1670| <<qemu_savevm_state_complete_precopy_iterable>> ret = se->ops->save_live_complete_precopy(f, se->opaque);
+	 *
+	 * 剩余可以一次性迁移的最后一部分数据
+	 */
         ret = se->ops->save_live_complete_precopy(f, se->opaque);
         trace_savevm_section_end(se->idstr, se->section_id, ret);
         save_section_footer(f, se);
@@ -1531,6 +1745,31 @@ int qemu_savevm_state_complete_precopy_iterable(QEMUFile *f, bool in_postcopy)
     return 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_save (vdev=0x555557fd7ef0, f=0x555557236b40) at ../hw/virtio/virtio.c:2791
+ * #1  0x0000555555d3853b in virtio_device_put (f=0x555557236b40, opaque=0x555557fd7ef0, size=0, field=0x555556e6ad20 <__compound_literal.7>, vmdesc=0x7ffdc8000cc0) at ../hw/virtio/virtio.c:2854
+ * #2  0x00005555560616c6 in vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7ef0, vmdesc=0x7ffdc8000cc0, version_id=11, errp=0x7fffeebee6a8)
+ *                                    at ../migration/vmstate.c:408
+ * #3  0x0000555556061356 in vmstate_save_state_with_err (f=0x555557236b40, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7ef0, vmdesc_id=0x7ffdc8000cc0, errp=0x7fffeebee6a8)
+ *                                    at ../migration/vmstate.c:347
+ * #4  0x0000555555ba0060 in vmstate_save (f=0x555557236b40, se=0x5555580feb30, vmdesc=0x7ffdc8000cc0) at ../migration/savevm.c:1037
+ * #5  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x555557236b40, in_postcopy=false, inactivate_disks=true) at ../migration/savevm.c:1553
+ * #6  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1628
+ * #7  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7fffeebee7c0) at ../migration/migration.c:2641
+ * #8  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #9  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #10 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #11 0x0000555555fd1784 in qemu_thread_start (args=0x555557234bc0) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - migration/migration.c|3519| <<bg_migration_thread>> if (qemu_savevm_state_complete_precopy_non_iterable(fb, false, false)) {
+ *   - migration/savevm.c|1709| <<qemu_savevm_state_complete_precopy>> ret = qemu_savevm_state_complete_precopy_non_iterable(f, in_postcopy, inactivate_disks);
+ *
+ * 遍历&savevm_state.handlers, 调用vmstate_save()
+ */
 int qemu_savevm_state_complete_precopy_non_iterable(QEMUFile *f,
                                                     bool in_postcopy,
                                                     bool inactivate_disks)
@@ -1550,6 +1789,12 @@ int qemu_savevm_state_complete_precopy_non_iterable(QEMUFile *f,
 
         start_ts_each = qemu_clock_get_us(QEMU_CLOCK_REALTIME);
 
+	/*
+	 * called by:
+	 *   - migration/savevm.c|1392| <<qemu_savevm_state_setup>> ret = vmstate_save(f, se, ms->vmdesc);
+	 *   - migration/savevm.c|1618| <<qemu_savevm_state_complete_precopy_non_iterable>> ret = vmstate_save(f, se, vmdesc);
+	 *   - migration/savevm.c|1844| <<qemu_save_device_state>> ret = vmstate_save(f, se, NULL);
+	 */
         ret = vmstate_save(f, se, vmdesc);
         if (ret) {
             qemu_file_set_error(f, ret);
@@ -1599,6 +1844,32 @@ int qemu_savevm_state_complete_precopy_non_iterable(QEMUFile *f,
     return 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_save (vdev=0x555557fd7ef0, f=0x555557236b40) at ../hw/virtio/virtio.c:2791
+ * #1  0x0000555555d3853b in virtio_device_put (f=0x555557236b40, opaque=0x555557fd7ef0, size=0, field=0x555556e6ad20 <__compound_literal.7>, vmdesc=0x7ffdc8000cc0) at ../hw/virtio/virtio.c:2854
+ * #2  0x00005555560616c6 in vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7ef0, vmdesc=0x7ffdc8000cc0, version_id=11, errp=0x7fffeebee6a8)
+ *                                    at ../migration/vmstate.c:408
+ * #3  0x0000555556061356 in vmstate_save_state_with_err (f=0x555557236b40, vmsd=0x555556dd1a20 <vmstate_virtio_net>, opaque=0x555557fd7ef0, vmdesc_id=0x7ffdc8000cc0, errp=0x7fffeebee6a8)
+ *                                    at ../migration/vmstate.c:347
+ * #4  0x0000555555ba0060 in vmstate_save (f=0x555557236b40, se=0x5555580feb30, vmdesc=0x7ffdc8000cc0) at ../migration/savevm.c:1037
+ * #5  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x555557236b40, in_postcopy=false, inactivate_disks=true) at ../migration/savevm.c:1553
+ * #6  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1628
+ * #7  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7fffeebee7c0) at ../migration/migration.c:2641
+ * #8  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #9  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #10 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #11 0x0000555555fd1784 in qemu_thread_start (args=0x555557234bc0) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - migration/migration.c|2451| <<postcopy_start>> qemu_savevm_state_complete_precopy(ms->to_dst_file, true, false);
+ *   - migration/migration.c|2496| <<postcopy_start>> qemu_savevm_state_complete_precopy(fb, false, false);
+ *   - migration/migration.c|2650| <<migration_completion_precopy>> ret = qemu_savevm_state_complete_precopy(s->to_dst_file, false, s->block_inactive);
+ *   - migration/savevm.c|1807| <<qemu_savevm_state>> qemu_savevm_state_complete_precopy(f, false, false);
+ *   - migration/savevm.c|1832| <<qemu_savevm_live_state>> qemu_savevm_state_complete_precopy(f, true, false);
+ */
 int qemu_savevm_state_complete_precopy(QEMUFile *f, bool iterable_only,
                                        bool inactivate_disks)
 {
@@ -1615,6 +1886,10 @@ int qemu_savevm_state_complete_precopy(QEMUFile *f, bool iterable_only,
     cpu_synchronize_all_states();
 
     if (!in_postcopy || iterable_only) {
+        /*
+	 * 遍历&savevm_state.handlers, 调用se->ops->save_live_complete_precopy
+	 * 剩余可以一次性迁移的最后一部分数据
+	 */
         ret = qemu_savevm_state_complete_precopy_iterable(f, in_postcopy);
         if (ret) {
             return ret;
@@ -1660,6 +1935,14 @@ void qemu_savevm_state_pending_estimate(uint64_t *must_precopy,
     }
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3258| <<migration_iteration_run>> qemu_savevm_state_pending_exact(&must_precopy, &can_postcopy);
+ *
+ * 遍历&savevm_state.handlers调用.state_pending_exact
+ * 比如ram是ram_state_pending_exact, vfio是vfio_state_pending_exact
+ * 把结果的和通过两个参数返回
+ */
 void qemu_savevm_state_pending_exact(uint64_t *must_precopy,
                                      uint64_t *can_postcopy)
 {
@@ -1677,6 +1960,18 @@ void qemu_savevm_state_pending_exact(uint64_t *must_precopy,
                 continue;
             }
         }
+	/*
+	 * 在以下使用SaveVMHandlers->state_pending_exact:
+	 *   - hw/s390x/s390-stattrib.c|338| <<global>> .state_pending_exact = cmma_state_pending,
+	 *   - hw/vfio/migration.c|677| <<global>> .state_pending_exact = vfio_state_pending_exact,
+	 *   - migration/block-dirty-bitmap.c|1254| <<global>> .state_pending_exact = dirty_bitmap_state_pending,
+	 *   - migration/block.c|1012| <<global>> .state_pending_exact = block_state_pending,
+	 *   - migration/ram.c|4375| <<global>> .state_pending_exact = ram_state_pending_exact,
+	 *   - migration/savevm.c|1827| <<qemu_savevm_state_pending_exact>> if (!se->ops || !se->ops->state_pending_exact) {
+	 *   - migration/savevm.c|1835| <<qemu_savevm_state_pending_exact>> se->ops->state_pending_exact(se->opaque, must_precopy, can_postcopy);
+	 *
+	 *  This calculate the exact remaining data to transfer
+	 */
         se->ops->state_pending_exact(se->opaque, must_precopy, can_postcopy);
     }
 }
@@ -1698,6 +1993,10 @@ void qemu_savevm_state_cleanup(void)
     }
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|3209| <<save_snapshot>> ret = qemu_savevm_state(f, errp);
+ */
 static int qemu_savevm_state(QEMUFile *f, Error **errp)
 {
     int ret;
@@ -1753,6 +2052,10 @@ static int qemu_savevm_state(QEMUFile *f, Error **errp)
     return ret;
 }
 
+/*
+ * called by:
+ *   - migration/colo.c|467| <<colo_do_checkpoint_transaction>> qemu_savevm_live_state(s->to_dst_file);
+ */
 void qemu_savevm_live_state(QEMUFile *f)
 {
     /* save QEMU_VM_SECTION_END section */
@@ -1760,6 +2063,11 @@ void qemu_savevm_live_state(QEMUFile *f)
     qemu_put_byte(f, QEMU_VM_EOF);
 }
 
+/*
+ * called by:
+ *   - migration/colo.c|452| <<colo_do_checkpoint_transaction>> ret = qemu_save_device_state(fb);
+ *   - migration/savevm.c|3263| <<qmp_xen_save_devices_state>> ret = qemu_save_device_state(f);
+ */
 int qemu_save_device_state(QEMUFile *f)
 {
     SaveStateEntry *se;
@@ -2924,12 +3232,24 @@ out:
     return ret;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|680| <<process_incoming_migration_co>> ret = qemu_loadvm_state(mis->from_src_file);
+ *   - migration/savevm.c|3402| <<qmp_xen_load_devices_state>> ret = qemu_loadvm_state(f);
+ *   - migration/savevm.c|3480| <<load_snapshot>> ret = qemu_loadvm_state(f);
+ *
+ * 主要的接收函数
+ */
 int qemu_loadvm_state(QEMUFile *f)
 {
     MigrationIncomingState *mis = migration_incoming_get_current();
     Error *local_err = NULL;
     int ret;
 
+    /*
+     * 遍历savevm_state.handlers
+     * 检验是否有不能迁移的设备,也就是se->vmsd->unmigratable
+     */
     if (qemu_savevm_state_blocked(&local_err)) {
         error_report_err(local_err);
         return -EINVAL;
@@ -3039,6 +3359,24 @@ int qemu_loadvm_approve_switchover(void)
     return migrate_send_rp_switchover_ack(mis);
 }
 
+/*
+ * 注释:
+ * save_snapshot: Save an internal snapshot.
+ * @name: name of internal snapshot
+ * @overwrite: replace existing snapshot with @name
+ * @vmstate: blockdev node name to store VM state in
+ * @has_devices: whether to use explicit device list
+ * @devices: explicit device list to snapshot
+ * @errp: pointer to error object
+ * On success, return %true.
+ * On failure, store an error through @errp and return %false.
+ *
+ * called by:
+ *   - migration/migration-hmp-cmds.c|418| <<hmp_savevm>> save_snapshot(qdict_get_try_str(qdict, "name"),
+ *   - migration/savevm.c|3731| <<snapshot_save_job_bh>> s->ret = save_snapshot(s->tag, false, s->vmstate, true, s->devices, s->errp);
+ *   - replay/replay-debugging.c|329| <<replay_gdb_attached>> if (!save_snapshot("start_debugging", true, NULL, false, NULL, NULL)) {
+ *   - replay/replay-snapshot.c|78| <<replay_vmstate_init>> if (!save_snapshot(replay_snapshot, true, NULL, false, NULL, &err)) {
+ */
 bool save_snapshot(const char *name, bool overwrite, const char *vmstate,
                   bool has_devices, strList *devices, Error **errp)
 {
diff --git a/migration/savevm.h b/migration/savevm.h
index 74669733d..5d361e519 100644
--- a/migration/savevm.h
+++ b/migration/savevm.h
@@ -14,6 +14,13 @@
 #ifndef MIGRATION_SAVEVM_H
 #define MIGRATION_SAVEVM_H
 
+/*
+ * 在以下使用QEMU_VM_FILE_MAGIC:
+ *   - migration/migration.c|910| <<migration_ioc_process_incoming>> default_channel = (channel_magic == cpu_to_be32(QEMU_VM_FILE_MAGIC));
+ *   - migration/savevm.c|1336| <<qemu_savevm_state_header>> qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
+ *   - migration/savevm.c|2033| <<qemu_save_device_state>> qemu_put_be32(f, QEMU_VM_FILE_MAGIC);
+ *   - migration/savevm.c|2960| <<qemu_loadvm_state_header>> if (v != QEMU_VM_FILE_MAGIC) {
+ */
 #define QEMU_VM_FILE_MAGIC           0x5145564d
 #define QEMU_VM_FILE_VERSION_COMPAT  0x00000002
 #define QEMU_VM_FILE_VERSION         0x00000003
diff --git a/migration/socket.c b/migration/socket.c
index 98e3ea151..441c26b4e 100644
--- a/migration/socket.c
+++ b/migration/socket.c
@@ -31,10 +31,26 @@
 #include "qapi/clone-visitor.h"
 #include "qapi/qapi-visit-sockets.h"
 
+/*
+ * 在以下使用outgoing_args (SocketOutgoingArgs类型):
+ *   - migration/socket.c|46| <<socket_send_channel_create>> qio_channel_socket_connect_async(sioc, outgoing_args.saddr,
+ *   - migration/socket.c|57| <<socket_send_channel_create_sync>> if (!outgoing_args.saddr) {
+ *   - migration/socket.c|63| <<socket_send_channel_create_sync>> if (qio_channel_socket_connect_sync(sioc, outgoing_args.saddr, errp) < 0) {
+ *   - migration/socket.c|75| <<socket_send_channel_destroy>> if (outgoing_args.saddr) {
+ *   - migration/socket.c|76| <<socket_send_channel_destroy>> qapi_free_SocketAddress(outgoing_args.saddr);
+ *   - migration/socket.c|77| <<socket_send_channel_destroy>> outgoing_args.saddr = NULL;
+ *   - migration/socket.c|140| <<socket_start_outgoing_migration>> qapi_free_SocketAddress(outgoing_args.saddr);
+ *   - migration/socket.c|141| <<socket_start_outgoing_migration>> outgoing_args.saddr = addr;
+ */
 struct SocketOutgoingArgs {
     SocketAddress *saddr;
 } outgoing_args;
 
+/*
+ * called by:
+ *   - migration/multifd.c|899| <<multifd_new_send_channel_create>> socket_send_channel_create(multifd_new_send_channel_async, opaque);
+ *   - migration/postcopy-ram.c|1721| <<postcopy_preempt_setup>> socket_send_channel_create(postcopy_preempt_send_channel_new, s);
+ */
 void socket_send_channel_create(QIOTaskFunc f, void *data)
 {
     QIOChannelSocket *sioc = qio_channel_socket_new();
@@ -42,6 +58,9 @@ void socket_send_channel_create(QIOTaskFunc f, void *data)
                                      f, data, NULL, NULL);
 }
 
+/*
+ * 没有调用???
+ */
 QIOChannel *socket_send_channel_create_sync(Error **errp)
 {
     QIOChannelSocket *sioc = qio_channel_socket_new();
@@ -86,6 +105,23 @@ static void socket_connect_data_free(void *opaque)
     g_free(data);
 }
 
+/*
+ * (gdb) bt
+ * #0  socket_outgoing_migration (task=0x555557e3dbb0, opaque=0x555557fce740) at ../migration/socket.c:91
+ * #1  0x0000555555dfc430 in qio_task_complete (task=0x555557e3dbb0) at ../io/task.c:197
+ * #2  0x0000555555dfc0c3 in qio_task_thread_result (opaque=0x555557e3dbb0) at ../io/task.c:112
+ * #3  0x00007ffff69ebd47 in g_idle_dispatch () at /lib64/libglib-2.0.so.0
+ * #4  0x00007ffff69ef119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #5  0x0000555555feb300 in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #6  0x0000555555feb37a in os_host_main_loop_wait (timeout=199616523) at ../util/main-loop.c:313
+ * #7  0x0000555555feb47f in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #8  0x0000555555b5be98 in qemu_main_loop () at ../system/runstate.c:782
+ * #9  0x0000555555dd6e12 in qemu_default_main () at ../system/main.c:37
+ * #10 0x0000555555dd6e4d in main (argc=18, argv=0x7fffffffdd88) at ../system/main.c:48
+ *
+ * 在以下使用socket_outgoing_migration:
+ *   - migration/socket.c|134| <<socket_start_outgoing_migration>> qio_channel_socket_connect_async(sioc, saddr, socket_outgoing_migration, data, socket_connect_data_free, NULL);
+ */
 static void socket_outgoing_migration(QIOTask *task,
                                       gpointer opaque)
 {
@@ -106,10 +142,22 @@ static void socket_outgoing_migration(QIOTask *task,
     }
 
 out:
+    /*
+     * called by:
+     *   - migration/exec.c|92| <<exec_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+     *   - migration/fd.c|42| <<fd_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+     *   - migration/file.c|60| <<file_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+     *   - migration/socket.c|121| <<socket_outgoing_migration>> migration_channel_connect(data->s, sioc, data->hostname, err);
+     *   - migration/tls.c|113| <<migration_tls_outgoing_handshake>> migration_channel_connect(s, ioc, NULL, err);
+     */
     migration_channel_connect(data->s, sioc, data->hostname, err);
     object_unref(OBJECT(sioc));
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1989| <<qmp_migrate>> socket_start_outgoing_migration(s, saddr, &local_err);
+ */
 void socket_start_outgoing_migration(MigrationState *s,
                                      SocketAddress *saddr,
                                      Error **errp)
@@ -120,6 +168,17 @@ void socket_start_outgoing_migration(MigrationState *s,
 
     data->s = s;
 
+    /*
+     * 在以下使用outgoing_args (SocketOutgoingArgs类型):
+     *   - migration/socket.c|46| <<socket_send_channel_create>> qio_channel_socket_connect_async(sioc, outgoing_args.saddr,
+     *   - migration/socket.c|57| <<socket_send_channel_create_sync>> if (!outgoing_args.saddr) {
+     *   - migration/socket.c|63| <<socket_send_channel_create_sync>> if (qio_channel_socket_connect_sync(sioc, outgoing_args.saddr, errp) < 0) {
+     *   - migration/socket.c|75| <<socket_send_channel_destroy>> if (outgoing_args.saddr) {
+     *   - migration/socket.c|76| <<socket_send_channel_destroy>> qapi_free_SocketAddress(outgoing_args.saddr);
+     *   - migration/socket.c|77| <<socket_send_channel_destroy>> outgoing_args.saddr = NULL;
+     *   - migration/socket.c|140| <<socket_start_outgoing_migration>> qapi_free_SocketAddress(outgoing_args.saddr);
+     *   - migration/socket.c|141| <<socket_start_outgoing_migration>> outgoing_args.saddr = addr;
+     */
     /* in case previous migration leaked it */
     qapi_free_SocketAddress(outgoing_args.saddr);
     outgoing_args.saddr = addr;
@@ -162,6 +221,10 @@ socket_incoming_migration_end(void *opaque)
     object_unref(OBJECT(listener));
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|563| <<qemu_start_incoming_migration>> socket_start_incoming_migration(saddr, errp);
+ */
 void socket_start_incoming_migration(SocketAddress *saddr,
                                      Error **errp)
 {
diff --git a/migration/vmstate.c b/migration/vmstate.c
index b7723a418..1cddf354c 100644
--- a/migration/vmstate.c
+++ b/migration/vmstate.c
@@ -100,6 +100,37 @@ static void vmstate_handle_alloc(void *ptr, const VMStateField *field,
     }
 }
 
+/*
+ * called by:
+ *   - hw/display/virtio-gpu.c|1330| <<virtio_gpu_load>> vmstate_load_state(f, &vmstate_virtio_gpu_scanouts, g, 1);
+ *   - hw/pci/pci.c|770| <<pci_device_load>> ret = vmstate_load_state(f, &vmstate_pci_device, s, s->version_id);
+ *   - hw/s390x/virtio-ccw.c|1138| <<virtio_ccw_load_config>> return vmstate_load_state(f, &vmstate_virtio_ccw_dev, dev, 1);
+ *   - hw/scsi/spapr_vscsi.c|651| <<vscsi_load_request>> rc = vmstate_load_state(f, &vmstate_spapr_vscsi_req, req, 1);
+ *   - hw/vfio/pci.c|2742| <<vfio_pci_load_config>> ret = vmstate_load_state(f, &vmstate_vfio_pci_config, vdev, 1);
+ *   - hw/virtio/virtio-mmio.c|615| <<virtio_mmio_load_extra_state>> return vmstate_load_state(f, &vmstate_virtio_mmio, proxy, 1);
+ *   - hw/virtio/virtio-pci.c|165| <<virtio_pci_load_extra_state>> return vmstate_load_state(f, &vmstate_virtio_pci, proxy, 1);
+ *   - hw/virtio/virtio.c|3068| <<virtio_load>> ret = vmstate_load_state(f, vdc->vmsd, vdev, version_id);
+ *   - hw/virtio/virtio.c|3075| <<virtio_load>> ret = vmstate_load_state(f, &vmstate_virtio, vdev, 1);
+ *   - migration/savevm.c|1021| <<vmstate_load>> return vmstate_load_state(f, se->vmsd, se->opaque, se->load_version_id);
+ *   - migration/savevm.c|2781| <<qemu_loadvm_state_header>> ret = vmstate_load_state(f, &vmstate_configuration, &savevm_state, 0);
+ *   - migration/vmstate-types.c|528| <<get_tmp>> ret = vmstate_load_state(f, vmsd, tmp, version_id);
+ *   - migration/vmstate-types.c|628| <<get_qtailq>> ret = vmstate_load_state(f, vmsd, elm, version_id);
+ *   - migration/vmstate-types.c|782| <<get_gtree>> ret = vmstate_load_state(f, key_vmsd, key, version_id);
+ *   - migration/vmstate-types.c|790| <<get_gtree>> ret = vmstate_load_state(f, val_vmsd, val, version_id);
+ *   - migration/vmstate-types.c|871| <<get_qlist>> ret = vmstate_load_state(f, vmsd, elm, version_id);
+ *   - migration/vmstate.c|153| <<vmstate_load_state>> ret = vmstate_load_state(f, field->vmsd, curr_elem,
+ *   - migration/vmstate.c|156| <<vmstate_load_state>> ret = vmstate_load_state(f, field->vmsd, curr_elem,
+ *   - migration/vmstate.c|506| <<vmstate_subsection_load>> ret = vmstate_load_state(f, sub_vmsd, opaque, version_id);
+ *   - tests/unit/test-vmstate.c|118| <<load_vmstate_one>> ret = vmstate_load_state(f, desc, obj, version);
+ *   - tests/unit/test-vmstate.c|369| <<test_load_v1>> vmstate_load_state(loading, &vmstate_versioned, &obj, 1);
+ *   - tests/unit/test-vmstate.c|395| <<test_load_v2>> vmstate_load_state(loading, &vmstate_versioned, &obj, 2);
+ *   - tests/unit/test-vmstate.c|484| <<test_load_noskip>> vmstate_load_state(loading, &vmstate_skipping, &obj, 2);
+ *   - tests/unit/test-vmstate.c|508| <<test_load_skip>> vmstate_load_state(loading, &vmstate_skipping, &obj, 2);
+ *   - tests/unit/test-vmstate.c|777| <<test_load_q>> vmstate_load_state(fload, &vmstate_q, &tgt, 1);
+ *   - tests/unit/test-vmstate.c|1131| <<test_gtree_load_domain>> vmstate_load_state(fload, &vmstate_domain, dest_domain, 1);
+ *   - tests/unit/test-vmstate.c|1245| <<test_gtree_load_iommu>> vmstate_load_state(fload, &vmstate_iommu, dest_iommu, 1);
+ *   - tests/unit/test-vmstate.c|1380| <<test_load_qlist>> vmstate_load_state(fload, &vmstate_container, dest_container, 1);
+ */
 int vmstate_load_state(QEMUFile *f, const VMStateDescription *vmsd,
                        void *opaque, int version_id)
 {
@@ -174,6 +205,9 @@ int vmstate_load_state(QEMUFile *f, const VMStateDescription *vmsd,
                          vmsd->name, field->name);
             return -1;
         }
+	/*
+	 * 这里增加!!!
+	 */
         field++;
     }
     assert(field->flags == VMS_END);
@@ -335,18 +369,115 @@ bool vmstate_section_needed(const VMStateDescription *vmsd, void *opaque)
 }
 
 
+/*
+ * called by:
+ *   - hw/display/virtio-gpu.c|1225| <<virtio_gpu_save>> return vmstate_save_state(f, &vmstate_virtio_gpu_scanouts, g, NULL);
+ *   - hw/pci/pci.c|762| <<pci_device_save>> vmstate_save_state(f, &vmstate_pci_device, s, NULL);
+ *   - hw/s390x/virtio-ccw.c|1132| <<virtio_ccw_save_config>> vmstate_save_state(f, &vmstate_virtio_ccw_dev, dev, NULL);
+ *   - hw/scsi/spapr_vscsi.c|633| <<vscsi_save_request>> vmstate_save_state(f, &vmstate_spapr_vscsi_req, req, NULL);
+ *   - hw/vfio/pci.c|2728| <<vfio_pci_save_config>> vmstate_save_state(f, &vmstate_vfio_pci_config, vdev, NULL);
+ *   - hw/virtio/virtio-mmio.c|608| <<virtio_mmio_save_extra_state>> vmstate_save_state(f, &vmstate_virtio_mmio, proxy, NULL);
+ *   - hw/virtio/virtio-pci.c|158| <<virtio_pci_save_extra_state>> vmstate_save_state(f, &vmstate_virtio_pci, proxy, NULL);
+ *   - hw/virtio/virtio.c|2892| <<virtio_save>> int ret = vmstate_save_state(f, vdc->vmsd, vdev, NULL);
+ *   - hw/virtio/virtio.c|2899| <<virtio_save>> return vmstate_save_state(f, &vmstate_virtio, vdev, NULL);
+ *   - migration/savevm.c|1344| <<qemu_savevm_state_header>> vmstate_save_state(f, &vmstate_configuration, &savevm_state, s->vmdesc);
+ *   - migration/vmstate-types.c|542| <<put_tmp>> ret = vmstate_save_state(f, vmsd, tmp, vmdesc);
+ *   - migration/vmstate-types.c|653| <<put_qtailq>> ret = vmstate_save_state(f, vmsd, elm, vmdesc);
+ *   - migration/vmstate-types.c|690| <<put_gtree_elem>> ret = vmstate_save_state(f, capsule->key_vmsd, key, capsule->vmdesc);
+ *   - migration/vmstate-types.c|698| <<put_gtree_elem>> ret = vmstate_save_state(f, capsule->val_vmsd, value, capsule->vmdesc);
+ *   - migration/vmstate-types.c|834| <<put_qlist>> ret = vmstate_save_state(f, vmsd, elm, vmdesc);
+ *   - migration/vmstate.c|432| <<vmstate_save_state_v>> ret = vmstate_save_state(f, field->vmsd, curr_elem,
+ *   - migration/vmstate.c|577| <<vmstate_subsection_save>> ret = vmstate_save_state(f, vmsdsub, opaque, vmdesc);
+ *   - tests/unit/test-vmstate.c|72| <<save_vmstate>> int ret = vmstate_save_state(f, desc, obj, NULL);
+ *   - tests/unit/test-vmstate.c|432| <<test_save_noskip>> int ret = vmstate_save_state(fsave, &vmstate_skipping, &obj, NULL);
+ *   - tests/unit/test-vmstate.c|454| <<test_save_skip>> int ret = vmstate_save_state(fsave, &vmstate_skipping, &obj, NULL);
+ */
 int vmstate_save_state(QEMUFile *f, const VMStateDescription *vmsd,
                        void *opaque, JSONWriter *vmdesc_id)
 {
     return vmstate_save_state_v(f, vmsd, opaque, vmdesc_id, vmsd->version_id, NULL);
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|1110| <<vmstate_save>> ret = vmstate_save_state_with_err(f, se->vmsd, se->opaque, vmdesc, &local_err);
+ */
 int vmstate_save_state_with_err(QEMUFile *f, const VMStateDescription *vmsd,
                        void *opaque, JSONWriter *vmdesc_id, Error **errp)
 {
     return vmstate_save_state_v(f, vmsd, opaque, vmdesc_id, vmsd->version_id, errp);
 }
 
+/*
+ * 一些例子.
+ *
+ * (gdb) bt
+ * #0  vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556dbab00 <vmstate_configuration>, opaque=0x555556e3bc00 <savevm_state>, vmdesc=0x7ffdb8000ab0, version_id=1, errp=0x0)
+ *     at ../migration/vmstate.c:353
+ * #1  0x000055555606130f in vmstate_save_state (f=0x555557236b40, vmsd=0x555556dbab00 <vmstate_configuration>, opaque=0x555556e3bc00 <savevm_state>, vmdesc_id=0x7ffdb8000ab0)
+ *     at ../migration/vmstate.c:341
+ * #2  0x0000555555ba08c6 in qemu_savevm_state_header (f=0x555557236b40) at ../migration/savevm.c:1271
+ * #3  0x0000555555b8b095 in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3310
+ * #4  0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #5  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #6  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556db87a0 <vmstate_timers>, opaque=0x555556f41be0 <timers_state>, vmdesc=0x7ffdb8000ab0, version_id=2, errp=0x7ffdcbcbb6a8)
+ *     at ../migration/vmstate.c:353
+ * #1  0x0000555556061356 in vmstate_save_state_with_err
+ *     (f=0x555557236b40, vmsd=0x555556db87a0 <vmstate_timers>, opaque=0x555556f41be0 <timers_state>, vmdesc_id=0x7ffdb8000ab0, errp=0x7ffdcbcbb6a8) at ../migration/vmstate.c:347
+ * #2  0x0000555555ba0060 in vmstate_save (f=0x555557236b40, se=0x555556fe5880, vmdesc=0x7ffdb8000ab0) at ../migration/savevm.c:1037
+ * #3  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x555557236b40, in_postcopy=false, inactivate_disks=true) at ../migration/savevm.c:1553
+ * #4  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1628
+ * #5  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7ffdcbcbb7c0) at ../migration/migration.c:2641
+ * #6  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #7  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #8  0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #9  0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #10 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #11 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vmstate_save_state_v (f=0x7ffdcbcbb5e0, vmsd=0x7ffff7ff3980 <vmstate_slirp>, opaque=0x55555727adf0 "\004", version_id=4) at ../subprojects/slirp/src/vmstate.c:300
+ * #1  0x00007ffff7feb661 in slirp_vmstate_save_state (f=0x7ffdcbcbb5e0, vmsd=0x7ffff7ff3980 <vmstate_slirp>, opaque=0x55555727adf0) at ../subprojects/slirp/src/vmstate.c:364
+ * #2  0x00007ffff7fe08d4 in slirp_state_save (slirp=0x55555727adf0, write_cb=0x555555bc8969 <net_slirp_stream_write>, opaque=0x555557236b40) at ../subprojects/slirp/src/state.c:332
+ * #3  0x0000555555bc8a24 in net_slirp_state_save (f=0x555557236b40, opaque=0x55555727adf0) at ../net/slirp.c:411
+ * #4  0x0000555555b9fca4 in vmstate_save_old_style (f=0x555557236b40, se=0x555556fe5a70, vmdesc=0x7ffdb8000ab0) at ../migration/savevm.c:963
+ * #5  0x0000555555ba0031 in vmstate_save (f=0x555557236b40, se=0x555556fe5a70, vmdesc=0x7ffdb8000ab0) at ../migration/savevm.c:1035
+ * #6  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x555557236b40, in_postcopy=false, inactivate_disks=true) at ../migration/savevm.c:1553
+ * #7  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1628
+ * #8  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7ffdcbcbb7c0) at ../migration/migration.c:2641
+ * #9  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #10 0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #11 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #12 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #14 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556dbcda0 <vmstate_fpreg>, opaque=0x5555572be6c0, vmdesc=0x7ffdb8000ab0, version_id=0, errp=0x0) at ../migration/vmstate.c:353
+ * #1  0x000055555606130f in vmstate_save_state (f=0x555557236b40, vmsd=0x555556dbcda0 <vmstate_fpreg>, opaque=0x5555572be6c0, vmdesc_id=0x7ffdb8000ab0) at ../migration/vmstate.c:341
+ * #2  0x0000555556061654 in vmstate_save_state_v (f=0x555557236b40, vmsd=0x555556dbe060 <vmstate_x86_cpu>, opaque=0x5555572bbc90, vmdesc=0x7ffdb8000ab0, version_id=12, errp=0x7ffdcbcbb6a8)
+ *     at ../migration/vmstate.c:401
+ * #3  0x0000555556061356 in vmstate_save_state_with_err (f=0x555557236b40, vmsd=0x555556dbe060 <vmstate_x86_cpu>, opaque=0x5555572bbc90, vmdesc_id=0x7ffdb8000ab0, errp=0x7ffdcbcbb6a8)
+ *     at ../migration/vmstate.c:347
+ * #4  0x0000555555ba0060 in vmstate_save (f=0x555557236b40, se=0x5555572c5060, vmdesc=0x7ffdb8000ab0) at ../migration/savevm.c:1037
+ * #5  0x0000555555ba13a6 in qemu_savevm_state_complete_precopy_non_iterable (f=0x555557236b40, in_postcopy=false, inactivate_disks=true) at ../migration/savevm.c:1553
+ * #6  0x0000555555ba1645 in qemu_savevm_state_complete_precopy (f=0x555557236b40, iterable_only=false, inactivate_disks=true) at ../migration/savevm.c:1628
+ * #7  0x0000555555b89e0a in migration_completion_precopy (s=0x5555572488a0, current_active_state=0x7ffdcbcbb7c0) at ../migration/migration.c:2641
+ * #8  0x0000555555b89f9f in migration_completion (s=0x5555572488a0) at ../migration/migration.c:2704
+ * #9  0x0000555555b8ab1c in migration_iteration_run (s=0x5555572488a0) at ../migration/migration.c:3101
+ * #10 0x0000555555b8b19a in migration_thread (opaque=0x5555572488a0) at ../migration/migration.c:3352
+ * #11 0x0000555555fd1784 in qemu_thread_start (args=0x555557243410) at ../util/qemu-thread-posix.c:541
+ * #12 0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - migration/vmstate.c|372| <<vmstate_save_state>> return vmstate_save_state_v(f, vmsd, opaque, vmdesc_id, vmsd->version_id, NULL);
+ *   - migration/vmstate.c|378| <<vmstate_save_state_with_err>> return vmstate_save_state_v(f, vmsd, opaque, vmdesc_id, vmsd->version_id, errp);
+ *   - migration/vmstate.c|435| <<vmstate_save_state_v>> ret = vmstate_save_state_v(f, field->vmsd, curr_elem, vmdesc_loop, field->struct_version_id, errp);
+ */
 int vmstate_save_state_v(QEMUFile *f, const VMStateDescription *vmsd,
                          void *opaque, JSONWriter *vmdesc, int version_id, Error **errp)
 {
@@ -432,6 +563,9 @@ int vmstate_save_state_v(QEMUFile *f, const VMStateDescription *vmsd,
                 assert(!(field->flags & VMS_MUST_EXIST));
             }
         }
+        /*
+	 * 这里增加!!
+	 */
         field++;
     }
     assert(field->flags == VMS_END);
@@ -514,6 +648,10 @@ static int vmstate_subsection_load(QEMUFile *f, const VMStateDescription *vmsd,
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/vmstate.c|577| <<vmstate_save_state_v>> ret = vmstate_subsection_save(f, vmsd, opaque, vmdesc);
+ */
 static int vmstate_subsection_save(QEMUFile *f, const VMStateDescription *vmsd,
                                    void *opaque, JSONWriter *vmdesc)
 {
diff --git a/monitor/hmp.c b/monitor/hmp.c
index 69c1b7e98..a846bc4a3 100644
--- a/monitor/hmp.c
+++ b/monitor/hmp.c
@@ -1173,6 +1173,27 @@ void handle_hmp_command(MonitorHMP *mon, const char *cmdline)
     qobject_unref(qdict);
 }
 
+/*
+ * (gdb) bt
+ * #0  cmd_completion (mon=0x555556846620, name=0x5555574f6f60 "", list=0x55555609d9a2 "balloon") at ../monitor/hmp.c:1174
+ * #1  0x0000555555acd505 in monitor_find_completion_by_table (mon=0x555556846620, cmd_table=0x5555566f7740 <hmp_info_cmds>, args=0x7fffffffc9e8, nb_args=1) at ../monitor/hmp.c:1284
+ * #2  0x0000555555acd5a5 in monitor_find_completion_by_table (mon=0x555556846620, cmd_table=0x5555566f8aa0 <hmp_cmds>, args=0x7fffffffc9e0, nb_args=2) at ../monitor/hmp.c:1301
+ * #3  0x0000555555acd90d in monitor_find_completion (opaque=0x555556846620, cmdline=0x55555718d000 "info ") at ../monitor/hmp.c:1380
+ * #4  0x0000555555f1906b in readline_completion (rs=0x555556ac0f10) at ../util/readline.c:307
+ * #5  0x0000555555f19505 in readline_handle_byte (rs=0x555556ac0f10, ch=9) at ../util/readline.c:395
+ * #6  0x0000555555acd9a8 in monitor_read (opaque=0x555556846620, buf=0x7fffffffcbf0 "\t\322\377\377\377\177", size=1) at ../monitor/hmp.c:1393
+ * #7  0x0000555555e31613 in qemu_chr_be_write_impl (s=0x555556a66ab0, buf=0x7fffffffcbf0 "\t\322\377\377\377\177", len=1) at ../chardev/char.c:201
+ * #8  0x0000555555e31677 in qemu_chr_be_write (s=0x555556a66ab0, buf=0x7fffffffcbf0 "\t\322\377\377\377\177", len=1) at ../chardev/char.c:213
+ * #9  0x0000555555e340a4 in fd_chr_read (chan=0x555556a66bc0, cond=G_IO_IN, opaque=0x555556a66ab0) at ../chardev/char-fd.c:72
+ * #10 0x0000555555d335b4 in qio_channel_fd_source_dispatch (source=0x5555578075c0, callback=0x555555e33f7a <fd_chr_read>, user_data=0x555556a66ab0) at ../io/channel-watch.c:84
+ * #11 0x00007ffff6c40119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #12 0x0000555555f12010 in glib_pollfds_poll () at ../util/main-loop.c:297
+ * #13 0x0000555555f1208a in os_host_main_loop_wait (timeout=1000000000) at ../util/main-loop.c:320
+ * #14 0x0000555555f1218f in main_loop_wait (nonblocking=0) at ../util/main-loop.c:596
+ * #15 0x0000555555a7a05e in qemu_main_loop () at ../softmmu/runstate.c:734
+ * #16 0x000055555581fe62 in qemu_main (argc=20, argv=0x7fffffffded8, envp=0x0) at ../softmmu/main.c:38
+ * #17 0x000055555581fe94 in main (argc=20, argv=0x7fffffffded8) at ../softmmu/main.c:47
+ */
 static void cmd_completion(MonitorHMP *mon, const char *name, const char *list)
 {
     const char *p, *pstart;
diff --git a/monitor/qmp.c b/monitor/qmp.c
index 6eee450fe..31817a002 100644
--- a/monitor/qmp.c
+++ b/monitor/qmp.c
@@ -378,6 +378,21 @@ void qmp_dispatcher_co_wake(void)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  handle_qmp_command (opaque=0x5555567fb9a0, req=0x7fffe0003740, err=0x0) at ../monitor/qmp.c:346
+ * #1  0x0000555555d27063 in json_message_process_token (lexer=lexer@entry=0x5555567fba60, input=0x555556777720, type=<optimized out>, x=<optimized out>, y=<optimized out>) at ../qobject/json-streamer.c:99
+ * #2  0x0000555555d57fbf in json_lexer_feed_char (lexer=lexer@entry=0x5555567fba60, ch=125 '}', flush=flush@entry=false) at ../qobject/json-lexer.c:313
+ * #3  0x0000555555d58119 in json_lexer_feed (lexer=0x5555567fba60, buffer=<optimized out>, size=<optimized out>) at ../qobject/json-lexer.c:350
+ * #4  0x0000555555c9a07e in tcp_chr_read (chan=<optimized out>, cond=<optimized out>, opaque=<optimized out>) at ../chardev/char-socket.c:508
+ * #5  0x00007ffff6c40119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #6  0x00007ffff6c40478 in g_main_context_iterate.isra () at /lib64/libglib-2.0.so.0
+ * #7  0x00007ffff6c4074a in g_main_loop_run () at /lib64/libglib-2.0.so.0
+ * #8  0x0000555555bfb9c9 in iothread_run (opaque=opaque@entry=0x5555567fbb00) at ../iothread.c:74
+ * #9  0x0000555555d2fd09 in qemu_thread_start (args=0x7fffee5ef950) at ../util/qemu-thread-posix.c:504
+ * #10 0x00007ffff53d0ea5 in start_thread () at /lib64/libpthread.so.0
+ * #11 0x00007ffff50f99fd in clone () at /lib64/libc.so.6
+ */
 static void handle_qmp_command(void *opaque, QObject *req, Error *err)
 {
     MonitorQMP *mon = opaque;
diff --git a/qapi/qmp-dispatch.c b/qapi/qmp-dispatch.c
index 555528b6b..95b2774d2 100644
--- a/qapi/qmp-dispatch.c
+++ b/qapi/qmp-dispatch.c
@@ -130,6 +130,35 @@ static void do_qmp_dispatch_bh(void *opaque)
     aio_co_wake(data->co);
 }
 
+/*
+ * (gdb) bt
+ * #0  qmp_dispatch (cmds=0x5555565525a0 <qmp_cap_negotiation_commands>, request=0x7fffe0003740, allow_oob=false, cur_mon=0x5555567fb9a0)
+ *                   at ../qapi/qmp-dispatch.c:139
+ * #1  0x0000555555ca2414 in monitor_qmp_dispatch (mon=0x5555567fb9a0, req=<optimized out>) at ../monitor/qmp.c:144
+ * #2  0x0000555555ca2df2 in monitor_qmp_dispatcher_co (data=<optimized out>) at ../monitor/qmp.c:318
+ * #3  0x0000555555d41c2b in coroutine_trampoline (i0=<optimized out>, i1=<optimized out>) at ../util/coroutine-ucontext.c:177
+ * #4  0x00007ffff5043190 in __start_context () at /lib64/libc.so.6
+ * #5  0x00007fffffffd1b0 in  ()
+ * #6  0x0000000000000000 in  ()
+ *
+ * 在json的message换成oob的样子.
+ *
+ * (gdb) bt
+ * #0  qmp_dispatch (cmds=0x5555565525b0 <qmp_commands>, request=0x7fffe0003710, allow_oob=false, cur_mon=cur_mon@entry=0x5555567fb9a0) at ../qapi/qmp-dispatch.c:141
+ * #1  0x0000555555ca2414 in monitor_qmp_dispatch (mon=0x5555567fb9a0, req=<optimized out>) at ../monitor/qmp.c:144
+ * #2  0x0000555555ca2756 in handle_qmp_command (opaque=0x5555567fb9a0, req=0x7fffe0003710, err=<optimized out>) at ../monitor/qmp.c:371
+ * #3  0x0000555555d27103 in json_message_process_token (lexer=lexer@entry=0x5555567fba60, input=0x555556777720, type=<optimized out>, x=<optimized out>, y=<optimized out>) at ../qobject/json-streamer.c:99
+ * #4  0x0000555555d5805f in json_lexer_feed_char (lexer=lexer@entry=0x5555567fba60, ch=125 '}', flush=flush@entry=false) at ../qobject/json-lexer.c:313
+ * #5  0x0000555555d581b9 in json_lexer_feed (lexer=0x5555567fba60, buffer=<optimized out>, size=<optimized out>) at ../qobject/json-lexer.c:350
+ * #6  0x0000555555c9a07e in tcp_chr_read (chan=<optimized out>, cond=<optimized out>, opaque=<optimized out>) at ../chardev/char-socket.c:508
+ * #7  0x00007ffff6c40119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #8  0x00007ffff6c40478 in g_main_context_iterate.isra () at /lib64/libglib-2.0.so.0
+ * #9  0x00007ffff6c4074a in g_main_loop_run () at /lib64/libglib-2.0.so.0
+ * #10 0x0000555555bfb9c9 in iothread_run (opaque=opaque@entry=0x5555567fbb00) at ../iothread.c:74
+ * #11 0x0000555555d2fda9 in qemu_thread_start (args=0x7fffee5ef950) at ../util/qemu-thread-posix.c:504
+ * #12 0x00007ffff53d0ea5 in start_thread () at /lib64/libpthread.so.0
+ * #13 0x00007ffff50f99fd in clone () at /lib64/libc.so.6
+ */
 /*
  * Runs outside of coroutine context for OOB commands, but in coroutine
  * context for everything else.
diff --git a/qom/qom-qmp-cmds.c b/qom/qom-qmp-cmds.c
index 7c087299d..3e67272b1 100644
--- a/qom/qom-qmp-cmds.c
+++ b/qom/qom-qmp-cmds.c
@@ -75,6 +75,52 @@ void qmp_qom_set(const char *path, const char *property, QObject *value,
     object_property_set_qobject(obj, property, value, errp);
 }
 
+/*
+ * 使用oob的时候
+ *
+ * { "execute": "qmp_capabilities", "arguments": { "enable": ["oob"] } }
+ * { "exec-oob": "qom-get", "arguments": { "path": "/machine/peripheral-anon/device[0]", "property": "queue-size" } }
+ *
+ * (gdb) bt
+ * #0  qmp_qom_get (path=0x7fffe0004ab0 "/machine/peripheral-anon/device[0]", property=0x7fffe00027c0 "queue-size", errp=errp@entry=0x7fffee5ee538) at ../qom/qom-qmp-cmds.c:82
+ * #1  0x0000555555cfe9cb in qmp_marshal_qom_get (args=<optimized out>, ret=0x7fffee5ee5d8, errp=0x7fffee5ee5d0) at qapi/qapi-commands-qom.c:131
+ * #2  0x0000555555d23c8c in qmp_dispatch (cmds=0x5555565525b0 <qmp_commands>, request=<optimized out>, allow_oob=<optimized out>, cur_mon=cur_mon@entry=0x5555567fb9a0) at ../qapi/qmp-dispatch.c:212
+ * #3  0x0000555555ca2414 in monitor_qmp_dispatch (mon=0x5555567fb9a0, req=<optimized out>) at ../monitor/qmp.c:145
+ * #4  0x0000555555ca2756 in handle_qmp_command (opaque=0x5555567fb9a0, req=0x7fffe00027e0, err=<optimized out>) at ../monitor/qmp.c:372
+ * #5  0x0000555555d27143 in json_message_process_token (lexer=lexer@entry=0x5555567fba60, input=0x555556777720, type=<optimized out>, x=<optimized out>, y=<optimized out>) at ../qobject/json-streamer.c:99
+ * #6  0x0000555555d5809f in json_lexer_feed_char (lexer=lexer@entry=0x5555567fba60, ch=125 '}', flush=flush@entry=false) at ../qobject/json-lexer.c:313
+ * #7  0x0000555555d581f9 in json_lexer_feed (lexer=0x5555567fba60, buffer=<optimized out>, size=<optimized out>) at ../qobject/json-lexer.c:350
+ * #8  0x0000555555c9a07e in tcp_chr_read (chan=<optimized out>, cond=<optimized out>, opaque=<optimized out>) at ../chardev/char-socket.c:508
+ * #9  0x00007ffff6c40119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #10 0x00007ffff6c40478 in g_main_context_iterate.isra () at /lib64/libglib-2.0.so.0
+ * #11 0x00007ffff6c4074a in g_main_loop_run () at /lib64/libglib-2.0.so.0
+ * #12 0x0000555555bfb9c9 in iothread_run (opaque=opaque@entry=0x5555567fbb00) at ../iothread.c:74
+ * #13 0x0000555555d2fde9 in qemu_thread_start (args=0x7fffee5ef950) at ../util/qemu-thread-posix.c:504
+ * #14 0x00007ffff53d0ea5 in start_thread () at /lib64/libpthread.so.0
+ * #15 0x00007ffff50f99fd in clone () at /lib64/libc.so.6
+ *
+ * 不使用oob的时候
+ *
+ * { "execute": "qmp_capabilities" }
+ * { "execute": "qom-get", "arguments": { "path": "/machine/peripheral-anon/device[0]", "property": "queue-size" } }
+ *
+ * (gdb) bt
+ * #0  qmp_qom_get (path=0x555556c38880 "/machine/peripheral-anon/device[0]", property=0x555556d9e560 "queue-size", errp=errp@entry=0x7fffffffd8a8) at ../qom/qom-qmp-cmds.c:82
+ * #1  0x0000555555cfe9cb in qmp_marshal_qom_get (args=<optimized out>, ret=0x7ffff7fcbea8, errp=0x7ffff7fcbea0) at qapi/qapi-commands-qom.c:131
+ * #2  0x0000555555d23379 in do_qmp_dispatch_bh (opaque=0x7ffff7fcbeb0) at ../qapi/qmp-dispatch.c:130
+ * #3  0x0000555555d3ff84 in aio_bh_call (bh=0x555556c74520) at ../util/async.c:178
+ * #4  aio_bh_poll (ctx=ctx@entry=0x5555565cac30) at ../util/async.c:178
+ * #5  0x0000555555d2cc1e in aio_dispatch (ctx=0x5555565cac30) at ../util/aio-posix.c:421
+ * #6  0x0000555555d3fbee in aio_ctx_dispatch (source=<optimized out>, callback=<optimized out>, user_data=<optimized out>) at ../util/async.c:320
+ * #7  0x00007ffff6c40119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #8  0x0000555555d4bea0 in glib_pollfds_poll () at ../util/main-loop.c:297
+ * #9  os_host_main_loop_wait (timeout=0) at ../util/main-loop.c:320
+ * #10 main_loop_wait (nonblocking=nonblocking@entry=0) at ../util/main-loop.c:596
+ * #11 0x00005555559d1181 in qemu_main_loop () at ../softmmu/runstate.c:734
+ * #12 0x0000555555825f5c in qemu_main (argc=<optimized out>, argv=<optimized out>, envp=<optimized out>) at ../softmmu/main.c:38
+ * #13 0x00007ffff501d555 in __libc_start_main () at /lib64/libc.so.6
+ * #14 0x0000555555825e8a in _start () at ../softmmu/main.c:47
+ */
 QObject *qmp_qom_get(const char *path, const char *property, Error **errp)
 {
     Object *obj;
diff --git a/system/cpus.c b/system/cpus.c
index a444a747f..517473017 100644
--- a/system/cpus.c
+++ b/system/cpus.c
@@ -129,6 +129,17 @@ void hw_error(const char *fmt, ...)
     abort();
 }
 
+/*
+ * called by:
+ *   - dump/dump.c|1807| <<dump_init>> cpu_synchronize_all_states();
+ *   - hw/i386/pc.c|1782| <<pc_machine_wakeup>> cpu_synchronize_all_states();
+ *   - migration/colo.c|681| <<colo_incoming_process_checkpoint>> cpu_synchronize_all_states();
+ *   - migration/migration.c|3797| <<bg_migration_thread>> cpu_synchronize_all_states();
+ *   - migration/savevm.c|1783| <<qemu_savevm_state_complete_precopy>> cpu_synchronize_all_states();
+ *   - migration/savevm.c|1968| <<qemu_save_device_state>> cpu_synchronize_all_states();
+ *   - system/runstate.c|518| <<qemu_system_reset>> cpu_synchronize_all_states();
+ *   - target/arm/hvf/hvf.c|2026| <<hvf_vm_state_change>> cpu_synchronize_all_states();
+ */
 void cpu_synchronize_all_states(void)
 {
     CPUState *cpu;
@@ -138,6 +149,11 @@ void cpu_synchronize_all_states(void)
     }
 }
 
+/*
+ * called by:
+ *   - hw/i386/pc.c|1784| <<pc_machine_wakeup>> cpu_synchronize_all_post_reset();
+ *   - system/runstate.c|504| <<qemu_system_reset>> cpu_synchronize_all_post_reset();
+ */
 void cpu_synchronize_all_post_reset(void)
 {
     CPUState *cpu;
@@ -147,6 +163,13 @@ void cpu_synchronize_all_post_reset(void)
     }
 }
 
+/*
+ * called by:
+ *   - hw/core/machine.c|1531| <<qdev_machine_creation_done>> cpu_synchronize_all_post_init();
+ *   - migration/savevm.c|2145| <<loadvm_postcopy_handle_run_bh>> cpu_synchronize_all_post_init();
+ *   - migration/savevm.c|3003| <<qemu_loadvm_state>> cpu_synchronize_all_post_init();
+ *   - migration/savevm.c|3020| <<qemu_load_device_state>> cpu_synchronize_all_post_init();
+ */
 void cpu_synchronize_all_post_init(void)
 {
     CPUState *cpu;
@@ -179,6 +202,23 @@ void cpu_synchronize_post_reset(CPUState *cpu)
     }
 }
 
+/*
+ * called by:
+ *   - hw/core/cpu-common.c|213| <<cpu_common_realizefn>> cpu_synchronize_post_init(cpu);
+ *   - system/cpus.c|155| <<cpu_synchronize_all_post_init>> cpu_synchronize_post_init(cpu);
+ *   - target/s390x/gdbstub.c|90| <<cpu_write_ac_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|197| <<cpu_write_c_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|233| <<cpu_write_virt_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|237| <<cpu_write_virt_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|241| <<cpu_write_virt_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|245| <<cpu_write_virt_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|281| <<cpu_write_virt_kvm_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|285| <<cpu_write_virt_kvm_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|289| <<cpu_write_virt_kvm_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|293| <<cpu_write_virt_kvm_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/gdbstub.c|317| <<cpu_write_gs_reg>> cpu_synchronize_post_init(env_cpu(env));
+ *   - target/s390x/sigp.c|298| <<sigp_set_prefix>> cpu_synchronize_post_init(cs);
+ */
 void cpu_synchronize_post_init(CPUState *cpu)
 {
     if (cpus_accel->synchronize_post_init) {
diff --git a/system/main.c b/system/main.c
index 9b91d21ea..193770869 100644
--- a/system/main.c
+++ b/system/main.c
@@ -30,6 +30,10 @@
 #include <SDL.h>
 #endif
 
+/*
+ * 在以下使用qemu_default_main():
+ *   - system/main.c|43| <<global>> int (*qemu_main)(void ) = qemu_default_main;
+ */
 int qemu_default_main(void)
 {
     int status;
diff --git a/system/memory.c b/system/memory.c
index 798b6c0a1..9db48a37d 100644
--- a/system/memory.c
+++ b/system/memory.c
@@ -40,6 +40,25 @@
 static unsigned memory_region_transaction_depth;
 static bool memory_region_update_pending;
 static bool ioeventfd_update_pending;
+/*
+ * 在以下设置global_dirty_tracking:
+ *   - system/memory.c|3081| <<memory_global_dirty_log_start>> global_dirty_tracking |= flags;
+ *   - system/memory.c|3096| <<memory_global_dirty_log_do_stop>> global_dirty_tracking &= ~flags;
+ * 在以下使用global_dirty_tracking:
+ *   - include/exec/ram_addr.h|390| <<cpu_physical_memory_set_dirty_lebitmap>> if (global_dirty_tracking) {
+ *   - include/exec/ram_addr.h|395| <<cpu_physical_memory_set_dirty_lebitmap>> if (unlikely(global_dirty_tracking & GLOBAL_DIRTY_DIRTY_RATE)) {
+ *   - include/exec/ram_addr.h|419| <<cpu_physical_memory_set_dirty_lebitmap>> if (!global_dirty_tracking) {
+ *   - include/exec/ram_addr.h|431| <<cpu_physical_memory_set_dirty_lebitmap>> if (unlikely(global_dirty_tracking & GLOBAL_DIRTY_DIRTY_RATE)) {
+ *   - migration/ram.c|2507| <<ram_save_cleanup>> if (global_dirty_tracking & GLOBAL_DIRTY_MIGRATION) {
+ *   - system/memory.c|1925| <<memory_region_get_dirty_log_mask>> if (global_dirty_tracking && ((rb && qemu_ram_is_migratable(rb)) ||
+ *   - system/memory.c|3075| <<memory_global_dirty_log_start>> flags &= ~global_dirty_tracking;
+ *   - system/memory.c|3080| <<memory_global_dirty_log_start>> old_flags = global_dirty_tracking;
+ *   - system/memory.c|3082| <<memory_global_dirty_log_start>> trace_global_dirty_changed(global_dirty_tracking);
+ *   - system/memory.c|3095| <<memory_global_dirty_log_do_stop>> assert((global_dirty_tracking & flags) == flags);
+ *   - system/memory.c|3098| <<memory_global_dirty_log_do_stop>> trace_global_dirty_changed(global_dirty_tracking);
+ *   - system/memory.c|3100| <<memory_global_dirty_log_do_stop>> if (!global_dirty_tracking) {
+ *   - system/memory.c|3162| <<listener_add_address_space>> if (global_dirty_tracking) {
+ */
 unsigned int global_dirty_tracking;
 
 static QTAILQ_HEAD(, MemoryListener) memory_listeners
@@ -151,6 +170,18 @@ enum ListenerDirection { Forward, Reverse };
         }                                                               \
     } while (0)
 
+/*
+ * called by:
+ *   - system/memory.c|930| <<flat_range_coalesced_io_notify>> MEMORY_LISTENER_UPDATE_REGION(fr, as, Forward, coalesced_io_add, int128_get64(tmp.start), int128_get64(tmp.size));
+ *   - system/memory.c|934| <<flat_range_coalesced_io_notify>> MEMORY_LISTENER_UPDATE_REGION(fr, as, Reverse, coalesced_io_del, int128_get64(tmp.start), int128_get64(tmp.size));
+ *   - system/memory.c|1042| <<address_space_update_topology_pass>> MEMORY_LISTENER_UPDATE_REGION(frold, as, Reverse, region_del);
+ *   - system/memory.c|1050| <<address_space_update_topology_pass>> MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_nop);
+ *   - system/memory.c|1052| <<address_space_update_topology_pass>> MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, log_start, frold->dirty_log_mask, frnew->dirty_log_mask);
+ *   - system/memory.c|1057| <<address_space_update_topology_pass>> MEMORY_LISTENER_UPDATE_REGION(frnew, as, Reverse, log_stop, frold->dirty_log_mask, frnew->dirty_log_mask);
+ *   - system/memory.c|1069| <<address_space_update_topology_pass>> MEMORY_LISTENER_UPDATE_REGION(frnew, as, Forward, region_add);
+ *
+ * 会生成一个MemoryRegionSection
+ */
 /* No need to ref/unref .mr, the FlatRange keeps it alive.  */
 #define MEMORY_LISTENER_UPDATE_REGION(fr, as, dir, callback, _args...)  \
     do {                                                                \
@@ -230,6 +261,15 @@ struct FlatRange {
 #define FOR_EACH_FLAT_RANGE(var, view)          \
     for (var = (view)->ranges; var < (view)->ranges + (view)->nr; ++var)
 
+/*
+ * called by:
+ *   - system/memory.c|176| <<MEMORY_LISTENER_UPDATE_REGION>> MemoryRegionSection mrs = section_from_flat_range(fr, \
+ *   - system/memory.c|789| <<generate_memory_topology>> section_from_flat_range(&view->ranges[i], view);
+ *   - system/memory.c|2438| <<memory_region_sync_dirty_bitmap>> MemoryRegionSection mrs = section_from_flat_range(fr, view);
+ *   - system/memory.c|2484| <<memory_region_clear_dirty_bitmap>> mrs = section_from_flat_range(fr, view);
+ *   - system/memory.c|3189| <<listener_add_address_space>> MemoryRegionSection section = section_from_flat_range(fr, view);
+ *   - system/memory.c|3215| <<listener_del_address_space>> MemoryRegionSection section = section_from_flat_range(fr, view);
+ */
 static inline MemoryRegionSection
 section_from_flat_range(FlatRange *fr, FlatView *fv)
 {
@@ -941,6 +981,52 @@ static void flat_range_coalesced_io_add(FlatRange *fr, AddressSpace *as)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  vhost_region_add_section (dev=0x5555572db3d0, section=0x7ffe66dfc1d0) at ../hw/virtio/vhost.c:681
+ * #1  0x0000555555b27839 in vhost_region_addnop (listener=0x5555572db3d8, section=0x7ffe66dfc1d0) at ../hw/virtio/vhost.c:794
+ * #2  0x0000555555d91169 in address_space_update_topology_pass (as=0x555556fba100 <address_space_memory>, old_view=0x7fffec2ab0a0, new_view=0x7ffe5c054f00, adding=true) at ../system/memory.c:985
+ * #3  0x0000555555d9168d in address_space_set_flatview (as=0x555556fba100 <address_space_memory>) at ../system/memory.c:1080
+ * #4  0x0000555555d91846 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #5  0x0000555555d959ec in memory_region_del_subregion (mr=0x55555713c800, subregion=0x55555785cce0) at ../system/memory.c:2680
+ * #6  0x0000555555a4a8b7 in pci_update_mappings (d=0x55555785ab30) at ../hw/pci/pci.c:1539
+ * #7  0x0000555555a4ad39 in pci_default_write_config (d=0x55555785ab30, addr=4, val_in=256, l=2) at ../hw/pci/pci.c:1607
+ * #8  0x0000555555a4f838 in pci_host_config_write_common (pci_dev=0x55555785ab30, addr=4, limit=256, val=256, len=2) at ../hw/pci/pci_host.c:96
+ * #9  0x0000555555a4fa5e in pci_data_write (s=0x5555573e6060, addr=2147485956, val=256, len=2) at ../hw/pci/pci_host.c:138
+ * #10 0x0000555555a4fbf5 in pci_host_data_write (opaque=0x5555573afb10, addr=0, val=256, len=2) at ../hw/pci/pci_host.c:188
+ * #11 0x0000555555d8f37d in memory_region_write_accessor (mr=0x5555573aff50, addr=0, value=0x7ffe66dfc5a8, size=2, shift=0, mask=65535, attrs=...) at ../system/memory.c:497
+ * #12 0x0000555555d8f692 in access_with_adjusted_size (addr=0, value=0x7ffe66dfc5a8, size=2, access_size_min=1, access_size_max=4, access_fn=
+ *                                    0x555555d8f287 <memory_region_write_accessor>, mr=0x5555573aff50, attrs=...) at ../system/memory.c:573
+ * #13 0x0000555555d92776 in memory_region_dispatch_write (mr=0x5555573aff50, addr=0, data=256, op=MO_16, attrs=...) at ../system/memory.c:1521
+ * #14 0x0000555555da00c4 in flatview_write_continue (fv=0x7fffec281390, addr=3324, attrs=..., ptr=0x7ffff42a0000, len=2, addr1=0, l=2, mr=0x5555573aff50) at ../system/physmem.c:2714
+ * #15 0x0000555555da0227 in flatview_write (fv=0x7fffec281390, addr=3324, attrs=..., buf=0x7ffff42a0000, len=2) at ../system/physmem.c:2756
+ * #16 0x0000555555da05d7 in address_space_write (as=0x555556fba0a0 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff42a0000, len=2) at ../system/physmem.c:2863
+ * #17 0x0000555555da0644 in address_space_rw (as=0x555556fba0a0 <address_space_io>, addr=3324, attrs=..., buf=0x7ffff42a0000, len=2, is_write=true) at ../system/physmem.c:2873
+ * #18 0x0000555555df2d18 in kvm_handle_io (port=3324, attrs=..., data=0x7ffff42a0000, direction=1, size=2, count=1) at ../accel/kvm/kvm-all.c:2632
+ * #19 0x0000555555df35bb in kvm_cpu_exec (cpu=0x5555573a4d30) at ../accel/kvm/kvm-all.c:2905
+ * #20 0x0000555555df62fa in kvm_vcpu_thread_fn (arg=0x5555573a4d30) at ../accel/kvm/kvm-accel-ops.c:51
+ * #21 0x00005555560017cc in qemu_thread_start (args=0x5555573aed40) at ../util/qemu-thread-posix.c:541
+ * #22 0x00007ffff749f812 in start_thread () at /lib64/libc.so.6
+ * #23 0x00007ffff743f450 in clone3 () at /lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vhost_region_add_section (dev=0x5555572db030, section=0x7fffffffdcc0) at ../hw/virtio/vhost.c:681
+ * #1  0x0000555555b27839 in vhost_region_addnop (listener=0x5555572db038, section=0x7fffffffdcc0) at ../hw/virtio/vhost.c:794
+ * #2  0x0000555555d9138d in address_space_update_topology_pass (as=0x555556fba100 <address_space_memory>, old_view=0x555557711610, new_view=0x55555772cf90, adding=true) at ../system/memory.c:1004
+ * #3  0x0000555555d9168d in address_space_set_flatview (as=0x555556fba100 <address_space_memory>) at ../system/memory.c:1080
+ * #4  0x0000555555d91846 in memory_region_transaction_commit () at ../system/memory.c:1132
+ * #5  0x0000555555d95725 in memory_region_update_container_subregions (subregion=0x5555570a8c00) at ../system/memory.c:2630
+ * #6  0x0000555555d957d0 in memory_region_add_subregion_common (mr=0x5555572c8400, offset=0, subregion=0x5555570a8c00) at ../system/memory.c:2645
+ * #7  0x0000555555d9580c in memory_region_add_subregion (mr=0x5555572c8400, offset=0, subregion=0x5555570a8c00) at ../system/memory.c:2653
+ * #8  0x0000555555c5dc59 in pc_memory_init (pcms=0x5555572bf2d0, system_memory=0x5555572c8400, rom_memory=0x5555570fc6e0, pci_hole64_size=2147483648) at ../hw/i386/pc.c:954
+ * #9  0x0000555555c3f9e7 in pc_init1 (machine=0x5555572bf2d0, host_type=0x555556219858 "i440FX-pcihost", pci_type=0x555556219851 "i440FX") at ../hw/i386/pc_piix.c:246
+ * #10 0x0000555555c405d7 in pc_init_v8_2 (machine=0x5555572bf2d0) at ../hw/i386/pc_piix.c:555
+ * #11 0x00005555559243b5 in machine_run_board_init (machine=0x5555572bf2d0, mem_path=0x0, errp=0x7fffffffe120) at ../hw/core/machine.c:1509
+ * #12 0x0000555555b75478 in qemu_init_board () at ../system/vl.c:2613
+ * #13 0x0000555555b756fd in qmp_x_exit_preconfig (errp=0x555556fce260 <error_fatal>) at ../system/vl.c:2704
+ * #14 0x0000555555b780dc in qemu_init (argc=23, argv=0x7fffffffe448) at ../system/vl.c:3753
+ * #15 0x0000555555e02daf in main (argc=23, argv=0x7fffffffe448) at ../system/main.c:47
+ */
 static void address_space_update_topology_pass(AddressSpace *as,
                                                const FlatView *old_view,
                                                const FlatView *new_view,
@@ -1134,6 +1220,9 @@ void memory_region_transaction_commit(void)
             }
             memory_region_update_pending = false;
             ioeventfd_update_pending = false;
+	    /*
+	     * 在这里commit!
+	     */
             MEMORY_LISTENER_CALL_GLOBAL(commit, Forward);
         } else if (ioeventfd_update_pending) {
             QTAILQ_FOREACH(as, &address_spaces, address_spaces_link) {
@@ -1546,6 +1635,19 @@ void memory_region_init_io(MemoryRegion *mr,
     mr->terminates = true;
 }
 
+/*
+ * called by:
+ *   - hw/block/onenand.c|814| <<onenand_realize>> memory_region_init_ram_nomigrate(&s->ram, OBJECT(s), "onenand.ram",
+ *   - hw/display/tcx.c|819| <<tcx_realizefn>> memory_region_init_ram_nomigrate(&s->vram_mem, OBJECT(s), "tcx.vram",
+ *   - hw/display/vga.c|2219| <<vga_common_init>> memory_region_init_ram_nomigrate(&s->vram, obj, "vga.vram", s->vram_size,
+ *   - hw/sparc/sun4m.c|582| <<idreg_realize>> memory_region_init_ram_nomigrate(&s->mem, OBJECT(ds), "sun4m.idreg",
+ *   - hw/sparc/sun4m.c|636| <<afx_realize>> memory_region_init_ram_nomigrate(&s->mem, OBJECT(ds), "sun4m.afx", 4,
+ *   - hw/sparc/sun4m.c|720| <<prom_realize>> memory_region_init_ram_nomigrate(&s->prom, OBJECT(ds), "sun4m.prom",
+ *   - hw/sparc64/sun4u.c|459| <<prom_realize>> memory_region_init_ram_nomigrate(&s->prom, OBJECT(ds), "sun4u.prom",
+ *   - hw/sparc64/sun4u.c|509| <<ram_realize>> memory_region_init_ram_nomigrate(&d->ram, OBJECT(d), "sun4u.ram", d->size,
+ *   - hw/xtensa/xtfpga.c|162| <<xtfpga_net_init>> memory_region_init_ram_nomigrate(ram, OBJECT(s), "open_eth.ram", 16 * KiB,
+ *   - system/memory.c|3588| <<memory_region_init_ram>> memory_region_init_ram_nomigrate(mr, owner, name, size, &err);
+ */
 void memory_region_init_ram_nomigrate(MemoryRegion *mr,
                                       Object *owner,
                                       const char *name,
@@ -1555,6 +1657,13 @@ void memory_region_init_ram_nomigrate(MemoryRegion *mr,
     memory_region_init_ram_flags_nomigrate(mr, owner, name, size, 0, errp);
 }
 
+/*
+ * called by:
+ *   - backends/hostmem-ram.c|33| <<ram_backend_memory_alloc>> memory_region_init_ram_flags_nomigrate(&backend->mr, OBJECT(backend), name,
+ *   - hw/m68k/next-cube.c|1087| <<next_cube_init>> memory_region_init_ram_flags_nomigrate(bmapm1, NULL, "next.bmapmem", 64,
+ *   - system/memory.c|1555| <<memory_region_init_ram_nomigrate>> memory_region_init_ram_flags_nomigrate(mr, owner, name, size, 0, errp);
+ *   - system/memory.c|1721| <<memory_region_init_rom_nomigrate>> memory_region_init_ram_flags_nomigrate(mr, owner, name, size, 0, errp);
+ */
 void memory_region_init_ram_flags_nomigrate(MemoryRegion *mr,
                                             Object *owner,
                                             const char *name,
@@ -1600,6 +1709,10 @@ void memory_region_init_resizeable_ram(MemoryRegion *mr,
 }
 
 #ifdef CONFIG_POSIX
+/*
+ * called by:
+ *   - backends/hostmem-file.c|89| <<file_backend_memory_alloc>> memory_region_init_ram_from_file(&backend->mr, OBJECT(backend), name,
+ */
 void memory_region_init_ram_from_file(MemoryRegion *mr,
                                       Object *owner,
                                       const char *name,
@@ -1617,6 +1730,9 @@ void memory_region_init_ram_from_file(MemoryRegion *mr,
     mr->terminates = true;
     mr->destructor = memory_region_destructor_ram;
     mr->align = align;
+    /*
+     * 只在此处调用
+     */
     mr->ram_block = qemu_ram_alloc_from_file(size, mr, ram_flags, path,
                                              offset, &err);
     if (err) {
@@ -1626,6 +1742,13 @@ void memory_region_init_ram_from_file(MemoryRegion *mr,
     }
 }
 
+/*
+ * called by:
+ *   - backends/hostmem-epc.c|41| <<sgx_epc_backend_memory_alloc>> memory_region_init_ram_from_fd(&backend->mr, OBJECT(backend),
+ *   - backends/hostmem-memfd.c|58| <<memfd_backend_memory_alloc>> memory_region_init_ram_from_fd(&backend->mr, OBJECT(backend), name,
+ *   - hw/misc/ivshmem.c|499| <<process_msg_shmem>> memory_region_init_ram_from_fd(&s->server_bar2, OBJECT(s), "ivshmem.bar2",
+ *   - hw/remote/memory.c|46| <<remote_sysmem_reconfig>> memory_region_init_ram_from_fd(subregion, NULL,
+ */
 void memory_region_init_ram_from_fd(MemoryRegion *mr,
                                     Object *owner,
                                     const char *name,
@@ -2029,6 +2152,47 @@ void memory_region_unmap_iommu_notifier_range(IOMMUNotifier *notifier)
     memory_region_notify_iommu_one(notifier, &event);
 }
 
+/*
+ * (gdb) bt
+ * #0  vfio_iommu_map_notify (n=0x7fefd80c9388, iotlb=0x7fffed379ff0) at ../hw/vfio/common.c:293
+ * #1  0x0000555555d79e23 in memory_region_notify_iommu_one (notifier=0x7fefd80c9388, event=0x7fffed37a070) at ../system/memory.c:2015
+ * #2  0x0000555555d79f3e in memory_region_notify_iommu (iommu_mr=0x55555803a9d0, iommu_idx=0, event=...) at ../system/memory.c:2042
+ * #3  0x0000555555b1673e in virtio_iommu_notify_map_unmap (mr=0x55555803a9d0, event=0x7fffed37a130, virt_start=4294422528, virt_end=4294426623) at ../hw/virtio/virtio-iommu.c:222
+ * #4  0x0000555555b16847 in virtio_iommu_notify_map (mr=0x55555803a9d0, virt_start=4294422528, virt_end=4294426623, paddr=4653244416, flags=2) at ../hw/virtio/virtio-iommu.c:251
+ * #5  0x0000555555b1742f in virtio_iommu_map (s=0x555557f600e0, req=0x7fffed37a210) at ../hw/virtio/virtio-iommu.c:595
+ * #6  0x0000555555b179db in virtio_iommu_handle_map (s=0x555557f600e0, iov=0x7fffe4034a68, iov_cnt=1) at ../hw/virtio/virtio-iommu.c:739
+ * #7  0x0000555555b17ccb in virtio_iommu_handle_command (vdev=0x555557f600e0, vq=0x555557f68b80) at ../hw/virtio/virtio-iommu.c:796
+ * #8  0x0000555555d47215 in virtio_queue_notify (vdev=0x555557f600e0, n=0) at ../hw/virtio/virtio.c:2288
+ * #9  0x0000555555b0b09f in virtio_pci_notify_write (opaque=0x555557f57d10, addr=0, val=0, size=2) at ../hw/virtio/virtio-pci.c:1688
+ * #10 0x0000555555d75521 in memory_region_write_accessor (mr=0x555557f58bb0, addr=0, value=0x7fffed37a448, size=2, shift=0, mask=65535, attrs=...) at ../system/memory.c:497
+ * #11 0x0000555555d75844 in access_with_adjusted_size (addr=0, value=0x7fffed37a448, size=2, access_size_min=1, access_size_max=4, access_fn=0x555555d7542b <memory_region_write_accessor>,
+ *                                       mr=0x555557f58bb0, attrs=...) at ../system/memory.c:573
+ * #12 0x0000555555d78ad8 in memory_region_dispatch_write (mr=0x555557f58bb0, addr=0, data=0, op=MO_16, attrs=...) at ../system/memory.c:1521
+ * #13 0x0000555555d864a1 in flatview_write_continue (fv=0x7fefd81e04b0, addr=61573724909568, attrs=..., ptr=0x7ffff7fc4028, len=2, addr1=0, l=2, mr=0x555557f58bb0) at ../system/physmem.c:2714
+ * #14 0x0000555555d86604 in flatview_write (fv=0x7fefd81e04b0, addr=61573724909568, attrs=..., buf=0x7ffff7fc4028, len=2) at ../system/physmem.c:2756
+ * #15 0x0000555555d869b4 in address_space_write (as=0x555556f51780 <address_space_memory>, addr=61573724909568, attrs=..., buf=0x7ffff7fc4028, len=2) at ../system/physmem.c:2863
+ * #16 0x0000555555d86a21 in address_space_rw (as=0x555556f51780 <address_space_memory>, addr=61573724909568, attrs=..., buf=0x7ffff7fc4028, len=2, is_write=true) at ../system/physmem.c:2873
+ * #17 0x0000555555dd98ac in kvm_cpu_exec (cpu=0x5555572f6fd0) at ../accel/kvm/kvm-all.c:2915
+ * #18 0x0000555555ddc588 in kvm_vcpu_thread_fn (arg=0x5555572f6fd0) at ../accel/kvm/kvm-accel-ops.c:51
+ * #19 0x0000555555fe280b in qemu_thread_start (args=0x555557300f40) at ../util/qemu-thread-posix.c:541
+ * #20 0x00007ffff48081da in start_thread () at /lib/../lib64/libpthread.so.0
+ * #21 0x00007ffff4439e73 in clone () at /lib/../lib64/libc.so.6
+ *
+ * called by:
+ *   - hw/i386/intel_iommu.c|1539| <<vtd_sync_shadow_page_hook>> memory_region_notify_iommu(private, 0, *event);
+ *   - hw/i386/intel_iommu.c|2232| <<vtd_iotlb_page_invalidate_notify>> memory_region_notify_iommu(&vtd_as->iommu, 0, event);
+ *   - hw/i386/intel_iommu.c|2696| <<vtd_process_device_iotlb_desc>> memory_region_notify_iommu(&vtd_dev_as->iommu, 0, event);
+ *   - hw/misc/tz-mpc.c|110| <<tz_mpc_iommu_notify>> memory_region_notify_iommu(&s->upstream, IOMMU_IDX_S, event);
+ *   - hw/misc/tz-mpc.c|111| <<tz_mpc_iommu_notify>> memory_region_notify_iommu(&s->upstream, IOMMU_IDX_NS, event);
+ *   - hw/misc/tz-mpc.c|120| <<tz_mpc_iommu_notify>> memory_region_notify_iommu(&s->upstream, IOMMU_IDX_S, event);
+ *   - hw/misc/tz-mpc.c|126| <<tz_mpc_iommu_notify>> memory_region_notify_iommu(&s->upstream, IOMMU_IDX_NS, event);
+ *   - hw/ppc/spapr_iommu.c|471| <<put_tce_emu>> memory_region_notify_iommu(&tcet->iommu, 0, event);
+ *   - hw/s390x/s390-pci-inst.c|645| <<s390_pci_update_iotlb>> memory_region_notify_iommu(&iommu->iommu_mr, 0, event);
+ *   - hw/s390x/s390-pci-inst.c|663| <<s390_pci_update_iotlb>> memory_region_notify_iommu(&iommu->iommu_mr, 0, event);
+ *   - hw/s390x/s390-pci-inst.c|687| <<s390_pci_batch_unmap>> memory_region_notify_iommu(&iommu->iommu_mr, 0, event);
+ *   - hw/virtio/virtio-iommu.c|214| <<virtio_iommu_notify_map_unmap>> memory_region_notify_iommu(mr, 0, *event);
+ *   - hw/virtio/virtio-iommu.c|222| <<virtio_iommu_notify_map_unmap>> memory_region_notify_iommu(mr, 0, *event);
+ */
 void memory_region_notify_iommu(IOMMUMemoryRegion *iommu_mr,
                                 int iommu_idx,
                                 IOMMUTLBEvent event)
@@ -2244,6 +2408,18 @@ void memory_region_set_log(MemoryRegion *mr, bool log, unsigned client)
     memory_region_transaction_commit();
 }
 
+/*
+ * 部分调用的例子:
+ *   - hw/arm/virt-acpi-build.c|1068| <<acpi_ram_update>> memory_region_set_dirty(mr, 0, size);
+ *   - hw/hyperv/hyperv.c|286| <<cpu_post_msg>> memory_region_set_dirty(&synic->msg_page_mr, 0, sizeof(*synic->msg_page));
+ *   - hw/hyperv/hyperv.c|363| <<hyperv_set_event_flag>> memory_region_set_dirty(&synic->event_page_mr, 0,
+ *   - hw/i386/acpi-build.c|2733| <<acpi_ram_update>> memory_region_set_dirty(mr, 0, size);
+ *   - hw/i386/kvm/xen_gnttab.c|257| <<gnt_unref>> memory_region_set_dirty(mrs->mr, mrs->offset_within_region,
+ *   - hw/i386/xen/xen-hvm.c|397| <<xen_sync_dirty_bitmap>> memory_region_set_dirty(framebuffer, 0, size);
+ *   - hw/i386/xen/xen-hvm.c|410| <<xen_sync_dirty_bitmap>> memory_region_set_dirty(framebuffer,
+ *   - hw/virtio/vhost.c|133| <<vhost_dev_sync_region>> memory_region_set_dirty(section->mr, mr_offset, VHOST_LOG_PAGE);
+ *   - target/i386/kvm/xen-emu.c|1911| <<kvm_get_xen_state>> memory_region_set_dirty(mrs.mr, mrs.offset_within_region,
+ */
 void memory_region_set_dirty(MemoryRegion *mr, hwaddr addr,
                              hwaddr size)
 {
@@ -2257,6 +2433,11 @@ void memory_region_set_dirty(MemoryRegion *mr, hwaddr addr,
  * If memory region `mr' is NULL, do global sync.  Otherwise, sync
  * dirty bitmap for the specified memory region.
  */
+/*
+ * called by:
+ *   - system/memory.c|2388| <<memory_region_snapshot_and_clear_dirty>> memory_region_sync_dirty_bitmap(mr, false);
+ *   - system/memory.c|2919| <<memory_global_dirty_log_sync>> memory_region_sync_dirty_bitmap(NULL, last_stage);
+ */
 static void memory_region_sync_dirty_bitmap(MemoryRegion *mr, bool last_stage)
 {
     MemoryListener *listener;
@@ -2276,6 +2457,9 @@ static void memory_region_sync_dirty_bitmap(MemoryRegion *mr, bool last_stage)
             FOR_EACH_FLAT_RANGE(fr, view) {
                 if (fr->dirty_log_mask && (!mr || fr->mr == mr)) {
                     MemoryRegionSection mrs = section_from_flat_range(fr, view);
+                    /*
+		     * kvm_log_sync()
+		     */
                     listener->log_sync(listener, &mrs);
                 }
             }
@@ -2880,6 +3064,14 @@ bool memory_region_present(MemoryRegion *container, hwaddr addr)
     return mr && mr != container;
 }
 
+/*
+ * called by:
+ *   - migration/dirtyrate.c|110| <<global_dirty_log_sync>> memory_global_dirty_log_sync(false);
+ *   - migration/dirtyrate.c|622| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_sync(false);
+ *   - migration/ram.c|1046| <<migration_bitmap_sync>> memory_global_dirty_log_sync(last_stage);
+ *   - migration/ram.c|3459| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_sync(false);
+ *   - migration/ram.c|3752| <<colo_flush_ram_cache>> memory_global_dirty_log_sync(false);
+ */
 void memory_global_dirty_log_sync(bool last_stage)
 {
     memory_region_sync_dirty_bitmap(NULL, last_stage);
@@ -2898,6 +3090,16 @@ static unsigned int postponed_stop_flags;
 static VMChangeStateEntry *vmstate_change;
 static void memory_global_dirty_log_stop_postponed_run(void);
 
+/*
+ * called by:
+ *   - hw/i386/xen/xen-hvm.c|656| <<qmp_xen_set_global_dirty_log>> memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
+ *   - migration/dirtyrate.c|95| <<global_dirty_log_change>> memory_global_dirty_log_start(flag);
+ *   - migration/dirtyrate.c|619| <<calculate_dirtyrate_dirty_bitmap>> memory_global_dirty_log_start(GLOBAL_DIRTY_DIRTY_RATE);
+ *   - migration/ram.c|2831| <<ram_init_bitmaps>> memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
+ *   - migration/ram.c|3607| <<colo_incoming_start_dirty_log>> memory_global_dirty_log_start(GLOBAL_DIRTY_MIGRATION);
+ *
+ * 开始统计脏页
+ */
 void memory_global_dirty_log_start(unsigned int flags)
 {
     unsigned int old_flags;
@@ -3562,6 +3764,34 @@ void mtree_info(bool flatview, bool dispatch_tree, bool owner, bool disabled)
     }
 }
 
+/*
+ * 部分调用的例子:
+ *   - hw/arm/vexpress.c|386| <<a15_daughterboard_init>> memory_region_init_ram(&vms->a15sram, NULL, "vexpress.a15sram", 0x10000,
+ *   - hw/arm/vexpress.c|672| <<vexpress_common_init>> memory_region_init_ram(&vms->sram, NULL, "vexpress.sram", sram_size,
+ *   - hw/arm/vexpress.c|677| <<vexpress_common_init>> memory_region_init_ram(&vms->vram, NULL, "vexpress.vram", vram_size,
+ *   - hw/arm/virt.c|1580| <<create_tag_ram>> memory_region_init_ram(tagram, NULL, name, size / 32, &error_fatal);
+ *   - hw/arm/virt.c|1594| <<create_secure_ram>> memory_region_init_ram(secram, NULL, "virt.secure-ram", size,
+ *   - hw/arm/virt.c|1993| <<virt_cpu_post_init>> memory_region_init_ram(pvtime, NULL, "pvtime", pvtime_size, NULL);
+ *   - hw/arm/xen_arm.c|114| <<xen_init_ram>> memory_region_init_ram(&ram_memory, NULL, "xen.ram", block_len,
+ *   - hw/hyperv/hyperv.c|110| <<synic_realize>> memory_region_init_ram(&synic->msg_page_mr, obj, msgp_name,
+ *   - hw/hyperv/hyperv.c|112| <<synic_realize>> memory_region_init_ram(&synic->event_page_mr, obj, eventp_name,
+ *   - hw/i386/kvm/xen_gnttab.c|77| <<xen_gnttab_realize>> memory_region_init_ram(&s->gnt_frames, OBJECT(dev), "xen:grant_table",
+ *   - hw/i386/kvm/xen_overlay.c|94| <<xen_overlay_realize>> memory_region_init_ram(&s->shinfo_mem, OBJECT(dev), "xen:shared_info",
+ *   - hw/i386/kvm/xen_primary_console.c|60| <<xen_primary_console_realize>> memory_region_init_ram(&s->console_page, OBJECT(dev), "xen:console_page",
+ *   - hw/i386/kvm/xen_xenstore.c|123| <<xen_xenstore_realize>> memory_region_init_ram(&s->xenstore_page, OBJECT(dev), "xen:xenstore_page",
+ *   - hw/i386/pc.c|1040| <<pc_memory_init>> memory_region_init_ram(option_rom_mr, NULL, "pc.rom", PC_ROM_SIZE,
+ *   - hw/i386/pc_sysfw.c|57| <<pc_isa_bios_init>> memory_region_init_ram(isa_bios, NULL, "isa-bios", isa_bios_size,
+ *   - hw/i386/x86.c|1158| <<x86_bios_rom_init>> memory_region_init_ram(bios, NULL, "pc.bios", bios_size, &error_fatal);
+ *   - hw/i386/xen/xen-hvm.c|152| <<xen_ram_init>> memory_region_init_ram(&ram_memory, NULL, "xen.ram", block_len,
+ *   - hw/openrisc/openrisc_sim.c|310| <<openrisc_sim_init>> memory_region_init_ram(ram, NULL, "openrisc.ram", ram_size, &error_fatal);
+ *   - hw/openrisc/virt.c|496| <<openrisc_virt_init>> memory_region_init_ram(ram, NULL, "openrisc.ram", ram_size, &error_fatal);
+ *   - hw/riscv/microchip_pfsoc.c|220| <<microchip_pfsoc_soc_realize>> memory_region_init_ram(rsvd0_mem, NULL, "microchip.pfsoc.rsvd0_mem",
+ *   - hw/riscv/microchip_pfsoc.c|227| <<microchip_pfsoc_soc_realize>> memory_region_init_ram(e51_dtim_mem, NULL, "microchip.pfsoc.e51_dtim_mem",
+ *   - hw/riscv/microchip_pfsoc.c|272| <<microchip_pfsoc_soc_realize>> memory_region_init_ram(l2lim_mem, NULL, "microchip.pfsoc.l2lim",
+ *   - hw/riscv/sifive_u.c|546| <<sifive_u_machine_init>> memory_region_init_ram(flash0, NULL, "riscv.sifive.u.flash0",
+ *   - hw/riscv/sifive_u.c|825| <<sifive_u_soc_realize>> memory_region_init_ram(l2lim_mem, NULL, "riscv.sifive.u.l2lim",
+ *   - hw/xen/xen_pt_load_rom.c|67| <<pci_assign_dev_load_option_rom>> memory_region_init_ram(&dev->rom, owner, name, dev->romsize, &error_abort);
+ */
 void memory_region_init_ram(MemoryRegion *mr,
                             Object *owner,
                             const char *name,
diff --git a/system/physmem.c b/system/physmem.c
index a63853a7b..eee18e890 100644
--- a/system/physmem.c
+++ b/system/physmem.c
@@ -1285,6 +1285,11 @@ static int64_t get_file_align(int fd)
     return align;
 }
 
+/*
+ * called by:
+ *   - system/physmem.c|1991| <<qemu_ram_alloc_from_file>> fd = file_ram_open(mem_path, memory_region_name(mr),
+ *   - system/physmem.c|2003| <<qemu_ram_alloc_from_file>> fd = file_ram_open(mem_path, memory_region_name(mr), true,
+ */
 static int file_ram_open(const char *path,
                          const char *region_name,
                          bool readonly,
@@ -1366,6 +1371,47 @@ static int file_ram_open(const char *path,
     return fd;
 }
 
+/*
+ * -object thread-context,id=tc01,node-affinity=0 \
+ * -object memory-backend-file,id=ram01,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=0,policy=bind,prealloc-context=tc01 \
+ * -numa node,nodeid=0,cpus=0-1,memdev=ram01 \
+ * -object thread-context,id=tc02,node-affinity=1 \
+ * -object memory-backend-file,id=ram02,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=1,policy=bind,prealloc-context=tc02 \
+ * -numa node,nodeid=1,cpus=2-3,memdev=ram02 \
+ *
+ * (gdb) bt
+ * #0  file_ram_alloc (block=0x5555570507a0, memory=68719476736, fd=12, truncate=true, offset=0, errp=0x7fffffffd8f0) at ../system/physmem.c:1379
+ * #1  0x0000555555d9d7e6 in qemu_ram_alloc_from_fd (size=68719476736, mr=0x555557245ec0, ram_flags=512, fd=12, offset=0, errp=0x7fffffffd8f0) at ../system/physmem.c:1941
+ * #2  0x0000555555d9da02 in qemu_ram_alloc_from_file (size=68719476736, mr=0x555557245ec0, ram_flags=512, mem_path=0x5555572ceda0 "/dev/hugepages/libvirt/qemu", offset=0,
+ *                                 errp=0x7fffffffd8f0) at ../system/physmem.c:1994
+ * #3  0x0000555555d91a1e in memory_region_init_ram_from_file (mr=0x555557245ec0, owner=0x555557245e50, name=0x555557051f90 "ram-node0", size=68719476736, align=0,
+ *                                 ram_flags=512, path=0x5555572ceda0 "/dev/hugepages/libvirt/qemu", offset=0, errp=0x7fffffffd990) at ../system/memory.c:1620
+ * #4  0x0000555555b7ee52 in file_backend_memory_alloc (backend=0x555557245e50, errp=0x7fffffffd990) at ../backends/hostmem-file.c:89
+ * #5  0x0000555555b7d0de in host_memory_backend_memory_complete (uc=0x555557245e50, errp=0x7fffffffda08) at ../backends/hostmem.c:332
+ * #6  0x0000555555e121a2 in user_creatable_complete (uc=0x555557245e50, errp=0x7fffffffda68) at ../qom/object_interfaces.c:28
+ * #7  0x0000555555e1262e in user_creatable_add_type (type=0x5555562af104 "memory-backend-file", id=0x555557053f10 "ram-node0", qdict=0x5555572d6b70, v=0x5555572d0d90,
+ *                                 errp=0x7fffffffda80) at ../qom/object_interfaces.c:125
+ * #8  0x0000555555e127d4 in user_creatable_add_qapi (options=0x555557053bc0, errp=0x555556fc8340 <error_fatal>) at ../qom/object_interfaces.c:157
+ * #9  0x0000555555b7231c in object_option_foreach_add (type_opt_predicate=0x555555b7295d <object_create_late>) at ../system/vl.c:1792
+ * #10 0x0000555555b729e4 in qemu_create_late_backends () at ../system/vl.c:2011
+ * #11 0x0000555555b76f4c in qemu_init (argc=31, argv=0x7fffffffde38) at ../system/vl.c:3727
+ * #12 0x0000555555e01cbf in main (argc=31, argv=0x7fffffffde38) at ../system/main.c:47
+ *
+ * 一个例子:
+ * host_memory_backend_memory_complete()
+ * -> bc->alloc = file_backend_memory_alloc()
+ *    -> memory_region_init_ram_from_file()
+ *       -> qemu_ram_alloc_from_file()
+ *          -> qemu_ram_alloc_from_fd()
+ *             -> file_ram_alloc() 
+ *                -> qemu_ram_mmap()
+ *                   -> mmap_activate()
+ *                      -> mmap()
+ *                -> "unable to map backing store for guest RAM"
+ *
+ * called by:
+ *   - system/physmem.c|1941| <<qemu_ram_alloc_from_fd>> new_block->host = file_ram_alloc(new_block, size, fd, !file_size, offset, errp);
+ */
 static void *file_ram_alloc(RAMBlock *block,
                             ram_addr_t memory,
                             int fd,
@@ -1762,6 +1808,10 @@ void qemu_ram_msync(RAMBlock *block, ram_addr_t start, ram_addr_t length)
     }
 }
 
+/*
+ * called by:
+ *   - system/physmem.c|1898| <<ram_block_add>> dirty_memory_extend(old_ram_size, new_ram_size);
+ */
 /* Called with ram_list.mutex held */
 static void dirty_memory_extend(ram_addr_t old_ram_size,
                                 ram_addr_t new_ram_size)
@@ -1803,6 +1853,11 @@ static void dirty_memory_extend(ram_addr_t old_ram_size,
     }
 }
 
+/*
+ * called by:
+ *   - system/physmem.c|1957| <<qemu_ram_alloc_from_fd>> ram_block_add(new_block, &local_err);
+ *   - system/physmem.c|2047| <<qemu_ram_alloc_internal>> ram_block_add(new_block, &local_err);
+ */
 static void ram_block_add(RAMBlock *new_block, Error **errp)
 {
     const bool noreserve = qemu_ram_is_noreserve(new_block);
@@ -1892,6 +1947,11 @@ static void ram_block_add(RAMBlock *new_block, Error **errp)
 }
 
 #ifdef CONFIG_POSIX
+/*
+ * called by:
+ *   - system/memory.c|1644| <<memory_region_init_ram_from_fd>> mr->ram_block = qemu_ram_alloc_from_fd(size, mr, ram_flags, fd, offset,
+ *   - system/physmem.c|1994| <<qemu_ram_alloc_from_file>> block = qemu_ram_alloc_from_fd(size, mr, ram_flags, fd, offset, errp);
+ */
 RAMBlock *qemu_ram_alloc_from_fd(ram_addr_t size, MemoryRegion *mr,
                                  uint32_t ram_flags, int fd, off_t offset,
                                  Error **errp)
@@ -1938,6 +1998,12 @@ RAMBlock *qemu_ram_alloc_from_fd(ram_addr_t size, MemoryRegion *mr,
     new_block->used_length = size;
     new_block->max_length = size;
     new_block->flags = ram_flags;
+    /*
+     * RAMBlock *new_block;
+     * -> uint8_t *host;
+     *
+     * 只在这里调用
+     */
     new_block->host = file_ram_alloc(new_block, size, fd, !file_size, offset,
                                      errp);
     if (!new_block->host) {
@@ -1956,6 +2022,22 @@ RAMBlock *qemu_ram_alloc_from_fd(ram_addr_t size, MemoryRegion *mr,
 }
 
 
+/*
+ * called by:
+ *   - system/memory.c|1620| <<memory_region_init_ram_from_file>> mr->ram_block = qemu_ram_alloc_from_file(size, mr, ram_flags, path,
+ *
+ * 一个例子:
+ * host_memory_backend_memory_complete()
+ * -> bc->alloc = file_backend_memory_alloc()
+ *    -> memory_region_init_ram_from_file()
+ *       -> qemu_ram_alloc_from_file()
+ *          -> qemu_ram_alloc_from_fd()
+ *             -> file_ram_alloc()
+ *                -> qemu_ram_mmap()
+ *                   -> mmap_activate()
+ *                      -> mmap()
+ *                -> "unable to map backing store for guest RAM"
+ */
 RAMBlock *qemu_ram_alloc_from_file(ram_addr_t size, MemoryRegion *mr,
                                    uint32_t ram_flags, const char *mem_path,
                                    off_t offset, Error **errp)
@@ -1991,6 +2073,11 @@ RAMBlock *qemu_ram_alloc_from_file(ram_addr_t size, MemoryRegion *mr,
         return NULL;
     }
 
+    /*
+     * called by:
+     *   - system/memory.c|1644| <<memory_region_init_ram_from_fd>> mr->ram_block = qemu_ram_alloc_from_fd(size, mr, ram_flags, fd, offset,
+     *   - system/physmem.c|1994| <<qemu_ram_alloc_from_file>> block = qemu_ram_alloc_from_fd(size, mr, ram_flags, fd, offset, errp);
+     */
     block = qemu_ram_alloc_from_fd(size, mr, ram_flags, fd, offset, errp);
     if (!block) {
         if (created) {
@@ -2004,6 +2091,12 @@ RAMBlock *qemu_ram_alloc_from_file(ram_addr_t size, MemoryRegion *mr,
 }
 #endif
 
+/*
+ * called by:
+ *   - system/physmem.c|2059| <<qemu_ram_alloc_from_ptr>> return qemu_ram_alloc_internal(size, size, NULL, host, RAM_PREALLOC, mr,
+ *   - system/physmem.c|2067| <<qemu_ram_alloc>> return qemu_ram_alloc_internal(size, size, NULL, NULL, ram_flags, mr, errp);
+ *   - system/physmem.c|2076| <<qemu_ram_alloc_resizeable>> return qemu_ram_alloc_internal(size, maxsz, resized, NULL,
+ */
 static
 RAMBlock *qemu_ram_alloc_internal(ram_addr_t size, ram_addr_t max_size,
                                   void (*resized)(const char*,
diff --git a/system/runstate.c b/system/runstate.c
index ea9d6c2a3..c49776626 100644
--- a/system/runstate.c
+++ b/system/runstate.c
@@ -277,6 +277,18 @@ struct VMChangeStateEntry {
     int priority;
 };
 
+/*
+ * 在以下使用vm_change_state_head:
+ *   - system/runstate.c|280| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, VMChangeStateEntry) vm_change_state_head = QTAILQ_HEAD_INITIALIZER(vm_change_state_head);
+ *   - system/runstate.c|281| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, VMChangeStateEntry) vm_change_state_head = QTAILQ_HEAD_INITIALIZER(vm_change_state_head);
+ *   - system/runstate.c|339| <<qemu_add_vm_change_state_handler_prio_full>> QTAILQ_FOREACH(other, &vm_change_state_head, entries) {
+ *   - system/runstate.c|346| <<qemu_add_vm_change_state_handler_prio_full>> QTAILQ_INSERT_TAIL(&vm_change_state_head, e, entries);
+ *   - system/runstate.c|358| <<qemu_del_vm_change_state_handler>> QTAILQ_REMOVE(&vm_change_state_head, e, entries);
+ *   - system/runstate.c|369| <<vm_state_notify>> QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next) {
+ *   - system/runstate.c|375| <<vm_state_notify>> QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next) {
+ *   - system/runstate.c|379| <<vm_state_notify>> QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next) {
+ *   - system/runstate.c|385| <<vm_state_notify>> QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next) {
+ */
 static QTAILQ_HEAD(, VMChangeStateEntry) vm_change_state_head =
     QTAILQ_HEAD_INITIALIZER(vm_change_state_head);
 
@@ -316,6 +328,11 @@ VMChangeStateEntry *qemu_add_vm_change_state_handler_prio(
  *
  * Returns: an entry to be freed using qemu_del_vm_change_state_handler()
  */
+/*
+ * called by:
+ *   - hw/core/vm-change-state-handler.c|78| <<qdev_add_vm_change_state_handler_full>> return qemu_add_vm_change_state_handler_prio_full(cb, prepare_cb, opaque,
+ *   - system/runstate.c|298| <<qemu_add_vm_change_state_handler_prio>> return qemu_add_vm_change_state_handler_prio_full(cb, NULL, opaque, priority);
+ */
 VMChangeStateEntry *
 qemu_add_vm_change_state_handler_prio_full(VMChangeStateHandler *cb,
                                            VMChangeStateHandler *prepare_cb,
@@ -338,6 +355,18 @@ qemu_add_vm_change_state_handler_prio_full(VMChangeStateHandler *cb,
         }
     }
 
+    /*
+     * 在以下使用vm_change_state_head:
+     *   - system/runstate.c|280| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, VMChangeStateEntry) vm_change_state_head = QTAILQ_HEAD_INITIALIZER(vm_change_state_head);
+     *   - system/runstate.c|281| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, VMChangeStateEntry) vm_change_state_head = QTAILQ_HEAD_INITIALIZER(vm_change_state_head);
+     *   - system/runstate.c|339| <<qemu_add_vm_change_state_handler_prio_full>> QTAILQ_FOREACH(other, &vm_change_state_head, entries) {
+     *   - system/runstate.c|346| <<qemu_add_vm_change_state_handler_prio_full>> QTAILQ_INSERT_TAIL(&vm_change_state_head, e, entries);
+     *   - system/runstate.c|358| <<qemu_del_vm_change_state_handler>> QTAILQ_REMOVE(&vm_change_state_head, e, entries);
+     *   - system/runstate.c|369| <<vm_state_notify>> QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next) {
+     *   - system/runstate.c|375| <<vm_state_notify>> QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next) {
+     *   - system/runstate.c|379| <<vm_state_notify>> QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next) {
+     *   - system/runstate.c|385| <<vm_state_notify>> QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next) {
+     */
     QTAILQ_INSERT_TAIL(&vm_change_state_head, e, entries);
     return e;
 }
@@ -774,6 +803,10 @@ static bool main_loop_should_exit(int *status)
     return false;
 }
 
+/*
+ * called by:
+ *   - system/main.c|37| <<qemu_default_main>> status = qemu_main_loop();
+ */
 int qemu_main_loop(void)
 {
     int status = EXIT_SUCCESS;
diff --git a/system/vl.c b/system/vl.c
index 2bcd9efb9..2641e1bcf 100644
--- a/system/vl.c
+++ b/system/vl.c
@@ -2000,6 +2000,10 @@ static bool object_create_late(const char *type)
     return !object_create_early(type) && !object_create_pre_sandbox(type);
 }
 
+/*
+ * called by:
+ *   - system/vl.c|3727| <<qemu_init>> qemu_create_late_backends();
+ */
 static void qemu_create_late_backends(void)
 {
     if (qtest_chrdev) {
diff --git a/target/i386/cpu-dump.c b/target/i386/cpu-dump.c
index 40697064d..44ba29a70 100644
--- a/target/i386/cpu-dump.c
+++ b/target/i386/cpu-dump.c
@@ -93,6 +93,47 @@ static const char *cc_op_str[CC_OP_NB] = {
     "CLR",
 };
 
+/*
+ *  89 // segment descriptor fields
+ *  90 #define DESC_G_SHIFT    23
+ *  91 #define DESC_G_MASK     (1 << DESC_G_SHIFT)
+ *  92 #define DESC_B_SHIFT    22
+ *  93 #define DESC_B_MASK     (1 << DESC_B_SHIFT)
+ *  94 #define DESC_L_SHIFT    21 // x86_64 only : 64 bit code segment
+ *  95 #define DESC_L_MASK     (1 << DESC_L_SHIFT)
+ *  96 #define DESC_AVL_SHIFT  20
+ *  97 #define DESC_AVL_MASK   (1 << DESC_AVL_SHIFT)
+ *  98 #define DESC_P_SHIFT    15
+ *  99 #define DESC_P_MASK     (1 << DESC_P_SHIFT)
+ * 100 #define DESC_DPL_SHIFT  13
+ * 101 #define DESC_DPL_MASK   (3 << DESC_DPL_SHIFT)
+ * 102 #define DESC_S_SHIFT    12
+ * 103 #define DESC_S_MASK     (1 << DESC_S_SHIFT)
+ * 104 #define DESC_TYPE_SHIFT 8
+ * 105 #define DESC_TYPE_MASK  (15 << DESC_TYPE_SHIFT)
+ * 106 #define DESC_A_MASK     (1 << 8)
+ *
+ * ES =0000 0000000000000000 ffffffff 00c00100
+ * CS =0010 0000000000000000 ffffffff 00a09b00 DPL=0 CS64 [-RA]
+ * SS =0018 0000000000000000 ffffffff 00c09300 DPL=0 DS   [-WA]
+ * DS =0000 0000000000000000 ffffffff 00c00100
+ * FS =0000 0000000000000000 ffffffff 00c00100
+ * GS =0000 ffffa0a49fc00000 ffffffff 00c00100
+ * LDT=0000 0000000000000000 ffffffff 00c00000
+ * TR =0040 fffffe4e5f540000 00004087 00008b00 DPL=0 TSS64-busy
+ *
+ * called by:
+ *   - target/i386/cpu-dump.c|419| <<x86_cpu_dump_state>> cpu_x86_dump_seg_cache(env, f, seg_name[i], &env->segs[i]);
+ *   - target/i386/cpu-dump.c|421| <<x86_cpu_dump_state>> cpu_x86_dump_seg_cache(env, f, "LDT", &env->ldt);
+ *   - target/i386/cpu-dump.c|422| <<x86_cpu_dump_state>> cpu_x86_dump_seg_cache(env, f, "TR", &env->tr);
+ *
+ * 1289 typedef struct SegmentCache {
+ * 1290     uint32_t selector;
+ * 1291     target_ulong base;
+ * 1292     uint32_t limit;
+ * 1293     uint32_t flags;
+ * 1294 } SegmentCache;
+ */
 static void
 cpu_x86_dump_seg_cache(CPUX86State *env, FILE *f,
                        const char *name, struct SegmentCache *sc)
diff --git a/target/i386/cpu.h b/target/i386/cpu.h
index ef987f344..f875050d2 100644
--- a/target/i386/cpu.h
+++ b/target/i386/cpu.h
@@ -1903,6 +1903,18 @@ struct ArchCPU {
     CPUX86State env;
     VMChangeStateEntry *vmsentry;
 
+    /*
+     * 在以下使用X86CPU->ucode_rev:
+     *   - target/i386/cpu.c|7878| <<global>> DEFINE_PROP_UINT64("ucode-rev", X86CPU, ucode_rev, 0),
+     *   - target/i386/cpu.c|7321| <<x86_cpu_realizefn>> if (cpu->ucode_rev == 0) {
+     *   - target/i386/cpu.c|7328| <<x86_cpu_realizefn>> cpu->ucode_rev = 0x01000065;
+     *   - target/i386/cpu.c|7330| <<x86_cpu_realizefn>> cpu->ucode_rev = 0x100000000ULL;
+     *   - target/i386/hvf/x86_emu.c|681| <<simulate_rdmsr>> val = cpu->ucode_rev;
+     *   - target/i386/kvm/kvm-cpu.c|47| <<kvm_cpu_realizefn>> if (cpu->ucode_rev == 0) {
+     *   - target/i386/kvm/kvm-cpu.c|48| <<kvm_cpu_realizefn>> cpu->ucode_rev =
+     *   - target/i386/kvm/kvm.c|3233| <<kvm_init_msrs>> kvm_msr_entry_add(cpu, MSR_IA32_UCODE_REV, cpu->ucode_rev);
+     *   - target/i386/tcg/sysemu/misc_helper.c|451| <<helper_rdmsr>> val = x86_cpu->ucode_rev;
+     */
     uint64_t ucode_rev;
 
     uint32_t hyperv_spinlock_attempts;
diff --git a/target/i386/kvm/kvm.c b/target/i386/kvm/kvm.c
index 4ce80555b..734b7d670 100644
--- a/target/i386/kvm/kvm.c
+++ b/target/i386/kvm/kvm.c
@@ -4286,6 +4286,10 @@ static int kvm_get_mp_state(X86CPU *cpu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|4753| <<kvm_arch_get_registers>> ret = kvm_get_apic(cpu);
+ */
 static int kvm_get_apic(X86CPU *cpu)
 {
     DeviceState *apic = cpu->apic_state;
@@ -4303,6 +4307,72 @@ static int kvm_get_apic(X86CPU *cpu)
     return 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  kvm_cpu_synchronize_post_init (cpu=0x555557ffdca0) at ../accel/kvm/kvm-all.c:2751
+ * #1  0x0000555555b4d8cc in cpu_synchronize_post_init (cpu=0x555557ffdca0) at ../system/cpus.c:185
+ * #2  0x0000555555860dfd in cpu_common_realizefn (dev=0x555557ffdca0, errp=0x7fffffffc450) at ../hw/core/cpu-common.c:213
+ * #3  0x0000555555c66ea8 in x86_cpu_realizefn (dev=0x555557ffdca0, errp=0x7fffffffc510) at ../target/i386/cpu.c:7462
+ * #4  0x0000555555c604c0 in max_x86_cpu_realize (dev=0x555557ffdca0, errp=0x7fffffffc510) at ../target/i386/cpu.c:4956
+ * #5  0x0000555555ddbbaa in device_set_realized (obj=0x555557ffdca0, value=true, errp=0x7fffffffc850) at ../hw/core/qdev.c:510
+ * #6  0x0000555555de5938 in property_set_bool (obj=0x555557ffdca0, v=0x5555577627f0, name=0x555556209d19 "realized", opaque=0x555556fec8c0, errp=0x7fffffffc850) at ../qom/object.c:2305
+ * #7  0x0000555555de393e in object_property_set (obj=0x555557ffdca0, name=0x555556209d19 "realized", v=0x5555577627f0, errp=0x7fffffffc850) at ../qom/object.c:1435
+ * #8  0x0000555555de7d2a in object_property_set_qobject (obj=0x555557ffdca0, name=0x555556209d19 "realized", value=0x555557b6dac0, errp=0x7fffffffc850) at ../qom/qom-qobject.c:28
+ * #9  0x0000555555de3ca3 in object_property_set_bool (obj=0x555557ffdca0, name=0x555556209d19 "realized", value=true, errp=0x7fffffffc850) at ../qom/object.c:1504
+ * #10 0x0000555555ddb3aa in qdev_realize (dev=0x555557ffdca0, bus=0x0, errp=0x7fffffffc850) at ../hw/core/qdev.c:292
+ * #11 0x0000555555b558ae in qdev_device_add_from_qdict (opts=0x555557e41390, from_json=false, errp=0x7fffffffc850) at ../system/qdev-monitor.c:719
+ * #12 0x0000555555b5594c in qdev_device_add (opts=0x555557bf44a0, errp=0x7fffffffc850) at ../system/qdev-monitor.c:738
+ * #13 0x0000555555b55f60 in qmp_device_add (qdict=0x555557e40370, ret_data=0x0, errp=0x7fffffffc850) at ../system/qdev-monitor.c:860
+ * #14 0x0000555555b56370 in hmp_device_add (mon=0x555557242b30, qdict=0x555557e40370) at ../system/qdev-monitor.c:968
+ * #15 0x0000555555bb0241 in handle_hmp_command_exec (mon=0x555557242b30, cmd=0x555556e732e0 <hmp_cmds+1920>, qdict=0x555557e40370) at ../monitor/hmp.c:1106
+ * #16 0x0000555555bb046e in handle_hmp_command (mon=0x555557242b30, cmdline=0x55555727c88b "host-x86_64-cpu,id=core4,socket-id=0,core-id=2,thread-id=0") at ../monitor/hmp.c:1158
+ * #17 0x0000555555bad95a in monitor_command_cb (opaque=0x555557242b30, cmdline=0x55555727c880 "device_add host-x86_64-cpu,id=core4,socket-id=0,core-id=2,thread-id=0", readline_opaque=0x0)
+ *                                               at ../monitor/hmp.c:47
+ * #18 0x0000555556001e9f in readline_handle_byte (rs=0x55555727c880, ch=13) at ../util/readline.c:419
+ * #19 0x0000555555bb0f19 in monitor_read (opaque=0x555557242b30, buf=0x7fffffffcab0 "\r\322\377\377\377\177", size=1) at ../monitor/hmp.c:1390
+ * #20 0x0000555555f06291 in qemu_chr_be_write_impl (s=0x555557240470, buf=0x7fffffffcab0 "\r\322\377\377\377\177", len=1) at ../chardev/char.c:202
+ * #21 0x0000555555f062f5 in qemu_chr_be_write (s=0x555557240470, buf=0x7fffffffcab0 "\r\322\377\377\377\177", len=1) at ../chardev/char.c:214
+ * #22 0x0000555555f08ca2 in fd_chr_read (chan=0x5555572403e0, cond=G_IO_IN, opaque=0x555557240470) at ../chardev/char-fd.c:72
+ * #23 0x0000555555df4fa8 in qio_channel_fd_source_dispatch (source=0x5555573ee7a0, callback=0x555555f08b78 <fd_chr_read>, user_data=0x555557240470) at ../io/channel-watch.c:84
+ * #24 0x00007ffff69ef119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #25 0x0000555555feb300 in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #26 0x0000555555feb37a in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:313
+ * #27 0x0000555555feb47f in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #28 0x0000555555b5be98 in qemu_main_loop () at ../system/runstate.c:782
+ * #29 0x0000555555dd6e12 in qemu_default_main () at ../system/main.c:37
+ * #30 0x0000555555dd6e4d in main (argc=22, argv=0x7fffffffdd78) at ../system/main.c:48
+ *
+ * Thread 35 "CPU 2/KVM" hit Breakpoint 5, do_kvm_cpu_synchronize_post_init (cpu=0x555557ffdca0, arg=...) at ../accel/kvm/kvm-all.c:2740
+ * 2740	    int ret = kvm_arch_put_registers(cpu, KVM_PUT_FULL_STATE);
+ * (gdb) bt
+ * #0  do_kvm_cpu_synchronize_post_init (cpu=0x555557ffdca0, arg=...) at ../accel/kvm/kvm-all.c:2740
+ * #1  0x0000555555863186 in process_queued_cpu_work (cpu=0x555557ffdca0) at ../cpu-common.c:360
+ * #2  0x0000555555b4deb4 in qemu_wait_io_event_common (cpu=0x555557ffdca0) at ../system/cpus.c:419
+ * #3  0x0000555555b4df46 in qemu_wait_io_event (cpu=0x555557ffdca0) at ../system/cpus.c:437
+ * #4  0x0000555555dca699 in kvm_vcpu_thread_fn (arg=0x555557ffdca0) at ../accel/kvm/kvm-accel-ops.c:56
+ * #5  0x0000555555fd1784 in qemu_thread_start (args=0x5555574bea40) at ../util/qemu-thread-posix.c:541
+ * #6  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #7  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ *
+ * Thread 35 "CPU 2/KVM" hit Breakpoint 3, kvm_put_vcpu_events (cpu=0x555557ffdca0, level=3) at ../target/i386/kvm/kvm.c:4307
+ * 4307	{
+ * (gdb) bt
+ * #0  kvm_put_vcpu_events (cpu=0x555557ffdca0, level=3) at ../target/i386/kvm/kvm.c:4307
+ * #1  0x0000555555c00838 in kvm_arch_put_registers (cpu=0x555557ffdca0, level=3) at ../target/i386/kvm/kvm.c:4620
+ * #2  0x0000555555dc74f7 in do_kvm_cpu_synchronize_post_init (cpu=0x555557ffdca0, arg=...) at ../accel/kvm/kvm-all.c:2740
+ * #3  0x0000555555863186 in process_queued_cpu_work (cpu=0x555557ffdca0) at ../cpu-common.c:360
+ * #4  0x0000555555b4deb4 in qemu_wait_io_event_common (cpu=0x555557ffdca0) at ../system/cpus.c:419
+ * #5  0x0000555555b4df46 in qemu_wait_io_event (cpu=0x555557ffdca0) at ../system/cpus.c:437
+ * #6  0x0000555555dca699 in kvm_vcpu_thread_fn (arg=0x555557ffdca0) at ../accel/kvm/kvm-accel-ops.c:56
+ * #7  0x0000555555fd1784 in qemu_thread_start (args=0x5555574bea40) at ../util/qemu-thread-posix.c:541
+ * #8  0x00007ffff5395ea5 in start_thread () at /lib64/libpthread.so.0
+ * #9  0x00007ffff50be9fd in clone () at /lib64/libc.so.6
+ */
+
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|4620| <<kvm_arch_put_registers>> ret = kvm_put_vcpu_events(x86_cpu, level);
+ */
 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
 {
     CPUState *cs = CPU(cpu);
@@ -4551,6 +4621,12 @@ static int kvm_get_nested_state(X86CPU *cpu)
     return ret;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2734| <<do_kvm_cpu_synchronize_post_reset>> int ret = kvm_arch_put_registers(cpu, KVM_PUT_RESET_STATE);
+ *   - accel/kvm/kvm-all.c|2751| <<do_kvm_cpu_synchronize_post_init>> int ret = kvm_arch_put_registers(cpu, KVM_PUT_FULL_STATE);
+ *   - accel/kvm/kvm-all.c|2853| <<kvm_cpu_exec>> ret = kvm_arch_put_registers(cpu, KVM_PUT_RUNTIME_STATE);
+ */
 int kvm_arch_put_registers(CPUState *cpu, int level)
 {
     X86CPU *x86_cpu = X86_CPU(cpu);
@@ -4848,6 +4924,10 @@ MemTxAttrs kvm_arch_post_run(CPUState *cpu, struct kvm_run *run)
     return cpu_get_mem_attrs(env);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2849| <<kvm_cpu_exec>> if (kvm_arch_process_async_events(cpu)) {
+ */
 int kvm_arch_process_async_events(CPUState *cs)
 {
     X86CPU *cpu = X86_CPU(cs);
@@ -5124,6 +5204,10 @@ void kvm_arch_update_guest_debug(CPUState *cpu, struct kvm_guest_debug *dbg)
     }
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|5258| <<kvm_filter_msr>> if (!kvm_install_msr_filters(s)) {
+ */
 static bool kvm_install_msr_filters(KVMState *s)
 {
     uint64_t zero = 0;
@@ -5162,6 +5246,10 @@ static bool kvm_install_msr_filters(KVMState *s)
     return true;
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|2707| <<kvm_arch_init>> r = kvm_filter_msr(s, MSR_CORE_THREAD_COUNT, kvm_rdmsr_core_thread_count, NULL);
+ */
 bool kvm_filter_msr(KVMState *s, uint32_t msr, QEMURDMSRHandler *rdmsr,
                     QEMUWRMSRHandler *wrmsr)
 {
diff --git a/util/bitmap.c b/util/bitmap.c
index 8d12e90a5..7894b149a 100644
--- a/util/bitmap.c
+++ b/util/bitmap.c
@@ -157,6 +157,30 @@ int slow_bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
     return result != 0;
 }
 
+/*
+ * called by:
+ *   - ui/vnc.c|660| <<global>> bitmap_set(dirty[y], x / VNC_DIRTY_PIXELS_PER_BIT, DIV_ROUND_UP(w, VNC_DIRTY_PIXELS_PER_BIT));
+ *   - backends/hostmem.c|148| <<host_memory_backend_set_host_nodes>> bitmap_set(backend->host_nodes, l->value, 1);
+ *   - block/iscsi.c|512| <<iscsi_allocmap_update>> bitmap_set(iscsilun->allocmap, cl_num_expanded, nb_cls_expanded);
+ *   - block/iscsi.c|524| <<iscsi_allocmap_update>> bitmap_set(iscsilun->allocmap_valid, cl_num_shrunk, nb_cls_shrunk);
+ *   - block/mirror.c|221| <<mirror_iteration_done>> bitmap_set(s->cow_bitmap, chunk_num, nb_chunks);
+ *   - block/mirror.c|561| <<mirror_iteration>> bitmap_set(s->in_flight_bitmap, offset / s->granularity, nb_chunks);
+ *   - block/mirror.c|1497| <<active_write_prepare>> bitmap_set(s->in_flight_bitmap, start_chunk, end_chunk - start_chunk);
+ *   - block/parallels.c|178| <<parallels_set_bat_entry>> bitmap_set(s->bat_dirty_bmap, bat_entry_off(index) / s->bat_dirty_block, 1);
+ *   - block/parallels.c|194| <<mark_used>> bitmap_set(bitmap, cluster_index, count);
+ *   - crypto/block-luks.c|1783| <<qcrypto_block_luks_amend_erase_keyslots>> bitmap_set(&slots_to_erase_bitmap, i, 1);
+ *   - hw/core/platform-bus.c|91| <<platform_bus_count_irqs>> bitmap_set(pbus->used_irqs, i, 1);
+ *   - hw/ppc/spapr_irq.c|63| <<spapr_irq_msi_alloc>> bitmap_set(spapr->irq_map, irq, num);
+ *   - hw/ppc/spapr_ovec.c|142| <<guest_byte_to_bitmap>> bitmap_set(bitmap, bitmap_offset + i, 1);
+ *   - hw/virtio/virtio-mem.c|439| <<virtio_mem_set_range_plugged>> bitmap_set(vmem->bitmap, bit, nbits);
+ *   - hw/xen/xen-mapcache.c|246| <<xen_remap_bucket>> bitmap_set(entry->valid_mapping, i, 1);
+ *   - include/exec/ram_addr.h|59| <<clear_bmap_set>> bitmap_set(rb->clear_bmap, start >> shift, clear_bmap_size(npages, shift));
+ *   - include/hw/ppc/xive.h|328| <<xive_source_irq_set_lsi>> bitmap_set(xsrc->lsi_map, srcno, 1);
+ *   - migration/ram.c|2863| <<ram_list_init_bitmaps>> bitmap_set(block->bmap, 0, pages);
+ *   - tests/unit/test-bitmap.c|42| <<check_bitmap_copy_with_offset>> bitmap_set(bmap1, 100, 145);
+ *   - tests/unit/test-bitmap.c|122| <<check_bitmap_set>> bitmap_set_case(bitmap_set);
+ *   - ui/vnc.c|2987| <<vnc_refresh_lossy_rect>> bitmap_set(vs->dirty[y + j], x / VNC_DIRTY_PIXELS_PER_BIT, VNC_STAT_RECT / VNC_DIRTY_PIXELS_PER_BIT);
+ */
 void bitmap_set(unsigned long *map, long start, long nr)
 {
     unsigned long *p = map + BIT_WORD(start);
diff --git a/util/main-loop.c b/util/main-loop.c
index 797b640c4..8fad8e3d8 100644
--- a/util/main-loop.c
+++ b/util/main-loop.c
@@ -293,6 +293,10 @@ static void glib_pollfds_poll(void)
 
 #define MAX_MAIN_LOOP_SPIN (1000)
 
+/*
+ * called by:
+ *   - util/main-loop.c|596| <<main_loop_wait>> ret = os_host_main_loop_wait(timeout_ns);
+ */
 static int os_host_main_loop_wait(int64_t timeout)
 {
     GMainContext *context = g_main_context_default();
@@ -305,6 +309,9 @@ static int os_host_main_loop_wait(int64_t timeout)
     qemu_mutex_unlock_iothread();
     replay_mutex_unlock();
 
+    /*
+     * 这里就是等
+     */
     ret = qemu_poll_ns((GPollFD *)gpollfds->data, gpollfds->len, timeout);
 
     replay_mutex_lock();
@@ -547,6 +554,14 @@ static int os_host_main_loop_wait(int64_t timeout)
 }
 #endif
 
+/*
+ * 在以下使用main_loop_poll_notifiers:
+ *   - util/main-loop.c|550| <<global>> static NotifierList main_loop_poll_notifiers = NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+ *   - util/main-loop.c|551| <<global>> NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+ *   - util/main-loop.c|555| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+ *   - util/main-loop.c|580| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+ *   - util/main-loop.c|594| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+ */
 static NotifierList main_loop_poll_notifiers =
     NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
 
@@ -560,6 +575,31 @@ void main_loop_poll_remove_notifier(Notifier *notify)
     notifier_remove(notify);
 }
 
+/*
+ * called by:
+ *   - qemu-img.c|2148| <<convert_do_copy>> main_loop_wait(false);
+ *   - qemu-img.c|4743| <<img_bench>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|651| <<do_aio_readv>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|665| <<do_aio_writev>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|1884| <<do_aio_zone_append>> main_loop_wait(false);
+ *   - qemu-io-cmds.c|2632| <<sleep_f>> main_loop_wait(false);
+ *   - qemu-io.c|453| <<command_loop>> main_loop_wait(false);
+ *   - qemu-nbd.c|1209| <<main>> main_loop_wait(false);
+ *   - scsi/qemu-pr-helper.c|1069| <<main>> main_loop_wait(false);
+ *   - storage-daemon/qemu-storage-daemon.c|434| <<main>> main_loop_wait(false);
+ *   - system/runstate.c|782| <<qemu_main_loop>> main_loop_wait(false);
+ *   - tests/qtest/fuzz/fuzz.c|50| <<flush_events>> main_loop_wait(false);
+ *   - tests/qtest/fuzz/fuzz.c|57| <<fuzz_reset>> main_loop_wait(true);
+ *   - tests/unit/test-char.c|34| <<main_loop>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|837| <<char_socket_server_test>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|860| <<char_socket_server_test>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|1044| <<char_socket_client_test>> main_loop_wait(false);
+ *   - tests/unit/test-char.c|1067| <<char_socket_client_test>> main_loop_wait(false);
+ *   - tests/unit/test-replication.c|78| <<test_blk_read>> main_loop_wait(false);
+ *   - tests/unit/test-replication.c|115| <<test_blk_write>> main_loop_wait(false);
+ *   - tests/unit/test-util-filemonitor.c|85| <<qemu_file_monitor_test_event_loop>> main_loop_wait(true);
+ *   - ui/gtk-clipboard.c|55| <<gd_clipboard_get_data>> main_loop_wait(false);
+ */
 void main_loop_wait(int nonblocking)
 {
     MainLoopPoll mlpoll = {
@@ -576,6 +616,14 @@ void main_loop_wait(int nonblocking)
 
     /* poll any events */
     g_array_set_size(gpollfds, 0); /* reset for new iteration */
+    /*
+     * 在以下使用main_loop_poll_notifiers:
+     *   - util/main-loop.c|550| <<global>> static NotifierList main_loop_poll_notifiers = NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|551| <<global>> NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|555| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+     *   - util/main-loop.c|580| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     *   - util/main-loop.c|594| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     */
     /* XXX: separate device handlers from system ones */
     notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
 
@@ -591,6 +639,14 @@ void main_loop_wait(int nonblocking)
 
     ret = os_host_main_loop_wait(timeout_ns);
     mlpoll.state = ret < 0 ? MAIN_LOOP_POLL_ERR : MAIN_LOOP_POLL_OK;
+    /*
+     * 在以下使用main_loop_poll_notifiers:
+     *   - util/main-loop.c|550| <<global>> static NotifierList main_loop_poll_notifiers = NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|551| <<global>> NOTIFIER_LIST_INITIALIZER(main_loop_poll_notifiers);
+     *   - util/main-loop.c|555| <<main_loop_poll_add_notifier>> notifier_list_add(&main_loop_poll_notifiers, notify);
+     *   - util/main-loop.c|580| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     *   - util/main-loop.c|594| <<main_loop_wait>> notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
+     */
     notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);
 
     if (icount_enabled()) {
diff --git a/util/mmap-alloc.c b/util/mmap-alloc.c
index ed14f9c64..13e305de0 100644
--- a/util/mmap-alloc.c
+++ b/util/mmap-alloc.c
@@ -174,6 +174,9 @@ static void *mmap_reserve(size_t size, int fd)
     flags |= MAP_ANONYMOUS;
 #endif
 
+    /*
+     * 没用fd!!!
+     */
     return mmap(0, size, PROT_NONE, flags, fd, 0);
 }
 
@@ -181,6 +184,10 @@ static void *mmap_reserve(size_t size, int fd)
  * Activate memory in a reserved region from the given fd (if any), to make
  * it accessible.
  */
+/*
+ * called by:
+ *   - util/mmap-alloc.c|320| <<qemu_ram_mmap>> ptr = mmap_activate(guardptr + offset, size, fd, qemu_map_flags,
+ */
 static void *mmap_activate(void *ptr, size_t size, int fd,
                            uint32_t qemu_map_flags, off_t map_offset)
 {
@@ -244,6 +251,52 @@ static inline size_t mmap_guard_pagesize(int fd)
 #endif
 }
 
+/*
+ * map bios的例子:
+ * (gdb) bt
+ * #0  qemu_ram_mmap (fd=-1, size=262144, align=2097152, qemu_map_flags=0, map_offset=0) at ../util/mmap-alloc.c:253
+ * #1  0x0000555555ffae09 in qemu_anon_ram_alloc (size=262144, alignment=0x5555570543a0, shared=false, noreserve=false) at ../util/oslib-posix.c:193
+ * #2  0x0000555555d9d25e in ram_block_add (new_block=0x5555573d5380, errp=0x7fffffffd638) at ../system/physmem.c:1830
+ * #3  0x0000555555d9dbf9 in qemu_ram_alloc_internal (size=262144, max_size=262144, resized=0x0, host=0x0, ram_flags=0, mr=0x555557054310, errp=0x7fffffffd6d0) at ../system/physmem.c:2034
+ * #4  0x0000555555d9dd06 in qemu_ram_alloc (size=262144, ram_flags=0, mr=0x555557054310, errp=0x7fffffffd6d0) at ../system/physmem.c:2054
+ * #5  0x0000555555d91807 in memory_region_init_ram_flags_nomigrate (mr=0x555557054310, owner=0x0, name=0x5555562131c0 "pc.bios", size=262144, ram_flags=0, errp=0x7fffffffd768) at ../system/memory.c:1570
+ * #6  0x0000555555d91779 in memory_region_init_ram_nomigrate (mr=0x555557054310, owner=0x0, name=0x5555562131c0 "pc.bios", size=262144, errp=0x7fffffffd768) at ../system/memory.c:1555
+ * #7  0x0000555555d9753f in memory_region_init_ram (mr=0x555557054310, owner=0x0, name=0x5555562131c0 "pc.bios", size=262144, errp=0x555556fc8340 <error_fatal>) at ../system/memory.c:3574
+ * #8  0x0000555555c35872 in x86_bios_rom_init (ms=0x5555572bb3d0, default_firmware=0x55555621e06e "bios.bin", rom_memory=0x555557054b60, isapc_ram_fw=false) at ../hw/i386/x86.c:1158
+ * #9  0x0000555555c5fe82 in pc_system_firmware_init (pcms=0x5555572bb3d0, rom_memory=0x555557054b60) at ../hw/i386/pc_sysfw.c:232
+ * #10 0x0000555555c5cfef in pc_memory_init (pcms=0x5555572bb3d0, system_memory=0x5555571832e0, rom_memory=0x555557054b60, pci_hole64_size=34359738368) at ../hw/i386/pc.c:1037
+ * #11 0x0000555555c4652f in pc_q35_init (machine=0x5555572bb3d0) at ../hw/i386/pc_q35.c:222
+ * #12 0x0000555555c46e8a in pc_init_v8_2 (machine=0x5555572bb3d0) at ../hw/i386/pc_q35.c:392
+ * #13 0x00005555559232c5 in machine_run_board_init (machine=0x5555572bb3d0, mem_path=0x0, errp=0x7fffffffdb10) at ../hw/core/machine.c:1509
+ * #14 0x0000555555b74388 in qemu_init_board () at ../system/vl.c:2613
+ * #15 0x0000555555b7460d in qmp_x_exit_preconfig (errp=0x555556fc8340 <error_fatal>) at ../system/vl.c:2704
+ * #16 0x0000555555b76fec in qemu_init (argc=31, argv=0x7fffffffde38) at ../system/vl.c:3753
+ * #17 0x0000555555e01cbf in main (argc=31, argv=0x7fffffffde38) at ../system/main.c:47
+ *
+ * map rom的例子:
+ * (gdb) bt
+ * #0  qemu_ram_mmap (fd=-1, size=131072, align=2097152, qemu_map_flags=0, map_offset=0) at ../util/mmap-alloc.c:253
+ * #1  0x0000555555ffae09 in qemu_anon_ram_alloc (size=131072, alignment=0x5555570533d0, shared=false, noreserve=false) at ../util/oslib-posix.c:193
+ * #2  0x0000555555d9d25e in ram_block_add (new_block=0x5555572c1d40, errp=0x7fffffffd708) at ../system/physmem.c:1830
+ * #3  0x0000555555d9dbf9 in qemu_ram_alloc_internal (size=131072, max_size=131072, resized=0x0, host=0x0, ram_flags=0, mr=0x555557053340, errp=0x7fffffffd7a0) at ../system/physmem.c:2034
+ * #4  0x0000555555d9dd06 in qemu_ram_alloc (size=131072, ram_flags=0, mr=0x555557053340, errp=0x7fffffffd7a0) at ../system/physmem.c:2054
+ * #5  0x0000555555d91807 in memory_region_init_ram_flags_nomigrate (mr=0x555557053340, owner=0x0, name=0x55555621d44f "pc.rom", size=131072, ram_flags=0, errp=0x7fffffffd838) at ../system/memory.c:1570
+ * #6  0x0000555555d91779 in memory_region_init_ram_nomigrate (mr=0x555557053340, owner=0x0, name=0x55555621d44f "pc.rom", size=131072, errp=0x7fffffffd838) at ../system/memory.c:1555
+ * #7  0x0000555555d9753f in memory_region_init_ram (mr=0x555557053340, owner=0x0, name=0x55555621d44f "pc.rom", size=131072, errp=0x555556fc8340 <error_fatal>) at ../system/memory.c:3574
+ * #8  0x0000555555c5d021 in pc_memory_init (pcms=0x5555572bb3d0, system_memory=0x5555571832e0, rom_memory=0x555557054b60, pci_hole64_size=34359738368) at ../hw/i386/pc.c:1040
+ * #9  0x0000555555c4652f in pc_q35_init (machine=0x5555572bb3d0) at ../hw/i386/pc_q35.c:222
+ * #10 0x0000555555c46e8a in pc_init_v8_2 (machine=0x5555572bb3d0) at ../hw/i386/pc_q35.c:392
+ * #11 0x00005555559232c5 in machine_run_board_init (machine=0x5555572bb3d0, mem_path=0x0, errp=0x7fffffffdb10) at ../hw/core/machine.c:1509
+ * #12 0x0000555555b74388 in qemu_init_board () at ../system/vl.c:2613
+ * #13 0x0000555555b7460d in qmp_x_exit_preconfig (errp=0x555556fc8340 <error_fatal>) at ../system/vl.c:2704
+ * #14 0x0000555555b76fec in qemu_init (argc=31, argv=0x7fffffffde38) at ../system/vl.c:3753
+ * #15 0x0000555555e01cbf in main (argc=31, argv=0x7fffffffde38) at ../system/main.c:47
+ */
+/*
+ * called by:
+ *   - system/physmem.c|1437| <<file_ram_alloc>> area = qemu_ram_mmap(fd, memory, block->mr->align, qemu_map_flags, offset);
+ *   - util/oslib-posix.c|193| <<qemu_anon_ram_alloc>> void *ptr = qemu_ram_mmap(-1, size, align, qemu_map_flags, 0);
+ */
 void *qemu_ram_mmap(int fd,
                     size_t size,
                     size_t align,
@@ -260,6 +313,9 @@ void *qemu_ram_mmap(int fd,
      */
     total = size + align;
 
+    /*
+     * 不用fd, 先用MAP_ANONYMOUS分配一些
+     */
     guardptr = mmap_reserve(total, fd);
     if (guardptr == MAP_FAILED) {
         return MAP_FAILED;
diff --git a/util/oslib-posix.c b/util/oslib-posix.c
index e86fd64e0..993a6a937 100644
--- a/util/oslib-posix.c
+++ b/util/oslib-posix.c
@@ -288,6 +288,16 @@ void qemu_set_tty_echo(int fd, bool echo)
     tcsetattr(fd, TCSANOW, &tty);
 }
 
+/*
+ * 在&sigbus_handler()调用siglongjmp(thread->env, 1);
+ *
+ * 在以下真正goto/jmp
+ * qemu_prealloc_mem()
+ * -> touch_all_pages()
+ *    -> do_touch_pages()
+ *       -> sigsetjmp(memset_args->env, 1)
+ *       -> for: *(volatile char *)addr = *addr;
+ */
 #ifdef CONFIG_LINUX
 static void sigbus_handler(int signal, siginfo_t *siginfo, void *ctx)
 #else /* CONFIG_LINUX */
@@ -327,6 +337,37 @@ static void sigbus_handler(int signal)
     warn_report("qemu_prealloc_mem: unrelated SIGBUS detected and ignored");
 }
 
+/*
+ * 597 void qemu_prealloc_mem(int fd, char *area, size_t sz, int max_threads,
+ * 598                        ThreadContext *tc, Error **errp)
+ * 599 {
+ * 624         memset(&act, 0, sizeof(act));
+ * 625 #ifdef CONFIG_LINUX
+ * 626         act.sa_sigaction = &sigbus_handler;
+ * 627         act.sa_flags = SA_SIGINFO;
+ * 628 #else  // CONFIG_LINUX
+ * 629         act.sa_handler = &sigbus_handler;
+ * 630         act.sa_flags = 0;
+ * 631 #endif // CONFIG_LINUX
+ * 632 
+ * 633        //
+ * 634        // The sigaction() system call is used to change the action taken by a
+ * 635        // process on receipt of a specific signal.
+ * 636        //
+ * 637         ret = sigaction(SIGBUS, &act, &sigbus_oldact);
+ *
+ * 在以下使用do_touch_pages():
+ *   - util/oslib-posix.c|482| <<touch_all_pages>> touch_fn = do_touch_pages;
+ *
+ * 在&sigbus_handler()调用siglongjmp(thread->env, 1);
+ *
+ * 在以下真正goto/jmp
+ * qemu_prealloc_mem()
+ * -> touch_all_pages()
+ *    -> do_touch_pages()
+ *       -> sigsetjmp(memset_args->env, 1)
+ *       -> for: *(volatile char *)addr = *addr;
+ */
 static void *do_touch_pages(void *arg)
 {
     MemsetThread *memset_args = (MemsetThread *)arg;
@@ -412,11 +453,48 @@ static inline int get_memset_num_threads(size_t hpagesize, size_t numpages,
     return ret;
 }
 
+/*
+ * -object thread-context,id=tc01,node-affinity=0 \
+ * -object memory-backend-file,id=ram01,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=0,policy=bind,prealloc-context=tc01 \
+ * -numa node,nodeid=0,cpus=0-1,memdev=ram01 \
+ * -object thread-context,id=tc02,node-affinity=1 \
+ * -object memory-backend-file,id=ram02,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=1,policy=bind,prealloc-context=tc02 \
+ * -numa node,nodeid=1,cpus=2-3,memdev=ram02 \
+ *
+ * Thread 8 "touch_pages" hit Breakpoint 1, do_madv_populate_write_pages (arg=0x5555570521d0) at ../util/oslib-posix.c:378
+ * 378	    MemsetThread *memset_args = (MemsetThread *)arg;
+ * (gdb) info threads 
+ *   Id   Target Id                                           Frame 
+ *   1    Thread 0x7ffff7460c80 (LWP 20402) "qemu-system-x86" 0x00007ffff772a3aa in __futex_abstimed_wait_common () from /lib64/libc.so.6
+ *   2    Thread 0x7ffff745f640 (LWP 20406) "qemu-system-x86" 0x00007ffff76cce5d in syscall () from /lib64/libc.so.6
+ *   3    Thread 0x7ffff6b5d640 (LWP 20407) "TC tc-ram-node0" 0x00007ffff772a3aa in __futex_abstimed_wait_common () from /lib64/libc.so.6
+ *   4    Thread 0x7ffff635c640 (LWP 20408) "TC tc-ram-node1" 0x00007ffff772a3aa in __futex_abstimed_wait_common () from /lib64/libc.so.6
+ *   6    Thread 0x7fffe7fff640 (LWP 20412) "touch_pages"     0x00007ffff76cceeb in madvise () from /lib64/libc.so.6
+ *   7    Thread 0x7fffe77fe640 (LWP 20413) "touch_pages"     0x0000555555ffb4d0 in do_madv_populate_write_pages (arg=0x5555570520e0) at ../util/oslib-posix.c:378
+ *   8    Thread 0x7fffe6ffd640 (LWP 20414) "touch_pages"     do_madv_populate_write_pages (arg=0x5555570521d0) at ../util/oslib-posix.c:378
+ *   9    Thread 0x7fffe67fc640 (LWP 20415) "touch_pages"     0x00007ffff77d5dd7 in mmap64 () from /lib64/libc.so.6
+ * (gdb) bt
+ * #0  do_madv_populate_write_pages (arg=0x5555570521d0) at ../util/oslib-posix.c:378
+ * #1  0x0000555555ffd7bb in qemu_thread_start (args=0x7fffe8000ea0) at ../util/qemu-thread-posix.c:541
+ * #2  0x00007ffff772d812 in start_thread () at /lib64/libc.so.6
+ * #3  0x00007ffff76cd450 in clone3 () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - util/oslib-posix.c|542| <<qemu_prealloc_mem>> ret = touch_all_pages(area, hpagesize, numpages, max_threads, tc, use_madv_populate_write);
+ */
 static int touch_all_pages(char *area, size_t hpagesize, size_t numpages,
                            int max_threads, ThreadContext *tc,
                            bool use_madv_populate_write)
 {
     static gsize initialized = 0;
+    /*
+     * typedef struct MemsetContext {
+     *     bool all_threads_created;
+     *     bool any_thread_failed;
+     *     struct MemsetThread *threads;
+     *     int num_threads;
+     * } MemsetContext;
+     */
     MemsetContext context = {
         .num_threads = get_memset_num_threads(hpagesize, numpages, max_threads),
     };
@@ -491,12 +569,62 @@ static int touch_all_pages(char *area, size_t hpagesize, size_t numpages,
     return ret;
 }
 
+/*
+ * called by:
+ *   - util/oslib-posix.c|558| <<qemu_prealloc_mem>> use_madv_populate_write = madv_populate_write_possible(area, hpagesize);
+ */
 static bool madv_populate_write_possible(char *area, size_t pagesize)
 {
+    /*
+     * 对于madvise():
+     * RETURN VALUE
+     * On success, madvise() returns zero.  On error, it returns -1 and  errno is set appropriately.
+     */
     return !qemu_madvise(area, pagesize, QEMU_MADV_POPULATE_WRITE) ||
            errno != EINVAL;
 }
 
+/*
+ * 程序的例子.
+ * #include <stdio.h>
+ * #include <time.h>
+ * #include <signal.h>
+ * #include <unistd.h>
+ *
+ * void alarm_handler(int signal)
+ * {
+ *     printf("This is alarm!\n");
+ * }
+ *
+ * int main(int argc, char **argv)
+ * {
+ *     int i = 0;
+ *     struct sigaction sa;
+ *
+ *     sa.sa_handler = alarm_handler;
+ *     sa.sa_flags = 0;
+ *
+ *     if(sigaction(SIGALRM, &sa, NULL))
+ *     {
+ *         printf("sigaction error!\n");
+ *         return -1;
+ *     }
+ *
+ *     alarm(5);
+ *
+ *     while(1);
+ *     printf("Hello World!\n");
+ *
+ *     return 0;
+ * }
+ */
+/*
+ * called by:
+ *   - backends/hostmem.c|240| <<host_memory_backend_set_prealloc>> qemu_prealloc_mem(fd, ptr, sz, backend->prealloc_threads,
+ *   - backends/hostmem.c|402| <<host_memory_backend_memory_complete>> qemu_prealloc_mem(memory_region_get_fd(&backend->mr), ptr, sz,
+ *   - hw/virtio/virtio-mem.c|608| <<virtio_mem_set_block_state>> qemu_prealloc_mem(fd, area, size, 1, NULL, &local_err);
+ *   - hw/virtio/virtio-mem.c|1252| <<virtio_mem_prealloc_range_cb>> qemu_prealloc_mem(fd, area, size, 1, NULL, &local_err);
+ */
 void qemu_prealloc_mem(int fd, char *area, size_t sz, int max_threads,
                        ThreadContext *tc, Error **errp)
 {
@@ -507,6 +635,10 @@ void qemu_prealloc_mem(int fd, char *area, size_t sz, int max_threads,
     bool use_madv_populate_write;
     struct sigaction act;
 
+    /*
+     * #define MADV_POPULATE_READ      22      // populate (prefault) page tables readable
+     * #define MADV_POPULATE_WRITE     23      // populate (prefault) page tables writable
+     */
     /*
      * Sense on every invocation, as MADV_POPULATE_WRITE cannot be used for
      * some special mappings, such as mapping /dev/mem.
@@ -529,6 +661,10 @@ void qemu_prealloc_mem(int fd, char *area, size_t sz, int max_threads,
         act.sa_flags = 0;
 #endif /* CONFIG_LINUX */
 
+	/*
+	 * The sigaction() system call is used to change the action taken by a
+	 * process on receipt of a specific signal.
+	 */
         ret = sigaction(SIGBUS, &act, &sigbus_oldact);
         if (ret) {
             qemu_mutex_unlock(&sigbus_mutex);
@@ -538,6 +674,9 @@ void qemu_prealloc_mem(int fd, char *area, size_t sz, int max_threads,
         }
     }
 
+    /*
+     * 只在这里调用
+     */
     /* touch pages simultaneously */
     ret = touch_all_pages(area, hpagesize, numpages, max_threads, tc,
                           use_madv_populate_write);
diff --git a/util/qemu-config.c b/util/qemu-config.c
index 42076efe1..0c1c1621b 100644
--- a/util/qemu-config.c
+++ b/util/qemu-config.c
@@ -9,13 +9,44 @@
 #include "qemu/config-file.h"
 #include "hw/boards.h"
 
+/*
+ * 在以下使用vm_config_groups[48]:
+ *   - util/qemu-config.c|36| <<qemu_find_opts>> ret = find_list(vm_config_groups, group, &local_err);
+ *   - util/qemu-config.c|228| <<qmp_query_command_line_options>> for (i = 0; vm_config_groups[i] != NULL; i++) {
+ *   - util/qemu-config.c|229| <<qmp_query_command_line_options>> if (!option || !strcmp(option, vm_config_groups[i]->name)) {
+ *   - util/qemu-config.c|231| <<qmp_query_command_line_options>> info->option = g_strdup(vm_config_groups[i]->name);
+ *   - util/qemu-config.c|232| <<qmp_query_command_line_options>> if (!strcmp("drive", vm_config_groups[i]->name)) {
+ *   - util/qemu-config.c|236| <<qmp_query_command_line_options>> query_option_descs(vm_config_groups[i]->desc);
+ *   - util/qemu-config.c|258| <<qemu_find_opts_err>> return find_list(vm_config_groups, group, errp);
+ *   - util/qemu-config.c|281| <<qemu_add_opts>> entries = ARRAY_SIZE(vm_config_groups);
+ *   - util/qemu-config.c|284| <<qemu_add_opts>> if (vm_config_groups[i] == NULL) {
+ *   - util/qemu-config.c|285| <<qemu_add_opts>> vm_config_groups[i] = list;
+ *   - util/qemu-config.c|396| <<qemu_read_config_file>> ret = qemu_config_foreach(f, cb, vm_config_groups, filename, errp);
+ */
 static QemuOptsList *vm_config_groups[48];
+/*
+ * 在以下使用drive_config_groups[5]:
+ *   - util/qemu-config.c|133| <<get_drive_infolist>> for (i = 0; drive_config_groups[i] != NULL; i++) {
+ *   - util/qemu-config.c|135| <<get_drive_infolist>> head = query_option_descs(drive_config_groups[i]->desc);
+ *   - util/qemu-config.c|137| <<get_drive_infolist>> cur = query_option_descs(drive_config_groups[i]->desc);
+ *   - util/qemu-config.c|265| <<qemu_add_drive_opts>> entries = ARRAY_SIZE(drive_config_groups);
+ *   - util/qemu-config.c|268| <<qemu_add_drive_opts>> if (drive_config_groups[i] == NULL) {
+ *   - util/qemu-config.c|269| <<qemu_add_drive_opts>> drive_config_groups[i] = list;
+ */
 static QemuOptsList *drive_config_groups[5];
 
+/*
+ * called by:
+ *   - util/qemu-config.c|36| <<qemu_find_opts>> ret = find_list(vm_config_groups, group, &local_err);
+ *   - util/qemu-config.c|258| <<qemu_find_opts_err>> return find_list(vm_config_groups, group, errp);
+ *   - util/qemu-config.c|373| <<qemu_config_do_parse>> list = find_list(lists, group, errp);
+ *
+ * 在二维的QemuOptsList **lists寻找和group内容相等的QemuOptsList
+ */
 static QemuOptsList *find_list(QemuOptsList **lists, const char *group,
                                Error **errp)
 {
-    int i;
+    nt i;
 
     qemu_load_module_for_opts(group);
     for (i = 0; lists[i] != NULL; i++) {
@@ -28,6 +59,11 @@ static QemuOptsList *find_list(QemuOptsList **lists, const char *group,
     return lists[i];
 }
 
+/*
+ * 很多的调用, 各种module
+ *
+ * 在vm_config_groups寻找对应group(char *)的QemuOptsList
+ */
 QemuOptsList *qemu_find_opts(const char *group)
 {
     QemuOptsList *ret;
@@ -46,6 +82,11 @@ QemuOpts *qemu_find_opts_singleton(const char *group)
     QemuOptsList *list;
     QemuOpts *opts;
 
+    /*
+     * 很多的调用, 各种module
+     *
+     * 在vm_config_groups寻找对应group(char *)的QemuOptsList
+     */
     list = qemu_find_opts(group);
     assert(list);
     opts = qemu_opts_find(list, NULL);
diff --git a/util/qemu-timer.c b/util/qemu-timer.c
index 6a0de33dd..7f9497b57 100644
--- a/util/qemu-timer.c
+++ b/util/qemu-timer.c
@@ -662,6 +662,10 @@ uint64_t timer_expire_time_ns(QEMUTimer *ts)
     return timer_pending(ts) ? ts->expire_time : -1;
 }
 
+/*
+ * called by:
+ *   - util/main-loop.c|603| <<main_loop_wait>> qemu_clock_run_all_timers();
+ */
 bool qemu_clock_run_all_timers(void)
 {
     bool progress = false;
diff --git a/util/thread-context.c b/util/thread-context.c
index 2bc7883b9..84d3b49dd 100644
--- a/util/thread-context.c
+++ b/util/thread-context.c
@@ -39,6 +39,42 @@ typedef struct ThreadContextCmdNew {
     int mode;
 } ThreadContextCmdNew;
 
+/*
+ * -object thread-context,id=tc01,node-affinity=0 \
+ * -object memory-backend-file,id=ram01,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=0,policy=bind,prealloc-context=tc01 \
+ * -numa node,nodeid=0,cpus=0-1,memdev=ram01 \
+ * -object thread-context,id=tc02,node-affinity=1 \
+ * -object memory-backend-file,id=ram02,mem-path=/dev/hugepages/libvirt/qemu,prealloc=true,size=68719476736,host-nodes=1,policy=bind,prealloc-context=tc02 \
+ * -numa node,nodeid=1,cpus=2-3,memdev=ram02 \
+ *
+ * Thread 8 "touch_pages" hit Breakpoint 1, do_madv_populate_write_pages (arg=0x5555570521d0) at ../util/oslib-posix.c:378
+ * 378      MemsetThread *memset_args = (MemsetThread *)arg;
+ * (gdb) info threads 
+ *   Id   Target Id                                           Frame 
+ *   1    Thread 0x7ffff7460c80 (LWP 20402) "qemu-system-x86" 0x00007ffff772a3aa in __futex_abstimed_wait_common () from /lib64/libc.so.6
+ *   2    Thread 0x7ffff745f640 (LWP 20406) "qemu-system-x86" 0x00007ffff76cce5d in syscall () from /lib64/libc.so.6
+ *   3    Thread 0x7ffff6b5d640 (LWP 20407) "TC tc-ram-node0" 0x00007ffff772a3aa in __futex_abstimed_wait_common () from /lib64/libc.so.6
+ *   4    Thread 0x7ffff635c640 (LWP 20408) "TC tc-ram-node1" 0x00007ffff772a3aa in __futex_abstimed_wait_common () from /lib64/libc.so.6
+ *   6    Thread 0x7fffe7fff640 (LWP 20412) "touch_pages"     0x00007ffff76cceeb in madvise () from /lib64/libc.so.6
+ *   7    Thread 0x7fffe77fe640 (LWP 20413) "touch_pages"     0x0000555555ffb4d0 in do_madv_populate_write_pages (arg=0x5555570520e0) at ../util/oslib-posix.c:378
+ * 8    Thread 0x7fffe6ffd640 (LWP 20414) "touch_pages"     do_madv_populate_write_pages (arg=0x5555570521d0) at ../util/oslib-posix.c:378
+ *   9    Thread 0x7fffe67fc640 (LWP 20415) "touch_pages"     0x00007ffff77d5dd7 in mmap64 () from /lib64/libc.so.6
+ * (gdb) bt
+ * #0  do_madv_populate_write_pages (arg=0x5555570521d0) at ../util/oslib-posix.c:378
+ * #1  0x0000555555ffd7bb in qemu_thread_start (args=0x7fffe8000ea0) at ../util/qemu-thread-posix.c:541
+ * #2  0x00007ffff772d812 in start_thread () at /lib64/libc.so.6
+ * #3  0x00007ffff76cd450 in clone3 () at /lib64/libc.so.6
+ *
+ *
+ * (gdb) bt
+ * #0  thread_context_run (opaque=0x555557051600) at ../util/thread-context.c:44
+ * #1  0x0000555555ffd7bb in qemu_thread_start (args=0x55555705a6c0) at ../util/qemu-thread-posix.c:541
+ * #2  0x00007ffff772d812 in start_thread () at /lib64/libc.so.6
+ * #3  0x00007ffff76cd450 in clone3 () at /lib64/libc.so.6
+ *
+ * 在以下使用thread_context_run():
+ *   - util/thread-context.c|256| <<thread_context_instance_complete>> qemu_thread_create(&tc->thread, thread_name, thread_context_run, tc, QEMU_THREAD_JOINABLE);
+ */
 static void *thread_context_run(void *opaque)
 {
     ThreadContext *tc = opaque;
@@ -78,6 +114,14 @@ static void *thread_context_run(void *opaque)
         default:
             g_assert_not_reached();
         }
+	/*
+	 * 在以下使用ThreadContext->sem_thread:
+	 *   - util/thread-context.c|117| <<thread_context_run>> qemu_sem_wait(&tc->sem_thread);
+	 *   - util/thread-context.c|345| <<thread_context_instance_init>> qemu_sem_init(&tc->sem_thread, 0);
+	 *   - util/thread-context.c|355| <<thread_context_instance_finalize>> qemu_sem_post(&tc->sem_thread);
+	 *   - util/thread-context.c|359| <<thread_context_instance_finalize>> qemu_sem_destroy(&tc->sem_thread);
+	 *   - util/thread-context.c|402| <<thread_context_create_thread>> qemu_sem_post(&tc->sem_thread);
+	 */
         qemu_sem_wait(&tc->sem_thread);
     }
 }
@@ -245,6 +289,18 @@ static void thread_context_get_thread_id(Object *obj, Visitor *v,
     visit_type_uint64(v, name, &value, errp);
 }
 
+/*
+ * (gdb) bt
+ * #0  thread_context_instance_complete (uc=0x555557051600, errp=0x7fffffffd9d8) at ../util/thread-context.c:250
+ * #1  0x0000555555e121a2 in user_creatable_complete (uc=0x555557051600, errp=0x7fffffffda38) at ../qom/object_interfaces.c:28
+ * #2  0x0000555555e1262e in user_creatable_add_type (type=0x5555562af1a1 "thread-context", id=0x555557053760 "tc-ram-node0", qdict=0x555557055730, v=0x555557056920, errp=0x7fffffffda50)
+ *                                                   at ../qom/object_interfaces.c:125
+ * #3  0x0000555555e127d4 in user_creatable_add_qapi (options=0x555557053550, errp=0x555556fc8340 <error_fatal>) at ../qom/object_interfaces.c:157
+ * #4  0x0000555555b7231c in object_option_foreach_add (type_opt_predicate=0x555555b725bb <object_create_pre_sandbox>) at ../system/vl.c:1792
+ * #5  0x0000555555b73fff in qemu_process_early_options () at ../system/vl.c:2488
+ * #6  0x0000555555b76de2 in qemu_init (argc=33, argv=0x7fffffffde18) at ../system/vl.c:3650
+ * #7  0x0000555555e01cbf in main (argc=33, argv=0x7fffffffde18) at ../system/main.c:47
+ */
 static void thread_context_instance_complete(UserCreatable *uc, Error **errp)
 {
     ThreadContext *tc = THREAD_CONTEXT(uc);
@@ -331,6 +387,10 @@ static void thread_context_register_types(void)
 }
 type_init(thread_context_register_types)
 
+/*
+ * called by:
+ *   - util/oslib-posix.c|461| <<touch_all_pages>> thread_context_create_thread(tc, &context.threads[i].pgthread, "touch_pages", touch_fn, &context.threads[i], QEMU_THREAD_JOINABLE);
+ */
 void thread_context_create_thread(ThreadContext *tc, QemuThread *thread,
                                   const char *name,
                                   void *(*start_routine)(void *), void *arg,
@@ -347,6 +407,14 @@ void thread_context_create_thread(ThreadContext *tc, QemuThread *thread,
     qemu_mutex_lock(&tc->mutex);
     tc->thread_cmd = TC_CMD_NEW;
     tc->thread_cmd_data = &data;
+    /*
+     * 在以下使用ThreadContext->sem_thread:
+     *   - util/thread-context.c|117| <<thread_context_run>> qemu_sem_wait(&tc->sem_thread);
+     *   - util/thread-context.c|345| <<thread_context_instance_init>> qemu_sem_init(&tc->sem_thread, 0);
+     *   - util/thread-context.c|355| <<thread_context_instance_finalize>> qemu_sem_post(&tc->sem_thread);
+     *   - util/thread-context.c|359| <<thread_context_instance_finalize>> qemu_sem_destroy(&tc->sem_thread);
+     *   - util/thread-context.c|402| <<thread_context_create_thread>> qemu_sem_post(&tc->sem_thread);
+     */
     qemu_sem_post(&tc->sem_thread);
 
     while (tc->thread_cmd != TC_CMD_NONE) {
-- 
2.34.1

