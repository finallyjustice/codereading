From 398626ab00fe0b7295bfaaa58ba4f4c874b41626 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 4 Sep 2020 10:59:09 -0700
Subject: [PATCH 1/1] linux-uek5-v4.14.35-1902.303.5.3-kvm

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/kvm/vmx.c                 |   19 +
 arch/x86/kvm/x86.c                 |    5 +
 drivers/acpi/osl.c                 |   56 ++
 drivers/net/virtio_net.c           |    4 +
 drivers/pci/hotplug/acpiphp_glue.c |    7 +
 drivers/pci/probe.c                |   12 +
 drivers/scsi/scsi_lib.c            |   10 +
 drivers/scsi/virtio_scsi.c         |   47 +
 drivers/vhost/net.c                |  309 +++++++
 drivers/vhost/scsi.c               |  143 +++
 drivers/vhost/vhost.c              | 1367 ++++++++++++++++++++++++++++
 drivers/vhost/vhost.h              |  160 ++++
 drivers/virtio/virtio_pci_modern.c |   21 +
 drivers/virtio/virtio_ring.c       |  210 +++++
 fs/eventfd.c                       |   81 ++
 include/kvm/iodev.h                |   32 +
 include/linux/irq.h                |   13 +
 include/linux/kvm_host.h           |   11 +
 include/linux/ptr_ring.h           |  286 ++++++
 include/linux/skb_array.h          |  153 +++-
 include/scsi/scsi_cmnd.h           |   13 +
 include/uapi/linux/virtio_ring.h   |   10 +
 kernel/irq/chip.c                  |   13 +
 kernel/irq/spurious.c              |    3 +
 virt/kvm/eventfd.c                 |  157 ++++
 virt/kvm/kvm_main.c                |   55 ++
 26 files changed, 3196 insertions(+), 1 deletion(-)

diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 18331ebf892f..f49f7c17b706 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -7234,6 +7234,25 @@ static int handle_triple_fault(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * [0] eventfd_signal
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_vmx_exit_handlers[EXIT_REASON_IO_INSTRUCTION] = handle_io()
+ */
 static int handle_io(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 8df6eef9a02e..25cfb662d33d 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -5470,6 +5470,11 @@ static int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,
 	return 0;
 }
 
+/*
+ * struct x86_emulate_ops emulate_ops.pio_out_emulated = emulator_pio_out_emulated()
+ *
+ * - arch/x86/kvm/x86.c|6378| <<kvm_fast_pio_out>> int ret = emulator_pio_out_emulated(&vcpu->arch.emulate_ctxt,
+ */
 static int emulator_pio_out_emulated(struct x86_emulate_ctxt *ctxt,
 				     int size, unsigned short port,
 				     const void *val, unsigned int count)
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 30b0d52b8641..5e296dded7a9 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -815,6 +815,11 @@ acpi_os_write_pci_configuration(struct acpi_pci_id * pci_id, u32 reg,
 	return (result ? AE_ERROR : AE_OK);
 }
 
+/*
+ * 在以下使用acpi_os_execute_deferred():
+ *   - drivers/acpi/osl.c|1078| <<acpi_os_execute>> INIT_WORK(&dpc->work, acpi_os_execute_deferred);
+ *   - drivers/acpi/osl.c|1081| <<acpi_os_execute>> INIT_WORK(&dpc->work, acpi_os_execute_deferred);
+ */
 static void acpi_os_execute_deferred(struct work_struct *work)
 {
 	struct acpi_os_dpc *dpc = container_of(work, struct acpi_os_dpc, work);
@@ -1032,6 +1037,43 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * [0] acpi_os_execute
+ * [0] acpi_ev_gpe_dispatch
+ * [0] acpi_ev_gpe_detect
+ * [0] acpi_ev_sci_xrupt_handler
+ * [0] acpi_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ *
+ * [0] acpi_os_execute
+ * [0] acpi_ev_queue_notify_request
+ * [0] acpi_ex_opcode_2A_0T_0R
+ * [0] acpi_ds_exec_end_op
+ * [0] acpi_ps_parse_loop
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_os_execute
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1135,6 +1177,20 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/bus.c|450| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|392| <<acpi_eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 693a0f045aa5..d3a12fc2ce81 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -2424,6 +2424,10 @@ static bool is_xdp_raw_buffer_queue(struct virtnet_info *vi, int q)
 		return false;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2939| <<remove_vq_common>> free_unused_bufs(vi);
+ */
 static void free_unused_bufs(struct virtnet_info *vi)
 {
 	void *buf;
diff --git a/drivers/pci/hotplug/acpiphp_glue.c b/drivers/pci/hotplug/acpiphp_glue.c
index 711875afdd70..2857e228af55 100644
--- a/drivers/pci/hotplug/acpiphp_glue.c
+++ b/drivers/pci/hotplug/acpiphp_glue.c
@@ -647,6 +647,13 @@ static void trim_stale_devices(struct pci_dev *dev)
  * Iterate over all slots under this bridge and make sure that if a
  * card is present they are enabled, and if not they are disabled.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/acpiphp_glue.c|727| <<acpiphp_check_host_bridge>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|757| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|767| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|774| <<hotplug_event>> acpiphp_check_bridge(func->parent);
+ */
 static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 {
 	struct acpiphp_slot *slot;
diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index 42ba2de2bcf2..18fee1705dbb 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -1006,6 +1006,18 @@ static void pci_ea_fixed_busnrs(struct pci_dev *dev, u8 *secondary,
  * them, we proceed to assigning numbers to the remaining buses in
  * order to avoid overlaps between old and new bus numbers.
  */
+/*
+ * [0] pci_scan_bridge
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 int pci_scan_bridge(struct pci_bus *bus, struct pci_dev *dev, int max, int pass)
 {
 	struct pci_bus *child;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 9ffc85f47445..909f52d913e9 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -1199,6 +1199,12 @@ void scsi_del_cmd_from_list(struct scsi_cmnd *cmd)
 }
 
 /* Called after a request has been started. */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|2307| <<scsi_ioctl_reset>> scsi_init_command(dev, scmd);
+ *   - drivers/scsi/scsi_lib.c|1403| <<scsi_prep_fn>> scsi_init_command(sdev, cmd);
+ *   - drivers/scsi/scsi_lib.c|1933| <<scsi_mq_prep_fn>> scsi_init_command(sdev, cmd);
+ */
 void scsi_init_command(struct scsi_device *dev, struct scsi_cmnd *cmd)
 {
 	void *buf = cmd->sense_buffer;
@@ -1917,6 +1923,10 @@ static inline blk_status_t prep_to_mq(int ret)
 }
 
 /* Size in bytes of the sg-list stored in the scsi-mq command-private data. */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|2026| <<scsi_queue_rq>> ret = prep_to_mq(scsi_mq_prep_fn(req));
+ */
 static unsigned int scsi_mq_sgl_size(struct Scsi_Host *shost)
 {
 	return min_t(unsigned int, shost->sg_tablesize, SG_CHUNK_SIZE) *
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 45d04631888a..26978476d197 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -144,6 +144,12 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|238| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|248| <<virtscsi_poll_requests>> virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|578| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -238,6 +244,10 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|649| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
@@ -417,6 +427,10 @@ static void virtscsi_event_done(struct virtqueue *vq)
  * @req_size	: size of the request buffer
  * @resp_size	: size of the response buffer
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|485| <<virtscsi_kick_cmd>> err = virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -462,6 +476,11 @@ static int virtscsi_add_cmd(struct virtqueue *vq,
 	return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|569| <<virtscsi_queuecommand>> ret = virtscsi_kick_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd));
+ *   - drivers/scsi/virtio_scsi.c|587| <<virtscsi_tmf>> if (virtscsi_kick_cmd(&vscsi->ctrl_vq, cmd,
+ */
 static int virtscsi_kick_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size)
@@ -531,6 +550,34 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 	return &vscsi->req_vqs[hwq];
 }
 
+/*
+ * [0] virtqueue_notify
+ * [0] virtscsi_kick_cmd
+ * [0] virtscsi_queuecommand
+ * [0] scsi_dispatch_cmd
+ * [0] scsi_queue_rq
+ * [0] blk_mq_dispatch_rq_list
+ * [0] blk_mq_do_dispatch_sched
+ * [0] blk_mq_sched_dispatch_requests
+ * [0] __blk_mq_run_hw_queue
+ * [0] __blk_mq_delay_run_hw_queue
+ * [0] blk_mq_run_hw_queue
+ * [0] blk_mq_flush_plug_list
+ * [0] blk_flush_plug_list
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_direct_write
+ * [0] __generic_file_write_iter
+ * [0] blkdev_write_iter
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index baea42621146..8d7102b674c0 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -87,6 +87,11 @@ struct vhost_net_ubuf_ref {
 	struct vhost_virtqueue *vq;
 };
 
+/*
+ * 在以下使用VHOST_RX_BATCH=64:
+ *   - drivers/vhost/net.c|162| <<vhost_net_buf_produce>> VHOST_RX_BATCH);
+ *   - drivers/vhost/net.c|925| <<vhost_net_open>> queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
+ */
 #define VHOST_RX_BATCH 64
 struct vhost_net_buf {
 	struct sk_buff **queue;
@@ -96,18 +101,66 @@ struct vhost_net_buf {
 
 struct vhost_net_virtqueue {
 	struct vhost_virtqueue vq;
+	/*
+	 * 在以下使用vhost_net_virtqueue->vhost_hlen:
+	 *   - drivers/vhost/net.c|293| <<vhost_net_vq_reset>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|502| <<handle_tx>> hdr_size = nvq->vhost_hlen;
+	 *   - drivers/vhost/net.c|801| <<handle_rx>> vhost_hlen = nvq->vhost_hlen;
+	 *   - drivers/vhost/net.c|984| <<vhost_net_open>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|1304| <<vhost_net_set_features>> n->vqs[i].vhost_hlen = vhost_hlen;
+	 */
 	size_t vhost_hlen;
+	/*
+	 * 在以下使用vhost_net_virtqueue->sock_hlen:
+	 *   - drivers/vhost/net.c|294| <<vhost_net_vq_reset>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|802| <<handle_rx>> sock_hlen = nvq->sock_hlen;
+	 *   - drivers/vhost/net.c|985| <<vhost_net_open>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|1305| <<vhost_net_set_features>> n->vqs[i].sock_hlen = sock_hlen;
+	 */
 	size_t sock_hlen;
 	/* vhost zerocopy support fields below: */
 	/* last used idx for outstanding DMA zerocopy buffers */
+	/*
+	 * 在以下使用vhost_net_virtqueue->upend_idx:
+	 *   - drivers/vhost/net.c|291| <<vhost_net_vq_reset>> n->vqs[i].upend_idx = 0;
+	 *   - drivers/vhost/net.c|342| <<vhost_zerocopy_signal_used>> for (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {
+	 *   - drivers/vhost/net.c|466| <<vhost_exceeds_maxpend>> return (nvq->upend_idx + vq->num - VHOST_MAX_PEND) % UIO_MAXIOV
+	 *   - drivers/vhost/net.c|549| <<handle_tx>> && (nvq->upend_idx + 1) % UIO_MAXIOV !=
+	 *   - drivers/vhost/net.c|556| <<handle_tx>> ubuf = nvq->ubuf_info + nvq->upend_idx;
+	 *   - drivers/vhost/net.c|558| <<handle_tx>> vq->heads[nvq->upend_idx].id = cpu_to_vhost32(vq, head);
+	 *   - drivers/vhost/net.c|559| <<handle_tx>> vq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;
+	 *   - drivers/vhost/net.c|562| <<handle_tx>> ubuf->desc = nvq->upend_idx;
+	 *   - drivers/vhost/net.c|568| <<handle_tx>> nvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;
+	 *   - drivers/vhost/net.c|588| <<handle_tx>> nvq->upend_idx = ((unsigned )nvq->upend_idx - 1)
+	 *   - drivers/vhost/net.c|982| <<vhost_net_open>> n->vqs[i].upend_idx = 0;
+	 */
 	int upend_idx;
 	/* first used idx for DMA done zerocopy buffers */
+	/*
+	 * 在以下使用vhost_net_virtqueue->done_idx:
+	 *   - drivers/vhost/net.c|290| <<vhost_net_vq_reset>> n->vqs[i].done_idx = 0;
+	 *   - drivers/vhost/net.c|342| <<vhost_zerocopy_signal_used>> for (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {
+	 *   - drivers/vhost/net.c|352| <<vhost_zerocopy_signal_used>> add = min(UIO_MAXIOV - nvq->done_idx, j);
+	 *   - drivers/vhost/net.c|354| <<vhost_zerocopy_signal_used>> &vq->heads[nvq->done_idx], add);
+	 *   - drivers/vhost/net.c|355| <<vhost_zerocopy_signal_used>> nvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;
+	 *   - drivers/vhost/net.c|467| <<vhost_exceeds_maxpend>> == nvq->done_idx;
+	 *   - drivers/vhost/net.c|983| <<vhost_net_open>> n->vqs[i].done_idx = 0;
+	 */
 	int done_idx;
 	/* an array of userspace buffers info */
 	struct ubuf_info *ubuf_info;
 	/* Reference counting for outstanding ubufs.
 	 * Protected by vq mutex. Writers must also take device mutex. */
 	struct vhost_net_ubuf_ref *ubufs;
+	/*
+	 * 在以下使用vhost_net_virtqueue->rx_array:
+	 *   - drivers/vhost/net.c|171| <<vhost_net_buf_produce>> rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
+	 *   - drivers/vhost/net.c|180| <<vhost_net_buf_unproduce>> if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
+	 *   - drivers/vhost/net.c|181| <<vhost_net_buf_unproduce>> skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
+	 *   - drivers/vhost/net.c|617| <<peek_head_len>> if (rvq->rx_array)
+	 *   - drivers/vhost/net.c|829| <<handle_rx>> if (nvq->rx_array)
+	 *   - drivers/vhost/net.c|1191| <<vhost_net_set_backend>> nvq->rx_array = get_tap_skb_array(fd);
+	 */
 	struct skb_array *rx_array;
 	struct vhost_net_buf rxq;
 };
@@ -121,6 +174,13 @@ struct vhost_net {
 	unsigned tx_packets;
 	/* Number of times zerocopy TX recently failed.
 	 * Protected by tx vq lock. */
+	/*
+	 * 在以下使用vhost_net->tx_zcopy_err:
+	 *   - drivers/vhost/net.c|306| <<vhost_net_tx_packet>> net->tx_zcopy_err = 0;
+	 *   - drivers/vhost/net.c|311| <<vhost_net_tx_err>> ++net->tx_zcopy_err;
+	 *   - drivers/vhost/net.c|320| <<vhost_net_tx_select_zcopy>> net->tx_packets / 64 >= net->tx_zcopy_err;
+	 *   - drivers/vhost/net.c|1203| <<vhost_net_set_backend>> n->tx_zcopy_err = 0
+	 */
 	unsigned tx_zcopy_err;
 	/* Flush in progress. Protected by tx vq lock. */
 	bool tx_flush;
@@ -128,6 +188,10 @@ struct vhost_net {
 
 static unsigned vhost_net_zcopy_mask __read_mostly;
 
+/*
+ * 如果vhost_net_buf->tail != vhost_net_buf->head, 返回vhost_net_buf->queue[rxq->head]
+ * 否则返回NULL
+ */
 static void *vhost_net_buf_get_ptr(struct vhost_net_buf *rxq)
 {
 	if (rxq->tail != rxq->head)
@@ -136,37 +200,98 @@ static void *vhost_net_buf_get_ptr(struct vhost_net_buf *rxq)
 		return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|240| <<vhost_net_buf_unproduce>> vhost_net_buf_get_size(rxq));
+ *
+ * 返回rxq->tail - rxq->head
+ */
 static int vhost_net_buf_get_size(struct vhost_net_buf *rxq)
 {
 	return rxq->tail - rxq->head;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|238| <<vhost_net_buf_unproduce>> if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
+ *   - drivers/vhost/net.c|253| <<vhost_net_buf_peek>> if (!vhost_net_buf_is_empty(rxq))
+ *
+ * 返回rxq->tail == rxq->head
+ */
 static int vhost_net_buf_is_empty(struct vhost_net_buf *rxq)
 {
 	return rxq->tail == rxq->head;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|892| <<handle_rx>> msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
+ *
+ * 核心思想是返回rxq->queue[rxq->head], 增加rxq->head
+ */
 static void *vhost_net_buf_consume(struct vhost_net_buf *rxq)
 {
+	/*
+	 * 如果rxq->tail != rxq->head, 返回rxq->queue[rxq->head]
+	 * 否则返回NULL
+	 */
 	void *ret = vhost_net_buf_get_ptr(rxq);
 	++rxq->head;
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|256| <<vhost_net_buf_peek>> if (!vhost_net_buf_produce(nvq))
+ *
+ * 把nvq->rx_array的ptr_ring的数据放入rxq->queue, 最多VHOST_RX_BATCH=64个
+ * 最后更新并返回的rxq->tail是从ptr_ring放入参数array的数目
+ */
 static int vhost_net_buf_produce(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
 
 	rxq->head = 0;
+	/*
+	 * struct vhost_net_virtqueue *nvq:
+	 *  -> struct skb_array *rx_array; --->
+	 *      -> struct ptr_ring ring;
+	 *  -> struct vhost_net_buf rxq;
+	 *      -> struct sk_buff **queue; --->
+	 *      -> int tail;
+	 *      -> int head;
+	 *
+	 * 在以下使用VHOST_RX_BATCH=64:
+	 *   - drivers/vhost/net.c|162| <<vhost_net_buf_produce>> VHOST_RX_BATCH);
+	 *   - drivers/vhost/net.c|925| <<vhost_net_open>> queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
+	 *
+	 * 把nvq->rx_array的ptr_ring的数据放入rxq->queue, 最多VHOST_RX_BATCH=64个
+	 *
+	 * 返回值是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+	 */
 	rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
 					      VHOST_RX_BATCH);
 	return rxq->tail;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1071| <<vhost_net_stop_vq>> vhost_net_buf_unproduce(nvq);
+ *   - drivers/vhost/net.c|1251| <<vhost_net_set_backend>> vhost_net_buf_unproduce(nvq);
+ */
 static void vhost_net_buf_unproduce(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
 
+	/*
+	 * struct vhost_net_virtqueue *nvq:
+	 *  -> struct skb_array *rx_array; --->
+	 *      -> struct ptr_ring ring;
+	 *  -> struct vhost_net_buf rxq;
+	 *      -> struct sk_buff **queue; --->
+	 *      -> int tail;
+	 *      -> int head;
+	 */
 	if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
 		skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
 				    vhost_net_buf_get_size(rxq));
@@ -174,20 +299,44 @@ static void vhost_net_buf_unproduce(struct vhost_net_virtqueue *nvq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|618| <<peek_head_len>> return vhost_net_buf_peek(rvq);
+ *
+ * 如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+ * 否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+ */
 static int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
 
+	/*
+	 * 返回rxq->tail == rxq->head
+	 */
 	if (!vhost_net_buf_is_empty(rxq))
 		goto out;
 
+	/*
+	 * 把nvq->rx_array的ptr_ring的数据放入rxq->queue, 最多VHOST_RX_BATCH=64个
+	 * 最后更新并返回的rxq->tail是从ptr_ring放入参数array的数目
+	 */
 	if (!vhost_net_buf_produce(nvq))
 		return 0;
 
 out:
+	/*
+	 * vhost_net_buf_get_ptr():
+	 * 如果vhost_net_buf->tail != vhost_net_buf->head, 返回vhost_net_buf->queue[rxq->head]
+	 * 否则返回NULL
+	 */
 	return __skb_array_len_with_tag(vhost_net_buf_get_ptr(rxq));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|280| <<vhost_net_vq_reset>> vhost_net_buf_init(&n->vqs[i].rxq);
+ *   - drivers/vhost/net.c|946| <<vhost_net_open>> vhost_net_buf_init(&n->vqs[i].rxq);
+ */
 static void vhost_net_buf_init(struct vhost_net_buf *rxq)
 {
 	rxq->head = rxq->tail = 0;
@@ -382,6 +531,12 @@ static bool vhost_can_busy_poll(struct vhost_dev *dev,
 	       !vhost_has_work(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|772| <<handle_rx>> vhost_net_disable_vq(net, vq);
+ *   - drivers/vhost/net.c|967| <<vhost_net_stop_vq>> vhost_net_disable_vq(n, vq);
+ *   - drivers/vhost/net.c|1147| <<vhost_net_set_backend>> vhost_net_disable_vq(n, vq);
+ */
 static void vhost_net_disable_vq(struct vhost_net *n,
 				 struct vhost_virtqueue *vq)
 {
@@ -393,6 +548,12 @@ static void vhost_net_disable_vq(struct vhost_net *n,
 	vhost_poll_stop(poll);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|866| <<handle_rx>> vhost_net_enable_vq(net, vq);
+ *   - drivers/vhost/net.c|1155| <<vhost_net_set_backend>> r = vhost_net_enable_vq(n, vq);
+ *   - drivers/vhost/net.c|1186| <<vhost_net_set_backend>> vhost_net_enable_vq(n, vq);
+ */
 static int vhost_net_enable_vq(struct vhost_net *n,
 				struct vhost_virtqueue *vq)
 {
@@ -581,16 +742,45 @@ static void handle_tx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|743| <<vhost_net_rx_peek_head_len>> int len = peek_head_len(rvq, sk);
+ *   - drivers/vhost/net.c|769| <<vhost_net_rx_peek_head_len>> len = peek_head_len(rvq, sk);
+ *
+ * 如果sock支持skb_array:
+ *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+ *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+ * 如果sock不支持skb_array:
+ *     从sk->sk_receive_queue取skb ... ...
+ */
 static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 {
 	struct sk_buff *head;
 	int len = 0;
 	unsigned long flags;
 
+	/*
+	 * 在以下使用vhost_net_virtqueue->rx_array:
+	 *   - drivers/vhost/net.c|1191| <<vhost_net_set_backend>> nvq->rx_array = get_tap_skb_array(fd);
+	 * 在以下使用vhost_net_virtqueue->rx_array:
+	 *   - drivers/vhost/net.c|171| <<vhost_net_buf_produce>> rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
+	 *   - drivers/vhost/net.c|180| <<vhost_net_buf_unproduce>> if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
+	 *   - drivers/vhost/net.c|181| <<vhost_net_buf_unproduce>> skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
+	 *   - drivers/vhost/net.c|617| <<peek_head_len>> if (rvq->rx_array)
+	 *   - drivers/vhost/net.c|829| <<handle_rx>> if (nvq->rx_array)
+	 *
+	 *
+	 * 关于vhost_net_buf_peek():
+	 * 如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+	 * 否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+	 */
 	if (rvq->rx_array)
 		return vhost_net_buf_peek(rvq);
 
 	spin_lock_irqsave(&sk->sk_receive_queue.lock, flags);
+	/*
+	 * struct sk_buff *head;
+	 */
 	head = skb_peek(&sk->sk_receive_queue);
 	if (likely(head)) {
 		len = head->len;
@@ -602,6 +792,10 @@ static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 	return len;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|754| <<vhost_net_rx_peek_head_len>> !sk_has_rx_data(sk) &&
+ */
 static int sk_has_rx_data(struct sock *sk)
 {
 	struct socket *sock = sk->sk_socket;
@@ -612,12 +806,30 @@ static int sk_has_rx_data(struct sock *sk)
 	return skb_queue_empty(&sk->sk_receive_queue);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|903| <<handle_rx>> while ((sock_len = vhost_net_rx_peek_head_len(net, sock->sk))) {
+ *
+ * 核心思想!!
+ * 如果sock支持skb_array:
+ *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+ *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+ * 如果sock不支持skb_array:
+ *     从sk->sk_receive_queue取skb ... ...
+ */
 static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 {
 	struct vhost_net_virtqueue *rvq = &net->vqs[VHOST_NET_VQ_RX];
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
 	struct vhost_virtqueue *vq = &nvq->vq;
 	unsigned long uninitialized_var(endtime);
+	/*
+	 * 如果sock支持skb_array:
+	 *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+	 *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+	 * 如果sock不支持skb_array:
+	 *     从sk->sk_receive_queue取skb ... ...
+	 */
 	int len = peek_head_len(rvq, sk);
 
 	if (!len && vq->busyloop_timeout) {
@@ -644,6 +856,13 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 
 		mutex_unlock(&vq->mutex);
 
+		/*
+		 * 如果sock支持skb_array:
+		 *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+		 *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+		 * 如果sock不支持skb_array:
+		 *     从sk->sk_receive_queue取skb ... ...
+		 */
 		len = peek_head_len(rvq, sk);
 	}
 
@@ -660,6 +879,14 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
  * @quota       - headcount quota, 1 for big buffer
  *	returns number of buffer heads allocated, negative on error
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|906| <<handle_rx>> headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ *
+ * 1019                 headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ * 1020                                         &in, vq_log, &log,
+ * 1021                                         likely(mergeable) ? UIO_MAXIOV : 1);
+ */
 static int get_rx_bufs(struct vhost_virtqueue *vq,
 		       struct vring_used_elem *heads,
 		       int datalen,
@@ -683,6 +910,13 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			r = -ENOBUFS;
 			goto err;
 		}
+		/*
+		 * struct vhost_virtqueue *vq:
+		 *   -> struct iovec iov[UIO_MAXIOV];
+		 *   -> struct vring_used_elem *heads;
+		 *
+		 * seg一开始初始化的时候是0
+		 */
 		r = vhost_get_vq_desc(vq, vq->iov + seg,
 				      ARRAY_SIZE(vq->iov) - seg, &out,
 				      &in, log, log_num);
@@ -704,6 +938,9 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			nlogs += *log_num;
 			log += *log_num;
 		}
+		/*
+		 * 函数一开始的时候headcount是0
+		 */
 		heads[headcount].id = cpu_to_vhost32(vq, d);
 		len = iov_length(vq->iov + seg, in);
 		heads[headcount].len = cpu_to_vhost32(vq, len);
@@ -729,6 +966,11 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|881| <<handle_rx_kick>> handle_rx(net);
+ *   - drivers/vhost/net.c|895| <<handle_rx_net>> handle_rx(net);
+ */
 static void handle_rx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];
@@ -773,9 +1015,21 @@ static void handle_rx(struct vhost_net *net)
 		vq->log : NULL;
 	mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
 
+	/*
+	 * vhost_net_rx_peek_head_len()核心思想!!
+	 * 如果sock支持skb_array:
+	 *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+	 *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+	 * 如果sock不支持skb_array:
+	 *     从sk->sk_receive_queue取skb ... ...
+	 */
 	while ((sock_len = vhost_net_rx_peek_head_len(net, sock->sk))) {
 		sock_len += sock_hlen;
 		vhost_len = sock_len + vhost_hlen;
+		/*
+		 * struct vhost_virtqueue *vq = &nvq->vq;
+		 *   -> struct vring_used_elem *heads;
+		 */
 		headcount = get_rx_bufs(vq, vq->heads, vhost_len,
 					&in, vq_log, &log,
 					likely(mergeable) ? UIO_MAXIOV : 1);
@@ -794,6 +1048,9 @@ static void handle_rx(struct vhost_net *net)
 			 * they refilled. */
 			goto out;
 		}
+		/*
+		 * 核心思想是返回rxq->queue[rxq->head], 增加rxq->head
+		 */
 		if (nvq->rx_array)
 			msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
 		/* On overrun, truncate and discard */
@@ -813,6 +1070,9 @@ static void handle_rx(struct vhost_net *net)
 			 */
 			iov_iter_advance(&msg.msg_iter, vhost_hlen);
 		}
+		/*
+		 * 对于macvlan/macvtap是tap_recvmsg
+		 */
 		err = sock->ops->recvmsg(sock, &msg,
 					 sock_len, MSG_DONTWAIT | MSG_TRUNC);
 		/* Userspace might have consumed the packet meanwhile:
@@ -895,6 +1155,13 @@ static void handle_rx_net(struct vhost_work *work)
 	handle_rx(net);
 }
 
+/*
+ * 真正有用的:
+ *   - handle_tx_kick()
+ *   - handle_rx_net()
+ *
+ * struct file_operations vhost_net_fops.open = vhost_net_open()
+ */
 static int vhost_net_open(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n;
@@ -912,6 +1179,11 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		return -ENOMEM;
 	}
 
+	/*
+	 * 在以下使用VHOST_RX_BATCH=64:
+	 *   - drivers/vhost/net.c|162| <<vhost_net_buf_produce>> VHOST_RX_BATCH);
+	 *   - drivers/vhost/net.c|925| <<vhost_net_open>> queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
+	 */
 	queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
 			      GFP_KERNEL);
 	if (!queue) {
@@ -919,6 +1191,14 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		kvfree(n);
 		return -ENOMEM;
 	}
+	/*
+	 * struct vhost_net *n:
+	 *  -> struct vhost_dev dev;
+	 *  -> struct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];
+	 *      -> struct vhost_virtqueue vq;
+	 *          -> struct vhost_poll poll;
+	 *  -> struct vhost_poll poll[VHOST_NET_VQ_MAX];
+	 */
 	n->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;
 
 	dev = &n->dev;
@@ -991,6 +1271,9 @@ static void vhost_net_flush(struct vhost_net *n)
 	}
 }
 
+/*
+ * struct file_operations vhost_net_fops.release = vhost_net_release()
+ */
 static int vhost_net_release(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n = f->private_data;
@@ -1041,6 +1324,10 @@ static struct socket *get_raw_socket(int fd)
 	return ERR_PTR(r);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1325| <<vhost_net_set_backend>> nvq->rx_array = get_tap_skb_array(fd);
+ */
 static struct skb_array *get_tap_skb_array(int fd)
 {
 	struct skb_array *array;
@@ -1216,6 +1503,10 @@ static long vhost_net_reset_owner(struct vhost_net *n)
 	return err;
 }
 
+/*
+ * 处理VHOST_SET_FEATURES:
+ *   - drivers/vhost/net.c|1318| <<vhost_net_ioctl>> return vhost_net_set_features(n, features);
+ */
 static int vhost_net_set_features(struct vhost_net *n, u64 features)
 {
 	size_t vhost_hlen, sock_hlen, hdr_len;
@@ -1280,6 +1571,9 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	return r;
 }
 
+/*
+ * struct file_operations vhost_net_fops.unlocked_ioctl = vhost_net_ioctl()
+ */
 static long vhost_net_ioctl(struct file *f, unsigned int ioctl,
 			    unsigned long arg)
 {
@@ -1323,6 +1617,9 @@ static long vhost_net_ioctl(struct file *f, unsigned int ioctl,
 }
 
 #ifdef CONFIG_COMPAT
+/*
+ * struct file_operations vhost_net_fops.compat_ioctl = vhost_net_compat_ioctl()
+ */
 static long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,
 				   unsigned long arg)
 {
@@ -1330,6 +1627,9 @@ static long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,
 }
 #endif
 
+/*
+ * struct file_operations vhost_net_fops.read_iter = vhost_net_chr_read_iter()
+ */
 static ssize_t vhost_net_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)
 {
 	struct file *file = iocb->ki_filp;
@@ -1340,6 +1640,9 @@ static ssize_t vhost_net_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)
 	return vhost_chr_read_iter(dev, to, noblock);
 }
 
+/*
+ * struct file_operations vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ */
 static ssize_t vhost_net_chr_write_iter(struct kiocb *iocb,
 					struct iov_iter *from)
 {
@@ -1350,6 +1653,9 @@ static ssize_t vhost_net_chr_write_iter(struct kiocb *iocb,
 	return vhost_chr_write_iter(dev, from);
 }
 
+/*
+ * struct file_operations vhost_net_fops.poll = vhost_net_chr_poll()
+ */
 static unsigned int vhost_net_chr_poll(struct file *file, poll_table *wait)
 {
 	struct vhost_net *n = file->private_data;
@@ -1358,6 +1664,9 @@ static unsigned int vhost_net_chr_poll(struct file *file, poll_table *wait)
 	return vhost_chr_poll(file, dev, wait);
 }
 
+/*
+ * struct miscdevice vhost_net_misc.fops = vhost_net_fops()
+ */
 static const struct file_operations vhost_net_fops = {
 	.owner          = THIS_MODULE,
 	.release        = vhost_net_release,
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 145dd3125da0..d891bfb8f401 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -281,21 +281,35 @@ static void vhost_scsi_put_inflight(struct vhost_scsi_inflight *inflight)
 	kref_put(&inflight->kref, vhost_scsi_done_inflight);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode = vhost_scsi_check_true()
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode_cache = vhost_scsi_check_true()
+ */
 static int vhost_scsi_check_true(struct se_portal_group *se_tpg)
 {
 	return 1;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode_write_protect = vhost_scsi_check_false()
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_prod_mode_write_protect = vhost_scsi_check_false()
+ */
 static int vhost_scsi_check_false(struct se_portal_group *se_tpg)
 {
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.get_fabric_name = vhost_scsi_get_fabric_name()
+ */
 static char *vhost_scsi_get_fabric_name(void)
 {
 	return "vhost";
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_wwn = vhost_scsi_get_fabric_wwn()
+ */
 static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -305,6 +319,9 @@ static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 	return &tport->tport_name[0];
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_tag = vhost_scsi_get_tpgt()
+ */
 static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -312,6 +329,9 @@ static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 	return tpg->tport_tpgt;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_prot_fabric_only = vhost_scsi_check_prot_fabric_only()
+ */
 static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -320,11 +340,17 @@ static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 	return tpg->tv_fabric_prot_type;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_inst_index = vhost_scsi_tpg_get_inst_index()
+ */
 static u32 vhost_scsi_tpg_get_inst_index(struct se_portal_group *se_tpg)
 {
 	return 1;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.release_cmd = vhost_scsi_release_cmd()
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *tv_cmd = container_of(se_cmd,
@@ -345,11 +371,17 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 	target_free_tag(se_sess, se_cmd);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.sess_get_index = vhost_scsi_sess_get_index()
+ */
 static u32 vhost_scsi_sess_get_index(struct se_session *se_sess)
 {
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.write_pending = vhost_scsi_write_pending()
+ */
 static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 {
 	/* Go ahead and process the write immediately */
@@ -357,16 +389,25 @@ static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.write_pending_status = vhost_scsi_write_pending_status()
+ */
 static int vhost_scsi_write_pending_status(struct se_cmd *se_cmd)
 {
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.set_default_node_attributes = vhost_scsi_set_default_node_attrs()
+ */
 static void vhost_scsi_set_default_node_attrs(struct se_node_acl *nacl)
 {
 	return;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.get_cmd_state = vhost_scsi_get_cmd_state()
+ */
 static int vhost_scsi_get_cmd_state(struct se_cmd *se_cmd)
 {
 	return 0;
@@ -381,6 +422,9 @@ static void vhost_scsi_complete_cmd(struct vhost_scsi_cmd *cmd)
 	vhost_work_queue(&vs->dev, &vs->vs_completion_work);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_data_in = vhost_scsi_queue_data_in()
+ */
 static int vhost_scsi_queue_data_in(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -389,6 +433,9 @@ static int vhost_scsi_queue_data_in(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_status = vhost_scsi_queue_status()
+ */
 static int vhost_scsi_queue_status(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -397,11 +444,17 @@ static int vhost_scsi_queue_status(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_tm_rsp = vhost_scsi_queue_tm_rsp()
+ */
 static void vhost_scsi_queue_tm_rsp(struct se_cmd *se_cmd)
 {
 	return;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.aborted_task = vhost_scsi_aborted_task()
+ */
 static void vhost_scsi_aborted_task(struct se_cmd *se_cmd)
 {
 	return;
@@ -448,6 +501,9 @@ static void vhost_scsi_free_cmd(struct vhost_scsi_cmd *cmd)
 
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.check_stop_free = vhost_scsi_check_stop_free()
+ */
 static int vhost_scsi_check_stop_free(struct se_cmd *se_cmd)
 {
 	return target_put_sess_cmd(se_cmd);
@@ -823,6 +879,11 @@ static void vhost_scsi_submission_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1162| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ *   - drivers/vhost/scsi.c|1310| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ */
 static void
 vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 			   struct vhost_virtqueue *vq,
@@ -842,12 +903,28 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1046| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1294| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * struct vhost_scsi_ctx {
+	 *     int head;
+	 *     unsigned int out, in;
+	 *     size_t req_size, rsp_size;
+	 *     size_t out_size, in_size;
+	 *     u8 *target, *lunp;
+	 *     void *req;
+	 *     struct iov_iter out_iter;
+	 * };
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -900,6 +977,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1075| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1353| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, NULL);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
@@ -940,6 +1022,16 @@ vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 	return ret;
 }
 
+/*
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|1368| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -957,7 +1049,11 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	bool t10_pi = vhost_has_feature(vq, VIRTIO_SCSI_F_T10_PI);
 	void *cdb;
 
+	/*
+	 * struct vhost_virtqueue *vq
+	 */
 	mutex_lock(&vq->mutex);
+
 	/*
 	 * We can handle the vq only after the endpoint is setup by calling the
 	 * VHOST_SCSI_SET_ENDPOINT ioctl.
@@ -972,6 +1068,11 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	for (;;) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1046| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *   - drivers/vhost/scsi.c|1294| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
@@ -1359,6 +1460,16 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 在以下使用vhost_scsi_handle_kick():
+ *   - drivers/vhost/scsi.c|1657| <<vhost_scsi_open>> vs->vqs[i].vq.handle_kick = vhost_scsi_handle_kick;
+ */
 static void vhost_scsi_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1844,6 +1955,9 @@ static void vhost_scsi_hotunplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 	vhost_scsi_do_plug(tpg, lun, false);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_post_link = vhost_scsi_port_link()
+ */
 static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 			       struct se_lun *lun)
 {
@@ -1863,6 +1977,9 @@ static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_pre_unlink = vhost_scsi_port_unlink()
+ */
 static void vhost_scsi_port_unlink(struct se_portal_group *se_tpg,
 				  struct se_lun *lun)
 {
@@ -1931,6 +2048,9 @@ static ssize_t vhost_scsi_tpg_attrib_fabric_prot_type_show(
 
 CONFIGFS_ATTR(vhost_scsi_tpg_attrib_, fabric_prot_type);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_attrib_attrs = vhost_scsi_tpg_attrib_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrib_attrs[] = {
 	&vhost_scsi_tpg_attrib_attr_fabric_prot_type,
 	NULL,
@@ -2163,11 +2283,17 @@ static ssize_t vhost_scsi_tpg_nexus_store(struct config_item *item,
 
 CONFIGFS_ATTR(vhost_scsi_tpg_, nexus);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_base_attrs = vhost_scsi_tpg_attrs[]
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrs[] = {
 	&vhost_scsi_tpg_attr_nexus,
 	NULL,
 };
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_tpg = vhost_scsi_make_tpg()
+ */
 static struct se_portal_group *
 vhost_scsi_make_tpg(struct se_wwn *wwn,
 		   struct config_group *group,
@@ -2207,6 +2333,9 @@ vhost_scsi_make_tpg(struct se_wwn *wwn,
 	return &tpg->se_tpg;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_tpg = vhost_scsi_drop_tpg()
+ */
 static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -2226,6 +2355,9 @@ static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 	kfree(tpg);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_wwn = vhost_scsi_make_tport()
+ */
 static struct se_wwn *
 vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 		     struct config_group *group,
@@ -2287,6 +2419,9 @@ vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 	return &tport->tport_wwn;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_wwn = vhost_scsi_drop_tport()
+ */
 static void vhost_scsi_drop_tport(struct se_wwn *wwn)
 {
 	struct vhost_scsi_tport *tport = container_of(wwn,
@@ -2309,11 +2444,19 @@ vhost_scsi_wwn_version_show(struct config_item *item, char *page)
 
 CONFIGFS_ATTR_RO(vhost_scsi_wwn_, version);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_wwn_attrs = vhost_scsi_wwn_attrs[]
+ */
 static struct configfs_attribute *vhost_scsi_wwn_attrs[] = {
 	&vhost_scsi_wwn_attr_version,
 	NULL,
 };
 
+/*
+ * 在以下使用vhost_scsi_ops:
+ *   - drivers/vhost/scsi.c|2469| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/vhost/scsi.c|2485| <<vhost_scsi_exit>> target_unregister_template(&vhost_scsi_ops);
+ */
 static const struct target_core_fabric_ops vhost_scsi_ops = {
 	.module				= THIS_MODULE,
 	.name				= "vhost",
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index a59fefc6e5ea..180ceeeb61f8 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -34,20 +34,86 @@
 
 #include "vhost.h"
 
+/*
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] eventfd_signal
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] eventfd_signal
+ * [0] vhost_add_used_and_signal_n
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ */
+
+/*
+ * 在以下使用max_mem_regions:
+ *   - drivers/vhost/vhost.c|1333| <<vhost_set_memory>> if (mem.nregions > max_mem_regions)
+ */
 static ushort max_mem_regions = 64;
 module_param(max_mem_regions, ushort, 0444);
 MODULE_PARM_DESC(max_mem_regions,
 	"Maximum number of memory regions in memory map. (default: 64)");
+/*
+ * 在以下使用max_iotlb_entries:
+ *   - drivers/vhost/vhost.c|958| <<vhost_new_umem_range>> if (umem->numem == max_iotlb_entries) {
+ */
 static int max_iotlb_entries = 2048;
 module_param(max_iotlb_entries, int, 0444);
 MODULE_PARM_DESC(max_iotlb_entries,
 	"Maximum number of iotlb entries. (default: 2048)");
 
 enum {
+	/* 没人用 ??? */
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * last_avail_idx表示前端处理到avail ring的哪个元素了
+ * ++之后表示下次待处理的avail ring的哪个元素
+ * 并将这个信息放入了used ring的最后一个元素
+ * 前端驱动通过读取used ring的最后一个元素就知道后端处理到哪里了
+ */
+
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2328| <<vhost_notify>> if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1799| <<vhost_update_avail_event>> vhost_avail_event(vq)))
+ *   - drivers/vhost/vhost.c|1806| <<vhost_update_avail_event>> used = vhost_avail_event(vq);
+ *   - drivers/vhost/vhost.c|1809| <<vhost_update_avail_event>> sizeof *vhost_avail_event(vq));
+ *   - drivers/vhost/vhost.c|2426| <<vhost_enable_notify>> vhost_avail_event(vq), r);
+ */
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
 INTERVAL_TREE_DEFINE(struct vhost_umem_node,
@@ -55,6 +121,9 @@ INTERVAL_TREE_DEFINE(struct vhost_umem_node,
 		     START, LAST, static inline, vhost_umem_interval_tree);
 
 #ifdef CONFIG_VHOST_CROSS_ENDIAN_LEGACY
+/*
+ * !!! 好多的CONFIG_VHOST_CROSS_ENDIAN_LEGACY都没设置!!!
+ */
 static void vhost_disable_cross_endian(struct vhost_virtqueue *vq)
 {
 	vq->user_be = !virtio_legacy_is_little_endian();
@@ -70,6 +139,10 @@ static void vhost_enable_cross_endian_little(struct vhost_virtqueue *vq)
 	vq->user_be = false;
 }
 
+/*
+ * 处理VHOST_SET_VRING_ENDIAN:
+ *   - drivers/vhost/vhost.c|1566| <<vhost_vring_ioctl>> r = vhost_set_vring_endian(vq, argp);
+ */
 static long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)
 {
 	struct vhost_vring_state s;
@@ -92,6 +165,10 @@ static long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)
 	return 0;
 }
 
+/*
+ * 处理VHOST_GET_VRING_ENDIAN:
+ *   - drivers/vhost/vhost.c|1569| <<vhost_vring_ioctl>> r = vhost_get_vring_endian(vq, idx, argp);
+ */
 static long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,
 				   int __user *argp)
 {
@@ -125,19 +202,39 @@ static long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)
 	return -ENOIOCTLCMD;
 }
 
+/*
+ * 处理VHOST_GET_VRING_ENDIAN:
+ *   - drivers/vhost/vhost.c|2146| <<vhost_vring_ioctl>> r = vhost_get_vring_endian(vq, idx, argp);
+ */
 static long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,
 				   int __user *argp)
 {
 	return -ENOIOCTLCMD;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|207| <<vhost_reset_is_le>> vhost_init_is_le(vq);
+ *   - drivers/vhost/vhost.c|2438| <<vhost_vq_init_access>> vhost_init_is_le(vq);
+ *
+ * 计算vq->is_le. 很多is_le是true
+ */
 static void vhost_init_is_le(struct vhost_virtqueue *vq)
 {
+	/*
+	 * 很多is_le是true
+	 */
 	vq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1)
 		|| virtio_legacy_is_little_endian();
 }
 #endif /* CONFIG_VHOST_CROSS_ENDIAN_LEGACY */
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|338| <<vhost_vq_reset>> vhost_reset_is_le(vq);
+ *
+ * 计算vq->is_le. 很多is_le是true
+ */
 static void vhost_reset_is_le(struct vhost_virtqueue *vq)
 {
 	vhost_init_is_le(vq);
@@ -145,27 +242,177 @@ static void vhost_reset_is_le(struct vhost_virtqueue *vq)
 
 struct vhost_flush_struct {
 	struct vhost_work work;
+	/*
+	 * 在以下使用vhost_flush_struct->wait_event:
+	 *   - drivers/vhost/vhost.c|191| <<vhost_flush_work>> complete(&s->wait_event);
+	 *   - drivers/vhost/vhost.c|378| <<vhost_work_flush>> init_completion(&flush.wait_event);
+	 *   - drivers/vhost/vhost.c|382| <<vhost_work_flush>> wait_for_completion(&flush.wait_event);
+	 */
 	struct completion wait_event;
 };
 
+/*
+ * 在以下使用vhost_flush_work():
+ *   - drivers/vhost/vhost.c|253| <<vhost_work_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ */
 static void vhost_flush_work(struct vhost_work *work)
 {
 	struct vhost_flush_struct *s;
 
 	s = container_of(work, struct vhost_flush_struct, work);
+	/*
+	 * 在以下使用vhost_flush_struct->wait_event:
+	 *   - drivers/vhost/vhost.c|191| <<vhost_flush_work>> complete(&s->wait_event);
+	 *   - drivers/vhost/vhost.c|378| <<vhost_work_flush>> init_completion(&flush.wait_event);
+	 *   - drivers/vhost/vhost.c|382| <<vhost_work_flush>> wait_for_completion(&flush.wait_event);
+	 */
 	complete(&s->wait_event);
 }
 
+/*
+ * [0] vhost_poll_func
+ * [0] eventfd_poll
+ * [0] vhost_poll_start.part.19
+ * [0] vhost_vring_ioctl
+ * [0] vhost_scsi_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vhost_poll_func
+ * [0] vhost_poll_start.part.19
+ * [0] vhost_poll_start
+ * [0] vhost_net_enable_vq
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|232| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ *
+ * 这个函数被配置给了poll_table->_qproc
+ * socket和eventfd的poll会调用poll_wait()-->调用poll_table->_qproc=vhost_poll_func()
+ *
+ * 把&poll->wait加入到wqh
+ *
+ * 对于socket: 把n->poll->wait加入到socket的wqh
+ * 对于eventfd, 把vhost_virtqueue->poll->wait加入到socket的wqh
+ *
+ *
+ * file->poll()的时候调用这个函数
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt, unsigned long unused)
 {
 	struct vhost_poll *poll;
 
+	/*
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	poll = container_of(pt, struct vhost_poll, table);
 	poll->wqh = wqh;
+	/*
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以对于socket和eventfd, 唤醒的waitqueue的func都是vhost_poll_wakeup()
+	 *
+	 * 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程
+	 *
+	 * socket的入口: tun_net_xmit()-->sock_def_readable()
+	 * eventfd的入口: handle_ept_misconfig()-->kvm_io_bus_write()-->eventfd_signal()
+	 *
+	 * struct vhost_poll {
+	 *     poll_table                table;
+	 *     wait_queue_head_t        *wqh;
+	 *     wait_queue_entry_t              wait;
+	 *     struct vhost_work         work;
+	 *     unsigned long             mask;
+	 *     struct vhost_dev         *dev;
+	 * };
+	 *
+	 * 把entry的&poll->wait加入head的wqh
+	 */
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * [0] vhost_poll_wakeup
+ * [0] __wake_up_common
+ * [0] __wake_up_locked_key
+ * [0] eventfd_signal
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * vhost_poll_wakeup()的例子:
+ * [0] vhost_poll_wakeup
+ * [0] __wake_up_common_lock
+ * [0] __wake_up_sync_key
+ * [0] sock_def_readable
+ * [0] __dta_tun_net_xmit_80
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __dev_queue_xmit
+ * [0] dev_queue_xmit
+ * [0] br_dev_queue_push_xmit
+ * [0] br_forward_finish
+ * [0] __br_forward
+ * [0] deliver_clone
+ * [0] br_flood
+ * [0] br_handle_frame_finish
+ * [0] br_handle_frame
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] ixgbe_clean_rx_irq
+ * [0] ixgbe_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|231| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|253| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, (void *)mask);
+ *
+ * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+ * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+ *
+ * 所以对于socket和eventfd, 唤醒的waitqueue的func都是vhost_poll_wakeup()
+ *
+ * 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程
+ *
+ * socket的入口: tun_net_xmit()-->sock_def_readable()
+ * eventfd的入口: handle_ept_misconfig()-->kvm_io_bus_write()-->eventfd_signal()
+ *
+ * 
+ * 作为wait entry的handler在poll被唤醒的时候调用
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -174,10 +421,22 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	if (!((unsigned long)key & poll->mask))
 		return 0;
 
+	/*
+	 * 把vhost_work挂到vhost_dev上然后唤醒内核线程
+	 */
 	vhost_poll_queue(poll);
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1645| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/scsi.c|1646| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/vhost.c|237| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|280| <<vhost_work_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vhost.c|517| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ *   - drivers/vhost/vsock.c|539| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -187,21 +446,78 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
 /* Init poll structure */
+/*
+ * called by:
+ *   - vhost_net_open(), 初始化n->poll
+ *   - vhost_net_open(), 初始化n->poll
+ *   - vhost_dev_init(), 初始化vhost_virtqueue->poll
+ *
+ * called by:
+ *   - drivers/vhost/net.c|950| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);
+ *   - drivers/vhost/net.c|951| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);
+ *   - drivers/vhost/vhost.c|529| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick,
+ */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     unsigned long mask, struct vhost_dev *dev)
 {
+	/*
+	 * 把vhost_poll->wait的handler设置成vhost_poll_wakeup
+	 * 这样当vhost_poll在fd上被唤醒的时候就调用vhost_poll_wakeup()
+	 */
 	init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	/*
+	 * 这个函数被配置给了poll_table->_qproc
+	 * socket和eventfd的poll会调用poll_wait()-->调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * 把&poll->wait加入到wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到socket的wqh
+	 *
+	 *
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	init_poll_funcptr(&poll->table, vhost_poll_func);
 	poll->mask = mask;
 	poll->dev = dev;
+	/*
+	 * 在以下使用vhost_poll->wqh:
+	 *   - drivers/vhost/vhost.c|261| <<vhost_poll_func>> poll->wqh = wqh;
+	 *   - drivers/vhost/vhost.c|274| <<vhost_poll_func>> add_wait_queue(wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|376| <<vhost_poll_init>> poll->wqh = NULL;
+	 *   - drivers/vhost/vhost.c|400| <<vhost_poll_start>> if (poll->wqh)
+	 *   - drivers/vhost/vhost.c|439| <<vhost_poll_stop>> if (poll->wqh) {
+	 *   - drivers/vhost/vhost.c|440| <<vhost_poll_stop>> remove_wait_queue(poll->wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|441| <<vhost_poll_stop>> poll->wqh = NULL
+	 */
 	poll->wqh = NULL;
 
+	/*
+	 * fn可能是:
+	 *   - handle_tx_net()
+	 *   - handle_rx_net()
+	 *   - vq->handle_kick
+	 */
 	vhost_work_init(&poll->work, fn);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|408| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/vhost.c|1669| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *
+ * - vhost_net_enable_vq()  ---> poll在sock->file
+ *   这里poll是n->poll, 是handle_rx_net()
+ *
+ * - vhost_vring_ioctl():VHOST_SET_VRING_KICK  --->设置host notifier, poll在eventfd
+ *   这里poll是vhost_virtqueue->poll, 是handle_tx_kick()
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	unsigned long mask;
@@ -210,6 +526,25 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (poll->wqh)
 		return 0;
 
+	/*
+	 * socket : tun_chr_poll() --> poll_wait()
+	 * eventfd: eventfd_poll() --> poll_wait()
+	 *
+	 * poll_wait()就是调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * vhost_poll_func()把&poll->wait加入到fd的wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以到时候waitqueue唤醒的时候都是调用vhost_poll_wakeup()
+	 *
+	 *
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	mask = file->f_op->poll(file, &poll->table);
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, (void *)mask);
@@ -224,8 +559,25 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|393| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|273| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|649| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1661| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
+	/*
+	 * struct vhost_poll {
+	 *     poll_table                table;
+	 *     wait_queue_head_t        *wqh;
+	 *     wait_queue_entry_t              wait;
+	 *     struct vhost_work         work;
+	 *     unsigned long             mask;
+	 *     struct vhost_dev         *dev;
+	 * };
+	 */
 	if (poll->wqh) {
 		remove_wait_queue(poll->wqh, &poll->wait);
 		poll->wqh = NULL;
@@ -233,6 +585,14 @@ void vhost_poll_stop(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_stop);
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1396| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1397| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|310| <<vhost_poll_flush>> vhost_work_flush(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|564| <<vhost_attach_cgroups>> vhost_work_flush(dev, &attach.work);
+ *   - drivers/vhost/vsock.c|554| <<vhost_vsock_flush>> vhost_work_flush(&vsock->dev, &vsock->send_pkt_work);
+ */
 void vhost_work_flush(struct vhost_dev *dev, struct vhost_work *work)
 {
 	struct vhost_flush_struct flush;
@@ -249,12 +609,111 @@ EXPORT_SYMBOL_GPL(vhost_work_flush);
 
 /* Flush any work that has been scheduled. When calling this, don't hold any
  * locks that are also used by the callback. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|983| <<vhost_net_flush_vq>> vhost_poll_flush(n->poll + index);
+ *   - drivers/vhost/net.c|984| <<vhost_net_flush_vq>> vhost_poll_flush(&n->vqs[index].vq.poll);
+ *   - drivers/vhost/scsi.c|1373| <<vhost_scsi_flush_vq>> vhost_poll_flush(&vs->vqs[index].vq.poll);
+ *   - drivers/vhost/test.c|145| <<vhost_test_flush_vq>> vhost_poll_flush(&n->vqs[index].poll);
+ *   - drivers/vhost/vhost.c|650| <<vhost_dev_stop>> vhost_poll_flush(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1674| <<vhost_vring_ioctl>> vhost_poll_flush(&vq->poll);
+ *   - drivers/vhost/vsock.c|553| <<vhost_vsock_flush>> vhost_poll_flush(&vsock->vqs[i].poll);
+ * 
+ * 测试的没找到调用
+ */
 void vhost_poll_flush(struct vhost_poll *poll)
 {
 	vhost_work_flush(poll->dev, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_flush);
 
+/*
+ * [0] vhost_work_queue
+ * [0] vhost_zerocopy_callback
+ * [0] skb_release_data
+ * [0] skb_release_all
+ * [0] napi_consume_skb
+ * [0] ixgbe_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * [0] vhost_work_queue
+ * [0] __wake_up_common
+ * [0] __wake_up_locked_key
+ * [0] eventfd_signal
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vhost_work_queue
+ * [0] __wake_up_common
+ * [0] __wake_up_common_lock
+ * [0] __wake_up_sync_key
+ * [0] sock_def_readable
+ * [0] __dta_tun_net_xmit_80
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __dev_queue_xmit
+ * [0] dev_queue_xmit
+ * [0] br_dev_queue_push_xmit
+ * [0] br_forward_finish
+ * [0] __br_forward
+ * [0] deliver_clone
+ * [0] br_flood
+ * [0] br_handle_frame_finish
+ * [0] br_handle_frame
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] ixgbe_clean_rx_irq
+ * [0] ixgbe_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|381| <<vhost_scsi_complete_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1343| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|282| <<vhost_work_flush>> vhost_work_queue(dev, &flush.work);
+ *   - drivers/vhost/vhost.c|321| <<vhost_poll_queue>> vhost_work_queue(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|518| <<vhost_attach_cgroups>> vhost_work_queue(dev, &attach.work);
+ *   - drivers/vhost/vhost.h|42| <<vhost_attach_cgroups>> void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work);
+ *   - drivers/vhost/vsock.c|223| <<vhost_transport_send_pkt>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ *
+ * 把vhost_work挂到vhost_dev上然后唤醒内核线程
+ */
 void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 {
 	if (!dev->worker)
@@ -265,6 +724,14 @@ void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 		 * sure it was not in the list.
 		 * test_and_set_bit() implies a memory barrier.
 		 */
+		/*
+		 * 在以下使用vhost_dev->work_list:
+		 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+		 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+		 */
 		llist_add(&work->node, &dev->work_list);
 		wake_up_process(dev->worker);
 	}
@@ -272,18 +739,44 @@ void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 EXPORT_SYMBOL_GPL(vhost_work_queue);
 
 /* A lockless hint for busy polling code to exit the loop */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|382| <<vhost_can_busy_poll>> !vhost_has_work(dev);
+ */
 bool vhost_has_work(struct vhost_dev *dev)
 {
 	return !llist_empty(&dev->work_list);
 }
 EXPORT_SYMBOL_GPL(vhost_has_work);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|366| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|576| <<handle_tx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|639| <<vhost_net_rx_peek_head_len>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|642| <<vhost_net_rx_peek_head_len>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|862| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/test.c|85| <<handle_vq>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|215| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|1025| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vsock.c|186| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|266| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *
+ * 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
+	/* 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程 */
 	vhost_work_queue(poll->dev, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_queue);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|721| <<vhost_vq_meta_reset>> __vhost_vq_meta_reset(d->vqs[i]);
+ *   - drivers/vhost/vhost.c|753| <<vhost_vq_reset>> __vhost_vq_meta_reset(vq);
+ *   - drivers/vhost/vhost.c|2201| <<vhost_init_device_iotlb>> __vhost_vq_meta_reset(vq);
+ */
 static void __vhost_vq_meta_reset(struct vhost_virtqueue *vq)
 {
 	int j;
@@ -292,6 +785,11 @@ static void __vhost_vq_meta_reset(struct vhost_virtqueue *vq)
 		vq->meta_iotlb[j] = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1554| <<vhost_process_iotlb_msg>> vhost_vq_meta_reset(dev);
+ *   - drivers/vhost/vhost.c|1564| <<vhost_process_iotlb_msg>> vhost_vq_meta_reset(dev);
+ */
 static void vhost_vq_meta_reset(struct vhost_dev *d)
 {
 	int i;
@@ -300,6 +798,11 @@ static void vhost_vq_meta_reset(struct vhost_dev *d)
 		__vhost_vq_meta_reset(d->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|892| <<vhost_dev_init>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|1086| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -332,6 +835,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	__vhost_vq_meta_reset(vq);
 }
 
+/*
+ * 在以下创建kthread的时候使用:
+ *   - drivers/vhost/vhost.c|843| <<vhost_dev_set_owner>> worker = kthread_create(vhost_worker, dev, "vhost-%d", current->pid);
+ */
 static int vhost_worker(void *data)
 {
 	struct vhost_dev *dev = data;
@@ -351,6 +858,14 @@ static int vhost_worker(void *data)
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_dev->work_list:
+		 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+		 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+		 */
 		node = llist_del_all(&dev->work_list);
 		if (!node)
 			schedule();
@@ -382,6 +897,10 @@ static void vhost_vq_free_iovecs(struct vhost_virtqueue *vq)
 }
 
 /* Helper to allocate iovec buffers for all vqs. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|970| <<vhost_dev_set_owner>> err = vhost_dev_alloc_iovecs(dev);
+ */
 static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)
 {
 	struct vhost_virtqueue *vq;
@@ -412,6 +931,13 @@ static void vhost_dev_free_iovecs(struct vhost_dev *dev)
 		vhost_vq_free_iovecs(dev->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|948| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);
+ *   - drivers/vhost/scsi.c|1659| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ);
+ *   - drivers/vhost/test.c|119| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX);
+ *   - drivers/vhost/vsock.c|534| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs)
 {
@@ -427,8 +953,24 @@ void vhost_dev_init(struct vhost_dev *dev,
 	dev->iotlb = NULL;
 	dev->mm = NULL;
 	dev->worker = NULL;
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	init_llist_head(&dev->work_list);
 	init_waitqueue_head(&dev->wait);
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	INIT_LIST_HEAD(&dev->read_list);
 	INIT_LIST_HEAD(&dev->pending_list);
 	spin_lock_init(&dev->iotlb_lock);
@@ -442,6 +984,14 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
 		vhost_vq_reset(dev, vq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|950| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);
+		 *   - drivers/vhost/net.c|951| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);
+		 *   - drivers/vhost/vhost.c|529| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick,
+		 *
+		 * 下面是POLLIN, 不是POLLOUT
+		 */
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
 					POLLIN, dev);
@@ -450,6 +1000,16 @@ void vhost_dev_init(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_dev_init);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1386| <<vhost_net_set_backend>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/net.c|1479| <<vhost_net_reset_owner>> err = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|178| <<vhost_test_run>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|226| <<vhost_test_reset_owner>> err = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/vhost.c|2263| <<vhost_dev_ioctl>> r = vhost_dev_check_owner(d);
+ *   - drivers/vhost/vsock.c|434| <<vhost_vsock_start>> ret = vhost_dev_check_owner(&vsock->dev);
+ *   - drivers/vhost/vsock.c|484| <<vhost_vsock_stop>> ret = vhost_dev_check_owner(&vsock->dev);
+ */
 long vhost_dev_check_owner(struct vhost_dev *dev)
 {
 	/* Are you the owner? If not, I don't think you mean to do that */
@@ -471,6 +1031,10 @@ static void vhost_attach_cgroups_work(struct vhost_work *work)
 	s->ret = cgroup_attach_task_all(s->owner, current);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1002| <<vhost_dev_set_owner>> err = vhost_attach_cgroups(dev);
+ */
 static int vhost_attach_cgroups(struct vhost_dev *dev)
 {
 	struct vhost_attach_cgroups_struct attach;
@@ -533,6 +1097,11 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1487| <<vhost_net_reset_owner>> umem = vhost_dev_reset_owner_prepare();
+ *   - drivers/vhost/test.c|229| <<vhost_test_reset_owner>> umem = vhost_dev_reset_owner_prepare();
+ */
 struct vhost_umem *vhost_dev_reset_owner_prepare(void)
 {
 	return kvzalloc(sizeof(struct vhost_umem), GFP_KERNEL);
@@ -540,6 +1109,11 @@ struct vhost_umem *vhost_dev_reset_owner_prepare(void)
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1490| <<vhost_net_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ *   - drivers/vhost/test.c|236| <<vhost_test_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ */
 void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_umem *umem)
 {
 	int i;
@@ -557,6 +1131,13 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_umem *umem)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1285| <<vhost_net_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/net.c|1494| <<vhost_net_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/scsi.c|1780| <<vhost_scsi_release>> vhost_dev_stop(&vs->dev);
+ *   - drivers/vhost/vsock.c|593| <<vhost_vsock_dev_release>> vhost_dev_stop(&vsock->dev);
+ */
 void vhost_dev_stop(struct vhost_dev *dev)
 {
 	int i;
@@ -570,6 +1151,16 @@ void vhost_dev_stop(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_stop);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1082| <<vhost_umem_clean>> vhost_umem_free(umem, node);
+ *   - drivers/vhost/vhost.c|1485| <<vhost_new_umem_range>> vhost_umem_free(umem, tmp);
+ *   - drivers/vhost/vhost.c|1528| <<vhost_del_umem_range>> vhost_umem_free(umem, node);
+ *
+ * 把vhost_umem_node从umem->umem_list删除
+ * 再从umem->umem_tree删除
+ * 减少umem->numem
+ */
 static void vhost_umem_free(struct vhost_umem *umem,
 			    struct vhost_umem_node *node)
 {
@@ -579,6 +1170,14 @@ static void vhost_umem_free(struct vhost_umem *umem,
 	umem->numem--;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1177| <<vhost_dev_cleanup>> vhost_umem_clean(dev->umem);
+ *   - drivers/vhost/vhost.c|1179| <<vhost_dev_cleanup>> vhost_umem_clean(dev->iotlb);
+ *   - drivers/vhost/vhost.c|2210| <<vhost_set_memory>> vhost_umem_clean(oldumem);
+ *   - drivers/vhost/vhost.c|2214| <<vhost_set_memory>> vhost_umem_clean(newumem);
+ *   - drivers/vhost/vhost.c|2459| <<vhost_init_device_iotlb>> vhost_umem_clean(oiotlb);
+ */
 static void vhost_umem_clean(struct vhost_umem *umem)
 {
 	struct vhost_umem_node *node, *tmp;
@@ -592,12 +1191,24 @@ static void vhost_umem_clean(struct vhost_umem *umem)
 	kvfree(umem);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1177| <<vhost_dev_cleanup>> vhost_clear_msg(dev);
+ */
 static void vhost_clear_msg(struct vhost_dev *dev)
 {
 	struct vhost_msg_node *node, *n;
 
 	spin_lock(&dev->iotlb_lock);
 
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	list_for_each_entry_safe(node, n, &dev->read_list, node) {
 		list_del(&node->node);
 		kfree(node);
@@ -612,6 +1223,14 @@ static void vhost_clear_msg(struct vhost_dev *dev)
 }
 
 /* Caller should have device mutex if and only if locked is set */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1286| <<vhost_net_release>> vhost_dev_cleanup(&n->dev, false);
+ *   - drivers/vhost/scsi.c|1781| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev, false);
+ *   - drivers/vhost/test.c|160| <<vhost_test_release>> vhost_dev_cleanup(&n->dev, false);
+ *   - drivers/vhost/vhost.c|1066| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev, true);
+ *   - drivers/vhost/vsock.c|606| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev, false);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)
 {
 	int i;
@@ -643,6 +1262,14 @@ void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)
 	dev->iotlb = NULL;
 	vhost_clear_msg(dev);
 	wake_up_interruptible_poll(&dev->wait, POLLIN | POLLRDNORM);
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	WARN_ON(!llist_empty(&dev->work_list));
 	if (dev->worker) {
 		kthread_stop(dev->worker);
@@ -674,6 +1301,11 @@ static bool vhost_overflow(u64 uaddr, u64 size)
 }
 
 /* Caller should have vq mutex and device mutex. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1194| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base,
+ *   - drivers/vhost/vhost.c|1768| <<vq_log_access_ok>> return vq_memory_access_ok(log_base, vq->umem,
+ */
 static int vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,
 			       int log_all)
 {
@@ -700,10 +1332,23 @@ static int vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1292| <<vhost_copy_to_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1330| <<vhost_copy_from_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1392| <<__vhost_get_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1839| <<iotlb_access_ok>> if (vhost_vq_meta_fetch(vq, addr, len, type))
+ */
 static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 					       u64 addr, unsigned int size,
 					       int type)
 {
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[]:
+	 *   - drivers/vhost/vhost.c|713| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1176| <<vhost_vq_meta_fetch>> const struct vhost_umem_node *node = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1693| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = node;
+	 */
 	const struct vhost_umem_node *node = vq->meta_iotlb[type];
 
 	if (!node)
@@ -714,6 +1359,11 @@ static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 
 /* Can we switch to this memory table? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1766| <<vhost_log_access_ok>> return memory_access_ok(dev, dev->umem, 1);
+ *   - drivers/vhost/vhost.c|1883| <<vhost_set_memory>> if (!memory_access_ok(d, newumem, 0))
+ */
 static int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,
 			    int log_all)
 {
@@ -741,6 +1391,10 @@ static int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2220| <<__vhost_add_used_n>> } else if (vhost_copy_to_user(vq, used, heads, count * sizeof *used)) {
+ */
 static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 			      const void *from, unsigned size)
 {
@@ -776,6 +1430,10 @@ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2864| <<vhost_get_vq_desc>> ret = vhost_copy_from_user(vq, &desc, vq->desc + i,
+ */
 static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
 				void __user *from, unsigned size)
 {
@@ -859,6 +1517,16 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	return __vhost_get_user_slow(vq, addr, size, type);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1762| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+ *   - drivers/vhost/vhost.c|1781| <<vhost_update_avail_event>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+ *   - drivers/vhost/vhost.c|2195| <<__vhost_add_used_n>> if (vhost_put_user(vq, heads[0].id, &used->id)) {
+ *   - drivers/vhost/vhost.c|2199| <<__vhost_add_used_n>> if (vhost_put_user(vq, heads[0].len, &used->len)) {
+ *   - drivers/vhost/vhost.c|2252| <<vhost_add_used_n>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+ *
+ * 把第二参数x给第三个参数的ptr
+ */
 #define vhost_put_user(vq, x, ptr)		\
 ({ \
 	int ret = -EFAULT; \
@@ -876,6 +1544,13 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	ret; \
 })
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|898| <<vhost_get_avail>> vhost_get_user(vq, x, ptr, VHOST_ADDR_AVAIL)
+ *   - drivers/vhost/vhost.c|901| <<vhost_get_used>> vhost_get_user(vq, x, ptr, VHOST_ADDR_USED)
+ *
+ * 把ptr的内容读入x
+ */
 #define vhost_get_user(vq, x, ptr, type)		\
 ({ \
 	int ret; \
@@ -894,12 +1569,34 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	ret; \
 })
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2762| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail(vq, avail_idx, &vq->avail->idx))) {
+ *   - drivers/vhost/vhost.c|2792| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail(vq, ring_head,
+ *   - drivers/vhost/vhost.c|3118| <<vhost_notify>> if (vhost_get_avail(vq, flags, &vq->avail->flags)) {
+ *   - drivers/vhost/vhost.c|3151| <<vhost_notify>> if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
+ *   - drivers/vhost/vhost.c|3215| <<vhost_vq_avail_empty>> r = vhost_get_avail(vq, avail_idx, &vq->avail->idx);
+ *   - drivers/vhost/vhost.c|3273| <<vhost_enable_notify>> r = vhost_get_avail(vq, avail_idx, &vq->avail->idx);
+ */
 #define vhost_get_avail(vq, x, ptr) \
 	vhost_get_user(vq, x, ptr, VHOST_ADDR_AVAIL)
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2493| <<vhost_vq_init_access>> r = vhost_get_used(vq, last_used_idx, &vq->used->idx);
+ */
 #define vhost_get_used(vq, x, ptr) \
 	vhost_get_user(vq, x, ptr, VHOST_ADDR_USED)
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1493| <<vhost_process_iotlb_msg>> vhost_dev_lock_vqs(dev);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_dev_lock_vqs()
+ */
 static void vhost_dev_lock_vqs(struct vhost_dev *d)
 {
 	int i = 0;
@@ -907,6 +1604,15 @@ static void vhost_dev_lock_vqs(struct vhost_dev *d)
 		mutex_lock_nested(&d->vqs[i]->mutex, i);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1523| <<vhost_process_iotlb_msg>> vhost_dev_unlock_vqs(dev);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_dev_unlock_vqs()
+ */
 static void vhost_dev_unlock_vqs(struct vhost_dev *d)
 {
 	int i = 0;
@@ -914,17 +1620,57 @@ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
 		mutex_unlock(&d->vqs[i]->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1495| <<vhost_process_iotlb_msg>> if (vhost_new_umem_range(dev->iotlb, msg->iova, msg->size,
+ *   - drivers/vhost/vhost.c|1812| <<vhost_set_memory>> if (vhost_new_umem_range(newumem
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_new_umem_range()
+ *
+ * vhost_dev_ioctl(VHOST_SET_MEM_TABLE)
+ * -> vhost_set_memory()
+ *
+ * 申请新的vhost_umem_node,插入umem->umem_list和umem->umem_tree
+ * 增加umem->numem++
+ */
 static int vhost_new_umem_range(struct vhost_umem *umem,
 				u64 start, u64 size, u64 end,
 				u64 userspace_addr, int perm)
 {
+	/*
+	 * struct vhost_umem_node {
+	 *     struct rb_node rb;
+	 *     struct list_head link;
+	 *     __u64 start;
+	 *     __u64 last;
+	 *     __u64 size;
+	 *     __u64 userspace_addr;
+	 *     __u32 perm;
+	 *    __u32 flags_padding;
+	 *    __u64 __subtree_last;
+	 * };
+	 */
 	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
 
 	if (!node)
 		return -ENOMEM;
 
+	/*
+	 * 在以下使用max_iotlb_entries:
+	 *   - drivers/vhost/vhost.c|958| <<vhost_new_umem_range>> if (umem->numem == max_iotlb_entries) {
+	 *
+	 * 如果数目已经最大了, 就要删除一个
+	 */
 	if (umem->numem == max_iotlb_entries) {
 		tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+		/*
+		 * 把vhost_umem_node从umem->umem_list删除
+		 * 再从umem->umem_tree删除
+		 * 减少umem->numem
+		 */
 		vhost_umem_free(umem, tmp);
 	}
 
@@ -934,13 +1680,42 @@ static int vhost_new_umem_range(struct vhost_umem *umem,
 	node->userspace_addr = userspace_addr;
 	node->perm = perm;
 	INIT_LIST_HEAD(&node->link);
+	/*
+	 * 在以下使用vhost_umem->umem_list:
+	 *   - drivers/vhost/vhost.c|1006| <<vhost_dev_reset_owner>> INIT_LIST_HEAD(&umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1045| <<vhost_umem_clean>> list_for_each_entry_safe(node, tmp, &umem->umem_list, link)
+	 *   - drivers/vhost/vhost.c|1154| <<vq_memory_access_ok>> list_for_each_entry(node, &umem->umem_list, link) {
+	 *   - drivers/vhost/vhost.c|1422| <<vhost_new_umem_range>> tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+	 *   - drivers/vhost/vhost.c|1432| <<vhost_new_umem_range>> list_add_tail(&node->link, &umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1808| <<vhost_umem_alloc>> INIT_LIST_HEAD(&umem->umem_list);
+	 */
 	list_add_tail(&node->link, &umem->umem_list);
+	/*
+	 * 在以下使用vhost_umem->umem_tree:
+	 *    - drivers/vhost/vhost.c|1032| <<vhost_umem_free>> vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1433| <<vhost_new_umem_range>> vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1444| <<vhost_del_umem_range>> while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1711| <<iotlb_access_ok>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1806| <<vhost_umem_alloc>> umem->umem_tree = RB_ROOT_CACHED;
+	 *    - drivers/vhost/vhost.c|2433| <<translate_desc>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *
+	 * 只在这里调用vhost_umem_interval_tree_insert()
+	 */
 	vhost_umem_interval_tree_insert(node, &umem->umem_tree);
 	umem->numem++;
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1680| <<vhost_process_iotlb_msg>> vhost_del_umem_range(dev->iotlb, msg->iova,
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_del_umem_range()
+ */
 static void vhost_del_umem_range(struct vhost_umem *umem,
 				 u64 start, u64 end)
 {
@@ -951,6 +1726,15 @@ static void vhost_del_umem_range(struct vhost_umem *umem,
 		vhost_umem_free(umem, node);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1634| <<vhost_process_iotlb_msg>> vhost_iotlb_notify_vq(dev, msg);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_iotlb_notify_vq()
+ */
 static void vhost_iotlb_notify_vq(struct vhost_dev *d,
 				  struct vhost_iotlb_msg *msg)
 {
@@ -972,6 +1756,15 @@ static void vhost_iotlb_notify_vq(struct vhost_dev *d,
 	spin_unlock(&d->iotlb_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1665| <<vhost_process_iotlb_msg>> if (umem_access_ok(msg->uaddr, msg->size, msg->perm)) {
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> umem_access_ok()
+ */
 static int umem_access_ok(u64 uaddr, u64 size, int access)
 {
 	unsigned long a = uaddr;
@@ -989,6 +1782,14 @@ static int umem_access_ok(u64 uaddr, u64 size, int access)
 	return 0;
 }
 
+/*
+ * called by VHOST_IOTLB_MSG:
+ *   - drivers/vhost/vhost.c|1544| <<vhost_chr_write_iter(VHOST_IOTLB_MSG)>> err = vhost_process_iotlb_msg(dev, &node.msg.iotlb);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ */
 static int vhost_process_iotlb_msg(struct vhost_dev *dev,
 				   struct vhost_iotlb_msg *msg)
 {
@@ -1030,6 +1831,14 @@ static int vhost_process_iotlb_msg(struct vhost_dev *dev,
 
 	return ret;
 }
+
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1636| <<vhost_net_chr_write_iter>> return vhost_chr_write_iter(dev, from);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ */
 ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 			     struct iov_iter *from)
 {
@@ -1060,6 +1869,10 @@ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 }
 EXPORT_SYMBOL(vhost_chr_write_iter);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1664| <<vhost_net_chr_poll>> return vhost_chr_poll(file, dev, wait);
+ */
 unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 			    poll_table *wait)
 {
@@ -1067,6 +1880,14 @@ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 
 	poll_wait(file, &dev->wait, wait);
 
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	if (!list_empty(&dev->read_list))
 		mask |= POLLIN | POLLRDNORM;
 
@@ -1074,6 +1895,13 @@ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 }
 EXPORT_SYMBOL(vhost_chr_poll);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1626| <<vhost_net_chr_read_iter>> return vhost_chr_read_iter(dev, to, noblock);
+ *
+ * vhost_net_fops.read_iter = vhost_net_chr_read_iter()
+ *    -> vhost_chr_read_iter()
+ */
 ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 			    int noblock)
 {
@@ -1090,6 +1918,14 @@ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 			prepare_to_wait(&dev->wait, &wait,
 					TASK_INTERRUPTIBLE);
 
+		/*
+		 * 在以下使用vhost_dev->read_list:
+		 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+		 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+		 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+		 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+		 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+		 */
 		node = vhost_dequeue_msg(dev, &dev->read_list);
 		if (node)
 			break;
@@ -1127,6 +1963,36 @@ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 }
 EXPORT_SYMBOL_GPL(vhost_chr_read_iter);
 
+/*
+ * This patch tries to implement an device IOTLB for vhost. This could be
+ * used with userspace(qemu) implementation of DMA remapping
+ * to emulate an IOMMU for the guest.
+ *
+ * The idea is simple, cache the translation in a software device IOTLB
+ * (which is implemented as an interval tree) in vhost and use vhost_net
+ * file descriptor for reporting IOTLB miss and IOTLB
+ * update/invalidation. When vhost meets an IOTLB miss, the fault
+ * address, size and access can be read from the file. After userspace
+ * finishes the translation, it writes the translated address to the
+ * vhost_net file to update the device IOTLB.
+ *
+ * When device IOTLB is enabled by setting VIRTIO_F_IOMMU_PLATFORM all vq
+ * addresses set by ioctl are treated as iova instead of virtual address and
+ * the accessing can only be done through IOTLB instead of direct userspace
+ * memory access. Before each round or vq processing, all vq metadata is
+ * prefetched in device IOTLB to make sure no translation fault happens
+ * during vq processing.
+ *
+ * In most cases, virtqueues are contiguous even in virtual address space.
+ * The IOTLB translation for virtqueue itself may make it a little
+ * slower. We might add fast path cache on top of this patch.
+ */
+
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1715| <<iotlb_access_ok>> vhost_iotlb_miss(vq, addr, access);
+ *   - drivers/vhost/vhost.c|2460| <<translate_desc>> vhost_iotlb_miss(vq, addr, access);
+ */
 static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
 {
 	struct vhost_dev *dev = vq->dev;
@@ -1142,11 +2008,24 @@ static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
 	msg->iova = iova;
 	msg->perm = access;
 
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	vhost_enqueue_msg(dev, &dev->read_list, node);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2025| <<vhost_vq_access_ok>> return vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);
+ *   - drivers/vhost/vhost.c|2239| <<vhost_vring_ioctl>> if (!vq_access_ok(vq, vq->num,
+ */
 static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 			struct vring_desc __user *desc,
 			struct vring_avail __user *avail,
@@ -1162,6 +2041,15 @@ static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 			sizeof *used + num * sizeof *used->ring + s);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1723| <<iotlb_access_ok>> vhost_vq_meta_update(vq, node, type);
+ *
+ * handle_tx() or handle_rx()
+ * -> vq_iotlb_prefetch()
+ *    -> iotlb_access_ok()
+ *       -> vhost_vq_meta_update()
+ */
 static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 				 const struct vhost_umem_node *node,
 				 int type)
@@ -1169,10 +2057,26 @@ static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 	int access = (type == VHOST_ADDR_USED) ?
 		     VHOST_ACCESS_WO : VHOST_ACCESS_RO;
 
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[]:
+	 *   - drivers/vhost/vhost.c|713| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1176| <<vhost_vq_meta_fetch>> const struct vhost_umem_node *node = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1693| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = node;
+	 */
 	if (likely(node->perm & access))
 		vq->meta_iotlb[type] = node;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1881| <<vq_iotlb_prefetch>> return iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->desc,
+ *   - drivers/vhost/vhost.c|1883| <<vq_iotlb_prefetch>> iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->avail,
+ *   - drivers/vhost/vhost.c|1887| <<vq_iotlb_prefetch>> iotlb_access_ok(vq, VHOST_ACCESS_WO, (u64)(uintptr_t)vq->used,
+ *
+ * handle_tx() or handle_rx()
+ * -> vq_iotlb_prefetch()
+ *    -> iotlb_access_ok()
+ */
 static int iotlb_access_ok(struct vhost_virtqueue *vq,
 			   int access, u64 addr, u64 len, int type)
 {
@@ -1209,6 +2113,11 @@ static int iotlb_access_ok(struct vhost_virtqueue *vq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|470| <<handle_tx>> if (!vq_iotlb_prefetch(vq))
+ *   - drivers/vhost/net.c|763| <<handle_rx>> if (!vq_iotlb_prefetch(vq))
+ */
 int vq_iotlb_prefetch(struct vhost_virtqueue *vq)
 {
 	size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
@@ -1232,6 +2141,13 @@ EXPORT_SYMBOL_GPL(vq_iotlb_prefetch);
 
 /* Can we log writes? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1530| <<vhost_net_set_features>> !vhost_log_access_ok(&n->dev))
+ *   - drivers/vhost/scsi.c|1712| <<vhost_scsi_set_features>> !vhost_log_access_ok(&vs->dev)) {
+ *   - drivers/vhost/test.c|248| <<vhost_test_set_features>> !vhost_log_access_ok(&n->dev)) {
+ *   - drivers/vhost/vsock.c|653| <<vhost_vsock_set_features>> !vhost_log_access_ok(&vsock->dev)) {
+ */
 int vhost_log_access_ok(struct vhost_dev *dev)
 {
 	return memory_access_ok(dev, dev->umem, 1);
@@ -1240,6 +2156,11 @@ EXPORT_SYMBOL_GPL(vhost_log_access_ok);
 
 /* Verify access for write logging. */
 /* Caller should have vq mutex and device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2018| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+ *   - drivers/vhost/vhost.c|2425| <<vhost_dev_ioctl>> if (vq->private_data && !vq_log_access_ok(vq, base))
+ */
 static int vq_log_access_ok(struct vhost_virtqueue *vq,
 			    void __user *log_base)
 {
@@ -1254,6 +2175,14 @@ static int vq_log_access_ok(struct vhost_virtqueue *vq,
 
 /* Can we start vq? */
 /* Caller should have vq mutex and device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1404| <<vhost_net_set_backend>> if (!vhost_vq_access_ok(vq)) {
+ *   - drivers/vhost/scsi.c|1528| <<vhost_scsi_set_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+ *   - drivers/vhost/scsi.c|1630| <<vhost_scsi_clear_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+ *   - drivers/vhost/test.c|184| <<vhost_test_run>> if (!vhost_vq_access_ok(&n->vqs[index])) {
+ *   - drivers/vhost/vsock.c|443| <<vhost_vsock_start>> if (!vhost_vq_access_ok(vq)) {
+ */
 int vhost_vq_access_ok(struct vhost_virtqueue *vq)
 {
 	if (!vq_log_access_ok(vq, vq->log_base))
@@ -1267,6 +2196,11 @@ int vhost_vq_access_ok(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2094| <<vhost_set_memory>> newumem = vhost_umem_alloc();
+ *   - drivers/vhost/vhost.c|2364| <<vhost_init_device_iotlb>> niotlb = vhost_umem_alloc();
+ */
 static struct vhost_umem *vhost_umem_alloc(void)
 {
 	struct vhost_umem *umem = kvzalloc(sizeof(*umem), GFP_KERNEL);
@@ -1281,9 +2215,28 @@ static struct vhost_umem *vhost_umem_alloc(void)
 	return umem;
 }
 
+/*
+ * 设置VHOST_SET_MEM_TABLE:
+ *   - drivers/vhost/vhost.c|2104| <<vhost_dev_ioctl(VHOST_SET_MEM_TABLE)>> r = vhost_set_memory(d, argp);
+ */
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
+	/*
+	 * struct vhost_memory {
+	 *     __u32 nregions;
+	 *     __u32 padding;
+	 *     struct vhost_memory_region regions[0];
+	 * };
+	 */
 	struct vhost_memory mem, *newmem;
+	/*
+	 * struct vhost_memory_region {
+	 *     __u64 guest_phys_addr;
+	 *     __u64 memory_size; // bytes
+	 *     __u64 userspace_addr;
+	 *     __u64 flags_padding; // No flags are currently specified.
+	 * };
+	 */
 	struct vhost_memory_region *region;
 	struct vhost_umem *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
@@ -1306,6 +2259,13 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 		return -EFAULT;
 	}
 
+	/*
+	 * struct vhost_umem {
+	 *     struct rb_root_cached umem_tree;
+	 *     struct list_head umem_list;
+	 *     int numem;
+	 * };
+	 */
 	newumem = vhost_umem_alloc();
 	if (!newumem) {
 		kvfree(newmem);
@@ -1333,6 +2293,10 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 
 	/* All memory accesses are done under some VQ mutex. */
 	for (i = 0; i < d->nvqs; ++i) {
+		/*
+		 * struct vhost_dev *d:
+		 *   -> struct vhost_virtqueue **vqs;
+		 */
 		mutex_lock(&d->vqs[i]->mutex);
 		d->vqs[i]->umem = newumem;
 		mutex_unlock(&d->vqs[i]->mutex);
@@ -1567,6 +2531,10 @@ long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)
 }
 EXPORT_SYMBOL_GPL(vhost_vring_ioctl);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1534| <<vhost_net_set_features>> if (vhost_init_device_iotlb(&n->dev, true))
+ */
 int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)
 {
 	struct vhost_umem *niotlb, *oiotlb;
@@ -1680,6 +2648,10 @@ EXPORT_SYMBOL_GPL(vhost_dev_ioctl);
  * (instruction directly accesses the data, with an exception table entry
  * returning -EFAULT). See Documentation/x86/exception-tables.txt.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2515| <<log_write>> r = set_bit_to_user(bit, (void __user *)(unsigned long )log);
+ */
 static int set_bit_to_user(int nr, void __user *addr)
 {
 	unsigned long log = (unsigned long)addr;
@@ -1700,6 +2672,14 @@ static int set_bit_to_user(int nr, void __user *addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2356| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+ *   - drivers/vhost/vhost.c|2389| <<vhost_update_used_flags>> log_write(vq->log_base, vq->log_addr +
+ *   - drivers/vhost/vhost.c|2413| <<vhost_update_avail_event>> log_write(vq->log_base, vq->log_addr +
+ *   - drivers/vhost/vhost.c|2960| <<__vhost_add_used_n>> log_write(vq->log_base,
+ *   - drivers/vhost/vhost.c|3037| <<vhost_add_used_n>> log_write(vq->log_base,
+ */
 static int log_write(void __user *log_base,
 		     u64 write_address, u64 write_length)
 {
@@ -1726,6 +2706,14 @@ static int log_write(void __user *log_base,
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1114| <<handle_rx>> vhost_log_write(vq, vq_log, log, vhost_len);
+ *
+ *  handle_rx_kick() or handle_rx_net()
+ *  -> handle_rx()
+ *     -> vhost_log_write()
+ */
 int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 		    unsigned int log_num, u64 len)
 {
@@ -1751,6 +2739,12 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 }
 EXPORT_SYMBOL_GPL(vhost_log_write);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2290| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|2981| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|3028| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+ */
 static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 {
 	void __user *used;
@@ -1771,6 +2765,10 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2988| <<vhost_enable_notify>> r = vhost_update_avail_event(vq, vq->avail_idx);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 {
 	if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
@@ -1791,6 +2789,13 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1424| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|199| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/vsock.c|450| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -1826,11 +2831,36 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * [0] translate_desc
+ * [0] vhost_get_vq_desc
+ * [0] vhost_scsi_get_desc
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|2404| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
+ *   - drivers/vhost/vhost.c|2450| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+ *   - drivers/vhost/vhost.c|2592| <<vhost_get_vq_desc>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+ *
+ * 把desc中的地址转换成qemu userspace的地址存放在iov中
+ *
+ * iov_size是从iov开始剩下可用的iov
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
 	const struct vhost_umem_node *node;
 	struct vhost_dev *dev = vq->dev;
+	/*
+	 * 如果vhost_dev->iotlb没设置, 返回dev->umem
+	 */
 	struct vhost_umem *umem = dev->iotlb ? dev->iotlb : dev->umem;
 	struct iovec *_iov;
 	u64 s = 0;
@@ -1838,14 +2868,29 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 
 	while ((u64)len > s) {
 		u64 size;
+		/*
+		 * ret在while()循环之前一开始的时候是0
+		 */
 		if (unlikely(ret >= iov_size)) {
 			ret = -ENOBUFS;
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_umem->umem_tree:
+		 *    - drivers/vhost/vhost.c|1032| <<vhost_umem_free>> vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+		 *    - drivers/vhost/vhost.c|1433| <<vhost_new_umem_range>> vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+		 *    - drivers/vhost/vhost.c|1444| <<vhost_del_umem_range>> while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+		 *    - drivers/vhost/vhost.c|1711| <<iotlb_access_ok>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+		 *    - drivers/vhost/vhost.c|1806| <<vhost_umem_alloc>> umem->umem_tree = RB_ROOT_CACHED;
+		 *    - drivers/vhost/vhost.c|2433| <<translate_desc>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+		 */
 		node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
 							addr, addr + len - 1);
 		if (node == NULL || node->start > addr) {
+			/*
+			 * 上面如果用的dev->umem, 这里umem就不会等于dev->iotlb
+			 */
 			if (umem != dev->iotlb) {
 				ret = -EFAULT;
 				break;
@@ -1857,9 +2902,36 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			break;
 		}
 
+		/*
+		 * struct vhost_umem_node {
+		 *     struct rb_node rb;
+		 *     struct list_head link;
+		 *     __u64 start;
+		 *     __u64 last;
+		 *     __u64 size;
+		 *     __u64 userspace_addr;
+		 *     __u32 perm;
+		 *     __u32 flags_padding;
+		 *     __u64 __subtree_last;
+		 * };
+		 */
 		_iov = iov + ret;
 		size = node->size - addr + node->start;
 		_iov->iov_len = min((u64)len - s, size);
+		/*
+		 * array_index_nospec - sanitize an array index after a bounds check
+		 *
+		 * For a code sequence like:
+		 *
+		 *     if (index < size) {
+		 *         index = array_index_nospec(index, size);
+		 *         val = array[index];
+		 *     }
+		 *
+		 * ...if the CPU speculates past the bounds check then
+		 * array_index_nospec() will clamp the index within the range of [0,
+		 * size).
+		 */
 		_iov->iov_base = (void __user *)
 			((unsigned long)node->userspace_addr +
 			 array_index_nospec((unsigned long)(addr - node->start),
@@ -1895,6 +2967,10 @@ static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2686| <<vhost_get_vq_desc>> ret = get_indirect(vq, iov, iov_size,
+ */
 static int get_indirect(struct vhost_virtqueue *vq,
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
@@ -2001,6 +3077,17 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|578| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|588| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|909| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg,
+ *   - drivers/vhost/scsi.c|472| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/scsi.c|851| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/test.c|56| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/vsock.c|112| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/vsock.c|374| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2014,14 +3101,42 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	int ret, access;
 
 	/* Check it isn't doing very strange things with descriptor numbers. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|310| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1413| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1419| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2022| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2143| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2155| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 */
 	last_avail_idx = vq->last_avail_idx;
 
+	/*
+	 * 在以下修改vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|311| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2030| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2330| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 */
 	if (vq->avail_idx == vq->last_avail_idx) {
+		/*
+		 * 从vq->avail->idx获取最新的avail_idx
+		 */
 		if (unlikely(vhost_get_avail(vq, avail_idx, &vq->avail->idx))) {
 			vq_err(vq, "Failed to access avail idx at %p\n",
 				&vq->avail->idx);
 			return -EFAULT;
 		}
+		/*
+		 * !!!! 更新了vq->avail_idx!!!
+		 */
 		vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
 
 		if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
@@ -2044,6 +3159,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	/* Grab the next descriptor number they're advertising, and increment
 	 * the index we've seen. */
+	/*
+	 * __virtio16 ring_head;
+	 */
 	if (unlikely(vhost_get_avail(vq, ring_head,
 		     &vq->avail->ring[last_avail_idx & (vq->num - 1)]))) {
 		vq_err(vq, "Failed to read head: idx %d address %p\n",
@@ -2063,9 +3181,13 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	/* When we start there are none of either input nor output. */
 	*out_num = *in_num = 0;
+	/*
+	 * 如果是vhost_scsi_get_desc()调用进来的, log是NULL
+	 */
 	if (unlikely(log))
 		*log_num = 0;
 
+	/* head是上面从avail ring buffer根据index读出来的__virtio16 */
 	i = head;
 	do {
 		unsigned iov_count = *in_num + *out_num;
@@ -2080,6 +3202,14 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			       i, vq->num, head);
 			return -EINVAL;
 		}
+		/*
+		 * struct vhost_virtqueue:
+		 *   - struct vring_desc __user *desc; 
+		 *   - struct vring_avail __user *avail;
+		 *   - struct vring_used __user *used;
+		 *
+		 * struct vring_desc desc;
+		 */
 		ret = vhost_copy_from_user(vq, &desc, vq->desc + i,
 					   sizeof desc);
 		if (unlikely(ret)) {
@@ -2100,10 +3230,18 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			continue;
 		}
 
+		/*
+		 * int ret, access;
+		 */
 		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * 把desc中的地址转换成qemu userspace的地址存放在iov中
+		 *
+		 * iov_size是从iov开始剩下可用的iov
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2135,6 +3273,33 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	} while ((i = next_desc(vq, &desc)) != -1);
 
 	/* On success, increment avail index. */
+	/*
+	 * 在以下调用vhost_avail_event():
+	 *   - drivers/vhost/vhost.c|1799| <<vhost_update_avail_event>> vhost_avail_event(vq)))
+	 *   - drivers/vhost/vhost.c|1806| <<vhost_update_avail_event>> used = vhost_avail_event(vq);
+	 *   - drivers/vhost/vhost.c|1809| <<vhost_update_avail_event>> sizeof *vhost_avail_event(vq));
+	 *   - drivers/vhost/vhost.c|2426| <<vhost_enable_notify>> vhost_avail_event(vq), r);
+	 *
+	 * 在以下修改vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|311| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2030| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2330| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|310| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1413| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1419| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2022| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2143| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2155| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 */
 	vq->last_avail_idx++;
 
 	/* Assume notifications from guest are disabled at this point,
@@ -2145,6 +3310,15 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|725| <<handle_tx>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|963| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1084| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1108| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *
+ * 只有vhost-net在使用
+ */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
 	vq->last_avail_idx -= n;
@@ -2153,6 +3327,13 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|562| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2303| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|159| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + pkt->len);
+ *   - drivers/vhost/vsock.c|404| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, sizeof(pkt->hdr) + len);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2164,6 +3345,11 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2225| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2231| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2173,6 +3359,17 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	int start;
 
 	start = vq->last_used_idx & (vq->num - 1);
+	/*
+	 * struct vhost_virtqueue *vq:
+	 *  -> struct vring_used __user *used;
+	 *      -> __virtio16 flags;  
+	 *      -> __virtio16 idx;
+	 *      -> struct vring_used_elem ring[];
+	 *          -> // Index of start of used descriptor chain.
+	 *             __virtio32 id;
+	 *          -> // Total length of the descriptor chain which was used (written to)
+	 *             __virtio32 len;
+	 */
 	used = vq->used->ring + start;
 	if (count == 1) {
 		if (vhost_put_user(vq, heads[0].id, &used->id)) {
@@ -2197,11 +3394,28 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			  count * sizeof *used);
 	}
 	old = vq->last_used_idx;
+	/*
+	 * 这里更新了vq->last_used_idx
+	 */
 	new = (vq->last_used_idx += count);
 	/* If the driver never bothers to signal in a very long while,
 	 * used index might wrap around. If that happens, invalidate
 	 * signalled_used index we stored. TODO: make sure driver
 	 * signals at least once in 2^16 and remove this. */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|313| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2239| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2320| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2322| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 *
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1830| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2240| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2321| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
 		vq->signalled_used_valid = false;
 	return 0;
@@ -2209,11 +3423,28 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2168| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2313| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
 	int start, n, r;
 
+	/*
+	 * 在以下修改vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|312| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1825| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2205| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2180| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2204| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2235| <<vhost_add_used_n>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2276| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	start = vq->last_used_idx & (vq->num - 1);
 	n = vq->num - start;
 	if (n < count) {
@@ -2244,6 +3475,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2348| <<vhost_signal>> if (vq->call_ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2254,10 +3489,17 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	 * interrupts. */
 	smp_mb();
 
+	/*
+	 * VIRTIO_F_NOTIFY_ON_EMPTY有一些不支持
+	 */
 	if (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&
 	    unlikely(vq->avail_idx == vq->last_avail_idx))
 		return true;
 
+	/*
+	 * 大部分支持VIRTIO_RING_F_EVENT_IDX
+	 * VIRTIO_RING_F_EVENT_IDX = 29
+	 */
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
 		__virtio16 flags;
 		if (vhost_get_avail(vq, flags, &vq->avail->flags)) {
@@ -2267,21 +3509,54 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
 	old = vq->signalled_used;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1830| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2240| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2321| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|313| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2239| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2320| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2322| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
 	if (unlikely(!v))
 		return true;
 
+	/*
+	 * vhost_used_event():
+	 * 返回((__virtio16 __user *)&vq->avail->ring[vq->num])
+	 */
 	if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * vring_need_event():
+	 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|575| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2897| <<vhost_signal>> void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
+ *   - drivers/vhost/vhost.c|2911| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2926| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.h|333| <<vhost_add_used_and_signal_n>> void vhost_signal(struct vhost_dev *, struct vhost_virtqueue *);
+ *   - drivers/vhost/vsock.c|180| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|410| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
@@ -2291,6 +3566,15 @@ void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_signal);
 
 /* And here's the combo meal deal.  Supersize me! */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|732| <<handle_tx>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|557| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|901| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1268| <<vhost_scsi_send_tmf_reject>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/scsi.c|1288| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|82| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
@@ -2301,6 +3585,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
 /* multi-buffer version of vhost_add_used_and_signal */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|338| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|856| <<handle_rx>> vhost_add_used_and_signal_n(&net->dev, vq, vq->heads,
+ */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
@@ -2311,6 +3600,13 @@ void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal_n);
 
 /* return true if we're sure that avaiable ring is empty */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|585| <<vhost_net_tx_get_vq_desc>> vhost_vq_avail_empty(vq->dev, vq))
+ *   - drivers/vhost/net.c|710| <<handle_tx>> !vhost_vq_avail_empty(&net->dev, vq) &&
+ *   - drivers/vhost/net.c|845| <<vhost_net_rx_peek_head_len>> vhost_vq_avail_empty(&net->dev, vq))
+ *   - drivers/vhost/net.c|850| <<vhost_net_rx_peek_head_len>> if (!vhost_vq_avail_empty(&net->dev, vq))
+ */
 bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
@@ -2329,11 +3625,33 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
 /* OK, now we need to know about added descriptors. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|658| <<handle_tx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|852| <<vhost_net_rx_peek_head_len>> else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1041| <<handle_rx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|480| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|864| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|65| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|103| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|129| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|380| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
 	int r;
 
+	/*
+	 * 在以下使用VRING_USED_F_NO_NOTIFY:
+	 *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+	 *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+	 */
 	if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
 		return false;
 	vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
@@ -2367,13 +3685,45 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
 /* We don't need to be notified again. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|634| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|659| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|838| <<vhost_net_rx_peek_head_len>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|853| <<vhost_net_rx_peek_head_len>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1008| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1044| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|527| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|931| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1053| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1301| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|53| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|66| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|90| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|130| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|362| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|381| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
 
+	/*
+	 * 在以下使用VRING_USED_F_NO_NOTIFY:
+	 *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+	 *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+	 */
 	if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
 		return;
 	vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	/*
+	 * 一般都设置VIRTIO_RING_F_EVENT_IDX
+	 */
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
 		r = vhost_update_used_flags(vq);
 		if (r)
@@ -2384,6 +3734,10 @@ void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_disable_notify);
 
 /* Create a new message. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1711| <<vhost_iotlb_miss>> node = vhost_new_msg(vq, VHOST_IOTLB_MISS);
+ */
 struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)
 {
 	struct vhost_msg_node *node = kmalloc(sizeof *node, GFP_KERNEL);
@@ -2398,6 +3752,11 @@ struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)
 }
 EXPORT_SYMBOL_GPL(vhost_new_msg);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1618| <<vhost_chr_read_iter>> vhost_enqueue_msg(dev, &dev->pending_list, node);
+ *   - drivers/vhost/vhost.c|1665| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+ */
 void vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,
 		       struct vhost_msg_node *node)
 {
@@ -2409,6 +3768,14 @@ void vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,
 }
 EXPORT_SYMBOL_GPL(vhost_enqueue_msg);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1638| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+ *
+ * vhost_net_fops.read_iter = vhost_net_chr_read_iter()
+ * -> vhost_chr_read_iter()
+ *    -> vhost_dequeue_msg()
+ */
 struct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,
 					 struct list_head *head)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 79c6e7a60a5e..f893a7e9a0d3 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -30,8 +30,32 @@ struct vhost_work {
 /* Poll a file (eventfd or socket) */
 /* Note: there's nothing vhost specific about this structure. */
 struct vhost_poll {
+	/*
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	poll_table                table;
+	/*
+	 * 在以下使用vhost_poll->wqh:
+	 *   - drivers/vhost/vhost.c|261| <<vhost_poll_func>> poll->wqh = wqh;
+	 *   - drivers/vhost/vhost.c|274| <<vhost_poll_func>> add_wait_queue(wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|376| <<vhost_poll_init>> poll->wqh = NULL;
+	 *   - drivers/vhost/vhost.c|400| <<vhost_poll_start>> if (poll->wqh)
+	 *   - drivers/vhost/vhost.c|439| <<vhost_poll_stop>> if (poll->wqh) {
+	 *   - drivers/vhost/vhost.c|440| <<vhost_poll_stop>> remove_wait_queue(poll->wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|441| <<vhost_poll_stop>> poll->wqh = NULL;
+	 */
 	wait_queue_head_t        *wqh;
+	/*
+	 * struct wait_queue_entry {
+	 *     unsigned int            flags;
+	 *     void                    *private;
+	 *     wait_queue_func_t       func;
+	 *     struct list_head        entry;
+	 * };
+	 */
 	wait_queue_entry_t              wait;
 	struct vhost_work	  work;
 	unsigned long		  mask;
@@ -72,7 +96,25 @@ struct vhost_umem_node {
 };
 
 struct vhost_umem {
+	/*
+	 * 在以下使用vhost_umem->umem_tree:
+	 *    - drivers/vhost/vhost.c|1032| <<vhost_umem_free>> vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1433| <<vhost_new_umem_range>> vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1444| <<vhost_del_umem_range>> while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1711| <<iotlb_access_ok>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1806| <<vhost_umem_alloc>> umem->umem_tree = RB_ROOT_CACHED;
+	 *    - drivers/vhost/vhost.c|2433| <<translate_desc>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 */
 	struct rb_root_cached umem_tree;
+	/*
+	 * 在以下使用vhost_umem->umem_list:
+	 *   - drivers/vhost/vhost.c|1006| <<vhost_dev_reset_owner>> INIT_LIST_HEAD(&umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1045| <<vhost_umem_clean>> list_for_each_entry_safe(node, tmp, &umem->umem_list, link)
+	 *   - drivers/vhost/vhost.c|1154| <<vq_memory_access_ok>> list_for_each_entry(node, &umem->umem_list, link) {
+	 *   - drivers/vhost/vhost.c|1422| <<vhost_new_umem_range>> tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+	 *   - drivers/vhost/vhost.c|1432| <<vhost_new_umem_range>> list_add_tail(&node->link, &umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1808| <<vhost_umem_alloc>> INIT_LIST_HEAD(&umem->umem_list);
+	 */
 	struct list_head umem_list;
 	int numem;
 };
@@ -94,11 +136,37 @@ struct vhost_virtqueue {
 	struct vring_desc __user *desc;
 	struct vring_avail __user *avail;
 	struct vring_used __user *used;
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[]:
+	 *   - drivers/vhost/vhost.c|713| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1176| <<vhost_vq_meta_fetch>> const struct vhost_umem_node *node = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1693| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = node;
+	 */
 	const struct vhost_umem_node *meta_iotlb[VHOST_NUM_ADDRS];
 	struct file *kick;
 	struct file *call;
 	struct file *error;
+	/*
+	 * 在以下使用vhost_virtqueue->call_ctx:
+	 *   - drivers/vhost/vhost.c|335| <<vhost_vq_reset>> vq->call_ctx = NULL;
+	 *   - drivers/vhost/vhost.c|637| <<vhost_dev_cleanup>> if (dev->vqs[i]->call_ctx)
+	 *   - drivers/vhost/vhost.c|638| <<vhost_dev_cleanup>> eventfd_ctx_put(dev->vqs[i]->call_ctx);
+	 *   - drivers/vhost/vhost.c|1539| <<vhost_vring_ioctl>> ctx = vq->call_ctx;
+	 *   - drivers/vhost/vhost.c|1541| <<vhost_vring_ioctl(VHOST_SET_VRING_CALL)>> vq->call_ctx = eventfp ?
+	 *   - drivers/vhost/vhost.c|2395| <<vhost_signal>> if (vq->call_ctx && vhost_notify(dev, vq))
+	 *   - drivers/vhost/vhost.c|2396| <<vhost_signal>> eventfd_signal(vq->call_ctx, 1);
+	 */
 	struct eventfd_ctx *call_ctx;
+	/*
+	 * 在以下使用vhost_virtqueue->error_ctx:
+	 *   - drivers/vhost/vhost.c|332| <<vhost_vq_reset>> vq->error_ctx = NULL;
+	 *   - drivers/vhost/vhost.c|631| <<vhost_dev_cleanup>> if (dev->vqs[i]->error_ctx)
+	 *   - drivers/vhost/vhost.c|632| <<vhost_dev_cleanup>> eventfd_ctx_put(dev->vqs[i]->error_ctx);
+	 *   - drivers/vhost/vhost.c|1559| <<vhost_vring_ioctl>> ctx = vq->error_ctx;
+	 *   -  drivers/vhost/vhost.c|1560| <<vhost_vring_ioctl>> vq->error_ctx = eventfp ?
+	 *   - drivers/vhost/vhost.h|299| <<vq_err>> if ((vq)->error_ctx) \
+	 *   - drivers/vhost/vhost.h|300| <<vq_err>> eventfd_signal((vq)->error_ctx, 1);\
+	 */
 	struct eventfd_ctx *error_ctx;
 	struct eventfd_ctx *log_ctx;
 
@@ -108,21 +176,81 @@ struct vhost_virtqueue {
 	vhost_work_fn_t handle_kick;
 
 	/* Last available index we saw. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|310| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1413| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1419| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2022| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2143| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2155| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 */
 	u16 last_avail_idx;
 
 	/* Caches available index value from user. */
+	/*
+	 * 在以下修改vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|311| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2030| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2330| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 * 在以下使用vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|1781| <<vhost_update_avail_event>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2032| <<vhost_get_vq_desc>> if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
+	 *   - drivers/vhost/vhost.c|2034| <<vhost_get_vq_desc>> last_avail_idx, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2363| <<vhost_enable_notify>> r = vhost_update_avail_event(vq, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2380| <<vhost_enable_notify>> return vhost16_to_cpu(vq, avail_idx) != vq->avail_idx;
+	 */
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下修改vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|312| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1825| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2205| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2180| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2204| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2235| <<vhost_add_used_n>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2276| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|313| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2239| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2320| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2322| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1830| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2240| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2321| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
@@ -137,6 +265,15 @@ struct vhost_virtqueue {
 	struct vhost_umem *umem;
 	struct vhost_umem *iotlb;
 	void *private_data;
+	/*
+	 * 在以下使用vhost_virtqueue->acked_features:
+	 *   - drivers/vhost/net.c|1259| <<vhost_net_set_features>> n->vqs[i].vq.acked_features = features;
+	 *   - drivers/vhost/scsi.c|1621| <<vhost_scsi_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/test.c|254| <<vhost_test_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/vhost.c|319| <<vhost_vq_reset>> vq->acked_features = 0;
+	 *   - drivers/vhost/vhost.h|245| <<vhost_has_feature>> return vq->acked_features & (1ULL << bit);
+	 *   - drivers/vhost/vsock.c|661| <<vhost_vsock_set_features>> vq->acked_features = features;
+	 */
 	u64 acked_features;
 	/* Log write descriptors */
 	void __user *log_base;
@@ -165,12 +302,35 @@ struct vhost_dev {
 	int nvqs;
 	struct file *log_file;
 	struct eventfd_ctx *log_ctx;
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	struct llist_head work_list;
 	struct task_struct *worker;
 	struct vhost_umem *umem;
 	struct vhost_umem *iotlb;
 	spinlock_t iotlb_lock;
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	struct list_head read_list;
+	/*
+	 * 在以下使用vhost_dev->pending_list:
+	 *   - drivers/vhost/vhost.c|770| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->pending_list);
+	 *   - drivers/vhost/vhost.c|943| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->pending_list, node) {
+	 *   - drivers/vhost/vhost.c|1319| <<vhost_iotlb_notify_vq>> list_for_each_entry_safe(node, n, &d->pending_list, node) {
+	 *   - drivers/vhost/vhost.c|1481| <<vhost_chr_read_iter>> vhost_enqueue_msg(dev, &dev->pending_list, node);
+	 */
 	struct list_head pending_list;
 	wait_queue_head_t wait;
 };
diff --git a/drivers/virtio/virtio_pci_modern.c b/drivers/virtio/virtio_pci_modern.c
index 2555d80f6eec..efbf2b576e2e 100644
--- a/drivers/virtio/virtio_pci_modern.c
+++ b/drivers/virtio/virtio_pci_modern.c
@@ -292,6 +292,26 @@ static u16 vp_config_vector(struct virtio_pci_device *vp_dev, u16 vector)
 	return vp_ioread16(&vp_dev->common->msix_config);
 }
 
+/*
+ * [0] setup_vq
+ * [0] vp_setup_vq
+ * [0] vp_find_vqs_msix
+ * [0] vp_find_vqs
+ * [0] vp_modern_find_vqs
+ * [0] virtscsi_init
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static struct virtqueue *setup_vq(struct virtio_pci_device *vp_dev,
 				  struct virtio_pci_vq_info *info,
 				  unsigned index,
@@ -343,6 +363,7 @@ static struct virtqueue *setup_vq(struct virtio_pci_device *vp_dev,
 	vp_iowrite64_twopart(virtqueue_get_used_addr(vq),
 			     &cfg->queue_used_lo, &cfg->queue_used_hi);
 
+	/* 在uek5设置了 */
 	if (vp_dev->notify_base) {
 		/* offset should not wrap */
 		if ((u64)off * vp_dev->notify_offset_multiplier + 2
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 71458f493cf8..1480aa209d60 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -82,15 +82,49 @@ struct vring_virtqueue {
 	/* Head of free buffer list. */
 	unsigned int free_head;
 	/* Number we've added since last sync. */
+	/*
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|428| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|435| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|582| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|584| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1064| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 */
 	unsigned int num_added;
 
 	/* Last used index we've seen. */
+	/*
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|893| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1151| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下修改vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|834| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|877| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|900| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|967| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1040| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1044| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	u16 last_used_idx;
 
 	/* Last written value to avail->flags */
 	u16 avail_flags_shadow;
 
 	/* Last written value to avail->idx in guest byte order */
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|406| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|924| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|400| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|407| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|562| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|563| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|925| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|988| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 */
 	u16 avail_idx_shadow;
 
 	/* How to notify other side. FIXME: commonalize hcalls! */
@@ -106,6 +140,16 @@ struct vring_virtqueue {
 	unsigned int in_use;
 
 	/* Figure out if their kicks are too delayed. */
+	/*
+	 * 在以下修改vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|315| <<virtqueue_add>> vq->last_add_time_valid = true;
+	 *   - drivers/virtio/virtio_ring.c|591| <<virtqueue_kick_prepare>> vq->last_add_time_valid = false;
+	 *   - drivers/virtio/virtio_ring.c|813| <<virtqueue_get_buf_ctx>> vq->last_add_time_valid = false;
+	 *   - drivers/virtio/virtio_ring.c|1068| <<bool>> vq->last_add_time_valid = false;
+	 * 在以下使用vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|311| <<virtqueue_add>> if (vq->last_add_time_valid)
+	 *   - drivers/virtio/virtio_ring.c|587| <<virtqueue_kick_prepare>> if (vq->last_add_time_valid) {
+	 */
 	bool last_add_time_valid;
 	ktime_t last_add_time;
 #endif
@@ -257,6 +301,13 @@ static struct vring_desc *alloc_indirect(struct virtqueue *_vq,
 	return desc;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|480| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs,
+ *   - drivers/virtio/virtio_ring.c|503| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|525| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|549| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+ */
 static inline int virtqueue_add(struct virtqueue *_vq,
 				struct scatterlist *sgs[],
 				unsigned int total_sg,
@@ -288,6 +339,16 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 		ktime_t now = ktime_get();
 
 		/* No kick or get, with .1 second between?  Warn. */
+		/*
+		 * 在以下修改vring_virtqueue->last_add_time_valid:
+		 *   - drivers/virtio/virtio_ring.c|315| <<virtqueue_add>> vq->last_add_time_valid = true;
+		 *   - drivers/virtio/virtio_ring.c|591| <<virtqueue_kick_prepare>> vq->last_add_time_valid = false;
+		 *   - drivers/virtio/virtio_ring.c|813| <<virtqueue_get_buf_ctx>> vq->last_add_time_valid = false;
+		 *   - drivers/virtio/virtio_ring.c|1068| <<bool>> vq->last_add_time_valid = false;
+		 * 在以下使用vring_virtqueue->last_add_time_valid:
+		 *   - drivers/virtio/virtio_ring.c|311| <<virtqueue_add>> if (vq->last_add_time_valid)
+		 *   - drivers/virtio/virtio_ring.c|587| <<virtqueue_kick_prepare>> if (vq->last_add_time_valid) {
+		 */
 		if (vq->last_add_time_valid)
 			WARN_ON(ktime_to_ms(ktime_sub(now, vq->last_add_time))
 					    > 100);
@@ -403,7 +464,21 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 	/* Descriptors and available array need to be set before we expose the
 	 * new available array entries. */
 	virtio_wmb(vq->weak_barriers);
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|406| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|924| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|400| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|407| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|562| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|563| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|925| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|988| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 */
 	vq->avail_idx_shadow++;
+	/* !!! 更新shared ring buffer idx !!! */
 	vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
 	vq->num_added++;
 
@@ -559,8 +634,29 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 	 * event. */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|406| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|924| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|400| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|407| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|562| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|563| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|925| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|988| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 */
 	old = vq->avail_idx_shadow - vq->num_added;
 	new = vq->avail_idx_shadow;
+	/*
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|428| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|435| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|582| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|584| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1064| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 */
 	vq->num_added = 0;
 
 #ifdef DEBUG
@@ -575,6 +671,20 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 		needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),
 					      new, old);
 	} else {
+		/*
+		 * The Host uses this in used->flags to advise the Guest: don't kick me when
+		 * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
+		 * will still kick if it's out of buffers.
+		 *
+		 * 在以下使用VRING_USED_F_NO_NOTIFY:
+		 *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+		 *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+		 *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+		 *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+		 *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+		 *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+		 *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+		 */
 		needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
 	}
 	END_USE(vq);
@@ -590,6 +700,40 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * [0] virtqueue_notify
+ * [0] virtscsi_kick_cmd
+ * [0] virtscsi_queuecommand
+ * [0] scsi_dispatch_cmd
+ * [0] scsi_queue_rq
+ * [0] blk_mq_dispatch_rq_list
+ * [0] blk_mq_do_dispatch_sched
+ * [0] blk_mq_sched_dispatch_requests
+ * [0] __blk_mq_run_hw_queue
+ * [0] __blk_mq_delay_run_hw_queue
+ * [0] blk_mq_run_hw_queue
+ * [0] blk_mq_flush_plug_list
+ * [0] blk_flush_plug_list
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_direct_write
+ * [0] __generic_file_write_iter
+ * [0] blkdev_write_iter
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/block/virtio_blk.c|288| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|481| <<virtscsi_kick_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|624| <<virtqueue_kick>> return virtqueue_notify(vq);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -618,6 +762,43 @@ EXPORT_SYMBOL_GPL(virtqueue_notify);
  *
  * Returns false if kick failed, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|273| <<virtio_queue_rq>> virtqueue_kick(vblk->vqs[qid].vq);
+ *   - drivers/char/hw_random/virtio-rng.c|63| <<register_buffer>> virtqueue_kick(vi->vq);
+ *   - drivers/char/virtio_console.c|513| <<add_inbuf>> virtqueue_kick(vq);
+ *   - drivers/char/virtio_console.c|582| <<__send_control_msg>> virtqueue_kick(vq);
+ *   - drivers/char/virtio_console.c|636| <<__send_to_port>> virtqueue_kick(out_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|181| <<virtio_crypto_alg_ablkcipher_init_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|254| <<virtio_crypto_alg_ablkcipher_close_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|462| <<__virtio_crypto_ablkcipher_do_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|554| <<virtio_crypto_ablkcipher_crypt_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|298| <<virtio_gpu_queue_ctrl_buffer_locked>> virtqueue_kick(vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|373| <<virtio_gpu_queue_cursor>> virtqueue_kick(vq);
+ *   - drivers/net/caif/caif_virtio.c|589| <<cfv_netdev_tx>> virtqueue_kick(cfv->vq_tx);
+ *   - drivers/net/virtio_net.c|425| <<virtnet_xdp_flush>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1123| <<try_fill_recv>> virtqueue_kick(rq->vq);
+ *   - drivers/net/virtio_net.c|1447| <<start_xmit>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1483| <<virtnet_send_command>> if (unlikely(!virtqueue_kick(vi->cvq)))
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|654| <<rpmsg_send_offchannel_raw>> virtqueue_kick(vrp->svq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|802| <<rpmsg_recv_done>> virtqueue_kick(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|284| <<virtscsi_kick_event>> virtqueue_kick(vscsi->event_vq.vq);
+ *   - drivers/virtio/virtio_balloon.c|123| <<tell_host>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|326| <<stats_handle_request>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|461| <<init_vqs>> virtqueue_kick(vb->stats_vq);
+ *   - drivers/virtio/virtio_input.c|48| <<virtinput_recv_events>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_input.c|77| <<virtinput_send_status>> virtqueue_kick(vi->sts);
+ *   - drivers/virtio/virtio_input.c|196| <<virtinput_fill_evt>> virtqueue_kick(vi->evt);
+ *   - drivers/virtio/virtio_ring.c|429| <<virtqueue_add>> virtqueue_kick(_vq);
+ *   - net/9p/trans_virtio.c|305| <<p9_virtio_request>> virtqueue_kick(chan->vq);
+ *   - net/9p/trans_virtio.c|498| <<p9_virtio_zc_request>> virtqueue_kick(chan->vq);
+ *   - net/vmw_vsock/virtio_transport.c|184| <<virtio_transport_send_pkt_work>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|296| <<virtio_vsock_rx_fill>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|411| <<virtio_vsock_event_fill>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - net/vmw_vsock/virtio_transport.c|468| <<virtio_transport_event_work>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - tools/virtio/virtio_test.c|176| <<run_test>> if (unlikely(!virtqueue_kick(vq->vq)))
+ *   - tools/virtio/vringh_test.c|402| <<bool>> virtqueue_kick(vq);
+ */
 bool virtqueue_kick(struct virtqueue *vq)
 {
 	if (virtqueue_kick_prepare(vq))
@@ -626,6 +807,11 @@ bool virtqueue_kick(struct virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_kick);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|892| <<virtqueue_get_buf_ctx>> detach_buf(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1088| <<virtqueue_detach_unused_buf>> detach_buf(vq, i, NULL);
+ */
 static void detach_buf(struct vring_virtqueue *vq, unsigned int head,
 		       void **ctx)
 {
@@ -736,6 +922,19 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 	/* detach_buf clears data, so grab it now. */
 	ret = vq->desc_state[i].data;
 	detach_buf(vq, i, ctx);
+	/*
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|893| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1151| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下修改vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|834| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|877| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|900| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|967| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1040| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1044| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
 	 * by writing event index and flush out the write before
@@ -907,6 +1106,17 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb_delayed);
  * This is not valid on an active queue; it is useful only for device
  * shutdown.
  */
+/*
+ * called by:
+ *   - drivers/char/virtio_console.c|1983| <<remove_vqs>> while ((buf = virtqueue_detach_unused_buf(vq)))
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|410| <<virtcrypto_free_unused_reqs>> while ((vc_req = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/net/caif/caif_virtio.c|469| <<cfv_netdev_close>> while ((buf_info = virtqueue_detach_unused_buf(cfv->vq_tx)))
+ *   - drivers/net/virtio_net.c|2434| <<free_unused_bufs>> while ((buf = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/net/virtio_net.c|2445| <<free_unused_bufs>> while ((buf = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/virtio/virtio_input.c|325| <<virtinput_remove>> while ((buf = virtqueue_detach_unused_buf(vi->sts)) != NULL)
+ *   - net/vmw_vsock/virtio_transport.c|643| <<virtio_vsock_remove>> while ((pkt = virtqueue_detach_unused_buf(vsock->vqs[VSOCK_VQ_RX])))
+ *   - net/vmw_vsock/virtio_transport.c|648| <<virtio_vsock_remove>> while ((pkt = virtqueue_detach_unused_buf(vsock->vqs[VSOCK_VQ_TX])))
+ */
 void *virtqueue_detach_unused_buf(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
diff --git a/fs/eventfd.c b/fs/eventfd.c
index 2fb4eadaa118..c3de745ad66e 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -51,6 +51,33 @@ struct eventfd_ctx {
  * Returns the amount by which the counter was incremented.  This will be less
  * than @n if the counter has overflowed.
  */
+/*
+ * [0] eventfd_signal
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] eventfd_signal
+ * [0] vhost_add_used_and_signal_n
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 一个caller是ioeventfd_write()
+ */
 __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 {
 	unsigned long flags;
@@ -59,6 +86,30 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 	if (ULLONG_MAX - ctx->count < n)
 		n = ULLONG_MAX - ctx->count;
 	ctx->count += n;
+	/*
+	 * struct eventfd_ctx *ctx:
+	 *   -> wait_queue_head_t wqh;
+	 *
+	 * 对于vhost rx, 会唤醒wait_queue_entry->func = irqfd_wakeup()
+	 * struct wait_queue_entry {
+	 *     unsigned int            flags;
+	 *     void                    *private;
+	 *     wait_queue_func_t       func;
+	 *     struct list_head        entry;
+	 * };
+	 *
+	 * 对于vm的ioeventfd下来的...
+	 * crash> wait_queue_entry ffff930ab1008df8
+	 * struct wait_queue_entry {
+	 *   flags = 0,
+	 *   private = 0x0,
+	 *   func = 0xffffffffc0661d10 <vhost_poll_wakeup>,
+	 *   entry = {
+	 *     next = 0xffff9332faca9010,
+	 *     prev = 0xffff9332faca9010
+	 *   }
+	 * }
+	 */
 	if (waitqueue_active(&ctx->wqh))
 		wake_up_locked_poll(&ctx->wqh, POLLIN);
 	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
@@ -67,6 +118,11 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 }
 EXPORT_SYMBOL_GPL(eventfd_signal);
 
+/*
+ * called by:
+ *   - fs/eventfd.c|104| <<eventfd_free>> eventfd_free_ctx(ctx);
+ *   - fs/eventfd.c|503| <<eventfd_file_create>> eventfd_free_ctx(ctx);
+ */
 static void eventfd_free_ctx(struct eventfd_ctx *ctx)
 {
 	kfree(ctx);
@@ -191,6 +247,17 @@ static void eventfd_ctx_do_read(struct eventfd_ctx *ctx, __u64 *cnt)
  * This is used to atomically remove a wait queue entry from the eventfd wait
  * queue head, and read/reset the counter value.
  */
+/*
+ * [0] eventfd_ctx_remove_wait_queue
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/vfio/virqfd.c|94| <<virqfd_shutdown>> eventfd_ctx_remove_wait_queue(virqfd->eventfd, &virqfd->wait, &cnt);
+ *   - virt/kvm/eventfd.c|132| <<irqfd_shutdown>> eventfd_ctx_remove_wait_queue(irqfd->eventfd, &irqfd->wait, &cnt);
+ */
 int eventfd_ctx_remove_wait_queue(struct eventfd_ctx *ctx, wait_queue_entry_t *wait,
 				  __u64 *cnt)
 {
@@ -403,6 +470,16 @@ EXPORT_SYMBOL_GPL(eventfd_ctx_fdget);
  *
  * -EINVAL   : The @fd file descriptor is not an eventfd file.
  */
+/*
+ * called by:
+ *   - drivers/vfio/virqfd.c|138| <<vfio_virqfd_enable>> ctx = eventfd_ctx_fileget(irqfd.file);
+ *   - drivers/vhost/vhost.c|1990| <<vhost_vring_ioctl>> eventfd_ctx_fileget(eventfp) : NULL;
+ *   - drivers/vhost/vhost.c|2009| <<vhost_vring_ioctl>> eventfd_ctx_fileget(eventfp) : NULL;
+ *   - drivers/vhost/vhost.c|2142| <<vhost_dev_ioctl>> eventfd_ctx_fileget(eventfp) : NULL;
+ *   - fs/eventfd.c|416| <<eventfd_ctx_fdget>> ctx = eventfd_ctx_fileget(f.file);
+ *   - mm/memcontrol.c|3801| <<memcg_write_event_control>> event->eventfd = eventfd_ctx_fileget(efile.file);
+ *   - virt/kvm/eventfd.c|317| <<kvm_irqfd_assign>> eventfd = eventfd_ctx_fileget(f.file);
+ */
 struct eventfd_ctx *eventfd_ctx_fileget(struct file *file)
 {
 	if (file->f_op != &eventfd_fops)
@@ -426,6 +503,10 @@ EXPORT_SYMBOL_GPL(eventfd_ctx_fileget);
  * can be avoided.
  * Returns an eventfd file pointer, or a proper error pointer.
  */
+/*
+ * called by:
+ *   - fs/eventfd.c|493| <<SYSCALL_DEFINE2(eventfd2)>> file = eventfd_file_create(count, flags);
+ */
 struct file *eventfd_file_create(unsigned int count, int flags)
 {
 	struct file *file;
diff --git a/include/kvm/iodev.h b/include/kvm/iodev.h
index a6d208b916f5..9112cdf94caa 100644
--- a/include/kvm/iodev.h
+++ b/include/kvm/iodev.h
@@ -45,6 +45,24 @@ struct kvm_io_device {
 	const struct kvm_io_device_ops *ops;
 };
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/mpic.c|1448| <<map_mmio>> kvm_iodevice_init(&opp->mmio, &mpic_mmio_ops);
+ *   - arch/x86/kvm/i8254.c|691| <<kvm_create_pit>> kvm_iodevice_init(&pit->dev, &pit_dev_ops);
+ *   - arch/x86/kvm/i8254.c|698| <<kvm_create_pit>> kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);
+ *   - arch/x86/kvm/i8259.c|603| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
+ *   - arch/x86/kvm/i8259.c|604| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
+ *   - arch/x86/kvm/i8259.c|605| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_eclr, &picdev_eclr_ops);
+ *   - arch/x86/kvm/ioapic.c|635| <<kvm_ioapic_init>> kvm_iodevice_init(&ioapic->dev, &ioapic_mmio_ops);
+ *   - arch/x86/kvm/lapic.c|2276| <<kvm_create_lapic>> kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
+ *   - virt/kvm/arm/vgic/vgic-its.c|1667| <<vgic_register_its_iodev>> kvm_iodevice_init(&iodev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v2.c|485| <<vgic_v2_init_dist_iodev>> kvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|597| <<vgic_v3_init_dist_iodev>> kvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|643| <<vgic_register_redist_iodev>> kvm_iodevice_init(&rd_dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|658| <<vgic_register_redist_iodev>> kvm_iodevice_init(&sgi_dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/coalesced_mmio.c|150| <<kvm_vm_ioctl_register_coalesced_mmio>> kvm_iodevice_init(&dev->dev, &coalesced_mmio_ops);
+ *   - virt/kvm/eventfd.c|917| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static inline void kvm_iodevice_init(struct kvm_io_device *dev,
 				     const struct kvm_io_device_ops *ops)
 {
@@ -59,6 +77,20 @@ static inline int kvm_iodevice_read(struct kvm_vcpu *vcpu,
 				: -EOPNOTSUPP;
 }
 
+/*
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static inline int kvm_iodevice_write(struct kvm_vcpu *vcpu,
 				     struct kvm_io_device *dev, gpa_t addr,
 				     int l, const void *v)
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 3705b8a87795..d309dc9fbbef 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -225,6 +225,19 @@ enum {
 	IRQD_MOVE_PCNTXT		= (1 << 15),
 	IRQD_IRQ_DISABLED		= (1 << 16),
 	IRQD_IRQ_MASKED			= (1 << 17),
+	/*
+	 * 在以下使用IRQD_IRQ_INPROGRESS:
+	 *   - kernel/irq/debugfs.c|103| <<global>> BIT_MASK_DESCR(IRQD_IRQ_INPROGRESS),
+	 *   - include/linux/irq.h|327| <<irqd_irq_inprogress>> return __irqd_to_state(d) & IRQD_IRQ_INPROGRESS;
+	 *   - kernel/irq/chip.c|473| <<handle_nested_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|484| <<handle_nested_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|500| <<irq_may_run>> unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
+	 *   - kernel/irq/chip.c|586| <<handle_untracked_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|592| <<handle_untracked_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|201| <<handle_irq_event>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|207| <<handle_irq_event>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/manage.c|1380| <<__setup_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 */
 	IRQD_IRQ_INPROGRESS		= (1 << 18),
 	IRQD_WAKEUP_ARMED		= (1 << 19),
 	IRQD_FORWARDED_TO_VCPU		= (1 << 20),
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 40b4f6598a1b..7e92f2f52789 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -447,6 +447,13 @@ struct kvm {
 		struct list_head  resampler_list;
 		struct mutex      resampler_lock;
 	} irqfds;
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|555| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|834| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|902| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|933| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	struct list_head ioeventfds;
 #endif
 	struct kvm_vm_stat stat;
@@ -511,6 +518,10 @@ struct kvm {
 
 static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
 {
+	/*
+	 * struct kvm:
+	 *   -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	return srcu_dereference_check(kvm->buses[idx], &kvm->srcu,
 				      lockdep_is_held(&kvm->slots_lock) ||
 				      !refcount_read(&kvm->users_count));
diff --git a/include/linux/ptr_ring.h b/include/linux/ptr_ring.h
index dc396196585a..db0e68ea1f59 100644
--- a/include/linux/ptr_ring.h
+++ b/include/linux/ptr_ring.h
@@ -32,9 +32,50 @@
 #endif
 
 struct ptr_ring {
+	/*
+	 * 在以下修改ptr_ring->producer:
+	 *   - include/linux/ptr_ring.h|116| <<__ptr_ring_produce>> r->queue[r->producer++] = ptr;
+	 *   - include/linux/ptr_ring.h|118| <<__ptr_ring_produce>> r->producer = 0;
+	 *   - include/linux/ptr_ring.h|549| <<ptr_ring_init>> r->producer = r->consumer_head = r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|637| <<__ptr_ring_swap_queue>> r->producer = producer;
+	 * 在以下使用ptr_ring->producer:
+	 *   - include/linux/ptr_ring.h|54| <<__ptr_ring_full>> return r->queue[r->producer];
+	 *   - include/linux/ptr_ring.h|109| <<__ptr_ring_produce>> if (unlikely(!r->size) || r->queue[r->producer])
+	 *   - include/linux/ptr_ring.h|117| <<__ptr_ring_produce>> if (unlikely(r->producer >= r->size))
+	 */
 	int producer ____cacheline_aligned_in_smp;
 	spinlock_t producer_lock;
+	/*
+	 * 在以下修改ptr_ring->consumer_head:
+	 *   - include/linux/ptr_ring.h|256| <<__ptr_ring_discard_one>> int head = r->consumer_head++;
+	 *   - include/linux/ptr_ring.h|275| <<__ptr_ring_discard_one>> r->consumer_head = 0;
+	 *   - include/linux/ptr_ring.h|638| <<__ptr_ring_swap_queue>> r->consumer_head = 0;
+	 * 在以下使用ptr_ring->consumer_head:
+	 *   - include/linux/ptr_ring.h|185| <<__ptr_ring_peek>> return r->queue[r->consumer_head];
+	 *   - include/linux/ptr_ring.h|263| <<__ptr_ring_discard_one>> if (unlikely(r->consumer_head - r->consumer_tail >= r->batch ||
+	 *   - include/linux/ptr_ring.h|264| <<__ptr_ring_discard_one>> r->consumer_head >= r->size)) {
+	 *   - include/linux/ptr_ring.h|272| <<__ptr_ring_discard_one>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|274| <<__ptr_ring_discard_one>> if (unlikely(r->consumer_head >= r->size)) {
+	 *   - include/linux/ptr_ring.h|549| <<ptr_ring_init>> r->producer = r->consumer_head = r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|586| <<ptr_ring_unconsume>> head = r->consumer_head - 1;
+	 *   - include/linux/ptr_ring.h|589| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|596| <<ptr_ring_unconsume>> head = r->consumer_head - 1;
+	 *   - include/linux/ptr_ring.h|604| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head = head;
+	 */
 	int consumer_head ____cacheline_aligned_in_smp; /* next valid entry */
+	/*
+	 * 在以下修改ptr->consumer_tail:
+	 *   - include/linux/ptr_ring.h|272| <<__ptr_ring_discard_one>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|276| <<__ptr_ring_discard_one>> r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|549| <<ptr_ring_init>> r->producer = r->consumer_head = r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|589| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|604| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head = head;
+	 *   - include/linux/ptr_ring.h|639| <<__ptr_ring_swap_queue>> r->consumer_tail = 0;
+	 * 在以下使用ptr->consumer_tail:
+	 *   - include/linux/ptr_ring.h|263| <<__ptr_ring_discard_one>> if (unlikely(r->consumer_head - r->consumer_tail >= r->batch ||
+	 *   - include/linux/ptr_ring.h|270| <<__ptr_ring_discard_one>> while (likely(head >= r->consumer_tail))
+	 *   - include/linux/ptr_ring.h|587| <<ptr_ring_unconsume>> while (likely(head >= r->consumer_tail))
+	 */
 	int consumer_tail; /* next entry to invalidate */
 	spinlock_t consumer_lock;
 	/* Shared consumer/producer data */
@@ -49,11 +90,29 @@ struct ptr_ring {
  * producer_lock - see e.g. ptr_ring_full.  Otherwise, if callers don't hold
  * producer_lock, the next call to __ptr_ring_produce may fail.
  */
+/*
+ * calld by:
+ *   - include/linux/ptr_ring.h|62| <<ptr_ring_full>> ret = __ptr_ring_full(r);
+ *   - include/linux/ptr_ring.h|73| <<ptr_ring_full_irq>> ret = __ptr_ring_full(r);
+ *   - include/linux/ptr_ring.h|85| <<ptr_ring_full_any>> ret = __ptr_ring_full(r);
+ *   - include/linux/ptr_ring.h|96| <<ptr_ring_full_bh>> ret = __ptr_ring_full(r);
+ *   - include/linux/skb_array.h|38| <<__skb_array_full>> return __ptr_ring_full(&a->ring);
+ *   - tools/virtio/ringtest/ptr_ring.c|152| <<get_buf>> if (tailcnt == headcnt || __ptr_ring_full(&array))
+ *   - tools/virtio/ringtest/ptr_ring.c|164| <<used_empty>> return (tailcnt == headcnt || __ptr_ring_full(&array));
+ *
+ * 返回ptr_ring->queue[ptr_ring->producer]
+ */
 static inline bool __ptr_ring_full(struct ptr_ring *r)
 {
 	return r->queue[r->producer];
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|43| <<skb_array_full>> return ptr_ring_full(&a->ring);
+ *
+ * 返回ptr_ring->queue[ptr_ring->producer]
+ */
 static inline bool ptr_ring_full(struct ptr_ring *r)
 {
 	bool ret;
@@ -65,6 +124,9 @@ static inline bool ptr_ring_full(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 static inline bool ptr_ring_full_irq(struct ptr_ring *r)
 {
 	bool ret;
@@ -76,6 +138,9 @@ static inline bool ptr_ring_full_irq(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 static inline bool ptr_ring_full_any(struct ptr_ring *r)
 {
 	unsigned long flags;
@@ -88,6 +153,9 @@ static inline bool ptr_ring_full_any(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 static inline bool ptr_ring_full_bh(struct ptr_ring *r)
 {
 	bool ret;
@@ -104,8 +172,19 @@ static inline bool ptr_ring_full_bh(struct ptr_ring *r)
  * Callers are responsible for making sure pointer that is being queued
  * points to a valid data.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|132| <<ptr_ring_produce>> ret = __ptr_ring_produce(r, ptr);
+ *   - include/linux/ptr_ring.h|143| <<ptr_ring_produce_irq>> ret = __ptr_ring_produce(r, ptr);
+ *   - include/linux/ptr_ring.h|155| <<ptr_ring_produce_any>> ret = __ptr_ring_produce(r, ptr);
+ *   - include/linux/ptr_ring.h|166| <<ptr_ring_produce_bh>> ret = __ptr_ring_produce(r, ptr);
+ *   - tools/virtio/ringtest/ptr_ring.c|133| <<add_inbuf>> ret = __ptr_ring_produce(&array, buf);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int __ptr_ring_produce(struct ptr_ring *r, void *ptr)
 {
+	/* 确认是不是满了或者没初始化 */
 	if (unlikely(!r->size) || r->queue[r->producer])
 		return -ENOSPC;
 
@@ -124,6 +203,12 @@ static inline int __ptr_ring_produce(struct ptr_ring *r, void *ptr)
  * consume in interrupt or BH context, you must disable interrupts/BH when
  * calling this.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|48| <<skb_array_produce>> return ptr_ring_produce(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce(struct ptr_ring *r, void *ptr)
 {
 	int ret;
@@ -135,6 +220,12 @@ static inline int ptr_ring_produce(struct ptr_ring *r, void *ptr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|53| <<skb_array_produce_irq>> return ptr_ring_produce_irq(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce_irq(struct ptr_ring *r, void *ptr)
 {
 	int ret;
@@ -146,6 +237,12 @@ static inline int ptr_ring_produce_irq(struct ptr_ring *r, void *ptr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|63| <<skb_array_produce_any>> return ptr_ring_produce_any(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce_any(struct ptr_ring *r, void *ptr)
 {
 	unsigned long flags;
@@ -158,6 +255,12 @@ static inline int ptr_ring_produce_any(struct ptr_ring *r, void *ptr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|58| <<skb_array_produce_bh>> return ptr_ring_produce_bh(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce_bh(struct ptr_ring *r, void *ptr)
 {
 	int ret;
@@ -175,8 +278,21 @@ static inline int ptr_ring_produce_bh(struct ptr_ring *r, void *ptr)
  * If ring is never resized, and if the pointer is merely
  * tested, there's no need to take the lock - see e.g.  __ptr_ring_empty.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|195| <<__ptr_ring_empty>> return !__ptr_ring_peek(r);
+ *   - include/linux/ptr_ring.h|284| <<__ptr_ring_consume>> ptr = __ptr_ring_peek(r);
+ *   - include/linux/ptr_ring.h|448| <<__PTR_RING_PEEK_CALL>> #define __PTR_RING_PEEK_CALL(r, f) ((f)(__ptr_ring_peek(r)))
+ *   - include/linux/skb_array.h|72| <<__skb_array_empty>> return !__ptr_ring_peek(&a->ring);
+ *
+ * 核心思想是返回ptr_ring->queue[ptr_ring->consumer_head];
+ */
 static inline void *__ptr_ring_peek(struct ptr_ring *r)
 {
+	/*
+	 * struct ptr_ring *r:
+	 *  -> void **queue;
+	 */
 	if (likely(r->size))
 		return r->queue[r->consumer_head];
 	return NULL;
@@ -186,11 +302,27 @@ static inline void *__ptr_ring_peek(struct ptr_ring *r)
  * for example cpu_relax(). Callers must take consumer_lock
  * if the ring is ever resized - see e.g. ptr_ring_empty.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|203| <<ptr_ring_empty>> ret = __ptr_ring_empty(r);
+ *   - include/linux/ptr_ring.h|214| <<ptr_ring_empty_irq>> ret = __ptr_ring_empty(r);
+ *   - include/linux/ptr_ring.h|226| <<ptr_ring_empty_any>> ret = __ptr_ring_empty(r);
+ *   - include/linux/ptr_ring.h|237| <<ptr_ring_empty_bh>> ret = __ptr_ring_empty(r);
+ *   - tools/virtio/ringtest/ptr_ring.c|195| <<avail_empty>> return __ptr_ring_empty(&array);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool __ptr_ring_empty(struct ptr_ring *r)
 {
 	return !__ptr_ring_peek(r);
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|77| <<skb_array_empty>> return ptr_ring_empty(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty(struct ptr_ring *r)
 {
 	bool ret;
@@ -202,6 +334,12 @@ static inline bool ptr_ring_empty(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|87| <<skb_array_empty_irq>> return ptr_ring_empty_irq(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty_irq(struct ptr_ring *r)
 {
 	bool ret;
@@ -213,6 +351,12 @@ static inline bool ptr_ring_empty_irq(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|92| <<skb_array_empty_any>> return ptr_ring_empty_any(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty_any(struct ptr_ring *r)
 {
 	unsigned long flags;
@@ -225,6 +369,12 @@ static inline bool ptr_ring_empty_any(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|82| <<skb_array_empty_bh>> return ptr_ring_empty_bh(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty_bh(struct ptr_ring *r)
 {
 	bool ret;
@@ -237,6 +387,10 @@ static inline bool ptr_ring_empty_bh(struct ptr_ring *r)
 }
 
 /* Must only be called after __ptr_ring_peek returned !NULL */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|286| <<__ptr_ring_consume>> __ptr_ring_discard_one(r);
+ */
 static inline void __ptr_ring_discard_one(struct ptr_ring *r)
 {
 	/* Fundamentally, what we want to do is update consumer
@@ -273,10 +427,23 @@ static inline void __ptr_ring_discard_one(struct ptr_ring *r)
 	}
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|301| <<__ptr_ring_consume_batched>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|320| <<ptr_ring_consume>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|335| <<ptr_ring_consume_irq>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|351| <<ptr_ring_consume_any>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|366| <<ptr_ring_consume_bh>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|628| <<__ptr_ring_swap_queue>> while ((ptr = __ptr_ring_consume(r)))
+ *   - tools/virtio/ringtest/ptr_ring.c|202| <<use_buf>> ptr = __ptr_ring_consume(&array);
+ */
 static inline void *__ptr_ring_consume(struct ptr_ring *r)
 {
 	void *ptr;
 
+	/*
+	 * 核心思想是返回ptr_ring->queue[ptr_ring->consumer_head];
+	 */
 	ptr = __ptr_ring_peek(r);
 	if (ptr)
 		__ptr_ring_discard_one(r);
@@ -287,6 +454,15 @@ static inline void *__ptr_ring_consume(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|382| <<ptr_ring_consume_batched>> ret = __ptr_ring_consume_batched(r, array, n);
+ *   - include/linux/ptr_ring.h|398| <<ptr_ring_consume_batched_irq>> ret = __ptr_ring_consume_batched(r, array, n);
+ *   - include/linux/ptr_ring.h|415| <<ptr_ring_consume_batched_any>> ret = __ptr_ring_consume_batched(r, array, n);
+ *   - include/linux/ptr_ring.h|431| <<ptr_ring_consume_batched_bh>> ret = __ptr_ring_consume_batched(r, array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int __ptr_ring_consume_batched(struct ptr_ring *r,
 					     void **array, int n)
 {
@@ -308,6 +484,11 @@ static inline int __ptr_ring_consume_batched(struct ptr_ring *r,
  * call this in interrupt or BH context, you must disable interrupts/BH when
  * producing.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|743| <<ptr_ring_cleanup>> while ((ptr = ptr_ring_consume(r)))
+ *   - include/linux/skb_array.h|97| <<skb_array_consume>> return ptr_ring_consume(&a->ring);
+ */
 static inline void *ptr_ring_consume(struct ptr_ring *r)
 {
 	void *ptr;
@@ -319,6 +500,10 @@ static inline void *ptr_ring_consume(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|108| <<skb_array_consume_irq>> return ptr_ring_consume_irq(&a->ring);
+ */
 static inline void *ptr_ring_consume_irq(struct ptr_ring *r)
 {
 	void *ptr;
@@ -330,6 +515,10 @@ static inline void *ptr_ring_consume_irq(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|119| <<skb_array_consume_any>> return ptr_ring_consume_any(&a->ring);
+ */
 static inline void *ptr_ring_consume_any(struct ptr_ring *r)
 {
 	unsigned long flags;
@@ -342,6 +531,10 @@ static inline void *ptr_ring_consume_any(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|131| <<skb_array_consume_bh>> return ptr_ring_consume_bh(&a->ring);
+ */
 static inline void *ptr_ring_consume_bh(struct ptr_ring *r)
 {
 	void *ptr;
@@ -353,6 +546,12 @@ static inline void *ptr_ring_consume_bh(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|103| <<skb_array_consume_batched>> return ptr_ring_consume_batched(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched(struct ptr_ring *r,
 					   void **array, int n)
 {
@@ -365,6 +564,12 @@ static inline int ptr_ring_consume_batched(struct ptr_ring *r,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|114| <<skb_array_consume_batched_irq>> return ptr_ring_consume_batched_irq(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched_irq(struct ptr_ring *r,
 					       void **array, int n)
 {
@@ -377,6 +582,12 @@ static inline int ptr_ring_consume_batched_irq(struct ptr_ring *r,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|125| <<skb_array_consume_batched_any>> return ptr_ring_consume_batched_any(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched_any(struct ptr_ring *r,
 					       void **array, int n)
 {
@@ -390,6 +601,12 @@ static inline int ptr_ring_consume_batched_any(struct ptr_ring *r,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|137| <<skb_array_consume_batched_bh>> return ptr_ring_consume_batched_bh(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 					      void **array, int n)
 {
@@ -406,8 +623,21 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
  * Function must return a value.
  * Callers must take consumer_lock.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|419| <<PTR_RING_PEEK_CALL>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *   - include/linux/ptr_ring.h|428| <<PTR_RING_PEEK_CALL_IRQ>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *   - include/linux/ptr_ring.h|437| <<PTR_RING_PEEK_CALL_BH>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *   - include/linux/ptr_ring.h|447| <<PTR_RING_PEEK_CALL_ANY>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *
+ * 核心思想是返回ptr_ring->queue[ptr_ring->consumer_head]作为参数被f调用
+ */
 #define __PTR_RING_PEEK_CALL(r, f) ((f)(__ptr_ring_peek(r)))
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|156| <<skb_array_peek_len>> return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	\
@@ -417,6 +647,10 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 	__PTR_RING_PEEK_CALL_v; \
 })
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|161| <<skb_array_peek_len_irq>> return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL_IRQ(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	\
@@ -426,6 +660,10 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 	__PTR_RING_PEEK_CALL_v; \
 })
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|166| <<skb_array_peek_len_bh>> return PTR_RING_PEEK_CALL_BH(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL_BH(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	\
@@ -435,6 +673,10 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 	__PTR_RING_PEEK_CALL_v; \
 })
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|171| <<skb_array_peek_len_any>> return PTR_RING_PEEK_CALL_ANY(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL_ANY(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	unsigned long __PTR_RING_PEEK_CALL_f;\
@@ -448,6 +690,17 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 /* Not all gfp_t flags (besides GFP_KERNEL) are allowed. See
  * documentation for vmalloc for which of them are legal.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|482| <<ptr_ring_init>> r->queue = __ptr_ring_init_queue_alloc(size, gfp);
+ *   - include/linux/ptr_ring.h|585| <<ptr_ring_resize>> void **queue = __ptr_ring_init_queue_alloc(size, gfp);
+ *   - include/linux/ptr_ring.h|624| <<ptr_ring_resize_multiple>> queues[i] = __ptr_ring_init_queue_alloc(size, gfp);
+ *
+ * 一个例子:
+ * skb_array_init()
+ *  -> ptr_ring_init()
+ *      -> __ptr_ring_init_queue_alloc()
+ */
 static inline void **__ptr_ring_init_queue_alloc(unsigned int size, gfp_t gfp)
 {
 	if (size > KMALLOC_MAX_SIZE / sizeof(void *))
@@ -455,6 +708,11 @@ static inline void **__ptr_ring_init_queue_alloc(unsigned int size, gfp_t gfp)
 	return kvmalloc_array(size, sizeof(void *), gfp | __GFP_ZERO);
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|486| <<ptr_ring_init>> __ptr_ring_set_size(r, size);
+ *   - include/linux/ptr_ring.h|565| <<__ptr_ring_swap_queue>> __ptr_ring_set_size(r, size);
+ */
 static inline void __ptr_ring_set_size(struct ptr_ring *r, int size)
 {
 	r->size = size;
@@ -468,6 +726,11 @@ static inline void __ptr_ring_set_size(struct ptr_ring *r, int size)
 		r->batch = 1;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|176| <<skb_array_init>> return ptr_ring_init(&a->ring, size, gfp);
+ *   - tools/virtio/ringtest/ptr_ring.c|121| <<alloc_ring>> int ret = ptr_ring_init(&array, ring_size, 0);
+ */
 static inline int ptr_ring_init(struct ptr_ring *r, int size, gfp_t gfp)
 {
 	r->queue = __ptr_ring_init_queue_alloc(size, gfp);
@@ -492,6 +755,12 @@ static inline int ptr_ring_init(struct ptr_ring *r, int size, gfp_t gfp)
  * In particular if you consume ring in interrupt or BH context, you must
  * disable interrupts/BH when doing so.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|192| <<skb_array_unconsume>> ptr_ring_unconsume(&a->ring, (void **)skbs, n, __skb_array_destroy_skb);
+ *
+ * 把数据归还给ptr_ring->queue[]
+ */
 static inline void ptr_ring_unconsume(struct ptr_ring *r, void **batch, int n,
 				      void (*destroy)(void *))
 {
@@ -537,6 +806,11 @@ static inline void ptr_ring_unconsume(struct ptr_ring *r, void **batch, int n,
 	spin_unlock_irqrestore(&r->consumer_lock, flags);
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|594| <<ptr_ring_resize>> old = __ptr_ring_swap_queue(r, queue, size, gfp, destroy);
+ *   - include/linux/ptr_ring.h|632| <<ptr_ring_resize_multiple>> queues[i] = __ptr_ring_swap_queue(rings[i], queues[i],
+ */
 static inline void **__ptr_ring_swap_queue(struct ptr_ring *r, void **queue,
 					   int size, gfp_t gfp,
 					   void (*destroy)(void *))
@@ -569,6 +843,10 @@ static inline void **__ptr_ring_swap_queue(struct ptr_ring *r, void **queue,
  * In particular if you consume ring in interrupt or BH context, you must
  * disable interrupts/BH when doing so.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|197| <<skb_array_resize>> return ptr_ring_resize(&a->ring, size, gfp, __skb_array_destroy_skb);
+ */
 static inline int ptr_ring_resize(struct ptr_ring *r, int size, gfp_t gfp,
 				  void (*destroy)(void *))
 {
@@ -598,6 +876,10 @@ static inline int ptr_ring_resize(struct ptr_ring *r, int size, gfp_t gfp,
  * In particular if you consume ring in interrupt or BH context, you must
  * disable interrupts/BH when doing so.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|205| <<skb_array_resize_multiple>> return ptr_ring_resize_multiple((struct ptr_ring **)rings,
+ */
 static inline int ptr_ring_resize_multiple(struct ptr_ring **rings,
 					   unsigned int nrings,
 					   int size,
@@ -643,6 +925,10 @@ static inline int ptr_ring_resize_multiple(struct ptr_ring **rings,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|212| <<skb_array_cleanup>> ptr_ring_cleanup(&a->ring, __skb_array_destroy_skb);
+ */
 static inline void ptr_ring_cleanup(struct ptr_ring *r, void (*destroy)(void *))
 {
 	void *ptr;
diff --git a/include/linux/skb_array.h b/include/linux/skb_array.h
index 8621ffdeecbf..bc52152b3513 100644
--- a/include/linux/skb_array.h
+++ b/include/linux/skb_array.h
@@ -33,31 +33,62 @@ struct skb_array {
 /* Might be slightly faster than skb_array_full below, but callers invoking
  * this in a loop must use a compiler barrier, for example cpu_relax().
  */
+/*
+ * called by:
+ *   - drivers/net/tap.c|333| <<tap_handle_frame>> if (__skb_array_full(&q->skb_array))
+ *
+ * 返回skb_array->ring->queue[skb_array->ring->producer]
+ */
 static inline bool __skb_array_full(struct skb_array *a)
 {
+	/* 返回ptr_ring->queue[ptr_ring->producer] */
 	return __ptr_ring_full(&a->ring);
 }
 
+/*
+ * 返回skb_array->ring->queue[skb_array->ring->producer]
+ *
+ * 没人调用
+ */
 static inline bool skb_array_full(struct skb_array *a)
 {
 	return ptr_ring_full(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|351| <<tap_handle_frame>> if (skb_array_produce(&q->skb_array, skb))
+ *   - drivers/net/tap.c|361| <<tap_handle_frame>> if (skb_array_produce(&q->skb_array, segs)) {
+ *   - drivers/net/tap.c|378| <<tap_handle_frame>> if (skb_array_produce(&q->skb_array, skb))
+ *   - drivers/net/tun.c|916| <<tun_net_xmit>> if (skb_array_produce(&tfile->tx_array, skb))
+ *
+ * 核心思想是skb_array->ring->queue[skb_array->ring->producer++] = skb
+ */
 static inline int skb_array_produce(struct skb_array *a, struct sk_buff *skb)
 {
+	/* 核心思想是skb_array->ring->queue[skb_array->ring->producer++] = skb; */
 	return ptr_ring_produce(&a->ring, skb);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_produce_irq(struct skb_array *a, struct sk_buff *skb)
 {
 	return ptr_ring_produce_irq(&a->ring, skb);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_produce_bh(struct skb_array *a, struct sk_buff *skb)
 {
 	return ptr_ring_produce_bh(&a->ring, skb);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_produce_any(struct skb_array *a, struct sk_buff *skb)
 {
 	return ptr_ring_produce_any(&a->ring, skb);
@@ -67,81 +98,147 @@ static inline int skb_array_produce_any(struct skb_array *a, struct sk_buff *skb
  * array is never resized. Also, callers invoking this in a loop must take care
  * to use a compiler barrier, for example cpu_relax().
  */
+/*
+ * 没人调用
+ */
 static inline bool __skb_array_empty(struct skb_array *a)
 {
 	return !__ptr_ring_peek(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|586| <<tap_poll>> if (!skb_array_empty(&q->skb_array))
+ *   - drivers/net/tun.c|1164| <<tun_chr_poll>> if (!skb_array_empty(&tfile->tx_array))
+ *
+ * 核心思想是判断skb_array->ring->queue[skb_array->ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool skb_array_empty(struct skb_array *a)
 {
+	/* 核心思想是判断skb_array->ring->queue[skb_array->ring->consumer_head]是否有值, 没有则为empty */
 	return ptr_ring_empty(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline bool skb_array_empty_bh(struct skb_array *a)
 {
 	return ptr_ring_empty_bh(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline bool skb_array_empty_irq(struct skb_array *a)
 {
 	return ptr_ring_empty_irq(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline bool skb_array_empty_any(struct skb_array *a)
 {
 	return ptr_ring_empty_any(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|850| <<tap_do_read>> skb = skb_array_consume(&q->skb_array);
+ *   - drivers/net/tun.c|530| <<tun_queue_purge>> while ((skb = skb_array_consume(&tfile->tx_array)) != NULL)
+ *   - drivers/net/tun.c|1704| <<tun_ring_recv>> skb = skb_array_consume(&tfile->tx_array);
+ *   - drivers/net/tun.c|1716| <<tun_ring_recv>> skb = skb_array_consume(&tfile->tx_array);
+ *
+ * 这里就consume一个, 返回的是被consume的地址
+ */
 static inline struct sk_buff *skb_array_consume(struct skb_array *a)
 {
 	return ptr_ring_consume(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|269| <<vhost_net_buf_produce>> rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
+ *
+ * 返回值是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int skb_array_consume_batched(struct skb_array *a,
 					    struct sk_buff **array, int n)
 {
+	/*
+	 * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+	 */
 	return ptr_ring_consume_batched(&a->ring, (void **)array, n);
 }
 
+/*
+ * 没人调用
+ */
 static inline struct sk_buff *skb_array_consume_irq(struct skb_array *a)
 {
 	return ptr_ring_consume_irq(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_consume_batched_irq(struct skb_array *a,
 						struct sk_buff **array, int n)
 {
 	return ptr_ring_consume_batched_irq(&a->ring, (void **)array, n);
 }
 
+/*
+ * 没人调用
+ */
 static inline struct sk_buff *skb_array_consume_any(struct skb_array *a)
 {
 	return ptr_ring_consume_any(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_consume_batched_any(struct skb_array *a,
 						struct sk_buff **array, int n)
 {
 	return ptr_ring_consume_batched_any(&a->ring, (void **)array, n);
 }
 
-
+/*
+ * 没人调用
+ */
 static inline struct sk_buff *skb_array_consume_bh(struct skb_array *a)
 {
 	return ptr_ring_consume_bh(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_consume_batched_bh(struct skb_array *a,
 					       struct sk_buff **array, int n)
 {
 	return ptr_ring_consume_batched_bh(&a->ring, (void **)array, n);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|320| <<vhost_net_buf_peek>> return __skb_array_len_with_tag(vhost_net_buf_get_ptr(rxq));
+ *   - include/linux/skb_array.h|168| <<skb_array_peek_len>> return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
+ *   - include/linux/skb_array.h|173| <<skb_array_peek_len_irq>> return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
+ *   - include/linux/skb_array.h|178| <<skb_array_peek_len_bh>> return PTR_RING_PEEK_CALL_BH(&a->ring, __skb_array_len_with_tag);
+ *   - include/linux/skb_array.h|183| <<skb_array_peek_len_any>> return PTR_RING_PEEK_CALL_ANY(&a->ring, __skb_array_len_with_tag);
+ */
 static inline int __skb_array_len_with_tag(struct sk_buff *skb)
 {
 	if (likely(skb)) {
 		int len = skb->len;
 
+		/*
+		 * 返回((__skb)->vlan_tci & VLAN_TAG_PRESENT)
+		 */
 		if (skb_vlan_tag_present(skb))
 			len += VLAN_HLEN;
 
@@ -151,47 +248,96 @@ static inline int __skb_array_len_with_tag(struct sk_buff *skb)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|1182| <<tap_peek_len>> return skb_array_peek_len(&q->skb_array);
+ *   - drivers/net/tun.c|1912| <<tun_peek_len>> ret = skb_array_peek_len(&tfile->tx_array);
+ *
+ * 核心思想是返回skb_array->ring->queue[skb_array->ring->consumer_head]作为参数被__skb_array_len_with_tag()调用
+ */
 static inline int skb_array_peek_len(struct skb_array *a)
 {
+	/*
+	 * 核心思想是返回skb_array->ring->queue[skb_array->ring->consumer_head]作为参数被__skb_array_len_with_tag()调用
+	 */
 	return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_peek_len_irq(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_peek_len_bh(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_BH(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_peek_len_any(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_ANY(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|520| <<tap_open>> if (skb_array_init(&q->skb_array, tap->dev->tx_queue_len, GFP_KERNEL)) {
+ *   - drivers/net/tun.c|2617| <<tun_chr_open>> if (skb_array_init(&tfile->tx_array, 0, GFP_KERNEL)) {
+ */
 static inline int skb_array_init(struct skb_array *a, int size, gfp_t gfp)
 {
 	return ptr_ring_init(&a->ring, size, gfp);
 }
 
+/*
+ * 在以下使用__skb_array_destroy_skb():
+ *   - include/linux/skb_array.h|192| <<skb_array_unconsume>> ptr_ring_unconsume(&a->ring, (void **)skbs, n, __skb_array_destroy_skb);
+ *   - include/linux/skb_array.h|197| <<skb_array_resize>> return ptr_ring_resize(&a->ring, size, gfp, __skb_array_destroy_skb);
+ *   - include/linux/skb_array.h|207| <<skb_array_resize_multiple>> __skb_array_destroy_skb);
+ *   - include/linux/skb_array.h|212| <<skb_array_cleanup>> ptr_ring_cleanup(&a->ring, __skb_array_destroy_skb);
+ */
 static void __skb_array_destroy_skb(void *ptr)
 {
 	kfree_skb(ptr);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|293| <<vhost_net_buf_unproduce>> skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
+ *
+ * 把数据归还给skb_array->ring->queue[]
+ */
 static inline void skb_array_unconsume(struct skb_array *a,
 				       struct sk_buff **skbs, int n)
 {
+	/*
+	 * 把数据归还给ptr_ring->queue[]
+	 */
 	ptr_ring_unconsume(&a->ring, (void **)skbs, n, __skb_array_destroy_skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tun.c|668| <<tun_attach>> skb_array_resize(&tfile->tx_array, dev->tx_queue_len, GFP_KERNEL)) {
+ */
 static inline int skb_array_resize(struct skb_array *a, int size, gfp_t gfp)
 {
 	return ptr_ring_resize(&a->ring, size, gfp, __skb_array_destroy_skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|1236| <<tap_queue_resize>> ret = skb_array_resize_multiple(arrays, n,
+ *   - drivers/net/tun.c|2803| <<tun_queue_resize>> ret = skb_array_resize_multiple(arrays, n,
+ */
 static inline int skb_array_resize_multiple(struct skb_array **rings,
 					    int nrings, unsigned int size,
 					    gfp_t gfp)
@@ -202,6 +348,11 @@ static inline int skb_array_resize_multiple(struct skb_array **rings,
 					__skb_array_destroy_skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|500| <<tap_sock_destruct>> skb_array_cleanup(&q->skb_array);
+ *   - drivers/net/tun.c|578| <<__tun_detach>> skb_array_cleanup(&tfile->tx_array);
+ */
 static inline void skb_array_cleanup(struct skb_array *a)
 {
 	ptr_ring_cleanup(&a->ring, __skb_array_destroy_skb);
diff --git a/include/scsi/scsi_cmnd.h b/include/scsi/scsi_cmnd.h
index cb9bf0fee1d4..1db0b7d2d61a 100644
--- a/include/scsi/scsi_cmnd.h
+++ b/include/scsi/scsi_cmnd.h
@@ -91,6 +91,19 @@ struct scsi_cmnd {
 	 * allocated.  It is used to time how long the command has
 	 * been outstanding
 	 */
+	/*
+	 * 在以下使用scsi_cmnd->jiffies_at_alloc:
+	 *   - drivers/scsi/libiscsi.c|2074| <<iscsi_eh_cmd_timed_out>> if (time_after(running_task->sc->jiffies_at_alloc,
+	 *   - drivers/scsi/libiscsi.c|2075| <<iscsi_eh_cmd_timed_out>> task->sc->jiffies_at_alloc))
+	 *   - drivers/scsi/megaraid/megaraid_sas_base.c|2885| <<megasas_reset_timer>> if (time_after(jiffies, scmd->jiffies_at_alloc +
+	 *   - drivers/scsi/qla2xxx/qla_target.c|4353| <<qlt_get_tag>> cmd->jiffies_at_alloc = get_jiffies_64();
+	 *   - drivers/scsi/scsi_debugfs.c|10| <<scsi_show_rq>> int msecs = jiffies_to_msecs(jiffies - cmd->jiffies_at_alloc);
+	 *   - drivers/scsi/scsi_lib.c|988| <<scsi_io_completion>> time_before(cmd->jiffies_at_alloc + wait_for, jiffies))
+	 *   - drivers/scsi/scsi_lib.c|1167| <<scsi_initialize_rq>> cmd->jiffies_at_alloc = jiffies;
+	 *   - drivers/scsi/scsi_lib.c|1216| <<scsi_init_command>> jiffies_at_alloc = cmd->jiffies_at_alloc;
+	 *   - drivers/scsi/scsi_lib.c|1227| <<scsi_init_command>> cmd->jiffies_at_alloc = jiffies_at_alloc;
+	 *   - drivers/scsi/scsi_lib.c|1652| <<scsi_softirq_done>> time_before(cmd->jiffies_at_alloc + wait_for, jiffies)) {
+	 */
 	unsigned long jiffies_at_alloc;
 
 	int retries;
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index 6d5d5faa989b..989296b50850 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -47,6 +47,16 @@
 /* The Host uses this in used->flags to advise the Guest: don't kick me when
  * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
  * will still kick if it's out of buffers. */
+/*
+ * 在以下使用VRING_USED_F_NO_NOTIFY:
+ *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+ *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+ *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+ *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+ *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+ */
 #define VRING_USED_F_NO_NOTIFY	1
 /* The Guest uses this in avail->flags to advise the Host: don't interrupt me
  * when you consume a buffer.  It's unreliable, so it's simply an
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index 043bfc35b353..180b7373960d 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -497,6 +497,19 @@ static bool irq_check_poll(struct irq_desc *desc)
 
 static bool irq_may_run(struct irq_desc *desc)
 {
+	/*
+	 * 在以下使用IRQD_IRQ_INPROGRESS:
+	 *   - kernel/irq/debugfs.c|103| <<global>> BIT_MASK_DESCR(IRQD_IRQ_INPROGRESS),
+	 *   - include/linux/irq.h|327| <<irqd_irq_inprogress>> return __irqd_to_state(d) & IRQD_IRQ_INPROGRESS;
+	 *   - kernel/irq/chip.c|473| <<handle_nested_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|484| <<handle_nested_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|500| <<irq_may_run>> unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
+	 *   - kernel/irq/chip.c|586| <<handle_untracked_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|592| <<handle_untracked_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|201| <<handle_irq_event>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|207| <<handle_irq_event>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/manage.c|1380| <<__setup_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 */
 	unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
 
 	/*
diff --git a/kernel/irq/spurious.c b/kernel/irq/spurious.c
index 987d7bca4864..92d012459037 100644
--- a/kernel/irq/spurious.c
+++ b/kernel/irq/spurious.c
@@ -94,6 +94,9 @@ static int try_one_irq(struct irq_desc *desc, bool force)
 		goto out;
 
 	/* Already running on another processor */
+	/*
+	 * 查看__irqd_to_state(d) & IRQD_IRQ_INPROGRESS;
+	 */
 	if (irqd_irq_inprogress(&desc->irq_data)) {
 		/*
 		 * Already running: If it is shared get the other
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index e4d90224507a..31f7b73bd0f4 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -114,6 +114,15 @@ irqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)
 /*
  * Race-free decouple logic (ordering is critical)
  */
+/*
+ * [0] irqfd_shutdown
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|308| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->shutdown, irqfd_shutdown);
+ */
 static void
 irqfd_shutdown(struct work_struct *work)
 {
@@ -165,6 +174,20 @@ irqfd_is_active(struct kvm_kernel_irqfd *irqfd)
  *
  * assumes kvm->irqfds.lock is held
  */
+/*
+ * [0] irqfd_deactivate
+ * [0] kvm_irqfd
+ * [0] kvm_vm_ioctl
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|253| <<irqfd_wakeup>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|577| <<kvm_irqfd_deassign>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|622| <<kvm_irqfd_release>> irqfd_deactivate(irqfd);
+ */
 static void
 irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 {
@@ -172,6 +195,9 @@ irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 
 	list_del_init(&irqfd->list);
 
+	/*
+	 * irqfd_shutdown()
+	 */
 	queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
 }
 
@@ -187,6 +213,10 @@ int __attribute__((weak)) kvm_arch_set_irq_inatomic(
 /*
  * Called with wqh->lock held and interrupts disabled
  */
+/*
+ * 在以下使用irqfd_wakeup():
+ *   - virt/kvm/eventfd.c|381| <<kvm_irqfd_assign>> init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);
+ */
 static int
 irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
@@ -284,6 +314,10 @@ int  __attribute__((weak)) kvm_arch_update_irqfd_routing(
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|607| <<kvm_irqfd>> return kvm_irqfd_assign(kvm, args);
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -525,12 +559,25 @@ kvm_eventfd_init(struct kvm *kvm)
 /*
  * shutdown any irqfd's that match fd+gsi
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|579| <<kvm_irqfd>> return kvm_irqfd_deassign(kvm, args);
+ */
 static int
 kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	struct kvm_kernel_irqfd *irqfd, *tmp;
 	struct eventfd_ctx *eventfd;
 
+	/*
+	 * struct kvm_irqfd {
+	 *     __u32 fd;
+	 *     __u32 gsi;
+	 *     __u32 flags;
+	 *     __u32 resamplefd;
+	 *     __u8  pad[16];
+	 * };
+	 */
 	eventfd = eventfd_ctx_fdget(args->fd);
 	if (IS_ERR(eventfd))
 		return PTR_ERR(eventfd);
@@ -565,9 +612,22 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * 处理KVM_IRQFD:
+ *   - virt/kvm/kvm_main.c|3212| <<kvm_vm_ioctl(KVM_IRQFD)>> r = kvm_irqfd(kvm, &data);
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
+	/*
+	 * struct kvm_irqfd {
+	 *     __u32 fd;
+	 *     __u32 gsi;
+	 *     __u32 flags;
+	 *     __u32 resamplefd;
+	 *      __u8  pad[16];
+	 * };
+	 */
 	if (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))
 		return -EINVAL;
 
@@ -581,6 +641,10 @@ kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
  * This function is called as the kvm VM fd is being released. Shutdown all
  * irqfds that still remain open
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|811| <<kvm_vm_release>> kvm_irqfd_release(kvm);
+ */
 void
 kvm_irqfd_release(struct kvm *kvm)
 {
@@ -632,6 +696,10 @@ void kvm_irq_routing_update(struct kvm *kvm)
  * aggregated from all vm* instances. We need our own isolated
  * queue to ease flushing work items when a VM exits.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4194| <<kvm_init>> r = kvm_irqfd_init();
+ */
 int kvm_irqfd_init(void)
 {
 	irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
@@ -670,9 +738,18 @@ struct _ioeventfd {
 static inline struct _ioeventfd *
 to_ioeventfd(struct kvm_io_device *dev)
 {
+	/*
+	 * struct _ioeventfd:
+	 *   -> struct kvm_io_device dev;
+	 */
 	return container_of(dev, struct _ioeventfd, dev);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|820| <<ioeventfd_destructor>> ioeventfd_release(p);
+ *   - virt/kvm/eventfd.c|950| <<kvm_deassign_ioeventfd_idx>> ioeventfd_release(p);
+ */
 static void
 ioeventfd_release(struct _ioeventfd *p)
 {
@@ -681,6 +758,10 @@ ioeventfd_release(struct _ioeventfd *p)
 	kfree(p);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|804| <<ioeventfd_write>> if (!ioeventfd_in_range(p, addr, len, val))
+ */
 static bool
 ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 {
@@ -727,6 +808,20 @@ ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 }
 
 /* MMIO/PIO writes trigger an event if the addr/val match */
+/*
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int
 ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
 		int len, const void *val)
@@ -752,6 +847,21 @@ ioeventfd_destructor(struct kvm_io_device *this)
 	ioeventfd_release(p);
 }
 
+/*
+ * 使用kvm_iodevice_init()使用ioevenfd_ops. 以下是调用kvm_iodevice_init()的地方:
+ *   - arch/x86/kvm/i8254.c|691| <<kvm_create_pit>> kvm_iodevice_init(&pit->dev, &pit_dev_ops);
+ *   - arch/x86/kvm/i8254.c|698| <<kvm_create_pit>> kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);
+ *   - arch/x86/kvm/i8259.c|603| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
+ *   - arch/x86/kvm/i8259.c|604| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
+ *   - arch/x86/kvm/i8259.c|605| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_eclr, &picdev_eclr_ops);
+ *   - arch/x86/kvm/ioapic.c|635| <<kvm_ioapic_init>> kvm_iodevice_init(&ioapic->dev, &ioapic_mmio_ops);
+ *   - arch/x86/kvm/lapic.c|2276| <<kvm_create_lapic>> kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
+ *   - virt/kvm/coalesced_mmio.c|150| <<kvm_vm_ioctl_register_coalesced_mmio>> kvm_iodevice_init(&dev->dev, &coalesced_mmio_ops);
+ *   - virt/kvm/eventfd.c|917| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ *
+ * 在以下使用ioeventfd_ops:
+ *   - virt/kvm/eventfd.c|894| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static const struct kvm_io_device_ops ioeventfd_ops = {
 	.write      = ioeventfd_write,
 	.destructor = ioeventfd_destructor,
@@ -763,6 +873,13 @@ ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
 {
 	struct _ioeventfd *_p;
 
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|555| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|834| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|902| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|933| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	list_for_each_entry(_p, &kvm->ioeventfds, list)
 		if (_p->bus_idx == p->bus_idx &&
 		    _p->addr == p->addr &&
@@ -775,6 +892,15 @@ ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
 	return false;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1019| <<kvm_deassign_ioeventfd>> enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
+ *   - virt/kvm/eventfd.c|1034| <<kvm_assign_ioeventfd>> bus_idx = ioeventfd_bus_from_flags(args->flags);
+ *
+ * --> flags & KVM_IOEVENTFD_FLAG_PIO               --> KVM_PIO_BUS
+ * --> flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY --> KVM_VIRTIO_CCW_NOTIFY_BUS
+ * --> else                                         --> KVM_VIRTIO_CCW_NOTIFY_BUS
+ */
 static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 {
 	if (flags & KVM_IOEVENTFD_FLAG_PIO)
@@ -784,6 +910,11 @@ static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 	return KVM_MMIO_BUS;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1004| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);
+ *   - virt/kvm/eventfd.c|1012| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);
+ */
 static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 				enum kvm_bus bus_idx,
 				struct kvm_ioeventfd *args)
@@ -823,6 +954,7 @@ static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 		goto unlock_fail;
 	}
 
+	/* 设置kvm_io_device->ops = ioeventfd_ops */
 	kvm_iodevice_init(&p->dev, &ioeventfd_ops);
 
 	ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
@@ -830,7 +962,18 @@ static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 	if (ret < 0)
 		goto unlock_fail;
 
+	/*
+	 * struct kvm:
+	 *   -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|555| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|834| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|902| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|933| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	list_add_tail(&p->list, &kvm->ioeventfds);
 
 	mutex_unlock(&kvm->slots_lock);
@@ -893,6 +1036,11 @@ kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,
 
 static int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
+	/*
+	 * --> flags & KVM_IOEVENTFD_FLAG_PIO               --> KVM_PIO_BUS
+	 * --> flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 * --> else                                         --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 */
 	enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
 	int ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);
 
@@ -908,6 +1056,11 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	enum kvm_bus              bus_idx;
 	int ret;
 
+	/*
+	 * --> flags & KVM_IOEVENTFD_FLAG_PIO               --> KVM_PIO_BUS
+	 * --> flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 * --> else                                         --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 */
 	bus_idx = ioeventfd_bus_from_flags(args->flags);
 	/* must be natural-word sized, or 0 to ignore length */
 	switch (args->len) {
@@ -954,6 +1107,10 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * 处理KVM_IOEVENTFD:
+ *   - virt/kvm/kvm_main.c|3224| <<kvm_vm_ioctl(KVM_IOEVENTFD)>> r = kvm_ioeventfd(kvm, &data);
+ */
 int
 kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index a7de45f4c1b5..ddf81f41e09d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -95,6 +95,9 @@ EXPORT_SYMBOL_GPL(halt_poll_ns_shrink);
 
 DEFINE_MUTEX(kvm_lock);
 static DEFINE_RAW_SPINLOCK(kvm_count_lock);
+/*
+ * 把kvm->vm_list链上去
+ */
 LIST_HEAD(vm_list);
 
 static cpumask_var_t cpus_hardware_enabled;
@@ -3611,9 +3614,22 @@ static int kvm_io_bus_sort_cmp(const void *p1, const void *p2)
 	return kvm_io_bus_cmp(p1, p2);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3651| <<__kvm_io_bus_write>> idx = kvm_io_bus_get_first_dev(bus, range->addr, range->len);
+ *   - virt/kvm/kvm_main.c|3735| <<__kvm_io_bus_read>> idx = kvm_io_bus_get_first_dev(bus, range->addr, range->len);
+ *   - virt/kvm/kvm_main.c|3886| <<kvm_io_bus_get_dev>> dev_idx = kvm_io_bus_get_first_dev(bus, addr, 1);
+ */
 static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 			     gpa_t addr, int len)
 {
+	/*
+	 * struct kvm_io_range {
+	 *     gpa_t addr;
+	 *     int len;
+	 *     struct kvm_io_device *dev;
+	 * };
+	 */
 	struct kvm_io_range *range, key;
 	int off;
 
@@ -3635,6 +3651,11 @@ static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 	return off;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3677| <<kvm_io_bus_write>> r = __kvm_io_bus_write(vcpu, bus, &range, val);
+ *   - virt/kvm/kvm_main.c|3708| <<kvm_io_bus_write_cookie>> return __kvm_io_bus_write(vcpu, bus, &range, val);
+ */
 static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 			      struct kvm_io_range *range, const void *val)
 {
@@ -3656,6 +3677,15 @@ static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 }
 
 /* kvm_io_bus_write - called under kvm->slots_lock */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s.c|943| <<kvmppc_h_logical_ci_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);
+ *   - arch/powerpc/kvm/powerpc.c|1329| <<kvmppc_handle_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
+ *   - arch/x86/kvm/vmx.c|7800| <<handle_ept_misconfig>> !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
+ *   - arch/x86/kvm/x86.c|4810| <<vcpu_mmio_write>> && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))
+ *   - arch/x86/kvm/x86.c|5415| <<kernel_pio>> r = kvm_io_bus_write(vcpu, KVM_PIO_BUS,
+ *   - virt/kvm/arm/mmio.c|194| <<io_mem_abort>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
+ */
 int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val)
 {
@@ -3668,6 +3698,11 @@ int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		.len = len,
 	};
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 *  -> struct kvm *kvm;
+	 *     -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	bus = srcu_dereference(vcpu->kvm->buses[bus_idx], &vcpu->kvm->srcu);
 	if (!bus)
 		return -ENOMEM;
@@ -3748,6 +3783,22 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 
 
 /* Caller must hold slots_lock. */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/mpic.c|1450| <<map_mmio>> kvm_io_bus_register_dev(opp->kvm, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/i8254.c|692| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
+ *   - arch/x86/kvm/i8254.c|699| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
+ *   - arch/x86/kvm/i8259.c|607| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
+ *   - arch/x86/kvm/i8259.c|616| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_eclr);
+ *   - arch/x86/kvm/ioapic.c|638| <<kvm_ioapic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, ioapic->base_address,
+ *   - virt/kvm/arm/vgic/vgic-its.c|1672| <<vgic_register_its_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|651| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|666| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, sgi_base,
+ *   - virt/kvm/arm/vgic/vgic-mmio.c|892| <<vgic_register_dist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,
+ *   - virt/kvm/coalesced_mmio.c|155| <<kvm_vm_ioctl_register_coalesced_mmio>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, zone->addr,
+ *   - virt/kvm/eventfd.c|960| <<kvm_assign_ioeventfd_idx>> ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
+ */
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev)
 {
@@ -3755,6 +3806,10 @@ int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 	struct kvm_io_bus *new_bus, *bus;
 	struct kvm_io_range range;
 
+	/*
+	 * struct kvm:
+	 *   -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	bus = kvm_get_bus(kvm, bus_idx);
 	if (!bus)
 		return -ENOMEM;
-- 
2.17.1

