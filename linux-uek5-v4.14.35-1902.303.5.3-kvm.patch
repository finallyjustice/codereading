From c606e6bade9d2695b473fe8b687dce2d44ececb4 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 8 Feb 2021 08:50:33 -0800
Subject: [PATCH 1/1] linux-uek5-v4.14.35-1902.303.5.3-kvm

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h            |   17 +
 arch/x86/include/asm/spec-ctrl.h           |    5 +
 arch/x86/include/asm/vmx.h                 |   22 +
 arch/x86/include/uapi/asm/kvm.h            |   11 +
 arch/x86/kernel/cpu/bugs.c                 |   29 +
 arch/x86/kernel/reboot.c                   |    5 +
 arch/x86/kvm/kvm_cache_regs.h              |   47 +
 arch/x86/kvm/lapic.c                       |    8 +
 arch/x86/kvm/mmu.c                         |   79 ++
 arch/x86/kvm/vmx.c                         | 1335 +++++++++++++++++++
 arch/x86/kvm/vmx_evmcs.h                   |    9 +
 arch/x86/kvm/x86.c                         |   34 +
 arch/x86/kvm/x86.h                         |   12 +
 drivers/acpi/osl.c                         |   56 +
 drivers/net/tap.c                          |    3 +
 drivers/net/tun.c                          |   31 +
 drivers/net/virtio_net.c                   |    4 +
 drivers/pci/hotplug/acpiphp_glue.c         |    7 +
 drivers/pci/probe.c                        |   16 +
 drivers/pci/setup-bus.c                    |    4 +
 drivers/scsi/scsi_lib.c                    |   10 +
 drivers/scsi/virtio_scsi.c                 |   47 +
 drivers/vhost/net.c                        |  309 +++++
 drivers/vhost/scsi.c                       |  143 ++
 drivers/vhost/vhost.c                      | 1367 ++++++++++++++++++++
 drivers/vhost/vhost.h                      |  160 +++
 drivers/virtio/virtio_pci_modern.c         |   21 +
 drivers/virtio/virtio_ring.c               |  210 +++
 drivers/xen/xenbus/xenbus_probe_frontend.c |   18 +
 fs/eventfd.c                               |   81 ++
 include/kvm/iodev.h                        |   32 +
 include/linux/irq.h                        |   13 +
 include/linux/kvm_host.h                   |   26 +
 include/linux/ptr_ring.h                   |  286 ++++
 include/linux/skb_array.h                  |  153 ++-
 include/scsi/scsi_cmnd.h                   |   13 +
 include/uapi/linux/virtio_ring.h           |   10 +
 kernel/irq/chip.c                          |   13 +
 kernel/irq/spurious.c                      |    3 +
 kernel/rcu/tree.c                          |    5 +
 lib/nmi_backtrace.c                        |    8 +
 net/core/dev.c                             |    5 +
 virt/kvm/eventfd.c                         |  157 +++
 virt/kvm/kvm_main.c                        |   94 ++
 44 files changed, 4917 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f1ad9c115d79..a06c3f2b0420 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -66,6 +66,16 @@
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_SCAN_IOAPIC:
+ *   - arch/x86/kvm/hyperv.c|130| <<synic_set_sint>> kvm_make_request(KVM_REQ_SCAN_IOAPIC, synic_to_vcpu(synic));
+ *   - arch/x86/kvm/x86.c|7379| <<kvm_make_scan_ioapic_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
+ *   - arch/x86/kvm/x86.c|7536| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_SCAN_IOAPIC, vcpu))
+ *
+ * KVM_REQ_SCAN_IOAPIC = 0x0000000000000317
+ * KVM_REQ_MMU_RELOAD  = 0x0000000000000301
+ * KVM_REQUEST_MASK    = 0x00000000000000ff
+ */
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
@@ -1423,6 +1433,13 @@ enum {
 #define HF_VINTR_MASK		(1 << 2)
 #define HF_NMI_MASK		(1 << 3)
 #define HF_IRET_MASK		(1 << 4)
+/*
+ * 在以下使用HF_GUEST_MASK:
+ *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+ *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+ */
 #define HF_GUEST_MASK		(1 << 5) /* VCPU is in guest-mode */
 #define HF_SMM_MASK		(1 << 6)
 #define HF_SMM_INSIDE_NMI_MASK	(1 << 7)
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 5393babc0598..7cd01b5b0e65 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -23,6 +23,11 @@ extern void x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bo
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|5611| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
+ *   - arch/x86/kvm/vmx.c|10997| <<vmx_vcpu_run>> x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index 9527ba5d62da..7d676417b21c 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -73,6 +73,10 @@
 #define SECONDARY_EXEC_RDRAND_EXITING		0x00000800
 #define SECONDARY_EXEC_ENABLE_INVPCID		0x00001000
 #define SECONDARY_EXEC_ENABLE_VMFUNC            0x00002000
+/*
+ * If this control is 1, executions of VMREAD and VMWRITE in VMX non-root operation
+ * may access a shadow VMCS (instead of causing VM exits).
+ */
 #define SECONDARY_EXEC_SHADOW_VMCS              0x00004000
 #define SECONDARY_EXEC_ENCLS_EXITING		0x00008000
 #define SECONDARY_EXEC_RDSEED_EXITING		0x00010000
@@ -220,6 +224,18 @@ enum vmcs_field {
 	TSC_MULTIPLIER_HIGH             = 0x00002033,
 	GUEST_PHYSICAL_ADDRESS          = 0x00002400,
 	GUEST_PHYSICAL_ADDRESS_HIGH     = 0x00002401,
+	/*
+	 * 在以下使用VMCS_LINK_POINTER (L0设置vmcs的地址直接vmptrld指令吧?只有L1的shadow才用这里):
+	 *   - arch/x86/kvm/vmx.c|1396| <<global>> FIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),
+	 *   - arch/x86/kvm/vmx_evmcs.h|71| <<global>> EVMCS1_FIELD(VMCS_LINK_POINTER, vmcs_link_pointer,
+	 *   - tools/testing/selftests/kvm/include/vmx.h|217| <<global>> VMCS_LINK_POINTER = 0x00002800,
+	 *   - arch/x86/kvm/vmx.c|7098| <<vmx_vcpu_setup>> vmcs_write64(VMCS_LINK_POINTER, -1ull);
+	 *   - arch/x86/kvm/vmx.c|9080| <<vmx_disable_shadow_vmcs>> vmcs_write64(VMCS_LINK_POINTER, -1ull);
+	 *   - arch/x86/kvm/vmx.c|9694| <<set_current_vmptr>> vmcs_write64(VMCS_LINK_POINTER,
+	 *   - arch/x86/kvm/vmx.c|13120| <<prepare_vmcs02_full>> vmcs_write64(VMCS_LINK_POINTER, -1ull);
+	 *   - tools/testing/selftests/kvm/lib/vmx.c|222| <<init_vmcs_guest_state>> vmwrite(VMCS_LINK_POINTER, -1ll);
+	 *   - tools/testing/selftests/kvm/state_test.c|84| <<l1_guest_code>> vmwrite(VMCS_LINK_POINTER, vmx_pages->shadow_vmcs_gpa);
+	 */
 	VMCS_LINK_POINTER               = 0x00002800,
 	VMCS_LINK_POINTER_HIGH          = 0x00002801,
 	GUEST_IA32_DEBUGCTL             = 0x00002802,
@@ -507,6 +523,12 @@ enum vmcs_field {
 #define ASM_VMX_VMCLEAR_RAX       ".byte 0x66, 0x0f, 0xc7, 0x30"
 #define ASM_VMX_VMLAUNCH          ".byte 0x0f, 0x01, 0xc2"
 #define ASM_VMX_VMRESUME          ".byte 0x0f, 0x01, 0xc3"
+/*
+ * VMPTRLD指令从内存中加载一个64位物理地址作为current-VMCS pointer,
+ * 这个current-VMCS pointer由处理器内部记录和维护,除了VMXON,VMPTRLD
+ * 和VMCLEAR指令需要提供VMXON或VMCS指针作为操作数外,其他的指令指定
+ * 都是在current-VMS上操作.
+ */
 #define ASM_VMX_VMPTRLD_RAX       ".byte 0x0f, 0xc7, 0x30"
 #define ASM_VMX_VMREAD_RDX_RAX    ".byte 0x0f, 0x78, 0xd0"
 #define ASM_VMX_VMWRITE_RAX_RDX   ".byte 0x0f, 0x79, 0xd0"
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index fd23d5778ea1..faaec4cc01cc 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -386,6 +386,17 @@ struct kvm_sync_regs {
 #define KVM_STATE_NESTED_SMM_VMXON	0x00000002
 
 struct kvm_vmx_nested_state {
+	/*
+	 * 在以下使用kvm_vmx_nested_state->vmxon_pa (__u64):
+	 *   - arch/x86/kvm/vmx.c|15375| <<vmx_get_nested_state>> .vmx.vmxon_pa = -1ull,
+	 *   - arch/x86/kvm/vmx.c|15386| <<vmx_get_nested_state>> kvm_state.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 *   - arch/x86/kvm/vmx.c|15470| <<vmx_set_nested_state>> return kvm_state->vmx.vmxon_pa == -1ull ? 0 : -EINVAL;
+	 *   - arch/x86/kvm/vmx.c|15472| <<vmx_set_nested_state>> if (kvm_state->vmx.vmxon_pa == -1ull) {
+	 *   - arch/x86/kvm/vmx.c|15483| <<vmx_set_nested_state>> if (!page_address_valid(vcpu, kvm_state->vmx.vmxon_pa))
+	 *   - arch/x86/kvm/vmx.c|15507| <<vmx_set_nested_state>> if (kvm_state->vmx.vmxon_pa == -1ull)
+	 *   - arch/x86/kvm/vmx.c|15510| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->vmx.vmxon_pa;
+	 *   - arch/x86/kvm/vmx.c|15519| <<vmx_set_nested_state>> if (kvm_state->vmx.vmcs_pa == kvm_state->vmx.vmxon_pa ||
+	 */
 	__u64 vmxon_pa;
 	__u64 vmcs_pa;
 
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index ed472e0bd6f8..b79b7e73bcd4 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -383,6 +383,35 @@ void x86_spec_ctrl_set(enum spec_ctrl_set_context context)
 }
 EXPORT_SYMBOL_GPL(x86_spec_ctrl_set);
 
+/*
+ * 多了两个MSR,IA32_SPEC_CTRL和IA32_PRED_CMD,IBRS和STIBP通过前一个MSR控制,IBPB通过后一个MSR控制.
+ *
+ * IBRS和STIBP是一种control,IBPB是一种command,具体来说,就是IBRS和STIBP会有状态信息,而IBPB是一种瞬时值.
+ *
+ * IBRS: Indirect Branch Restricted Speculation,一般情况下,在高权限
+ * 代码里面向IBRS的控制位写1,就能够保证indirect branch不被低权限时
+ * 候train出来的predictor影响,也能够防止逻辑处理器的影响(超线程的时
+ * 候).这里权限转换就是host user->host kernel, guest->host等等.可以
+ * 把IBRS理解成不同特权级之间的predictor隔离.IBRS不能防止同一个级别
+ * 的predictor共享,需要配合IBPB.IBRS也不能防止RSB的污染,需要在进入特
+ * 权级的时候情况RSB.
+ *
+ * STIBP: Single thread indirect branch predictors, 超线程中,一个core
+ * 的逻辑处理器会共享一个indirect branch predictor,STIBP就是禁止这种
+ * 共享,防止一个逻辑处理器的predictor被另一个污染.STIBP是IBRS的一个子
+ * 集,所以一般开启了IBRS就不用开STIBP了.
+ *
+ * IBPB: Indirect Branch Predictor Barrier (IBPB): IBPB类似于一个barrier,
+ * 在这之前的indirect branch predictor不会影响这之后的.
+ *
+ * IBRS和IBPB可以结合起来一起作为spectre变种2的mitigation: IBRS用于防
+ * 止权限之间的predictor污染,IBPB用来阻止同一个权限下不同的实体之间的
+ * predictor污染(比如应用程序之间或者虚拟机之间).
+ *
+ * called by:
+ *   - arch/x86/include/asm/spec-ctrl.h|29| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, true);
+ *   - arch/x86/include/asm/spec-ctrl.h|43| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, false);
+ */
 void
 x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
 {
diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c
index d6b58653cf1f..a978202f985f 100644
--- a/arch/x86/kernel/reboot.c
+++ b/arch/x86/kernel/reboot.c
@@ -780,6 +780,11 @@ static nmi_shootdown_cb shootdown_callback;
 static atomic_t waiting_for_crash_ipi;
 static int crash_ipi_issued;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/reboot.c|833| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/reboot.c|864| <<run_crash_ipi_callback>> crash_nmi_callback(0, regs);
+ */
 static int crash_nmi_callback(unsigned int val, struct pt_regs *regs)
 {
 	int cpu;
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 9619dcc2b325..23ec9905f603 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -64,6 +64,11 @@ static inline ulong kvm_read_cr4_bits(struct kvm_vcpu *vcpu, ulong mask)
 	ulong tmask = mask & KVM_POSSIBLE_CR4_GUEST_BITS;
 	if (tmask & vcpu->arch.cr4_guest_owned_bits)
 		kvm_x86_ops->decache_cr4_guest_bits(vcpu);
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 *  -> struct kvm_vcpu_arch arch;
+	 *     -> unsigned long cr4;
+	 */
 	return vcpu->arch.cr4 & mask;
 }
 
@@ -85,13 +90,41 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_register_read(vcpu, VCPU_REGS_RDX) & -1u) << 32);
 }
 
+/*
+ * 为vcpu->arch.hflags设置HF_GUEST_MASK, 在以下使用HF_GUEST_MASK:
+ *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+ *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	vcpu->arch.hflags |= HF_GUEST_MASK;
 }
 
+/*
+ * 为vcpu->arch.hflags清除HF_GUEST_MASK, 在以下使用HF_GUEST_MASK:
+ *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+ *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
 
 	if (vcpu->arch.load_eoi_exitmap_pending) {
@@ -100,8 +133,22 @@ static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 判断为vcpu->arch.hflags是否设置了HF_GUEST_MASK, 在以下使用HF_GUEST_MASK:
+ *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+ *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+ *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+ */
 static inline bool is_guest_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	return vcpu->arch.hflags & HF_GUEST_MASK;
 }
 
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index e2e4807f4836..1bb1a3556f02 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -44,6 +44,14 @@
 #include "cpuid.h"
 #include "hyperv.h"
 
+/*
+ * smp_kvm_posted_intr_ipi() Posted-interrupt notification event
+ *
+ * smp_kvm_posted_intr_wakeup_ipi() Nested posted-interrupt event
+ *
+ * smp_kvm_posted_intr_nested_ipi() Posted-interrupt wakeup event
+ */
+
 #ifndef CONFIG_X86_64
 #define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
 #else
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index d3626520bb7b..15bf0c980819 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -5014,6 +5014,79 @@ static void init_kvm_softmmu(struct kvm_vcpu *vcpu)
 	context->inject_page_fault = kvm_inject_page_fault;
 }
 
+/*
+ * 在L1中创建L2的时候会调用几次kvm_init_mmu()
+ *
+ * kvm_init_mmu
+ * prepare_vmcs02
+ * enter_vmx_non_root_mode
+ * nested_vmx_run
+ * handle_vmlaunch
+ * __dta_vmx_handle_exit_444
+ * __dta_vcpu_enter_guest_1349
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_init_mmu
+ * load_vmcs12_mmu_host_state
+ * load_vmcs12_host_state
+ * __dta_nested_vmx_vmexit_416
+ * vmx_check_nested_events
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_init_mmu
+ * load_vmcs12_mmu_host_state
+ * load_vmcs12_host_state
+ * __dta_nested_vmx_vmexit_416
+ * nested_ept_inject_page_fault
+ * ept_page_fault
+ * kvm_mmu_page_fault
+ * handle_ept_violation
+ * __dta_vmx_handle_exit_444
+ * __dta_vcpu_enter_guest_1349
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_init_mmu
+ * load_vmcs12_mmu_host_state
+ * load_vmcs12_host_state
+ * __dta_nested_vmx_vmexit_416
+ * __dta_vmx_handle_exit_444
+ * __dta_vcpu_enter_guest_1349
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_init_mmu
+ * prepare_vmcs02
+ * enter_vmx_non_root_mode
+ * nested_vmx_run
+ * handle_vmresume
+ * __dta_vmx_handle_exit_444
+ * __dta_vcpu_enter_guest_1349
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *g_context = &vcpu->arch.nested_mmu;
@@ -5057,6 +5130,12 @@ static void init_kvm_nested_mmu(struct kvm_vcpu *vcpu)
 	update_last_nonleaf_level(vcpu, g_context);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.c|5169| <<kvm_mmu_reset_context>> kvm_init_mmu(vcpu, true);
+ *   - arch/x86/kvm/mmu.c|5659| <<kvm_mmu_setup>> kvm_init_mmu(vcpu, false);
+ *   - arch/x86/kvm/vmx.c|12143| <<nested_vmx_load_cr3>> kvm_init_mmu(vcpu, false);
+ */
 void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)
 {
 	if (reset_roots) {
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 18331ebf892f..6c4b43146c91 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -62,6 +62,94 @@
 #include "pmu.h"
 #include "vmx_evmcs.h"
 
+/*
+ * 有一个结构是struct vmcs.
+ *
+ * struct vmcs {
+ *     struct vmcs_hdr hdr;
+ *     u32 abort;
+ *     char data[0];
+ * };
+ *
+ * 在alloc_vmcs_cpu()分配的时候是:
+ *
+ * pages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);
+ *
+ *
+ * loaded_vmcs points to the VMCS currently used in this vcpu. For a
+ * non-nested (L1) guest, it always points to vmcs01. For a nested
+ * guest (L2), it points to a different VMCS.  loaded_cpu_state points
+ * to the VMCS whose state is loaded into the CPU registers that only
+ * need to be switched when transitioning to/from the kernel; a NULL
+ * value indicates that host state is loaded.
+ *
+ * struct vcpu_vmx:
+ *  -> struct kvm_vcpu vcpu;
+ *  -> u8 fail;
+ *  -> struct loaded_vmcs vmcs01;
+ *      -> struct vmcs *vmcs;
+ *      -> struct vmcs *shadow_vmcs;
+ *      -> int cpu;
+ *  -> struct loaded_vmcs *loaded_vmcs;
+ *  -> struct loaded_vmcs *loaded_cpu_state;
+ *  -> u32 exit_reason;
+ *  -> struct nested_vmx nested;
+ *      -> struct vmcs12 *cached_vmcs12;
+ *      -> struct vmcs12 *cached_shadow_vmcs12;
+ *      -> struct loaded_vmcs vmcs02;
+ *          -> struct vmcs *vmcs;
+ *          -> struct vmcs *shadow_vmcs;
+ *          -> int cpu;
+ *
+ * ----------------------------------------
+ *
+ * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested
+ * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it
+ * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2
+ * guest in a way that will both be appropriate to L1's requests, and our
+ * needs. In addition to modifying the active vmcs (which is vmcs02), this
+ * function also has additional necessary side-effects, like setting various
+ * vcpu->arch fields.
+ *
+ * 假设是L1中要vmresume.
+ *
+ * 1. 为从L0进入L2做准备.
+ *
+ * handle_vmresume()
+ * -> nested_vmx_run(vcpu, false);
+ *
+ * copy_shadow_to_vmcs12(vmx)会把vmx->vmcs01.shadow_vmcs(类型vmcs)的数据同
+ * 步到vmx->nested.cached_vmcs12(类型vmcs12) 这里的shadow vmcs是L1中用来支
+ * 持L2的vmcs.
+ * 
+ * Copy the writable VMCS shadow fields back to the VMCS12, in case
+ * they have been modified by the L1 guest. Note that the "read-only"
+ * VM-exit information fields are actually writable if the vCPU is
+ * configured to support "VMWRITE to any supported field in the VMCS."
+ *
+ *
+ * enter_vmx_non_root_mode()
+ * -> enter_guest_mode(vcpu) --> 为vcpu->arch.hflags设置HF_GUEST_MASK
+ * -> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02); --> 把vmx->loaded_vmcs设置成参数的vmx->nested.vmcs02
+ * -> prepare_vmcs02()
+ *
+ * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested
+ * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it
+ * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2
+ * guest in a way that will both be appropriate to L1's requests, and our
+ * needs. In addition to modifying the active vmcs (which is vmcs02), this
+ * function also has additional necessary side-effects, like setting various
+ * vcpu->arch fields.
+ *
+ * 2. 从L0进入L2.
+ *
+ * vmx_vcpu_run()
+ * -> copy_vmcs12_to_shadow() --> 把vmx->nested.cached_vmcs12(类型vmcs12)给同步到vmx->vmcs01.shadow_vmcs(类型vmcs)
+ *                                这里为了在L1修改针对L2的vmcs时不用trap到L0了
+ * -> vmx->__launched = vmx->loaded_vmcs->launched;
+ * -> 进入guest mode (直接vmcs02的L2)
+ */
+
 #define __ex(x) __kvm_handle_fault_on_reboot(x)
 #define __ex_clear(x, reg) \
 	____kvm_handle_fault_on_reboot(x, "xor " reg " , " reg)
@@ -362,6 +450,14 @@ struct kvm_vmx {
 
 struct vmcs_hdr {
 	u32 revision_id:31;
+	/*
+	 * 在以下使用vmcs_hdr->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|4999| <<alloc_vmcs_cpu>> vmcs->hdr.shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx.c|13341| <<nested_vmx_check_vmcs_link_ptr>> shadow->hdr.shadow_vmcs != nested_cpu_has_shadow_vmcs(vmcs12))
+	 *   - arch/x86/kvm/vmx.c|9481| <<handle_vmptrld>> (new_vmcs12->hdr.shadow_vmcs &&
+	 *   - arch/x86/kvm/vmx.c|13540| <<nested_vmx_run>> if (vmcs12->hdr.shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|15014| <<vmx_set_nested_state>> !shadow_vmcs12->hdr.shadow_vmcs)
+	 */
 	u32 shadow_vmcs:1;
 };
 
@@ -395,16 +491,57 @@ struct vmcs_host_state {
  */
 struct loaded_vmcs {
 	struct vmcs *vmcs;
+	/*
+	 * 在以下修改loaded_vmcs->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|4878| <<alloc_loaded_vmcs>> loaded_vmcs->shadow_vmcs = NULL;
+	 *   - arch/x86/kvm/vmx.c|8410| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+	 *   - arch/x86/kvm/vmx.c|8627| <<free_nested>> vmx->vmcs01.shadow_vmcs = NULL;
+	 * 在以下使用loaded_vmcs->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|2203| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx.c|2204| <<loaded_vmcs_init>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|4864| <<free_loaded_vmcs>> WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
+	 *   - arch/x86/kvm/vmx.c|8407| <<alloc_shadow_vmcs>> WARN_ON(loaded_vmcs == &vmx->vmcs01 && loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8409| <<alloc_shadow_vmcs>> if (!loaded_vmcs->shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|8411| <<alloc_shadow_vmcs>> if (loaded_vmcs->shadow_vmcs)
+	 *   - arch/x86/kvm/vmx.c|8412| <<alloc_shadow_vmcs>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8414| <<alloc_shadow_vmcs>> return loaded_vmcs->shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|8625| <<free_nested>> vmcs_clear(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8626| <<free_nested>> free_vmcs(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8807| <<copy_shadow_to_vmcs12>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|8864| <<copy_vmcs12_to_shadow>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|9062| <<set_current_vmptr>> __pa(vmx->vmcs01.shadow_vmcs));
+	 *
+	 * 用set_current_vmptr()把shadow_vmcs给硬件
+	 */
 	struct vmcs *shadow_vmcs;
 	int cpu;
+	/*
+	 * 在以下使用loaded_vmcs->launched:
+	 *   - arch/x86/kvm/vmx.c|2236| <<loaded_vmcs_init>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx.c|11243| <<vmx_vcpu_run>> vmx->loaded_vmcs->launched = 1;
+	 *   - arch/x86/kvm/vmx.c|2233| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx.c|11027| <<vmx_vcpu_run>> vmx->__launched = vmx->loaded_vmcs->launched;
+	 */
 	bool launched;
 	bool nmi_known_unmasked;
 	bool hv_timer_armed;
 	/* Support for vnmi-less CPUs */
 	int soft_vnmi_blocked;
+	/*
+	 * 在以下使用loaded_vmcs->entry_time:
+	 *   - arch/x86/kvm/vmx.c|10858| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->entry_time));
+	 *   - arch/x86/kvm/vmx.c|11003| <<vmx_vcpu_run>> vmx->loaded_vmcs->entry_time = ktime_get();
+	 */
 	ktime_t entry_time;
 	s64 vnmi_blocked_time;
 	unsigned long *msr_bitmap;
+	/*
+	 * 在以下使用loaded_vmcs->loaded_vmcss_on_cpu_link:
+	 *   - arch/x86/kvm/vmx.c|2455| <<crash_vmclear_local_loaded_vmcss>> loaded_vmcss_on_cpu_link)
+	 *   - arch/x86/kvm/vmx.c|2473| <<__loaded_vmcs_clear>> list_del(&loaded_vmcs->loaded_vmcss_on_cpu_link);
+	 *   - arch/x86/kvm/vmx.c|3370| <<vmx_vcpu_load>> list_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,
+	 *   - arch/x86/kvm/vmx.c|4771| <<vmclear_local_loaded_vmcss>> loaded_vmcss_on_cpu_link)
+	 */
 	struct list_head loaded_vmcss_on_cpu_link;
 	struct vmcs_host_state host_state;
 };
@@ -811,34 +948,110 @@ struct nested_vmx_msrs {
  */
 struct nested_vmx {
 	/* Has the level1 guest done vmxon? */
+	/*
+	 * 在以下修改nested_vmx->vmxon:
+	 *   - arch/x86/kvm/vmx.c|8566| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx.c|8751| <<free_nested>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx.c|14524| <<vmx_pre_enter_smm>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx.c|14535| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx.c|14716| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+	 */
 	bool vmxon;
+	/*
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx.c|8660| <<handle_vmon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|8828| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr) {
+	 *   - arch/x86/kvm/vmx.c|9285| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr) {
+	 *   - arch/x86/kvm/vmx.c|14575| <<vmx_get_nested_state>> kvm_state.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 *   - arch/x86/kvm/vmx.c|14699| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->vmx.vmxon_pa;
+	 */
 	gpa_t vmxon_ptr;
 	bool pml_full;
 
 	/* The guest-physical address of the current VMCS L1 keeps for L2 */
+	/*
+	 * 在以下修改nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx.c|8657| <<nested_release_vmcs12>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|8682| <<free_nested>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|9141| <<set_current_vmptr>> vmx->nested.current_vmptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|11494| <<vmx_create_vcpu>> vmx->nested.current_vmptr = -1ull;
+	 */
 	gpa_t current_vmptr;
 	/*
 	 * Cache of the guest's VMCS, existing outside of guest memory.
 	 * Loaded from guest memory during VMPTRLD. Flushed to guest
 	 * memory during VMCLEAR and VMPTRLD.
 	 */
+	/*
+	 * 在以下使用nested_vmx->cached_vmcs12:
+	 *   - arch/x86/kvm/vmx.c|1372| <<get_vmcs12>> return to_vmx(vcpu)->nested.cached_vmcs12;
+	 *   - arch/x86/kvm/vmx.c|8586| <<enter_vmx_operation>> vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL);
+	 *   - arch/x86/kvm/vmx.c|8587| <<enter_vmx_operation>> if (!vmx->nested.cached_vmcs12)
+	 *   - arch/x86/kvm/vmx.c|8618| <<enter_vmx_operation>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx.c|8809| <<nested_release_vmcs12>> vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx.c|8860| <<free_nested>> kfree(vmx->nested.cached_vmcs12);
+	 *   - arch/x86/kvm/vmx.c|9404| <<handle_vmptrld>> memcpy(vmx->nested.cached_vmcs12, new_vmcs12, VMCS12_SIZE);
+	 */
 	struct vmcs12 *cached_vmcs12;
 	/*
 	 * Cache of the guest's shadow VMCS, existing outside of guest
 	 * memory. Loaded from guest memory during VM entry. Flushed
 	 * to guest memory during VM exit.
 	 */
+	/*
+	 * 在以下使用nested_vmx->cached_shadow_vmcs12:
+	 *   - arch/x86/kvm/vmx.c|1391| <<get_shadow_vmcs12>> return to_vmx(vcpu)->nested.cached_shadow_vmcs12;
+	 *   - arch/x86/kvm/vmx.c|8590| <<enter_vmx_operation>> vmx->nested.cached_shadow_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL);
+	 *   - arch/x86/kvm/vmx.c|8591| <<enter_vmx_operation>> if (!vmx->nested.cached_shadow_vmcs12)
+	 *   - arch/x86/kvm/vmx.c|8615| <<enter_vmx_operation>> kfree(vmx->nested.cached_shadow_vmcs12);
+	 *   - arch/x86/kvm/vmx.c|8861| <<free_nested>> kfree(vmx->nested.cached_shadow_vmcs12);
+	 */
 	struct vmcs12 *cached_shadow_vmcs12;
 	/*
 	 * Indicates if the shadow vmcs must be updated with the
 	 * data hold by vmcs12
 	 */
+	/*
+	 * 在以下使用和修改nested_vmx->sync_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|8610| <<nested_release_vmcs12>> vmx->nested.sync_shadow_vmcs = false;
+	 *   - arch/x86/kvm/vmx.c|9107| <<set_current_vmptr>> vmx->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|10954| <<vmx_vcpu_run>> vmx->nested.sync_shadow_vmcs = false;
+	 *   - arch/x86/kvm/vmx.c|13720| <<nested_vmx_vmexit>> vmx->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|13810| <<nested_vmx_entry_failure>> to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|10952| <<vmx_vcpu_run>> if (vmx->nested.sync_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|14359| <<vmx_get_nested_state>> else if (enable_shadow_vmcs && !vmx->nested.sync_shadow_vmcs)
+	 */
 	bool sync_shadow_vmcs;
 	bool dirty_vmcs12;
 
 	bool change_vmcs01_virtual_apic_mode;
 
 	/* L2 must run next, and mustn't decide to exit to L1. */
+	/*
+	 * 在以下修改nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx.c|11616| <<vmx_vcpu_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|13526| <<nested_vmx_run>> vmx->nested.nested_run_pending = 1;
+	 *   - arch/x86/kvm/vmx.c|13530| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|13555| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|14218| <<vmx_leave_nested>> to_vmx(vcpu)->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|14902| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = !!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);
+	 * 在以下使用nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx.c|7171| <<vmx_nmi_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx.c|7185| <<vmx_interrupt_allowed>> return (!to_vmx(vcpu)->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|8024| <<handle_invalid_guest_state>> WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx.c|10205| <<nested_vmx_exit_reflected>> if (vmx->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx.c|12123| <<vmx_inject_page_fault_nested>> !to_vmx(vcpu)->nested.nested_run_pending) {
+	 *   - arch/x86/kvm/vmx.c|12793| <<prepare_vmcs02_full>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|12860| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|12868| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending) {
+	 *   - arch/x86/kvm/vmx.c|13005| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|13076| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|13309| <<check_vmentry_postreqs>> if (to_vmx(vcpu)->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|13648| <<vmx_check_nested_events>> vmx->nested.nested_run_pending || kvm_event_needs_reinjection(vcpu);
+	 *   - arch/x86/kvm/vmx.c|14054| <<nested_vmx_vmexit>> WARN_ON_ONCE(vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx.c|14680| <<vmx_smi_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx.c|14766| <<vmx_get_nested_state>> if (vmx->nested.nested_run_pending)
+	 */
 	bool nested_run_pending;
 
 	struct loaded_vmcs vmcs02;
@@ -992,8 +1205,33 @@ struct vcpu_vmx {
 	 * value indicates that host state is loaded.
 	 */
 	struct loaded_vmcs    vmcs01;
+	/*
+	 * 大概只在以下修改vcpu_vmx->loaded_vmcs:
+	 *   - arch/x86/kvm/vmx.c|11675| <<vmx_switch_vmcs>> vmx->loaded_vmcs = vmcs;
+	 *   - arch/x86/kvm/vmx.c|11770| <<vmx_create_vcpu>> vmx->loaded_vmcs = &vmx->vmcs01;
+	 */
 	struct loaded_vmcs   *loaded_vmcs;
+	/*
+	 * 在以下使用vcpu_vmx->loaded_cpu_state:
+	 *   - arch/x86/kvm/vmx.c|3148| <<vmx_prepare_switch_to_guest>> if (!vmx->loaded_cpu_state || vmx->guest_msrs_dirty) {
+	 *   - arch/x86/kvm/vmx.c|3156| <<vmx_prepare_switch_to_guest>> if (vmx->loaded_cpu_state)
+	 *   - arch/x86/kvm/vmx.c|3159| <<vmx_prepare_switch_to_guest>> vmx->loaded_cpu_state = vmx->loaded_vmcs;
+	 *   - arch/x86/kvm/vmx.c|3160| <<vmx_prepare_switch_to_guest>> host_state = &vmx->loaded_cpu_state->host_state;
+	 *   - arch/x86/kvm/vmx.c|3222| <<vmx_prepare_switch_to_host>> if (!vmx->loaded_cpu_state)
+	 *   - arch/x86/kvm/vmx.c|3225| <<vmx_prepare_switch_to_host>> WARN_ON_ONCE(vmx->loaded_cpu_state != vmx->loaded_vmcs);
+	 *   - arch/x86/kvm/vmx.c|3226| <<vmx_prepare_switch_to_host>> host_state = &vmx->loaded_cpu_state->host_state;
+	 *   - arch/x86/kvm/vmx.c|3229| <<vmx_prepare_switch_to_host>> vmx->loaded_cpu_state = NULL;
+	 *   - arch/x86/kvm/vmx.c|3261| <<vmx_read_guest_kernel_gs_base>> if (vmx->loaded_cpu_state)
+	 *   - arch/x86/kvm/vmx.c|3270| <<vmx_write_guest_kernel_gs_base>> if (vmx->loaded_cpu_state)
+	 */
 	struct loaded_vmcs   *loaded_cpu_state;
+	/*
+	 * 在以下使用vcpu_vmx->__launched:
+	 *   - arch/x86/kvm/vmx.c|11027| <<vmx_vcpu_run>> vmx->__launched = vmx->loaded_vmcs->launched;
+	 *   - arch/x86/kvm/vmx.c|11130| <<vmx_vcpu_run>> [launched]"i"(offsetof(struct vcpu_vmx, __launched)),
+	 *
+	 * Check if vmlaunch of vmresume is needed
+	 */
 	bool                  __launched; /* temporary, used in vmx_vcpu_run */
 	struct msr_autoload {
 		struct vmx_msrs guest;
@@ -1015,6 +1253,19 @@ struct vcpu_vmx {
 		} seg[8];
 	} segment_cache;
 	int vpid;
+	/*
+	 * 在以下使用vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx.c|3354| <<vmx_set_rflags>> to_vmx(vcpu)->emulation_required = emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx.c|3498| <<vmx_queue_exception>> WARN_ON_ONCE(vmx->emulation_required);
+	 *   - arch/x86/kvm/vmx.c|5462| <<vmx_set_cr0>> vmx->emulation_required = emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx.c|5709| <<vmx_set_segment>> vmx->emulation_required = emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx.c|7948| <<handle_invalid_guest_state>> WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx.c|7953| <<handle_invalid_guest_state>> while (vmx->emulation_required && count-- != 0) {
+	 *   - arch/x86/kvm/vmx.c|7971| <<handle_invalid_guest_state>> if (vmx->emulation_required && !vmx->rmode.vm86_active &&
+	 *   - arch/x86/kvm/vmx.c|10387| <<vmx_handle_exit>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx.c|11007| <<vmx_vcpu_run>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx.c|12789| <<prepare_vmcs02>> if (vmx->emulation_required) {
+	 */
 	bool emulation_required;
 
 	u32 exit_reason;
@@ -1268,13 +1519,41 @@ static inline short vmcs_field_to_offset(unsigned long field)
 	return offset;
 }
 
+/*
+ * 返回to_vmx(vcpu)->nested.cached_vmcs12;
+ *
+ * struct vcpu_vmx:
+ *  -> struct nested_vmx nested;
+ *      -> struct vmcs12 *cached_vmcs12;
+ *      -> struct vmcs12 *cached_shadow_vmcs12;
+ */
 static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
+/*
+ * 返回to_vmx(vcpu)->nested.cached_shadow_vmcs12;
+ *
+ * struct vcpu_vmx:
+ *  -> struct nested_vmx nested;
+ *      -> struct vmcs12 *cached_vmcs12;
+ *      -> struct vmcs12 *cached_shadow_vmcs12;
+ */
 static inline struct vmcs12 *get_shadow_vmcs12(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	return to_vmx(vcpu)->nested.cached_shadow_vmcs12;
 }
 
@@ -1341,6 +1620,43 @@ static bool cpu_has_load_perf_global_ctrl;
 static DECLARE_BITMAP(vmx_vpid_bitmap, VMX_NR_VPIDS);
 static DEFINE_SPINLOCK(vmx_vpid_lock);
 
+/*
+ * crash> vmcs_config
+ * vmcs_config = $1 = {
+ *   size = 1024,
+ *   order = 0,
+ *   basic_cap = 14286848,
+ *   revision_id = 4,
+ *   pin_based_exec_ctrl = 255,
+ *   cpu_based_exec_ctrl = 3047190010,
+ *   cpu_based_2nd_exec_ctrl = 34832383,
+ *   vmexit_ctrl = 9433087,
+ *   vmentry_ctrl = 86527,
+ *   nested = {
+ *     procbased_ctls_low = 67133810,
+ *     procbased_ctls_high = 4294574078,
+ *     secondary_ctls_low = 0,
+ *     secondary_ctls_high = 156663,
+ *     pinbased_ctls_low = 22,
+ *     pinbased_ctls_high = 255,
+ *     exit_ctls_low = 224763,
+ *     exit_ctls_high = 8384511,
+ *     entry_ctls_low = 4603,
+ *     entry_ctls_high = 54271,
+ *     misc_low = 536871013,
+ *     misc_high = 0,
+ *     ept_caps = 104022081,
+ *     vpid_caps = 3841,
+ *     basic = 60816187455798992,
+ *     cr0_fixed0 = 2147483681,
+ *     cr0_fixed1 = 4294967295,
+ *     cr4_fixed0 = 8192,
+ *     cr4_fixed1 = 7825407,
+ *     vmcs_enum = 46,
+ *     vmfunc_controls = 1
+ *   }
+ * }
+ */
 static struct vmcs_config {
 	int size;
 	int order;
@@ -2178,6 +2494,14 @@ static inline void loaded_vmcs_init(struct loaded_vmcs *loaded_vmcs)
 	loaded_vmcs->launched = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|3142| <<vmx_vcpu_load>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx.c|8747| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx.c|8763| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx.c|8783| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx.c|8794| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ */
 static void vmcs_load(struct vmcs *vmcs)
 {
 	u64 phys_addr = __pa(vmcs);
@@ -2186,6 +2510,18 @@ static void vmcs_load(struct vmcs *vmcs)
 	if (static_branch_unlikely(&enable_evmcs))
 		return evmcs_load(phys_addr);
 
+	/*
+	 * The behavior of the VMCS-maintenance instructions is summarized below:
+	 * VMPTRLD — This instruction takes a single 64-bit source operand that is in memory. It makes the referenced
+	 * VMCS active and current, loading the current-VMCS pointer with this operand and establishes the current VMCS
+	 * based on the contents of VMCS-data area in the referenced VMCS region. Because this makes the referenced
+	 * VMCS active, a logical processor may start maintaining on the processor some of the VMCS data for the VMCS.
+	 *
+	 * VMPTRLD指令从内存中加载一个64位物理地址作为current-VMCS pointer,
+	 * 这个current-VMCS pointer由处理器内部记录和维护,除了VMXON,VMPTRLD
+	 * 和VMCLEAR指令需要提供VMXON或VMCS指针作为操作数外,其他的指令指定
+	 * 都是在current-VMS上操作.
+	 */
 	asm volatile (__ex(ASM_VMX_VMPTRLD_RAX) CC_SET(na)
 		      : CC_OUT(na) (error) : "a"(&phys_addr), "m"(phys_addr)
 		      : "memory");
@@ -3048,6 +3384,10 @@ static void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|3455| <<vmx_vcpu_load>> vmx_vcpu_pi_load(vcpu, cpu);
+ */
 static void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -3114,8 +3454,18 @@ static void decache_tsc_multiplier(struct vcpu_vmx *vmx)
  * Switches to specified vcpu, until a matching vcpu_put(), but assumes
  * vcpu mutex is already taken.
  */
+/*
+ * 在以下使用vmx_vcpu_load():
+ *   - struct kvm_x86_ops vmx_x86_ops.vcpu_load = vmx_vcpu_load()
+ *   - arch/x86/kvm/vmx.c|11100| <<vmx_switch_vmcs>> vmx_vcpu_load(vcpu, cpu);
+ *   - arch/x86/kvm/vmx.c|11183| <<vmx_create_vcpu>> vmx_vcpu_load(&vmx->vcpu, cpu);
+ */
 static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
+	/*
+	 * struct vcpu_vmx:
+	 *  -> struct kvm_vcpu       vcpu;
+	 */
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	bool already_loaded = vmx->loaded_vmcs->cpu == cpu;
 
@@ -4220,6 +4570,17 @@ static void vmx_leave_nested(struct kvm_vcpu *vcpu);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s.c|736| <<kvmppc_set_msr>> vcpu->kvm->arch.kvm_ops->set_msr(vcpu, msr);
+ *   - arch/x86/kvm/emulate.c|2521| <<rsm_load_state_64>> ctxt->ops->set_msr(ctxt, MSR_EFER, val & ~EFER_LMA);
+ *   - arch/x86/kvm/emulate.c|2613| <<em_rsm>> ctxt->ops->set_msr(ctxt, MSR_EFER, efer);
+ *   - arch/x86/kvm/emulate.c|3661| <<em_wrmsr>> if (ctxt->ops->set_msr(ctxt, reg_read(ctxt, VCPU_REGS_RCX), msr_data))
+ *   - arch/x86/kvm/pmu.c|320| <<kvm_pmu_set_msr>> return kvm_x86_ops->pmu_ops->set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|1261| <<kvm_set_msr>> return kvm_x86_ops->set_msr(vcpu, msr);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.set_msr = vmx_set_msr()
+ */
 static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4775,24 +5136,96 @@ static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|5025| <<alloc_vmcs>> return alloc_vmcs_cpu(shadow, raw_smp_processor_id());
+ *   - arch/x86/kvm/vmx.c|5169| <<alloc_kvm_area>> vmcs = alloc_vmcs_cpu(false, cpu);
+ */
 static struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu)
 {
 	int node = cpu_to_node(cpu);
 	struct page *pages;
 	struct vmcs *vmcs;
 
+	/*
+	 * struct vmcs {
+	 *     struct vmcs_hdr hdr;
+	 *     u32 abort;
+	 *     char data[0];
+	 * };
+	 */
 	pages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);
 	if (!pages)
 		return NULL;
 	vmcs = page_address(pages);
+	/*
+	 * crash> vmcs_config
+	 * vmcs_config = $1 = {
+	 *   size = 1024,
+	 *   order = 0,
+	 *   basic_cap = 14286848,
+	 *   revision_id = 4,
+	 *   pin_based_exec_ctrl = 255,
+	 *   cpu_based_exec_ctrl = 3047190010,
+	 *   cpu_based_2nd_exec_ctrl = 34832383,
+	 *   vmexit_ctrl = 9433087,
+	 *   vmentry_ctrl = 86527,
+	 *   nested = {
+	 *     procbased_ctls_low = 67133810,
+	 *     procbased_ctls_high = 4294574078,
+	 *     secondary_ctls_low = 0,
+	 *     secondary_ctls_high = 156663,
+	 *     pinbased_ctls_low = 22,
+	 *     pinbased_ctls_high = 255,
+	 *     exit_ctls_low = 224763,
+	 *     exit_ctls_high = 8384511,
+	 *     entry_ctls_low = 4603,
+	 *     entry_ctls_high = 54271,
+	 *     misc_low = 536871013,
+	 *     misc_high = 0,
+	 *     ept_caps = 104022081,
+	 *     vpid_caps = 3841,
+	 *     basic = 60816187455798992,
+	 *     cr0_fixed0 = 2147483681,
+	 *     cr0_fixed1 = 4294967295,
+	 *     cr4_fixed0 = 8192,
+	 *     cr4_fixed1 = 7825407,
+	 *     vmcs_enum = 46,
+	 *     vmfunc_controls = 1
+	 *   }
+	 * }
+	 */
 	memset(vmcs, 0, vmcs_config.size);
 
 	/* KVM supports Enlightened VMCS v1 only */
+	/*
+	 * crash> enable_evmcs
+	 * enable_evmcs = $2 = {
+	 *   key = {
+	 *     enabled = {
+	 *       counter = 0
+	 *     },
+	 *     {
+	 *       type = 18446635581865343170,
+	 *       entries = 0xffff9d53c62a38c2,
+	 *       next = 0xffff9d53c62a38c2
+	 *     }
+	 *   }
+	 * }
+	 */
 	if (static_branch_unlikely(&enable_evmcs))
 		vmcs->hdr.revision_id = KVM_EVMCS_VERSION;
 	else
 		vmcs->hdr.revision_id = vmcs_config.revision_id;
 
+	/*
+	 * 在以下使用vmcs_hdr->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|4999| <<alloc_vmcs_cpu>> vmcs->hdr.shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx.c|13341| <<nested_vmx_check_vmcs_link_ptr>> shadow->hdr.shadow_vmcs != nested_cpu_has_shadow_vmcs(vmcs12))
+	 *   - arch/x86/kvm/vmx.c|9481| <<handle_vmptrld>> (new_vmcs12->hdr.shadow_vmcs &&
+	 *   - arch/x86/kvm/vmx.c|13540| <<nested_vmx_run>> if (vmcs12->hdr.shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|15014| <<vmx_set_nested_state>> !shadow_vmcs12->hdr.shadow_vmcs)
+	 */
 	if (shadow)
 		vmcs->hdr.shadow_vmcs = 1;
 	return vmcs;
@@ -4818,11 +5251,21 @@ static void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 	WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|5035| <<alloc_loaded_vmcs>> loaded_vmcs->vmcs = alloc_vmcs(false);
+ *   - arch/x86/kvm/vmx.c|8605| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+ */
 static struct vmcs *alloc_vmcs(bool shadow)
 {
 	return alloc_vmcs_cpu(shadow, raw_smp_processor_id());
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8422| <<enter_vmx_operation>> r = alloc_loaded_vmcs(&vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx.c|11277| <<vmx_create_vcpu>> err = alloc_loaded_vmcs(&vmx->vmcs01);
+ */
 static int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)
 {
 	loaded_vmcs->vmcs = alloc_vmcs(false);
@@ -4952,6 +5395,10 @@ static void init_vmcs_shadow_fields(void)
 	max_shadow_read_write_fields = j;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8311| <<hardware_setup>> return alloc_kvm_area();
+ */
 static __init int alloc_kvm_area(void)
 {
 	int cpu;
@@ -7234,6 +7681,25 @@ static int handle_triple_fault(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * [0] eventfd_signal
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_vmx_exit_handlers[EXIT_REASON_IO_INSTRUCTION] = handle_io()
+ */
 static int handle_io(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
@@ -8153,6 +8619,19 @@ static int handle_monitor(struct kvm_vcpu *vcpu)
  * set the success or error code of an emulated VMX instruction, as specified
  * by Vol 2B, VMX Instruction Reference, "Conventions".
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8665| <<handle_vmon>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|8806| <<handle_vmoff>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|8840| <<handle_vmclear>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|9122| <<handle_vmread>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|9218| <<handle_vmwrite>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|9322| <<handle_vmptrld>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|9346| <<handle_vmptrst>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|9403| <<handle_invept>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|9495| <<handle_invvpid>> nested_vmx_succeed(vcpu);
+ *   - arch/x86/kvm/vmx.c|14068| <<nested_vmx_entry_failure>> nested_vmx_succeed(vcpu);
+ */
 static void nested_vmx_succeed(struct kvm_vcpu *vcpu)
 {
 	vmx_set_rflags(vcpu, vmx_get_rflags(vcpu)
@@ -8168,6 +8647,27 @@ static void nested_vmx_failInvalid(struct kvm_vcpu *vcpu)
 			| X86_EFLAGS_CF);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8552| <<handle_vmon>> nested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
+ *   - arch/x86/kvm/vmx.c|8736| <<handle_vmclear>> nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);
+ *   - arch/x86/kvm/vmx.c|8741| <<handle_vmclear>> nested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);
+ *   - arch/x86/kvm/vmx.c|9014| <<handle_vmread>> nested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
+ *   - arch/x86/kvm/vmx.c|9085| <<handle_vmwrite>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|9106| <<handle_vmwrite>> nested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
+ *   - arch/x86/kvm/vmx.c|9193| <<handle_vmptrld>> nested_vmx_failValid(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);
+ *   - arch/x86/kvm/vmx.c|9198| <<handle_vmptrld>> nested_vmx_failValid(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);
+ *   - arch/x86/kvm/vmx.c|9216| <<handle_vmptrld>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|9290| <<handle_invept>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|9354| <<handle_invvpid>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|9370| <<handle_invvpid>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|9379| <<handle_invvpid>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|9393| <<handle_invvpid>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|13185| <<nested_vmx_run>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|13191| <<nested_vmx_run>> nested_vmx_failValid(vcpu,
+ *   - arch/x86/kvm/vmx.c|13199| <<nested_vmx_run>> nested_vmx_failValid(vcpu, ret);
+ *   - arch/x86/kvm/vmx.c|13868| <<nested_vmx_vmexit>> nested_vmx_failValid(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
+ */
 static void nested_vmx_failValid(struct kvm_vcpu *vcpu,
 					u32 vm_instruction_error)
 {
@@ -8328,9 +8828,18 @@ static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer)
  * VMCS, unless such a shadow VMCS already exists. The newly allocated
  * VMCS is also VMCLEARed, so that it is ready for use.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8642| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+ */
 static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * 大概只在以下修改vcpu_vmx->loaded_vmcs:
+	 *   - arch/x86/kvm/vmx.c|11675| <<vmx_switch_vmcs>> vmx->loaded_vmcs = vmcs;
+	 *   - arch/x86/kvm/vmx.c|11770| <<vmx_create_vcpu>> vmx->loaded_vmcs = &vmx->vmcs01;
+	 */
 	struct loaded_vmcs *loaded_vmcs = vmx->loaded_vmcs;
 
 	/*
@@ -8341,6 +8850,26 @@ static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 	 */
 	WARN_ON(loaded_vmcs == &vmx->vmcs01 && loaded_vmcs->shadow_vmcs);
 
+	/*
+	 * 在以下修改loaded_vmcs->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|4878| <<alloc_loaded_vmcs>> loaded_vmcs->shadow_vmcs = NULL;
+	 *   - arch/x86/kvm/vmx.c|8410| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+	 *   - arch/x86/kvm/vmx.c|8627| <<free_nested>> vmx->vmcs01.shadow_vmcs = NULL;
+	 * 在以下使用loaded_vmcs->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|2203| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx.c|2204| <<loaded_vmcs_init>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|4864| <<free_loaded_vmcs>> WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
+	 *   - arch/x86/kvm/vmx.c|8407| <<alloc_shadow_vmcs>> WARN_ON(loaded_vmcs == &vmx->vmcs01 && loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8409| <<alloc_shadow_vmcs>> if (!loaded_vmcs->shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|8411| <<alloc_shadow_vmcs>> if (loaded_vmcs->shadow_vmcs)
+	 *   - arch/x86/kvm/vmx.c|8412| <<alloc_shadow_vmcs>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8414| <<alloc_shadow_vmcs>> return loaded_vmcs->shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|8625| <<free_nested>> vmcs_clear(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8626| <<free_nested>> free_vmcs(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8807| <<copy_shadow_to_vmcs12>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|8864| <<copy_vmcs12_to_shadow>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|9062| <<set_current_vmptr>> __pa(vmx->vmcs01.shadow_vmcs));
+	 */
 	if (!loaded_vmcs->shadow_vmcs) {
 		loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
 		if (loaded_vmcs->shadow_vmcs)
@@ -8349,6 +8878,19 @@ static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 	return loaded_vmcs->shadow_vmcs;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8645| <<handle_vmon>> ret = enter_vmx_operation(vcpu);
+ *   - arch/x86/kvm/vmx.c|14641| <<vmx_set_nested_state>> ret = enter_vmx_operation(vcpu);
+ *
+ * 会修改nested_vmx->vmxon为true
+ * 在以下修改nested_vmx->vmxon:
+ *   - arch/x86/kvm/vmx.c|8566| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+ *   - arch/x86/kvm/vmx.c|8751| <<free_nested>> vmx->nested.vmxon = false;
+ *   - arch/x86/kvm/vmx.c|14524| <<vmx_pre_enter_smm>> vmx->nested.vmxon = false;
+ *   - arch/x86/kvm/vmx.c|14535| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true;
+ *   - arch/x86/kvm/vmx.c|14716| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+ */
 static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8375,6 +8917,14 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 
 	vmx->nested.vpid02 = allocate_vpid();
 
+	/*
+	 * 在以下修改nested_vmx->vmxon:
+	 *   - arch/x86/kvm/vmx.c|8566| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx.c|8751| <<free_nested>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx.c|14524| <<vmx_pre_enter_smm>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx.c|14535| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx.c|14716| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+	 */
 	vmx->nested.vmxon = true;
 	return 0;
 
@@ -8469,7 +9019,28 @@ static int handle_vmon(struct kvm_vcpu *vcpu)
 	kunmap(page);
 	kvm_release_page_clean(page);
 
+	/*
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx.c|8660| <<handle_vmon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|14699| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->vmx.vmxon_pa;
+	 *   - arch/x86/kvm/vmx.c|8828| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr) {
+	 *   - arch/x86/kvm/vmx.c|9285| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr) {
+	 *   - arch/x86/kvm/vmx.c|14575| <<vmx_get_nested_state>> kvm_state.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 */
 	vmx->nested.vmxon_ptr = vmptr;
+	/*
+	 * 在以下调用enter_vmx_operation():
+	 *   - arch/x86/kvm/vmx.c|8645| <<handle_vmon>> ret = enter_vmx_operation(vcpu);
+	 *   - arch/x86/kvm/vmx.c|14641| <<vmx_set_nested_state>> ret = enter_vmx_operation(vcpu);
+	 *
+	 * 会修改nested_vmx->vmxon为true
+	 * 在以下修改nested_vmx->vmxon:
+	 *   - arch/x86/kvm/vmx.c|8566| <<enter_vmx_operation>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx.c|8751| <<free_nested>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx.c|14524| <<vmx_pre_enter_smm>> vmx->nested.vmxon = false;
+	 *   - arch/x86/kvm/vmx.c|14535| <<vmx_pre_leave_smm>> vmx->nested.vmxon = true;
+	 *   - arch/x86/kvm/vmx.c|14716| <<vmx_set_nested_state>> vmx->nested.vmxon = false;
+	 */
 	ret = enter_vmx_operation(vcpu);
 	if (ret)
 		return ret;
@@ -8500,16 +9071,33 @@ static int nested_vmx_check_permission(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8714| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+ *   - arch/x86/kvm/vmx.c|8752| <<free_nested>> vmx_disable_shadow_vmcs(vmx); 
+ */
 static void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)
 {
 	vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL, SECONDARY_EXEC_SHADOW_VMCS);
 	vmcs_write64(VMCS_LINK_POINTER, -1ull);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8735| <<handle_vmclear>> nested_release_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx.c|9177| <<handle_vmptrld>> nested_release_vmcs12(vcpu);
+ */
 static inline void nested_release_vmcs12(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
+	/*
+	 * 在以下修改nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx.c|8657| <<nested_release_vmcs12>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|8682| <<free_nested>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|9141| <<set_current_vmptr>> vmx->nested.current_vmptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|11494| <<vmx_create_vcpu>> vmx->nested.current_vmptr = -1ull;
+	 */
 	if (vmx->nested.current_vmptr == -1ull)
 		return;
 
@@ -8517,6 +9105,16 @@ static inline void nested_release_vmcs12(struct kvm_vcpu *vcpu)
 		/* copy to memory all shadowed fields in case
 		   they were modified */
 		copy_shadow_to_vmcs12(vmx);
+		/*
+		 * 在以下使用和修改nested_vmx->sync_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx.c|8610| <<nested_release_vmcs12>> vmx->nested.sync_shadow_vmcs = false;
+		 *   - arch/x86/kvm/vmx.c|9107| <<set_current_vmptr>> vmx->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|10954| <<vmx_vcpu_run>> vmx->nested.sync_shadow_vmcs = false;
+		 *   - arch/x86/kvm/vmx.c|13720| <<nested_vmx_vmexit>> vmx->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|13810| <<nested_vmx_entry_failure>> to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|10952| <<vmx_vcpu_run>> if (vmx->nested.sync_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx.c|14359| <<vmx_get_nested_state>> else if (enable_shadow_vmcs && !vmx->nested.sync_shadow_vmcs)
+		 */
 		vmx->nested.sync_shadow_vmcs = false;
 		vmx_disable_shadow_vmcs(vmx);
 	}
@@ -8536,6 +9134,27 @@ static inline void nested_release_vmcs12(struct kvm_vcpu *vcpu)
  * Free whatever needs to be freed from vmx->nested when L1 goes down, or
  * just stops using VMX.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8623| <<handle_vmoff>> free_nested(vcpu);
+ *   - arch/x86/kvm/vmx.c|11144| <<vmx_free_vcpu_nested>> free_nested(vcpu);
+ *   - arch/x86/kvm/vmx.c|13615| <<vmx_leave_nested>> free_nested(vcpu);
+ *
+ * 例子1:
+ * handle_vmoff()
+ * -> free_nested()
+ *
+ * 例子2: 对于x86.
+ * kvm_arch_vcpu_free() or kvm_arch_vcpu_destroy()
+ * -> kvm_x86_ops->vcpu_free = vmx_free_vcpu()
+ *    -> vmx_free_vcpu_nested()
+ *       -> free_nested()
+ *
+ * 例子3:
+ * vmx_set_nested_state() or vmx_set_msr()
+ * -> vmx_leave_nested()
+ *    -> free_nested()
+ */
 static void free_nested(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8553,6 +9172,21 @@ static void free_nested(struct kvm_vcpu *vcpu)
 		vmx_disable_shadow_vmcs(vmx);
 		vmcs_clear(vmx->vmcs01.shadow_vmcs);
 		free_vmcs(vmx->vmcs01.shadow_vmcs);
+		/*
+		 * 在以下使用和修改nested_vmx->sync_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx.c|9107| <<set_current_vmptr>> vmx->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|13720| <<nested_vmx_vmexit>> vmx->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|13810| <<nested_vmx_entry_failure>> to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|10954| <<vmx_vcpu_run>> vmx->nested.sync_shadow_vmcs = false;
+		 *   - arch/x86/kvm/vmx.c|8610| <<nested_release_vmcs12>> vmx->nested.sync_shadow_vmcs = false;
+		 *   - arch/x86/kvm/vmx.c|10952| <<vmx_vcpu_run>> if (vmx->nested.sync_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx.c|14359| <<vmx_get_nested_state>> else if (enable_shadow_vmcs && !vmx->nested.sync_shadow_vmcs)
+		 *
+		 * 在以下修改loaded_vmcs->shadow_vmcs:
+		 *   - arch/x86/kvm/vmx.c|4878| <<alloc_loaded_vmcs>> loaded_vmcs->shadow_vmcs = NULL;
+		 *   - arch/x86/kvm/vmx.c|8410| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+		 *   - arch/x86/kvm/vmx.c|8627| <<free_nested>> vmx->vmcs01.shadow_vmcs = NULL;
+		 */
 		vmx->vmcs01.shadow_vmcs = NULL;
 	}
 	kfree(vmx->nested.cached_vmcs12);
@@ -8583,6 +9217,12 @@ static int handle_vmoff(struct kvm_vcpu *vcpu)
 {
 	if (!nested_vmx_check_permission(vcpu))
 		return 1;
+	/*
+	 * 在以下调用free_nested():
+	 *   - arch/x86/kvm/vmx.c|8623| <<handle_vmoff>> free_nested(vcpu);
+	 *   - arch/x86/kvm/vmx.c|11144| <<vmx_free_vcpu_nested>> free_nested(vcpu);
+	 *   - arch/x86/kvm/vmx.c|13615| <<vmx_leave_nested>> free_nested(vcpu);
+	 */
 	free_nested(vcpu);
 	nested_vmx_succeed(vcpu);
 	return kvm_skip_emulated_instruction(vcpu);
@@ -8611,6 +9251,17 @@ static int handle_vmclear(struct kvm_vcpu *vcpu)
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * 在以下修改nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx.c|8657| <<nested_release_vmcs12>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|8682| <<free_nested>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|9141| <<set_current_vmptr>> vmx->nested.current_vmptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|11494| <<vmx_create_vcpu>> vmx->nested.current_vmptr = -1ull;
+	 *
+	 * 在以下调用nested_release_vmcs12():
+	 *   - arch/x86/kvm/vmx.c|8735| <<handle_vmclear>> nested_release_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx.c|9177| <<handle_vmptrld>> nested_release_vmcs12(vcpu);
+	 */
 	if (vmptr == vmx->nested.current_vmptr)
 		nested_release_vmcs12(vcpu);
 
@@ -8708,6 +9359,15 @@ static inline int vmcs12_write_any(struct vmcs12 *vmcs12,
  * VM-exit information fields are actually writable if the vCPU is
  * configured to support "VMWRITE to any supported field in the VMCS."
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8556| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+ *   - arch/x86/kvm/vmx.c|12903| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+ *   - arch/x86/kvm/vmx.c|14190| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+ *
+ * 把vmx->vmcs01.shadow_vmcs(类型vmcs)的数据同步到vmx->nested.cached_vmcs12(类型vmcs12)
+ * 这里的shadow vmcs是L1中用来支持L2的vmcs
+ */
 static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 {
 	const u16 *fields[] = {
@@ -8721,6 +9381,12 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 	int i, q;
 	unsigned long field;
 	u64 field_value;
+	/*
+	 * struct vcpu_vmx *vmx:
+	 *  -> struct loaded_vmcs vmcs01;
+	 *      -> struct vmcs *vmcs;
+	 *      -> struct vmcs *shadow_vmcs;
+	 */
 	struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
 
 	preempt_disable();
@@ -8731,6 +9397,14 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 		for (i = 0; i < max_fields[q]; i++) {
 			field = fields[q][i];
 			field_value = __vmcs_readl(field);
+			/*
+			 * get_vmcs12()返回to_vmx(vcpu)->nested.cached_vmcs12;
+			 *
+			 * struct vcpu_vmx:
+			 *  -> struct nested_vmx nested;
+			 *      -> struct vmcs12 *cached_vmcs12;
+			 *      -> struct vmcs12 *cached_shadow_vmcs12;
+			 */
 			vmcs12_write_any(get_vmcs12(&vmx->vcpu), field, field_value);
 		}
 		/*
@@ -8746,6 +9420,15 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|10825| <<vmx_vcpu_run>> copy_vmcs12_to_shadow(vmx);
+ *
+ * 把vmx->nested.cached_vmcs12(类型vmcs12)给同步到vmx->vmcs01.shadow_vmcs(类型vmcs)
+ * 这里为了在L1修改针对L2的vmcs时不用trap到L0了
+ *
+ * 用set_current_vmptr()把shadow_vmcs给硬件
+ */
 static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 {
 	const u16 *fields[] = {
@@ -8759,6 +9442,32 @@ static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 	int i, q;
 	unsigned long field;
 	u64 field_value = 0;
+	/*
+	 * 在以下修改loaded_vmcs->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|4878| <<alloc_loaded_vmcs>> loaded_vmcs->shadow_vmcs = NULL;
+	 *   - arch/x86/kvm/vmx.c|8410| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+	 *   - arch/x86/kvm/vmx.c|8627| <<free_nested>> vmx->vmcs01.shadow_vmcs = NULL;
+	 * 在以下使用loaded_vmcs->shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|2203| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx.c|2204| <<loaded_vmcs_init>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|4864| <<free_loaded_vmcs>> WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
+	 *   - arch/x86/kvm/vmx.c|8407| <<alloc_shadow_vmcs>> WARN_ON(loaded_vmcs == &vmx->vmcs01 && loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8409| <<alloc_shadow_vmcs>> if (!loaded_vmcs->shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|8411| <<alloc_shadow_vmcs>> if (loaded_vmcs->shadow_vmcs)
+	 *   - arch/x86/kvm/vmx.c|8412| <<alloc_shadow_vmcs>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8414| <<alloc_shadow_vmcs>> return loaded_vmcs->shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|8625| <<free_nested>> vmcs_clear(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8626| <<free_nested>> free_vmcs(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx.c|8807| <<copy_shadow_to_vmcs12>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|8864| <<copy_vmcs12_to_shadow>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+	 *   - arch/x86/kvm/vmx.c|9062| <<set_current_vmptr>> __pa(vmx->vmcs01.shadow_vmcs));
+	 *
+	 * struct vcpu_vmx:
+	 *   -> struct loaded_vmcs    vmcs01;
+	 *       -> struct vmcs *vmcs;
+	 *       -> struct vmcs *shadow_vmcs;
+	 *   -> struct loaded_vmcs   *loaded_vmcs;
+	 */
 	struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
 
 	vmcs_load(shadow_vmcs);
@@ -8766,7 +9475,20 @@ static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 	for (q = 0; q < ARRAY_SIZE(fields); q++) {
 		for (i = 0; i < max_fields[q]; i++) {
 			field = fields[q][i];
+			/*
+			 * get_vmcs12()返回to_vmx(vcpu)->nested.cached_vmcs12;
+			 *
+			 * struct vcpu_vmx:
+			 *  -> struct nested_vmx nested;
+			 *      -> struct vmcs12 *cached_vmcs12;
+			 *      -> struct vmcs12 *cached_shadow_vmcs12;
+			 */
 			vmcs12_read_any(get_vmcs12(&vmx->vcpu), field, &field_value);
+			/*
+			 * 如果vmx->vmcs01.shadow_vmcs是NULL
+			 * 这里就相当于是写入了正在要使用的vmcs
+			 * 神仙才知道会发生什么!
+			 */
 			__vmcs_writel(field, field_value);
 		}
 	}
@@ -8804,6 +9526,14 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 	if (!nested_vmx_check_vmcs12(vcpu))
 		return kvm_skip_emulated_instruction(vcpu);
 
+	/*
+	 * 返回to_vmx(vcpu)->nested.cached_vmcs12;
+	 *
+	 * struct vcpu_vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	if (!is_guest_mode(vcpu))
 		vmcs12 = get_vmcs12(vcpu);
 	else {
@@ -8815,6 +9545,14 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 			nested_vmx_failInvalid(vcpu);
 			return kvm_skip_emulated_instruction(vcpu);
 		}
+		/*
+		 * 返回to_vmx(vcpu)->nested.cached_shadow_vmcs12;
+		 *
+		 * struct vcpu_vmx:
+		 *  -> struct nested_vmx nested;
+		 *      -> struct vmcs12 *cached_vmcs12;
+		 *      -> struct vmcs12 *cached_shadow_vmcs12;
+		 */
 		vmcs12 = get_shadow_vmcs12(vcpu);
 	}
 
@@ -8942,14 +9680,61 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|9192| <<handle_vmptrld>> set_current_vmptr(vmx, vmptr);
+ *   - arch/x86/kvm/vmx.c|14518| <<vmx_set_nested_state>> set_current_vmptr(vmx, kvm_state->vmx.vmcs_pa);
+ *
+ * 最重要的是把__pa(vmx->vmcs01.shadow_vmcs)写入L0上的vmcs的VMCS_LINK_POINTER
+ * 如果是针对L1的L0上的vmcs, 用vmptrld指令写入吧?
+ */
 static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 {
 	vmx->nested.current_vmptr = vmptr;
 	if (enable_shadow_vmcs) {
+		/*
+		 * If this control is 1, executions of VMREAD and VMWRITE in VMX non-root operation
+		 * may access a shadow VMCS (instead of causing VM exits).
+		 */
 		vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
 			      SECONDARY_EXEC_SHADOW_VMCS);
+		/*
+		 * 在以下使用VMCS_LINK_POINTER (L0设置vmcs的地址直接vmptrld指令吧?只有L1的shadow才用这里):
+		 *   - arch/x86/kvm/vmx.c|1396| <<global>> FIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),
+		 *   - arch/x86/kvm/vmx_evmcs.h|71| <<global>> EVMCS1_FIELD(VMCS_LINK_POINTER, vmcs_link_pointer,
+		 *   - tools/testing/selftests/kvm/include/vmx.h|217| <<global>> VMCS_LINK_POINTER = 0x00002800,
+		 *   - arch/x86/kvm/vmx.c|7098| <<vmx_vcpu_setup>> vmcs_write64(VMCS_LINK_POINTER, -1ull);
+		 *   - arch/x86/kvm/vmx.c|9080| <<vmx_disable_shadow_vmcs>> vmcs_write64(VMCS_LINK_POINTER, -1ull);
+		 *   - arch/x86/kvm/vmx.c|9694| <<set_current_vmptr>> vmcs_write64(VMCS_LINK_POINTER,
+		 *   - arch/x86/kvm/vmx.c|13120| <<prepare_vmcs02_full>> vmcs_write64(VMCS_LINK_POINTER, -1ull);
+		 *   - tools/testing/selftests/kvm/lib/vmx.c|222| <<init_vmcs_guest_state>> vmwrite(VMCS_LINK_POINTER, -1ll);
+		 *   - tools/testing/selftests/kvm/state_test.c|84| <<l1_guest_code>> vmwrite(VMCS_LINK_POINTER, vmx_pages->shadow_vmcs_gpa);
+		 *
+		 * 在以下修改loaded_vmcs->shadow_vmcs:
+		 *   - arch/x86/kvm/vmx.c|4878| <<alloc_loaded_vmcs>> loaded_vmcs->shadow_vmcs = NULL;
+		 *   - arch/x86/kvm/vmx.c|8410| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+		 *   - arch/x86/kvm/vmx.c|8627| <<free_nested>> vmx->vmcs01.shadow_vmcs = NULL;
+		 * 在以下使用loaded_vmcs->shadow_vmcs:
+		 *   - arch/x86/kvm/vmx.c|2203| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+		 *   - arch/x86/kvm/vmx.c|2204| <<loaded_vmcs_init>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+		 *   - arch/x86/kvm/vmx.c|4864| <<free_loaded_vmcs>> WARN_ON(loaded_vmcs->shadow_vmcs != NULL);
+		 *   - arch/x86/kvm/vmx.c|8407| <<alloc_shadow_vmcs>> WARN_ON(loaded_vmcs == &vmx->vmcs01 && loaded_vmcs->shadow_vmcs);
+		 *   - arch/x86/kvm/vmx.c|8409| <<alloc_shadow_vmcs>> if (!loaded_vmcs->shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx.c|8411| <<alloc_shadow_vmcs>> if (loaded_vmcs->shadow_vmcs)
+		 *   - arch/x86/kvm/vmx.c|8412| <<alloc_shadow_vmcs>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+		 *   - arch/x86/kvm/vmx.c|8414| <<alloc_shadow_vmcs>> return loaded_vmcs->shadow_vmcs;
+		 *   - arch/x86/kvm/vmx.c|8625| <<free_nested>> vmcs_clear(vmx->vmcs01.shadow_vmcs);
+		 *   - arch/x86/kvm/vmx.c|8626| <<free_nested>> free_vmcs(vmx->vmcs01.shadow_vmcs);
+		 *   - arch/x86/kvm/vmx.c|8807| <<copy_shadow_to_vmcs12>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+		 *   - arch/x86/kvm/vmx.c|8864| <<copy_vmcs12_to_shadow>> struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
+		 *   - arch/x86/kvm/vmx.c|9062| <<set_current_vmptr>> __pa(vmx->vmcs01.shadow_vmcs));
+		 */
 		vmcs_write64(VMCS_LINK_POINTER,
 			     __pa(vmx->vmcs01.shadow_vmcs));
+		/*
+		 * Indicates if the shadow vmcs must be updated with the
+		 * data hold by vmcs12
+		 */
 		vmx->nested.sync_shadow_vmcs = true;
 	}
 	vmx->nested.dirty_vmcs12 = true;
@@ -8972,11 +9757,28 @@ static int handle_vmptrld(struct kvm_vcpu *vcpu)
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx.c|8660| <<handle_vmon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|8828| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr) {
+	 *   - arch/x86/kvm/vmx.c|9285| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr) {
+	 *   - arch/x86/kvm/vmx.c|14575| <<vmx_get_nested_state>> kvm_state.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 *   - arch/x86/kvm/vmx.c|14699| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->vmx.vmxon_pa;
+	 */
 	if (vmptr == vmx->nested.vmxon_ptr) {
 		nested_vmx_failValid(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * 在以下修改nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx.c|8657| <<nested_release_vmcs12>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|8682| <<free_nested>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx.c|9141| <<set_current_vmptr>> vmx->nested.current_vmptr = vmptr;
+	 *   - arch/x86/kvm/vmx.c|11494| <<vmx_create_vcpu>> vmx->nested.current_vmptr = -1ull;
+	 *
+	 * The guest-physical address of the current VMCS L1 keeps for L2
+	 */
 	if (vmx->nested.current_vmptr != vmptr) {
 		struct vmcs12 *new_vmcs12;
 		struct page *page;
@@ -9002,10 +9804,32 @@ static int handle_vmptrld(struct kvm_vcpu *vcpu)
 		 * Load VMCS12 from guest memory since it is not already
 		 * cached.
 		 */
+		/*
+		 * 在以下使用nested_vmx->cached_vmcs12:
+		 *   - arch/x86/kvm/vmx.c|1372| <<get_vmcs12>> return to_vmx(vcpu)->nested.cached_vmcs12;
+		 *   - arch/x86/kvm/vmx.c|8586| <<enter_vmx_operation>> vmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL);
+		 *   - arch/x86/kvm/vmx.c|8587| <<enter_vmx_operation>> if (!vmx->nested.cached_vmcs12)
+		 *   - arch/x86/kvm/vmx.c|8618| <<enter_vmx_operation>> kfree(vmx->nested.cached_vmcs12);
+		 *   - arch/x86/kvm/vmx.c|8809| <<nested_release_vmcs12>> vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx.c|8860| <<free_nested>> kfree(vmx->nested.cached_vmcs12);
+		 *   - arch/x86/kvm/vmx.c|9404| <<handle_vmptrld>> memcpy(vmx->nested.cached_vmcs12, new_vmcs12, VMCS12_SIZE);
+		 *
+		 * Cache of the guest's VMCS, existing outside of guest memory.
+		 * Loaded from guest memory during VMPTRLD. Flushed to guest
+		 * memory during VMCLEAR and VMPTRLD.
+		 */
 		memcpy(vmx->nested.cached_vmcs12, new_vmcs12, VMCS12_SIZE);
 		kunmap(page);
 		kvm_release_page_clean(page);
 
+		/*
+		 * 在以下调用set_current_vmptr():
+		 *   - arch/x86/kvm/vmx.c|9192| <<handle_vmptrld>> set_current_vmptr(vmx, vmptr);
+		 *   - arch/x86/kvm/vmx.c|14518| <<vmx_set_nested_state>> set_current_vmptr(vmx, kvm_state->vmx.vmcs_pa);
+		 *
+		 * 最重要的是把__pa(vmx->vmcs01.shadow_vmcs)写入L0上的vmcs的VMCS_LINK_POINTER
+		 * 如果是针对L1的L0上的vmcs, 用vmptrld指令写入吧?
+		 */
 		set_current_vmptr(vmx, vmptr);
 	}
 
@@ -9895,6 +10719,14 @@ static bool nested_vmx_exit_reflected(struct kvm_vcpu *vcpu, u32 exit_reason)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|10979| <<vmx_handle_exit>> return nested_vmx_reflect_vmexit(vcpu, exit_reason);
+ *
+ * vcpu_enter_guest()
+ * -> kvm_x86_ops->handle_exit = vmx_handle_exit()
+ *    -> nested_vmx_reflect_vmexit()
+ */
 static int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)
 {
 	u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
@@ -10143,6 +10975,12 @@ static void dump_vmcs(void)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7754| <<vcpu_enter_guest>> r = kvm_x86_ops->handle_exit(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.handle_exit = vmx_handle_exit()
+ */
 static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10593,6 +11431,10 @@ static bool vmx_xsaves_supported(void)
 		SECONDARY_EXEC_XSAVES;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|11412| <<vmx_vcpu_run>> vmx_recover_nmi_blocking(vmx);
+ */
 static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 {
 	u32 exit_intr_info;
@@ -10734,6 +11576,10 @@ static void vmx_arm_hv_timer(struct vcpu_vmx *vmx, u32 val)
 	vmx->loaded_vmcs->hv_timer_armed = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|11152| <<vmx_vcpu_run>> vmx_update_hv_timer(vcpu);
+ */
 static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -10764,18 +11610,54 @@ static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 	vmx->loaded_vmcs->hv_timer_armed = false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7688| <<vcpu_enter_guest>> kvm_x86_ops->run(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.run = vmx_vcpu_run()
+ */
 static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	unsigned long cr3, cr4, evmcs_rsp;
 
 	/* Record the guest's net vcpu time for enforced NMI injections. */
+	/*
+	 * Delivery of a non-maskable interrupt (NMI) or a system-management interrupt (SMI) blocks
+	 * subsequent NMIs until the next execution of IRET. See Section 25.3 for how this behavior of
+	 * IRET may change in VMX non-root operation. Setting this bit indicates that blocking of NMIs is
+	 * in effect. Clearing this bit does not imply that NMIs are not (temporarily) blocked for other
+	 * reasons.
+	 *
+	 * If the "virtual NMIs" VM-execution control (see Section 24.6.1) is 1, this bit does not control the
+	 * blocking of NMIs. Instead, it refers to “virtual-NMI blocking” (the fact that guest software is not
+	 * ready for an NMI).
+	 *
+	 * VM entry will fail if these bits are not 0. See Section 26.3.1.5.
+	 *
+	 * 在以下使用loaded_vmcs->entry_time:
+	 *   - arch/x86/kvm/vmx.c|10858| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->entry_time));
+	 *   - arch/x86/kvm/vmx.c|11003| <<vmx_vcpu_run>> vmx->loaded_vmcs->entry_time = ktime_get();
+	 */
 	if (unlikely(!enable_vnmi &&
 		     vmx->loaded_vmcs->soft_vnmi_blocked))
 		vmx->loaded_vmcs->entry_time = ktime_get();
 
 	/* Don't enter VMX if guest state is invalid, let the exit handler
 	   start emulation until we arrive back to a valid state */
+	/*
+	 * 在以下使用vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx.c|3354| <<vmx_set_rflags>> to_vmx(vcpu)->emulation_required = emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx.c|3498| <<vmx_queue_exception>> WARN_ON_ONCE(vmx->emulation_required);
+	 *   - arch/x86/kvm/vmx.c|5462| <<vmx_set_cr0>> vmx->emulation_required = emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx.c|5709| <<vmx_set_segment>> vmx->emulation_required = emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx.c|7948| <<handle_invalid_guest_state>> WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx.c|7953| <<handle_invalid_guest_state>> while (vmx->emulation_required && count-- != 0) {
+	 *   - arch/x86/kvm/vmx.c|7971| <<handle_invalid_guest_state>> if (vmx->emulation_required && !vmx->rmode.vm86_active &&
+	 *   - arch/x86/kvm/vmx.c|10387| <<vmx_handle_exit>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx.c|11007| <<vmx_vcpu_run>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx.c|12789| <<prepare_vmcs02>> if (vmx->emulation_required) {
+	 */
 	if (vmx->emulation_required)
 		return;
 
@@ -10784,8 +11666,37 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 	}
 
+	/*
+	 * 在以下使用和修改nested_vmx->sync_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|9107| <<set_current_vmptr>> vmx->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|13720| <<nested_vmx_vmexit>> vmx->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|13810| <<nested_vmx_entry_failure>> to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|8610| <<nested_release_vmcs12>> vmx->nested.sync_shadow_vmcs = false;
+	 *   - arch/x86/kvm/vmx.c|10954| <<vmx_vcpu_run>> vmx->nested.sync_shadow_vmcs = false;
+	 *   - arch/x86/kvm/vmx.c|10952| <<vmx_vcpu_run>> if (vmx->nested.sync_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|14359| <<vmx_get_nested_state>> else if (enable_shadow_vmcs && !vmx->nested.sync_shadow_vmcs)
+	 *
+	 * struct vcpu_vmx *vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> bool sync_shadow_vmcs;
+	 *
+	 * 把vmx->nested.cached_vmcs12(类型vmcs12)给同步到vmx->vmcs01.shadow_vmcs(类型vmcs)
+	 * 这里为了在L1修改针对L2的vmcs时不用trap到L0了
+	 *
+	 * 用set_current_vmptr()把shadow_vmcs给硬件
+	 */
 	if (vmx->nested.sync_shadow_vmcs) {
 		copy_vmcs12_to_shadow(vmx);
+		/*
+		 * 在以下使用和修改nested_vmx->sync_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx.c|8610| <<nested_release_vmcs12>> vmx->nested.sync_shadow_vmcs = false;
+		 *   - arch/x86/kvm/vmx.c|9107| <<set_current_vmptr>> vmx->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|10954| <<vmx_vcpu_run>> vmx->nested.sync_shadow_vmcs = false;
+		 *   - arch/x86/kvm/vmx.c|13720| <<nested_vmx_vmexit>> vmx->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|13810| <<nested_vmx_entry_failure>> to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
+		 *   - arch/x86/kvm/vmx.c|10952| <<vmx_vcpu_run>> if (vmx->nested.sync_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx.c|14359| <<vmx_get_nested_state>> else if (enable_shadow_vmcs && !vmx->nested.sync_shadow_vmcs)
+		 */
 		vmx->nested.sync_shadow_vmcs = false;
 	}
 
@@ -10829,8 +11740,27 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	 * is no need to worry about the conditional branch over the wrmsr
 	 * being speculatively taken.
 	 */
+	/*
+	 * x86_spec_ctrl_set_guest - Set speculation control registers for the guest
+	 * @guest_spec_ctrl:            The guest content of MSR_SPEC_CTRL
+	 * @guest_virt_spec_ctrl:       The guest controlled bits of MSR_VIRT_SPEC_CTRL
+	 *                              (may get translated to MSR_AMD64_LS_CFG bits)
+	 *
+	 * Avoids writing to the MSR if the content/bits are the same
+	 */
 	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
 
+	/*
+	 * 在以下使用loaded_vmcs->launched:
+	 *   - arch/x86/kvm/vmx.c|2236| <<loaded_vmcs_init>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx.c|11243| <<vmx_vcpu_run>> vmx->loaded_vmcs->launched = 1;
+	 *   - arch/x86/kvm/vmx.c|2233| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx.c|11027| <<vmx_vcpu_run>> vmx->__launched = vmx->loaded_vmcs->launched;
+	 *
+	 * 在以下使用vcpu_vmx->__launched:
+	 *   - arch/x86/kvm/vmx.c|11027| <<vmx_vcpu_run>> vmx->__launched = vmx->loaded_vmcs->launched;
+	 *   - arch/x86/kvm/vmx.c|11130| <<vmx_vcpu_run>> [launched]"i"(offsetof(struct vcpu_vmx, __launched)),
+	 */
 	vmx->__launched = vmx->loaded_vmcs->launched;
 
 	evmcs_rsp = static_branch_unlikely(&enable_evmcs) ?
@@ -10989,6 +11919,14 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 			vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 	}
 
+	/*
+	 * x86_spec_ctrl_restore_host - Restore host speculation control registers
+	 * @guest_spec_ctrl:            The guest content of MSR_SPEC_CTRL
+	 * @guest_virt_spec_ctrl:       The guest controlled bits of MSR_VIRT_SPEC_CTRL
+	 *                              (may get translated to MSR_AMD64_LS_CFG bits)
+	 *
+	 * Avoids writing to the MSR if the content/bits are the same
+	 */
 	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
 
 	/* All fields are clean at this point */
@@ -11039,6 +11977,13 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	if (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))
 		return;
 
+	/*
+	 * 在以下使用loaded_vmcs->launched:
+	 *   - arch/x86/kvm/vmx.c|2236| <<loaded_vmcs_init>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx.c|11243| <<vmx_vcpu_run>> vmx->loaded_vmcs->launched = 1;
+	 *   - arch/x86/kvm/vmx.c|2233| <<loaded_vmcs_init>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx.c|11027| <<vmx_vcpu_run>> vmx->__launched = vmx->loaded_vmcs->launched;
+	 */
 	vmx->loaded_vmcs->launched = 1;
 	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
 
@@ -11059,6 +12004,38 @@ static void vmx_vm_free(struct kvm *kvm)
 	vfree(to_kvm_vmx(kvm));
 }
 
+/*
+ * struct vcpu_vmx:
+ *  -> struct kvm_vcpu vcpu;
+ *  -> u8 fail;
+ *  -> struct loaded_vmcs vmcs01;
+ *      -> struct vmcs *vmcs;
+ *      -> struct vmcs *shadow_vmcs;
+ *      -> int cpu;
+ *  -> struct loaded_vmcs *loaded_vmcs;
+ *  -> struct loaded_vmcs *loaded_cpu_state;
+ *  -> u32 exit_reason;
+ *  -> struct nested_vmx nested;
+ *      -> struct vmcs12 *cached_vmcs12;
+ *      -> struct vmcs12 *cached_shadow_vmcs12;
+ *      -> struct loaded_vmcs vmcs02;
+ *          -> struct vmcs *vmcs;
+ *          -> struct vmcs *shadow_vmcs;
+ *          -> int cpu;
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx.c|11155| <<vmx_free_vcpu_nested>> vmx_switch_vmcs(vcpu, &to_vmx(vcpu)->vmcs01);
+ *   - arch/x86/kvm/vmx.c|12829| <<enter_vmx_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx.c|12888| <<enter_vmx_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx.c|13534| <<nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *
+ * vmcs12用于记录L1上管理的L2的vmcs
+ * vmcs02用于记录host到L2的vmcs
+ * vmcs01用于管理host到L1VM的vmcs
+ *
+ * 关键一步是把vmx->loaded_vmcs设置成参数的vmcs
+ * vmx->loaded_vmcs = vmcs;
+ */
 static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -11069,6 +12046,21 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 
 	cpu = get_cpu();
 	vmx_vcpu_put(vcpu);
+	/*
+	 * loaded_vmcs points to the VMCS currently used in this vcpu. For a
+	 * non-nested (L1) guest, it always points to vmcs01. For a nested
+	 * guest (L2), it points to a different VMCS.  loaded_cpu_state points
+	 * to the VMCS whose state is loaded into the CPU registers that only
+	 * need to be switched when transitioning to/from the kernel; a NULL
+	 * value indicates that host state is loaded.
+	 *
+	 * 在以下使用vcpu_vmx->loaded_vmcs:
+	 *   - arch/x86/kvm/vmx.c|11675| <<vmx_switch_vmcs>> vmx->loaded_vmcs = vmcs;
+	 *   - arch/x86/kvm/vmx.c|11770| <<vmx_create_vcpu>> vmx->loaded_vmcs = &vmx->vmcs01;
+	 *
+	 * struct vcpu_vmx:
+	 *  -> struct loaded_vmcs *loaded_vmcs;
+	 */
 	vmx->loaded_vmcs = vmcs;
 	vmx_vcpu_load(vcpu, cpu);
 	put_cpu();
@@ -11078,14 +12070,36 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
  * Ensure that the current vmcs of the logical processor is the
  * vmcs01 of the vcpu before calling free_nested().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|11258| <<vmx_free_vcpu>> vmx_free_vcpu_nested(vcpu);
+ *
+ * 对于x86.
+ * kvm_arch_vcpu_free() or kvm_arch_vcpu_destroy()
+ * -> kvm_x86_ops->vcpu_free = vmx_free_vcpu()
+ *    -> vmx_free_vcpu_nested()
+ */
 static void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)
 {
 	vcpu_load(vcpu);
+	/*
+	 * 关键一步是把vmx->loaded_vmcs设置成参数的vmcs
+	 * vmx->loaded_vmcs = vmcs;
+	 */
 	vmx_switch_vmcs(vcpu, &to_vmx(vcpu)->vmcs01);
 	free_nested(vcpu);
 	vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s.c|773| <<kvmppc_core_vcpu_free>> vcpu->kvm->arch.kvm_ops->vcpu_free(vcpu);
+ *   - arch/powerpc/kvm/booke.c|2132| <<kvmppc_core_vcpu_free>> vcpu->kvm->arch.kvm_ops->vcpu_free(vcpu);
+ *   - arch/x86/kvm/x86.c|8543| <<kvm_arch_vcpu_free>> kvm_x86_ops->vcpu_free(vcpu);
+ *   - arch/x86/kvm/x86.c|8609| <<kvm_arch_vcpu_destroy>> kvm_x86_ops->vcpu_free(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_free = vmx_free_vcpu()
+ */
 static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -11101,6 +12115,14 @@ static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vmx);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s.c|768| <<kvmppc_core_vcpu_create>> return kvm->arch.kvm_ops->vcpu_create(kvm, id);
+ *   - arch/powerpc/kvm/booke.c|2127| <<kvmppc_core_vcpu_create>> return kvm->arch.kvm_ops->vcpu_create(kvm, id);
+ *   - arch/x86/kvm/x86.c|8569| <<kvm_arch_vcpu_create>> vcpu = kvm_x86_ops->vcpu_create(kvm, id);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_create = vmx_create_vcpu()
+ */
 static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 {
 	int err;
@@ -11151,6 +12173,21 @@ static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 	vmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
 	vmx->msr_bitmap_mode = 0;
 
+	/*
+	 * loaded_vmcs points to the VMCS currently used in this vcpu. For a
+	 * non-nested (L1) guest, it always points to vmcs01. For a nested
+	 * guest (L2), it points to a different VMCS.  loaded_cpu_state points
+	 * to the VMCS whose state is loaded into the CPU registers that only
+	 * need to be switched when transitioning to/from the kernel; a NULL
+	 * value indicates that host state is loaded.
+	 *
+	 * 在以下使用vcpu_vmx->loaded_vmcs:
+	 *   - arch/x86/kvm/vmx.c|11675| <<vmx_switch_vmcs>> vmx->loaded_vmcs = vmcs;
+	 *   - arch/x86/kvm/vmx.c|11770| <<vmx_create_vcpu>> vmx->loaded_vmcs = &vmx->vmcs01;
+	 *
+	 * struct vcpu_vmx:
+	 *  -> struct loaded_vmcs *loaded_vmcs;
+	 */
 	vmx->loaded_vmcs = &vmx->vmcs01;
 	cpu = get_cpu();
 	vmx_vcpu_load(&vmx->vcpu, cpu);
@@ -11761,6 +12798,14 @@ static void nested_cache_shadow_vmcs12(struct kvm_vcpu *vcpu,
 	    vmcs12->vmcs_link_pointer == -1ull)
 		return;
 
+	/*
+	 * 返回to_vmx(vcpu)->nested.cached_shadow_vmcs12;
+	 *
+	 * struct vcpu_vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	shadow = get_shadow_vmcs12(vcpu);
 	page = kvm_vcpu_gpa_to_page(vcpu, vmcs12->vmcs_link_pointer);
 
@@ -12074,6 +13119,16 @@ static int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3, bool ne
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|13064| <<prepare_vmcs02>> prepare_vmcs02_full(vcpu, vmcs12);
+ *
+ * enter_vmx_non_root_mode()被几个调用.一个例子.
+ * nested_vmx_run()
+ * -> enter_vmx_non_root_mode()
+ *    -> prepare_vmcs02()
+ *       -> prepare_vmcs02_full()
+ */
 static void prepare_vmcs02_full(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -12210,6 +13265,15 @@ static void prepare_vmcs02_full(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|12821| <<enter_vmx_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry ? exit_qual : &dummy_exit_qual))
+ *
+ * enter_vmx_non_root_mode()被几个调用.一个例子.
+ * nested_vmx_run()
+ * -> enter_vmx_non_root_mode()
+ *    -> prepare_vmcs02()
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  u32 *entry_failure_code)
 {
@@ -12718,6 +13782,12 @@ static int check_vmentry_postreqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
  * If exit_qual is NULL, this is being called from state restore (either RSM
  * or KVM_SET_NESTED_STATE).  Otherwise it's called from vmlaunch/vmresume.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|12967| <<nested_vmx_run>> ret = enter_vmx_non_root_mode(vcpu, &exit_qual);
+ *   - arch/x86/kvm/vmx.c|14121| <<vmx_pre_leave_smm>> ret = enter_vmx_non_root_mode(vcpu, NULL);
+ *   - arch/x86/kvm/vmx.c|14331| <<vmx_set_nested_state>> ret = enter_vmx_non_root_mode(vcpu, NULL);
+ */
 static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, u32 *exit_qual)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -12732,6 +13802,13 @@ static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, u32 *exit_qual)
 	if (likely(!evaluate_pending_interrupts) && kvm_vcpu_apicv_active(vcpu))
 		evaluate_pending_interrupts |= vmx_has_apicv_interrupt(vcpu);
 
+	/*
+	 * 为vcpu->arch.hflags设置HF_GUEST_MASK, 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	enter_guest_mode(vcpu);
 
 	if (!(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS))
@@ -12740,6 +13817,32 @@ static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, u32 *exit_qual)
 		!(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS))
 		vmx->nested.vmcs01_guest_bndcfgs = vmcs_read64(GUEST_BNDCFGS);
 
+	/*
+	 * struct vcpu_vmx:
+	 *  -> struct kvm_vcpu vcpu;
+	 *  -> u8 fail;
+	 *  -> struct loaded_vmcs vmcs01;
+	 *      -> struct vmcs *vmcs;
+	 *      -> struct vmcs *shadow_vmcs;
+	 *      -> int cpu;
+	 *  -> struct loaded_vmcs *loaded_vmcs;
+	 *  -> struct loaded_vmcs *loaded_cpu_state;
+	 *  -> u32 exit_reason;
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 *      -> struct loaded_vmcs vmcs02; <------ !!!!
+	 *          -> struct vmcs *vmcs;
+	 *          -> struct vmcs *shadow_vmcs;
+	 *          -> int cpu;
+	 *
+	 * vmcs12用于记录L1上管理的L2的vmcs
+	 * vmcs02用于记录host到L2的vmcs
+	 * vmcs01用于管理host到L1VM的vmcs
+	 *
+	 * 关键一步是把vmx->loaded_vmcs设置成参数的vmcs
+	 * vmx->loaded_vmcs = vmcs;
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
 	vmx_segment_cache_clear(vmx);
 
@@ -12747,6 +13850,17 @@ static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, u32 *exit_qual)
 		vcpu->arch.tsc_offset += vmcs12->tsc_offset;
 
 	r = EXIT_REASON_INVALID_STATE;
+	/*
+	 * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested
+	 * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it
+	 * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2
+	 * guest in a way that will both be appropriate to L1's requests, and our
+	 * needs. In addition to modifying the active vmcs (which is vmcs02), this
+	 * function also has additional necessary side-effects, like setting various
+	 * vcpu->arch fields.
+	 * Returns 0 on success, 1 on failure. Invalid state exit qualification code
+	 * is assigned to entry_failure_code on failure.
+	 */
 	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry ? exit_qual : &dummy_exit_qual))
 		goto fail;
 
@@ -12798,7 +13912,22 @@ static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, u32 *exit_qual)
 fail:
 	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)
 		vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	/*
+	 * 为vcpu->arch.hflags清除HF_GUEST_MASK, 在以下使用HF_GUEST_MASK:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|95| <<enter_guest_mode>> vcpu->arch.hflags |= HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|100| <<leave_guest_mode>> vcpu->arch.hflags &= ~HF_GUEST_MASK;
+	 *   - arch/x86/kvm/kvm_cache_regs.h|110| <<is_guest_mode>> return vcpu->arch.hflags & HF_GUEST_MASK;
+	 *   - arch/x86/kvm/x86.c|5911| <<init_emulate_ctxt>> BUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);
+	 */
 	leave_guest_mode(vcpu);
+	/*
+	 * vmcs12用于记录L1上管理的L2的vmcs
+	 * vmcs02用于记录host到L2的vmcs
+	 * vmcs01用于管理host到L1VM的vmcs
+	 *
+	 * 关键一步是把vmx->loaded_vmcs设置成参数的vmcs
+	 * vmx->loaded_vmcs = vmcs;
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 	return r;
 }
@@ -12807,6 +13936,32 @@ static int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu, u32 *exit_qual)
  * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1
  * for running an L2 nested guest.
  */
+/*
+ * struct vcpu_vmx:
+ *  -> struct kvm_vcpu vcpu;
+ *  -> u8 fail;
+ *  -> struct loaded_vmcs vmcs01;
+ *      -> struct vmcs *vmcs;
+ *      -> struct vmcs *shadow_vmcs;
+ *      -> int cpu;
+ *  -> struct loaded_vmcs *loaded_vmcs;
+ *  -> struct loaded_vmcs *loaded_cpu_state;
+ *  -> u32 exit_reason;
+ *  -> struct nested_vmx nested;
+ *      -> struct vmcs12 *cached_vmcs12;
+ *      -> struct vmcs12 *cached_shadow_vmcs12;
+ *      -> struct loaded_vmcs vmcs02;
+ *          -> struct vmcs *vmcs;
+ *          -> struct vmcs *shadow_vmcs;
+ *          -> int cpu;
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx.c|8673| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+ *   - arch/x86/kvm/vmx.c|8680| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+ *
+ * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1
+ * for running an L2 nested guest.
+ */
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 {
 	struct vmcs12 *vmcs12;
@@ -12821,6 +13976,14 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	if (!nested_vmx_check_vmcs12(vcpu))
 		goto out;
 
+	/*
+	 * 返回to_vmx(vcpu)->nested.cached_vmcs12;
+	 *
+	 * struct vcpu_vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	vmcs12 = get_vmcs12(vcpu);
 
 	/*
@@ -12834,6 +13997,15 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 		goto out;
 	}
 
+	/*
+	 * Copy the writable VMCS shadow fields back to the VMCS12, in case
+	 * they have been modified by the L1 guest. Note that the "read-only"
+	 * VM-exit information fields are actually writable if the vCPU is
+	 * configured to support "VMWRITE to any supported field in the VMCS."
+	 *
+	 * 把vmx->vmcs01.shadow_vmcs(类型vmcs)的数据同步到vmx->nested.cached_vmcs12(类型vmcs12)
+	 * 这里的shadow vmcs是L1中用来支持L2的vmcs
+	 */
 	if (enable_shadow_vmcs)
 		copy_shadow_to_vmcs12(vmx);
 
@@ -12877,6 +14049,13 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 
 	ret = check_vmentry_postreqs(vcpu, vmcs12, &exit_qual);
 	if (ret) {
+		/*
+		 * L1's failure to enter L2 is a subset of a normal exit, as explained in
+		 * 23.7 "VM-entry failures during or after loading guest state" (this also
+		 * lists the acceptable exit-reason and exit-qualification parameters).
+		 * It should only be called before L2 actually succeeded to run, and when
+		 * vmcs01 is current (it doesn't leave_guest_mode() or switch vmcss).
+		 */
 		nested_vmx_entry_failure(vcpu, vmcs12,
 					 EXIT_REASON_INVALID_STATE, exit_qual);
 		return 1;
@@ -12887,9 +14066,41 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	 * the nested entry.
 	 */
 
+	/*
+	 * 在以下修改nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx.c|11616| <<vmx_vcpu_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|13526| <<nested_vmx_run>> vmx->nested.nested_run_pending = 1;
+	 *   - arch/x86/kvm/vmx.c|13530| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|13555| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|14218| <<vmx_leave_nested>> to_vmx(vcpu)->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx.c|14902| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = !!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);
+	 * 在以下使用nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx.c|7171| <<vmx_nmi_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx.c|7185| <<vmx_interrupt_allowed>> return (!to_vmx(vcpu)->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|8024| <<handle_invalid_guest_state>> WARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx.c|10205| <<nested_vmx_exit_reflected>> if (vmx->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx.c|12123| <<vmx_inject_page_fault_nested>> !to_vmx(vcpu)->nested.nested_run_pending) {
+	 *   - arch/x86/kvm/vmx.c|12793| <<prepare_vmcs02_full>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|12860| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|12868| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending) {
+	 *   - arch/x86/kvm/vmx.c|13005| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|13076| <<prepare_vmcs02>> if (vmx->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|13309| <<check_vmentry_postreqs>> if (to_vmx(vcpu)->nested.nested_run_pending &&
+	 *   - arch/x86/kvm/vmx.c|13648| <<vmx_check_nested_events>> vmx->nested.nested_run_pending || kvm_event_needs_reinjection(vcpu);
+	 *   - arch/x86/kvm/vmx.c|14054| <<nested_vmx_vmexit>> WARN_ON_ONCE(vmx->nested.nested_run_pending);
+	 *   - arch/x86/kvm/vmx.c|14680| <<vmx_smi_allowed>> if (to_vmx(vcpu)->nested.nested_run_pending)
+	 *   - arch/x86/kvm/vmx.c|14766| <<vmx_get_nested_state>> if (vmx->nested.nested_run_pending)
+	 */
 	vmx->nested.nested_run_pending = 1;
 	ret = enter_vmx_non_root_mode(vcpu, &exit_qual);
 	if (ret) {
+		/*
+		 * L1's failure to enter L2 is a subset of a normal exit, as explained in
+		 * 23.7 "VM-entry failures during or after loading guest state" (this also
+		 * lists the acceptable exit-reason and exit-qualification parameters).
+		 * It should only be called before L2 actually succeeded to run, and when
+		 * vmcs01 is current (it doesn't leave_guest_mode() or switch vmcss).
+		 */
 		nested_vmx_entry_failure(vcpu, vmcs12, ret, exit_qual);
 		vmx->nested.nested_run_pending = 0;
 		return 1;
@@ -13196,6 +14407,16 @@ static void sync_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
  * exit-information fields only. Other fields are modified by L1 with VMWRITE,
  * which already writes to vmcs12 directly.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|14601| <<nested_vmx_vmexit>> prepare_vmcs12(vcpu, vmcs12, exit_reason, exit_intr_info,
+ *
+ * 其中一个例子.
+ * vcpu_enter_guest()
+ * -> kvm_x86_ops->handle_exit = vmx_handle_exit()
+ *    -> nested_vmx_reflect_vmexit() 
+ *       -> nested_vmx_vmexit()
+ */
 static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			   u32 exit_reason, u32 exit_intr_info,
 			   unsigned long exit_qualification)
@@ -13394,11 +14615,42 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
  * and modify vmcs12 to make it see what it would expect to see there if
  * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|3450| <<nested_vmx_inject_exception_vmexit>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx.c|9722| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->exit_reason,
+ *   - arch/x86/kvm/vmx.c|10205| <<nested_vmx_reflect_vmexit>> nested_vmx_vmexit(vcpu, exit_reason, exit_intr_info,
+ *   - arch/x86/kvm/vmx.c|11868| <<nested_ept_inject_page_fault>> nested_vmx_vmexit(vcpu, exit_reason, 0, exit_qualification);
+ *   - arch/x86/kvm/vmx.c|11934| <<vmx_inject_page_fault_nested>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,
+ *   - arch/x86/kvm/vmx.c|13471| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
+ *   - arch/x86/kvm/vmx.c|13478| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,
+ *   - arch/x86/kvm/vmx.c|13494| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);
+ *   - arch/x86/kvm/vmx.c|14005| <<vmx_leave_nested>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *   - arch/x86/kvm/vmx.c|14477| <<vmx_pre_enter_smm>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *
+ * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+ * and modify vmcs12 to make it see what it would expect to see there if
+ * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+ *
+ * 其中一个例子.
+ * vcpu_enter_guest()
+ * -> kvm_x86_ops->handle_exit = vmx_handle_exit()
+ *    -> nested_vmx_reflect_vmexit()
+ *       -> nested_vmx_vmexit()
+ */
 static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 			      u32 exit_intr_info,
 			      unsigned long exit_qualification)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * 返回to_vmx(vcpu)->nested.cached_vmcs12;
+	 *
+	 * struct vcpu_vmx:
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 */
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
 
 	/* trying to cancel vmlaunch/vmresume is a bug */
@@ -13417,6 +14669,17 @@ static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)
 		vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
 
+	/*
+	 * prepare_vmcs12 is part of what we need to do when the nested L2 guest exits
+	 * and we want to prepare to run its L1 parent. L1 keeps a vmcs for L2 (vmcs12),
+	 * and this function updates it to reflect the changes to the guest state while
+	 * L2 was running (and perhaps made some exits which were handled directly by L0
+	 * without going back to L1), and to reflect the exit reason.
+	 * Note that we do not have to copy here all VMCS fields, just those that
+	 * could have changed by the L2 guest or the exit - i.e., the guest-state and
+	 * exit-information fields only. Other fields are modified by L1 with VMWRITE,
+	 * which already writes to vmcs12 directly.
+	 */
 	if (likely(!vmx->fail)) {
 		if (exit_reason == -1)
 			sync_vmcs12(vcpu, vmcs12);
@@ -13440,6 +14703,28 @@ static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 			nested_vmx_abort(vcpu, VMX_ABORT_SAVE_GUEST_MSR_FAIL);
 	}
 
+	/*
+	 * struct vcpu_vmx:
+	 *  -> struct kvm_vcpu vcpu;
+	 *  -> u8 fail;
+	 *  -> struct loaded_vmcs vmcs01;
+	 *      -> struct vmcs *vmcs;
+	 *      -> struct vmcs *shadow_vmcs;
+	 *      -> int cpu;
+	 *  -> struct loaded_vmcs *loaded_vmcs;
+	 *  -> struct loaded_vmcs *loaded_cpu_state;
+	 *  -> u32 exit_reason;
+	 *  -> struct nested_vmx nested;
+	 *      -> struct vmcs12 *cached_vmcs12;
+	 *      -> struct vmcs12 *cached_shadow_vmcs12;
+	 *      -> struct loaded_vmcs vmcs02;
+	 *          -> struct vmcs *vmcs;
+	 *          -> struct vmcs *shadow_vmcs;
+	 *          -> int cpu;
+	 *
+	 * 关键一步是把vmx->loaded_vmcs设置成参数的vmcs
+	 * vmx->loaded_vmcs = vmcs;
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 	vm_entry_controls_reset_shadow(vmx);
 	vm_exit_controls_reset_shadow(vmx);
@@ -13487,6 +14772,16 @@ static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 	 */
 	kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 
+	/*
+	 * 在以下使用和修改nested_vmx->sync_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx.c|8610| <<nested_release_vmcs12>> vmx->nested.sync_shadow_vmcs = false;
+	 *   - arch/x86/kvm/vmx.c|9107| <<set_current_vmptr>> vmx->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|10954| <<vmx_vcpu_run>> vmx->nested.sync_shadow_vmcs = false;
+	 *   - arch/x86/kvm/vmx.c|13720| <<nested_vmx_vmexit>> vmx->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|13810| <<nested_vmx_entry_failure>> to_vmx(vcpu)->nested.sync_shadow_vmcs = true;
+	 *   - arch/x86/kvm/vmx.c|10952| <<vmx_vcpu_run>> if (vmx->nested.sync_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx.c|14359| <<vmx_get_nested_state>> else if (enable_shadow_vmcs && !vmx->nested.sync_shadow_vmcs)
+	 */
 	if (enable_shadow_vmcs && exit_reason != -1)
 		vmx->nested.sync_shadow_vmcs = true;
 
@@ -13547,6 +14842,12 @@ static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 /*
  * Forcibly leave nested mode in order to be able to reset the VCPU later on.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|4428| <<vmx_set_msr>> vmx_leave_nested(vcpu);
+ *   - arch/x86/kvm/vmx.c|14348| <<vmx_set_nested_state>> vmx_leave_nested(vcpu);
+ *   - arch/x86/kvm/vmx.c|14375| <<vmx_set_nested_state>> vmx_leave_nested(vcpu);
+ */
 static void vmx_leave_nested(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu)) {
@@ -13563,6 +14864,11 @@ static void vmx_leave_nested(struct kvm_vcpu *vcpu)
  * It should only be called before L2 actually succeeded to run, and when
  * vmcs01 is current (it doesn't leave_guest_mode() or switch vmcss).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|14032| <<nested_vmx_run>> nested_vmx_entry_failure(vcpu, vmcs12,
+ *   - arch/x86/kvm/vmx.c|14070| <<nested_vmx_run>> nested_vmx_entry_failure(vcpu, vmcs12, ret, exit_qual);
+ */
 static void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,
 			struct vmcs12 *vmcs12,
 			u32 reason, unsigned long qualification)
@@ -14142,6 +15448,12 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	return kvm_state.size;
 }
 
+/*
+ * called by (KVM_SET_NESTED_STATE):
+ *   - rch/x86/kvm/x86.c|4115| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops->set_nested_state(vcpu, user_kvm_nested_state, &kvm_state);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.set_nested_state = vmx_set_nested_state()
+ */
 static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state)
@@ -14157,6 +15469,17 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 	if (!nested_vmx_allowed(vcpu))
 		return kvm_state->vmx.vmxon_pa == -1ull ? 0 : -EINVAL;
 
+	/*
+	 * 在以下使用kvm_vmx_nested_state->vmxon_pa (__u64):
+	 *   - arch/x86/kvm/vmx.c|15375| <<vmx_get_nested_state>> .vmx.vmxon_pa = -1ull,
+	 *   - arch/x86/kvm/vmx.c|15386| <<vmx_get_nested_state>> kvm_state.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 *   - arch/x86/kvm/vmx.c|15470| <<vmx_set_nested_state>> return kvm_state->vmx.vmxon_pa == -1ull ? 0 : -EINVAL;
+	 *   - arch/x86/kvm/vmx.c|15472| <<vmx_set_nested_state>> if (kvm_state->vmx.vmxon_pa == -1ull) {
+	 *   - arch/x86/kvm/vmx.c|15483| <<vmx_set_nested_state>> if (!page_address_valid(vcpu, kvm_state->vmx.vmxon_pa))
+	 *   - arch/x86/kvm/vmx.c|15507| <<vmx_set_nested_state>> if (kvm_state->vmx.vmxon_pa == -1ull)
+	 *   - arch/x86/kvm/vmx.c|15510| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->vmx.vmxon_pa;
+	 *   - arch/x86/kvm/vmx.c|15519| <<vmx_set_nested_state>> if (kvm_state->vmx.vmcs_pa == kvm_state->vmx.vmxon_pa ||
+	 */
 	if (kvm_state->vmx.vmxon_pa == -1ull) {
 		if (kvm_state->vmx.smm.flags)
 			return -EINVAL;
@@ -14208,6 +15531,14 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 	    !page_address_valid(vcpu, kvm_state->vmx.vmcs_pa))
 		return -EINVAL;
 
+	/*
+	 * 在以下调用set_current_vmptr():
+	 *   - arch/x86/kvm/vmx.c|9192| <<handle_vmptrld>> set_current_vmptr(vmx, vmptr);
+	 *   - arch/x86/kvm/vmx.c|14518| <<vmx_set_nested_state>> set_current_vmptr(vmx, kvm_state->vmx.vmcs_pa);
+	 *
+	 * 最重要的是把__pa(vmx->vmcs01.shadow_vmcs)写入L0上的vmcs的VMCS_LINK_POINTER
+	 * 如果是针对L1的L0上的vmcs, 用vmptrld指令写入吧?
+	 */
 	set_current_vmptr(vmx, kvm_state->vmx.vmcs_pa);
 
 	if (kvm_state->vmx.smm.flags & KVM_STATE_NESTED_SMM_VMXON) {
@@ -14452,6 +15783,10 @@ static void vmx_exit(void)
 }
 module_exit(vmx_exit);
 
+/*
+ * 在以下使用:
+ *   - arch/x86/kvm/vmx.c|14741| <<global>> module_init(vmx_init);
+ */
 static int __init vmx_init(void)
 {
 	int r;
diff --git a/arch/x86/kvm/vmx_evmcs.h b/arch/x86/kvm/vmx_evmcs.h
index 210a884090ad..4bd5e26970b1 100644
--- a/arch/x86/kvm/vmx_evmcs.h
+++ b/arch/x86/kvm/vmx_evmcs.h
@@ -299,6 +299,15 @@ static const struct evmcs_field vmcs_field_to_evmcs_1[] = {
 		     HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_XLAT),
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|1729| <<evmcs_write64>> int offset = get_evmcs_offset(field, &clean_field);
+ *   - arch/x86/kvm/vmx.c|1742| <<evmcs_write32>> int offset = get_evmcs_offset(field, &clean_field);
+ *   - arch/x86/kvm/vmx.c|1754| <<evmcs_write16>> int offset = get_evmcs_offset(field, &clean_field);
+ *   - arch/x86/kvm/vmx.c|1765| <<evmcs_read64>> int offset = get_evmcs_offset(field, NULL);
+ *   - arch/x86/kvm/vmx.c|1775| <<evmcs_read32>> int offset = get_evmcs_offset(field, NULL);
+ *   - arch/x86/kvm/vmx.c|1785| <<evmcs_read16>> int offset = get_evmcs_offset(field, NULL);
+ */
 static __always_inline int get_evmcs_offset(unsigned long field,
 					    u16 *clean_field)
 {
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 8df6eef9a02e..e9dc786773e8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1231,6 +1231,14 @@ EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|4405| <<wrmsr_interception>> if (kvm_set_msr(&svm->vcpu, &msr)) {
+ *   - arch/x86/kvm/vmx.c|7684| <<handle_wrmsr>> if (kvm_set_msr(vcpu, &msr) != 0) {
+ *   - arch/x86/kvm/vmx.c|12401| <<nested_vmx_load_msr>> if (kvm_set_msr(vcpu, &msr)) {
+ *   - arch/x86/kvm/x86.c|1290| <<do_set_msr>> return kvm_set_msr(vcpu, &msr);
+ *   - arch/x86/kvm/x86.c|5728| <<emulator_set_msr>> return kvm_set_msr(emul_to_vcpu(ctxt), &msr);
+ */
 int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 {
 	switch (msr->index) {
@@ -4112,6 +4120,11 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		if (kvm_state.flags == KVM_STATE_NESTED_RUN_PENDING)
 			break;
 
+		/*
+		 * vcpu来自一开始的struct kvm_vcpu *vcpu = filp->private_data;
+		 *
+		 * vmx_set_nested_state()
+		 */
 		r = kvm_x86_ops->set_nested_state(vcpu, user_kvm_nested_state, &kvm_state);
 		break;
 	}
@@ -5470,6 +5483,11 @@ static int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,
 	return 0;
 }
 
+/*
+ * struct x86_emulate_ops emulate_ops.pio_out_emulated = emulator_pio_out_emulated()
+ *
+ * - arch/x86/kvm/x86.c|6378| <<kvm_fast_pio_out>> int ret = emulator_pio_out_emulated(&vcpu->arch.emulate_ctxt,
+ */
 static int emulator_pio_out_emulated(struct x86_emulate_ctxt *ctxt,
 				     int size, unsigned short port,
 				     const void *val, unsigned int count)
@@ -7361,11 +7379,23 @@ static void process_smi(struct kvm_vcpu *vcpu)
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|274| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/ioapic.c|333| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+ *   - arch/x86/kvm/ioapic.c|682| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/irq_comm.c|405| <<kvm_arch_post_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/lapic.c|243| <<recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+ */
 void kvm_make_scan_ioapic_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7537| <<vcpu_enter_guest>> vcpu_scan_ioapic(vcpu);
+ */
 static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_apic_present(vcpu))
@@ -7451,6 +7481,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7813| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 67b9568613f3..1de81a03cad7 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -228,6 +228,18 @@ static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx.c|7517| <<handle_cr>> val = kvm_register_readl(vcpu, reg);
+ *   - arch/x86/kvm/vmx.c|7642| <<handle_dr>> if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
+ *   - arch/x86/kvm/vmx.c|9188| <<handle_vmread>> field = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
+ *   - arch/x86/kvm/vmx.c|9241| <<handle_vmwrite>> field_value = kvm_register_readl(vcpu,
+ *   - arch/x86/kvm/vmx.c|9255| <<handle_vmwrite>> field = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
+ *   - arch/x86/kvm/vmx.c|9462| <<handle_invept>> type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
+ *   - arch/x86/kvm/vmx.c|9525| <<handle_invvpid>> type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
+ *   - arch/x86/kvm/vmx.c|9609| <<handle_invpcid>> type = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);
+ *   - arch/x86/kvm/vmx.c|10021| <<nested_vmx_exit_handled_cr>> val = kvm_register_readl(vcpu, reg);
+ */
 static inline unsigned long kvm_register_readl(struct kvm_vcpu *vcpu,
 					       enum kvm_reg reg)
 {
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 30b0d52b8641..5e296dded7a9 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -815,6 +815,11 @@ acpi_os_write_pci_configuration(struct acpi_pci_id * pci_id, u32 reg,
 	return (result ? AE_ERROR : AE_OK);
 }
 
+/*
+ * 在以下使用acpi_os_execute_deferred():
+ *   - drivers/acpi/osl.c|1078| <<acpi_os_execute>> INIT_WORK(&dpc->work, acpi_os_execute_deferred);
+ *   - drivers/acpi/osl.c|1081| <<acpi_os_execute>> INIT_WORK(&dpc->work, acpi_os_execute_deferred);
+ */
 static void acpi_os_execute_deferred(struct work_struct *work)
 {
 	struct acpi_os_dpc *dpc = container_of(work, struct acpi_os_dpc, work);
@@ -1032,6 +1037,43 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * [0] acpi_os_execute
+ * [0] acpi_ev_gpe_dispatch
+ * [0] acpi_ev_gpe_detect
+ * [0] acpi_ev_sci_xrupt_handler
+ * [0] acpi_irq
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_fasteoi_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ *
+ * [0] acpi_os_execute
+ * [0] acpi_ev_queue_notify_request
+ * [0] acpi_ex_opcode_2A_0T_0R
+ * [0] acpi_ds_exec_end_op
+ * [0] acpi_ps_parse_loop
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_os_execute
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1135,6 +1177,20 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/bus.c|450| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|392| <<acpi_eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 773a3fea8f0e..0adcab5f8b73 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -314,6 +314,9 @@ void tap_del_queues(struct tap_dev *tap)
 }
 EXPORT_SYMBOL_GPL(tap_del_queues);
 
+/*
+ * 被__netif_receive_skb_core()调用
+ */
 rx_handler_result_t tap_handle_frame(struct sk_buff **pskb)
 {
 	struct sk_buff *skb = *pskb;
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index e0baea2dfd3c..bbc2f2e1ae5a 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -854,6 +854,37 @@ static int tun_net_close(struct net_device *dev)
 }
 
 /* Net device start xmit */
+/*
+ * tun_net_xmit
+ * sch_direct_xmit
+ * __dev_queue_xmit
+ * dev_queue_xmit
+ * br_dev_queue_push_xmit
+ * br_forward_finish
+ * __br_forward
+ * deliver_clone
+ * br_flood
+ * br_handle_frame_finish
+ * br_handle_frame
+ * __netif_receive_skb_core
+ * __netif_receive_skb
+ * netif_receive_skb_internal
+ * napi_gro_receive
+ * ixgbe_clean_rx_irq
+ * ixgbe_poll
+ * net_rx_action
+ * __softirqentry_text_start
+ * irq_exit
+ * do_IRQ
+ * ret_from_intr
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ */
 static netdev_tx_t tun_net_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct tun_struct *tun = netdev_priv(dev);
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 693a0f045aa5..d3a12fc2ce81 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -2424,6 +2424,10 @@ static bool is_xdp_raw_buffer_queue(struct virtnet_info *vi, int q)
 		return false;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2939| <<remove_vq_common>> free_unused_bufs(vi);
+ */
 static void free_unused_bufs(struct virtnet_info *vi)
 {
 	void *buf;
diff --git a/drivers/pci/hotplug/acpiphp_glue.c b/drivers/pci/hotplug/acpiphp_glue.c
index 711875afdd70..2857e228af55 100644
--- a/drivers/pci/hotplug/acpiphp_glue.c
+++ b/drivers/pci/hotplug/acpiphp_glue.c
@@ -647,6 +647,13 @@ static void trim_stale_devices(struct pci_dev *dev)
  * Iterate over all slots under this bridge and make sure that if a
  * card is present they are enabled, and if not they are disabled.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/acpiphp_glue.c|727| <<acpiphp_check_host_bridge>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|757| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|767| <<hotplug_event>> acpiphp_check_bridge(bridge);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|774| <<hotplug_event>> acpiphp_check_bridge(func->parent);
+ */
 static void acpiphp_check_bridge(struct acpiphp_bridge *bridge)
 {
 	struct acpiphp_slot *slot;
diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index 42ba2de2bcf2..c7e9a46a3c6d 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -1006,6 +1006,18 @@ static void pci_ea_fixed_busnrs(struct pci_dev *dev, u8 *secondary,
  * them, we proceed to assigning numbers to the remaining buses in
  * order to avoid overlaps between old and new bus numbers.
  */
+/*
+ * [0] pci_scan_bridge
+ * [0] enable_slot
+ * [0] acpiphp_check_bridge
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 int pci_scan_bridge(struct pci_bus *bus, struct pci_dev *dev, int max, int pass)
 {
 	struct pci_bus *child;
@@ -2505,6 +2517,10 @@ static bool embedded_discard_found_device(int devfn, u32 l)
  * Read the config data for a PCI device, sanity-check it
  * and fill in the dev structure...
  */
+/*
+ * called by:
+ *   - drivers/pci/probe.c|2689| <<pci_scan_single_device>> dev = pci_scan_device(bus, devfn);
+ */
 static struct pci_dev *pci_scan_device(struct pci_bus *bus, int devfn)
 {
 	struct pci_dev *dev;
diff --git a/drivers/pci/setup-bus.c b/drivers/pci/setup-bus.c
index 958da7db9033..d5f7f3ce81df 100644
--- a/drivers/pci/setup-bus.c
+++ b/drivers/pci/setup-bus.c
@@ -738,6 +738,10 @@ int pci_claim_bridge_resource(struct pci_dev *bridge, int i)
 /* Check whether the bridge supports optional I/O and
    prefetchable memory ranges. If not, the respective
    base/limit registers must be read-only and read as 0. */
+/*
+ * called by:
+ *   - drivers/pci/setup-bus.c|1251| <<__pci_bus_size_bridges>> pci_bridge_check_ranges(bus);
+ */
 static void pci_bridge_check_ranges(struct pci_bus *bus)
 {
 	u16 io;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 9ffc85f47445..909f52d913e9 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -1199,6 +1199,12 @@ void scsi_del_cmd_from_list(struct scsi_cmnd *cmd)
 }
 
 /* Called after a request has been started. */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|2307| <<scsi_ioctl_reset>> scsi_init_command(dev, scmd);
+ *   - drivers/scsi/scsi_lib.c|1403| <<scsi_prep_fn>> scsi_init_command(sdev, cmd);
+ *   - drivers/scsi/scsi_lib.c|1933| <<scsi_mq_prep_fn>> scsi_init_command(sdev, cmd);
+ */
 void scsi_init_command(struct scsi_device *dev, struct scsi_cmnd *cmd)
 {
 	void *buf = cmd->sense_buffer;
@@ -1917,6 +1923,10 @@ static inline blk_status_t prep_to_mq(int ret)
 }
 
 /* Size in bytes of the sg-list stored in the scsi-mq command-private data. */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|2026| <<scsi_queue_rq>> ret = prep_to_mq(scsi_mq_prep_fn(req));
+ */
 static unsigned int scsi_mq_sgl_size(struct Scsi_Host *shost)
 {
 	return min_t(unsigned int, shost->sg_tablesize, SG_CHUNK_SIZE) *
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 45d04631888a..26978476d197 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -144,6 +144,12 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|238| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|248| <<virtscsi_poll_requests>> virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|578| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -238,6 +244,10 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|649| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
@@ -417,6 +427,10 @@ static void virtscsi_event_done(struct virtqueue *vq)
  * @req_size	: size of the request buffer
  * @resp_size	: size of the response buffer
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|485| <<virtscsi_kick_cmd>> err = virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -462,6 +476,11 @@ static int virtscsi_add_cmd(struct virtqueue *vq,
 	return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|569| <<virtscsi_queuecommand>> ret = virtscsi_kick_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd));
+ *   - drivers/scsi/virtio_scsi.c|587| <<virtscsi_tmf>> if (virtscsi_kick_cmd(&vscsi->ctrl_vq, cmd,
+ */
 static int virtscsi_kick_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size)
@@ -531,6 +550,34 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 	return &vscsi->req_vqs[hwq];
 }
 
+/*
+ * [0] virtqueue_notify
+ * [0] virtscsi_kick_cmd
+ * [0] virtscsi_queuecommand
+ * [0] scsi_dispatch_cmd
+ * [0] scsi_queue_rq
+ * [0] blk_mq_dispatch_rq_list
+ * [0] blk_mq_do_dispatch_sched
+ * [0] blk_mq_sched_dispatch_requests
+ * [0] __blk_mq_run_hw_queue
+ * [0] __blk_mq_delay_run_hw_queue
+ * [0] blk_mq_run_hw_queue
+ * [0] blk_mq_flush_plug_list
+ * [0] blk_flush_plug_list
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_direct_write
+ * [0] __generic_file_write_iter
+ * [0] blkdev_write_iter
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index baea42621146..8d7102b674c0 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -87,6 +87,11 @@ struct vhost_net_ubuf_ref {
 	struct vhost_virtqueue *vq;
 };
 
+/*
+ * 在以下使用VHOST_RX_BATCH=64:
+ *   - drivers/vhost/net.c|162| <<vhost_net_buf_produce>> VHOST_RX_BATCH);
+ *   - drivers/vhost/net.c|925| <<vhost_net_open>> queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
+ */
 #define VHOST_RX_BATCH 64
 struct vhost_net_buf {
 	struct sk_buff **queue;
@@ -96,18 +101,66 @@ struct vhost_net_buf {
 
 struct vhost_net_virtqueue {
 	struct vhost_virtqueue vq;
+	/*
+	 * 在以下使用vhost_net_virtqueue->vhost_hlen:
+	 *   - drivers/vhost/net.c|293| <<vhost_net_vq_reset>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|502| <<handle_tx>> hdr_size = nvq->vhost_hlen;
+	 *   - drivers/vhost/net.c|801| <<handle_rx>> vhost_hlen = nvq->vhost_hlen;
+	 *   - drivers/vhost/net.c|984| <<vhost_net_open>> n->vqs[i].vhost_hlen = 0;
+	 *   - drivers/vhost/net.c|1304| <<vhost_net_set_features>> n->vqs[i].vhost_hlen = vhost_hlen;
+	 */
 	size_t vhost_hlen;
+	/*
+	 * 在以下使用vhost_net_virtqueue->sock_hlen:
+	 *   - drivers/vhost/net.c|294| <<vhost_net_vq_reset>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|802| <<handle_rx>> sock_hlen = nvq->sock_hlen;
+	 *   - drivers/vhost/net.c|985| <<vhost_net_open>> n->vqs[i].sock_hlen = 0;
+	 *   - drivers/vhost/net.c|1305| <<vhost_net_set_features>> n->vqs[i].sock_hlen = sock_hlen;
+	 */
 	size_t sock_hlen;
 	/* vhost zerocopy support fields below: */
 	/* last used idx for outstanding DMA zerocopy buffers */
+	/*
+	 * 在以下使用vhost_net_virtqueue->upend_idx:
+	 *   - drivers/vhost/net.c|291| <<vhost_net_vq_reset>> n->vqs[i].upend_idx = 0;
+	 *   - drivers/vhost/net.c|342| <<vhost_zerocopy_signal_used>> for (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {
+	 *   - drivers/vhost/net.c|466| <<vhost_exceeds_maxpend>> return (nvq->upend_idx + vq->num - VHOST_MAX_PEND) % UIO_MAXIOV
+	 *   - drivers/vhost/net.c|549| <<handle_tx>> && (nvq->upend_idx + 1) % UIO_MAXIOV !=
+	 *   - drivers/vhost/net.c|556| <<handle_tx>> ubuf = nvq->ubuf_info + nvq->upend_idx;
+	 *   - drivers/vhost/net.c|558| <<handle_tx>> vq->heads[nvq->upend_idx].id = cpu_to_vhost32(vq, head);
+	 *   - drivers/vhost/net.c|559| <<handle_tx>> vq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;
+	 *   - drivers/vhost/net.c|562| <<handle_tx>> ubuf->desc = nvq->upend_idx;
+	 *   - drivers/vhost/net.c|568| <<handle_tx>> nvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;
+	 *   - drivers/vhost/net.c|588| <<handle_tx>> nvq->upend_idx = ((unsigned )nvq->upend_idx - 1)
+	 *   - drivers/vhost/net.c|982| <<vhost_net_open>> n->vqs[i].upend_idx = 0;
+	 */
 	int upend_idx;
 	/* first used idx for DMA done zerocopy buffers */
+	/*
+	 * 在以下使用vhost_net_virtqueue->done_idx:
+	 *   - drivers/vhost/net.c|290| <<vhost_net_vq_reset>> n->vqs[i].done_idx = 0;
+	 *   - drivers/vhost/net.c|342| <<vhost_zerocopy_signal_used>> for (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {
+	 *   - drivers/vhost/net.c|352| <<vhost_zerocopy_signal_used>> add = min(UIO_MAXIOV - nvq->done_idx, j);
+	 *   - drivers/vhost/net.c|354| <<vhost_zerocopy_signal_used>> &vq->heads[nvq->done_idx], add);
+	 *   - drivers/vhost/net.c|355| <<vhost_zerocopy_signal_used>> nvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;
+	 *   - drivers/vhost/net.c|467| <<vhost_exceeds_maxpend>> == nvq->done_idx;
+	 *   - drivers/vhost/net.c|983| <<vhost_net_open>> n->vqs[i].done_idx = 0;
+	 */
 	int done_idx;
 	/* an array of userspace buffers info */
 	struct ubuf_info *ubuf_info;
 	/* Reference counting for outstanding ubufs.
 	 * Protected by vq mutex. Writers must also take device mutex. */
 	struct vhost_net_ubuf_ref *ubufs;
+	/*
+	 * 在以下使用vhost_net_virtqueue->rx_array:
+	 *   - drivers/vhost/net.c|171| <<vhost_net_buf_produce>> rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
+	 *   - drivers/vhost/net.c|180| <<vhost_net_buf_unproduce>> if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
+	 *   - drivers/vhost/net.c|181| <<vhost_net_buf_unproduce>> skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
+	 *   - drivers/vhost/net.c|617| <<peek_head_len>> if (rvq->rx_array)
+	 *   - drivers/vhost/net.c|829| <<handle_rx>> if (nvq->rx_array)
+	 *   - drivers/vhost/net.c|1191| <<vhost_net_set_backend>> nvq->rx_array = get_tap_skb_array(fd);
+	 */
 	struct skb_array *rx_array;
 	struct vhost_net_buf rxq;
 };
@@ -121,6 +174,13 @@ struct vhost_net {
 	unsigned tx_packets;
 	/* Number of times zerocopy TX recently failed.
 	 * Protected by tx vq lock. */
+	/*
+	 * 在以下使用vhost_net->tx_zcopy_err:
+	 *   - drivers/vhost/net.c|306| <<vhost_net_tx_packet>> net->tx_zcopy_err = 0;
+	 *   - drivers/vhost/net.c|311| <<vhost_net_tx_err>> ++net->tx_zcopy_err;
+	 *   - drivers/vhost/net.c|320| <<vhost_net_tx_select_zcopy>> net->tx_packets / 64 >= net->tx_zcopy_err;
+	 *   - drivers/vhost/net.c|1203| <<vhost_net_set_backend>> n->tx_zcopy_err = 0
+	 */
 	unsigned tx_zcopy_err;
 	/* Flush in progress. Protected by tx vq lock. */
 	bool tx_flush;
@@ -128,6 +188,10 @@ struct vhost_net {
 
 static unsigned vhost_net_zcopy_mask __read_mostly;
 
+/*
+ * 如果vhost_net_buf->tail != vhost_net_buf->head, 返回vhost_net_buf->queue[rxq->head]
+ * 否则返回NULL
+ */
 static void *vhost_net_buf_get_ptr(struct vhost_net_buf *rxq)
 {
 	if (rxq->tail != rxq->head)
@@ -136,37 +200,98 @@ static void *vhost_net_buf_get_ptr(struct vhost_net_buf *rxq)
 		return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|240| <<vhost_net_buf_unproduce>> vhost_net_buf_get_size(rxq));
+ *
+ * 返回rxq->tail - rxq->head
+ */
 static int vhost_net_buf_get_size(struct vhost_net_buf *rxq)
 {
 	return rxq->tail - rxq->head;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|238| <<vhost_net_buf_unproduce>> if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
+ *   - drivers/vhost/net.c|253| <<vhost_net_buf_peek>> if (!vhost_net_buf_is_empty(rxq))
+ *
+ * 返回rxq->tail == rxq->head
+ */
 static int vhost_net_buf_is_empty(struct vhost_net_buf *rxq)
 {
 	return rxq->tail == rxq->head;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|892| <<handle_rx>> msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
+ *
+ * 核心思想是返回rxq->queue[rxq->head], 增加rxq->head
+ */
 static void *vhost_net_buf_consume(struct vhost_net_buf *rxq)
 {
+	/*
+	 * 如果rxq->tail != rxq->head, 返回rxq->queue[rxq->head]
+	 * 否则返回NULL
+	 */
 	void *ret = vhost_net_buf_get_ptr(rxq);
 	++rxq->head;
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|256| <<vhost_net_buf_peek>> if (!vhost_net_buf_produce(nvq))
+ *
+ * 把nvq->rx_array的ptr_ring的数据放入rxq->queue, 最多VHOST_RX_BATCH=64个
+ * 最后更新并返回的rxq->tail是从ptr_ring放入参数array的数目
+ */
 static int vhost_net_buf_produce(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
 
 	rxq->head = 0;
+	/*
+	 * struct vhost_net_virtqueue *nvq:
+	 *  -> struct skb_array *rx_array; --->
+	 *      -> struct ptr_ring ring;
+	 *  -> struct vhost_net_buf rxq;
+	 *      -> struct sk_buff **queue; --->
+	 *      -> int tail;
+	 *      -> int head;
+	 *
+	 * 在以下使用VHOST_RX_BATCH=64:
+	 *   - drivers/vhost/net.c|162| <<vhost_net_buf_produce>> VHOST_RX_BATCH);
+	 *   - drivers/vhost/net.c|925| <<vhost_net_open>> queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
+	 *
+	 * 把nvq->rx_array的ptr_ring的数据放入rxq->queue, 最多VHOST_RX_BATCH=64个
+	 *
+	 * 返回值是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+	 */
 	rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
 					      VHOST_RX_BATCH);
 	return rxq->tail;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1071| <<vhost_net_stop_vq>> vhost_net_buf_unproduce(nvq);
+ *   - drivers/vhost/net.c|1251| <<vhost_net_set_backend>> vhost_net_buf_unproduce(nvq);
+ */
 static void vhost_net_buf_unproduce(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
 
+	/*
+	 * struct vhost_net_virtqueue *nvq:
+	 *  -> struct skb_array *rx_array; --->
+	 *      -> struct ptr_ring ring;
+	 *  -> struct vhost_net_buf rxq;
+	 *      -> struct sk_buff **queue; --->
+	 *      -> int tail;
+	 *      -> int head;
+	 */
 	if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
 		skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
 				    vhost_net_buf_get_size(rxq));
@@ -174,20 +299,44 @@ static void vhost_net_buf_unproduce(struct vhost_net_virtqueue *nvq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|618| <<peek_head_len>> return vhost_net_buf_peek(rvq);
+ *
+ * 如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+ * 否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+ */
 static int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
 
+	/*
+	 * 返回rxq->tail == rxq->head
+	 */
 	if (!vhost_net_buf_is_empty(rxq))
 		goto out;
 
+	/*
+	 * 把nvq->rx_array的ptr_ring的数据放入rxq->queue, 最多VHOST_RX_BATCH=64个
+	 * 最后更新并返回的rxq->tail是从ptr_ring放入参数array的数目
+	 */
 	if (!vhost_net_buf_produce(nvq))
 		return 0;
 
 out:
+	/*
+	 * vhost_net_buf_get_ptr():
+	 * 如果vhost_net_buf->tail != vhost_net_buf->head, 返回vhost_net_buf->queue[rxq->head]
+	 * 否则返回NULL
+	 */
 	return __skb_array_len_with_tag(vhost_net_buf_get_ptr(rxq));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|280| <<vhost_net_vq_reset>> vhost_net_buf_init(&n->vqs[i].rxq);
+ *   - drivers/vhost/net.c|946| <<vhost_net_open>> vhost_net_buf_init(&n->vqs[i].rxq);
+ */
 static void vhost_net_buf_init(struct vhost_net_buf *rxq)
 {
 	rxq->head = rxq->tail = 0;
@@ -382,6 +531,12 @@ static bool vhost_can_busy_poll(struct vhost_dev *dev,
 	       !vhost_has_work(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|772| <<handle_rx>> vhost_net_disable_vq(net, vq);
+ *   - drivers/vhost/net.c|967| <<vhost_net_stop_vq>> vhost_net_disable_vq(n, vq);
+ *   - drivers/vhost/net.c|1147| <<vhost_net_set_backend>> vhost_net_disable_vq(n, vq);
+ */
 static void vhost_net_disable_vq(struct vhost_net *n,
 				 struct vhost_virtqueue *vq)
 {
@@ -393,6 +548,12 @@ static void vhost_net_disable_vq(struct vhost_net *n,
 	vhost_poll_stop(poll);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|866| <<handle_rx>> vhost_net_enable_vq(net, vq);
+ *   - drivers/vhost/net.c|1155| <<vhost_net_set_backend>> r = vhost_net_enable_vq(n, vq);
+ *   - drivers/vhost/net.c|1186| <<vhost_net_set_backend>> vhost_net_enable_vq(n, vq);
+ */
 static int vhost_net_enable_vq(struct vhost_net *n,
 				struct vhost_virtqueue *vq)
 {
@@ -581,16 +742,45 @@ static void handle_tx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|743| <<vhost_net_rx_peek_head_len>> int len = peek_head_len(rvq, sk);
+ *   - drivers/vhost/net.c|769| <<vhost_net_rx_peek_head_len>> len = peek_head_len(rvq, sk);
+ *
+ * 如果sock支持skb_array:
+ *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+ *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+ * 如果sock不支持skb_array:
+ *     从sk->sk_receive_queue取skb ... ...
+ */
 static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 {
 	struct sk_buff *head;
 	int len = 0;
 	unsigned long flags;
 
+	/*
+	 * 在以下使用vhost_net_virtqueue->rx_array:
+	 *   - drivers/vhost/net.c|1191| <<vhost_net_set_backend>> nvq->rx_array = get_tap_skb_array(fd);
+	 * 在以下使用vhost_net_virtqueue->rx_array:
+	 *   - drivers/vhost/net.c|171| <<vhost_net_buf_produce>> rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
+	 *   - drivers/vhost/net.c|180| <<vhost_net_buf_unproduce>> if (nvq->rx_array && !vhost_net_buf_is_empty(rxq)) {
+	 *   - drivers/vhost/net.c|181| <<vhost_net_buf_unproduce>> skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
+	 *   - drivers/vhost/net.c|617| <<peek_head_len>> if (rvq->rx_array)
+	 *   - drivers/vhost/net.c|829| <<handle_rx>> if (nvq->rx_array)
+	 *
+	 *
+	 * 关于vhost_net_buf_peek():
+	 * 如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+	 * 否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+	 */
 	if (rvq->rx_array)
 		return vhost_net_buf_peek(rvq);
 
 	spin_lock_irqsave(&sk->sk_receive_queue.lock, flags);
+	/*
+	 * struct sk_buff *head;
+	 */
 	head = skb_peek(&sk->sk_receive_queue);
 	if (likely(head)) {
 		len = head->len;
@@ -602,6 +792,10 @@ static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 	return len;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|754| <<vhost_net_rx_peek_head_len>> !sk_has_rx_data(sk) &&
+ */
 static int sk_has_rx_data(struct sock *sk)
 {
 	struct socket *sock = sk->sk_socket;
@@ -612,12 +806,30 @@ static int sk_has_rx_data(struct sock *sk)
 	return skb_queue_empty(&sk->sk_receive_queue);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|903| <<handle_rx>> while ((sock_len = vhost_net_rx_peek_head_len(net, sock->sk))) {
+ *
+ * 核心思想!!
+ * 如果sock支持skb_array:
+ *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+ *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+ * 如果sock不支持skb_array:
+ *     从sk->sk_receive_queue取skb ... ...
+ */
 static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 {
 	struct vhost_net_virtqueue *rvq = &net->vqs[VHOST_NET_VQ_RX];
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
 	struct vhost_virtqueue *vq = &nvq->vq;
 	unsigned long uninitialized_var(endtime);
+	/*
+	 * 如果sock支持skb_array:
+	 *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+	 *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+	 * 如果sock不支持skb_array:
+	 *     从sk->sk_receive_queue取skb ... ...
+	 */
 	int len = peek_head_len(rvq, sk);
 
 	if (!len && vq->busyloop_timeout) {
@@ -644,6 +856,13 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
 
 		mutex_unlock(&vq->mutex);
 
+		/*
+		 * 如果sock支持skb_array:
+		 *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+		 *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+		 * 如果sock不支持skb_array:
+		 *     从sk->sk_receive_queue取skb ... ...
+		 */
 		len = peek_head_len(rvq, sk);
 	}
 
@@ -660,6 +879,14 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk)
  * @quota       - headcount quota, 1 for big buffer
  *	returns number of buffer heads allocated, negative on error
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|906| <<handle_rx>> headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ *
+ * 1019                 headcount = get_rx_bufs(vq, vq->heads, vhost_len,
+ * 1020                                         &in, vq_log, &log,
+ * 1021                                         likely(mergeable) ? UIO_MAXIOV : 1);
+ */
 static int get_rx_bufs(struct vhost_virtqueue *vq,
 		       struct vring_used_elem *heads,
 		       int datalen,
@@ -683,6 +910,13 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			r = -ENOBUFS;
 			goto err;
 		}
+		/*
+		 * struct vhost_virtqueue *vq:
+		 *   -> struct iovec iov[UIO_MAXIOV];
+		 *   -> struct vring_used_elem *heads;
+		 *
+		 * seg一开始初始化的时候是0
+		 */
 		r = vhost_get_vq_desc(vq, vq->iov + seg,
 				      ARRAY_SIZE(vq->iov) - seg, &out,
 				      &in, log, log_num);
@@ -704,6 +938,9 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			nlogs += *log_num;
 			log += *log_num;
 		}
+		/*
+		 * 函数一开始的时候headcount是0
+		 */
 		heads[headcount].id = cpu_to_vhost32(vq, d);
 		len = iov_length(vq->iov + seg, in);
 		heads[headcount].len = cpu_to_vhost32(vq, len);
@@ -729,6 +966,11 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|881| <<handle_rx_kick>> handle_rx(net);
+ *   - drivers/vhost/net.c|895| <<handle_rx_net>> handle_rx(net);
+ */
 static void handle_rx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];
@@ -773,9 +1015,21 @@ static void handle_rx(struct vhost_net *net)
 		vq->log : NULL;
 	mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
 
+	/*
+	 * vhost_net_rx_peek_head_len()核心思想!!
+	 * 如果sock支持skb_array:
+	 *     如果vhost_net_buf中不是empty的, 就立刻返回vhost_net_buf->queue[vhost_net_buf->head]的skb->len
+	 *     否则先从tap/tun的skb_array中取一些放入vhost_net_buf再返回skb->len
+	 * 如果sock不支持skb_array:
+	 *     从sk->sk_receive_queue取skb ... ...
+	 */
 	while ((sock_len = vhost_net_rx_peek_head_len(net, sock->sk))) {
 		sock_len += sock_hlen;
 		vhost_len = sock_len + vhost_hlen;
+		/*
+		 * struct vhost_virtqueue *vq = &nvq->vq;
+		 *   -> struct vring_used_elem *heads;
+		 */
 		headcount = get_rx_bufs(vq, vq->heads, vhost_len,
 					&in, vq_log, &log,
 					likely(mergeable) ? UIO_MAXIOV : 1);
@@ -794,6 +1048,9 @@ static void handle_rx(struct vhost_net *net)
 			 * they refilled. */
 			goto out;
 		}
+		/*
+		 * 核心思想是返回rxq->queue[rxq->head], 增加rxq->head
+		 */
 		if (nvq->rx_array)
 			msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
 		/* On overrun, truncate and discard */
@@ -813,6 +1070,9 @@ static void handle_rx(struct vhost_net *net)
 			 */
 			iov_iter_advance(&msg.msg_iter, vhost_hlen);
 		}
+		/*
+		 * 对于macvlan/macvtap是tap_recvmsg
+		 */
 		err = sock->ops->recvmsg(sock, &msg,
 					 sock_len, MSG_DONTWAIT | MSG_TRUNC);
 		/* Userspace might have consumed the packet meanwhile:
@@ -895,6 +1155,13 @@ static void handle_rx_net(struct vhost_work *work)
 	handle_rx(net);
 }
 
+/*
+ * 真正有用的:
+ *   - handle_tx_kick()
+ *   - handle_rx_net()
+ *
+ * struct file_operations vhost_net_fops.open = vhost_net_open()
+ */
 static int vhost_net_open(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n;
@@ -912,6 +1179,11 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		return -ENOMEM;
 	}
 
+	/*
+	 * 在以下使用VHOST_RX_BATCH=64:
+	 *   - drivers/vhost/net.c|162| <<vhost_net_buf_produce>> VHOST_RX_BATCH);
+	 *   - drivers/vhost/net.c|925| <<vhost_net_open>> queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
+	 */
 	queue = kmalloc_array(VHOST_RX_BATCH, sizeof(struct sk_buff *),
 			      GFP_KERNEL);
 	if (!queue) {
@@ -919,6 +1191,14 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		kvfree(n);
 		return -ENOMEM;
 	}
+	/*
+	 * struct vhost_net *n:
+	 *  -> struct vhost_dev dev;
+	 *  -> struct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];
+	 *      -> struct vhost_virtqueue vq;
+	 *          -> struct vhost_poll poll;
+	 *  -> struct vhost_poll poll[VHOST_NET_VQ_MAX];
+	 */
 	n->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;
 
 	dev = &n->dev;
@@ -991,6 +1271,9 @@ static void vhost_net_flush(struct vhost_net *n)
 	}
 }
 
+/*
+ * struct file_operations vhost_net_fops.release = vhost_net_release()
+ */
 static int vhost_net_release(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n = f->private_data;
@@ -1041,6 +1324,10 @@ static struct socket *get_raw_socket(int fd)
 	return ERR_PTR(r);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1325| <<vhost_net_set_backend>> nvq->rx_array = get_tap_skb_array(fd);
+ */
 static struct skb_array *get_tap_skb_array(int fd)
 {
 	struct skb_array *array;
@@ -1216,6 +1503,10 @@ static long vhost_net_reset_owner(struct vhost_net *n)
 	return err;
 }
 
+/*
+ * 处理VHOST_SET_FEATURES:
+ *   - drivers/vhost/net.c|1318| <<vhost_net_ioctl>> return vhost_net_set_features(n, features);
+ */
 static int vhost_net_set_features(struct vhost_net *n, u64 features)
 {
 	size_t vhost_hlen, sock_hlen, hdr_len;
@@ -1280,6 +1571,9 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	return r;
 }
 
+/*
+ * struct file_operations vhost_net_fops.unlocked_ioctl = vhost_net_ioctl()
+ */
 static long vhost_net_ioctl(struct file *f, unsigned int ioctl,
 			    unsigned long arg)
 {
@@ -1323,6 +1617,9 @@ static long vhost_net_ioctl(struct file *f, unsigned int ioctl,
 }
 
 #ifdef CONFIG_COMPAT
+/*
+ * struct file_operations vhost_net_fops.compat_ioctl = vhost_net_compat_ioctl()
+ */
 static long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,
 				   unsigned long arg)
 {
@@ -1330,6 +1627,9 @@ static long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,
 }
 #endif
 
+/*
+ * struct file_operations vhost_net_fops.read_iter = vhost_net_chr_read_iter()
+ */
 static ssize_t vhost_net_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)
 {
 	struct file *file = iocb->ki_filp;
@@ -1340,6 +1640,9 @@ static ssize_t vhost_net_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)
 	return vhost_chr_read_iter(dev, to, noblock);
 }
 
+/*
+ * struct file_operations vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ */
 static ssize_t vhost_net_chr_write_iter(struct kiocb *iocb,
 					struct iov_iter *from)
 {
@@ -1350,6 +1653,9 @@ static ssize_t vhost_net_chr_write_iter(struct kiocb *iocb,
 	return vhost_chr_write_iter(dev, from);
 }
 
+/*
+ * struct file_operations vhost_net_fops.poll = vhost_net_chr_poll()
+ */
 static unsigned int vhost_net_chr_poll(struct file *file, poll_table *wait)
 {
 	struct vhost_net *n = file->private_data;
@@ -1358,6 +1664,9 @@ static unsigned int vhost_net_chr_poll(struct file *file, poll_table *wait)
 	return vhost_chr_poll(file, dev, wait);
 }
 
+/*
+ * struct miscdevice vhost_net_misc.fops = vhost_net_fops()
+ */
 static const struct file_operations vhost_net_fops = {
 	.owner          = THIS_MODULE,
 	.release        = vhost_net_release,
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 145dd3125da0..d891bfb8f401 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -281,21 +281,35 @@ static void vhost_scsi_put_inflight(struct vhost_scsi_inflight *inflight)
 	kref_put(&inflight->kref, vhost_scsi_done_inflight);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode = vhost_scsi_check_true()
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode_cache = vhost_scsi_check_true()
+ */
 static int vhost_scsi_check_true(struct se_portal_group *se_tpg)
 {
 	return 1;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode_write_protect = vhost_scsi_check_false()
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_prod_mode_write_protect = vhost_scsi_check_false()
+ */
 static int vhost_scsi_check_false(struct se_portal_group *se_tpg)
 {
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.get_fabric_name = vhost_scsi_get_fabric_name()
+ */
 static char *vhost_scsi_get_fabric_name(void)
 {
 	return "vhost";
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_wwn = vhost_scsi_get_fabric_wwn()
+ */
 static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -305,6 +319,9 @@ static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 	return &tport->tport_name[0];
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_tag = vhost_scsi_get_tpgt()
+ */
 static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -312,6 +329,9 @@ static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 	return tpg->tport_tpgt;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_prot_fabric_only = vhost_scsi_check_prot_fabric_only()
+ */
 static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -320,11 +340,17 @@ static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 	return tpg->tv_fabric_prot_type;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_inst_index = vhost_scsi_tpg_get_inst_index()
+ */
 static u32 vhost_scsi_tpg_get_inst_index(struct se_portal_group *se_tpg)
 {
 	return 1;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.release_cmd = vhost_scsi_release_cmd()
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *tv_cmd = container_of(se_cmd,
@@ -345,11 +371,17 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 	target_free_tag(se_sess, se_cmd);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.sess_get_index = vhost_scsi_sess_get_index()
+ */
 static u32 vhost_scsi_sess_get_index(struct se_session *se_sess)
 {
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.write_pending = vhost_scsi_write_pending()
+ */
 static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 {
 	/* Go ahead and process the write immediately */
@@ -357,16 +389,25 @@ static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.write_pending_status = vhost_scsi_write_pending_status()
+ */
 static int vhost_scsi_write_pending_status(struct se_cmd *se_cmd)
 {
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.set_default_node_attributes = vhost_scsi_set_default_node_attrs()
+ */
 static void vhost_scsi_set_default_node_attrs(struct se_node_acl *nacl)
 {
 	return;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.get_cmd_state = vhost_scsi_get_cmd_state()
+ */
 static int vhost_scsi_get_cmd_state(struct se_cmd *se_cmd)
 {
 	return 0;
@@ -381,6 +422,9 @@ static void vhost_scsi_complete_cmd(struct vhost_scsi_cmd *cmd)
 	vhost_work_queue(&vs->dev, &vs->vs_completion_work);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_data_in = vhost_scsi_queue_data_in()
+ */
 static int vhost_scsi_queue_data_in(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -389,6 +433,9 @@ static int vhost_scsi_queue_data_in(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_status = vhost_scsi_queue_status()
+ */
 static int vhost_scsi_queue_status(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -397,11 +444,17 @@ static int vhost_scsi_queue_status(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_tm_rsp = vhost_scsi_queue_tm_rsp()
+ */
 static void vhost_scsi_queue_tm_rsp(struct se_cmd *se_cmd)
 {
 	return;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.aborted_task = vhost_scsi_aborted_task()
+ */
 static void vhost_scsi_aborted_task(struct se_cmd *se_cmd)
 {
 	return;
@@ -448,6 +501,9 @@ static void vhost_scsi_free_cmd(struct vhost_scsi_cmd *cmd)
 
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.check_stop_free = vhost_scsi_check_stop_free()
+ */
 static int vhost_scsi_check_stop_free(struct se_cmd *se_cmd)
 {
 	return target_put_sess_cmd(se_cmd);
@@ -823,6 +879,11 @@ static void vhost_scsi_submission_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1162| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ *   - drivers/vhost/scsi.c|1310| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ */
 static void
 vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 			   struct vhost_virtqueue *vq,
@@ -842,12 +903,28 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1046| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1294| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * struct vhost_scsi_ctx {
+	 *     int head;
+	 *     unsigned int out, in;
+	 *     size_t req_size, rsp_size;
+	 *     size_t out_size, in_size;
+	 *     u8 *target, *lunp;
+	 *     void *req;
+	 *     struct iov_iter out_iter;
+	 * };
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -900,6 +977,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1075| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1353| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, NULL);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
@@ -940,6 +1022,16 @@ vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 	return ret;
 }
 
+/*
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|1368| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -957,7 +1049,11 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	bool t10_pi = vhost_has_feature(vq, VIRTIO_SCSI_F_T10_PI);
 	void *cdb;
 
+	/*
+	 * struct vhost_virtqueue *vq
+	 */
 	mutex_lock(&vq->mutex);
+
 	/*
 	 * We can handle the vq only after the endpoint is setup by calling the
 	 * VHOST_SCSI_SET_ENDPOINT ioctl.
@@ -972,6 +1068,11 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	for (;;) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1046| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *   - drivers/vhost/scsi.c|1294| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
@@ -1359,6 +1460,16 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 在以下使用vhost_scsi_handle_kick():
+ *   - drivers/vhost/scsi.c|1657| <<vhost_scsi_open>> vs->vqs[i].vq.handle_kick = vhost_scsi_handle_kick;
+ */
 static void vhost_scsi_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1844,6 +1955,9 @@ static void vhost_scsi_hotunplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 	vhost_scsi_do_plug(tpg, lun, false);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_post_link = vhost_scsi_port_link()
+ */
 static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 			       struct se_lun *lun)
 {
@@ -1863,6 +1977,9 @@ static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_pre_unlink = vhost_scsi_port_unlink()
+ */
 static void vhost_scsi_port_unlink(struct se_portal_group *se_tpg,
 				  struct se_lun *lun)
 {
@@ -1931,6 +2048,9 @@ static ssize_t vhost_scsi_tpg_attrib_fabric_prot_type_show(
 
 CONFIGFS_ATTR(vhost_scsi_tpg_attrib_, fabric_prot_type);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_attrib_attrs = vhost_scsi_tpg_attrib_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrib_attrs[] = {
 	&vhost_scsi_tpg_attrib_attr_fabric_prot_type,
 	NULL,
@@ -2163,11 +2283,17 @@ static ssize_t vhost_scsi_tpg_nexus_store(struct config_item *item,
 
 CONFIGFS_ATTR(vhost_scsi_tpg_, nexus);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_base_attrs = vhost_scsi_tpg_attrs[]
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrs[] = {
 	&vhost_scsi_tpg_attr_nexus,
 	NULL,
 };
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_tpg = vhost_scsi_make_tpg()
+ */
 static struct se_portal_group *
 vhost_scsi_make_tpg(struct se_wwn *wwn,
 		   struct config_group *group,
@@ -2207,6 +2333,9 @@ vhost_scsi_make_tpg(struct se_wwn *wwn,
 	return &tpg->se_tpg;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_tpg = vhost_scsi_drop_tpg()
+ */
 static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -2226,6 +2355,9 @@ static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 	kfree(tpg);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_wwn = vhost_scsi_make_tport()
+ */
 static struct se_wwn *
 vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 		     struct config_group *group,
@@ -2287,6 +2419,9 @@ vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 	return &tport->tport_wwn;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_wwn = vhost_scsi_drop_tport()
+ */
 static void vhost_scsi_drop_tport(struct se_wwn *wwn)
 {
 	struct vhost_scsi_tport *tport = container_of(wwn,
@@ -2309,11 +2444,19 @@ vhost_scsi_wwn_version_show(struct config_item *item, char *page)
 
 CONFIGFS_ATTR_RO(vhost_scsi_wwn_, version);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_wwn_attrs = vhost_scsi_wwn_attrs[]
+ */
 static struct configfs_attribute *vhost_scsi_wwn_attrs[] = {
 	&vhost_scsi_wwn_attr_version,
 	NULL,
 };
 
+/*
+ * 在以下使用vhost_scsi_ops:
+ *   - drivers/vhost/scsi.c|2469| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/vhost/scsi.c|2485| <<vhost_scsi_exit>> target_unregister_template(&vhost_scsi_ops);
+ */
 static const struct target_core_fabric_ops vhost_scsi_ops = {
 	.module				= THIS_MODULE,
 	.name				= "vhost",
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index a59fefc6e5ea..180ceeeb61f8 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -34,20 +34,86 @@
 
 #include "vhost.h"
 
+/*
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] eventfd_signal
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] eventfd_signal
+ * [0] vhost_add_used_and_signal_n
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ */
+
+/*
+ * 在以下使用max_mem_regions:
+ *   - drivers/vhost/vhost.c|1333| <<vhost_set_memory>> if (mem.nregions > max_mem_regions)
+ */
 static ushort max_mem_regions = 64;
 module_param(max_mem_regions, ushort, 0444);
 MODULE_PARM_DESC(max_mem_regions,
 	"Maximum number of memory regions in memory map. (default: 64)");
+/*
+ * 在以下使用max_iotlb_entries:
+ *   - drivers/vhost/vhost.c|958| <<vhost_new_umem_range>> if (umem->numem == max_iotlb_entries) {
+ */
 static int max_iotlb_entries = 2048;
 module_param(max_iotlb_entries, int, 0444);
 MODULE_PARM_DESC(max_iotlb_entries,
 	"Maximum number of iotlb entries. (default: 2048)");
 
 enum {
+	/* 没人用 ??? */
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * last_avail_idx表示前端处理到avail ring的哪个元素了
+ * ++之后表示下次待处理的avail ring的哪个元素
+ * 并将这个信息放入了used ring的最后一个元素
+ * 前端驱动通过读取used ring的最后一个元素就知道后端处理到哪里了
+ */
+
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2328| <<vhost_notify>> if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1799| <<vhost_update_avail_event>> vhost_avail_event(vq)))
+ *   - drivers/vhost/vhost.c|1806| <<vhost_update_avail_event>> used = vhost_avail_event(vq);
+ *   - drivers/vhost/vhost.c|1809| <<vhost_update_avail_event>> sizeof *vhost_avail_event(vq));
+ *   - drivers/vhost/vhost.c|2426| <<vhost_enable_notify>> vhost_avail_event(vq), r);
+ */
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
 INTERVAL_TREE_DEFINE(struct vhost_umem_node,
@@ -55,6 +121,9 @@ INTERVAL_TREE_DEFINE(struct vhost_umem_node,
 		     START, LAST, static inline, vhost_umem_interval_tree);
 
 #ifdef CONFIG_VHOST_CROSS_ENDIAN_LEGACY
+/*
+ * !!! 好多的CONFIG_VHOST_CROSS_ENDIAN_LEGACY都没设置!!!
+ */
 static void vhost_disable_cross_endian(struct vhost_virtqueue *vq)
 {
 	vq->user_be = !virtio_legacy_is_little_endian();
@@ -70,6 +139,10 @@ static void vhost_enable_cross_endian_little(struct vhost_virtqueue *vq)
 	vq->user_be = false;
 }
 
+/*
+ * 处理VHOST_SET_VRING_ENDIAN:
+ *   - drivers/vhost/vhost.c|1566| <<vhost_vring_ioctl>> r = vhost_set_vring_endian(vq, argp);
+ */
 static long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)
 {
 	struct vhost_vring_state s;
@@ -92,6 +165,10 @@ static long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)
 	return 0;
 }
 
+/*
+ * 处理VHOST_GET_VRING_ENDIAN:
+ *   - drivers/vhost/vhost.c|1569| <<vhost_vring_ioctl>> r = vhost_get_vring_endian(vq, idx, argp);
+ */
 static long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,
 				   int __user *argp)
 {
@@ -125,19 +202,39 @@ static long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)
 	return -ENOIOCTLCMD;
 }
 
+/*
+ * 处理VHOST_GET_VRING_ENDIAN:
+ *   - drivers/vhost/vhost.c|2146| <<vhost_vring_ioctl>> r = vhost_get_vring_endian(vq, idx, argp);
+ */
 static long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,
 				   int __user *argp)
 {
 	return -ENOIOCTLCMD;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|207| <<vhost_reset_is_le>> vhost_init_is_le(vq);
+ *   - drivers/vhost/vhost.c|2438| <<vhost_vq_init_access>> vhost_init_is_le(vq);
+ *
+ * 计算vq->is_le. 很多is_le是true
+ */
 static void vhost_init_is_le(struct vhost_virtqueue *vq)
 {
+	/*
+	 * 很多is_le是true
+	 */
 	vq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1)
 		|| virtio_legacy_is_little_endian();
 }
 #endif /* CONFIG_VHOST_CROSS_ENDIAN_LEGACY */
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|338| <<vhost_vq_reset>> vhost_reset_is_le(vq);
+ *
+ * 计算vq->is_le. 很多is_le是true
+ */
 static void vhost_reset_is_le(struct vhost_virtqueue *vq)
 {
 	vhost_init_is_le(vq);
@@ -145,27 +242,177 @@ static void vhost_reset_is_le(struct vhost_virtqueue *vq)
 
 struct vhost_flush_struct {
 	struct vhost_work work;
+	/*
+	 * 在以下使用vhost_flush_struct->wait_event:
+	 *   - drivers/vhost/vhost.c|191| <<vhost_flush_work>> complete(&s->wait_event);
+	 *   - drivers/vhost/vhost.c|378| <<vhost_work_flush>> init_completion(&flush.wait_event);
+	 *   - drivers/vhost/vhost.c|382| <<vhost_work_flush>> wait_for_completion(&flush.wait_event);
+	 */
 	struct completion wait_event;
 };
 
+/*
+ * 在以下使用vhost_flush_work():
+ *   - drivers/vhost/vhost.c|253| <<vhost_work_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ */
 static void vhost_flush_work(struct vhost_work *work)
 {
 	struct vhost_flush_struct *s;
 
 	s = container_of(work, struct vhost_flush_struct, work);
+	/*
+	 * 在以下使用vhost_flush_struct->wait_event:
+	 *   - drivers/vhost/vhost.c|191| <<vhost_flush_work>> complete(&s->wait_event);
+	 *   - drivers/vhost/vhost.c|378| <<vhost_work_flush>> init_completion(&flush.wait_event);
+	 *   - drivers/vhost/vhost.c|382| <<vhost_work_flush>> wait_for_completion(&flush.wait_event);
+	 */
 	complete(&s->wait_event);
 }
 
+/*
+ * [0] vhost_poll_func
+ * [0] eventfd_poll
+ * [0] vhost_poll_start.part.19
+ * [0] vhost_vring_ioctl
+ * [0] vhost_scsi_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vhost_poll_func
+ * [0] vhost_poll_start.part.19
+ * [0] vhost_poll_start
+ * [0] vhost_net_enable_vq
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|232| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ *
+ * 这个函数被配置给了poll_table->_qproc
+ * socket和eventfd的poll会调用poll_wait()-->调用poll_table->_qproc=vhost_poll_func()
+ *
+ * 把&poll->wait加入到wqh
+ *
+ * 对于socket: 把n->poll->wait加入到socket的wqh
+ * 对于eventfd, 把vhost_virtqueue->poll->wait加入到socket的wqh
+ *
+ *
+ * file->poll()的时候调用这个函数
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt, unsigned long unused)
 {
 	struct vhost_poll *poll;
 
+	/*
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	poll = container_of(pt, struct vhost_poll, table);
 	poll->wqh = wqh;
+	/*
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以对于socket和eventfd, 唤醒的waitqueue的func都是vhost_poll_wakeup()
+	 *
+	 * 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程
+	 *
+	 * socket的入口: tun_net_xmit()-->sock_def_readable()
+	 * eventfd的入口: handle_ept_misconfig()-->kvm_io_bus_write()-->eventfd_signal()
+	 *
+	 * struct vhost_poll {
+	 *     poll_table                table;
+	 *     wait_queue_head_t        *wqh;
+	 *     wait_queue_entry_t              wait;
+	 *     struct vhost_work         work;
+	 *     unsigned long             mask;
+	 *     struct vhost_dev         *dev;
+	 * };
+	 *
+	 * 把entry的&poll->wait加入head的wqh
+	 */
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * [0] vhost_poll_wakeup
+ * [0] __wake_up_common
+ * [0] __wake_up_locked_key
+ * [0] eventfd_signal
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * vhost_poll_wakeup()的例子:
+ * [0] vhost_poll_wakeup
+ * [0] __wake_up_common_lock
+ * [0] __wake_up_sync_key
+ * [0] sock_def_readable
+ * [0] __dta_tun_net_xmit_80
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __dev_queue_xmit
+ * [0] dev_queue_xmit
+ * [0] br_dev_queue_push_xmit
+ * [0] br_forward_finish
+ * [0] __br_forward
+ * [0] deliver_clone
+ * [0] br_flood
+ * [0] br_handle_frame_finish
+ * [0] br_handle_frame
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] ixgbe_clean_rx_irq
+ * [0] ixgbe_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|231| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|253| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, (void *)mask);
+ *
+ * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+ * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+ *
+ * 所以对于socket和eventfd, 唤醒的waitqueue的func都是vhost_poll_wakeup()
+ *
+ * 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程
+ *
+ * socket的入口: tun_net_xmit()-->sock_def_readable()
+ * eventfd的入口: handle_ept_misconfig()-->kvm_io_bus_write()-->eventfd_signal()
+ *
+ * 
+ * 作为wait entry的handler在poll被唤醒的时候调用
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -174,10 +421,22 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	if (!((unsigned long)key & poll->mask))
 		return 0;
 
+	/*
+	 * 把vhost_work挂到vhost_dev上然后唤醒内核线程
+	 */
 	vhost_poll_queue(poll);
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1645| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/scsi.c|1646| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/vhost.c|237| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|280| <<vhost_work_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vhost.c|517| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ *   - drivers/vhost/vsock.c|539| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -187,21 +446,78 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
 /* Init poll structure */
+/*
+ * called by:
+ *   - vhost_net_open(), 初始化n->poll
+ *   - vhost_net_open(), 初始化n->poll
+ *   - vhost_dev_init(), 初始化vhost_virtqueue->poll
+ *
+ * called by:
+ *   - drivers/vhost/net.c|950| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);
+ *   - drivers/vhost/net.c|951| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);
+ *   - drivers/vhost/vhost.c|529| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick,
+ */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     unsigned long mask, struct vhost_dev *dev)
 {
+	/*
+	 * 把vhost_poll->wait的handler设置成vhost_poll_wakeup
+	 * 这样当vhost_poll在fd上被唤醒的时候就调用vhost_poll_wakeup()
+	 */
 	init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	/*
+	 * 这个函数被配置给了poll_table->_qproc
+	 * socket和eventfd的poll会调用poll_wait()-->调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * 把&poll->wait加入到wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到socket的wqh
+	 *
+	 *
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	init_poll_funcptr(&poll->table, vhost_poll_func);
 	poll->mask = mask;
 	poll->dev = dev;
+	/*
+	 * 在以下使用vhost_poll->wqh:
+	 *   - drivers/vhost/vhost.c|261| <<vhost_poll_func>> poll->wqh = wqh;
+	 *   - drivers/vhost/vhost.c|274| <<vhost_poll_func>> add_wait_queue(wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|376| <<vhost_poll_init>> poll->wqh = NULL;
+	 *   - drivers/vhost/vhost.c|400| <<vhost_poll_start>> if (poll->wqh)
+	 *   - drivers/vhost/vhost.c|439| <<vhost_poll_stop>> if (poll->wqh) {
+	 *   - drivers/vhost/vhost.c|440| <<vhost_poll_stop>> remove_wait_queue(poll->wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|441| <<vhost_poll_stop>> poll->wqh = NULL
+	 */
 	poll->wqh = NULL;
 
+	/*
+	 * fn可能是:
+	 *   - handle_tx_net()
+	 *   - handle_rx_net()
+	 *   - vq->handle_kick
+	 */
 	vhost_work_init(&poll->work, fn);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|408| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/vhost.c|1669| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *
+ * - vhost_net_enable_vq()  ---> poll在sock->file
+ *   这里poll是n->poll, 是handle_rx_net()
+ *
+ * - vhost_vring_ioctl():VHOST_SET_VRING_KICK  --->设置host notifier, poll在eventfd
+ *   这里poll是vhost_virtqueue->poll, 是handle_tx_kick()
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	unsigned long mask;
@@ -210,6 +526,25 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (poll->wqh)
 		return 0;
 
+	/*
+	 * socket : tun_chr_poll() --> poll_wait()
+	 * eventfd: eventfd_poll() --> poll_wait()
+	 *
+	 * poll_wait()就是调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * vhost_poll_func()把&poll->wait加入到fd的wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以到时候waitqueue唤醒的时候都是调用vhost_poll_wakeup()
+	 *
+	 *
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	mask = file->f_op->poll(file, &poll->table);
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, (void *)mask);
@@ -224,8 +559,25 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|393| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|273| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|649| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1661| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
+	/*
+	 * struct vhost_poll {
+	 *     poll_table                table;
+	 *     wait_queue_head_t        *wqh;
+	 *     wait_queue_entry_t              wait;
+	 *     struct vhost_work         work;
+	 *     unsigned long             mask;
+	 *     struct vhost_dev         *dev;
+	 * };
+	 */
 	if (poll->wqh) {
 		remove_wait_queue(poll->wqh, &poll->wait);
 		poll->wqh = NULL;
@@ -233,6 +585,14 @@ void vhost_poll_stop(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_stop);
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1396| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1397| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|310| <<vhost_poll_flush>> vhost_work_flush(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|564| <<vhost_attach_cgroups>> vhost_work_flush(dev, &attach.work);
+ *   - drivers/vhost/vsock.c|554| <<vhost_vsock_flush>> vhost_work_flush(&vsock->dev, &vsock->send_pkt_work);
+ */
 void vhost_work_flush(struct vhost_dev *dev, struct vhost_work *work)
 {
 	struct vhost_flush_struct flush;
@@ -249,12 +609,111 @@ EXPORT_SYMBOL_GPL(vhost_work_flush);
 
 /* Flush any work that has been scheduled. When calling this, don't hold any
  * locks that are also used by the callback. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|983| <<vhost_net_flush_vq>> vhost_poll_flush(n->poll + index);
+ *   - drivers/vhost/net.c|984| <<vhost_net_flush_vq>> vhost_poll_flush(&n->vqs[index].vq.poll);
+ *   - drivers/vhost/scsi.c|1373| <<vhost_scsi_flush_vq>> vhost_poll_flush(&vs->vqs[index].vq.poll);
+ *   - drivers/vhost/test.c|145| <<vhost_test_flush_vq>> vhost_poll_flush(&n->vqs[index].poll);
+ *   - drivers/vhost/vhost.c|650| <<vhost_dev_stop>> vhost_poll_flush(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1674| <<vhost_vring_ioctl>> vhost_poll_flush(&vq->poll);
+ *   - drivers/vhost/vsock.c|553| <<vhost_vsock_flush>> vhost_poll_flush(&vsock->vqs[i].poll);
+ * 
+ * 测试的没找到调用
+ */
 void vhost_poll_flush(struct vhost_poll *poll)
 {
 	vhost_work_flush(poll->dev, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_flush);
 
+/*
+ * [0] vhost_work_queue
+ * [0] vhost_zerocopy_callback
+ * [0] skb_release_data
+ * [0] skb_release_all
+ * [0] napi_consume_skb
+ * [0] ixgbe_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * [0] vhost_work_queue
+ * [0] __wake_up_common
+ * [0] __wake_up_locked_key
+ * [0] eventfd_signal
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vhost_work_queue
+ * [0] __wake_up_common
+ * [0] __wake_up_common_lock
+ * [0] __wake_up_sync_key
+ * [0] sock_def_readable
+ * [0] __dta_tun_net_xmit_80
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __dev_queue_xmit
+ * [0] dev_queue_xmit
+ * [0] br_dev_queue_push_xmit
+ * [0] br_forward_finish
+ * [0] __br_forward
+ * [0] deliver_clone
+ * [0] br_flood
+ * [0] br_handle_frame_finish
+ * [0] br_handle_frame
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] ixgbe_clean_rx_irq
+ * [0] ixgbe_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|381| <<vhost_scsi_complete_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1343| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|282| <<vhost_work_flush>> vhost_work_queue(dev, &flush.work);
+ *   - drivers/vhost/vhost.c|321| <<vhost_poll_queue>> vhost_work_queue(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|518| <<vhost_attach_cgroups>> vhost_work_queue(dev, &attach.work);
+ *   - drivers/vhost/vhost.h|42| <<vhost_attach_cgroups>> void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work);
+ *   - drivers/vhost/vsock.c|223| <<vhost_transport_send_pkt>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ *
+ * 把vhost_work挂到vhost_dev上然后唤醒内核线程
+ */
 void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 {
 	if (!dev->worker)
@@ -265,6 +724,14 @@ void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 		 * sure it was not in the list.
 		 * test_and_set_bit() implies a memory barrier.
 		 */
+		/*
+		 * 在以下使用vhost_dev->work_list:
+		 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+		 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+		 */
 		llist_add(&work->node, &dev->work_list);
 		wake_up_process(dev->worker);
 	}
@@ -272,18 +739,44 @@ void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 EXPORT_SYMBOL_GPL(vhost_work_queue);
 
 /* A lockless hint for busy polling code to exit the loop */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|382| <<vhost_can_busy_poll>> !vhost_has_work(dev);
+ */
 bool vhost_has_work(struct vhost_dev *dev)
 {
 	return !llist_empty(&dev->work_list);
 }
 EXPORT_SYMBOL_GPL(vhost_has_work);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|366| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|576| <<handle_tx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|639| <<vhost_net_rx_peek_head_len>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|642| <<vhost_net_rx_peek_head_len>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|862| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/test.c|85| <<handle_vq>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|215| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|1025| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vsock.c|186| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|266| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *
+ * 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
+	/* 把poll->work (vhost_work)挂到poll->dev (vhost_dev)上然后唤醒vhost_dev的内核线程 */
 	vhost_work_queue(poll->dev, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_queue);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|721| <<vhost_vq_meta_reset>> __vhost_vq_meta_reset(d->vqs[i]);
+ *   - drivers/vhost/vhost.c|753| <<vhost_vq_reset>> __vhost_vq_meta_reset(vq);
+ *   - drivers/vhost/vhost.c|2201| <<vhost_init_device_iotlb>> __vhost_vq_meta_reset(vq);
+ */
 static void __vhost_vq_meta_reset(struct vhost_virtqueue *vq)
 {
 	int j;
@@ -292,6 +785,11 @@ static void __vhost_vq_meta_reset(struct vhost_virtqueue *vq)
 		vq->meta_iotlb[j] = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1554| <<vhost_process_iotlb_msg>> vhost_vq_meta_reset(dev);
+ *   - drivers/vhost/vhost.c|1564| <<vhost_process_iotlb_msg>> vhost_vq_meta_reset(dev);
+ */
 static void vhost_vq_meta_reset(struct vhost_dev *d)
 {
 	int i;
@@ -300,6 +798,11 @@ static void vhost_vq_meta_reset(struct vhost_dev *d)
 		__vhost_vq_meta_reset(d->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|892| <<vhost_dev_init>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|1086| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -332,6 +835,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	__vhost_vq_meta_reset(vq);
 }
 
+/*
+ * 在以下创建kthread的时候使用:
+ *   - drivers/vhost/vhost.c|843| <<vhost_dev_set_owner>> worker = kthread_create(vhost_worker, dev, "vhost-%d", current->pid);
+ */
 static int vhost_worker(void *data)
 {
 	struct vhost_dev *dev = data;
@@ -351,6 +858,14 @@ static int vhost_worker(void *data)
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_dev->work_list:
+		 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+		 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+		 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+		 */
 		node = llist_del_all(&dev->work_list);
 		if (!node)
 			schedule();
@@ -382,6 +897,10 @@ static void vhost_vq_free_iovecs(struct vhost_virtqueue *vq)
 }
 
 /* Helper to allocate iovec buffers for all vqs. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|970| <<vhost_dev_set_owner>> err = vhost_dev_alloc_iovecs(dev);
+ */
 static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)
 {
 	struct vhost_virtqueue *vq;
@@ -412,6 +931,13 @@ static void vhost_dev_free_iovecs(struct vhost_dev *dev)
 		vhost_vq_free_iovecs(dev->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|948| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);
+ *   - drivers/vhost/scsi.c|1659| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ);
+ *   - drivers/vhost/test.c|119| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX);
+ *   - drivers/vhost/vsock.c|534| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs)
 {
@@ -427,8 +953,24 @@ void vhost_dev_init(struct vhost_dev *dev,
 	dev->iotlb = NULL;
 	dev->mm = NULL;
 	dev->worker = NULL;
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	init_llist_head(&dev->work_list);
 	init_waitqueue_head(&dev->wait);
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	INIT_LIST_HEAD(&dev->read_list);
 	INIT_LIST_HEAD(&dev->pending_list);
 	spin_lock_init(&dev->iotlb_lock);
@@ -442,6 +984,14 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
 		vhost_vq_reset(dev, vq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|950| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, POLLOUT, dev);
+		 *   - drivers/vhost/net.c|951| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, POLLIN, dev);
+		 *   - drivers/vhost/vhost.c|529| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick,
+		 *
+		 * 下面是POLLIN, 不是POLLOUT
+		 */
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
 					POLLIN, dev);
@@ -450,6 +1000,16 @@ void vhost_dev_init(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_dev_init);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1386| <<vhost_net_set_backend>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/net.c|1479| <<vhost_net_reset_owner>> err = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|178| <<vhost_test_run>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|226| <<vhost_test_reset_owner>> err = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/vhost.c|2263| <<vhost_dev_ioctl>> r = vhost_dev_check_owner(d);
+ *   - drivers/vhost/vsock.c|434| <<vhost_vsock_start>> ret = vhost_dev_check_owner(&vsock->dev);
+ *   - drivers/vhost/vsock.c|484| <<vhost_vsock_stop>> ret = vhost_dev_check_owner(&vsock->dev);
+ */
 long vhost_dev_check_owner(struct vhost_dev *dev)
 {
 	/* Are you the owner? If not, I don't think you mean to do that */
@@ -471,6 +1031,10 @@ static void vhost_attach_cgroups_work(struct vhost_work *work)
 	s->ret = cgroup_attach_task_all(s->owner, current);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1002| <<vhost_dev_set_owner>> err = vhost_attach_cgroups(dev);
+ */
 static int vhost_attach_cgroups(struct vhost_dev *dev)
 {
 	struct vhost_attach_cgroups_struct attach;
@@ -533,6 +1097,11 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1487| <<vhost_net_reset_owner>> umem = vhost_dev_reset_owner_prepare();
+ *   - drivers/vhost/test.c|229| <<vhost_test_reset_owner>> umem = vhost_dev_reset_owner_prepare();
+ */
 struct vhost_umem *vhost_dev_reset_owner_prepare(void)
 {
 	return kvzalloc(sizeof(struct vhost_umem), GFP_KERNEL);
@@ -540,6 +1109,11 @@ struct vhost_umem *vhost_dev_reset_owner_prepare(void)
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1490| <<vhost_net_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ *   - drivers/vhost/test.c|236| <<vhost_test_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ */
 void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_umem *umem)
 {
 	int i;
@@ -557,6 +1131,13 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_umem *umem)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1285| <<vhost_net_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/net.c|1494| <<vhost_net_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/scsi.c|1780| <<vhost_scsi_release>> vhost_dev_stop(&vs->dev);
+ *   - drivers/vhost/vsock.c|593| <<vhost_vsock_dev_release>> vhost_dev_stop(&vsock->dev);
+ */
 void vhost_dev_stop(struct vhost_dev *dev)
 {
 	int i;
@@ -570,6 +1151,16 @@ void vhost_dev_stop(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_stop);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1082| <<vhost_umem_clean>> vhost_umem_free(umem, node);
+ *   - drivers/vhost/vhost.c|1485| <<vhost_new_umem_range>> vhost_umem_free(umem, tmp);
+ *   - drivers/vhost/vhost.c|1528| <<vhost_del_umem_range>> vhost_umem_free(umem, node);
+ *
+ * 把vhost_umem_node从umem->umem_list删除
+ * 再从umem->umem_tree删除
+ * 减少umem->numem
+ */
 static void vhost_umem_free(struct vhost_umem *umem,
 			    struct vhost_umem_node *node)
 {
@@ -579,6 +1170,14 @@ static void vhost_umem_free(struct vhost_umem *umem,
 	umem->numem--;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1177| <<vhost_dev_cleanup>> vhost_umem_clean(dev->umem);
+ *   - drivers/vhost/vhost.c|1179| <<vhost_dev_cleanup>> vhost_umem_clean(dev->iotlb);
+ *   - drivers/vhost/vhost.c|2210| <<vhost_set_memory>> vhost_umem_clean(oldumem);
+ *   - drivers/vhost/vhost.c|2214| <<vhost_set_memory>> vhost_umem_clean(newumem);
+ *   - drivers/vhost/vhost.c|2459| <<vhost_init_device_iotlb>> vhost_umem_clean(oiotlb);
+ */
 static void vhost_umem_clean(struct vhost_umem *umem)
 {
 	struct vhost_umem_node *node, *tmp;
@@ -592,12 +1191,24 @@ static void vhost_umem_clean(struct vhost_umem *umem)
 	kvfree(umem);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1177| <<vhost_dev_cleanup>> vhost_clear_msg(dev);
+ */
 static void vhost_clear_msg(struct vhost_dev *dev)
 {
 	struct vhost_msg_node *node, *n;
 
 	spin_lock(&dev->iotlb_lock);
 
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	list_for_each_entry_safe(node, n, &dev->read_list, node) {
 		list_del(&node->node);
 		kfree(node);
@@ -612,6 +1223,14 @@ static void vhost_clear_msg(struct vhost_dev *dev)
 }
 
 /* Caller should have device mutex if and only if locked is set */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1286| <<vhost_net_release>> vhost_dev_cleanup(&n->dev, false);
+ *   - drivers/vhost/scsi.c|1781| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev, false);
+ *   - drivers/vhost/test.c|160| <<vhost_test_release>> vhost_dev_cleanup(&n->dev, false);
+ *   - drivers/vhost/vhost.c|1066| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev, true);
+ *   - drivers/vhost/vsock.c|606| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev, false);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)
 {
 	int i;
@@ -643,6 +1262,14 @@ void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)
 	dev->iotlb = NULL;
 	vhost_clear_msg(dev);
 	wake_up_interruptible_poll(&dev->wait, POLLIN | POLLRDNORM);
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	WARN_ON(!llist_empty(&dev->work_list));
 	if (dev->worker) {
 		kthread_stop(dev->worker);
@@ -674,6 +1301,11 @@ static bool vhost_overflow(u64 uaddr, u64 size)
 }
 
 /* Caller should have vq mutex and device mutex. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1194| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base,
+ *   - drivers/vhost/vhost.c|1768| <<vq_log_access_ok>> return vq_memory_access_ok(log_base, vq->umem,
+ */
 static int vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,
 			       int log_all)
 {
@@ -700,10 +1332,23 @@ static int vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1292| <<vhost_copy_to_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1330| <<vhost_copy_from_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1392| <<__vhost_get_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1839| <<iotlb_access_ok>> if (vhost_vq_meta_fetch(vq, addr, len, type))
+ */
 static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 					       u64 addr, unsigned int size,
 					       int type)
 {
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[]:
+	 *   - drivers/vhost/vhost.c|713| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1176| <<vhost_vq_meta_fetch>> const struct vhost_umem_node *node = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1693| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = node;
+	 */
 	const struct vhost_umem_node *node = vq->meta_iotlb[type];
 
 	if (!node)
@@ -714,6 +1359,11 @@ static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 
 /* Can we switch to this memory table? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1766| <<vhost_log_access_ok>> return memory_access_ok(dev, dev->umem, 1);
+ *   - drivers/vhost/vhost.c|1883| <<vhost_set_memory>> if (!memory_access_ok(d, newumem, 0))
+ */
 static int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,
 			    int log_all)
 {
@@ -741,6 +1391,10 @@ static int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2220| <<__vhost_add_used_n>> } else if (vhost_copy_to_user(vq, used, heads, count * sizeof *used)) {
+ */
 static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 			      const void *from, unsigned size)
 {
@@ -776,6 +1430,10 @@ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2864| <<vhost_get_vq_desc>> ret = vhost_copy_from_user(vq, &desc, vq->desc + i,
+ */
 static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
 				void __user *from, unsigned size)
 {
@@ -859,6 +1517,16 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	return __vhost_get_user_slow(vq, addr, size, type);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1762| <<vhost_update_used_flags>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+ *   - drivers/vhost/vhost.c|1781| <<vhost_update_avail_event>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+ *   - drivers/vhost/vhost.c|2195| <<__vhost_add_used_n>> if (vhost_put_user(vq, heads[0].id, &used->id)) {
+ *   - drivers/vhost/vhost.c|2199| <<__vhost_add_used_n>> if (vhost_put_user(vq, heads[0].len, &used->len)) {
+ *   - drivers/vhost/vhost.c|2252| <<vhost_add_used_n>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+ *
+ * 把第二参数x给第三个参数的ptr
+ */
 #define vhost_put_user(vq, x, ptr)		\
 ({ \
 	int ret = -EFAULT; \
@@ -876,6 +1544,13 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	ret; \
 })
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|898| <<vhost_get_avail>> vhost_get_user(vq, x, ptr, VHOST_ADDR_AVAIL)
+ *   - drivers/vhost/vhost.c|901| <<vhost_get_used>> vhost_get_user(vq, x, ptr, VHOST_ADDR_USED)
+ *
+ * 把ptr的内容读入x
+ */
 #define vhost_get_user(vq, x, ptr, type)		\
 ({ \
 	int ret; \
@@ -894,12 +1569,34 @@ static inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,
 	ret; \
 })
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2762| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail(vq, avail_idx, &vq->avail->idx))) {
+ *   - drivers/vhost/vhost.c|2792| <<vhost_get_vq_desc>> if (unlikely(vhost_get_avail(vq, ring_head,
+ *   - drivers/vhost/vhost.c|3118| <<vhost_notify>> if (vhost_get_avail(vq, flags, &vq->avail->flags)) {
+ *   - drivers/vhost/vhost.c|3151| <<vhost_notify>> if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
+ *   - drivers/vhost/vhost.c|3215| <<vhost_vq_avail_empty>> r = vhost_get_avail(vq, avail_idx, &vq->avail->idx);
+ *   - drivers/vhost/vhost.c|3273| <<vhost_enable_notify>> r = vhost_get_avail(vq, avail_idx, &vq->avail->idx);
+ */
 #define vhost_get_avail(vq, x, ptr) \
 	vhost_get_user(vq, x, ptr, VHOST_ADDR_AVAIL)
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2493| <<vhost_vq_init_access>> r = vhost_get_used(vq, last_used_idx, &vq->used->idx);
+ */
 #define vhost_get_used(vq, x, ptr) \
 	vhost_get_user(vq, x, ptr, VHOST_ADDR_USED)
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1493| <<vhost_process_iotlb_msg>> vhost_dev_lock_vqs(dev);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_dev_lock_vqs()
+ */
 static void vhost_dev_lock_vqs(struct vhost_dev *d)
 {
 	int i = 0;
@@ -907,6 +1604,15 @@ static void vhost_dev_lock_vqs(struct vhost_dev *d)
 		mutex_lock_nested(&d->vqs[i]->mutex, i);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1523| <<vhost_process_iotlb_msg>> vhost_dev_unlock_vqs(dev);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_dev_unlock_vqs()
+ */
 static void vhost_dev_unlock_vqs(struct vhost_dev *d)
 {
 	int i = 0;
@@ -914,17 +1620,57 @@ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
 		mutex_unlock(&d->vqs[i]->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1495| <<vhost_process_iotlb_msg>> if (vhost_new_umem_range(dev->iotlb, msg->iova, msg->size,
+ *   - drivers/vhost/vhost.c|1812| <<vhost_set_memory>> if (vhost_new_umem_range(newumem
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_new_umem_range()
+ *
+ * vhost_dev_ioctl(VHOST_SET_MEM_TABLE)
+ * -> vhost_set_memory()
+ *
+ * 申请新的vhost_umem_node,插入umem->umem_list和umem->umem_tree
+ * 增加umem->numem++
+ */
 static int vhost_new_umem_range(struct vhost_umem *umem,
 				u64 start, u64 size, u64 end,
 				u64 userspace_addr, int perm)
 {
+	/*
+	 * struct vhost_umem_node {
+	 *     struct rb_node rb;
+	 *     struct list_head link;
+	 *     __u64 start;
+	 *     __u64 last;
+	 *     __u64 size;
+	 *     __u64 userspace_addr;
+	 *     __u32 perm;
+	 *    __u32 flags_padding;
+	 *    __u64 __subtree_last;
+	 * };
+	 */
 	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);
 
 	if (!node)
 		return -ENOMEM;
 
+	/*
+	 * 在以下使用max_iotlb_entries:
+	 *   - drivers/vhost/vhost.c|958| <<vhost_new_umem_range>> if (umem->numem == max_iotlb_entries) {
+	 *
+	 * 如果数目已经最大了, 就要删除一个
+	 */
 	if (umem->numem == max_iotlb_entries) {
 		tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+		/*
+		 * 把vhost_umem_node从umem->umem_list删除
+		 * 再从umem->umem_tree删除
+		 * 减少umem->numem
+		 */
 		vhost_umem_free(umem, tmp);
 	}
 
@@ -934,13 +1680,42 @@ static int vhost_new_umem_range(struct vhost_umem *umem,
 	node->userspace_addr = userspace_addr;
 	node->perm = perm;
 	INIT_LIST_HEAD(&node->link);
+	/*
+	 * 在以下使用vhost_umem->umem_list:
+	 *   - drivers/vhost/vhost.c|1006| <<vhost_dev_reset_owner>> INIT_LIST_HEAD(&umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1045| <<vhost_umem_clean>> list_for_each_entry_safe(node, tmp, &umem->umem_list, link)
+	 *   - drivers/vhost/vhost.c|1154| <<vq_memory_access_ok>> list_for_each_entry(node, &umem->umem_list, link) {
+	 *   - drivers/vhost/vhost.c|1422| <<vhost_new_umem_range>> tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+	 *   - drivers/vhost/vhost.c|1432| <<vhost_new_umem_range>> list_add_tail(&node->link, &umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1808| <<vhost_umem_alloc>> INIT_LIST_HEAD(&umem->umem_list);
+	 */
 	list_add_tail(&node->link, &umem->umem_list);
+	/*
+	 * 在以下使用vhost_umem->umem_tree:
+	 *    - drivers/vhost/vhost.c|1032| <<vhost_umem_free>> vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1433| <<vhost_new_umem_range>> vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1444| <<vhost_del_umem_range>> while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1711| <<iotlb_access_ok>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1806| <<vhost_umem_alloc>> umem->umem_tree = RB_ROOT_CACHED;
+	 *    - drivers/vhost/vhost.c|2433| <<translate_desc>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *
+	 * 只在这里调用vhost_umem_interval_tree_insert()
+	 */
 	vhost_umem_interval_tree_insert(node, &umem->umem_tree);
 	umem->numem++;
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1680| <<vhost_process_iotlb_msg>> vhost_del_umem_range(dev->iotlb, msg->iova,
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_del_umem_range()
+ */
 static void vhost_del_umem_range(struct vhost_umem *umem,
 				 u64 start, u64 end)
 {
@@ -951,6 +1726,15 @@ static void vhost_del_umem_range(struct vhost_umem *umem,
 		vhost_umem_free(umem, node);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1634| <<vhost_process_iotlb_msg>> vhost_iotlb_notify_vq(dev, msg);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> vhost_iotlb_notify_vq()
+ */
 static void vhost_iotlb_notify_vq(struct vhost_dev *d,
 				  struct vhost_iotlb_msg *msg)
 {
@@ -972,6 +1756,15 @@ static void vhost_iotlb_notify_vq(struct vhost_dev *d,
 	spin_unlock(&d->iotlb_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1665| <<vhost_process_iotlb_msg>> if (umem_access_ok(msg->uaddr, msg->size, msg->perm)) {
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ *       -> umem_access_ok()
+ */
 static int umem_access_ok(u64 uaddr, u64 size, int access)
 {
 	unsigned long a = uaddr;
@@ -989,6 +1782,14 @@ static int umem_access_ok(u64 uaddr, u64 size, int access)
 	return 0;
 }
 
+/*
+ * called by VHOST_IOTLB_MSG:
+ *   - drivers/vhost/vhost.c|1544| <<vhost_chr_write_iter(VHOST_IOTLB_MSG)>> err = vhost_process_iotlb_msg(dev, &node.msg.iotlb);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ *    -> vhost_process_iotlb_msg()
+ */
 static int vhost_process_iotlb_msg(struct vhost_dev *dev,
 				   struct vhost_iotlb_msg *msg)
 {
@@ -1030,6 +1831,14 @@ static int vhost_process_iotlb_msg(struct vhost_dev *dev,
 
 	return ret;
 }
+
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1636| <<vhost_net_chr_write_iter>> return vhost_chr_write_iter(dev, from);
+ *
+ * vhost_net_fops.write_iter = vhost_net_chr_write_iter()
+ * -> vhost_chr_write_iter()
+ */
 ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 			     struct iov_iter *from)
 {
@@ -1060,6 +1869,10 @@ ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 }
 EXPORT_SYMBOL(vhost_chr_write_iter);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1664| <<vhost_net_chr_poll>> return vhost_chr_poll(file, dev, wait);
+ */
 unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 			    poll_table *wait)
 {
@@ -1067,6 +1880,14 @@ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 
 	poll_wait(file, &dev->wait, wait);
 
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	if (!list_empty(&dev->read_list))
 		mask |= POLLIN | POLLRDNORM;
 
@@ -1074,6 +1895,13 @@ unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 }
 EXPORT_SYMBOL(vhost_chr_poll);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1626| <<vhost_net_chr_read_iter>> return vhost_chr_read_iter(dev, to, noblock);
+ *
+ * vhost_net_fops.read_iter = vhost_net_chr_read_iter()
+ *    -> vhost_chr_read_iter()
+ */
 ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 			    int noblock)
 {
@@ -1090,6 +1918,14 @@ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 			prepare_to_wait(&dev->wait, &wait,
 					TASK_INTERRUPTIBLE);
 
+		/*
+		 * 在以下使用vhost_dev->read_list:
+		 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+		 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+		 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+		 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+		 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+		 */
 		node = vhost_dequeue_msg(dev, &dev->read_list);
 		if (node)
 			break;
@@ -1127,6 +1963,36 @@ ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 }
 EXPORT_SYMBOL_GPL(vhost_chr_read_iter);
 
+/*
+ * This patch tries to implement an device IOTLB for vhost. This could be
+ * used with userspace(qemu) implementation of DMA remapping
+ * to emulate an IOMMU for the guest.
+ *
+ * The idea is simple, cache the translation in a software device IOTLB
+ * (which is implemented as an interval tree) in vhost and use vhost_net
+ * file descriptor for reporting IOTLB miss and IOTLB
+ * update/invalidation. When vhost meets an IOTLB miss, the fault
+ * address, size and access can be read from the file. After userspace
+ * finishes the translation, it writes the translated address to the
+ * vhost_net file to update the device IOTLB.
+ *
+ * When device IOTLB is enabled by setting VIRTIO_F_IOMMU_PLATFORM all vq
+ * addresses set by ioctl are treated as iova instead of virtual address and
+ * the accessing can only be done through IOTLB instead of direct userspace
+ * memory access. Before each round or vq processing, all vq metadata is
+ * prefetched in device IOTLB to make sure no translation fault happens
+ * during vq processing.
+ *
+ * In most cases, virtqueues are contiguous even in virtual address space.
+ * The IOTLB translation for virtqueue itself may make it a little
+ * slower. We might add fast path cache on top of this patch.
+ */
+
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1715| <<iotlb_access_ok>> vhost_iotlb_miss(vq, addr, access);
+ *   - drivers/vhost/vhost.c|2460| <<translate_desc>> vhost_iotlb_miss(vq, addr, access);
+ */
 static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
 {
 	struct vhost_dev *dev = vq->dev;
@@ -1142,11 +2008,24 @@ static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
 	msg->iova = iova;
 	msg->perm = access;
 
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	vhost_enqueue_msg(dev, &dev->read_list, node);
 
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2025| <<vhost_vq_access_ok>> return vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);
+ *   - drivers/vhost/vhost.c|2239| <<vhost_vring_ioctl>> if (!vq_access_ok(vq, vq->num,
+ */
 static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 			struct vring_desc __user *desc,
 			struct vring_avail __user *avail,
@@ -1162,6 +2041,15 @@ static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 			sizeof *used + num * sizeof *used->ring + s);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1723| <<iotlb_access_ok>> vhost_vq_meta_update(vq, node, type);
+ *
+ * handle_tx() or handle_rx()
+ * -> vq_iotlb_prefetch()
+ *    -> iotlb_access_ok()
+ *       -> vhost_vq_meta_update()
+ */
 static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 				 const struct vhost_umem_node *node,
 				 int type)
@@ -1169,10 +2057,26 @@ static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 	int access = (type == VHOST_ADDR_USED) ?
 		     VHOST_ACCESS_WO : VHOST_ACCESS_RO;
 
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[]:
+	 *   - drivers/vhost/vhost.c|713| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1176| <<vhost_vq_meta_fetch>> const struct vhost_umem_node *node = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1693| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = node;
+	 */
 	if (likely(node->perm & access))
 		vq->meta_iotlb[type] = node;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1881| <<vq_iotlb_prefetch>> return iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->desc,
+ *   - drivers/vhost/vhost.c|1883| <<vq_iotlb_prefetch>> iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->avail,
+ *   - drivers/vhost/vhost.c|1887| <<vq_iotlb_prefetch>> iotlb_access_ok(vq, VHOST_ACCESS_WO, (u64)(uintptr_t)vq->used,
+ *
+ * handle_tx() or handle_rx()
+ * -> vq_iotlb_prefetch()
+ *    -> iotlb_access_ok()
+ */
 static int iotlb_access_ok(struct vhost_virtqueue *vq,
 			   int access, u64 addr, u64 len, int type)
 {
@@ -1209,6 +2113,11 @@ static int iotlb_access_ok(struct vhost_virtqueue *vq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|470| <<handle_tx>> if (!vq_iotlb_prefetch(vq))
+ *   - drivers/vhost/net.c|763| <<handle_rx>> if (!vq_iotlb_prefetch(vq))
+ */
 int vq_iotlb_prefetch(struct vhost_virtqueue *vq)
 {
 	size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
@@ -1232,6 +2141,13 @@ EXPORT_SYMBOL_GPL(vq_iotlb_prefetch);
 
 /* Can we log writes? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1530| <<vhost_net_set_features>> !vhost_log_access_ok(&n->dev))
+ *   - drivers/vhost/scsi.c|1712| <<vhost_scsi_set_features>> !vhost_log_access_ok(&vs->dev)) {
+ *   - drivers/vhost/test.c|248| <<vhost_test_set_features>> !vhost_log_access_ok(&n->dev)) {
+ *   - drivers/vhost/vsock.c|653| <<vhost_vsock_set_features>> !vhost_log_access_ok(&vsock->dev)) {
+ */
 int vhost_log_access_ok(struct vhost_dev *dev)
 {
 	return memory_access_ok(dev, dev->umem, 1);
@@ -1240,6 +2156,11 @@ EXPORT_SYMBOL_GPL(vhost_log_access_ok);
 
 /* Verify access for write logging. */
 /* Caller should have vq mutex and device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2018| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+ *   - drivers/vhost/vhost.c|2425| <<vhost_dev_ioctl>> if (vq->private_data && !vq_log_access_ok(vq, base))
+ */
 static int vq_log_access_ok(struct vhost_virtqueue *vq,
 			    void __user *log_base)
 {
@@ -1254,6 +2175,14 @@ static int vq_log_access_ok(struct vhost_virtqueue *vq,
 
 /* Can we start vq? */
 /* Caller should have vq mutex and device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1404| <<vhost_net_set_backend>> if (!vhost_vq_access_ok(vq)) {
+ *   - drivers/vhost/scsi.c|1528| <<vhost_scsi_set_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+ *   - drivers/vhost/scsi.c|1630| <<vhost_scsi_clear_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+ *   - drivers/vhost/test.c|184| <<vhost_test_run>> if (!vhost_vq_access_ok(&n->vqs[index])) {
+ *   - drivers/vhost/vsock.c|443| <<vhost_vsock_start>> if (!vhost_vq_access_ok(vq)) {
+ */
 int vhost_vq_access_ok(struct vhost_virtqueue *vq)
 {
 	if (!vq_log_access_ok(vq, vq->log_base))
@@ -1267,6 +2196,11 @@ int vhost_vq_access_ok(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2094| <<vhost_set_memory>> newumem = vhost_umem_alloc();
+ *   - drivers/vhost/vhost.c|2364| <<vhost_init_device_iotlb>> niotlb = vhost_umem_alloc();
+ */
 static struct vhost_umem *vhost_umem_alloc(void)
 {
 	struct vhost_umem *umem = kvzalloc(sizeof(*umem), GFP_KERNEL);
@@ -1281,9 +2215,28 @@ static struct vhost_umem *vhost_umem_alloc(void)
 	return umem;
 }
 
+/*
+ * 设置VHOST_SET_MEM_TABLE:
+ *   - drivers/vhost/vhost.c|2104| <<vhost_dev_ioctl(VHOST_SET_MEM_TABLE)>> r = vhost_set_memory(d, argp);
+ */
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
+	/*
+	 * struct vhost_memory {
+	 *     __u32 nregions;
+	 *     __u32 padding;
+	 *     struct vhost_memory_region regions[0];
+	 * };
+	 */
 	struct vhost_memory mem, *newmem;
+	/*
+	 * struct vhost_memory_region {
+	 *     __u64 guest_phys_addr;
+	 *     __u64 memory_size; // bytes
+	 *     __u64 userspace_addr;
+	 *     __u64 flags_padding; // No flags are currently specified.
+	 * };
+	 */
 	struct vhost_memory_region *region;
 	struct vhost_umem *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
@@ -1306,6 +2259,13 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 		return -EFAULT;
 	}
 
+	/*
+	 * struct vhost_umem {
+	 *     struct rb_root_cached umem_tree;
+	 *     struct list_head umem_list;
+	 *     int numem;
+	 * };
+	 */
 	newumem = vhost_umem_alloc();
 	if (!newumem) {
 		kvfree(newmem);
@@ -1333,6 +2293,10 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 
 	/* All memory accesses are done under some VQ mutex. */
 	for (i = 0; i < d->nvqs; ++i) {
+		/*
+		 * struct vhost_dev *d:
+		 *   -> struct vhost_virtqueue **vqs;
+		 */
 		mutex_lock(&d->vqs[i]->mutex);
 		d->vqs[i]->umem = newumem;
 		mutex_unlock(&d->vqs[i]->mutex);
@@ -1567,6 +2531,10 @@ long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)
 }
 EXPORT_SYMBOL_GPL(vhost_vring_ioctl);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1534| <<vhost_net_set_features>> if (vhost_init_device_iotlb(&n->dev, true))
+ */
 int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)
 {
 	struct vhost_umem *niotlb, *oiotlb;
@@ -1680,6 +2648,10 @@ EXPORT_SYMBOL_GPL(vhost_dev_ioctl);
  * (instruction directly accesses the data, with an exception table entry
  * returning -EFAULT). See Documentation/x86/exception-tables.txt.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2515| <<log_write>> r = set_bit_to_user(bit, (void __user *)(unsigned long )log);
+ */
 static int set_bit_to_user(int nr, void __user *addr)
 {
 	unsigned long log = (unsigned long)addr;
@@ -1700,6 +2672,14 @@ static int set_bit_to_user(int nr, void __user *addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2356| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+ *   - drivers/vhost/vhost.c|2389| <<vhost_update_used_flags>> log_write(vq->log_base, vq->log_addr +
+ *   - drivers/vhost/vhost.c|2413| <<vhost_update_avail_event>> log_write(vq->log_base, vq->log_addr +
+ *   - drivers/vhost/vhost.c|2960| <<__vhost_add_used_n>> log_write(vq->log_base,
+ *   - drivers/vhost/vhost.c|3037| <<vhost_add_used_n>> log_write(vq->log_base,
+ */
 static int log_write(void __user *log_base,
 		     u64 write_address, u64 write_length)
 {
@@ -1726,6 +2706,14 @@ static int log_write(void __user *log_base,
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1114| <<handle_rx>> vhost_log_write(vq, vq_log, log, vhost_len);
+ *
+ *  handle_rx_kick() or handle_rx_net()
+ *  -> handle_rx()
+ *     -> vhost_log_write()
+ */
 int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 		    unsigned int log_num, u64 len)
 {
@@ -1751,6 +2739,12 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 }
 EXPORT_SYMBOL_GPL(vhost_log_write);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2290| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|2981| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|3028| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+ */
 static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 {
 	void __user *used;
@@ -1771,6 +2765,10 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2988| <<vhost_enable_notify>> r = vhost_update_avail_event(vq, vq->avail_idx);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 {
 	if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
@@ -1791,6 +2789,13 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1424| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|199| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/vsock.c|450| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -1826,11 +2831,36 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * [0] translate_desc
+ * [0] vhost_get_vq_desc
+ * [0] vhost_scsi_get_desc
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|2404| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
+ *   - drivers/vhost/vhost.c|2450| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+ *   - drivers/vhost/vhost.c|2592| <<vhost_get_vq_desc>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+ *
+ * 把desc中的地址转换成qemu userspace的地址存放在iov中
+ *
+ * iov_size是从iov开始剩下可用的iov
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
 	const struct vhost_umem_node *node;
 	struct vhost_dev *dev = vq->dev;
+	/*
+	 * 如果vhost_dev->iotlb没设置, 返回dev->umem
+	 */
 	struct vhost_umem *umem = dev->iotlb ? dev->iotlb : dev->umem;
 	struct iovec *_iov;
 	u64 s = 0;
@@ -1838,14 +2868,29 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 
 	while ((u64)len > s) {
 		u64 size;
+		/*
+		 * ret在while()循环之前一开始的时候是0
+		 */
 		if (unlikely(ret >= iov_size)) {
 			ret = -ENOBUFS;
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_umem->umem_tree:
+		 *    - drivers/vhost/vhost.c|1032| <<vhost_umem_free>> vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+		 *    - drivers/vhost/vhost.c|1433| <<vhost_new_umem_range>> vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+		 *    - drivers/vhost/vhost.c|1444| <<vhost_del_umem_range>> while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+		 *    - drivers/vhost/vhost.c|1711| <<iotlb_access_ok>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+		 *    - drivers/vhost/vhost.c|1806| <<vhost_umem_alloc>> umem->umem_tree = RB_ROOT_CACHED;
+		 *    - drivers/vhost/vhost.c|2433| <<translate_desc>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+		 */
 		node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
 							addr, addr + len - 1);
 		if (node == NULL || node->start > addr) {
+			/*
+			 * 上面如果用的dev->umem, 这里umem就不会等于dev->iotlb
+			 */
 			if (umem != dev->iotlb) {
 				ret = -EFAULT;
 				break;
@@ -1857,9 +2902,36 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			break;
 		}
 
+		/*
+		 * struct vhost_umem_node {
+		 *     struct rb_node rb;
+		 *     struct list_head link;
+		 *     __u64 start;
+		 *     __u64 last;
+		 *     __u64 size;
+		 *     __u64 userspace_addr;
+		 *     __u32 perm;
+		 *     __u32 flags_padding;
+		 *     __u64 __subtree_last;
+		 * };
+		 */
 		_iov = iov + ret;
 		size = node->size - addr + node->start;
 		_iov->iov_len = min((u64)len - s, size);
+		/*
+		 * array_index_nospec - sanitize an array index after a bounds check
+		 *
+		 * For a code sequence like:
+		 *
+		 *     if (index < size) {
+		 *         index = array_index_nospec(index, size);
+		 *         val = array[index];
+		 *     }
+		 *
+		 * ...if the CPU speculates past the bounds check then
+		 * array_index_nospec() will clamp the index within the range of [0,
+		 * size).
+		 */
 		_iov->iov_base = (void __user *)
 			((unsigned long)node->userspace_addr +
 			 array_index_nospec((unsigned long)(addr - node->start),
@@ -1895,6 +2967,10 @@ static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2686| <<vhost_get_vq_desc>> ret = get_indirect(vq, iov, iov_size,
+ */
 static int get_indirect(struct vhost_virtqueue *vq,
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
@@ -2001,6 +3077,17 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|578| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|588| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/net.c|909| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg,
+ *   - drivers/vhost/scsi.c|472| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/scsi.c|851| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/test.c|56| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/vsock.c|112| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/vsock.c|374| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2014,14 +3101,42 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	int ret, access;
 
 	/* Check it isn't doing very strange things with descriptor numbers. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|310| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1413| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1419| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2022| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2143| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2155| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 */
 	last_avail_idx = vq->last_avail_idx;
 
+	/*
+	 * 在以下修改vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|311| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2030| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2330| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 */
 	if (vq->avail_idx == vq->last_avail_idx) {
+		/*
+		 * 从vq->avail->idx获取最新的avail_idx
+		 */
 		if (unlikely(vhost_get_avail(vq, avail_idx, &vq->avail->idx))) {
 			vq_err(vq, "Failed to access avail idx at %p\n",
 				&vq->avail->idx);
 			return -EFAULT;
 		}
+		/*
+		 * !!!! 更新了vq->avail_idx!!!
+		 */
 		vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
 
 		if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
@@ -2044,6 +3159,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	/* Grab the next descriptor number they're advertising, and increment
 	 * the index we've seen. */
+	/*
+	 * __virtio16 ring_head;
+	 */
 	if (unlikely(vhost_get_avail(vq, ring_head,
 		     &vq->avail->ring[last_avail_idx & (vq->num - 1)]))) {
 		vq_err(vq, "Failed to read head: idx %d address %p\n",
@@ -2063,9 +3181,13 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	/* When we start there are none of either input nor output. */
 	*out_num = *in_num = 0;
+	/*
+	 * 如果是vhost_scsi_get_desc()调用进来的, log是NULL
+	 */
 	if (unlikely(log))
 		*log_num = 0;
 
+	/* head是上面从avail ring buffer根据index读出来的__virtio16 */
 	i = head;
 	do {
 		unsigned iov_count = *in_num + *out_num;
@@ -2080,6 +3202,14 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			       i, vq->num, head);
 			return -EINVAL;
 		}
+		/*
+		 * struct vhost_virtqueue:
+		 *   - struct vring_desc __user *desc; 
+		 *   - struct vring_avail __user *avail;
+		 *   - struct vring_used __user *used;
+		 *
+		 * struct vring_desc desc;
+		 */
 		ret = vhost_copy_from_user(vq, &desc, vq->desc + i,
 					   sizeof desc);
 		if (unlikely(ret)) {
@@ -2100,10 +3230,18 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			continue;
 		}
 
+		/*
+		 * int ret, access;
+		 */
 		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * 把desc中的地址转换成qemu userspace的地址存放在iov中
+		 *
+		 * iov_size是从iov开始剩下可用的iov
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2135,6 +3273,33 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 	} while ((i = next_desc(vq, &desc)) != -1);
 
 	/* On success, increment avail index. */
+	/*
+	 * 在以下调用vhost_avail_event():
+	 *   - drivers/vhost/vhost.c|1799| <<vhost_update_avail_event>> vhost_avail_event(vq)))
+	 *   - drivers/vhost/vhost.c|1806| <<vhost_update_avail_event>> used = vhost_avail_event(vq);
+	 *   - drivers/vhost/vhost.c|1809| <<vhost_update_avail_event>> sizeof *vhost_avail_event(vq));
+	 *   - drivers/vhost/vhost.c|2426| <<vhost_enable_notify>> vhost_avail_event(vq), r);
+	 *
+	 * 在以下修改vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|311| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2030| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2330| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|310| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1413| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1419| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2022| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2143| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2155| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 */
 	vq->last_avail_idx++;
 
 	/* Assume notifications from guest are disabled at this point,
@@ -2145,6 +3310,15 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|725| <<handle_tx>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|963| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1084| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1108| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *
+ * 只有vhost-net在使用
+ */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
 	vq->last_avail_idx -= n;
@@ -2153,6 +3327,13 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|562| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2303| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|159| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + pkt->len);
+ *   - drivers/vhost/vsock.c|404| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, sizeof(pkt->hdr) + len);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2164,6 +3345,11 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2225| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2231| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2173,6 +3359,17 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	int start;
 
 	start = vq->last_used_idx & (vq->num - 1);
+	/*
+	 * struct vhost_virtqueue *vq:
+	 *  -> struct vring_used __user *used;
+	 *      -> __virtio16 flags;  
+	 *      -> __virtio16 idx;
+	 *      -> struct vring_used_elem ring[];
+	 *          -> // Index of start of used descriptor chain.
+	 *             __virtio32 id;
+	 *          -> // Total length of the descriptor chain which was used (written to)
+	 *             __virtio32 len;
+	 */
 	used = vq->used->ring + start;
 	if (count == 1) {
 		if (vhost_put_user(vq, heads[0].id, &used->id)) {
@@ -2197,11 +3394,28 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			  count * sizeof *used);
 	}
 	old = vq->last_used_idx;
+	/*
+	 * 这里更新了vq->last_used_idx
+	 */
 	new = (vq->last_used_idx += count);
 	/* If the driver never bothers to signal in a very long while,
 	 * used index might wrap around. If that happens, invalidate
 	 * signalled_used index we stored. TODO: make sure driver
 	 * signals at least once in 2^16 and remove this. */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|313| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2239| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2320| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2322| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 *
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1830| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2240| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2321| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
 		vq->signalled_used_valid = false;
 	return 0;
@@ -2209,11 +3423,28 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2168| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2313| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
 	int start, n, r;
 
+	/*
+	 * 在以下修改vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|312| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1825| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2205| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2180| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2204| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2235| <<vhost_add_used_n>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2276| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	start = vq->last_used_idx & (vq->num - 1);
 	n = vq->num - start;
 	if (n < count) {
@@ -2244,6 +3475,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2348| <<vhost_signal>> if (vq->call_ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2254,10 +3489,17 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	 * interrupts. */
 	smp_mb();
 
+	/*
+	 * VIRTIO_F_NOTIFY_ON_EMPTY有一些不支持
+	 */
 	if (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&
 	    unlikely(vq->avail_idx == vq->last_avail_idx))
 		return true;
 
+	/*
+	 * 大部分支持VIRTIO_RING_F_EVENT_IDX
+	 * VIRTIO_RING_F_EVENT_IDX = 29
+	 */
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
 		__virtio16 flags;
 		if (vhost_get_avail(vq, flags, &vq->avail->flags)) {
@@ -2267,21 +3509,54 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
 	old = vq->signalled_used;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1830| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2240| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2321| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|313| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2239| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2320| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2322| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
 	if (unlikely(!v))
 		return true;
 
+	/*
+	 * vhost_used_event():
+	 * 返回((__virtio16 __user *)&vq->avail->ring[vq->num])
+	 */
 	if (vhost_get_avail(vq, event, vhost_used_event(vq))) {
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * vring_need_event():
+	 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|575| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2897| <<vhost_signal>> void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
+ *   - drivers/vhost/vhost.c|2911| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2926| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.h|333| <<vhost_add_used_and_signal_n>> void vhost_signal(struct vhost_dev *, struct vhost_virtqueue *);
+ *   - drivers/vhost/vsock.c|180| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|410| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
@@ -2291,6 +3566,15 @@ void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_signal);
 
 /* And here's the combo meal deal.  Supersize me! */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|732| <<handle_tx>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|557| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|901| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1268| <<vhost_scsi_send_tmf_reject>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/scsi.c|1288| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|82| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
@@ -2301,6 +3585,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
 /* multi-buffer version of vhost_add_used_and_signal */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|338| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|856| <<handle_rx>> vhost_add_used_and_signal_n(&net->dev, vq, vq->heads,
+ */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
@@ -2311,6 +3600,13 @@ void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal_n);
 
 /* return true if we're sure that avaiable ring is empty */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|585| <<vhost_net_tx_get_vq_desc>> vhost_vq_avail_empty(vq->dev, vq))
+ *   - drivers/vhost/net.c|710| <<handle_tx>> !vhost_vq_avail_empty(&net->dev, vq) &&
+ *   - drivers/vhost/net.c|845| <<vhost_net_rx_peek_head_len>> vhost_vq_avail_empty(&net->dev, vq))
+ *   - drivers/vhost/net.c|850| <<vhost_net_rx_peek_head_len>> if (!vhost_vq_avail_empty(&net->dev, vq))
+ */
 bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
@@ -2329,11 +3625,33 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
 /* OK, now we need to know about added descriptors. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|658| <<handle_tx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|852| <<vhost_net_rx_peek_head_len>> else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1041| <<handle_rx>> if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|480| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|864| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|65| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|103| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|129| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|380| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
 	int r;
 
+	/*
+	 * 在以下使用VRING_USED_F_NO_NOTIFY:
+	 *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+	 *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+	 */
 	if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
 		return false;
 	vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
@@ -2367,13 +3685,45 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
 /* We don't need to be notified again. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|634| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|659| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|838| <<vhost_net_rx_peek_head_len>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|853| <<vhost_net_rx_peek_head_len>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1008| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1044| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|527| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|931| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1053| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1301| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|53| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|66| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|90| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|130| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|362| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|381| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
 
+	/*
+	 * 在以下使用VRING_USED_F_NO_NOTIFY:
+	 *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+	 *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+	 */
 	if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
 		return;
 	vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	/*
+	 * 一般都设置VIRTIO_RING_F_EVENT_IDX
+	 */
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
 		r = vhost_update_used_flags(vq);
 		if (r)
@@ -2384,6 +3734,10 @@ void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_disable_notify);
 
 /* Create a new message. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1711| <<vhost_iotlb_miss>> node = vhost_new_msg(vq, VHOST_IOTLB_MISS);
+ */
 struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)
 {
 	struct vhost_msg_node *node = kmalloc(sizeof *node, GFP_KERNEL);
@@ -2398,6 +3752,11 @@ struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)
 }
 EXPORT_SYMBOL_GPL(vhost_new_msg);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1618| <<vhost_chr_read_iter>> vhost_enqueue_msg(dev, &dev->pending_list, node);
+ *   - drivers/vhost/vhost.c|1665| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+ */
 void vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,
 		       struct vhost_msg_node *node)
 {
@@ -2409,6 +3768,14 @@ void vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,
 }
 EXPORT_SYMBOL_GPL(vhost_enqueue_msg);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1638| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+ *
+ * vhost_net_fops.read_iter = vhost_net_chr_read_iter()
+ * -> vhost_chr_read_iter()
+ *    -> vhost_dequeue_msg()
+ */
 struct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,
 					 struct list_head *head)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 79c6e7a60a5e..f893a7e9a0d3 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -30,8 +30,32 @@ struct vhost_work {
 /* Poll a file (eventfd or socket) */
 /* Note: there's nothing vhost specific about this structure. */
 struct vhost_poll {
+	/*
+	 * 在以下使用vhost_poll->table:
+	 *   - drivers/vhost/vhost.c|260| <<vhost_poll_func>> poll = container_of(pt, struct vhost_poll, table);
+	 *   - drivers/vhost/vhost.c|399| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+	 *   - drivers/vhost/vhost.c|458| <<vhost_poll_start>> mask = file->f_op->poll(file, &poll->table);
+	 */
 	poll_table                table;
+	/*
+	 * 在以下使用vhost_poll->wqh:
+	 *   - drivers/vhost/vhost.c|261| <<vhost_poll_func>> poll->wqh = wqh;
+	 *   - drivers/vhost/vhost.c|274| <<vhost_poll_func>> add_wait_queue(wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|376| <<vhost_poll_init>> poll->wqh = NULL;
+	 *   - drivers/vhost/vhost.c|400| <<vhost_poll_start>> if (poll->wqh)
+	 *   - drivers/vhost/vhost.c|439| <<vhost_poll_stop>> if (poll->wqh) {
+	 *   - drivers/vhost/vhost.c|440| <<vhost_poll_stop>> remove_wait_queue(poll->wqh, &poll->wait);
+	 *   - drivers/vhost/vhost.c|441| <<vhost_poll_stop>> poll->wqh = NULL;
+	 */
 	wait_queue_head_t        *wqh;
+	/*
+	 * struct wait_queue_entry {
+	 *     unsigned int            flags;
+	 *     void                    *private;
+	 *     wait_queue_func_t       func;
+	 *     struct list_head        entry;
+	 * };
+	 */
 	wait_queue_entry_t              wait;
 	struct vhost_work	  work;
 	unsigned long		  mask;
@@ -72,7 +96,25 @@ struct vhost_umem_node {
 };
 
 struct vhost_umem {
+	/*
+	 * 在以下使用vhost_umem->umem_tree:
+	 *    - drivers/vhost/vhost.c|1032| <<vhost_umem_free>> vhost_umem_interval_tree_remove(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1433| <<vhost_new_umem_range>> vhost_umem_interval_tree_insert(node, &umem->umem_tree);
+	 *    - drivers/vhost/vhost.c|1444| <<vhost_del_umem_range>> while ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1711| <<iotlb_access_ok>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 *    - drivers/vhost/vhost.c|1806| <<vhost_umem_alloc>> umem->umem_tree = RB_ROOT_CACHED;
+	 *    - drivers/vhost/vhost.c|2433| <<translate_desc>> node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,
+	 */
 	struct rb_root_cached umem_tree;
+	/*
+	 * 在以下使用vhost_umem->umem_list:
+	 *   - drivers/vhost/vhost.c|1006| <<vhost_dev_reset_owner>> INIT_LIST_HEAD(&umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1045| <<vhost_umem_clean>> list_for_each_entry_safe(node, tmp, &umem->umem_list, link)
+	 *   - drivers/vhost/vhost.c|1154| <<vq_memory_access_ok>> list_for_each_entry(node, &umem->umem_list, link) {
+	 *   - drivers/vhost/vhost.c|1422| <<vhost_new_umem_range>> tmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);
+	 *   - drivers/vhost/vhost.c|1432| <<vhost_new_umem_range>> list_add_tail(&node->link, &umem->umem_list);
+	 *   - drivers/vhost/vhost.c|1808| <<vhost_umem_alloc>> INIT_LIST_HEAD(&umem->umem_list);
+	 */
 	struct list_head umem_list;
 	int numem;
 };
@@ -94,11 +136,37 @@ struct vhost_virtqueue {
 	struct vring_desc __user *desc;
 	struct vring_avail __user *avail;
 	struct vring_used __user *used;
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[]:
+	 *   - drivers/vhost/vhost.c|713| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|1176| <<vhost_vq_meta_fetch>> const struct vhost_umem_node *node = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1693| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = node;
+	 */
 	const struct vhost_umem_node *meta_iotlb[VHOST_NUM_ADDRS];
 	struct file *kick;
 	struct file *call;
 	struct file *error;
+	/*
+	 * 在以下使用vhost_virtqueue->call_ctx:
+	 *   - drivers/vhost/vhost.c|335| <<vhost_vq_reset>> vq->call_ctx = NULL;
+	 *   - drivers/vhost/vhost.c|637| <<vhost_dev_cleanup>> if (dev->vqs[i]->call_ctx)
+	 *   - drivers/vhost/vhost.c|638| <<vhost_dev_cleanup>> eventfd_ctx_put(dev->vqs[i]->call_ctx);
+	 *   - drivers/vhost/vhost.c|1539| <<vhost_vring_ioctl>> ctx = vq->call_ctx;
+	 *   - drivers/vhost/vhost.c|1541| <<vhost_vring_ioctl(VHOST_SET_VRING_CALL)>> vq->call_ctx = eventfp ?
+	 *   - drivers/vhost/vhost.c|2395| <<vhost_signal>> if (vq->call_ctx && vhost_notify(dev, vq))
+	 *   - drivers/vhost/vhost.c|2396| <<vhost_signal>> eventfd_signal(vq->call_ctx, 1);
+	 */
 	struct eventfd_ctx *call_ctx;
+	/*
+	 * 在以下使用vhost_virtqueue->error_ctx:
+	 *   - drivers/vhost/vhost.c|332| <<vhost_vq_reset>> vq->error_ctx = NULL;
+	 *   - drivers/vhost/vhost.c|631| <<vhost_dev_cleanup>> if (dev->vqs[i]->error_ctx)
+	 *   - drivers/vhost/vhost.c|632| <<vhost_dev_cleanup>> eventfd_ctx_put(dev->vqs[i]->error_ctx);
+	 *   - drivers/vhost/vhost.c|1559| <<vhost_vring_ioctl>> ctx = vq->error_ctx;
+	 *   -  drivers/vhost/vhost.c|1560| <<vhost_vring_ioctl>> vq->error_ctx = eventfp ?
+	 *   - drivers/vhost/vhost.h|299| <<vq_err>> if ((vq)->error_ctx) \
+	 *   - drivers/vhost/vhost.h|300| <<vq_err>> eventfd_signal((vq)->error_ctx, 1);\
+	 */
 	struct eventfd_ctx *error_ctx;
 	struct eventfd_ctx *log_ctx;
 
@@ -108,21 +176,81 @@ struct vhost_virtqueue {
 	vhost_work_fn_t handle_kick;
 
 	/* Last available index we saw. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_avail_idx:
+	 *   - drivers/vhost/vhost.c|310| <<vhost_vq_reset>> vq->last_avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1413| <<vhost_vring_ioctl>> vq->last_avail_idx = s.num;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|1419| <<vhost_vring_ioctl>> s.num = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2022| <<vhost_get_vq_desc>> last_avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2143| <<vhost_get_vq_desc>> vq->last_avail_idx++;
+	 *   - drivers/vhost/vhost.c|2155| <<vhost_discard_vq_desc>> vq->last_avail_idx -= n;
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 */
 	u16 last_avail_idx;
 
 	/* Caches available index value from user. */
+	/*
+	 * 在以下修改vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|311| <<vhost_vq_reset>> vq->avail_idx = 0;
+	 *   - drivers/vhost/vhost.c|1415| <<vhost_vring_ioctl>> vq->avail_idx = vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2030| <<vhost_get_vq_desc>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 *   - drivers/vhost/vhost.c|2330| <<vhost_vq_avail_empty>> vq->avail_idx = vhost16_to_cpu(vq, avail_idx);
+	 * 在以下使用vhost_virtqueue->avail_idx:
+	 *   - drivers/vhost/vhost.c|1781| <<vhost_update_avail_event>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),
+	 *   - drivers/vhost/vhost.c|2024| <<vhost_get_vq_desc>> if (vq->avail_idx == vq->last_avail_idx) {
+	 *   - drivers/vhost/vhost.c|2032| <<vhost_get_vq_desc>> if (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {
+	 *   - drivers/vhost/vhost.c|2034| <<vhost_get_vq_desc>> last_avail_idx, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2041| <<vhost_get_vq_desc>> if (vq->avail_idx == last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2263| <<vhost_notify>> unlikely(vq->avail_idx == vq->last_avail_idx))
+	 *   - drivers/vhost/vhost.c|2324| <<vhost_vq_avail_empty>> if (vq->avail_idx != vq->last_avail_idx)
+	 *   - drivers/vhost/vhost.c|2332| <<vhost_vq_avail_empty>> return vq->avail_idx == vq->last_avail_idx;
+	 *   - drivers/vhost/vhost.c|2363| <<vhost_enable_notify>> r = vhost_update_avail_event(vq, vq->avail_idx);
+	 *   - drivers/vhost/vhost.c|2380| <<vhost_enable_notify>> return vhost16_to_cpu(vq, avail_idx) != vq->avail_idx;
+	 */
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下修改vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|312| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1825| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2205| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|2180| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2204| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2235| <<vhost_add_used_n>> if (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2276| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|313| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2239| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2320| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2322| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|1830| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2240| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2321| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2323| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
@@ -137,6 +265,15 @@ struct vhost_virtqueue {
 	struct vhost_umem *umem;
 	struct vhost_umem *iotlb;
 	void *private_data;
+	/*
+	 * 在以下使用vhost_virtqueue->acked_features:
+	 *   - drivers/vhost/net.c|1259| <<vhost_net_set_features>> n->vqs[i].vq.acked_features = features;
+	 *   - drivers/vhost/scsi.c|1621| <<vhost_scsi_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/test.c|254| <<vhost_test_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/vhost.c|319| <<vhost_vq_reset>> vq->acked_features = 0;
+	 *   - drivers/vhost/vhost.h|245| <<vhost_has_feature>> return vq->acked_features & (1ULL << bit);
+	 *   - drivers/vhost/vsock.c|661| <<vhost_vsock_set_features>> vq->acked_features = features;
+	 */
 	u64 acked_features;
 	/* Log write descriptors */
 	void __user *log_base;
@@ -165,12 +302,35 @@ struct vhost_dev {
 	int nvqs;
 	struct file *log_file;
 	struct eventfd_ctx *log_ctx;
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|585| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|598| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|691| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|767| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|983| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	struct llist_head work_list;
 	struct task_struct *worker;
 	struct vhost_umem *umem;
 	struct vhost_umem *iotlb;
 	spinlock_t iotlb_lock;
+	/*
+	 * 在以下使用vhost_dev->read_list:
+	 *   - drivers/vhost/vhost.c|916| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->read_list);
+	 *   - drivers/vhost/vhost.c|1113| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->read_list, node) {
+	 *   - drivers/vhost/vhost.c|1722| <<vhost_chr_poll>> if (!list_empty(&dev->read_list))
+	 *   - drivers/vhost/vhost.c|1752| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+	 *   - drivers/vhost/vhost.c|1834| <<vhost_iotlb_miss>> vhost_enqueue_msg(dev, &dev->read_list, node);
+	 */
 	struct list_head read_list;
+	/*
+	 * 在以下使用vhost_dev->pending_list:
+	 *   - drivers/vhost/vhost.c|770| <<vhost_dev_init>> INIT_LIST_HEAD(&dev->pending_list);
+	 *   - drivers/vhost/vhost.c|943| <<vhost_clear_msg>> list_for_each_entry_safe(node, n, &dev->pending_list, node) {
+	 *   - drivers/vhost/vhost.c|1319| <<vhost_iotlb_notify_vq>> list_for_each_entry_safe(node, n, &d->pending_list, node) {
+	 *   - drivers/vhost/vhost.c|1481| <<vhost_chr_read_iter>> vhost_enqueue_msg(dev, &dev->pending_list, node);
+	 */
 	struct list_head pending_list;
 	wait_queue_head_t wait;
 };
diff --git a/drivers/virtio/virtio_pci_modern.c b/drivers/virtio/virtio_pci_modern.c
index 2555d80f6eec..efbf2b576e2e 100644
--- a/drivers/virtio/virtio_pci_modern.c
+++ b/drivers/virtio/virtio_pci_modern.c
@@ -292,6 +292,26 @@ static u16 vp_config_vector(struct virtio_pci_device *vp_dev, u16 vector)
 	return vp_ioread16(&vp_dev->common->msix_config);
 }
 
+/*
+ * [0] setup_vq
+ * [0] vp_setup_vq
+ * [0] vp_find_vqs_msix
+ * [0] vp_find_vqs
+ * [0] vp_modern_find_vqs
+ * [0] virtscsi_init
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static struct virtqueue *setup_vq(struct virtio_pci_device *vp_dev,
 				  struct virtio_pci_vq_info *info,
 				  unsigned index,
@@ -343,6 +363,7 @@ static struct virtqueue *setup_vq(struct virtio_pci_device *vp_dev,
 	vp_iowrite64_twopart(virtqueue_get_used_addr(vq),
 			     &cfg->queue_used_lo, &cfg->queue_used_hi);
 
+	/* 在uek5设置了 */
 	if (vp_dev->notify_base) {
 		/* offset should not wrap */
 		if ((u64)off * vp_dev->notify_offset_multiplier + 2
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 71458f493cf8..1480aa209d60 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -82,15 +82,49 @@ struct vring_virtqueue {
 	/* Head of free buffer list. */
 	unsigned int free_head;
 	/* Number we've added since last sync. */
+	/*
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|428| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|435| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|582| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|584| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1064| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 */
 	unsigned int num_added;
 
 	/* Last used index we've seen. */
+	/*
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|893| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1151| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下修改vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|834| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|877| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|900| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|967| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1040| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1044| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	u16 last_used_idx;
 
 	/* Last written value to avail->flags */
 	u16 avail_flags_shadow;
 
 	/* Last written value to avail->idx in guest byte order */
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|406| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|924| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|400| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|407| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|562| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|563| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|925| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|988| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 */
 	u16 avail_idx_shadow;
 
 	/* How to notify other side. FIXME: commonalize hcalls! */
@@ -106,6 +140,16 @@ struct vring_virtqueue {
 	unsigned int in_use;
 
 	/* Figure out if their kicks are too delayed. */
+	/*
+	 * 在以下修改vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|315| <<virtqueue_add>> vq->last_add_time_valid = true;
+	 *   - drivers/virtio/virtio_ring.c|591| <<virtqueue_kick_prepare>> vq->last_add_time_valid = false;
+	 *   - drivers/virtio/virtio_ring.c|813| <<virtqueue_get_buf_ctx>> vq->last_add_time_valid = false;
+	 *   - drivers/virtio/virtio_ring.c|1068| <<bool>> vq->last_add_time_valid = false;
+	 * 在以下使用vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|311| <<virtqueue_add>> if (vq->last_add_time_valid)
+	 *   - drivers/virtio/virtio_ring.c|587| <<virtqueue_kick_prepare>> if (vq->last_add_time_valid) {
+	 */
 	bool last_add_time_valid;
 	ktime_t last_add_time;
 #endif
@@ -257,6 +301,13 @@ static struct vring_desc *alloc_indirect(struct virtqueue *_vq,
 	return desc;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|480| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs,
+ *   - drivers/virtio/virtio_ring.c|503| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|525| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|549| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+ */
 static inline int virtqueue_add(struct virtqueue *_vq,
 				struct scatterlist *sgs[],
 				unsigned int total_sg,
@@ -288,6 +339,16 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 		ktime_t now = ktime_get();
 
 		/* No kick or get, with .1 second between?  Warn. */
+		/*
+		 * 在以下修改vring_virtqueue->last_add_time_valid:
+		 *   - drivers/virtio/virtio_ring.c|315| <<virtqueue_add>> vq->last_add_time_valid = true;
+		 *   - drivers/virtio/virtio_ring.c|591| <<virtqueue_kick_prepare>> vq->last_add_time_valid = false;
+		 *   - drivers/virtio/virtio_ring.c|813| <<virtqueue_get_buf_ctx>> vq->last_add_time_valid = false;
+		 *   - drivers/virtio/virtio_ring.c|1068| <<bool>> vq->last_add_time_valid = false;
+		 * 在以下使用vring_virtqueue->last_add_time_valid:
+		 *   - drivers/virtio/virtio_ring.c|311| <<virtqueue_add>> if (vq->last_add_time_valid)
+		 *   - drivers/virtio/virtio_ring.c|587| <<virtqueue_kick_prepare>> if (vq->last_add_time_valid) {
+		 */
 		if (vq->last_add_time_valid)
 			WARN_ON(ktime_to_ms(ktime_sub(now, vq->last_add_time))
 					    > 100);
@@ -403,7 +464,21 @@ static inline int virtqueue_add(struct virtqueue *_vq,
 	/* Descriptors and available array need to be set before we expose the
 	 * new available array entries. */
 	virtio_wmb(vq->weak_barriers);
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|406| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|924| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|400| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|407| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|562| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|563| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|925| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|988| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 */
 	vq->avail_idx_shadow++;
+	/* !!! 更新shared ring buffer idx !!! */
 	vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
 	vq->num_added++;
 
@@ -559,8 +634,29 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 	 * event. */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 在以下修改vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|406| <<virtqueue_add>> vq->avail_idx_shadow++;
+	 *   - drivers/virtio/virtio_ring.c|924| <<virtqueue_detach_unused_buf>> vq->avail_idx_shadow--;
+	 * 在以下使用vring_virtqueue->avail_idx_shadow:
+	 *   - drivers/virtio/virtio_ring.c|400| <<virtqueue_add>> avail = vq->avail_idx_shadow & (vq->vring.num - 1);
+	 *   - drivers/virtio/virtio_ring.c|407| <<virtqueue_add>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|562| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|563| <<virtqueue_kick_prepare>> new = vq->avail_idx_shadow;
+	 *   - drivers/virtio/virtio_ring.c|886| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|925| <<virtqueue_detach_unused_buf>> vq->vring.avail->idx = cpu_to_virtio16(_vq->vdev, vq->avail_idx_shadow);
+	 *   - drivers/virtio/virtio_ring.c|988| <<__vring_new_virtqueue>> vq->avail_idx_shadow = 0;
+	 */
 	old = vq->avail_idx_shadow - vq->num_added;
 	new = vq->avail_idx_shadow;
+	/*
+	 * 在以下使用vring_virtqueue->num_added:
+	 *   - drivers/virtio/virtio_ring.c|428| <<virtqueue_add>> vq->num_added++;
+	 *   - drivers/virtio/virtio_ring.c|435| <<virtqueue_add>> if (unlikely(vq->num_added == (1 << 16) - 1))
+	 *   - drivers/virtio/virtio_ring.c|582| <<virtqueue_kick_prepare>> old = vq->avail_idx_shadow - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|584| <<virtqueue_kick_prepare>> vq->num_added = 0;
+	 *   - drivers/virtio/virtio_ring.c|1064| <<__vring_new_virtqueue>> vq->num_added = 0;
+	 */
 	vq->num_added = 0;
 
 #ifdef DEBUG
@@ -575,6 +671,20 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq)
 		needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev, vring_avail_event(&vq->vring)),
 					      new, old);
 	} else {
+		/*
+		 * The Host uses this in used->flags to advise the Guest: don't kick me when
+		 * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
+		 * will still kick if it's out of buffers.
+		 *
+		 * 在以下使用VRING_USED_F_NO_NOTIFY:
+		 *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+		 *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+		 *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+		 *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+		 *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+		 *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+		 *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+		 */
 		needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
 	}
 	END_USE(vq);
@@ -590,6 +700,40 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * [0] virtqueue_notify
+ * [0] virtscsi_kick_cmd
+ * [0] virtscsi_queuecommand
+ * [0] scsi_dispatch_cmd
+ * [0] scsi_queue_rq
+ * [0] blk_mq_dispatch_rq_list
+ * [0] blk_mq_do_dispatch_sched
+ * [0] blk_mq_sched_dispatch_requests
+ * [0] __blk_mq_run_hw_queue
+ * [0] __blk_mq_delay_run_hw_queue
+ * [0] blk_mq_run_hw_queue
+ * [0] blk_mq_flush_plug_list
+ * [0] blk_flush_plug_list
+ * [0] blk_mq_make_request
+ * [0] generic_make_request
+ * [0] submit_bio
+ * [0] __blkdev_direct_IO_simple
+ * [0] blkdev_direct_IO
+ * [0] generic_file_direct_write
+ * [0] __generic_file_write_iter
+ * [0] blkdev_write_iter
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/block/virtio_blk.c|288| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|481| <<virtscsi_kick_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|624| <<virtqueue_kick>> return virtqueue_notify(vq);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -618,6 +762,43 @@ EXPORT_SYMBOL_GPL(virtqueue_notify);
  *
  * Returns false if kick failed, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|273| <<virtio_queue_rq>> virtqueue_kick(vblk->vqs[qid].vq);
+ *   - drivers/char/hw_random/virtio-rng.c|63| <<register_buffer>> virtqueue_kick(vi->vq);
+ *   - drivers/char/virtio_console.c|513| <<add_inbuf>> virtqueue_kick(vq);
+ *   - drivers/char/virtio_console.c|582| <<__send_control_msg>> virtqueue_kick(vq);
+ *   - drivers/char/virtio_console.c|636| <<__send_to_port>> virtqueue_kick(out_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|181| <<virtio_crypto_alg_ablkcipher_init_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|254| <<virtio_crypto_alg_ablkcipher_close_session>> virtqueue_kick(vcrypto->ctrl_vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|462| <<__virtio_crypto_ablkcipher_do_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|554| <<virtio_crypto_ablkcipher_crypt_req>> virtqueue_kick(data_vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|298| <<virtio_gpu_queue_ctrl_buffer_locked>> virtqueue_kick(vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|373| <<virtio_gpu_queue_cursor>> virtqueue_kick(vq);
+ *   - drivers/net/caif/caif_virtio.c|589| <<cfv_netdev_tx>> virtqueue_kick(cfv->vq_tx);
+ *   - drivers/net/virtio_net.c|425| <<virtnet_xdp_flush>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1123| <<try_fill_recv>> virtqueue_kick(rq->vq);
+ *   - drivers/net/virtio_net.c|1447| <<start_xmit>> virtqueue_kick(sq->vq);
+ *   - drivers/net/virtio_net.c|1483| <<virtnet_send_command>> if (unlikely(!virtqueue_kick(vi->cvq)))
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|654| <<rpmsg_send_offchannel_raw>> virtqueue_kick(vrp->svq);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|802| <<rpmsg_recv_done>> virtqueue_kick(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|284| <<virtscsi_kick_event>> virtqueue_kick(vscsi->event_vq.vq);
+ *   - drivers/virtio/virtio_balloon.c|123| <<tell_host>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|326| <<stats_handle_request>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_balloon.c|461| <<init_vqs>> virtqueue_kick(vb->stats_vq);
+ *   - drivers/virtio/virtio_input.c|48| <<virtinput_recv_events>> virtqueue_kick(vq);
+ *   - drivers/virtio/virtio_input.c|77| <<virtinput_send_status>> virtqueue_kick(vi->sts);
+ *   - drivers/virtio/virtio_input.c|196| <<virtinput_fill_evt>> virtqueue_kick(vi->evt);
+ *   - drivers/virtio/virtio_ring.c|429| <<virtqueue_add>> virtqueue_kick(_vq);
+ *   - net/9p/trans_virtio.c|305| <<p9_virtio_request>> virtqueue_kick(chan->vq);
+ *   - net/9p/trans_virtio.c|498| <<p9_virtio_zc_request>> virtqueue_kick(chan->vq);
+ *   - net/vmw_vsock/virtio_transport.c|184| <<virtio_transport_send_pkt_work>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|296| <<virtio_vsock_rx_fill>> virtqueue_kick(vq);
+ *   - net/vmw_vsock/virtio_transport.c|411| <<virtio_vsock_event_fill>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - net/vmw_vsock/virtio_transport.c|468| <<virtio_transport_event_work>> virtqueue_kick(vsock->vqs[VSOCK_VQ_EVENT]);
+ *   - tools/virtio/virtio_test.c|176| <<run_test>> if (unlikely(!virtqueue_kick(vq->vq)))
+ *   - tools/virtio/vringh_test.c|402| <<bool>> virtqueue_kick(vq);
+ */
 bool virtqueue_kick(struct virtqueue *vq)
 {
 	if (virtqueue_kick_prepare(vq))
@@ -626,6 +807,11 @@ bool virtqueue_kick(struct virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_kick);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|892| <<virtqueue_get_buf_ctx>> detach_buf(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1088| <<virtqueue_detach_unused_buf>> detach_buf(vq, i, NULL);
+ */
 static void detach_buf(struct vring_virtqueue *vq, unsigned int head,
 		       void **ctx)
 {
@@ -736,6 +922,19 @@ void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 	/* detach_buf clears data, so grab it now. */
 	ret = vq->desc_state[i].data;
 	detach_buf(vq, i, ctx);
+	/*
+	 * 在以下使用vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|893| <<virtqueue_get_buf_ctx>> vq->last_used_idx++;
+	 *   - drivers/virtio/virtio_ring.c|1151| <<__vring_new_virtqueue>> vq->last_used_idx = 0;
+	 * 在以下修改vring_virtqueue->last_used_idx:
+	 *   - drivers/virtio/virtio_ring.c|834| <<more_used>> return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev, vq->vring.used->idx);
+	 *   - drivers/virtio/virtio_ring.c|877| <<virtqueue_get_buf_ctx>> last_used = (vq->last_used_idx & (vq->vring.num - 1));
+	 *   - drivers/virtio/virtio_ring.c|900| <<virtqueue_get_buf_ctx>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+	 *   - drivers/virtio/virtio_ring.c|967| <<virtqueue_enable_cb_prepare>> vring_used_event(&vq->vring) = cpu_to_virtio16(_vq->vdev, last_used_idx = vq->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|1040| <<virtqueue_enable_cb_delayed>> bufs = (u16)(vq->avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+	 *   - drivers/virtio/virtio_ring.c|1044| <<virtqueue_enable_cb_delayed>> cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+	 *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_enable_cb_delayed>> if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->vring.used->idx) - vq->last_used_idx) > bufs)) {
+	 */
 	vq->last_used_idx++;
 	/* If we expect an interrupt for the next entry, tell host
 	 * by writing event index and flush out the write before
@@ -907,6 +1106,17 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb_delayed);
  * This is not valid on an active queue; it is useful only for device
  * shutdown.
  */
+/*
+ * called by:
+ *   - drivers/char/virtio_console.c|1983| <<remove_vqs>> while ((buf = virtqueue_detach_unused_buf(vq)))
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|410| <<virtcrypto_free_unused_reqs>> while ((vc_req = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/net/caif/caif_virtio.c|469| <<cfv_netdev_close>> while ((buf_info = virtqueue_detach_unused_buf(cfv->vq_tx)))
+ *   - drivers/net/virtio_net.c|2434| <<free_unused_bufs>> while ((buf = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/net/virtio_net.c|2445| <<free_unused_bufs>> while ((buf = virtqueue_detach_unused_buf(vq)) != NULL) {
+ *   - drivers/virtio/virtio_input.c|325| <<virtinput_remove>> while ((buf = virtqueue_detach_unused_buf(vi->sts)) != NULL)
+ *   - net/vmw_vsock/virtio_transport.c|643| <<virtio_vsock_remove>> while ((pkt = virtqueue_detach_unused_buf(vsock->vqs[VSOCK_VQ_RX])))
+ *   - net/vmw_vsock/virtio_transport.c|648| <<virtio_vsock_remove>> while ((pkt = virtqueue_detach_unused_buf(vsock->vqs[VSOCK_VQ_TX])))
+ */
 void *virtqueue_detach_unused_buf(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
diff --git a/drivers/xen/xenbus/xenbus_probe_frontend.c b/drivers/xen/xenbus/xenbus_probe_frontend.c
index 07896f4b2736..64496153b6ba 100644
--- a/drivers/xen/xenbus/xenbus_probe_frontend.c
+++ b/drivers/xen/xenbus/xenbus_probe_frontend.c
@@ -226,6 +226,10 @@ static int exists_non_essential_connecting_device(struct device_driver *drv)
 				non_essential_device_connecting);
 }
 
+/*
+ * called by:
+ *   - drivers/xen/xenbus/xenbus_probe_frontend.c|309| <<wait_for_devices>> print_device_status);
+ */
 static int print_device_status(struct device *dev, void *data)
 {
 	struct xenbus_device *xendev = to_xenbus_device(dev);
@@ -252,6 +256,11 @@ static int print_device_status(struct device *dev, void *data)
 /* We only wait for device setup after most initcalls have run. */
 static int ready_to_wait_for_devices;
 
+/*
+ * called by:
+ *   - drivers/xen/xenbus/xenbus_probe_frontend.c|297| <<wait_for_devices>> if (wait_loop(start, 30, &seconds_waited))
+ *   - drivers/xen/xenbus/xenbus_probe_frontend.c|302| <<wait_for_devices>> if (wait_loop(start, 270, &seconds_waited))
+ */
 static bool wait_loop(unsigned long start, unsigned int max_delay,
 		     unsigned int *seconds_waited)
 {
@@ -284,6 +293,11 @@ static bool wait_loop(unsigned long start, unsigned int max_delay,
  * boot slightly, but of course needs tools or manual intervention to set up
  * those flags correctly.
  */
+/*
+ * called by:
+ *   - drivers/xen/xenbus/xenbus_probe_frontend.c|325| <<__xenbus_register_frontend>> wait_for_devices(drv);
+ *   - drivers/xen/xenbus/xenbus_probe_frontend.c|495| <<boot_wait_for_devices>> wait_for_devices(NULL);
+ */
 static void wait_for_devices(struct xenbus_driver *xendrv)
 {
 	unsigned long start = jiffies;
@@ -309,6 +323,10 @@ static void wait_for_devices(struct xenbus_driver *xendrv)
 			 print_device_status);
 }
 
+/*
+ * called by:
+ *   - include/xen/xenbus.h|122| <<xenbus_register_frontend>> __xenbus_register_frontend(drv, THIS_MODULE, KBUILD_MODNAME)
+ */
 int __xenbus_register_frontend(struct xenbus_driver *drv, struct module *owner,
 			       const char *mod_name)
 {
diff --git a/fs/eventfd.c b/fs/eventfd.c
index 2fb4eadaa118..c3de745ad66e 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -51,6 +51,33 @@ struct eventfd_ctx {
  * Returns the amount by which the counter was incremented.  This will be less
  * than @n if the counter has overflowed.
  */
+/*
+ * [0] eventfd_signal
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] kernel_pio
+ * [0] emulator_pio_out_emulated
+ * [0] kvm_fast_pio
+ * [0] handle_io
+ * [0] __dta_vmx_handle_exit_439
+ * [0] __dta_vcpu_enter_guest_1347
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] __dta_kvm_vcpu_ioctl_639
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] eventfd_signal
+ * [0] vhost_add_used_and_signal_n
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 一个caller是ioeventfd_write()
+ */
 __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 {
 	unsigned long flags;
@@ -59,6 +86,30 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 	if (ULLONG_MAX - ctx->count < n)
 		n = ULLONG_MAX - ctx->count;
 	ctx->count += n;
+	/*
+	 * struct eventfd_ctx *ctx:
+	 *   -> wait_queue_head_t wqh;
+	 *
+	 * 对于vhost rx, 会唤醒wait_queue_entry->func = irqfd_wakeup()
+	 * struct wait_queue_entry {
+	 *     unsigned int            flags;
+	 *     void                    *private;
+	 *     wait_queue_func_t       func;
+	 *     struct list_head        entry;
+	 * };
+	 *
+	 * 对于vm的ioeventfd下来的...
+	 * crash> wait_queue_entry ffff930ab1008df8
+	 * struct wait_queue_entry {
+	 *   flags = 0,
+	 *   private = 0x0,
+	 *   func = 0xffffffffc0661d10 <vhost_poll_wakeup>,
+	 *   entry = {
+	 *     next = 0xffff9332faca9010,
+	 *     prev = 0xffff9332faca9010
+	 *   }
+	 * }
+	 */
 	if (waitqueue_active(&ctx->wqh))
 		wake_up_locked_poll(&ctx->wqh, POLLIN);
 	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
@@ -67,6 +118,11 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 }
 EXPORT_SYMBOL_GPL(eventfd_signal);
 
+/*
+ * called by:
+ *   - fs/eventfd.c|104| <<eventfd_free>> eventfd_free_ctx(ctx);
+ *   - fs/eventfd.c|503| <<eventfd_file_create>> eventfd_free_ctx(ctx);
+ */
 static void eventfd_free_ctx(struct eventfd_ctx *ctx)
 {
 	kfree(ctx);
@@ -191,6 +247,17 @@ static void eventfd_ctx_do_read(struct eventfd_ctx *ctx, __u64 *cnt)
  * This is used to atomically remove a wait queue entry from the eventfd wait
  * queue head, and read/reset the counter value.
  */
+/*
+ * [0] eventfd_ctx_remove_wait_queue
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/vfio/virqfd.c|94| <<virqfd_shutdown>> eventfd_ctx_remove_wait_queue(virqfd->eventfd, &virqfd->wait, &cnt);
+ *   - virt/kvm/eventfd.c|132| <<irqfd_shutdown>> eventfd_ctx_remove_wait_queue(irqfd->eventfd, &irqfd->wait, &cnt);
+ */
 int eventfd_ctx_remove_wait_queue(struct eventfd_ctx *ctx, wait_queue_entry_t *wait,
 				  __u64 *cnt)
 {
@@ -403,6 +470,16 @@ EXPORT_SYMBOL_GPL(eventfd_ctx_fdget);
  *
  * -EINVAL   : The @fd file descriptor is not an eventfd file.
  */
+/*
+ * called by:
+ *   - drivers/vfio/virqfd.c|138| <<vfio_virqfd_enable>> ctx = eventfd_ctx_fileget(irqfd.file);
+ *   - drivers/vhost/vhost.c|1990| <<vhost_vring_ioctl>> eventfd_ctx_fileget(eventfp) : NULL;
+ *   - drivers/vhost/vhost.c|2009| <<vhost_vring_ioctl>> eventfd_ctx_fileget(eventfp) : NULL;
+ *   - drivers/vhost/vhost.c|2142| <<vhost_dev_ioctl>> eventfd_ctx_fileget(eventfp) : NULL;
+ *   - fs/eventfd.c|416| <<eventfd_ctx_fdget>> ctx = eventfd_ctx_fileget(f.file);
+ *   - mm/memcontrol.c|3801| <<memcg_write_event_control>> event->eventfd = eventfd_ctx_fileget(efile.file);
+ *   - virt/kvm/eventfd.c|317| <<kvm_irqfd_assign>> eventfd = eventfd_ctx_fileget(f.file);
+ */
 struct eventfd_ctx *eventfd_ctx_fileget(struct file *file)
 {
 	if (file->f_op != &eventfd_fops)
@@ -426,6 +503,10 @@ EXPORT_SYMBOL_GPL(eventfd_ctx_fileget);
  * can be avoided.
  * Returns an eventfd file pointer, or a proper error pointer.
  */
+/*
+ * called by:
+ *   - fs/eventfd.c|493| <<SYSCALL_DEFINE2(eventfd2)>> file = eventfd_file_create(count, flags);
+ */
 struct file *eventfd_file_create(unsigned int count, int flags)
 {
 	struct file *file;
diff --git a/include/kvm/iodev.h b/include/kvm/iodev.h
index a6d208b916f5..9112cdf94caa 100644
--- a/include/kvm/iodev.h
+++ b/include/kvm/iodev.h
@@ -45,6 +45,24 @@ struct kvm_io_device {
 	const struct kvm_io_device_ops *ops;
 };
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/mpic.c|1448| <<map_mmio>> kvm_iodevice_init(&opp->mmio, &mpic_mmio_ops);
+ *   - arch/x86/kvm/i8254.c|691| <<kvm_create_pit>> kvm_iodevice_init(&pit->dev, &pit_dev_ops);
+ *   - arch/x86/kvm/i8254.c|698| <<kvm_create_pit>> kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);
+ *   - arch/x86/kvm/i8259.c|603| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
+ *   - arch/x86/kvm/i8259.c|604| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
+ *   - arch/x86/kvm/i8259.c|605| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_eclr, &picdev_eclr_ops);
+ *   - arch/x86/kvm/ioapic.c|635| <<kvm_ioapic_init>> kvm_iodevice_init(&ioapic->dev, &ioapic_mmio_ops);
+ *   - arch/x86/kvm/lapic.c|2276| <<kvm_create_lapic>> kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
+ *   - virt/kvm/arm/vgic/vgic-its.c|1667| <<vgic_register_its_iodev>> kvm_iodevice_init(&iodev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v2.c|485| <<vgic_v2_init_dist_iodev>> kvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|597| <<vgic_v3_init_dist_iodev>> kvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|643| <<vgic_register_redist_iodev>> kvm_iodevice_init(&rd_dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|658| <<vgic_register_redist_iodev>> kvm_iodevice_init(&sgi_dev->dev, &kvm_io_gic_ops);
+ *   - virt/kvm/coalesced_mmio.c|150| <<kvm_vm_ioctl_register_coalesced_mmio>> kvm_iodevice_init(&dev->dev, &coalesced_mmio_ops);
+ *   - virt/kvm/eventfd.c|917| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static inline void kvm_iodevice_init(struct kvm_io_device *dev,
 				     const struct kvm_io_device_ops *ops)
 {
@@ -59,6 +77,20 @@ static inline int kvm_iodevice_read(struct kvm_vcpu *vcpu,
 				: -EOPNOTSUPP;
 }
 
+/*
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static inline int kvm_iodevice_write(struct kvm_vcpu *vcpu,
 				     struct kvm_io_device *dev, gpa_t addr,
 				     int l, const void *v)
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 3705b8a87795..d309dc9fbbef 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -225,6 +225,19 @@ enum {
 	IRQD_MOVE_PCNTXT		= (1 << 15),
 	IRQD_IRQ_DISABLED		= (1 << 16),
 	IRQD_IRQ_MASKED			= (1 << 17),
+	/*
+	 * 在以下使用IRQD_IRQ_INPROGRESS:
+	 *   - kernel/irq/debugfs.c|103| <<global>> BIT_MASK_DESCR(IRQD_IRQ_INPROGRESS),
+	 *   - include/linux/irq.h|327| <<irqd_irq_inprogress>> return __irqd_to_state(d) & IRQD_IRQ_INPROGRESS;
+	 *   - kernel/irq/chip.c|473| <<handle_nested_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|484| <<handle_nested_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|500| <<irq_may_run>> unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
+	 *   - kernel/irq/chip.c|586| <<handle_untracked_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|592| <<handle_untracked_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|201| <<handle_irq_event>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|207| <<handle_irq_event>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/manage.c|1380| <<__setup_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 */
 	IRQD_IRQ_INPROGRESS		= (1 << 18),
 	IRQD_WAKEUP_ARMED		= (1 << 19),
 	IRQD_FORWARDED_TO_VCPU		= (1 << 20),
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 40b4f6598a1b..be6afb0b6182 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -117,6 +117,11 @@ static inline bool is_error_page(struct page *page)
 	return IS_ERR(page);
 }
 
+/*
+ * KVM_REQ_SCAN_IOAPIC = 0x0000000000000317
+ * KVM_REQ_MMU_RELOAD  = 0x0000000000000301
+ * KVM_REQUEST_MASK    = 0x00000000000000ff
+ */
 #define KVM_REQUEST_MASK           GENMASK(7,0)
 #define KVM_REQUEST_NO_WAKEUP      BIT(8)
 #define KVM_REQUEST_WAIT           BIT(9)
@@ -125,6 +130,11 @@ static inline bool is_error_page(struct page *page)
  * Bits 4-7 are reserved for more arch-independent bits.
  */
 #define KVM_REQ_TLB_FLUSH         (0 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * KVM_REQ_SCAN_IOAPIC = 0x0000000000000317
+ * KVM_REQ_MMU_RELOAD  = 0x0000000000000301
+ * KVM_REQUEST_MASK    = 0x00000000000000ff
+ */
 #define KVM_REQ_MMU_RELOAD        (1 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_PENDING_TIMER     2
 #define KVM_REQ_UNHALT            3
@@ -447,6 +457,13 @@ struct kvm {
 		struct list_head  resampler_list;
 		struct mutex      resampler_lock;
 	} irqfds;
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|555| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|834| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|902| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|933| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	struct list_head ioeventfds;
 #endif
 	struct kvm_vm_stat stat;
@@ -511,6 +528,10 @@ struct kvm {
 
 static inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)
 {
+	/*
+	 * struct kvm:
+	 *   -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	return srcu_dereference_check(kvm->buses[idx], &kvm->srcu,
 				      lockdep_is_held(&kvm->slots_lock) ||
 				      !refcount_read(&kvm->users_count));
@@ -1173,6 +1194,11 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	 * caller.  Paired with the smp_mb__after_atomic in kvm_check_request.
 	 */
 	smp_wmb();
+	/*
+	 * KVM_REQ_SCAN_IOAPIC = 0x0000000000000317
+	 * KVM_REQ_MMU_RELOAD  = 0x0000000000000301
+	 * KVM_REQUEST_MASK    = 0x00000000000000ff
+	 */
 	set_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
diff --git a/include/linux/ptr_ring.h b/include/linux/ptr_ring.h
index dc396196585a..d3cbe88fd4b9 100644
--- a/include/linux/ptr_ring.h
+++ b/include/linux/ptr_ring.h
@@ -32,9 +32,50 @@
 #endif
 
 struct ptr_ring {
+	/*
+	 * 在以下修改ptr_ring->producer:
+	 *   - include/linux/ptr_ring.h|116| <<__ptr_ring_produce>> r->queue[r->producer++] = ptr;
+	 *   - include/linux/ptr_ring.h|118| <<__ptr_ring_produce>> r->producer = 0;
+	 *   - include/linux/ptr_ring.h|549| <<ptr_ring_init>> r->producer = r->consumer_head = r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|637| <<__ptr_ring_swap_queue>> r->producer = producer;
+	 * 在以下使用ptr_ring->producer:
+	 *   - include/linux/ptr_ring.h|54| <<__ptr_ring_full>> return r->queue[r->producer];
+	 *   - include/linux/ptr_ring.h|109| <<__ptr_ring_produce>> if (unlikely(!r->size) || r->queue[r->producer])
+	 *   - include/linux/ptr_ring.h|117| <<__ptr_ring_produce>> if (unlikely(r->producer >= r->size))
+	 */
 	int producer ____cacheline_aligned_in_smp;
 	spinlock_t producer_lock;
+	/*
+	 * 在以下修改ptr_ring->consumer_head:
+	 *   - include/linux/ptr_ring.h|256| <<__ptr_ring_discard_one>> int head = r->consumer_head++;
+	 *   - include/linux/ptr_ring.h|275| <<__ptr_ring_discard_one>> r->consumer_head = 0;
+	 *   - include/linux/ptr_ring.h|638| <<__ptr_ring_swap_queue>> r->consumer_head = 0;
+	 * 在以下使用ptr_ring->consumer_head:
+	 *   - include/linux/ptr_ring.h|185| <<__ptr_ring_peek>> return r->queue[r->consumer_head];
+	 *   - include/linux/ptr_ring.h|263| <<__ptr_ring_discard_one>> if (unlikely(r->consumer_head - r->consumer_tail >= r->batch ||
+	 *   - include/linux/ptr_ring.h|264| <<__ptr_ring_discard_one>> r->consumer_head >= r->size)) {
+	 *   - include/linux/ptr_ring.h|272| <<__ptr_ring_discard_one>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|274| <<__ptr_ring_discard_one>> if (unlikely(r->consumer_head >= r->size)) {
+	 *   - include/linux/ptr_ring.h|549| <<ptr_ring_init>> r->producer = r->consumer_head = r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|586| <<ptr_ring_unconsume>> head = r->consumer_head - 1;
+	 *   - include/linux/ptr_ring.h|589| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|596| <<ptr_ring_unconsume>> head = r->consumer_head - 1;
+	 *   - include/linux/ptr_ring.h|604| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head = head;
+	 */
 	int consumer_head ____cacheline_aligned_in_smp; /* next valid entry */
+	/*
+	 * 在以下修改ptr->consumer_tail:
+	 *   - include/linux/ptr_ring.h|272| <<__ptr_ring_discard_one>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|276| <<__ptr_ring_discard_one>> r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|549| <<ptr_ring_init>> r->producer = r->consumer_head = r->consumer_tail = 0;
+	 *   - include/linux/ptr_ring.h|589| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head;
+	 *   - include/linux/ptr_ring.h|604| <<ptr_ring_unconsume>> r->consumer_tail = r->consumer_head = head;
+	 *   - include/linux/ptr_ring.h|639| <<__ptr_ring_swap_queue>> r->consumer_tail = 0;
+	 * 在以下使用ptr->consumer_tail:
+	 *   - include/linux/ptr_ring.h|263| <<__ptr_ring_discard_one>> if (unlikely(r->consumer_head - r->consumer_tail >= r->batch ||
+	 *   - include/linux/ptr_ring.h|270| <<__ptr_ring_discard_one>> while (likely(head >= r->consumer_tail))
+	 *   - include/linux/ptr_ring.h|587| <<ptr_ring_unconsume>> while (likely(head >= r->consumer_tail))
+	 */
 	int consumer_tail; /* next entry to invalidate */
 	spinlock_t consumer_lock;
 	/* Shared consumer/producer data */
@@ -49,11 +90,29 @@ struct ptr_ring {
  * producer_lock - see e.g. ptr_ring_full.  Otherwise, if callers don't hold
  * producer_lock, the next call to __ptr_ring_produce may fail.
  */
+/*
+ * calld by:
+ *   - include/linux/ptr_ring.h|62| <<ptr_ring_full>> ret = __ptr_ring_full(r);
+ *   - include/linux/ptr_ring.h|73| <<ptr_ring_full_irq>> ret = __ptr_ring_full(r);
+ *   - include/linux/ptr_ring.h|85| <<ptr_ring_full_any>> ret = __ptr_ring_full(r);
+ *   - include/linux/ptr_ring.h|96| <<ptr_ring_full_bh>> ret = __ptr_ring_full(r);
+ *   - include/linux/skb_array.h|38| <<__skb_array_full>> return __ptr_ring_full(&a->ring);
+ *   - tools/virtio/ringtest/ptr_ring.c|152| <<get_buf>> if (tailcnt == headcnt || __ptr_ring_full(&array))
+ *   - tools/virtio/ringtest/ptr_ring.c|164| <<used_empty>> return (tailcnt == headcnt || __ptr_ring_full(&array));
+ *
+ * 返回ptr_ring->queue[ptr_ring->producer]
+ */
 static inline bool __ptr_ring_full(struct ptr_ring *r)
 {
 	return r->queue[r->producer];
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|43| <<skb_array_full>> return ptr_ring_full(&a->ring);
+ *
+ * 返回ptr_ring->queue[ptr_ring->producer] (有spinlock)
+ */
 static inline bool ptr_ring_full(struct ptr_ring *r)
 {
 	bool ret;
@@ -65,6 +124,9 @@ static inline bool ptr_ring_full(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 static inline bool ptr_ring_full_irq(struct ptr_ring *r)
 {
 	bool ret;
@@ -76,6 +138,9 @@ static inline bool ptr_ring_full_irq(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 static inline bool ptr_ring_full_any(struct ptr_ring *r)
 {
 	unsigned long flags;
@@ -88,6 +153,9 @@ static inline bool ptr_ring_full_any(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * 没人调用
+ */
 static inline bool ptr_ring_full_bh(struct ptr_ring *r)
 {
 	bool ret;
@@ -104,8 +172,19 @@ static inline bool ptr_ring_full_bh(struct ptr_ring *r)
  * Callers are responsible for making sure pointer that is being queued
  * points to a valid data.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|132| <<ptr_ring_produce>> ret = __ptr_ring_produce(r, ptr);
+ *   - include/linux/ptr_ring.h|143| <<ptr_ring_produce_irq>> ret = __ptr_ring_produce(r, ptr);
+ *   - include/linux/ptr_ring.h|155| <<ptr_ring_produce_any>> ret = __ptr_ring_produce(r, ptr);
+ *   - include/linux/ptr_ring.h|166| <<ptr_ring_produce_bh>> ret = __ptr_ring_produce(r, ptr);
+ *   - tools/virtio/ringtest/ptr_ring.c|133| <<add_inbuf>> ret = __ptr_ring_produce(&array, buf);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int __ptr_ring_produce(struct ptr_ring *r, void *ptr)
 {
+	/* 确认是不是满了或者没初始化 */
 	if (unlikely(!r->size) || r->queue[r->producer])
 		return -ENOSPC;
 
@@ -124,6 +203,12 @@ static inline int __ptr_ring_produce(struct ptr_ring *r, void *ptr)
  * consume in interrupt or BH context, you must disable interrupts/BH when
  * calling this.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|48| <<skb_array_produce>> return ptr_ring_produce(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr; (有spinlock)
+ */
 static inline int ptr_ring_produce(struct ptr_ring *r, void *ptr)
 {
 	int ret;
@@ -135,6 +220,12 @@ static inline int ptr_ring_produce(struct ptr_ring *r, void *ptr)
 	return ret;
 }
 
+/*
+ * called by(skb_array_produce_irq()没人调用):
+ *   - include/linux/skb_array.h|53| <<skb_array_produce_irq>> return ptr_ring_produce_irq(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce_irq(struct ptr_ring *r, void *ptr)
 {
 	int ret;
@@ -146,6 +237,12 @@ static inline int ptr_ring_produce_irq(struct ptr_ring *r, void *ptr)
 	return ret;
 }
 
+/*
+ * called by(skb_array_produce_any()没人调用):
+ *   - include/linux/skb_array.h|63| <<skb_array_produce_any>> return ptr_ring_produce_any(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce_any(struct ptr_ring *r, void *ptr)
 {
 	unsigned long flags;
@@ -158,6 +255,12 @@ static inline int ptr_ring_produce_any(struct ptr_ring *r, void *ptr)
 	return ret;
 }
 
+/*
+ * called by(skb_array_produce_bh()没人调用):
+ *   - include/linux/skb_array.h|58| <<skb_array_produce_bh>> return ptr_ring_produce_bh(&a->ring, skb);
+ *
+ * 核心思想是ptr_ring->queue[ptr_ring->producer++] = ptr;
+ */
 static inline int ptr_ring_produce_bh(struct ptr_ring *r, void *ptr)
 {
 	int ret;
@@ -175,8 +278,21 @@ static inline int ptr_ring_produce_bh(struct ptr_ring *r, void *ptr)
  * If ring is never resized, and if the pointer is merely
  * tested, there's no need to take the lock - see e.g.  __ptr_ring_empty.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|195| <<__ptr_ring_empty>> return !__ptr_ring_peek(r);
+ *   - include/linux/ptr_ring.h|284| <<__ptr_ring_consume>> ptr = __ptr_ring_peek(r);
+ *   - include/linux/ptr_ring.h|448| <<__PTR_RING_PEEK_CALL>> #define __PTR_RING_PEEK_CALL(r, f) ((f)(__ptr_ring_peek(r)))
+ *   - include/linux/skb_array.h|72| <<__skb_array_empty>> return !__ptr_ring_peek(&a->ring);
+ *
+ * 核心思想是返回ptr_ring->queue[ptr_ring->consumer_head];
+ */
 static inline void *__ptr_ring_peek(struct ptr_ring *r)
 {
+	/*
+	 * struct ptr_ring *r:
+	 *  -> void **queue;
+	 */
 	if (likely(r->size))
 		return r->queue[r->consumer_head];
 	return NULL;
@@ -186,11 +302,27 @@ static inline void *__ptr_ring_peek(struct ptr_ring *r)
  * for example cpu_relax(). Callers must take consumer_lock
  * if the ring is ever resized - see e.g. ptr_ring_empty.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|203| <<ptr_ring_empty>> ret = __ptr_ring_empty(r);
+ *   - include/linux/ptr_ring.h|214| <<ptr_ring_empty_irq>> ret = __ptr_ring_empty(r);
+ *   - include/linux/ptr_ring.h|226| <<ptr_ring_empty_any>> ret = __ptr_ring_empty(r);
+ *   - include/linux/ptr_ring.h|237| <<ptr_ring_empty_bh>> ret = __ptr_ring_empty(r);
+ *   - tools/virtio/ringtest/ptr_ring.c|195| <<avail_empty>> return __ptr_ring_empty(&array);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool __ptr_ring_empty(struct ptr_ring *r)
 {
 	return !__ptr_ring_peek(r);
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|77| <<skb_array_empty>> return ptr_ring_empty(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty(struct ptr_ring *r)
 {
 	bool ret;
@@ -202,6 +334,12 @@ static inline bool ptr_ring_empty(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * called by(skb_array_empty_irq()没人调用):
+ *   - include/linux/skb_array.h|87| <<skb_array_empty_irq>> return ptr_ring_empty_irq(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty_irq(struct ptr_ring *r)
 {
 	bool ret;
@@ -213,6 +351,12 @@ static inline bool ptr_ring_empty_irq(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * called by(skb_array_empty_any()没人调用):
+ *   - include/linux/skb_array.h|92| <<skb_array_empty_any>> return ptr_ring_empty_any(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty_any(struct ptr_ring *r)
 {
 	unsigned long flags;
@@ -225,6 +369,12 @@ static inline bool ptr_ring_empty_any(struct ptr_ring *r)
 	return ret;
 }
 
+/*
+ * called by(skb_array_empty_bh()没人调用):
+ *   - include/linux/skb_array.h|82| <<skb_array_empty_bh>> return ptr_ring_empty_bh(&a->ring);
+ *
+ * 核心思想是判断ptr_ring->queue[ptr_ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool ptr_ring_empty_bh(struct ptr_ring *r)
 {
 	bool ret;
@@ -237,6 +387,10 @@ static inline bool ptr_ring_empty_bh(struct ptr_ring *r)
 }
 
 /* Must only be called after __ptr_ring_peek returned !NULL */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|286| <<__ptr_ring_consume>> __ptr_ring_discard_one(r);
+ */
 static inline void __ptr_ring_discard_one(struct ptr_ring *r)
 {
 	/* Fundamentally, what we want to do is update consumer
@@ -273,10 +427,23 @@ static inline void __ptr_ring_discard_one(struct ptr_ring *r)
 	}
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|301| <<__ptr_ring_consume_batched>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|320| <<ptr_ring_consume>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|335| <<ptr_ring_consume_irq>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|351| <<ptr_ring_consume_any>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|366| <<ptr_ring_consume_bh>> ptr = __ptr_ring_consume(r);
+ *   - include/linux/ptr_ring.h|628| <<__ptr_ring_swap_queue>> while ((ptr = __ptr_ring_consume(r)))
+ *   - tools/virtio/ringtest/ptr_ring.c|202| <<use_buf>> ptr = __ptr_ring_consume(&array);
+ */
 static inline void *__ptr_ring_consume(struct ptr_ring *r)
 {
 	void *ptr;
 
+	/*
+	 * 核心思想是返回ptr_ring->queue[ptr_ring->consumer_head];
+	 */
 	ptr = __ptr_ring_peek(r);
 	if (ptr)
 		__ptr_ring_discard_one(r);
@@ -287,6 +454,15 @@ static inline void *__ptr_ring_consume(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|382| <<ptr_ring_consume_batched>> ret = __ptr_ring_consume_batched(r, array, n);
+ *   - include/linux/ptr_ring.h|398| <<ptr_ring_consume_batched_irq>> ret = __ptr_ring_consume_batched(r, array, n);
+ *   - include/linux/ptr_ring.h|415| <<ptr_ring_consume_batched_any>> ret = __ptr_ring_consume_batched(r, array, n);
+ *   - include/linux/ptr_ring.h|431| <<ptr_ring_consume_batched_bh>> ret = __ptr_ring_consume_batched(r, array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int __ptr_ring_consume_batched(struct ptr_ring *r,
 					     void **array, int n)
 {
@@ -308,6 +484,11 @@ static inline int __ptr_ring_consume_batched(struct ptr_ring *r,
  * call this in interrupt or BH context, you must disable interrupts/BH when
  * producing.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|743| <<ptr_ring_cleanup>> while ((ptr = ptr_ring_consume(r)))
+ *   - include/linux/skb_array.h|97| <<skb_array_consume>> return ptr_ring_consume(&a->ring);
+ */
 static inline void *ptr_ring_consume(struct ptr_ring *r)
 {
 	void *ptr;
@@ -319,6 +500,10 @@ static inline void *ptr_ring_consume(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|108| <<skb_array_consume_irq>> return ptr_ring_consume_irq(&a->ring);
+ */
 static inline void *ptr_ring_consume_irq(struct ptr_ring *r)
 {
 	void *ptr;
@@ -330,6 +515,10 @@ static inline void *ptr_ring_consume_irq(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|119| <<skb_array_consume_any>> return ptr_ring_consume_any(&a->ring);
+ */
 static inline void *ptr_ring_consume_any(struct ptr_ring *r)
 {
 	unsigned long flags;
@@ -342,6 +531,10 @@ static inline void *ptr_ring_consume_any(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|131| <<skb_array_consume_bh>> return ptr_ring_consume_bh(&a->ring);
+ */
 static inline void *ptr_ring_consume_bh(struct ptr_ring *r)
 {
 	void *ptr;
@@ -353,6 +546,12 @@ static inline void *ptr_ring_consume_bh(struct ptr_ring *r)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|103| <<skb_array_consume_batched>> return ptr_ring_consume_batched(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched(struct ptr_ring *r,
 					   void **array, int n)
 {
@@ -365,6 +564,12 @@ static inline int ptr_ring_consume_batched(struct ptr_ring *r,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|114| <<skb_array_consume_batched_irq>> return ptr_ring_consume_batched_irq(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched_irq(struct ptr_ring *r,
 					       void **array, int n)
 {
@@ -377,6 +582,12 @@ static inline int ptr_ring_consume_batched_irq(struct ptr_ring *r,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|125| <<skb_array_consume_batched_any>> return ptr_ring_consume_batched_any(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched_any(struct ptr_ring *r,
 					       void **array, int n)
 {
@@ -390,6 +601,12 @@ static inline int ptr_ring_consume_batched_any(struct ptr_ring *r,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|137| <<skb_array_consume_batched_bh>> return ptr_ring_consume_batched_bh(&a->ring, (void **)array, n);
+ *
+ * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 					      void **array, int n)
 {
@@ -406,8 +623,21 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
  * Function must return a value.
  * Callers must take consumer_lock.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|419| <<PTR_RING_PEEK_CALL>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *   - include/linux/ptr_ring.h|428| <<PTR_RING_PEEK_CALL_IRQ>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *   - include/linux/ptr_ring.h|437| <<PTR_RING_PEEK_CALL_BH>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *   - include/linux/ptr_ring.h|447| <<PTR_RING_PEEK_CALL_ANY>> __PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \
+ *
+ * 核心思想是返回ptr_ring->queue[ptr_ring->consumer_head]作为参数被f调用
+ */
 #define __PTR_RING_PEEK_CALL(r, f) ((f)(__ptr_ring_peek(r)))
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|156| <<skb_array_peek_len>> return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	\
@@ -417,6 +647,10 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 	__PTR_RING_PEEK_CALL_v; \
 })
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|161| <<skb_array_peek_len_irq>> return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL_IRQ(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	\
@@ -426,6 +660,10 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 	__PTR_RING_PEEK_CALL_v; \
 })
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|166| <<skb_array_peek_len_bh>> return PTR_RING_PEEK_CALL_BH(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL_BH(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	\
@@ -435,6 +673,10 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 	__PTR_RING_PEEK_CALL_v; \
 })
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|171| <<skb_array_peek_len_any>> return PTR_RING_PEEK_CALL_ANY(&a->ring, __skb_array_len_with_tag);
+ */
 #define PTR_RING_PEEK_CALL_ANY(r, f) ({ \
 	typeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \
 	unsigned long __PTR_RING_PEEK_CALL_f;\
@@ -448,6 +690,17 @@ static inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,
 /* Not all gfp_t flags (besides GFP_KERNEL) are allowed. See
  * documentation for vmalloc for which of them are legal.
  */
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|482| <<ptr_ring_init>> r->queue = __ptr_ring_init_queue_alloc(size, gfp);
+ *   - include/linux/ptr_ring.h|585| <<ptr_ring_resize>> void **queue = __ptr_ring_init_queue_alloc(size, gfp);
+ *   - include/linux/ptr_ring.h|624| <<ptr_ring_resize_multiple>> queues[i] = __ptr_ring_init_queue_alloc(size, gfp);
+ *
+ * 一个例子:
+ * skb_array_init()
+ *  -> ptr_ring_init()
+ *      -> __ptr_ring_init_queue_alloc()
+ */
 static inline void **__ptr_ring_init_queue_alloc(unsigned int size, gfp_t gfp)
 {
 	if (size > KMALLOC_MAX_SIZE / sizeof(void *))
@@ -455,6 +708,11 @@ static inline void **__ptr_ring_init_queue_alloc(unsigned int size, gfp_t gfp)
 	return kvmalloc_array(size, sizeof(void *), gfp | __GFP_ZERO);
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|486| <<ptr_ring_init>> __ptr_ring_set_size(r, size);
+ *   - include/linux/ptr_ring.h|565| <<__ptr_ring_swap_queue>> __ptr_ring_set_size(r, size);
+ */
 static inline void __ptr_ring_set_size(struct ptr_ring *r, int size)
 {
 	r->size = size;
@@ -468,6 +726,11 @@ static inline void __ptr_ring_set_size(struct ptr_ring *r, int size)
 		r->batch = 1;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|176| <<skb_array_init>> return ptr_ring_init(&a->ring, size, gfp);
+ *   - tools/virtio/ringtest/ptr_ring.c|121| <<alloc_ring>> int ret = ptr_ring_init(&array, ring_size, 0);
+ */
 static inline int ptr_ring_init(struct ptr_ring *r, int size, gfp_t gfp)
 {
 	r->queue = __ptr_ring_init_queue_alloc(size, gfp);
@@ -492,6 +755,12 @@ static inline int ptr_ring_init(struct ptr_ring *r, int size, gfp_t gfp)
  * In particular if you consume ring in interrupt or BH context, you must
  * disable interrupts/BH when doing so.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|192| <<skb_array_unconsume>> ptr_ring_unconsume(&a->ring, (void **)skbs, n, __skb_array_destroy_skb);
+ *
+ * 把数据归还给ptr_ring->queue[]
+ */
 static inline void ptr_ring_unconsume(struct ptr_ring *r, void **batch, int n,
 				      void (*destroy)(void *))
 {
@@ -537,6 +806,11 @@ static inline void ptr_ring_unconsume(struct ptr_ring *r, void **batch, int n,
 	spin_unlock_irqrestore(&r->consumer_lock, flags);
 }
 
+/*
+ * called by:
+ *   - include/linux/ptr_ring.h|594| <<ptr_ring_resize>> old = __ptr_ring_swap_queue(r, queue, size, gfp, destroy);
+ *   - include/linux/ptr_ring.h|632| <<ptr_ring_resize_multiple>> queues[i] = __ptr_ring_swap_queue(rings[i], queues[i],
+ */
 static inline void **__ptr_ring_swap_queue(struct ptr_ring *r, void **queue,
 					   int size, gfp_t gfp,
 					   void (*destroy)(void *))
@@ -569,6 +843,10 @@ static inline void **__ptr_ring_swap_queue(struct ptr_ring *r, void **queue,
  * In particular if you consume ring in interrupt or BH context, you must
  * disable interrupts/BH when doing so.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|197| <<skb_array_resize>> return ptr_ring_resize(&a->ring, size, gfp, __skb_array_destroy_skb);
+ */
 static inline int ptr_ring_resize(struct ptr_ring *r, int size, gfp_t gfp,
 				  void (*destroy)(void *))
 {
@@ -598,6 +876,10 @@ static inline int ptr_ring_resize(struct ptr_ring *r, int size, gfp_t gfp,
  * In particular if you consume ring in interrupt or BH context, you must
  * disable interrupts/BH when doing so.
  */
+/*
+ * called by:
+ *   - include/linux/skb_array.h|205| <<skb_array_resize_multiple>> return ptr_ring_resize_multiple((struct ptr_ring **)rings,
+ */
 static inline int ptr_ring_resize_multiple(struct ptr_ring **rings,
 					   unsigned int nrings,
 					   int size,
@@ -643,6 +925,10 @@ static inline int ptr_ring_resize_multiple(struct ptr_ring **rings,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - include/linux/skb_array.h|212| <<skb_array_cleanup>> ptr_ring_cleanup(&a->ring, __skb_array_destroy_skb);
+ */
 static inline void ptr_ring_cleanup(struct ptr_ring *r, void (*destroy)(void *))
 {
 	void *ptr;
diff --git a/include/linux/skb_array.h b/include/linux/skb_array.h
index 8621ffdeecbf..bc52152b3513 100644
--- a/include/linux/skb_array.h
+++ b/include/linux/skb_array.h
@@ -33,31 +33,62 @@ struct skb_array {
 /* Might be slightly faster than skb_array_full below, but callers invoking
  * this in a loop must use a compiler barrier, for example cpu_relax().
  */
+/*
+ * called by:
+ *   - drivers/net/tap.c|333| <<tap_handle_frame>> if (__skb_array_full(&q->skb_array))
+ *
+ * 返回skb_array->ring->queue[skb_array->ring->producer]
+ */
 static inline bool __skb_array_full(struct skb_array *a)
 {
+	/* 返回ptr_ring->queue[ptr_ring->producer] */
 	return __ptr_ring_full(&a->ring);
 }
 
+/*
+ * 返回skb_array->ring->queue[skb_array->ring->producer]
+ *
+ * 没人调用
+ */
 static inline bool skb_array_full(struct skb_array *a)
 {
 	return ptr_ring_full(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|351| <<tap_handle_frame>> if (skb_array_produce(&q->skb_array, skb))
+ *   - drivers/net/tap.c|361| <<tap_handle_frame>> if (skb_array_produce(&q->skb_array, segs)) {
+ *   - drivers/net/tap.c|378| <<tap_handle_frame>> if (skb_array_produce(&q->skb_array, skb))
+ *   - drivers/net/tun.c|916| <<tun_net_xmit>> if (skb_array_produce(&tfile->tx_array, skb))
+ *
+ * 核心思想是skb_array->ring->queue[skb_array->ring->producer++] = skb
+ */
 static inline int skb_array_produce(struct skb_array *a, struct sk_buff *skb)
 {
+	/* 核心思想是skb_array->ring->queue[skb_array->ring->producer++] = skb; */
 	return ptr_ring_produce(&a->ring, skb);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_produce_irq(struct skb_array *a, struct sk_buff *skb)
 {
 	return ptr_ring_produce_irq(&a->ring, skb);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_produce_bh(struct skb_array *a, struct sk_buff *skb)
 {
 	return ptr_ring_produce_bh(&a->ring, skb);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_produce_any(struct skb_array *a, struct sk_buff *skb)
 {
 	return ptr_ring_produce_any(&a->ring, skb);
@@ -67,81 +98,147 @@ static inline int skb_array_produce_any(struct skb_array *a, struct sk_buff *skb
  * array is never resized. Also, callers invoking this in a loop must take care
  * to use a compiler barrier, for example cpu_relax().
  */
+/*
+ * 没人调用
+ */
 static inline bool __skb_array_empty(struct skb_array *a)
 {
 	return !__ptr_ring_peek(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|586| <<tap_poll>> if (!skb_array_empty(&q->skb_array))
+ *   - drivers/net/tun.c|1164| <<tun_chr_poll>> if (!skb_array_empty(&tfile->tx_array))
+ *
+ * 核心思想是判断skb_array->ring->queue[skb_array->ring->consumer_head]是否有值, 没有则为empty
+ */
 static inline bool skb_array_empty(struct skb_array *a)
 {
+	/* 核心思想是判断skb_array->ring->queue[skb_array->ring->consumer_head]是否有值, 没有则为empty */
 	return ptr_ring_empty(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline bool skb_array_empty_bh(struct skb_array *a)
 {
 	return ptr_ring_empty_bh(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline bool skb_array_empty_irq(struct skb_array *a)
 {
 	return ptr_ring_empty_irq(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline bool skb_array_empty_any(struct skb_array *a)
 {
 	return ptr_ring_empty_any(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|850| <<tap_do_read>> skb = skb_array_consume(&q->skb_array);
+ *   - drivers/net/tun.c|530| <<tun_queue_purge>> while ((skb = skb_array_consume(&tfile->tx_array)) != NULL)
+ *   - drivers/net/tun.c|1704| <<tun_ring_recv>> skb = skb_array_consume(&tfile->tx_array);
+ *   - drivers/net/tun.c|1716| <<tun_ring_recv>> skb = skb_array_consume(&tfile->tx_array);
+ *
+ * 这里就consume一个, 返回的是被consume的地址
+ */
 static inline struct sk_buff *skb_array_consume(struct skb_array *a)
 {
 	return ptr_ring_consume(&a->ring);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|269| <<vhost_net_buf_produce>> rxq->tail = skb_array_consume_batched(nvq->rx_array, rxq->queue,
+ *
+ * 返回值是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+ */
 static inline int skb_array_consume_batched(struct skb_array *a,
 					    struct sk_buff **array, int n)
 {
+	/*
+	 * 返回的是从ptr_ring放入参数array的数目 (最大是最后一个参数)
+	 */
 	return ptr_ring_consume_batched(&a->ring, (void **)array, n);
 }
 
+/*
+ * 没人调用
+ */
 static inline struct sk_buff *skb_array_consume_irq(struct skb_array *a)
 {
 	return ptr_ring_consume_irq(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_consume_batched_irq(struct skb_array *a,
 						struct sk_buff **array, int n)
 {
 	return ptr_ring_consume_batched_irq(&a->ring, (void **)array, n);
 }
 
+/*
+ * 没人调用
+ */
 static inline struct sk_buff *skb_array_consume_any(struct skb_array *a)
 {
 	return ptr_ring_consume_any(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_consume_batched_any(struct skb_array *a,
 						struct sk_buff **array, int n)
 {
 	return ptr_ring_consume_batched_any(&a->ring, (void **)array, n);
 }
 
-
+/*
+ * 没人调用
+ */
 static inline struct sk_buff *skb_array_consume_bh(struct skb_array *a)
 {
 	return ptr_ring_consume_bh(&a->ring);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_consume_batched_bh(struct skb_array *a,
 					       struct sk_buff **array, int n)
 {
 	return ptr_ring_consume_batched_bh(&a->ring, (void **)array, n);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|320| <<vhost_net_buf_peek>> return __skb_array_len_with_tag(vhost_net_buf_get_ptr(rxq));
+ *   - include/linux/skb_array.h|168| <<skb_array_peek_len>> return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
+ *   - include/linux/skb_array.h|173| <<skb_array_peek_len_irq>> return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
+ *   - include/linux/skb_array.h|178| <<skb_array_peek_len_bh>> return PTR_RING_PEEK_CALL_BH(&a->ring, __skb_array_len_with_tag);
+ *   - include/linux/skb_array.h|183| <<skb_array_peek_len_any>> return PTR_RING_PEEK_CALL_ANY(&a->ring, __skb_array_len_with_tag);
+ */
 static inline int __skb_array_len_with_tag(struct sk_buff *skb)
 {
 	if (likely(skb)) {
 		int len = skb->len;
 
+		/*
+		 * 返回((__skb)->vlan_tci & VLAN_TAG_PRESENT)
+		 */
 		if (skb_vlan_tag_present(skb))
 			len += VLAN_HLEN;
 
@@ -151,47 +248,96 @@ static inline int __skb_array_len_with_tag(struct sk_buff *skb)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|1182| <<tap_peek_len>> return skb_array_peek_len(&q->skb_array);
+ *   - drivers/net/tun.c|1912| <<tun_peek_len>> ret = skb_array_peek_len(&tfile->tx_array);
+ *
+ * 核心思想是返回skb_array->ring->queue[skb_array->ring->consumer_head]作为参数被__skb_array_len_with_tag()调用
+ */
 static inline int skb_array_peek_len(struct skb_array *a)
 {
+	/*
+	 * 核心思想是返回skb_array->ring->queue[skb_array->ring->consumer_head]作为参数被__skb_array_len_with_tag()调用
+	 */
 	return PTR_RING_PEEK_CALL(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_peek_len_irq(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_IRQ(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_peek_len_bh(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_BH(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * 没人调用
+ */
 static inline int skb_array_peek_len_any(struct skb_array *a)
 {
 	return PTR_RING_PEEK_CALL_ANY(&a->ring, __skb_array_len_with_tag);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|520| <<tap_open>> if (skb_array_init(&q->skb_array, tap->dev->tx_queue_len, GFP_KERNEL)) {
+ *   - drivers/net/tun.c|2617| <<tun_chr_open>> if (skb_array_init(&tfile->tx_array, 0, GFP_KERNEL)) {
+ */
 static inline int skb_array_init(struct skb_array *a, int size, gfp_t gfp)
 {
 	return ptr_ring_init(&a->ring, size, gfp);
 }
 
+/*
+ * 在以下使用__skb_array_destroy_skb():
+ *   - include/linux/skb_array.h|192| <<skb_array_unconsume>> ptr_ring_unconsume(&a->ring, (void **)skbs, n, __skb_array_destroy_skb);
+ *   - include/linux/skb_array.h|197| <<skb_array_resize>> return ptr_ring_resize(&a->ring, size, gfp, __skb_array_destroy_skb);
+ *   - include/linux/skb_array.h|207| <<skb_array_resize_multiple>> __skb_array_destroy_skb);
+ *   - include/linux/skb_array.h|212| <<skb_array_cleanup>> ptr_ring_cleanup(&a->ring, __skb_array_destroy_skb);
+ */
 static void __skb_array_destroy_skb(void *ptr)
 {
 	kfree_skb(ptr);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|293| <<vhost_net_buf_unproduce>> skb_array_unconsume(nvq->rx_array, rxq->queue + rxq->head,
+ *
+ * 把数据归还给skb_array->ring->queue[]
+ */
 static inline void skb_array_unconsume(struct skb_array *a,
 				       struct sk_buff **skbs, int n)
 {
+	/*
+	 * 把数据归还给ptr_ring->queue[]
+	 */
 	ptr_ring_unconsume(&a->ring, (void **)skbs, n, __skb_array_destroy_skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tun.c|668| <<tun_attach>> skb_array_resize(&tfile->tx_array, dev->tx_queue_len, GFP_KERNEL)) {
+ */
 static inline int skb_array_resize(struct skb_array *a, int size, gfp_t gfp)
 {
 	return ptr_ring_resize(&a->ring, size, gfp, __skb_array_destroy_skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|1236| <<tap_queue_resize>> ret = skb_array_resize_multiple(arrays, n,
+ *   - drivers/net/tun.c|2803| <<tun_queue_resize>> ret = skb_array_resize_multiple(arrays, n,
+ */
 static inline int skb_array_resize_multiple(struct skb_array **rings,
 					    int nrings, unsigned int size,
 					    gfp_t gfp)
@@ -202,6 +348,11 @@ static inline int skb_array_resize_multiple(struct skb_array **rings,
 					__skb_array_destroy_skb);
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|500| <<tap_sock_destruct>> skb_array_cleanup(&q->skb_array);
+ *   - drivers/net/tun.c|578| <<__tun_detach>> skb_array_cleanup(&tfile->tx_array);
+ */
 static inline void skb_array_cleanup(struct skb_array *a)
 {
 	ptr_ring_cleanup(&a->ring, __skb_array_destroy_skb);
diff --git a/include/scsi/scsi_cmnd.h b/include/scsi/scsi_cmnd.h
index cb9bf0fee1d4..1db0b7d2d61a 100644
--- a/include/scsi/scsi_cmnd.h
+++ b/include/scsi/scsi_cmnd.h
@@ -91,6 +91,19 @@ struct scsi_cmnd {
 	 * allocated.  It is used to time how long the command has
 	 * been outstanding
 	 */
+	/*
+	 * 在以下使用scsi_cmnd->jiffies_at_alloc:
+	 *   - drivers/scsi/libiscsi.c|2074| <<iscsi_eh_cmd_timed_out>> if (time_after(running_task->sc->jiffies_at_alloc,
+	 *   - drivers/scsi/libiscsi.c|2075| <<iscsi_eh_cmd_timed_out>> task->sc->jiffies_at_alloc))
+	 *   - drivers/scsi/megaraid/megaraid_sas_base.c|2885| <<megasas_reset_timer>> if (time_after(jiffies, scmd->jiffies_at_alloc +
+	 *   - drivers/scsi/qla2xxx/qla_target.c|4353| <<qlt_get_tag>> cmd->jiffies_at_alloc = get_jiffies_64();
+	 *   - drivers/scsi/scsi_debugfs.c|10| <<scsi_show_rq>> int msecs = jiffies_to_msecs(jiffies - cmd->jiffies_at_alloc);
+	 *   - drivers/scsi/scsi_lib.c|988| <<scsi_io_completion>> time_before(cmd->jiffies_at_alloc + wait_for, jiffies))
+	 *   - drivers/scsi/scsi_lib.c|1167| <<scsi_initialize_rq>> cmd->jiffies_at_alloc = jiffies;
+	 *   - drivers/scsi/scsi_lib.c|1216| <<scsi_init_command>> jiffies_at_alloc = cmd->jiffies_at_alloc;
+	 *   - drivers/scsi/scsi_lib.c|1227| <<scsi_init_command>> cmd->jiffies_at_alloc = jiffies_at_alloc;
+	 *   - drivers/scsi/scsi_lib.c|1652| <<scsi_softirq_done>> time_before(cmd->jiffies_at_alloc + wait_for, jiffies)) {
+	 */
 	unsigned long jiffies_at_alloc;
 
 	int retries;
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index 6d5d5faa989b..989296b50850 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -47,6 +47,16 @@
 /* The Host uses this in used->flags to advise the Guest: don't kick me when
  * you add a buffer.  It's unreliable, so it's simply an optimization.  Guest
  * will still kick if it's out of buffers. */
+/*
+ * 在以下使用VRING_USED_F_NO_NOTIFY:
+ *   - drivers/vhost/vhost.c|2142| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+ *   - drivers/vhost/vhost.c|2337| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+ *   - drivers/vhost/vhost.c|2339| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vhost.c|2374| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+ *   - drivers/vhost/vhost.c|2376| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+ *   - drivers/vhost/vringh.c|544| <<__vringh_notify_disable>> VRING_USED_F_NO_NOTIFY)) {
+ *   - drivers/virtio/virtio_ring.c|578| <<virtqueue_kick_prepare>> needs_kick = !(vq->vring.used->flags & cpu_to_virtio16(_vq->vdev, VRING_USED_F_NO_NOTIFY));
+ */
 #define VRING_USED_F_NO_NOTIFY	1
 /* The Guest uses this in avail->flags to advise the Host: don't interrupt me
  * when you consume a buffer.  It's unreliable, so it's simply an
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index 043bfc35b353..180b7373960d 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -497,6 +497,19 @@ static bool irq_check_poll(struct irq_desc *desc)
 
 static bool irq_may_run(struct irq_desc *desc)
 {
+	/*
+	 * 在以下使用IRQD_IRQ_INPROGRESS:
+	 *   - kernel/irq/debugfs.c|103| <<global>> BIT_MASK_DESCR(IRQD_IRQ_INPROGRESS),
+	 *   - include/linux/irq.h|327| <<irqd_irq_inprogress>> return __irqd_to_state(d) & IRQD_IRQ_INPROGRESS;
+	 *   - kernel/irq/chip.c|473| <<handle_nested_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|484| <<handle_nested_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|500| <<irq_may_run>> unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
+	 *   - kernel/irq/chip.c|586| <<handle_untracked_irq>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/chip.c|592| <<handle_untracked_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|201| <<handle_irq_event>> irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/handle.c|207| <<handle_irq_event>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 *   - kernel/irq/manage.c|1380| <<__setup_irq>> irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	 */
 	unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;
 
 	/*
diff --git a/kernel/irq/spurious.c b/kernel/irq/spurious.c
index 987d7bca4864..92d012459037 100644
--- a/kernel/irq/spurious.c
+++ b/kernel/irq/spurious.c
@@ -94,6 +94,9 @@ static int try_one_irq(struct irq_desc *desc, bool force)
 		goto out;
 
 	/* Already running on another processor */
+	/*
+	 * 查看__irqd_to_state(d) & IRQD_IRQ_INPROGRESS;
+	 */
 	if (irqd_irq_inprogress(&desc->irq_data)) {
 		/*
 		 * Already running: If it is shared get the other
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 3e3650e94ae6..b1a758c48439 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -1383,6 +1383,11 @@ static void rcu_check_gp_kthread_starvation(struct rcu_state *rsp)
  * that don't support NMI-based stack dumps.  The NMI-triggered stack
  * traces are more accurate because they are printed by the target CPU.
  */
+/*
+ * called by:
+ *   - kernel/rcu/tree.c|1485| <<print_other_cpu_stall>> rcu_dump_cpu_stacks(rsp);
+ *   - kernel/rcu/tree.c|1542| <<print_cpu_stall>> rcu_dump_cpu_stacks(rsp);
+ */
 static void rcu_dump_cpu_stacks(struct rcu_state *rsp)
 {
 	int cpu;
diff --git a/lib/nmi_backtrace.c b/lib/nmi_backtrace.c
index 70b1f9d830cd..26b1f79cf1ac 100644
--- a/lib/nmi_backtrace.c
+++ b/lib/nmi_backtrace.c
@@ -33,6 +33,14 @@ static unsigned long backtrace_flag;
  * directly from their raise() functions may rely on the mask
  * they are passed being updated as a side effect of this call.
  */
+/*
+ * called by:
+ *   - arch/arm/kernel/smp.c|757| <<arch_trigger_cpumask_backtrace>> nmi_trigger_cpumask_backtrace(mask, exclude_self, raise_nmi);
+ *   - arch/mips/kernel/process.c|694| <<arch_trigger_cpumask_backtrace>> nmi_trigger_cpumask_backtrace(mask, exclude_self, raise_backtrace);
+ *   - arch/powerpc/kernel/watchdog.c|415| <<arch_trigger_cpumask_backtrace>> nmi_trigger_cpumask_backtrace(mask, exclude_self, raise_backtrace_ipi);
+ *   - arch/tile/kernel/process.c|656| <<arch_trigger_cpumask_backtrace>> nmi_trigger_cpumask_backtrace(mask, exclude_self,
+ *   - arch/x86/kernel/apic/hw_nmi.c|38| <<arch_trigger_cpumask_backtrace>> nmi_trigger_cpumask_backtrace(mask, exclude_self,
+ */
 void nmi_trigger_cpumask_backtrace(const cpumask_t *mask,
 				   bool exclude_self,
 				   void (*raise)(cpumask_t *mask))
diff --git a/net/core/dev.c b/net/core/dev.c
index 5d035486ed4a..3febd12fab7f 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -4404,6 +4404,11 @@ static inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - net/core/dev.c|4573| <<__netif_receive_skb>> ret = __netif_receive_skb_core(skb, true);
+ *   - net/core/dev.c|4576| <<__netif_receive_skb>> ret = __netif_receive_skb_core(skb, false);
+ */
 static int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)
 {
 	struct packet_type *ptype, *pt_prev;
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index e4d90224507a..31f7b73bd0f4 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -114,6 +114,15 @@ irqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)
 /*
  * Race-free decouple logic (ordering is critical)
  */
+/*
+ * [0] irqfd_shutdown
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|308| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->shutdown, irqfd_shutdown);
+ */
 static void
 irqfd_shutdown(struct work_struct *work)
 {
@@ -165,6 +174,20 @@ irqfd_is_active(struct kvm_kernel_irqfd *irqfd)
  *
  * assumes kvm->irqfds.lock is held
  */
+/*
+ * [0] irqfd_deactivate
+ * [0] kvm_irqfd
+ * [0] kvm_vm_ioctl
+ * [0] do_vfs_ioctl
+ * [0] sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|253| <<irqfd_wakeup>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|577| <<kvm_irqfd_deassign>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|622| <<kvm_irqfd_release>> irqfd_deactivate(irqfd);
+ */
 static void
 irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 {
@@ -172,6 +195,9 @@ irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 
 	list_del_init(&irqfd->list);
 
+	/*
+	 * irqfd_shutdown()
+	 */
 	queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
 }
 
@@ -187,6 +213,10 @@ int __attribute__((weak)) kvm_arch_set_irq_inatomic(
 /*
  * Called with wqh->lock held and interrupts disabled
  */
+/*
+ * 在以下使用irqfd_wakeup():
+ *   - virt/kvm/eventfd.c|381| <<kvm_irqfd_assign>> init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);
+ */
 static int
 irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
@@ -284,6 +314,10 @@ int  __attribute__((weak)) kvm_arch_update_irqfd_routing(
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|607| <<kvm_irqfd>> return kvm_irqfd_assign(kvm, args);
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -525,12 +559,25 @@ kvm_eventfd_init(struct kvm *kvm)
 /*
  * shutdown any irqfd's that match fd+gsi
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|579| <<kvm_irqfd>> return kvm_irqfd_deassign(kvm, args);
+ */
 static int
 kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	struct kvm_kernel_irqfd *irqfd, *tmp;
 	struct eventfd_ctx *eventfd;
 
+	/*
+	 * struct kvm_irqfd {
+	 *     __u32 fd;
+	 *     __u32 gsi;
+	 *     __u32 flags;
+	 *     __u32 resamplefd;
+	 *     __u8  pad[16];
+	 * };
+	 */
 	eventfd = eventfd_ctx_fdget(args->fd);
 	if (IS_ERR(eventfd))
 		return PTR_ERR(eventfd);
@@ -565,9 +612,22 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * 处理KVM_IRQFD:
+ *   - virt/kvm/kvm_main.c|3212| <<kvm_vm_ioctl(KVM_IRQFD)>> r = kvm_irqfd(kvm, &data);
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
+	/*
+	 * struct kvm_irqfd {
+	 *     __u32 fd;
+	 *     __u32 gsi;
+	 *     __u32 flags;
+	 *     __u32 resamplefd;
+	 *      __u8  pad[16];
+	 * };
+	 */
 	if (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))
 		return -EINVAL;
 
@@ -581,6 +641,10 @@ kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
  * This function is called as the kvm VM fd is being released. Shutdown all
  * irqfds that still remain open
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|811| <<kvm_vm_release>> kvm_irqfd_release(kvm);
+ */
 void
 kvm_irqfd_release(struct kvm *kvm)
 {
@@ -632,6 +696,10 @@ void kvm_irq_routing_update(struct kvm *kvm)
  * aggregated from all vm* instances. We need our own isolated
  * queue to ease flushing work items when a VM exits.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4194| <<kvm_init>> r = kvm_irqfd_init();
+ */
 int kvm_irqfd_init(void)
 {
 	irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
@@ -670,9 +738,18 @@ struct _ioeventfd {
 static inline struct _ioeventfd *
 to_ioeventfd(struct kvm_io_device *dev)
 {
+	/*
+	 * struct _ioeventfd:
+	 *   -> struct kvm_io_device dev;
+	 */
 	return container_of(dev, struct _ioeventfd, dev);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|820| <<ioeventfd_destructor>> ioeventfd_release(p);
+ *   - virt/kvm/eventfd.c|950| <<kvm_deassign_ioeventfd_idx>> ioeventfd_release(p);
+ */
 static void
 ioeventfd_release(struct _ioeventfd *p)
 {
@@ -681,6 +758,10 @@ ioeventfd_release(struct _ioeventfd *p)
 	kfree(p);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|804| <<ioeventfd_write>> if (!ioeventfd_in_range(p, addr, len, val))
+ */
 static bool
 ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 {
@@ -727,6 +808,20 @@ ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 }
 
 /* MMIO/PIO writes trigger an event if the addr/val match */
+/*
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int
 ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
 		int len, const void *val)
@@ -752,6 +847,21 @@ ioeventfd_destructor(struct kvm_io_device *this)
 	ioeventfd_release(p);
 }
 
+/*
+ * 使用kvm_iodevice_init()使用ioevenfd_ops. 以下是调用kvm_iodevice_init()的地方:
+ *   - arch/x86/kvm/i8254.c|691| <<kvm_create_pit>> kvm_iodevice_init(&pit->dev, &pit_dev_ops);
+ *   - arch/x86/kvm/i8254.c|698| <<kvm_create_pit>> kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);
+ *   - arch/x86/kvm/i8259.c|603| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
+ *   - arch/x86/kvm/i8259.c|604| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
+ *   - arch/x86/kvm/i8259.c|605| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_eclr, &picdev_eclr_ops);
+ *   - arch/x86/kvm/ioapic.c|635| <<kvm_ioapic_init>> kvm_iodevice_init(&ioapic->dev, &ioapic_mmio_ops);
+ *   - arch/x86/kvm/lapic.c|2276| <<kvm_create_lapic>> kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
+ *   - virt/kvm/coalesced_mmio.c|150| <<kvm_vm_ioctl_register_coalesced_mmio>> kvm_iodevice_init(&dev->dev, &coalesced_mmio_ops);
+ *   - virt/kvm/eventfd.c|917| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ *
+ * 在以下使用ioeventfd_ops:
+ *   - virt/kvm/eventfd.c|894| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static const struct kvm_io_device_ops ioeventfd_ops = {
 	.write      = ioeventfd_write,
 	.destructor = ioeventfd_destructor,
@@ -763,6 +873,13 @@ ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
 {
 	struct _ioeventfd *_p;
 
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|555| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|834| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|902| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|933| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	list_for_each_entry(_p, &kvm->ioeventfds, list)
 		if (_p->bus_idx == p->bus_idx &&
 		    _p->addr == p->addr &&
@@ -775,6 +892,15 @@ ioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)
 	return false;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1019| <<kvm_deassign_ioeventfd>> enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
+ *   - virt/kvm/eventfd.c|1034| <<kvm_assign_ioeventfd>> bus_idx = ioeventfd_bus_from_flags(args->flags);
+ *
+ * --> flags & KVM_IOEVENTFD_FLAG_PIO               --> KVM_PIO_BUS
+ * --> flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY --> KVM_VIRTIO_CCW_NOTIFY_BUS
+ * --> else                                         --> KVM_VIRTIO_CCW_NOTIFY_BUS
+ */
 static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 {
 	if (flags & KVM_IOEVENTFD_FLAG_PIO)
@@ -784,6 +910,11 @@ static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 	return KVM_MMIO_BUS;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1004| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);
+ *   - virt/kvm/eventfd.c|1012| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);
+ */
 static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 				enum kvm_bus bus_idx,
 				struct kvm_ioeventfd *args)
@@ -823,6 +954,7 @@ static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 		goto unlock_fail;
 	}
 
+	/* 设置kvm_io_device->ops = ioeventfd_ops */
 	kvm_iodevice_init(&p->dev, &ioeventfd_ops);
 
 	ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
@@ -830,7 +962,18 @@ static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 	if (ret < 0)
 		goto unlock_fail;
 
+	/*
+	 * struct kvm:
+	 *   -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	kvm_get_bus(kvm, bus_idx)->ioeventfd_count++;
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|555| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|834| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|902| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|933| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	list_add_tail(&p->list, &kvm->ioeventfds);
 
 	mutex_unlock(&kvm->slots_lock);
@@ -893,6 +1036,11 @@ kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,
 
 static int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
+	/*
+	 * --> flags & KVM_IOEVENTFD_FLAG_PIO               --> KVM_PIO_BUS
+	 * --> flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 * --> else                                         --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 */
 	enum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);
 	int ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);
 
@@ -908,6 +1056,11 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	enum kvm_bus              bus_idx;
 	int ret;
 
+	/*
+	 * --> flags & KVM_IOEVENTFD_FLAG_PIO               --> KVM_PIO_BUS
+	 * --> flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 * --> else                                         --> KVM_VIRTIO_CCW_NOTIFY_BUS
+	 */
 	bus_idx = ioeventfd_bus_from_flags(args->flags);
 	/* must be natural-word sized, or 0 to ignore length */
 	switch (args->len) {
@@ -954,6 +1107,10 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * 处理KVM_IOEVENTFD:
+ *   - virt/kvm/kvm_main.c|3224| <<kvm_vm_ioctl(KVM_IOEVENTFD)>> r = kvm_ioeventfd(kvm, &data);
+ */
 int
 kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index a7de45f4c1b5..d7450d52c959 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -95,6 +95,21 @@ EXPORT_SYMBOL_GPL(halt_poll_ns_shrink);
 
 DEFINE_MUTEX(kvm_lock);
 static DEFINE_RAW_SPINLOCK(kvm_count_lock);
+/*
+ * struct kvm {
+ *     spinlock_t mmu_lock;
+ *     struct mutex slots_lock;
+ *     struct mm_struct *mm;
+ *     struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+ *     struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
+ *
+ *     atomic_t online_vcpus;
+ *     int created_vcpus;
+ *     int last_boosted_vcpu;
+ *     struct list_head vm_list;
+ *
+ * 把kvm->vm_list链上去
+ */
 LIST_HEAD(vm_list);
 
 static cpumask_var_t cpus_hardware_enabled;
@@ -210,6 +225,11 @@ static inline bool kvm_kick_many_cpus(const struct cpumask *cpus, bool wait)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1371| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm,
+ *   - virt/kvm/kvm_main.c|255| <<kvm_make_all_cpus_request>> called = kvm_make_vcpus_request_mask(kvm, req, vcpu_bitmap, cpus);
+ */
 bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 				 unsigned long *vcpu_bitmap, cpumask_var_t tmp)
 {
@@ -240,6 +260,17 @@ bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 	return called;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1940| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|4671| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|7371| <<kvm_make_scan_ioapic_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
+ *   - arch/x86/kvm/x86.c|7420| <<kvm_arch_mmu_notifier_invalidate_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+ *   - virt/kvm/arm/arm.c|598| <<kvm_arm_halt_guest>> kvm_make_all_cpus_request(kvm, KVM_REQ_SLEEP);
+ *   - virt/kvm/arm/psci.c|219| <<kvm_prepare_system_event>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
+ *   - virt/kvm/kvm_main.c|299| <<kvm_flush_remote_tlbs>> || kvm_make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))
+ *   - virt/kvm/kvm_main.c|308| <<kvm_reload_remote_mmus>> kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);
+ */
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 {
 	cpumask_var_t cpus;
@@ -3611,9 +3642,22 @@ static int kvm_io_bus_sort_cmp(const void *p1, const void *p2)
 	return kvm_io_bus_cmp(p1, p2);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3651| <<__kvm_io_bus_write>> idx = kvm_io_bus_get_first_dev(bus, range->addr, range->len);
+ *   - virt/kvm/kvm_main.c|3735| <<__kvm_io_bus_read>> idx = kvm_io_bus_get_first_dev(bus, range->addr, range->len);
+ *   - virt/kvm/kvm_main.c|3886| <<kvm_io_bus_get_dev>> dev_idx = kvm_io_bus_get_first_dev(bus, addr, 1);
+ */
 static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 			     gpa_t addr, int len)
 {
+	/*
+	 * struct kvm_io_range {
+	 *     gpa_t addr;
+	 *     int len;
+	 *     struct kvm_io_device *dev;
+	 * };
+	 */
 	struct kvm_io_range *range, key;
 	int off;
 
@@ -3635,6 +3679,11 @@ static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 	return off;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3677| <<kvm_io_bus_write>> r = __kvm_io_bus_write(vcpu, bus, &range, val);
+ *   - virt/kvm/kvm_main.c|3708| <<kvm_io_bus_write_cookie>> return __kvm_io_bus_write(vcpu, bus, &range, val);
+ */
 static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 			      struct kvm_io_range *range, const void *val)
 {
@@ -3656,6 +3705,15 @@ static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 }
 
 /* kvm_io_bus_write - called under kvm->slots_lock */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s.c|943| <<kvmppc_h_logical_ci_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);
+ *   - arch/powerpc/kvm/powerpc.c|1329| <<kvmppc_handle_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
+ *   - arch/x86/kvm/vmx.c|7800| <<handle_ept_misconfig>> !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
+ *   - arch/x86/kvm/x86.c|4810| <<vcpu_mmio_write>> && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))
+ *   - arch/x86/kvm/x86.c|5415| <<kernel_pio>> r = kvm_io_bus_write(vcpu, KVM_PIO_BUS,
+ *   - virt/kvm/arm/mmio.c|194| <<io_mem_abort>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
+ */
 int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val)
 {
@@ -3668,6 +3726,11 @@ int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		.len = len,
 	};
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 *  -> struct kvm *kvm;
+	 *     -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	bus = srcu_dereference(vcpu->kvm->buses[bus_idx], &vcpu->kvm->srcu);
 	if (!bus)
 		return -ENOMEM;
@@ -3748,6 +3811,22 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 
 
 /* Caller must hold slots_lock. */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/mpic.c|1450| <<map_mmio>> kvm_io_bus_register_dev(opp->kvm, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/i8254.c|692| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
+ *   - arch/x86/kvm/i8254.c|699| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
+ *   - arch/x86/kvm/i8259.c|607| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
+ *   - arch/x86/kvm/i8259.c|616| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_eclr);
+ *   - arch/x86/kvm/ioapic.c|638| <<kvm_ioapic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, ioapic->base_address,
+ *   - virt/kvm/arm/vgic/vgic-its.c|1672| <<vgic_register_its_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|651| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,
+ *   - virt/kvm/arm/vgic/vgic-mmio-v3.c|666| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, sgi_base,
+ *   - virt/kvm/arm/vgic/vgic-mmio.c|892| <<vgic_register_dist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,
+ *   - virt/kvm/coalesced_mmio.c|155| <<kvm_vm_ioctl_register_coalesced_mmio>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, zone->addr,
+ *   - virt/kvm/eventfd.c|960| <<kvm_assign_ioeventfd_idx>> ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
+ */
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev)
 {
@@ -3755,6 +3834,10 @@ int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 	struct kvm_io_bus *new_bus, *bus;
 	struct kvm_io_range range;
 
+	/*
+	 * struct kvm:
+	 *   -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	bus = kvm_get_bus(kvm, bus_idx);
 	if (!bus)
 		return -ENOMEM;
@@ -4171,6 +4254,17 @@ static void kvm_sched_out(struct preempt_notifier *pn,
 	kvm_arch_vcpu_put(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/mips/kvm/mips.c|1707| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1015| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|556| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|425| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|4144| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm.c|7183| <<svm_init>> return kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx.c|14713| <<vmx_init>> r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+ *   - virt/kvm/arm/arm.c|1686| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
-- 
2.17.1

