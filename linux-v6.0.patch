From d21fc56811f7fc2decdce3f338b56cb91bbe03b0 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 16 Nov 2022 03:25:09 -0800
Subject: [PATCH 1/1] linux v6.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/events/amd/core.c             |  26 ++
 arch/x86/events/core.c                 |  14 +
 arch/x86/events/perf_event.h           |  10 +
 arch/x86/include/asm/kvm_host.h        | 100 +++++
 arch/x86/include/asm/nmi.h             |  45 ++
 arch/x86/include/asm/nospec-branch.h   |   7 +
 arch/x86/include/asm/perf_event.h      |  23 +
 arch/x86/include/asm/spec-ctrl.h       |   8 +
 arch/x86/kernel/cpu/bugs.c             |  12 +
 arch/x86/kernel/nmi.c                  |  95 ++++
 arch/x86/kernel/nmi_selftest.c         |  32 ++
 arch/x86/kernel/sev.c                  |  18 +
 arch/x86/kvm/cpuid.c                   |   9 +
 arch/x86/kvm/cpuid.h                   |  19 +
 arch/x86/kvm/kvm_cache_regs.h          |   7 +
 arch/x86/kvm/lapic.c                   |   7 +
 arch/x86/kvm/mmu/mmu.c                 |  12 +
 arch/x86/kvm/mmu/tdp_mmu.c             |  13 +
 arch/x86/kvm/pmu.c                     | 228 ++++++++++
 arch/x86/kvm/pmu.h                     |  73 +++
 arch/x86/kvm/reverse_cpuid.h           |  12 +
 arch/x86/kvm/svm/pmu.c                 | 128 ++++++
 arch/x86/kvm/svm/svm.c                 |   7 +
 arch/x86/kvm/vmx/capabilities.h        |  44 ++
 arch/x86/kvm/vmx/pmu_intel.c           | 107 +++++
 arch/x86/kvm/vmx/vmx.c                 |   4 +
 arch/x86/kvm/x86.c                     |  60 +++
 arch/x86/mm/extable.c                  |   7 +
 block/blk-mq.c                         |   8 +
 drivers/block/ublk_drv.c               |   8 +
 drivers/block/virtio_blk.c             |   5 +
 drivers/pci/iov.c                      |   7 +
 drivers/virtio/virtio.c                |  43 ++
 drivers/virtio/virtio_balloon.c        |   7 +
 drivers/virtio/virtio_pci_common.c     |  40 ++
 drivers/virtio/virtio_pci_modern.c     |  13 +
 drivers/virtio/virtio_pci_modern_dev.c |   7 +
 drivers/virtio/virtio_ring.c           | 585 +++++++++++++++++++++++++
 include/linux/kvm_host.h               |  11 +
 include/uapi/linux/virtio_ring.h       |  28 ++
 kernel/cpu.c                           |  11 +
 kernel/events/core.c                   |  21 +
 security/integrity/ima/ima_efi.c       |  19 +
 security/integrity/ima/ima_main.c      |   4 +
 tools/perf/builtin-stat.c              |  38 ++
 tools/perf/util/evsel.h                |  38 ++
 tools/perf/util/stat-display.c         |  21 +
 47 files changed, 2041 insertions(+)

diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index 9ac371841..488d9c87e 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -1348,6 +1348,10 @@ static const struct attribute_group *amd_attr_update[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|1455| <<amd_pmu_init>> ret = amd_core_pmu_init();
+ */
 static int __init amd_core_pmu_init(void)
 {
 	union cpuid_0x80000022_ebx ebx;
@@ -1371,6 +1375,10 @@ static int __init amd_core_pmu_init(void)
 
 	/* Check for Performance Monitoring v2 support */
 	if (boot_cpu_has(X86_FEATURE_PERFMON_V2)) {
+		/*
+		 * AMD Extended Performance Monitoring and Debug cpuid feature detection
+		 * #define EXT_PERFMON_DEBUG_FEATURES 0x80000022
+		 */
 		ebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);
 
 		/* Update PMU version for later usage */
@@ -1442,14 +1450,32 @@ static int __init amd_core_pmu_init(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2105| <<init_hw_perf_events>> err = amd_pmu_init();
+ *   - arch/x86/events/core.c|2108| <<init_hw_perf_events>> err = amd_pmu_init();
+ */
 __init int amd_pmu_init(void)
 {
 	int ret;
 
 	/* Performance-monitoring supported from K7 and later: */
+	/*
+	 * 应该是在以下获得x86:
+	 * 1929 void __init identify_boot_cpu(void)
+	 * 1930 {
+	 * 1931         identify_cpu(&boot_cpu_data);
+	 */
 	if (boot_cpu_data.x86 < 6)
 		return -ENODEV;
 
+	/*
+	 * static __initconst const struct x86_pmu amd_pmu = {
+	 *     .name                   = "AMD",
+	 *     .eventsel               = MSR_K7_EVNTSEL0,
+	 *     .perfctr                = MSR_K7_PERFCTR0,
+	 *     .num_counters           = AMD64_NUM_COUNTERS,
+	 */
 	x86_pmu = amd_pmu;
 
 	ret = amd_core_pmu_init();
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index f969410d0..722496d8b 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -247,6 +247,11 @@ static void release_pmc_hardware(void) {}
 
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2113| <<init_hw_perf_events>> if (!check_hw_exists(&pmu, x86_pmu.num_counters, x86_pmu.num_counters_fixed))
+ *   - arch/x86/events/intel/core.c|4504| <<init_hybrid_pmu>> if (!check_hw_exists(&pmu->pmu, pmu->num_counters, pmu->num_counters_fixed))
+ */
 bool check_hw_exists(struct pmu *pmu, int num_counters, int num_counters_fixed)
 {
 	u64 val, val_fail = -1, val_new= ~0;
@@ -1434,6 +1439,11 @@ int x86_perf_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|768| <<amd_pmu_enable_event>> x86_pmu_enable_event(event);
+ *   - arch/x86/events/intel/core.c|4105| <<core_pmu_enable_event>> x86_pmu_enable_event(event);
+ */
 void x86_pmu_enable_event(struct perf_event *event)
 {
 	if (__this_cpu_read(cpu_hw_events.enabled))
@@ -2076,6 +2086,10 @@ void x86_pmu_update_cpu_context(struct pmu *pmu, int cpu)
 	cpuctx->ctx.pmu = pmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2227| <<global>> early_initcall(init_hw_perf_events); 
+ */
 static int __init init_hw_perf_events(void)
 {
 	struct x86_pmu_quirk *quirk;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 266143abc..3574a0001 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1131,6 +1131,16 @@ static inline bool is_counter_pair(struct hw_perf_event *hwc)
 	return hwc->flags & PERF_X86_EVENT_PAIR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1440| <<x86_pmu_enable_event>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2330| <<intel_pmu_nhm_workaround>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ */
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa381ab69..f2d94d4ac 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -489,6 +489,17 @@ struct kvm_pmc {
 	enum pmc_type type;
 	u8 idx;
 	u64 counter;
+	/*
+	 * 在以下设置kvm_pmc->eventsel:
+	 *   - arch/x86/kvm/pmu.c|405| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_OS;
+	 *   - arch/x86/kvm/pmu.c|407| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_USR;
+	 *   - arch/x86/kvm/pmu.c|409| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_INT;
+	 *   - arch/x86/kvm/svm/pmu.c|240| <<amd_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/svm/pmu.c|293| <<amd_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|520| <<intel_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|541| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|700| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 */
 	u64 eventsel;
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
@@ -496,6 +507,16 @@ struct kvm_pmc {
 	 * eventsel value for general purpose counters,
 	 * ctrl value for fixed counters.
 	 */
+	/*
+	 * 在以下使用kvm_pmc->current_config:
+	 *   - arch/x86/kvm/pmu.c|324| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+	 *   - arch/x86/kvm/pmu.c|329| <<reprogram_counter>> pmc->current_config = new_config;
+	 *   - arch/x86/kvm/pmu.c|567| <<cpl_is_matched>> u64 config = pmc->current_config;
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_release_perf_event>> pmc->current_config = 0;
+	 *   - arch/x86/kvm/svm/pmu.c|280| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|624| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|631| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+	 */
 	u64 current_config;
 	bool is_paused;
 	bool intr;
@@ -503,8 +524,27 @@ struct kvm_pmc {
 
 #define KVM_PMC_MAX_FIXED	3
 struct kvm_pmu {
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|335| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|564| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|587| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned nr_arch_gp_counters;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	unsigned nr_arch_fixed_counters;
+	/*
+	 * 在以下使用kvm_pmu->available_event_types:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|120| <<intel_hw_event_available>> if ((i < 7) && !(pmu->available_event_types & (1 << i)))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|594| <<intel_pmu_refresh>> pmu->available_event_types = ~entry->ebx &
+	 */
 	unsigned available_event_types;
 	u64 fixed_ctr_ctrl;
 	u64 fixed_ctr_ctrl_mask;
@@ -519,8 +559,42 @@ struct kvm_pmu {
 	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
 	struct irq_work irq_work;
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
 	DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->all_valid_pmc_idx:
+	 *   - arch/x86/kvm/pmu.c|633| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/pmu.c|705| <<kvm_pmu_trigger_event>> for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/svm/pmu.c|266| <<amd_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|589| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|591| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
+	 */
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	DECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);
 
 	u64 ds_area;
@@ -542,12 +616,31 @@ struct kvm_pmu {
 	 * The gate to release perf_events not marked in
 	 * pmc_in_use only once in a vcpu time slice.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	bool need_cleanup;
 
 	/*
 	 * The total number of programmed perf_events and it helps to avoid
 	 * redundant check before cleanup if guest don't use vPMU at all.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - arch/x86/kvm/pmu.c|218| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|519| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|72| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|246| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|297| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12059| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 */
 	u8 event_count;
 };
 
@@ -677,6 +770,13 @@ struct kvm_vcpu_arch {
 	u64 ia32_xss;
 	u64 microcode_version;
 	u64 arch_capabilities;
+	/*
+	 * 在以下使用kvm_vcpu_arch->perf_capabilities:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|187| <<vcpu_get_perf_capabilities>> return vcpu->arch.perf_capabilities;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|689| <<intel_pmu_init>> vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();
+	 *   - arch/x86/kvm/x86.c|3560| <<kvm_set_msr_common>> vcpu->arch.perf_capabilities = data;
+	 *   - arch/x86/kvm/x86.c|3987| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.perf_capabilities;
+	 */
 	u64 perf_capabilities;
 
 	/*
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..84c03635c 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -23,6 +23,29 @@ extern int unknown_nmi_panic;
 
 #define NMI_FLAG_FIRST	1
 
+/*
+ * 注册了NMI_LOCAL的:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|101| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|850| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *
+ * 注册了NMI_UNKNOWN的:
+ *   - rch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 enum {
 	NMI_LOCAL=0,
 	NMI_UNKNOWN,
@@ -44,6 +67,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index c936ce9f0..143fe1e1d 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -311,6 +311,13 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 		: "memory");
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1496| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1355| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|437| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|448| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
 	u64 val = PRED_CMD_IBPB;
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f6fc8dd51..c781e2ec6 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -28,6 +28,29 @@
 #define ARCH_PERFMON_EVENTSEL_PIN_CONTROL		(1ULL << 19)
 #define ARCH_PERFMON_EVENTSEL_INT			(1ULL << 20)
 #define ARCH_PERFMON_EVENTSEL_ANY			(1ULL << 21)
+/*
+ * 在以下使用ARCH_PERFMON_EVENTSEL_ENABLE:
+ *   - arch/x86/events/amd/core.c|21| <<AMD_MERGE_EVENT_ENABLE>> #define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/amd/uncore.c|109| <<amd_uncore_start>> wrmsrl(hwc->config_base, (hwc->config | ARCH_PERFMON_EVENTSEL_ENABLE));
+ *   - arch/x86/events/core.c|266| <<check_hw_exists>> if (val & ARCH_PERFMON_EVENTSEL_ENABLE) {
+ *   - arch/x86/events/core.c|687| <<x86_pmu_disable_all>> if (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))
+ *   - arch/x86/events/core.c|689| <<x86_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1441| <<x86_pmu_enable_event>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2331| <<intel_pmu_nhm_workaround>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4090| <<core_guest_get_msrs>> event->hw.config | ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4093| <<core_guest_get_msrs>> arr[idx].host &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4095| <<core_guest_get_msrs>> arr[idx].guest &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/knc.c|183| <<knc_pmu_disable_event>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/knc.c|194| <<knc_pmu_enable_event>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|144| <<p6_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|154| <<p6_pmu_enable_all>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/kvm/pmu.h|158| <<pmc_speculative_in_use>> return pmc->eventsel & ARCH_PERFMON_EVENTSEL_ENABLE;
+ */
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 5393babc0..dab2a3d90 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -23,6 +23,10 @@ extern void x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bo
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4009| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
@@ -37,6 +41,10 @@ void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4036| <<svm_vcpu_run>> x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_restore_host(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index da7c361f4..de0172cbe 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -199,6 +199,18 @@ void __init check_bugs(void)
  * NOTE: This function is *only* called for SVM.  VMX spec_ctrl handling is
  * done in vmenter.S.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/spec-ctrl.h|29| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, true);
+ *   - arch/x86/include/asm/spec-ctrl.h|43| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, false);
+ *
+ * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+ * the guest has, while on VMEXIT we restore the host view. This
+ * would be easier if SPEC_CTRL were architecturally maskable or
+ * shadowable for guests but this is not (currently) the case.
+ * Takes the guest view of SPEC_CTRL MSR as a parameter and also
+ * the guest's version of VIRT_SPEC_CTRL, if emulated.
+ */
 void
 x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
 {
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index cec0bfa3b..97e14c23c 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -73,6 +73,12 @@ struct nmi_stats {
 
 static DEFINE_PER_CPU(struct nmi_stats, nmi_stats);
 
+/*
+ * 在以下使用ignoe_nmis:
+ *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+ *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+ *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+ */
 static int ignore_nmis __read_mostly;
 
 int unknown_nmi_panic;
@@ -101,6 +107,10 @@ static int __init nmi_warning_debugfs(void)
 }
 fs_initcall(nmi_warning_debugfs);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|145| <<nmi_handle>> nmi_check_duration(a, delta);
+ */
 static void nmi_check_duration(struct nmiaction *action, u64 duration)
 {
 	int remainder_ns, decimal_msecs;
@@ -118,6 +128,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|234| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|261| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|310| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|359| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -152,6 +169,33 @@ static int nmi_handle(unsigned int type, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_handle);
 
+/*
+ * 5.4上看到这两个NMI LOCAL的.
+ *
+ * crash> nmiaction ffffffffa6617ec0
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6641a80,
+ *     prev = 0xffffffffa6634368
+ *   },
+ *   handler = 0xffffffffa4c07aa0, --> perf_event_nmi_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa60b4811 "PMI"
+ * }
+ * crash> nmiaction ffffffffa6641a80
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6634368,
+ *     prev = 0xffffffffa6617ec0
+ *   },
+ *   handler = 0xffffffffa4c7aeb0, --> nmi_cpu_backtrace_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa6041a34 "arch_bt"
+ * }
+ */
+
 int __register_nmi_handler(unsigned int type, struct nmiaction *action)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -213,6 +257,10 @@ void unregister_nmi_handler(unsigned int type, const char *name)
 }
 EXPORT_SYMBOL_GPL(unregister_nmi_handler);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|370| <<default_do_nmi>> pci_serr_error(reason, regs);
+ */
 static void
 pci_serr_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -234,6 +282,10 @@ pci_serr_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(pci_serr_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|372| <<default_do_nmi>> io_check_error(reason, regs);
+ */
 static void
 io_check_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -274,11 +326,25 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(io_check_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|419| <<default_do_nmi>> unknown_nmi_error(reason, regs);
+ */
 static void
 unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 {
 	int handled;
 
+	/*
+	 * 注册了NMI_UNKNOWN的:
+	 *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+	 *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+	 *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+	 *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+	 *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+	 *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+	 */
+
 	/*
 	 * Use 'false' as back-to-back NMIs are dealt with one level up.
 	 * Of course this makes having multiple 'unknown' handlers useless
@@ -304,8 +370,19 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 NOKPROBE_SYMBOL(unknown_nmi_error);
 
 static DEFINE_PER_CPU(bool, swallow_nmi);
+/*
+ * 在以下使用last_nmi_rip:
+ *   - arch/x86/kernel/nmi.c|329| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+ *   - arch/x86/kernel/nmi.c|350| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+ *   - arch/x86/kernel/nmi.c|355| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+ *   - arch/x86/kernel/nmi.c|575| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+ */
 static DEFINE_PER_CPU(unsigned long, last_nmi_rip);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|535| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> default_do_nmi(regs);
+ */
 static noinstr void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;
@@ -472,6 +549,14 @@ enum nmi_states {
 	NMI_EXECUTING,
 	NMI_LATCHED,
 };
+/*
+ * 在以下使用nmi_state:
+ *   - arch/x86/kernel/nmi.c|497| <<global>> static DEFINE_PER_CPU(enum nmi_states, nmi_state);
+ *   - arch/x86/kernel/nmi.c|514| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_read(nmi_state) != NMI_NOT_RUNNING) {
+ *   - arch/x86/kernel/nmi.c|515| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_LATCHED);
+ *   - arch/x86/kernel/nmi.c|518| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_EXECUTING);
+ *   - arch/x86/kernel/nmi.c|545| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_dec_return(nmi_state))
+ */
 static DEFINE_PER_CPU(enum nmi_states, nmi_state);
 static DEFINE_PER_CPU(unsigned long, nmi_cr2);
 static DEFINE_PER_CPU(unsigned long, nmi_dr7);
@@ -509,6 +594,12 @@ DEFINE_IDTENTRY_RAW(exc_nmi)
 
 	inc_irq_stat(__nmi_count);
 
+	/*
+	 * 在以下使用ignoe_nmis:
+	 *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+	 *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+	 *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+	 */
 	if (!ignore_nmis)
 		default_do_nmi(regs);
 
@@ -548,6 +639,10 @@ void restart_nmi(void)
 }
 
 /* reset the back-to-back NMI logic */
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|709| <<arch_cpu_idle_enter>> local_touch_nmi();
+ */
 void local_touch_nmi(void)
 {
 	__this_cpu_write(last_nmi_rip, 0);
diff --git a/arch/x86/kernel/nmi_selftest.c b/arch/x86/kernel/nmi_selftest.c
index a1a96df3d..d1a391bb2 100644
--- a/arch/x86/kernel/nmi_selftest.c
+++ b/arch/x86/kernel/nmi_selftest.c
@@ -23,6 +23,15 @@
 #define FAILURE		1
 #define TIMEOUT		2
 
+/*
+ * 在以下使用nmi_fail:
+ *   - arch/x86/kernel/nmi_selftest.c|71| <<test_nmi_ipi>> nmi_fail = FAILURE;
+ *   - arch/x86/kernel/nmi_selftest.c|89| <<test_nmi_ipi>> nmi_fail = TIMEOUT;
+ *   - arch/x86/kernel/nmi_selftest.c|110| <<reset_nmi>> nmi_fail = 0;
+ *   - arch/x86/kernel/nmi_selftest.c|119| <<dotest>> if (nmi_fail != expected) {
+ *   - arch/x86/kernel/nmi_selftest.c|122| <<dotest>> if (nmi_fail == FAILURE)
+ *   - arch/x86/kernel/nmi_selftest.c|124| <<dotest>> else if (nmi_fail == TIMEOUT)
+ */
 static int __initdata nmi_fail;
 
 /* check to see if NMI IPIs work on this machine */
@@ -62,6 +71,29 @@ static int __init test_nmi_ipi_callback(unsigned int val, struct pt_regs *regs)
         return NMI_DONE;
 }
 
+/*
+ * called by:
+ 49  *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ 50  *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ 51  *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ 52  *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ 53  *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ 54  *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ 55  *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ 56  *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ 57  *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ 58  *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ 59  *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ 60  *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ 61  *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ 62  *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ 63  *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ 64  *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ 65  *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ 66  *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ 67  *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 static void __init test_nmi_ipi(struct cpumask *mask)
 {
 	unsigned long timeout;
diff --git a/arch/x86/kernel/sev.c b/arch/x86/kernel/sev.c
index a428c6233..b267bb976 100644
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@ -1485,6 +1485,10 @@ static enum es_result vc_do_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt,
  * rare operation. If it turns out to be a performance problem the split
  * operations can be moved to memcpy_fromio() and memcpy_toio().
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1604| <<vc_handle_mmio>> ret = vc_handle_mmio_movs(ctxt, bytes);
+ */
 static enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,
 					  unsigned int bytes)
 {
@@ -1533,6 +1537,10 @@ static enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,
 		return ES_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1805| <<vc_handle_exitcode>> result = vc_handle_mmio(ghcb, ctxt);
+ */
 static enum es_result vc_handle_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt)
 {
 	struct insn *insn = &ctxt->insn;
@@ -1753,6 +1761,11 @@ static enum es_result vc_handle_trap_ac(struct ghcb *ghcb,
 	return ES_EXCEPTION;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1878| <<vc_raw_handle_exception>> result = vc_handle_exitcode(&ctxt, ghcb, error_code);
+ *   - arch/x86/kernel/sev.c|2017| <<handle_vc_boot_ghcb>> result = vc_handle_exitcode(&ctxt, boot_ghcb, exit_code);
+ */
 static enum es_result vc_handle_exitcode(struct es_em_ctxt *ctxt,
 					 struct ghcb *ghcb,
 					 unsigned long exit_code)
@@ -1861,6 +1874,11 @@ static __always_inline bool vc_from_invalid_context(struct pt_regs *regs)
 	return is_vc2_stack(sp) && !is_vc2_stack(prev_sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1962| <<DEFINE_IDTENTRY_VC_KERNEL(exc_vmm_communication)>> if (!vc_raw_handle_exception(regs, error_code)) {
+ *   - arch/x86/kernel/sev.c|1994| <<DEFINE_IDTENTRY_VC_USER(exc_vmm_communication)>> if (!vc_raw_handle_exception(regs, error_code)) {
+ */
 static bool vc_raw_handle_exception(struct pt_regs *regs, unsigned long error_code)
 {
 	struct ghcb_state state;
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 2796dde06..120a1f8c7 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -311,6 +311,10 @@ void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_update_cpuid_runtime);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|421| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -382,6 +386,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|462| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|488| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index b1658c0de..755545d9f 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,11 +42,30 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|52| <<kvm_vcpu_is_illegal_gpa>> return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|58| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/svm/nested.c|261| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|262| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|805| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|324| <<__nested_vmcb_check_save>> CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))
+ *   - arch/x86/kvm/svm/nested.c|522| <<nested_svm_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))
+ *   - arch/x86/kvm/vmx/nested.c|1113| <<nested_vmx_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3))) {
+ *   - arch/x86/kvm/vmx/nested.c|2695| <<nested_vmx_check_eptp>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/nested.c|2890| <<nested_vmx_check_host_state>> CC(kvm_vcpu_is_illegal_gpa(vcpu, vmcs12->host_cr3)))
+ *   - arch/x86/kvm/vmx/vmx.c|5669| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/x86.c|1239| <<kvm_set_cr3>> if (kvm_vcpu_is_illegal_gpa(vcpu, cr3))
+ *   - arch/x86/kvm/x86.c|11183| <<kvm_is_valid_sregs>> if (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))
+ */
 static inline bool kvm_vcpu_is_illegal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 3febc3423..2f8f859ec 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -183,6 +183,13 @@ static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|966| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1175| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3496| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4565| <<nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9dda989a1..2db3ed547 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -2524,6 +2524,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1712| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2547| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|432| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|4958| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 3552e6af3..9d2c8d459 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4434,6 +4434,14 @@ static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
 		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5117| <<kvm_init_shadow_npt_mmu>> kvm_mmu_new_pgd(vcpu, nested_cr3);
+ *   - arch/x86/kvm/mmu/mmu.c|5172| <<kvm_init_shadow_ept_mmu>> kvm_mmu_new_pgd(vcpu, new_eptp);
+ *   - arch/x86/kvm/svm/nested.c|535| <<nested_svm_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/vmx/nested.c|1135| <<nested_vmx_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/x86.c|1246| <<kvm_set_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ */
 void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -6319,6 +6327,10 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 }
 
 /* Must be called with the mmu_lock held in write-mode. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1334| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_try_split_huge_pages(kvm, slot, start, end, PG_LEVEL_4K);
+ */
 void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot,
 				   u64 start, u64 end,
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bf2ccf9de..8d59c0869 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -1460,6 +1460,10 @@ static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1547| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))
+ */
 static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 				   struct kvm_mmu_page *sp, bool shared)
 {
@@ -1500,6 +1504,10 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1581| <<kvm_tdp_mmu_try_split_huge_pages>> r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
+ */
 static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 					 struct kvm_mmu_page *root,
 					 gfn_t start, gfn_t end,
@@ -1567,6 +1575,11 @@ static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 /*
  * Try to split all huge pages mapped by the TDP MMU down to the target level.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6341| <<kvm_mmu_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, false);
+ *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_slot_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);
+ */
 void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      const struct kvm_memory_slot *slot,
 				      gfn_t start, gfn_t end,
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 02f9e4f24..9a822e654 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,8 +23,36 @@
 #include "pmu.h"
 
 /* This is enough to filter the vast majority of currently defined events. */
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - arch/x86/kvm/pmu.c|583| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/cpuid.c|914| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|915| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|916| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|917| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len; 
+ *   - arch/x86/kvm/cpuid.c|918| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|919| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|921| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|927| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/pmu.h|167| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|173| <<kvm_init_pmu_capability>> if ((is_intel && !kvm_pmu_cap.version) || !kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/pmu.h|177| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|181| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|182| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed,
+ *   - arch/x86/kvm/vmx/capabilities.h|401| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|540| <<intel_pmu_refresh>> kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|542| <<intel_pmu_refresh>> kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|545| <<intel_pmu_refresh>> kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> (size_t)kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|557| <<intel_pmu_refresh>> kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|6944| <<kvm_init_msr_list>> min(INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))
+ *   - arch/x86/kvm/x86.c|6949| <<kvm_init_msr_list>> min(INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
@@ -70,6 +98,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11967| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -85,9 +117,19 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 
 static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
 {
+	/*
+	 * intel_pmc_is_enabled()
+	 * amd_pmc_is_enabled()
+	 */
 	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);
 }
 
+/*
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|485| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ *
+ * 在__kvm_perf_overflow()被调用
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
@@ -96,11 +138,19 @@ static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|144| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|554| <<kvm_pmu_incr_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 	bool skip_pmi = false;
 
+	/*
+	 * Set a bit and return its old value
+	 */
 	/* Ignore counters that have been reprogrammed already. */
 	if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
 		return;
@@ -117,6 +167,12 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 	if (!pmc->intr || skip_pmi)
 		return;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|178| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8161| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10337| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 	/*
 	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
 	 * can be ejected on a guest mode re-entry. Otherwise we can't
@@ -131,6 +187,10 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|201| <<pmc_reprogram_counter>> kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -140,6 +200,10 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	__kvm_perf_overflow(pmc, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|326| <<reprogram_counter>> pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ */
 static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 				  u64 config, bool exclude_user,
 				  bool exclude_kernel, bool intr)
@@ -217,6 +281,10 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|374| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -247,6 +315,10 @@ static int cmp_u64(const void *pa, const void *pb)
 	return (a > b) - (a < b);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|356| <<reprogram_counter>> if (!check_pmu_event_filter(pmc))
+ */
 static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu_event_filter *filter;
@@ -283,9 +355,37 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 	return allow_event;
 }
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_pmu pmu;
+ *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+ *          -> u8 idx; 
+ *          -> u64 counter;
+ *          -> u64 eventsel;
+ *          -> struct perf_event *perf_event;
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> u64 current_config;
+ *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|359| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|561| <<kvm_pmu_incr_counter>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|241| <<amd_pmu_set_msr>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|55| <<reprogram_fixed_counters>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|83| <<global_ctrl_changed>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|480| <<intel_pmu_set_msr>> reprogram_counter(pmc);
+ */
 void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
+	/*
+	 * 在以下设置kvm_pmc->eventsel:
+	 *   - arch/x86/kvm/svm/pmu.c|240| <<amd_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|485| <<intel_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 */
 	u64 eventsel = pmc->eventsel;
 	u64 new_config = eventsel;
 	u8 fixed_ctr_ctrl;
@@ -327,6 +427,10 @@ void reprogram_counter(struct kvm_pmc *pmc)
 }
 EXPORT_SYMBOL_GPL(reprogram_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10336| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -342,6 +446,16 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 		reprogram_counter(pmc);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	/*
 	 * Unused perf_events are only released if the corresponding MSRs
 	 * weren't accessed during the last vCPU time slice. kvm_arch_sched_in
@@ -417,9 +531,22 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|178| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8161| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10337| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|96| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10338| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
@@ -431,23 +558,60 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|591| <<kvm_pmu_set_msr>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 */
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	struct kvm_pmc *pmc = static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr);
 
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	if (pmc)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3957| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3971| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4219| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ */
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3768| <<kvm_set_msr_common(MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3854| <<kvm_set_msr_common(MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3862| <<kvm_set_msr_common(default: kvm_pmu_is_valid_msr)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+	/*
+	 * intel_pmu_set_msr
+	 */
 	return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);
 }
 
@@ -455,11 +619,22 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|349| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|626| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3561| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|522| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11767| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -481,6 +656,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 }
 
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|368| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -488,12 +667,46 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	DECLARE_BITMAP(bitmask, X86_PMC_IDX_MAX);
 	int i;
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	pmu->need_cleanup = false;
 
+	/*
+	 * 在以下使用kvm_pmu->all_valid_pmc_idx:
+	 *   - arch/x86/kvm/pmu.c|633| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/pmu.c|705| <<kvm_pmu_trigger_event>> for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/svm/pmu.c|266| <<amd_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|589| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|591| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
+	 *
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
 		      pmu->pmc_in_use, X86_PMC_IDX_MAX);
 
 	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {
+		/*
+		 * amd_pmc_idx_to_pmc()
+		 */
 		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);
 
 		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
@@ -505,11 +718,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11689| <<kvm_arch_vcpu_destroy>> kvm_pmu_destroy(vcpu);
+ */
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
 	u64 prev_count;
@@ -545,6 +766,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3532| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8535| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8806| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8808| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 5cc5721f2..18acee2c1 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -4,6 +4,20 @@
 
 #include <linux/nospec.h>
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_pmu pmu;
+ *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+ *          -> u8 idx; 
+ *          -> u64 counter;
+ *          -> u64 eventsel;
+ *          -> struct perf_event *perf_event;
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> u64 current_config;
+ *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+ */
+
 #define vcpu_to_pmu(vcpu) (&(vcpu)->arch.pmu)
 #define pmu_to_vcpu(pmu)  (container_of((pmu), struct kvm_vcpu, arch.pmu))
 #define pmc_to_pmu(pmc)   (&(pmc)->vcpu->arch.pmu)
@@ -63,8 +77,34 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * [0] pmc_release_perf_event
+ * [0] kvm_pmu_cleanup
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|336| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|80| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
+	/*
+	 * struct kvm_pmc {
+	 *     enum pmc_type type;
+	 *     u8 idx;
+	 *     u64 counter;
+	 *     u64 eventsel;
+	 *     struct perf_event *perf_event;
+	 *     struct kvm_vcpu *vcpu;
+	 *     u64 current_config;
+	 *     bool is_paused;
+	 *     bool intr;
+	 * };
+	 */
 	if (pmc->perf_event) {
 		perf_event_release_kernel(pmc->perf_event);
 		pmc->perf_event = NULL;
@@ -73,6 +113,13 @@ static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|541| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|292| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|649| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -101,6 +148,21 @@ static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
  */
+/*
+ * 只被intel调用:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|79| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|200| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|249| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|268| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|269| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|411| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|412| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|422| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|497| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|498| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|512| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 					 u32 base)
 {
@@ -147,6 +209,13 @@ static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|392| <<reprogram_counter>> if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))
+ *   - arch/x86/kvm/pmu.c|639| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/pmu.c|708| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|791| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -160,6 +229,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11990| <<kvm_arch_hardware_setup>> kvm_init_pmu_capability();
+ */
 static inline void kvm_init_pmu_capability(void)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
diff --git a/arch/x86/kvm/reverse_cpuid.h b/arch/x86/kvm/reverse_cpuid.h
index a19d473d0..08d1f76e5 100644
--- a/arch/x86/kvm/reverse_cpuid.h
+++ b/arch/x86/kvm/reverse_cpuid.h
@@ -30,6 +30,12 @@ struct cpuid_reg {
 	int reg;
 };
 
+/*
+ * 在以下使用reverse_cpuid[]:
+ *   - arch/x86/kvm/reverse_cpuid.h|67| <<reverse_cpuid_check>> BUILD_BUG_ON(x86_leaf >= ARRAY_SIZE(reverse_cpuid));
+ *   - arch/x86/kvm/reverse_cpuid.h|68| <<reverse_cpuid_check>> BUILD_BUG_ON(reverse_cpuid[x86_leaf].function == 0);
+ *   - arch/x86/kvm/reverse_cpuid.h|111| <<x86_feature_cpuid>> return reverse_cpuid[x86_leaf];
+ */
 static const struct cpuid_reg reverse_cpuid[] = {
 	[CPUID_1_EDX]         = {         1, 0, CPUID_EDX},
 	[CPUID_8000_0001_EDX] = {0x80000001, 0, CPUID_EDX},
@@ -103,6 +109,12 @@ static __always_inline u32 __feature_bit(int x86_feature)
 
 #define feature_bit(name)  __feature_bit(X86_FEATURE_##name)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|527| <<__kvm_cpu_cap_mask>> const struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);
+ *   - arch/x86/kvm/cpuid.h|97| <<guest_cpuid_get_register>> const struct cpuid_reg cpuid = x86_feature_cpuid(x86_feature);
+ *   - arch/x86/kvm/reverse_cpuid.h|135| <<cpuid_entry_get_reg>> const struct cpuid_reg cpuid = x86_feature_cpuid(x86_feature);
+ */
 static __always_inline struct cpuid_reg x86_feature_cpuid(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index f24613a10..f163b22d9 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -33,10 +33,20 @@ enum index {
 	INDEX_ERROR,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|144| <<amd_pmc_idx_to_pmc>> unsigned int base = get_msr_base(pmu, PMU_TYPE_COUNTER);
+ *
+ * 对于extention,是MSR_F15H_PERF_CTR和MSR_F15H_PERF_CTL
+ * 对于非extention,是MSR_K7_PERFCTR0和MSR_K7_EVNTSEL0
+ */
 static unsigned int get_msr_base(struct kvm_pmu *pmu, enum pmu_type type)
 {
 	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
 
+	/*
+	 * X86_FEATURE_PERFCTR_CORE是Core performance counter extensions
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE)) {
 		if (type == PMU_TYPE_COUNTER)
 			return MSR_F15H_PERF_CTR;
@@ -50,6 +60,37 @@ static unsigned int get_msr_base(struct kvm_pmu *pmu, enum pmu_type type)
 	}
 }
 
+/*
+ * INDEX_ZERO
+ *   - MSR_F15H_PERF_CTL0 : MSR_F15H_PERF_CTL = 0xc0010200
+ *   - MSR_F15H_PERF_CTR0 : MSR_F15H_PERF_CTR = 0xc0010201
+ *   - MSR_K7_EVNTSEL0    : 0xc0010000
+ *   - MSR_K7_PERFCTR0    : 0xc0010004
+ * INDEX_ONE
+ *   - MSR_F15H_PERF_CTL1 : MSR_F15H_PERF_CTL + 2 = 0xc0010202
+ *   - MSR_F15H_PERF_CTR1 : MSR_F15H_PERF_CTR + 2 = 0xc0010203
+ *   - MSR_K7_EVNTSEL1    : 0xc0010001
+ *   - MSR_K7_PERFCTR1    : 0xc0010005
+ * INDEX_TWO
+ *   - MSR_F15H_PERF_CTL2 : MSR_F15H_PERF_CTL + 4 = 0xc0010204
+ *   - MSR_F15H_PERF_CTR2 : MSR_F15H_PERF_CTR + 4 = 0xc0010205
+ *   - MSR_K7_EVNTSEL2    : 0xc0010002
+ *   - MSR_K7_PERFCTR2    : 0xc0010006
+ * INDEX_THREE
+ *   - MSR_F15H_PERF_CTL3 : MSR_F15H_PERF_CTL + 6 = 0xc0010206
+ *   - MSR_F15H_PERF_CTR3 : MSR_F15H_PERF_CTR + 6 = 0xc0010207
+ *   - MSR_K7_EVNTSEL3    : 0xc0010003
+ *   - MSR_K7_PERFCTR3    : 0xc0010007
+ * INDEX_FOUR
+ *   - MSR_F15H_PERF_CTL4 : MSR_F15H_PERF_CTL + 8 = 0xc0010208
+ *   - MSR_F15H_PERF_CTR4 : MSR_F15H_PERF_CTR + 8 = 0xc0010209
+ * INDEX_FIVE
+ *   - MSR_F15H_PERF_CTL5 : MSR_F15H_PERF_CTL + 10 = 0xc001020a
+ *   - MSR_F15H_PERF_CTR5 : MSR_F15H_PERF_CTR + 10 = 0xc001020b
+ *
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|126| <<get_gp_pmc_amd>> return &pmu->gp_counters[msr_to_index(msr)];
+ */
 static enum index msr_to_index(u32 msr)
 {
 	switch (msr) {
@@ -84,6 +125,43 @@ static enum index msr_to_index(u32 msr)
 	}
 }
 
+/*
+ * INDEX_ZERO
+ *   - MSR_F15H_PERF_CTL0 : MSR_F15H_PERF_CTL = 0xc0010200
+ *   - MSR_F15H_PERF_CTR0 : MSR_F15H_PERF_CTR = 0xc0010201
+ *   - MSR_K7_EVNTSEL0    : 0xc0010000
+ *   - MSR_K7_PERFCTR0    : 0xc0010004
+ * INDEX_ONE
+ *   - MSR_F15H_PERF_CTL1 : MSR_F15H_PERF_CTL + 2 = 0xc0010202
+ *   - MSR_F15H_PERF_CTR1 : MSR_F15H_PERF_CTR + 2 = 0xc0010203
+ *   - MSR_K7_EVNTSEL1    : 0xc0010001
+ *   - MSR_K7_PERFCTR1    : 0xc0010005
+ * INDEX_TWO
+ *   - MSR_F15H_PERF_CTL2 : MSR_F15H_PERF_CTL + 4 = 0xc0010204
+ *   - MSR_F15H_PERF_CTR2 : MSR_F15H_PERF_CTR + 4 = 0xc0010205
+ *   - MSR_K7_EVNTSEL2    : 0xc0010002
+ *   - MSR_K7_PERFCTR2    : 0xc0010006
+ * INDEX_THREE
+ *   - MSR_F15H_PERF_CTL3 : MSR_F15H_PERF_CTL + 6 = 0xc0010206
+ *   - MSR_F15H_PERF_CTR3 : MSR_F15H_PERF_CTR + 6 = 0xc0010207
+ *   - MSR_K7_EVNTSEL3    : 0xc0010003
+ *   - MSR_K7_PERFCTR3    : 0xc0010007
+ * INDEX_FOUR
+ *   - MSR_F15H_PERF_CTL4 : MSR_F15H_PERF_CTL + 8 = 0xc0010208
+ *   - MSR_F15H_PERF_CTR4 : MSR_F15H_PERF_CTR + 8 = 0xc0010209
+ * INDEX_FIVE
+ *   - MSR_F15H_PERF_CTL5 : MSR_F15H_PERF_CTL + 10 = 0xc001020a
+ *   - MSR_F15H_PERF_CTR5 : MSR_F15H_PERF_CTR + 10 = 0xc001020b
+ *
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|155| <<amd_pmc_idx_to_pmc>> return get_gp_pmc_amd(pmu, base + pmc_idx, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|193| <<amd_msr_idx_to_pmc>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|194| <<amd_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|206| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|212| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|229| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|236| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ */
 static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 					     enum pmu_type type)
 {
@@ -126,6 +204,9 @@ static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 	return &pmu->gp_counters[msr_to_index(msr)];
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.hw_event_available = amd_hw_event_available()
+ */
 static bool amd_hw_event_available(struct kvm_pmc *pmc)
 {
 	return true;
@@ -134,11 +215,17 @@ static bool amd_hw_event_available(struct kvm_pmc *pmc)
 /* check if a PMC is enabled by comparing it against global_ctrl bits. Because
  * AMD CPU doesn't have global_ctrl MSR, all PMCs are enabled (return TRUE).
  */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_is_enabled = amd_pmc_is_enabled()
+ */
 static bool amd_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	unsigned int base = get_msr_base(pmu, PMU_TYPE_COUNTER);
@@ -155,6 +242,9 @@ static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 	return get_gp_pmc_amd(pmu, base + pmc_idx, PMU_TYPE_COUNTER);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_rdpmc_ecx = amd_id_valid_rdpmc_ecx()
+ */
 static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -165,6 +255,9 @@ static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 }
 
 /* idx is the ECX register of RDPMC instruction */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc()
+ */
 static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	unsigned int idx, u64 *mask)
 {
@@ -179,12 +272,18 @@ static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[idx];
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_msr = amd_is_valid_msr()
+ */
 static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	/* All MSRs refer to exactly one PMC, so msr_idx_to_pmc is enough.  */
 	return false;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.msr_idx_to_pmc = amd_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -196,6 +295,9 @@ static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 	return pmc;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.get_msr = amd_pmu_get_msr()
+ */
 static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -218,6 +320,9 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.set_msr = amd_pmu_set_msr()
+ */
 static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -246,10 +351,20 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.refresh = amd_pmu_refresh()
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|335| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|564| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|587| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE))
 		pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
 	else
@@ -261,11 +376,21 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 	pmu->version = 1;
 	/* not applicable to AMD; but clean them to prevent any fall out */
 	pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	pmu->nr_arch_fixed_counters = 0;
 	pmu->global_status = 0;
 	bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.init = amd_pmu_init()
+ */
 static void amd_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -281,6 +406,9 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.reset = amd_pmu_reset()
+ */
 static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index f3813dbac..8dc1b2149 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -1351,6 +1351,13 @@ static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 		__svm_vcpu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|804| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/nested.c|1021| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1177| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1667| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ */
 void svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)
 {
 	svm->current_vmcb = target_vmcb;
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index c5e5dfef6..3fdeef0b9 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -21,6 +21,28 @@ extern int __read_mostly pt_mode;
 #define PT_MODE_SYSTEM		0
 #define PT_MODE_HOST_GUEST	1
 
+/*
+ * version 1:
+ *
+ * IA32_PMCx MSRs 从 0x0c1开始
+ * IA32_PERFEVTSELx MSRs 从0x186开始
+ *
+ * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+ * IA32_PMCx从0x4c1开始
+ *
+ * kvm-unit-tests中:
+ *
+ * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+ * 821                 gp_counter_base = MSR_IA32_PMC0;
+ * 822                 report_prefix_push("full-width writes");
+ * 823                 check_counters();
+ * 824                 check_gp_counters_write_width();
+ * 825         }
+ *
+ * 在以下使用PMU_CAP_FW_WRITES:
+ *   - arch/x86/kvm/vmx/capabilities.h|406| <<vmx_get_perf_capabilities>> u64 perf_cap = PMU_CAP_FW_WRITES;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+ */
 #define PMU_CAP_FW_WRITES	(1ULL << 13)
 #define PMU_CAP_LBR_FMT		0x3f
 
@@ -403,6 +425,28 @@ static inline bool vmx_pebs_supported(void)
 
 static inline u64 vmx_get_perf_capabilities(void)
 {
+	/*
+	 * version 1:
+	 *
+	 * IA32_PMCx MSRs 从 0x0c1开始
+	 * IA32_PERFEVTSELx MSRs 从0x186开始
+	 *
+	 * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+	 * IA32_PMCx从0x4c1开始
+	 *
+	 * kvm-unit-tests中:
+	 *
+	 * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+	 * 821                 gp_counter_base = MSR_IA32_PMC0;
+	 * 822                 report_prefix_push("full-width writes");
+	 * 823                 check_counters();
+	 * 824                 check_gp_counters_write_width();
+	 * 825         }
+	 *
+	 * 在以下使用PMU_CAP_FW_WRITES:
+	 *   - arch/x86/kvm/vmx/capabilities.h|406| <<vmx_get_perf_capabilities>> u64 perf_cap = PMU_CAP_FW_WRITES;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+	 */
 	u64 perf_cap = PMU_CAP_FW_WRITES;
 	u64 host_perf_cap = 0;
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index c399637a3..d87a86893 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -18,8 +18,21 @@
 #include "nested.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用MSR_PMC_FULL_WIDTH_BIT:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_set_msr>> if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|507| <<intel_pmu_set_msr>> !(msr & MSR_PMC_FULL_WIDTH_BIT))
+ */
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|100| <<intel_hw_event_available>> for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|101| <<intel_hw_event_available>> if (intel_arch_events[i].eventsel != event_select ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|102| <<intel_hw_event_available>> intel_arch_events[i].unit_mask != unit_mask)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+ *   - arch/x86/kvm/vmx/pmu_intel.c|507| <<setup_fixed_pmc_eventsel>> intel_arch_events[event].eventsel;
+ */
 static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
 	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
@@ -33,8 +46,18 @@ static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 };
 
 /* mapping between fixed pmc index and intel_arch_events array */
+/*
+ * 在以下使用fixed_pmc_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|492| <<setup_fixed_pmc_eventsel>> size_t size = ARRAY_SIZE(fixed_pmc_events);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|499| <<setup_fixed_pmc_eventsel>> event = fixed_pmc_events[array_index_nospec(i, size)];
+ *   - arch/x86/kvm/vmx/pmu_intel.c|553| <<intel_pmu_refresh>> min3(ARRAY_SIZE(fixed_pmc_events),
+ */
 static int fixed_pmc_events[] = {1, 0, 7};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|480| <<intel_pmu_set_msr(MSR_CORE_PERF_FIXED_CTR_CTRL)>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	struct kvm_pmc *pmc;
@@ -42,6 +65,13 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 	int i;
 
 	pmu->fixed_ctr_ctrl = data;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	for (i = 0; i < pmu->nr_arch_fixed_counters; i++) {
 		u8 new_ctrl = fixed_ctrl_field(data, i);
 		u8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);
@@ -56,6 +86,9 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_idx_to_pmc = intel_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	if (pmc_idx < INTEL_PMC_IDX_FIXED) {
@@ -84,6 +117,9 @@ static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.hw_event_available = 
+ */
 static bool intel_hw_event_available(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -107,6 +143,9 @@ static bool intel_hw_event_available(struct kvm_pmc *pmc)
 }
 
 /* check if a PMC is enabled by comparing it with globl_ctrl bits. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_is_enabled = 
+ */
 static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -117,6 +156,9 @@ static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_rdpmc_ecx =
+ */
 static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -128,6 +170,9 @@ static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 		     : idx < pmu->nr_arch_gp_counters;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.rdpmc_ecx_to_pmc = 
+ */
 static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 					    unsigned int idx, u64 *mask)
 {
@@ -150,6 +195,13 @@ static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[array_index_nospec(idx, num_counters)];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|224| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|270| <<intel_is_valid_msr>> ret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|276| <<intel_is_valid_msr>> perf_capabilities = vcpu_get_perf_capabilities(vcpu);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|672| <<intel_pmu_refresh>> perf_capabilities = vcpu_get_perf_capabilities(vcpu);
+ */
 static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
 {
 	if (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))
@@ -158,6 +210,26 @@ static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
 	return vcpu->arch.perf_capabilities;
 }
 
+/*
+ * version 1:
+ *
+ * IA32_PMCx MSRs 从 0x0c1开始
+ * IA32_PERFEVTSELx MSRs 从0x186开始
+ *
+ * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+ * IA32_PMCx从0x4c1开始
+ *
+ * 在kvm-unit-tests
+ *
+ * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+ * 821                 gp_counter_base = MSR_IA32_PMC0;822                 report_prefix_push("full-width writes");
+ * 823                 check_counters();
+ * 824                 check_gp_counters_write_width();
+ * 825         }
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|197| <<get_fw_gp_pmc>> if (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))
+ */
 static inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)
 {
 	return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
@@ -189,6 +261,9 @@ static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr =
+ */
 static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -224,6 +299,9 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.msr_idx_to_pmc =
+ */
 static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -341,6 +419,9 @@ static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.get_msr = intel_pmu_get_msr()
+ */
 static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -391,6 +472,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.set_msr = intel_pmu_set_msr()
+ */
 static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -453,6 +537,10 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		}
 		break;
 	default:
+		/*
+		 * MSR_IA32_PERFCTR0 0x000000c1
+		 * MSR_IA32_PMC0     0x000004c1
+		 */
 		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
 		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
 			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
@@ -487,6 +575,10 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|604| <<intel_pmu_refresh>> setup_fixed_pmc_eventsel(pmu);
+ */
 static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 {
 	size_t size = ARRAY_SIZE(fixed_pmc_events);
@@ -502,6 +594,9 @@ static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.refresh = intel_pmu_refresh()
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -611,6 +706,9 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.init = intel_pmu_init()
+ */
 static void intel_pmu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -637,6 +735,9 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 	lbr_desc->msr_passthrough = false;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.reset = intel_pmu_reset()
+ */
 static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -680,6 +781,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.deliver_pmi = intel_pmu_deliver_pmi()
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
@@ -767,6 +871,9 @@ void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
 		vcpu->vcpu_id);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.cleanup = intel_pmu_cleanup()
+ */
 static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c9b49a09e..2d731bad6 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -2006,6 +2006,10 @@ static u64 nested_vmx_truncate_sysenter_addr(struct kvm_vcpu *vcpu,
 	return (unsigned long)data;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2091| <<vmx_set_msr>> u64 invalid = data & ~vcpu_supported_debugctl(vcpu);
+ */
 static u64 vcpu_supported_debugctl(struct kvm_vcpu *vcpu)
 {
 	u64 debugctl = vmx_supported_debugctl();
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index b0c47b41c..0eae3026d 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1399,6 +1399,16 @@ EXPORT_SYMBOL_GPL(kvm_emulate_rdpmc);
  * may depend on host virtualization features rather than host cpu features.
  */
 
+/*
+ * 在以下使用msrs_to_save_all[]:
+ *   - arch/x86/kvm/x86.c|6897| <<kvm_init_msr_list>> for (i = 0; i < ARRAY_SIZE(msrs_to_save_all); i++) {
+ *   - arch/x86/kvm/x86.c|6898| <<kvm_init_msr_list>> if (rdmsr_safe(msrs_to_save_all[i], &dummy[0], &dummy[1]) < 0)
+ *   - arch/x86/kvm/x86.c|6905| <<kvm_init_msr_list>> switch (msrs_to_save_all[i]) {
+ *   - arch/x86/kvm/x86.c|6938| <<kvm_init_msr_list>> msrs_to_save_all[i] - MSR_IA32_RTIT_ADDR0_A >=
+ *   - arch/x86/kvm/x86.c|6943| <<kvm_init_msr_list>> if (msrs_to_save_all[i] - MSR_ARCH_PERFMON_PERFCTR0 >=
+ *   - arch/x86/kvm/x86.c|6948| <<kvm_init_msr_list>> if (msrs_to_save_all[i] - MSR_ARCH_PERFMON_EVENTSEL0 >=
+ *   - arch/x86/kvm/x86.c|6961| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static const u32 msrs_to_save_all[] = {
 	MSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,
 	MSR_STAR,
@@ -1449,7 +1459,20 @@ static const u32 msrs_to_save_all[] = {
 	MSR_IA32_XFD, MSR_IA32_XFD_ERR,
 };
 
+/*
+ * 在以下使用msrs_to_save[ARRAY_SIZE(msrs_to_save_all)]:
+ *   - arch/x86/kvm/x86.c|4582| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6975| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];
+/*
+ * 在以下使用num_msrs_to_save:
+ *   - arch/x86/kvm/x86.c|4580| <<kvm_arch_dev_ioctl>> msr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;
+ *   - arch/x86/kvm/x86.c|4588| <<kvm_arch_dev_ioctl>> num_msrs_to_save * sizeof(u32)))
+ *   - arch/x86/kvm/x86.c|4590| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices + num_msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6912| <<kvm_init_msr_list>> num_msrs_to_save = 0;
+ *   - arch/x86/kvm/x86.c|6980| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static unsigned num_msrs_to_save;
 
 static const u32 emulated_msrs_all[] = {
@@ -1524,6 +1547,13 @@ static unsigned num_emulated_msrs;
  * List of msr numbers which are used to expose MSR-based features that
  * can be used by a hypervisor to validate requested CPU features.
  */
+/*
+ * 在以下使用msr_based_features_all[]:
+ *   - arch/x86/kvm/x86.c|1568| <<global>> static u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];
+ *   - arch/x86/kvm/x86.c|6990| <<kvm_init_msr_list>> for (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {
+ *   - arch/x86/kvm/x86.c|6993| <<kvm_init_msr_list>> msr.index = msr_based_features_all[i];
+ *   - arch/x86/kvm/x86.c|6997| <<kvm_init_msr_list>> msr_based_features[num_msr_based_features++] = msr_based_features_all[i];
+ */
 static const u32 msr_based_features_all[] = {
 	MSR_IA32_VMX_BASIC,
 	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
@@ -4554,6 +4584,12 @@ long kvm_arch_dev_ioctl(struct file *filp,
 
 	switch (ioctl) {
 	case KVM_GET_MSR_INDEX_LIST: {
+		/*
+		 * struct kvm_msr_list {
+		 *     __u32 nmsrs; // number of msrs in entries
+		 *     __u32 indices[];
+		 * };
+		 */
 		struct kvm_msr_list __user *user_msr_list = argp;
 		struct kvm_msr_list msr_list;
 		unsigned n;
@@ -6882,6 +6918,10 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12019| <<kvm_arch_hardware_setup>> kvm_init_msr_list();
+ */
 static void kvm_init_msr_list(void)
 {
 	u32 dummy[2];
@@ -11699,6 +11739,12 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		static_branch_dec(&kvm_has_noapic_vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3077| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2126| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11633| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -11949,6 +11995,10 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11992| <<kvm_arch_hardware_setup>> kvm_ops_update(ops);
+ */
 static inline void kvm_ops_update(struct kvm_x86_init_ops *ops)
 {
 	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
@@ -12051,6 +12101,16 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
+		/*
+		 * 在以下使用kvm_pmu->need_cleanup:
+		 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+		 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+		 *
+		 * The gate to release perf_events not marked in
+		 * pmc_in_use only once in a vcpu time slice.
+		 */
 		pmu->need_cleanup = true;
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
diff --git a/arch/x86/mm/extable.c b/arch/x86/mm/extable.c
index 60814e110..94a24fa5d 100644
--- a/arch/x86/mm/extable.c
+++ b/arch/x86/mm/extable.c
@@ -144,6 +144,13 @@ static bool ex_handler_copy(const struct exception_table_entry *fixup,
 	return ex_handler_fault(fixup, regs, trapnr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/extable.c|251| <<fixup_exception>> return ex_handler_msr(e, regs, true, false, reg);
+ *   - arch/x86/mm/extable.c|253| <<fixup_exception>> return ex_handler_msr(e, regs, false, false, reg);
+ *   - arch/x86/mm/extable.c|255| <<fixup_exception>> return ex_handler_msr(e, regs, true, true, reg);
+ *   - arch/x86/mm/extable.c|257| <<fixup_exception>> return ex_handler_msr(e, regs, false, true, reg);
+ */
 static bool ex_handler_msr(const struct exception_table_entry *fixup,
 			   struct pt_regs *regs, bool wrmsr, bool safe, int reg)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index c96c8c4f7..2506ba904 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -3301,6 +3301,14 @@ static inline bool blk_mq_last_cpu_in_hctx(unsigned int cpu,
 	return true;
 }
 
+/*
+ * [0] blk_mq_hctx_notify_offline
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,
diff --git a/drivers/block/ublk_drv.c b/drivers/block/ublk_drv.c
index 6a4a94b4c..fb480fea4 100644
--- a/drivers/block/ublk_drv.c
+++ b/drivers/block/ublk_drv.c
@@ -561,6 +561,10 @@ static inline bool ubq_daemon_is_dying(struct ublk_queue *ubq)
 }
 
 /* todo: handle partial completion */
+/*
+ * called by:
+ *   - drivers/block/ublk_drv.c|892| <<ublk_commit_completion>> ublk_complete_rq(req);
+ */
 static void ublk_complete_rq(struct request *req)
 {
 	struct ublk_queue *ubq = req->mq_hctx->driver_data;
@@ -873,6 +877,10 @@ static int ublk_ch_mmap(struct file *filp, struct vm_area_struct *vma)
 	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
 }
 
+/*
+ * 处理UBLK_IO_COMMIT_AND_FETCH_REQ:
+ *   - drivers/block/ublk_drv.c|1113| <<ublk_ch_uring_cmd>> ublk_commit_completion(ub, ub_cmd);
+ */
 static void ublk_commit_completion(struct ublk_device *ub,
 		struct ublksrv_io_cmd *ub_cmd)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index dd9a05174..0ac47ec87 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -322,6 +322,11 @@ static blk_status_t virtblk_prep_rq(struct blk_mq_hw_ctx *hctx,
 	if (unlikely(status))
 		return status;
 
+	/*
+	 * struct virtblk_req *vbr:
+	 * -> struct sg_table sg_table;
+	 *    -> unsigned int nents;
+	 */
 	vbr->sg_table.nents = virtblk_map_data(hctx, req, vbr);
 	if (unlikely(vbr->sg_table.nents < 0)) {
 		virtblk_cleanup_cmd(req);
diff --git a/drivers/pci/iov.c b/drivers/pci/iov.c
index 952217572..8c0f8b2a8 100644
--- a/drivers/pci/iov.c
+++ b/drivers/pci/iov.c
@@ -1201,6 +1201,13 @@ EXPORT_SYMBOL_GPL(pci_sriov_get_totalvfs);
  * before enabling SR-IOV.  Return value is negative on error, or number of
  * VFs allocated on success.
  */
+/*
+ * 在以下使用pci_sriov_configure_simple():
+ *   - drivers/misc/pci_endpoint_test.c|990| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/net/ethernet/amazon/ena/ena_netdev.c|4543| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/nvme/host/pci.c|3562| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/pci/pci-pf-stub.c|38| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ */
 int pci_sriov_configure_simple(struct pci_dev *dev, int nr_virtfn)
 {
 	int rc;
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index 828ced060..1771c84c2 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -246,9 +246,29 @@ static int virtio_dev_probe(struct device *_d)
 	/* We have a driver! */
 	virtio_add_status(dev, VIRTIO_CONFIG_S_DRIVER);
 
+	/*
+	 * legacy: vp_get_features()
+	 * modern: vp_get_features()
+	 *
+	 * 关于modern, 写入device_feature_select, 读取device_feature
+	 */
 	/* Figure out what features the device supports. */
 	device_features = dev->config->get_features(dev);
 
+	/*
+	 * virtio-scsi的feature的例子
+	 * 969 static unsigned int features[] = {
+	 * 970         VIRTIO_SCSI_F_HOTPLUG,
+	 * 971         VIRTIO_SCSI_F_CHANGE,
+	 * 972 #ifdef CONFIG_BLK_DEV_INTEGRITY
+	 * 973         VIRTIO_SCSI_F_T10_PI,
+	 * 974 #endif
+	 * 975 };
+	 * 976
+	 * 977 static struct virtio_driver virtio_scsi_driver = {
+	 * 978         .feature_table = features,
+	 * 979         .feature_table_size = ARRAY_SIZE(features),
+	 */
 	/* Figure out what features the driver supports. */
 	driver_features = 0;
 	for (i = 0; i < drv->feature_table_size; i++) {
@@ -269,16 +289,29 @@ static int virtio_dev_probe(struct device *_d)
 		driver_features_legacy = driver_features;
 	}
 
+	/*
+	 * device_features是上面用vp_get_features()读取
+	 * 关于modern, 写入device_feature_select, 读取device_feature
+	 */
 	if (device_features & (1ULL << VIRTIO_F_VERSION_1))
 		dev->features = driver_features & device_features;
 	else
 		dev->features = driver_features_legacy & device_features;
 
+	/*
+	 * #define VIRTIO_TRANSPORT_F_START        28
+	 * #define VIRTIO_TRANSPORT_F_END          41
+	 * #define VIRTIO_F_VERSION_1              32
+	 */
 	/* Transport features always preserved to pass to finalize_features. */
 	for (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++)
 		if (device_features & (1ULL << i))
 			__virtio_set_bit(dev, i);
 
+	/*
+	 * legacy: vp_finalize_features()
+	 * modern: vp_finalize_features()
+	 */
 	err = dev->config->finalize_features(dev);
 	if (err)
 		goto err;
@@ -415,6 +448,16 @@ static int virtio_device_of_init(struct virtio_device *dev)
  *
  * Returns: 0 on suceess, -error on failure
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1218| <<virtio_uml_probe>> rc = register_virtio_device(&vu_dev->vdev);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1097| <<mlxbf_tmfifo_create_vdev>> ret = register_virtio_device(&tm_vdev->vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|430| <<rproc_add_virtio_dev>> ret = register_virtio_device(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1345| <<virtio_ccw_online>> ret = register_virtio_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|667| <<virtio_mmio_probe>> rc = register_virtio_device(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|559| <<virtio_pci_probe>> rc = register_virtio_device(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|379| <<virtio_vdpa_probe>> ret = register_virtio_device(&vd_dev->vdev);
+ */
 int register_virtio_device(struct virtio_device *dev)
 {
 	int err;
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index 3f78a3a1e..ad9f21a05 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -447,6 +447,13 @@ static void virtballoon_changed(struct virtio_device *vdev)
 	spin_unlock_irqrestore(&vb->stop_update_lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_balloon.c|484| <<update_balloon_size_func>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|843| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1015| <<remove_common>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1078| <<virtballoon_restore>> update_balloon_size(vb);
+ */
 static void update_balloon_size(struct virtio_balloon *vb)
 {
 	u32 actual = vb->num_pages;
diff --git a/drivers/virtio/virtio_pci_common.c b/drivers/virtio/virtio_pci_common.c
index ad258a9d3..97299b608 100644
--- a/drivers/virtio/virtio_pci_common.c
+++ b/drivers/virtio/virtio_pci_common.c
@@ -16,6 +16,13 @@
 
 #include "virtio_pci_common.h"
 
+/*
+ * 在以下使用force_legacy:
+ *   - drivers/virtio/virtio_pci_common.c|19| <<global>> static bool force_legacy = false;
+ *   - drivers/virtio/virtio_pci_common.c|22| <<global>> module_param(force_legacy, bool, 0444);
+ *   - drivers/virtio/virtio_pci_common.c|23| <<global>> MODULE_PARM_DESC(force_legacy,
+ *   - drivers/virtio/virtio_pci_common.c|540| <<virtio_pci_probe>> if (force_legacy) {
+ */
 static bool force_legacy = false;
 
 #if IS_ENABLED(CONFIG_VIRTIO_PCI_LEGACY)
@@ -537,6 +544,13 @@ static int virtio_pci_probe(struct pci_dev *pci_dev,
 	if (rc)
 		goto err_enable_device;
 
+	/*
+	 * 在以下使用force_legacy:
+	 *   - drivers/virtio/virtio_pci_common.c|19| <<global>> static bool force_legacy = false;
+	 *   - divers/virtio/virtio_pci_common.c|22| <<global>> module_param(force_legacy, bool, 0444);
+	 *   - drivers/virtio/virtio_pci_common.c|23| <<global>> MODULE_PARM_DESC(force_legacy,
+	 *   - drivers/virtio/virtio_pci_common.c|540| <<virtio_pci_probe>> if (force_legacy) {
+	 */
 	if (force_legacy) {
 		rc = virtio_pci_legacy_probe(vp_dev);
 		/* Also try modern mode if we can't map BAR0 (no IO space). */
@@ -554,6 +568,13 @@ static int virtio_pci_probe(struct pci_dev *pci_dev,
 
 	pci_set_master(pci_dev);
 
+	/*
+	 * struct virtio_pci_device *vp_dev:
+	 * -> struct virtio_pci_legacy_device ldev;
+	 *    -> u8 __iomem *isr;
+	 *    -> void __iomem *ioaddr;
+	 * -> struct virtio_pci_modern_device mdev;
+	 */
 	vp_dev->is_legacy = vp_dev->ldev.ioaddr ? true : false;
 
 	rc = register_virtio_device(&vp_dev->vdev);
@@ -603,6 +624,25 @@ static void virtio_pci_remove(struct pci_dev *pci_dev)
 	put_device(dev);
 }
 
+/*
+ * commit cfecc2918d2b3c5e86ff1a6c95eabbbb17bb8fd3
+ * Author: Tiwei Bie <tiwei.bie@intel.com>
+ * Date:   Fri Jun 1 12:02:39 2018 +0800
+ *
+ * virtio_pci: support enabling VFs
+ *
+ * There is a new feature bit allocated in virtio spec to
+ * support SR-IOV (Single Root I/O Virtualization):
+ *
+ * https://github.com/oasis-tcs/virtio-spec/issues/11
+ *
+ * This patch enables the support for this feature bit in
+ * virtio driver.
+ *
+ * Signed-off-by: Tiwei Bie <tiwei.bie@intel.com>
+ * Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
+ */
+
 static int virtio_pci_sriov_configure(struct pci_dev *pci_dev, int num_vfs)
 {
 	struct virtio_pci_device *vp_dev = pci_get_drvdata(pci_dev);
diff --git a/drivers/virtio/virtio_pci_modern.c b/drivers/virtio/virtio_pci_modern.c
index c3b9f2761..6892dc5b2 100644
--- a/drivers/virtio/virtio_pci_modern.c
+++ b/drivers/virtio/virtio_pci_modern.c
@@ -26,6 +26,10 @@ static u64 vp_get_features(struct virtio_device *vdev)
 	return vp_modern_get_features(&vp_dev->mdev);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|52| <<vp_finalize_features>> vp_transport_features(vdev, features);
+ */
 static void vp_transport_features(struct virtio_device *vdev, u64 features)
 {
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
@@ -45,6 +49,15 @@ static int vp_finalize_features(struct virtio_device *vdev)
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
 	u64 features = vdev->features;
 
+	/*
+	 * 清空virtio_device->features中在VIRTIO_TRANSPORT_F_START->VIRTIO_TRANSPORT_F_END并且不在以下的bit
+	 * - VIRTIO_RING_F_INDIRECT_DESC
+	 * - VIRTIO_RING_F_EVENT_IDX
+	 * - VIRTIO_F_VERSION_1:
+	 * - VIRTIO_F_ACCESS_PLATFORM:
+	 * - VIRTIO_F_RING_PACKED:
+	 * - VIRTIO_F_ORDER_PLATFORM:
+	 */
 	/* Give virtio_ring a chance to accept features. */
 	vring_transport_features(vdev);
 
diff --git a/drivers/virtio/virtio_pci_modern_dev.c b/drivers/virtio/virtio_pci_modern_dev.c
index 869cb46be..486067f8b 100644
--- a/drivers/virtio/virtio_pci_modern_dev.c
+++ b/drivers/virtio/virtio_pci_modern_dev.c
@@ -111,6 +111,13 @@ vp_modern_map_capability(struct virtio_pci_modern_device *mdev, int off,
  *
  * Returns offset of the capability, or 0.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern_dev.c|240| <<vp_modern_probe>> common = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_COMMON_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|250| <<vp_modern_probe>> isr = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_ISR_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|253| <<vp_modern_probe>> notify = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_NOTIFY_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|273| <<vp_modern_probe>> device = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_DEVICE_CFG,
+ */
 static inline int virtio_pci_find_capability(struct pci_dev *dev, u8 cfg_type,
 					     u32 ioresource_types, int *bars)
 {
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 4620e9d79..d2ddaa4bf 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -32,6 +32,16 @@
 	} while (0)
 #define END_USE(_vq) \
 	do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while(0)
+/*
+ * 在以下使用vring_virtqueue->last_add_time:
+ *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+ *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+ *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+ *
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|535| <<virtqueue_add_split>> LAST_ADD_TIME_UPDATE(vq);
+ *   - drivers/virtio/virtio_ring.c|1363| <<virtqueue_add_packed>> LAST_ADD_TIME_UPDATE(vq);
+ */
 #define LAST_ADD_TIME_UPDATE(_vq)				\
 	do {							\
 		ktime_t now = ktime_get();			\
@@ -43,6 +53,11 @@
 		(_vq)->last_add_time = now;			\
 		(_vq)->last_add_time_valid = true;		\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|707| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_CHECK(vq);
+ *   - drivers/virtio/virtio_ring.c|1515| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_CHECK(vq);
+ */
 #define LAST_ADD_TIME_CHECK(_vq)				\
 	do {							\
 		if ((_vq)->last_add_time_valid) {		\
@@ -50,6 +65,13 @@
 				      (_vq)->last_add_time)) > 100); \
 		}						\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|765| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|888| <<virtqueue_get_buf_ctx_split>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|1612| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|1766| <<virtqueue_get_buf_ctx_packed>> LAST_ADD_TIME_INVALID(vq);
+ */
 #define LAST_ADD_TIME_INVALID(_vq)				\
 	((_vq)->last_add_time_valid = false)
 #else
@@ -90,6 +112,22 @@ struct vring_virtqueue_split {
 	struct vring vring;
 
 	/* Last written value to avail->flags */
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1068| <<virtqueue_get_buf_ctx_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|1083| <<virtqueue_disable_cb_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|1084| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1091| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow); 
+	 *   - drivers/virtio/virtio_ring.c|1111| <<virtqueue_enable_cb_prepare_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1112| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1116| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1148| <<virtqueue_enable_cb_delayed_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1149| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1153| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1215| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1218| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow);
+	 */
 	u16 avail_flags_shadow;
 
 	/*
@@ -100,6 +138,10 @@ struct vring_virtqueue_split {
 
 	/* Per-descriptor state. */
 	struct vring_desc_state_split *desc_state;
+	/*
+	 * 在以下分配vring_virtqueue_split->desc_extra, num个:
+	 *   - drivers/virtio/virtio_ring.c|1096| <<vring_alloc_state_extra_split>> vring_split->desc_extra = extra;
+	 */
 	struct vring_desc_extra *desc_extra;
 
 	/* DMA address and size information */
@@ -111,9 +153,22 @@ struct vring_virtqueue_split {
 	 * vring.
 	 */
 	u32 vring_align;
+	/*
+	 * 在以下使用vring_virtqueue_split->may_reduce_num:
+	 *   - drivers/virtio/virtio_ring.c|1159| <<vring_alloc_queue_split>> vring_split->may_reduce_num = may_reduce_num;
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_resize_split>> vq->split.may_reduce_num);
+	 */
 	bool may_reduce_num;
 };
 
+/*
+ * struct vring_packed_desc_event {
+ *     // Descriptor Ring Change Event Offset/Wrap Counter.
+ *     __le16 off_wrap;
+ *     // Descriptor Ring Change Event Flags.
+ *     __le16 flags;
+ * };
+ */
 struct vring_virtqueue_packed {
 	/* Actual memory layout for this queue. */
 	struct {
@@ -124,18 +179,80 @@ struct vring_virtqueue_packed {
 	} vring;
 
 	/* Driver ring wrap counter. */
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_wrap_counter:
+	 *   - drivers/virtio/virtio_ring.c|1417| <<virtqueue_add_indirect_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1655| <<virtqueue_kick_prepare_packed>> if (wrap_counter != vq->packed.avail_wrap_counter)
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_vring_init_packed>> vring_packed->avail_wrap_counter = 1;
+	 *
+	 * 主要和vq->packed.vring.device.off_wrap比较,
+	 * 在add的时候当avail index wrap回0的时候会flip
+	 */
 	bool avail_wrap_counter;
 
 	/* Avail used flags. */
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_used_flags:
+	 *   - drivers/virtio/virtio_ring.c|1466| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags);
+	 *   - drivers/virtio/virtio_ring.c|1486| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags ^=
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> avail_used_flags = vq->packed.avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|1602| <<virtqueue_add_packed>> flags = cpu_to_le16(vq->packed.avail_used_flags |
+	 *   - drivers/virtio/virtio_ring.c|1625| <<virtqueue_add_packed>> vq->packed.avail_used_flags ^=
+	 *   - drivers/virtio/virtio_ring.c|1667| <<virtqueue_add_packed>> vq->packed.avail_used_flags = avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|2143| <<virtqueue_vring_init_packed>> vring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+	 *
+	 * 表示在add的时候在desc[i].flags中应该用什么标记avail/used
+	 */
 	u16 avail_used_flags;
 
+	/*
+	 * 在以下使用vring_virtqueue_packed->next_avail_idx:
+	 *   - drivers/virtio/virtio_ring.c|1429| <<virtqueue_add_indirect_packed>> head = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|1501| <<virtqueue_add_indirect_packed>> vq->packed.next_avail_idx = n;
+	 *   - drivers/virtio/virtio_ring.c|1575| <<virtqueue_add_packed>> head = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|1669| <<virtqueue_add_packed>> vq->packed.next_avail_idx = i;
+	 *   - drivers/virtio/virtio_ring.c|1734| <<virtqueue_kick_prepare_packed>> old = vq->packed.next_avail_idx - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|1735| <<virtqueue_kick_prepare_packed>> new = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|2189| <<virtqueue_vring_init_packed>> vring_packed->next_avail_idx = 0;
+	 */
 	/* Index of the next avail descriptor. */
+	/*
+	 * 用来索引vq->packed.vring.desc[i]
+	 */
 	u16 next_avail_idx;
 
 	/*
 	 * Last written value to driver->flags in
 	 * guest byte order.
 	 */
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_FLAG_DESC:
+	 *   - drivers/virtio/virtio_ring.c|1822| <<virtqueue_kick_prepare_packed>> if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+	 *   - drivers/virtio/virtio_ring.c|1997| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+	 *   - drivers/virtio/virtio_ring.c|2042| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2101| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *
+	 * 在以下使用vring_virtqueue_pack->event_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1916| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+	 *   - drivers/virtio/virtio_ring.c|1931| <<virtqueue_disable_cb_packed>> if (vq->packed.event_flags_shadow != VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|1932| <<virtqueue_disable_cb_packed>> vq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+	 *   - drivers/virtio/virtio_ring.c|1934| <<virtqueue_disable_cb_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1959| <<virtqueue_enable_cb_prepare_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|1960| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ?
+	 *   - drivers/virtio/virtio_ring.c|1964| <<virtqueue_enable_cb_prepare_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|2018| <<virtqueue_enable_cb_delayed_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|2019| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ?
+	 *   - drivers/virtio/virtio_ring.c|2023| <<virtqueue_enable_cb_delayed_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|2191| <<virtqueue_vring_init_packed>> vring_packed->event_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2196| <<virtqueue_vring_init_packed>> vring_packed->event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+	 *   - drivers/virtio/virtio_ring.c|2198| <<virtqueue_vring_init_packed>> cpu_to_le16(vring_packed->event_flags_shadow);
+	 *
+	 * 设置为VRING_PACKED_EVENT_FLAG_DESC或者VRING_PACKED_EVENT_FLAG_ENABLE/VRING_PACKED_EVENT_FLAG_DISABLE
+	 * 用来控制virtio是否想接受QEMU的中断
+	 * 比如说: 也可以判断是否使用event idx告诉backend前端的progress
+	 */
 	u16 event_flags_shadow;
 
 	/* Per-descriptor state. */
@@ -144,9 +261,35 @@ struct vring_virtqueue_packed {
 
 	/* DMA address and size information */
 	dma_addr_t ring_dma_addr;
+	/*
+	 * 在以下使用vring_virtqueue_packed->driver_event_dma_addr:
+	 *   - drivers/virtio/virtio_ring.c|1942| <<vring_free_packed>> vring_packed->driver_event_dma_addr);
+	 *   - drivers/virtio/virtio_ring.c|1984| <<vring_alloc_queue_packed>> vring_packed->driver_event_dma_addr = driver_event_dma_addr;
+	 *   - drivers/virtio/virtio_ring.c|2754| <<vring_free>> vq->packed.driver_event_dma_addr);
+	 *   - drivers/virtio/virtio_ring.c|2926| <<virtqueue_get_avail_addr>> return vq->packed.driver_event_dma_addr;
+	 *
+	 * 代表了avail
+	 */
 	dma_addr_t driver_event_dma_addr;
 	dma_addr_t device_event_dma_addr;
+	/*
+	 * 在以下使用vring_virtqueue_packed->ring_size_in_bytes:
+	 *   - drivers/virtio/virtio_ring.c|2174| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->ring_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2211| <<vring_alloc_queue_packed>> vring_packed->ring_size_in_bytes = ring_size_in_bytes;
+	 *   - drivers/virtio/virtio_ring.c|2301| <<virtqueue_reinit_packed>> memset(vq->packed.vring.desc, 0, vq->packed.ring_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|3100| <<vring_free>> vq->packed.ring_size_in_bytes,
+	 */
 	size_t ring_size_in_bytes;
+	/*
+	 * 在以下使用vring_virtqueue_packed->event_size_in_bytes:
+	 *   - drivers/virtio/virtio_ring.c|2179| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2184| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2222| <<vring_alloc_queue_packed>> vring_packed->event_size_in_bytes = event_size_in_bytes;
+	 *   - drivers/virtio/virtio_ring.c|2297| <<virtqueue_reinit_packed>> memset(vq->packed.vring.device, 0, vq->packed.event_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|2298| <<virtqueue_reinit_packed>> memset(vq->packed.vring.driver, 0, vq->packed.event_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|3105| <<vring_free>> vq->packed.event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|3110| <<vring_free>> vq->packed.event_size_in_bytes,
+	 */
 	size_t event_size_in_bytes;
 };
 
@@ -154,6 +297,11 @@ struct vring_virtqueue {
 	struct virtqueue vq;
 
 	/* Is this a packed ring? */
+	/*
+	 * 在以下设置vring_virtqueue->packed_ring:
+	 *   - drivers/virtio/virtio_ring.c|2008| <<bool>> vq->packed_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2488| <<bool>> vq->packed_ring = false;
+	 */
 	bool packed_ring;
 
 	/* Is DMA API used? */
@@ -169,6 +317,22 @@ struct vring_virtqueue {
 	bool indirect;
 
 	/* Host publishes avail event idx */
+	/*
+	 * 在以下设置vring_virtqueue->event:
+	 *   - drivers/virtio/virtio_ring.c|2377| <<vring_create_virtqueue_packed>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+	 *   - drivers/virtio/virtio_ring.c|2973| <<__vring_new_virtqueue>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+	 * 在以下使用vring_virtqueue->event:
+	 *   - drivers/virtio/virtio_ring.c|928| <<virtqueue_kick_prepare_split>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|1070| <<virtqueue_disable_cb_split>> if (vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1098| <<virtqueue_enable_cb_prepare_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1135| <<virtqueue_enable_cb_delayed_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1201| <<virtqueue_vring_init_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|2062| <<virtqueue_enable_cb_prepare_packed>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2109| <<virtqueue_enable_cb_delayed_packed>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|2132| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2919| <<vring_interrupt>> if (vq->event)
+	 */
 	bool event;
 
 	/* Head of free buffer list. */
@@ -185,6 +349,16 @@ struct vring_virtqueue {
 	u16 last_used_idx;
 
 	/* Hint for event idx: already triggered no need to disable. */
+	/*
+	 * 在以下使用vring_virtqueue->event_triggered:
+	 *   - drivers/virtio/virtio_ring.c|500| <<virtqueue_init>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2465| <<virtqueue_disable_cb>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2491| <<virtqueue_enable_cb_prepare>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2492| <<virtqueue_enable_cb_prepare>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2557| <<virtqueue_enable_cb_delayed>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2558| <<virtqueue_enable_cb_delayed>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2616| <<vring_interrupt>> vq->event_triggered = true;
+	 */
 	bool event_triggered;
 
 	union {
@@ -199,14 +373,47 @@ struct vring_virtqueue {
 	bool (*notify)(struct virtqueue *vq);
 
 	/* DMA, allocation, and size information */
+	/*
+	 * 在以下使用vring_virtqueue->we_own_ring:
+	 *   - drivers/virtio/virtio_ring.c|1213| <<vring_create_virtqueue_split>> to_vvq(vq)->we_own_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2141| <<vring_create_virtqueue_packed>> vq->we_own_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2641| <<__vring_new_virtqueue>> vq->we_own_ring = false;
+	 *   - drivers/virtio/virtio_ring.c|2731| <<virtqueue_resize>> if (!vq->we_own_ring)
+	 *   - drivers/virtio/virtio_ring.c|2795| <<vring_free>> if (vq->we_own_ring) {
+	 *   - drivers/virtio/virtio_ring.c|2961| <<virtqueue_get_desc_addr>> BUG_ON(!vq->we_own_ring);
+	 *   - drivers/virtio/virtio_ring.c|2974| <<virtqueue_get_avail_addr>> BUG_ON(!vq->we_own_ring);
+	 *   - drivers/virtio/virtio_ring.c|2988| <<virtqueue_get_used_addr>> BUG_ON(!vq->we_own_ring);
+	 */
 	bool we_own_ring;
 
 #ifdef DEBUG
 	/* They're supposed to lock for us. */
+	/*
+	 * 在以下使用vring_virtqueue->in_use:
+	 *   - drivers/virtio/virtio_ring.c|28| <<START_USE>> if ((_vq)->in_use) \
+	 *   - drivers/virtio/virtio_ring.c|30| <<START_USE>> (_vq)->vq.name, (_vq)->in_use); \
+	 *   - drivers/virtio/virtio_ring.c|31| <<START_USE>> (_vq)->in_use = __LINE__; \
+	 *   - drivers/virtio/virtio_ring.c|34| <<END_USE>> do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while (0)
+	 *   - drivers/virtio/virtio_ring.c|401| <<virtqueue_init>> vq->in_use = false;
+	 */
 	unsigned int in_use;
 
 	/* Figure out if their kicks are too delayed. */
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|40| <<LAST_ADD_TIME_UPDATE>> if ((_vq)->last_add_time_valid) \
+	 *   - drivers/virtio/virtio_ring.c|44| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time_valid = true; \
+	 *   - drivers/virtio/virtio_ring.c|48| <<LAST_ADD_TIME_CHECK>> if ((_vq)->last_add_time_valid) { \
+	 *   - drivers/virtio/virtio_ring.c|54| <<LAST_ADD_TIME_INVALID>> ((_vq)->last_add_time_valid = false)
+	 *   - drivers/virtio/virtio_ring.c|402| <<virtqueue_init>> vq->last_add_time_valid = false;
+	 */
 	bool last_add_time_valid;
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time:
+	 *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+	 *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+	 *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+	 */
 	ktime_t last_add_time;
 #endif
 };
@@ -284,6 +491,10 @@ static bool vring_use_dma_api(struct virtio_device *vdev)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|997| <<virtblk_probe>> max_size = virtio_max_dma_size(vdev);
+ */
 size_t virtio_max_dma_size(struct virtio_device *vdev)
 {
 	size_t max_segment_size = SIZE_MAX;
@@ -295,6 +506,16 @@ size_t virtio_max_dma_size(struct virtio_device *vdev)
 }
 EXPORT_SYMBOL_GPL(virtio_max_dma_size);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1054| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1068| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1868| <<vring_alloc_queue_packed>> ring = vring_alloc_queue(vdev, ring_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1880| <<vring_alloc_queue_packed>> driver = vring_alloc_queue(vdev, event_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1890| <<vring_alloc_queue_packed>> device = vring_alloc_queue(vdev, event_size_in_bytes,
+ *
+ * 分配size
+ */
 static void *vring_alloc_queue(struct virtio_device *vdev, size_t size,
 			      dma_addr_t *dma_handle, gfp_t flag)
 {
@@ -342,6 +563,18 @@ static void vring_free_queue(struct virtio_device *vdev, size_t size,
  * making all of the arch DMA ops work on the vring device itself
  * is a mess.  For now, we use the parent device for DMA ops.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|363| <<vring_map_one_sg>> return dma_map_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|375| <<vring_map_single>> return dma_map_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|385| <<vring_mapping_error>> return dma_mapping_error(vring_dma_dev(vq), addr);
+ *   - drivers/virtio/virtio_ring.c|421| <<vring_unmap_one_split_indirect>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|440| <<vring_unmap_one_split>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|446| <<vring_unmap_one_split>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1176| <<vring_unmap_extra_packed>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1181| <<vring_unmap_extra_packed>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1198| <<vring_unmap_desc_packed>> dma_unmap_page(vring_dma_dev(vq),
+ */
 static inline struct device *vring_dma_dev(const struct vring_virtqueue *vq)
 {
 	return vq->vq.vdev->dev.parent;
@@ -385,6 +618,15 @@ static int vring_mapping_error(const struct vring_virtqueue *vq,
 	return dma_mapping_error(vring_dma_dev(vq), addr);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1097| <<virtqueue_reinit_split>> virtqueue_init(vq, num);
+ *   - drivers/virtio/virtio_ring.c|1254| <<virtqueue_resize_split>> virtqueue_init(vq, vring_split.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2120| <<virtqueue_reinit_packed>> virtqueue_init(vq, vq->packed.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2176| <<vring_create_virtqueue_packed>> virtqueue_init(vq, num);
+ *   - drivers/virtio/virtio_ring.c|2210| <<virtqueue_resize_packed>> virtqueue_init(vq, vring_packed.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2677| <<__vring_new_virtqueue>> virtqueue_init(vq, vring_split->vring.num);
+ */
 static void virtqueue_init(struct vring_virtqueue *vq, u32 num)
 {
 	vq->vq.num_free = num;
@@ -506,6 +748,10 @@ static inline unsigned int virtqueue_add_desc_split(struct virtqueue *vq,
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2240| <<virtqueue_add>> virtqueue_add_split(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_split(struct virtqueue *_vq,
 				      struct scatterlist *sgs[],
 				      unsigned int total_sg,
@@ -689,6 +935,10 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2363| <<virtqueue_kick_prepare>> virtqueue_kick_prepare_split(_vq);
+ */
 static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -720,6 +970,11 @@ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 	return needs_kick;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|928| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+ */
 static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
 			     void **ctx)
 {
@@ -776,6 +1031,10 @@ static inline bool more_used_split(const struct vring_virtqueue *vq)
 			vq->split.vring.used->idx);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2378| <<virtqueue_get_buf_ctx>> virtqueue_get_buf_ctx_split(_vq, len, ctx);
+ */
 static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 					 unsigned int *len,
 					 void **ctx)
@@ -850,6 +1109,10 @@ static void virtqueue_disable_cb_split(struct virtqueue *_vq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2495| <<virtqueue_enable_cb_prepare>> virtqueue_enable_cb_prepare_split(_vq);
+ */
 static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -875,6 +1138,10 @@ static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 	return last_used_idx;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2359| <<virtqueue_poll>> virtqueue_poll_split(_vq, last_used_idx);
+ */
 static bool virtqueue_poll_split(struct virtqueue *_vq, unsigned int last_used_idx)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -919,6 +1186,10 @@ static bool virtqueue_enable_cb_delayed_split(struct virtqueue *_vq)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2578| <<virtqueue_detach_unused_buf>> virtqueue_detach_unused_buf_split(_vq);
+ */
 static void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1082,6 +1353,10 @@ static int vring_alloc_queue_split(struct vring_virtqueue_split *vring_split,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2704| <<vring_create_virtqueue>> return vring_create_virtqueue_split(index, num, vring_align,
+ */
 static struct virtqueue *vring_create_virtqueue_split(
 	unsigned int index,
 	unsigned int num,
@@ -1115,6 +1390,10 @@ static struct virtqueue *vring_create_virtqueue_split(
 	return vq;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2612| <<virtqueue_resize>> err = virtqueue_resize_split(_vq, num);
+ */
 static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 {
 	struct vring_virtqueue_split vring_split = {};
@@ -1152,8 +1431,27 @@ static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 /*
  * Packed ring specific functions - *_packed().
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1854| <<more_used_packed>> used_wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|1884| <<virtqueue_get_buf_ctx_packed>> used_wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|2000| <<virtqueue_enable_cb_delayed_packed>> wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|2033| <<virtqueue_enable_cb_delayed_packed>> wrap_counter = packed_used_wrap_counter(last_used_idx);
+ */
 static inline bool packed_used_wrap_counter(u16 last_used_idx)
 {
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+	 *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 */
 	return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
@@ -1162,6 +1460,11 @@ static inline u16 packed_last_used(u16 last_used_idx)
 	return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_packed>> vring_unmap_extra_packed(vq, &vq->packed.desc_extra[curr]);
+ *   - drivers/virtio/virtio_ring.c|1555| <<detach_buf_packed>> vring_unmap_extra_packed(vq,
+ */
 static void vring_unmap_extra_packed(const struct vring_virtqueue *vq,
 				     struct vring_desc_extra *extra)
 {
@@ -1219,6 +1522,10 @@ static struct vring_packed_desc *alloc_indirect_packed(unsigned int total_sg,
 	return desc;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1675| <<virtqueue_add_packed>> err = virtqueue_add_indirect_packed(vq, sgs, total_sg, out_sgs,
+ */
 static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 					 struct scatterlist *sgs[],
 					 unsigned int total_sg,
@@ -1333,6 +1640,10 @@ static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2168| <<virtqueue_add>> return vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_packed(struct virtqueue *_vq,
 				       struct scatterlist *sgs[],
 				       unsigned int total_sg,
@@ -1380,7 +1691,20 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 
 	WARN_ON_ONCE(total_sg > vq->packed.vring.num && !vq->indirect);
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *        struct {
+	 *            unsigned int num;
+	 *            struct vring_packed_desc *desc;
+	 *            struct vring_packed_desc_event *driver;
+	 *            struct vring_packed_desc_event *device;
+	 *        } vring;
+	 */
 	desc = vq->packed.vring.desc;
+	/*
+	 * i用来索引desc
+	 */
 	i = head;
 	descs_used = total_sg;
 
@@ -1403,6 +1727,22 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 			if (vring_mapping_error(vq, addr))
 				goto unmap_release;
 
+			/*
+			 * 在以下使用vring_virtqueue_packed->avail_used_flags:
+			 *   - drivers/virtio/virtio_ring.c|1466| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags);
+			 *   - drivers/virtio/virtio_ring.c|1486| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags ^=
+			 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> avail_used_flags = vq->packed.avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|1602| <<virtqueue_add_packed>> flags = cpu_to_le16(vq->packed.avail_used_flags |
+			 *   - drivers/virtio/virtio_ring.c|1625| <<virtqueue_add_packed>> vq->packed.avail_used_flags ^=
+			 *   - drivers/virtio/virtio_ring.c|1667| <<virtqueue_add_packed>> vq->packed.avail_used_flags = avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|2143| <<virtqueue_vring_init_packed>> vring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+			 *
+			 * flags分成三部分:
+			 * - vq->packed.avail_used_flags
+			 * - (++c == total_sg ? 0 : VRING_DESC_F_NEXT)
+			 * - (n < out_sgs ? 0 : VRING_DESC_F_WRITE)
+			 */
 			flags = cpu_to_le16(vq->packed.avail_used_flags |
 				    (++c == total_sg ? 0 : VRING_DESC_F_NEXT) |
 				    (n < out_sgs ? 0 : VRING_DESC_F_WRITE));
@@ -1433,6 +1773,15 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 		}
 	}
 
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_wrap_counter:
+	 *   - drivers/virtio/virtio_ring.c|1417| <<virtqueue_add_indirect_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1655| <<virtqueue_kick_prepare_packed>> if (wrap_counter != vq->packed.avail_wrap_counter)
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_vring_init_packed>> vring_packed->avail_wrap_counter = 1;
+	 *
+	 * 似乎只用在kick的判断上
+	 */
 	if (i < head)
 		vq->packed.avail_wrap_counter ^= 1;
 
@@ -1484,6 +1833,10 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 	return -EIO;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2503| <<virtqueue_kick_prepare>> return vq->packed_ring ? virtqueue_kick_prepare_packed(_vq) :
+ */
 static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1505,16 +1858,43 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 	 */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 用来索引desc = vq->packed.vring.desc
+	 */
 	old = vq->packed.next_avail_idx - vq->num_added;
 	new = vq->packed.next_avail_idx;
 	vq->num_added = 0;
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *    -> struct {
+	 *           unsigned int num;
+	 *           struct vring_packed_desc *desc;
+	 *           struct vring_packed_desc_event *driver;
+	 *           struct vring_packed_desc_event *device;
+	 *       } vring;
+	 */
 	snapshot.u32 = *(u32 *)vq->packed.vring.device;
 	flags = le16_to_cpu(snapshot.flags);
 
 	LAST_ADD_TIME_CHECK(vq);
 	LAST_ADD_TIME_INVALID(vq);
 
+	/*
+	 * 有这样类似的代码
+	 * 2012                 vq->packed.event_flags_shadow = vq->event ?
+	 * 2013                                 VRING_PACKED_EVENT_FLAG_DESC :
+	 * 2014                                 VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *
+	 * 在QEMU side, 设置handle_request (avai)是否想接收中断.
+	 *
+	 * disable: 设置VRING_PACKED_EVENT_FLAG_DISABLE
+	 * enable:  设置VRING_PACKED_EVENT_FLAG_DESC或者VRING_PACKED_EVENT_FLAG_ENABLE
+	 *
+	 * 如果设置VRING_PACKED_EVENT_FLAG_DESC, 直接跳过, 说明是ENABLE
+	 * 否则有可能是DISABLE
+	 */
 	if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
 		needs_kick = (flags != VRING_PACKED_EVENT_FLAG_DISABLE);
 		goto out;
@@ -1527,12 +1907,21 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 	if (wrap_counter != vq->packed.avail_wrap_counter)
 		event_idx -= vq->packed.vring.num;
 
+	/*
+	 * event_idx应该相当于现在otherend处理到的,比如vring_used_event(&ring)
+	 */
 	needs_kick = vring_need_event(event_idx, new, old);
 out:
 	END_USE(vq);
 	return needs_kick;
 }
 
+
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1745| <<virtqueue_get_buf_ctx_packed>> detach_buf_packed(vq, id, ctx);
+ *   - drivers/virtio/virtio_ring.c|1902| <<virtqueue_detach_unused_buf_packed>> detach_buf_packed(vq, i, NULL);
+ */
 static void detach_buf_packed(struct vring_virtqueue *vq,
 			      unsigned int id, void **ctx)
 {
@@ -1579,16 +1968,35 @@ static void detach_buf_packed(struct vring_virtqueue *vq,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1806| <<more_used_packed>> return is_used_desc_packed(vq, last_used, used_wrap_counter);
+ *   - drivers/virtio/virtio_ring.c|1931| <<virtqueue_poll_packed>> return is_used_desc_packed(vq, used_idx, wrap_counter);
+ *   - drivers/virtio/virtio_ring.c|1986| <<virtqueue_enable_cb_delayed_packed>> if (is_used_desc_packed(vq, used_idx, wrap_counter)) {
+ */
 static inline bool is_used_desc_packed(const struct vring_virtqueue *vq,
 				       u16 idx, bool used_wrap_counter)
 {
 	bool avail, used;
 	u16 flags;
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *    -> struct {
+	 *           unsigned int num;
+	 *           struct vring_packed_desc *desc;
+	 *           struct vring_packed_desc_event *driver;
+	 *           struct vring_packed_desc_event *device;
+	 *       } vring;
+	 */
 	flags = le16_to_cpu(vq->packed.vring.desc[idx].flags);
 	avail = !!(flags & (1 << VRING_PACKED_DESC_F_AVAIL));
 	used = !!(flags & (1 << VRING_PACKED_DESC_F_USED));
 
+	/*
+	 * 如果只设置了avail, 没设置used, 就返回false
+	 */
 	return avail == used && used == used_wrap_counter;
 }
 
@@ -1673,6 +2081,10 @@ static void *virtqueue_get_buf_ctx_packed(struct virtqueue *_vq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2755| <<virtqueue_disable_cb>> virtqueue_disable_cb_packed(_vq);
+ */
 static void virtqueue_disable_cb_packed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2203,6 +2615,27 @@ EXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);
  * This is sometimes useful because the virtqueue_kick_prepare() needs
  * to be serialized, but the actual virtqueue_notify() call does not.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|307| <<virtio_commit_rqs>> kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/block/virtio_blk.c|379| <<virtio_queue_rq>> if (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))
+ *   - drivers/block/virtio_blk.c|419| <<virtblk_add_req_batch>> kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|429| <<virtio_gpu_notify>> notify = virtqueue_kick_prepare(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|471| <<virtio_gpu_queue_cursor>> notify = virtqueue_kick_prepare(vq);
+ *   - drivers/net/virtio_net.c|682| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1461| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1682| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1885| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> notify = virtqueue_kick_prepare(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|468| <<virtscsi_kick_vq>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|495| <<virtscsi_add_cmd>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2546| <<virtqueue_kick>> if (virtqueue_kick_prepare(vq))
+ *   - fs/fuse/virtio_fs.c|457| <<send_forget_request>> notify = virtqueue_kick_prepare(vq);
+ *   - fs/fuse/virtio_fs.c|1203| <<virtio_fs_enqueue_req>> notify = virtqueue_kick_prepare(vq);
+ *   - sound/virtio/virtio_card.c|44| <<virtsnd_event_send>> if (virtqueue_kick_prepare(vqueue))
+ *   - sound/virtio/virtio_ctl_msg.c|154| <<virtsnd_ctl_msg_send>> notify = virtqueue_kick_prepare(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|245| <<virtsnd_pcm_msg_send>> notify = virtqueue_kick_prepare(vqueue);
+ */
 bool virtqueue_kick_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2220,6 +2653,27 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|311| <<virtio_commit_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/block/virtio_blk.c|384| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/block/virtio_blk.c|445| <<virtio_queue_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|433| <<virtio_gpu_notify>> virtqueue_notify(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|477| <<virtio_gpu_queue_cursor>> virtqueue_notify(vq);
+ *   - drivers/net/virtio_net.c|682| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1461| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1682| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1885| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|995| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|472| <<virtscsi_kick_vq>> virtqueue_notify(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|500| <<virtscsi_add_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2547| <<virtqueue_kick>> return virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|461| <<send_forget_request>> virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|1208| <<virtio_fs_enqueue_req>> virtqueue_notify(vq);
+ *   - sound/virtio/virtio_card.c|45| <<virtsnd_event_send>> virtqueue_notify(vqueue);
+ *   - sound/virtio/virtio_ctl_msg.c|174| <<virtsnd_ctl_msg_send>> virtqueue_notify(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|248| <<virtsnd_pcm_msg_send>> virtqueue_notify(vqueue);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2273,6 +2727,12 @@ EXPORT_SYMBOL_GPL(virtqueue_kick);
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1128| <<receive_mergeable>> buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx);
+ *   - drivers/net/virtio_net.c|1552| <<virtnet_receive>> (buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx))) {
+ *   - drivers/virtio/virtio_ring.c|2384| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -2326,6 +2786,14 @@ EXPORT_SYMBOL_GPL(virtqueue_disable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/firmware/arm_scmi/virtio.c|520| <<virtio_send_message>> msg->poll_idx = virtqueue_enable_cb_prepare(vioch->vqueue);
+ *   - drivers/firmware/arm_scmi/virtio.c|771| <<virtio_poll_done>> msg->poll_idx = virtqueue_enable_cb_prepare(vioch->vqueue);
+ *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+ *   - drivers/net/virtio_net.c|1747| <<virtnet_poll_tx>> opaque = virtqueue_enable_cb_prepare(sq->vq);
+ *   - drivers/virtio/virtio_ring.c|2584| <<virtqueue_enable_cb>> unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);
+ */
 unsigned int virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2371,6 +2839,29 @@ EXPORT_SYMBOL_GPL(virtqueue_poll);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|292| <<virtblk_done>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|47| <<virtcrypto_ctrlq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|94| <<virtcrypto_dataq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/firmware/arm_scmi/virtio.c|293| <<scmi_vio_complete_cb>> if (virtqueue_enable_cb(vqueue)) {
+ *   - drivers/firmware/arm_scmi/virtio.c|769| <<virtio_poll_done>> pending = !virtqueue_enable_cb(vioch->vqueue);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|212| <<virtio_gpu_dequeue_ctrl_func>> } while (!virtqueue_enable_cb(vgdev->ctrlq.vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|260| <<virtio_gpu_dequeue_cursor_func>> } while (!virtqueue_enable_cb(vgdev->cursorq.vq));
+ *   - drivers/net/caif/caif_virtio.c|565| <<cfv_netdev_tx>> virtqueue_enable_cb(cfv->vq_tx);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|486| <<rpmsg_upref_sleepers>> virtqueue_enable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|187| <<virtscsi_vq_done>> } while (!virtqueue_enable_cb(vq));
+ *   - fs/fuse/virtio_fs.c|348| <<virtio_fs_hiprio_done_work>> } while (!virtqueue_enable_cb(vq) && likely(!virtqueue_is_broken(vq)));
+ *   - fs/fuse/virtio_fs.c|630| <<virtio_fs_requests_done_work>> } while (!virtqueue_enable_cb(vq) && likely(!virtqueue_is_broken(vq)));
+ *   - net/vmw_vsock/virtio_transport.c|310| <<virtio_transport_tx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|418| <<virtio_transport_event_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|561| <<virtio_transport_rx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - sound/virtio/virtio_card.c|96| <<virtsnd_event_notify_cb>> } while (!virtqueue_enable_cb(vqueue));
+ *   - sound/virtio/virtio_card.c|166| <<virtsnd_enable_event_vq>> if (!virtqueue_enable_cb(queue->vqueue))
+ *   - sound/virtio/virtio_ctl_msg.c|308| <<virtsnd_ctl_notify_cb>> } while (!virtqueue_enable_cb(vqueue));
+ *   - sound/virtio/virtio_pcm_msg.c|350| <<virtsnd_pcm_notify_cb>> } while (!virtqueue_enable_cb(queue->vqueue));
+ *   - tools/virtio/virtio_test.c|266| <<run_test>> if (virtqueue_enable_cb(vq->vq))
+ */
 bool virtqueue_enable_cb(struct virtqueue *_vq)
 {
 	unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);
@@ -2392,6 +2883,15 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1651| <<virtnet_poll_cleantx>> } while (unlikely(!virtqueue_enable_cb_delayed(sq->vq)));
+ *   - drivers/net/virtio_net.c|1835| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq)));
+ *   - drivers/net/virtio_net.c|1874| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {
+ *   - tools/virtio/virtio_test.c|263| <<run_test>> if (virtqueue_enable_cb_delayed(vq->vq))
+ *   - tools/virtio/vringh_test.c|388| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ *   - tools/virtio/vringh_test.c|422| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ */
 bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2453,6 +2953,16 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 #endif
 	}
 
+	/*
+	 * 在以下使用vring_virtqueue->event_triggered:
+	 *   - drivers/virtio/virtio_ring.c|500| <<virtqueue_init>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2465| <<virtqueue_disable_cb>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2491| <<virtqueue_enable_cb_prepare>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2492| <<virtqueue_enable_cb_prepare>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2557| <<virtqueue_enable_cb_delayed>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2558| <<virtqueue_enable_cb_delayed>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2616| <<vring_interrupt>> vq->event_triggered = true;
+	 */
 	/* Just a hint for performance: so it's ok that this can be racy! */
 	if (vq->event)
 		vq->event_triggered = true;
@@ -2466,6 +2976,11 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 EXPORT_SYMBOL_GPL(vring_interrupt);
 
 /* Only available for split ring */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1267| <<vring_create_virtqueue_split>> vq = __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,
+ *   - drivers/virtio/virtio_ring.c|2852| <<vring_new_virtqueue>> return __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,
+ */
 static struct virtqueue *__vring_new_virtqueue(unsigned int index,
 					       struct vring_virtqueue_split *vring_split,
 					       struct virtio_device *vdev,
@@ -2525,6 +3040,15 @@ static struct virtqueue *__vring_new_virtqueue(unsigned int index,
 	return &vq->vq;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|954| <<vu_setup_vq>> vq = vring_create_virtqueue(index, num, PAGE_SIZE, vdev, true, true,
+ *   - drivers/s390/virtio/virtio_ccw.c|525| <<virtio_ccw_setup_vq>> vq = vring_create_virtqueue(i, info->num, KVM_VIRTIO_CCW_RING_ALIGN,
+ *   - drivers/virtio/virtio_mmio.c|399| <<vm_setup_vq>> vq = vring_create_virtqueue(index, num, VIRTIO_MMIO_VRING_ALIGN, vdev,
+ *   - drivers/virtio/virtio_pci_legacy.c|131| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_pci_modern.c|334| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_vdpa.c|178| <<virtio_vdpa_setup_vq>> vq = vring_create_virtqueue(index, max_num, align, vdev,
+ */
 struct virtqueue *vring_create_virtqueue(
 	unsigned int index,
 	unsigned int num,
@@ -2573,6 +3097,11 @@ EXPORT_SYMBOL_GPL(vring_create_virtqueue);
  * -EPERM: Operation not permitted
  *
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1906| <<virtnet_rx_resize>> err = virtqueue_resize(rq->vq, ring_num, virtnet_rq_free_unused_buf);
+ *   - drivers/net/virtio_net.c|1945| <<virtnet_tx_resize>> err = virtqueue_resize(sq->vq, ring_num, virtnet_sq_free_unused_buf);
+ */
 int virtqueue_resize(struct virtqueue *_vq, u32 num,
 		     void (*recycle)(struct virtqueue *vq, void *buf))
 {
@@ -2641,6 +3170,12 @@ struct virtqueue *vring_new_virtqueue(unsigned int index,
 }
 EXPORT_SYMBOL_GPL(vring_new_virtqueue);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1300| <<virtqueue_resize_split>> vring_free(&vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2256| <<virtqueue_resize_packed>> vring_free(&vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2901| <<vring_del_virtqueue>> vring_free(_vq);
+ */
 static void vring_free(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2692,6 +3227,24 @@ void vring_del_virtqueue(struct virtqueue *_vq)
 EXPORT_SYMBOL_GPL(vring_del_virtqueue);
 
 /* Manipulates transport-specific feature bits. */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1077| <<vu_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|248| <<rproc_virtio_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|821| <<virtio_ccw_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_mmio.c|128| <<vm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_legacy.c|36| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_modern.c|49| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_vdpa.c|320| <<virtio_vdpa_finalize_features>> vring_transport_features(vdev);
+ *
+ * 清空virtio_device->features中在VIRTIO_TRANSPORT_F_START->VIRTIO_TRANSPORT_F_END并且不在以下的bit
+ * - VIRTIO_RING_F_INDIRECT_DESC
+ * - VIRTIO_RING_F_EVENT_IDX
+ * - VIRTIO_F_VERSION_1:
+ * - VIRTIO_F_ACCESS_PLATFORM:
+ * - VIRTIO_F_RING_PACKED:
+ * - VIRTIO_F_ORDER_PLATFORM:
+ */
 void vring_transport_features(struct virtio_device *vdev)
 {
 	unsigned int i;
@@ -2749,6 +3302,10 @@ EXPORT_SYMBOL_GPL(__virtqueue_break);
 /*
  * This function should only be called by the core, not directly by the driver.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|277| <<vp_modern_enable_vq_after_reset>> __virtqueue_unbreak(vq);
+ */
 void __virtqueue_unbreak(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2770,6 +3327,17 @@ EXPORT_SYMBOL_GPL(virtqueue_is_broken);
  * This should prevent the device from being used, allowing drivers to
  * recover.  You may need to grab appropriate locks to flush.
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|171| <<vhost_user_check_reset>> virtio_break_device(&vu_dev->vdev);
+ *   - drivers/char/virtio_console.c|1961| <<virtcons_remove>> virtio_break_device(vdev);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|269| <<virtcrypto_update_status>> virtio_break_device(vcrypto->vdev);
+ *   - drivers/firmware/arm_scmi/virtio.c|164| <<scmi_vio_channel_cleanup_sync>> virtio_break_device(vioch->vqueue->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1233| <<virtio_ccw_remove>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1250| <<virtio_ccw_offline>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio.c|229| <<virtio_reset_device>> virtio_break_device(dev);
+ *   - drivers/virtio/virtio_pci_common.c|591| <<virtio_pci_remove>> virtio_break_device(&vp_dev->vdev);
+ */
 void virtio_break_device(struct virtio_device *dev)
 {
 	struct virtqueue *_vq;
@@ -2813,6 +3381,9 @@ dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * num个sizeof(struct vring_packed_desc)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.ring_dma_addr;
 
@@ -2820,12 +3391,23 @@ dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_desc_addr);
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|989| <<vu_setup_vq>> virtqueue_get_avail_addr(vq),
+ *   - drivers/s390/virtio/virtio_ccw.c|553| <<virtio_ccw_setup_vq>> info->info_block->s.avail = (__u64)virtqueue_get_avail_addr(vq);
+ *   - drivers/virtio/virtio_mmio.c|436| <<vm_setup_vq>> addr = virtqueue_get_avail_addr(vq);
+ *   - drivers/virtio/virtio_pci_modern.c|193| <<vp_active_vq>> virtqueue_get_avail_addr(vq),
+ *   - drivers/virtio/virtio_vdpa.c|195| <<virtio_vdpa_setup_vq>> driver_addr = virtqueue_get_avail_addr(vq);
+ */
 dma_addr_t virtqueue_get_avail_addr(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * 分配的时候只有sizeof(struct vring_packed_desc_event)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.driver_event_dma_addr;
 
@@ -2840,6 +3422,9 @@ dma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * 分配的时候只有sizeof(struct vring_packed_desc_event)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.device_event_dma_addr;
 
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4519d368..eab1ab16f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -745,6 +745,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|876| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|955| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1176| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1337| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1347| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1353| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1367| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index f8c20d3de..7bd9cd285 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -64,6 +64,13 @@
 #define VRING_PACKED_EVENT_FLAG_ENABLE	0x0
 /* Disable events in packed ring. */
 #define VRING_PACKED_EVENT_FLAG_DISABLE	0x1
+/*
+ * 在以下使用VRING_PACKED_EVENT_FLAG_DESC:
+ *   - drivers/virtio/virtio_ring.c|1822| <<virtqueue_kick_prepare_packed>> if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+ *   - drivers/virtio/virtio_ring.c|1997| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+ *   - drivers/virtio/virtio_ring.c|2042| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+ *   - drivers/virtio/virtio_ring.c|2101| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+ */
 /*
  * Enable events for a specific descriptor in packed ring.
  * (as specified by Descriptor Ring Change Event Offset/Wrap Counter).
@@ -75,6 +82,18 @@
  * Wrap counter bit shift in event suppression structure
  * of packed ring.
  */
+/*
+ * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+ *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ */
 #define VRING_PACKED_EVENT_F_WRAP_CTR	15
 
 /* We support indirect buffer descriptors */
@@ -219,6 +238,15 @@ static inline unsigned vring_size(unsigned int num, unsigned long align)
 /* Assuming a given event_idx value from the other side, if
  * we have just incremented index from old to new_idx,
  * should we trigger an event? */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2475| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+ *   - drivers/vhost/vringh.c|525| <<__vringh_need_notify>> notify = vring_need_event(used_event,
+ *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_kick_prepare_split>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,
+ *   - drivers/virtio/virtio_ring.c|1798| <<virtqueue_kick_prepare_packed>> needs_kick = vring_need_event(event_idx, new, old);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 static inline int vring_need_event(__u16 event_idx, __u16 new_idx, __u16 old)
 {
 	/* Note: Xen has similar logic for notification hold-off
diff --git a/kernel/cpu.c b/kernel/cpu.c
index bbad5e375..3ef3da5e3 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1123,6 +1123,17 @@ static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 }
 
 /* Requires cpu_add_remove_lock to be held */
+/*
+ * [0] _cpu_down
+ * [0] cpu_device_down
+ * [0] device_offline
+ * [0] online_store
+ * [0] kernfs_fop_write_iter
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
 			   enum cpuhp_state target)
 {
diff --git a/kernel/events/core.c b/kernel/events/core.c
index ff4bffc50..0da2e0e94 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -2612,6 +2612,14 @@ static int group_can_go_on(struct perf_event *event,
 	return can_add_hw;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2768| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2771| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2825| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2895| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|13060| <<inherit_event>> add_event_to_ctx(child_event, child_ctx);
+ */
 static void add_event_to_ctx(struct perf_event *event,
 			       struct perf_event_context *ctx)
 {
@@ -12525,6 +12533,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+ *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+ *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+ *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+ *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/security/integrity/ima/ima_efi.c b/security/integrity/ima/ima_efi.c
index 9db66fe31..205bd10ba 100644
--- a/security/integrity/ima/ima_efi.c
+++ b/security/integrity/ima/ima_efi.c
@@ -11,8 +11,20 @@
 #define arch_ima_efi_boot_mode efi_secureboot_mode_unset
 #endif
 
+/*
+ * called by:
+ *   - security/integrity/ima/ima_efi.c|42| <<arch_ima_get_secureboot>> sb_mode = get_sb_mode();
+ */
 static enum efi_secureboot_mode get_sb_mode(void)
 {
+	/*
+	 * enum efi_secureboot_mode {
+	 *     efi_secureboot_mode_unset,
+	 *     efi_secureboot_mode_unknown,
+	 *     efi_secureboot_mode_disabled,
+	 *     efi_secureboot_mode_enabled,
+	 * };
+	 */
 	enum efi_secureboot_mode mode;
 
 	if (!efi_rt_services_supported(EFI_RT_SUPPORTED_GET_VARIABLE)) {
@@ -30,6 +42,13 @@ static enum efi_secureboot_mode get_sb_mode(void)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - security/integrity/ima/ima_appraise.c|29| <<ima_appraise_parse_cmdline>> bool sb_state = arch_ima_get_secureboot();
+ *   - security/integrity/ima/ima_efi.c|67| <<arch_get_ima_policy>> if (IS_ENABLED(CONFIG_IMA_ARCH_POLICY) && arch_ima_get_secureboot()) {
+ *   - security/integrity/ima/ima_main.c|803| <<ima_load_data>> && arch_ima_get_secureboot()) {
+ *   - security/integrity/platform_certs/load_uefi.c|214| <<load_uefi_certs>> if (!arch_ima_get_secureboot())
+ */
 bool arch_ima_get_secureboot(void)
 {
 	static enum efi_secureboot_mode sb_mode;
diff --git a/security/integrity/ima/ima_main.c b/security/integrity/ima/ima_main.c
index 040b03ddc..389d11546 100644
--- a/security/integrity/ima/ima_main.c
+++ b/security/integrity/ima/ima_main.c
@@ -790,6 +790,10 @@ int ima_post_read_file(struct file *file, void *buf, loff_t size,
  *
  * For permission return 0, otherwise return -EACCES.
  */
+/*
+ * called by:
+ *   - security/security.c|1777| <<security_kernel_load_data>> return ima_load_data(id, contents);
+ */
 int ima_load_data(enum kernel_load_data_id id, bool contents)
 {
 	bool ima_enforce, sig_enforce;
diff --git a/tools/perf/builtin-stat.c b/tools/perf/builtin-stat.c
index 0b4a62e4f..51e5469d1 100644
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@ -760,6 +760,13 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 		 * errored is a sticky flag that means one of the counter's
 		 * cpu event had a problem and needs to be reexamined.
 		 */
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		counter->errored = true;
 
 		if ((evsel__leader(counter) != counter) ||
@@ -826,6 +833,16 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	}
 
 	evlist__for_each_entry(evsel_list, counter) {
+		/*
+		 * 在以下使用evsel->reset_group:
+		 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+		 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true;
+		 */
 		counter->reset_group = false;
 		if (bpf_counter__load(counter, &target))
 			return -1;
@@ -843,6 +860,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		if (target.use_bpf)
 			break;
 
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		if (counter->reset_group || counter->errored)
 			continue;
 		if (evsel__is_bpf(counter))
@@ -892,6 +916,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 
@@ -901,6 +932,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 			if (!counter->reset_group)
diff --git a/tools/perf/util/evsel.h b/tools/perf/util/evsel.h
index d927713b5..f100fed98 100644
--- a/tools/perf/util/evsel.h
+++ b/tools/perf/util/evsel.h
@@ -118,6 +118,27 @@ struct evsel {
 	void			*priv;
 	u64			db_id;
 	bool			uniquified_name;
+	/*
+	 * 在以下使用counter->supported:
+	 *   - tools/perf/builtin-record.c|1269| <<record__open>> pos->supported = true;
+	 *   - tools/perf/builtin-stat.c|382| <<read_counter_cpu>> if (!counter->supported)
+	 *   - tools/perf/builtin-stat.c|758| <<stat_handle_error>> counter->supported = false;
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|962| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|968| <<__run_perf_stat>> if (!counter->supported) {
+	 *   - tools/perf/util/bpf_counter_cgroup.c|139| <<bperf_load_program>> evsel->supported = true;
+	 *   - tools/perf/util/mem-events.c|136| <<perf_mem_events__init>> e->supported = perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|141| <<perf_mem_events__init>> e->supported |= perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|145| <<perf_mem_events__init>> if (e->supported)
+	 *   - tools/perf/util/mem-events.c|163| <<perf_mem_events__list>> e->supported ? ": available" : "");
+	 *   - tools/perf/util/mem-events.c|198| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/mem-events.c|207| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/stat-display.c|587| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED);
+	 *   - tools/perf/util/stat-display.c|591| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED,
+	 *   - tools/perf/util/stat-display.c|595| <<printout>> if (counter->supported) {
+	 *   - tools/perf/util/stat.c|502| <<perf_event__process_stat_event>> counter->supported = true;
+	 *   - tools/perf/util/synthetic-events.c|2066| <<perf_event__synthesize_extra_attr>> if (!evsel->supported)
+	 */
 	bool 			supported;
 	bool 			needs_swap;
 	bool 			disabled;
@@ -128,7 +149,24 @@ struct evsel {
 	bool			forced_leader;
 	bool			cmdline_group_boundary;
 	bool			merged_stat;
+	/*
+	 * 在以下使用evsel->reset_group:
+	 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+	 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true; 
+	 */
 	bool			reset_group;
+	/*
+	 * 在以下使用evsel->errored:
+	 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 */
 	bool			errored;
 	bool			needs_auxtrace_mmap;
 	struct hashmap		*per_pkg_mask;
diff --git a/tools/perf/util/stat-display.c b/tools/perf/util/stat-display.c
index b82844cb0..fa66885c7 100644
--- a/tools/perf/util/stat-display.c
+++ b/tools/perf/util/stat-display.c
@@ -531,6 +531,15 @@ static bool is_mixed_hw_group(struct evsel *counter)
 	return false;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|862| <<print_counter_aggrdata>> printout(config, id, nr, counter, uval,
+ *   - tools/perf/util/stat-display.c|983| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|987| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|1031| <<print_counter_aggr>> printout(config, aggr_cpu_id__empty(), 0, counter, uval, prefix, cd.avg_running,
+ *   - tools/perf/util/stat-display.c|1076| <<print_counter>> printout(config, id, 0, counter, uval, prefix,
+ *   - tools/perf/util/stat-display.c|1115| <<print_no_aggr_metric>> printout(config, id, 0, counter, uval, prefix,
+ */
 static void printout(struct perf_stat_config *config, struct aggr_cpu_id id, int nr,
 		     struct evsel *counter, double uval,
 		     char *prefix, u64 run, u64 ena, double noise,
@@ -1049,6 +1058,10 @@ static void counter_cb(struct perf_stat_config *config __maybe_unused,
  * Print out the results of a single counter:
  * does not use aggregated count in system-wide
  */
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1519| <<evlist__print_counters>> print_counter(config, counter, prefix);
+ */
 static void print_counter(struct perf_stat_config *config,
 			  struct evsel *counter, char *prefix)
 {
@@ -1268,6 +1281,10 @@ static void print_interval(struct perf_stat_config *config,
 		num_print_interval = 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1471| <<evlist__print_counters>> print_header(config, _target, argc, argv);
+ */
 static void print_header(struct perf_stat_config *config,
 			 struct target *_target,
 			 int argc, const char **argv)
@@ -1445,6 +1462,10 @@ static void print_percore(struct perf_stat_config *config,
 		fputc('\n', output);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|1117| <<print_counters>> evlist__print_counters(evsel_list, &stat_config, &target, ts, argc, argv);
+ */
 void evlist__print_counters(struct evlist *evlist, struct perf_stat_config *config,
 			    struct target *_target, struct timespec *ts, int argc, const char **argv)
 {
-- 
2.34.1

