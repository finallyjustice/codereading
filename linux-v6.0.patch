From e032fc2c81787a545db2641bfc32ced5c4081c2b Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 17 Oct 2022 00:36:56 -0700
Subject: [PATCH 1/1] linux v6.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/events/core.c            |  9 +++
 arch/x86/events/perf_event.h      | 10 ++++
 arch/x86/include/asm/nmi.h        | 45 +++++++++++++++
 arch/x86/include/asm/perf_event.h | 23 ++++++++
 arch/x86/kernel/nmi.c             | 95 ++++++++++++++++++++++++++++++
 arch/x86/kernel/nmi_selftest.c    | 32 +++++++++++
 arch/x86/kvm/cpuid.c              |  9 +++
 arch/x86/kvm/cpuid.h              | 19 ++++++
 arch/x86/kvm/lapic.c              |  7 +++
 arch/x86/kvm/pmu.c                | 23 ++++++++
 arch/x86/kvm/vmx/vmx.c            |  4 ++
 arch/x86/kvm/x86.c                |  6 ++
 drivers/virtio/virtio_ring.c      | 96 +++++++++++++++++++++++++++++++
 include/linux/kvm_host.h          | 11 ++++
 include/uapi/linux/virtio_ring.h  | 12 ++++
 tools/perf/builtin-stat.c         | 38 ++++++++++++
 tools/perf/util/evsel.h           | 38 ++++++++++++
 tools/perf/util/stat-display.c    |  9 +++
 18 files changed, 486 insertions(+)

diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index f969410d0..9deacfa44 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -247,6 +247,11 @@ static void release_pmc_hardware(void) {}
 
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2113| <<init_hw_perf_events>> if (!check_hw_exists(&pmu, x86_pmu.num_counters, x86_pmu.num_counters_fixed))
+ *   - arch/x86/events/intel/core.c|4504| <<init_hybrid_pmu>> if (!check_hw_exists(&pmu->pmu, pmu->num_counters, pmu->num_counters_fixed))
+ */
 bool check_hw_exists(struct pmu *pmu, int num_counters, int num_counters_fixed)
 {
 	u64 val, val_fail = -1, val_new= ~0;
@@ -2076,6 +2081,10 @@ void x86_pmu_update_cpu_context(struct pmu *pmu, int cpu)
 	cpuctx->ctx.pmu = pmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2227| <<global>> early_initcall(init_hw_perf_events); 
+ */
 static int __init init_hw_perf_events(void)
 {
 	struct x86_pmu_quirk *quirk;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 266143abc..3574a0001 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1131,6 +1131,16 @@ static inline bool is_counter_pair(struct hw_perf_event *hwc)
 	return hwc->flags & PERF_X86_EVENT_PAIR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1440| <<x86_pmu_enable_event>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2330| <<intel_pmu_nhm_workaround>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ */
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..84c03635c 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -23,6 +23,29 @@ extern int unknown_nmi_panic;
 
 #define NMI_FLAG_FIRST	1
 
+/*
+ * 注册了NMI_LOCAL的:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|101| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|850| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *
+ * 注册了NMI_UNKNOWN的:
+ *   - rch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 enum {
 	NMI_LOCAL=0,
 	NMI_UNKNOWN,
@@ -44,6 +67,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f6fc8dd51..c781e2ec6 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -28,6 +28,29 @@
 #define ARCH_PERFMON_EVENTSEL_PIN_CONTROL		(1ULL << 19)
 #define ARCH_PERFMON_EVENTSEL_INT			(1ULL << 20)
 #define ARCH_PERFMON_EVENTSEL_ANY			(1ULL << 21)
+/*
+ * 在以下使用ARCH_PERFMON_EVENTSEL_ENABLE:
+ *   - arch/x86/events/amd/core.c|21| <<AMD_MERGE_EVENT_ENABLE>> #define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/amd/uncore.c|109| <<amd_uncore_start>> wrmsrl(hwc->config_base, (hwc->config | ARCH_PERFMON_EVENTSEL_ENABLE));
+ *   - arch/x86/events/core.c|266| <<check_hw_exists>> if (val & ARCH_PERFMON_EVENTSEL_ENABLE) {
+ *   - arch/x86/events/core.c|687| <<x86_pmu_disable_all>> if (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))
+ *   - arch/x86/events/core.c|689| <<x86_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1441| <<x86_pmu_enable_event>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2331| <<intel_pmu_nhm_workaround>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4090| <<core_guest_get_msrs>> event->hw.config | ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4093| <<core_guest_get_msrs>> arr[idx].host &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4095| <<core_guest_get_msrs>> arr[idx].guest &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/knc.c|183| <<knc_pmu_disable_event>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/knc.c|194| <<knc_pmu_enable_event>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|144| <<p6_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|154| <<p6_pmu_enable_all>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/kvm/pmu.h|158| <<pmc_speculative_in_use>> return pmc->eventsel & ARCH_PERFMON_EVENTSEL_ENABLE;
+ */
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index cec0bfa3b..97e14c23c 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -73,6 +73,12 @@ struct nmi_stats {
 
 static DEFINE_PER_CPU(struct nmi_stats, nmi_stats);
 
+/*
+ * 在以下使用ignoe_nmis:
+ *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+ *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+ *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+ */
 static int ignore_nmis __read_mostly;
 
 int unknown_nmi_panic;
@@ -101,6 +107,10 @@ static int __init nmi_warning_debugfs(void)
 }
 fs_initcall(nmi_warning_debugfs);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|145| <<nmi_handle>> nmi_check_duration(a, delta);
+ */
 static void nmi_check_duration(struct nmiaction *action, u64 duration)
 {
 	int remainder_ns, decimal_msecs;
@@ -118,6 +128,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|234| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|261| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|310| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|359| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -152,6 +169,33 @@ static int nmi_handle(unsigned int type, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_handle);
 
+/*
+ * 5.4上看到这两个NMI LOCAL的.
+ *
+ * crash> nmiaction ffffffffa6617ec0
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6641a80,
+ *     prev = 0xffffffffa6634368
+ *   },
+ *   handler = 0xffffffffa4c07aa0, --> perf_event_nmi_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa60b4811 "PMI"
+ * }
+ * crash> nmiaction ffffffffa6641a80
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6634368,
+ *     prev = 0xffffffffa6617ec0
+ *   },
+ *   handler = 0xffffffffa4c7aeb0, --> nmi_cpu_backtrace_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa6041a34 "arch_bt"
+ * }
+ */
+
 int __register_nmi_handler(unsigned int type, struct nmiaction *action)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -213,6 +257,10 @@ void unregister_nmi_handler(unsigned int type, const char *name)
 }
 EXPORT_SYMBOL_GPL(unregister_nmi_handler);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|370| <<default_do_nmi>> pci_serr_error(reason, regs);
+ */
 static void
 pci_serr_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -234,6 +282,10 @@ pci_serr_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(pci_serr_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|372| <<default_do_nmi>> io_check_error(reason, regs);
+ */
 static void
 io_check_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -274,11 +326,25 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(io_check_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|419| <<default_do_nmi>> unknown_nmi_error(reason, regs);
+ */
 static void
 unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 {
 	int handled;
 
+	/*
+	 * 注册了NMI_UNKNOWN的:
+	 *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+	 *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+	 *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+	 *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+	 *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+	 *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+	 */
+
 	/*
 	 * Use 'false' as back-to-back NMIs are dealt with one level up.
 	 * Of course this makes having multiple 'unknown' handlers useless
@@ -304,8 +370,19 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 NOKPROBE_SYMBOL(unknown_nmi_error);
 
 static DEFINE_PER_CPU(bool, swallow_nmi);
+/*
+ * 在以下使用last_nmi_rip:
+ *   - arch/x86/kernel/nmi.c|329| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+ *   - arch/x86/kernel/nmi.c|350| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+ *   - arch/x86/kernel/nmi.c|355| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+ *   - arch/x86/kernel/nmi.c|575| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+ */
 static DEFINE_PER_CPU(unsigned long, last_nmi_rip);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|535| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> default_do_nmi(regs);
+ */
 static noinstr void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;
@@ -472,6 +549,14 @@ enum nmi_states {
 	NMI_EXECUTING,
 	NMI_LATCHED,
 };
+/*
+ * 在以下使用nmi_state:
+ *   - arch/x86/kernel/nmi.c|497| <<global>> static DEFINE_PER_CPU(enum nmi_states, nmi_state);
+ *   - arch/x86/kernel/nmi.c|514| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_read(nmi_state) != NMI_NOT_RUNNING) {
+ *   - arch/x86/kernel/nmi.c|515| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_LATCHED);
+ *   - arch/x86/kernel/nmi.c|518| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_EXECUTING);
+ *   - arch/x86/kernel/nmi.c|545| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_dec_return(nmi_state))
+ */
 static DEFINE_PER_CPU(enum nmi_states, nmi_state);
 static DEFINE_PER_CPU(unsigned long, nmi_cr2);
 static DEFINE_PER_CPU(unsigned long, nmi_dr7);
@@ -509,6 +594,12 @@ DEFINE_IDTENTRY_RAW(exc_nmi)
 
 	inc_irq_stat(__nmi_count);
 
+	/*
+	 * 在以下使用ignoe_nmis:
+	 *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+	 *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+	 *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+	 */
 	if (!ignore_nmis)
 		default_do_nmi(regs);
 
@@ -548,6 +639,10 @@ void restart_nmi(void)
 }
 
 /* reset the back-to-back NMI logic */
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|709| <<arch_cpu_idle_enter>> local_touch_nmi();
+ */
 void local_touch_nmi(void)
 {
 	__this_cpu_write(last_nmi_rip, 0);
diff --git a/arch/x86/kernel/nmi_selftest.c b/arch/x86/kernel/nmi_selftest.c
index a1a96df3d..d1a391bb2 100644
--- a/arch/x86/kernel/nmi_selftest.c
+++ b/arch/x86/kernel/nmi_selftest.c
@@ -23,6 +23,15 @@
 #define FAILURE		1
 #define TIMEOUT		2
 
+/*
+ * 在以下使用nmi_fail:
+ *   - arch/x86/kernel/nmi_selftest.c|71| <<test_nmi_ipi>> nmi_fail = FAILURE;
+ *   - arch/x86/kernel/nmi_selftest.c|89| <<test_nmi_ipi>> nmi_fail = TIMEOUT;
+ *   - arch/x86/kernel/nmi_selftest.c|110| <<reset_nmi>> nmi_fail = 0;
+ *   - arch/x86/kernel/nmi_selftest.c|119| <<dotest>> if (nmi_fail != expected) {
+ *   - arch/x86/kernel/nmi_selftest.c|122| <<dotest>> if (nmi_fail == FAILURE)
+ *   - arch/x86/kernel/nmi_selftest.c|124| <<dotest>> else if (nmi_fail == TIMEOUT)
+ */
 static int __initdata nmi_fail;
 
 /* check to see if NMI IPIs work on this machine */
@@ -62,6 +71,29 @@ static int __init test_nmi_ipi_callback(unsigned int val, struct pt_regs *regs)
         return NMI_DONE;
 }
 
+/*
+ * called by:
+ 49  *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ 50  *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ 51  *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ 52  *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ 53  *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ 54  *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ 55  *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ 56  *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ 57  *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ 58  *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ 59  *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ 60  *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ 61  *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ 62  *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ 63  *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ 64  *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ 65  *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ 66  *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ 67  *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 static void __init test_nmi_ipi(struct cpumask *mask)
 {
 	unsigned long timeout;
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 2796dde06..120a1f8c7 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -311,6 +311,10 @@ void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_update_cpuid_runtime);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|421| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -382,6 +386,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|462| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|488| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index b1658c0de..755545d9f 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,11 +42,30 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|52| <<kvm_vcpu_is_illegal_gpa>> return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|58| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/svm/nested.c|261| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|262| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|805| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|324| <<__nested_vmcb_check_save>> CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))
+ *   - arch/x86/kvm/svm/nested.c|522| <<nested_svm_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))
+ *   - arch/x86/kvm/vmx/nested.c|1113| <<nested_vmx_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3))) {
+ *   - arch/x86/kvm/vmx/nested.c|2695| <<nested_vmx_check_eptp>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/nested.c|2890| <<nested_vmx_check_host_state>> CC(kvm_vcpu_is_illegal_gpa(vcpu, vmcs12->host_cr3)))
+ *   - arch/x86/kvm/vmx/vmx.c|5669| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/x86.c|1239| <<kvm_set_cr3>> if (kvm_vcpu_is_illegal_gpa(vcpu, cr3))
+ *   - arch/x86/kvm/x86.c|11183| <<kvm_is_valid_sregs>> if (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))
+ */
 static inline bool kvm_vcpu_is_illegal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9dda989a1..2db3ed547 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -2524,6 +2524,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1712| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2547| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|432| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|4958| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 02f9e4f24..09df7bf48 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -88,6 +88,10 @@ static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
 	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);
 }
 
+/*
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|485| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
@@ -417,9 +421,17 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|96| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10338| <<vcpu_enter_guest>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
@@ -445,6 +457,12 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3768| <<kvm_set_msr_common(MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3854| <<kvm_set_msr_common(MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3862| <<kvm_set_msr_common(default: kvm_pmu_is_valid_msr)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
@@ -460,6 +478,11 @@ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|522| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11767| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c9b49a09e..2d731bad6 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -2006,6 +2006,10 @@ static u64 nested_vmx_truncate_sysenter_addr(struct kvm_vcpu *vcpu,
 	return (unsigned long)data;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2091| <<vmx_set_msr>> u64 invalid = data & ~vcpu_supported_debugctl(vcpu);
+ */
 static u64 vcpu_supported_debugctl(struct kvm_vcpu *vcpu)
 {
 	u64 debugctl = vmx_supported_debugctl();
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index b0c47b41c..1e0e156dc 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -11699,6 +11699,12 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		static_branch_dec(&kvm_has_noapic_vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3077| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2126| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11633| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 4620e9d79..d578f4d2b 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -32,6 +32,11 @@
 	} while (0)
 #define END_USE(_vq) \
 	do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while(0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|535| <<virtqueue_add_split>> LAST_ADD_TIME_UPDATE(vq);
+ *   - drivers/virtio/virtio_ring.c|1363| <<virtqueue_add_packed>> LAST_ADD_TIME_UPDATE(vq);
+ */
 #define LAST_ADD_TIME_UPDATE(_vq)				\
 	do {							\
 		ktime_t now = ktime_get();			\
@@ -43,6 +48,11 @@
 		(_vq)->last_add_time = now;			\
 		(_vq)->last_add_time_valid = true;		\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|707| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_CHECK(vq);
+ *   - drivers/virtio/virtio_ring.c|1515| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_CHECK(vq);
+ */
 #define LAST_ADD_TIME_CHECK(_vq)				\
 	do {							\
 		if ((_vq)->last_add_time_valid) {		\
@@ -154,6 +164,11 @@ struct vring_virtqueue {
 	struct virtqueue vq;
 
 	/* Is this a packed ring? */
+	/*
+	 * 在以下设置vring_virtqueue->packed_ring:
+	 *   - drivers/virtio/virtio_ring.c|2008| <<bool>> vq->packed_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2488| <<bool>> vq->packed_ring = false;
+	 */
 	bool packed_ring;
 
 	/* Is DMA API used? */
@@ -203,10 +218,32 @@ struct vring_virtqueue {
 
 #ifdef DEBUG
 	/* They're supposed to lock for us. */
+	/*
+	 * 在以下使用vring_virtqueue->in_use:
+	 *   - drivers/virtio/virtio_ring.c|28| <<START_USE>> if ((_vq)->in_use) \
+	 *   - drivers/virtio/virtio_ring.c|30| <<START_USE>> (_vq)->vq.name, (_vq)->in_use); \
+	 *   - drivers/virtio/virtio_ring.c|31| <<START_USE>> (_vq)->in_use = __LINE__; \
+	 *   - drivers/virtio/virtio_ring.c|34| <<END_USE>> do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while (0)
+	 *   - drivers/virtio/virtio_ring.c|401| <<virtqueue_init>> vq->in_use = false;
+	 */
 	unsigned int in_use;
 
 	/* Figure out if their kicks are too delayed. */
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|40| <<LAST_ADD_TIME_UPDATE>> if ((_vq)->last_add_time_valid) \
+	 *   - drivers/virtio/virtio_ring.c|44| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time_valid = true; \
+	 *   - drivers/virtio/virtio_ring.c|48| <<LAST_ADD_TIME_CHECK>> if ((_vq)->last_add_time_valid) { \
+	 *   - drivers/virtio/virtio_ring.c|54| <<LAST_ADD_TIME_INVALID>> ((_vq)->last_add_time_valid = false)
+	 *   - drivers/virtio/virtio_ring.c|402| <<virtqueue_init>> vq->last_add_time_valid = false;
+	 */
 	bool last_add_time_valid;
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time:
+	 *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+	 *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+	 *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+	 */
 	ktime_t last_add_time;
 #endif
 };
@@ -295,6 +332,14 @@ size_t virtio_max_dma_size(struct virtio_device *vdev)
 }
 EXPORT_SYMBOL_GPL(virtio_max_dma_size);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1054| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1068| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1868| <<vring_alloc_queue_packed>> ring = vring_alloc_queue(vdev, ring_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1880| <<vring_alloc_queue_packed>> driver = vring_alloc_queue(vdev, event_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1890| <<vring_alloc_queue_packed>> device = vring_alloc_queue(vdev, event_size_in_bytes,
+ */
 static void *vring_alloc_queue(struct virtio_device *vdev, size_t size,
 			      dma_addr_t *dma_handle, gfp_t flag)
 {
@@ -342,6 +387,18 @@ static void vring_free_queue(struct virtio_device *vdev, size_t size,
  * making all of the arch DMA ops work on the vring device itself
  * is a mess.  For now, we use the parent device for DMA ops.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|363| <<vring_map_one_sg>> return dma_map_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|375| <<vring_map_single>> return dma_map_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|385| <<vring_mapping_error>> return dma_mapping_error(vring_dma_dev(vq), addr);
+ *   - drivers/virtio/virtio_ring.c|421| <<vring_unmap_one_split_indirect>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|440| <<vring_unmap_one_split>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|446| <<vring_unmap_one_split>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1176| <<vring_unmap_extra_packed>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1181| <<vring_unmap_extra_packed>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1198| <<vring_unmap_desc_packed>> dma_unmap_page(vring_dma_dev(vq),
+ */
 static inline struct device *vring_dma_dev(const struct vring_virtqueue *vq)
 {
 	return vq->vq.vdev->dev.parent;
@@ -875,6 +932,10 @@ static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 	return last_used_idx;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2359| <<virtqueue_poll>> virtqueue_poll_split(_vq, last_used_idx);
+ */
 static bool virtqueue_poll_split(struct virtqueue *_vq, unsigned int last_used_idx)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1115,6 +1176,10 @@ static struct virtqueue *vring_create_virtqueue_split(
 	return vq;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2612| <<virtqueue_resize>> err = virtqueue_resize_split(_vq, num);
+ */
 static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 {
 	struct vring_virtqueue_split vring_split = {};
@@ -1154,6 +1219,18 @@ static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
  */
 static inline bool packed_used_wrap_counter(u16 last_used_idx)
 {
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+	 *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 */
 	return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
@@ -1162,6 +1239,11 @@ static inline u16 packed_last_used(u16 last_used_idx)
 	return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_packed>> vring_unmap_extra_packed(vq, &vq->packed.desc_extra[curr]);
+ *   - drivers/virtio/virtio_ring.c|1555| <<detach_buf_packed>> vring_unmap_extra_packed(vq,
+ */
 static void vring_unmap_extra_packed(const struct vring_virtqueue *vq,
 				     struct vring_desc_extra *extra)
 {
@@ -1333,6 +1415,10 @@ static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2168| <<virtqueue_add>> return vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_packed(struct virtqueue *_vq,
 				       struct scatterlist *sgs[],
 				       unsigned int total_sg,
@@ -1380,6 +1466,16 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 
 	WARN_ON_ONCE(total_sg > vq->packed.vring.num && !vq->indirect);
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *        struct {
+	 *            unsigned int num;
+	 *            struct vring_packed_desc *desc;
+	 *            struct vring_packed_desc_event *driver;
+	 *            struct vring_packed_desc_event *device;
+	 *        } vring;
+	 */
 	desc = vq->packed.vring.desc;
 	i = head;
 	descs_used = total_sg;
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4519d368..eab1ab16f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -745,6 +745,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|876| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|955| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1176| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1337| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1347| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1353| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1367| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index f8c20d3de..e936c84d5 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -75,6 +75,18 @@
  * Wrap counter bit shift in event suppression structure
  * of packed ring.
  */
+/*
+ * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+ *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ */
 #define VRING_PACKED_EVENT_F_WRAP_CTR	15
 
 /* We support indirect buffer descriptors */
diff --git a/tools/perf/builtin-stat.c b/tools/perf/builtin-stat.c
index 0b4a62e4f..51e5469d1 100644
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@ -760,6 +760,13 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 		 * errored is a sticky flag that means one of the counter's
 		 * cpu event had a problem and needs to be reexamined.
 		 */
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		counter->errored = true;
 
 		if ((evsel__leader(counter) != counter) ||
@@ -826,6 +833,16 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	}
 
 	evlist__for_each_entry(evsel_list, counter) {
+		/*
+		 * 在以下使用evsel->reset_group:
+		 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+		 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true;
+		 */
 		counter->reset_group = false;
 		if (bpf_counter__load(counter, &target))
 			return -1;
@@ -843,6 +860,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		if (target.use_bpf)
 			break;
 
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		if (counter->reset_group || counter->errored)
 			continue;
 		if (evsel__is_bpf(counter))
@@ -892,6 +916,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 
@@ -901,6 +932,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 			if (!counter->reset_group)
diff --git a/tools/perf/util/evsel.h b/tools/perf/util/evsel.h
index d927713b5..f100fed98 100644
--- a/tools/perf/util/evsel.h
+++ b/tools/perf/util/evsel.h
@@ -118,6 +118,27 @@ struct evsel {
 	void			*priv;
 	u64			db_id;
 	bool			uniquified_name;
+	/*
+	 * 在以下使用counter->supported:
+	 *   - tools/perf/builtin-record.c|1269| <<record__open>> pos->supported = true;
+	 *   - tools/perf/builtin-stat.c|382| <<read_counter_cpu>> if (!counter->supported)
+	 *   - tools/perf/builtin-stat.c|758| <<stat_handle_error>> counter->supported = false;
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|962| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|968| <<__run_perf_stat>> if (!counter->supported) {
+	 *   - tools/perf/util/bpf_counter_cgroup.c|139| <<bperf_load_program>> evsel->supported = true;
+	 *   - tools/perf/util/mem-events.c|136| <<perf_mem_events__init>> e->supported = perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|141| <<perf_mem_events__init>> e->supported |= perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|145| <<perf_mem_events__init>> if (e->supported)
+	 *   - tools/perf/util/mem-events.c|163| <<perf_mem_events__list>> e->supported ? ": available" : "");
+	 *   - tools/perf/util/mem-events.c|198| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/mem-events.c|207| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/stat-display.c|587| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED);
+	 *   - tools/perf/util/stat-display.c|591| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED,
+	 *   - tools/perf/util/stat-display.c|595| <<printout>> if (counter->supported) {
+	 *   - tools/perf/util/stat.c|502| <<perf_event__process_stat_event>> counter->supported = true;
+	 *   - tools/perf/util/synthetic-events.c|2066| <<perf_event__synthesize_extra_attr>> if (!evsel->supported)
+	 */
 	bool 			supported;
 	bool 			needs_swap;
 	bool 			disabled;
@@ -128,7 +149,24 @@ struct evsel {
 	bool			forced_leader;
 	bool			cmdline_group_boundary;
 	bool			merged_stat;
+	/*
+	 * 在以下使用evsel->reset_group:
+	 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+	 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true; 
+	 */
 	bool			reset_group;
+	/*
+	 * 在以下使用evsel->errored:
+	 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 */
 	bool			errored;
 	bool			needs_auxtrace_mmap;
 	struct hashmap		*per_pkg_mask;
diff --git a/tools/perf/util/stat-display.c b/tools/perf/util/stat-display.c
index b82844cb0..018b3d68c 100644
--- a/tools/perf/util/stat-display.c
+++ b/tools/perf/util/stat-display.c
@@ -531,6 +531,15 @@ static bool is_mixed_hw_group(struct evsel *counter)
 	return false;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|862| <<print_counter_aggrdata>> printout(config, id, nr, counter, uval,
+ *   - tools/perf/util/stat-display.c|983| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|987| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|1031| <<print_counter_aggr>> printout(config, aggr_cpu_id__empty(), 0, counter, uval, prefix, cd.avg_running,
+ *   - tools/perf/util/stat-display.c|1076| <<print_counter>> printout(config, id, 0, counter, uval, prefix,
+ *   - tools/perf/util/stat-display.c|1115| <<print_no_aggr_metric>> printout(config, id, 0, counter, uval, prefix,
+ */
 static void printout(struct perf_stat_config *config, struct aggr_cpu_id id, int nr,
 		     struct evsel *counter, double uval,
 		     char *prefix, u64 run, u64 ena, double noise,
-- 
2.34.1

