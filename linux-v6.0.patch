From dc6409fe90814be67bb91fad8c30271ee2116a76 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 27 Feb 2023 08:28:19 -0800
Subject: [PATCH 1/1] linux v6.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/events/amd/core.c             |  36 ++
 arch/x86/events/amd/ibs.c              |   5 +
 arch/x86/events/core.c                 |  38 ++
 arch/x86/events/perf_event.h           |  10 +
 arch/x86/include/asm/kvm_host.h        | 156 +++++++
 arch/x86/include/asm/nmi.h             |  45 ++
 arch/x86/include/asm/nospec-branch.h   |   7 +
 arch/x86/include/asm/perf_event.h      |  23 +
 arch/x86/include/asm/spec-ctrl.h       |   8 +
 arch/x86/kernel/acpi/boot.c            |  32 ++
 arch/x86/kernel/check.c                |  15 +
 arch/x86/kernel/cpu/bugs.c             |  12 +
 arch/x86/kernel/cpu/common.c           |  29 ++
 arch/x86/kernel/nmi.c                  |  95 ++++
 arch/x86/kernel/nmi_selftest.c         |  32 ++
 arch/x86/kernel/sev.c                  |  18 +
 arch/x86/kernel/smpboot.c              |   8 +
 arch/x86/kvm/cpuid.c                   |  56 +++
 arch/x86/kvm/cpuid.h                   |  74 +++
 arch/x86/kvm/kvm_cache_regs.h          |   7 +
 arch/x86/kvm/lapic.c                   |   7 +
 arch/x86/kvm/mmu.h                     |   4 +
 arch/x86/kvm/mmu/mmu.c                 |  68 +++
 arch/x86/kvm/mmu/page_track.c          |  14 +
 arch/x86/kvm/mmu/tdp_mmu.c             |  22 +
 arch/x86/kvm/pmu.c                     | 236 ++++++++++
 arch/x86/kvm/pmu.h                     |  91 ++++
 arch/x86/kvm/reverse_cpuid.h           |  12 +
 arch/x86/kvm/svm/pmu.c                 | 128 ++++++
 arch/x86/kvm/svm/svm.c                 |  17 +
 arch/x86/kvm/vmx/capabilities.h        |  44 ++
 arch/x86/kvm/vmx/pmu_intel.c           | 107 +++++
 arch/x86/kvm/vmx/vmx.c                 |   9 +
 arch/x86/kvm/x86.c                     | 139 ++++++
 arch/x86/mm/extable.c                  |   7 +
 block/blk-mq.c                         |   8 +
 drivers/acpi/cppc_acpi.c               |  42 ++
 drivers/acpi/utils.c                   |  17 +
 drivers/block/ublk_drv.c               |   8 +
 drivers/block/virtio_blk.c             |   5 +
 drivers/net/virtio_net.c               |  27 ++
 drivers/net/xen-netfront.c             |   7 +
 drivers/pci/iov.c                      |   7 +
 drivers/scsi/virtio_scsi.c             |  10 +
 drivers/target/target_core_iblock.c    |  11 +
 drivers/target/target_core_transport.c |  38 ++
 drivers/vhost/scsi.c                   | 198 ++++++++
 drivers/vhost/vhost.c                  | 133 ++++++
 drivers/vhost/vhost.h                  |  26 ++
 drivers/virtio/virtio.c                |  43 ++
 drivers/virtio/virtio_balloon.c        |   7 +
 drivers/virtio/virtio_pci_common.c     |  40 ++
 drivers/virtio/virtio_pci_modern.c     |  13 +
 drivers/virtio/virtio_pci_modern_dev.c |   7 +
 drivers/virtio/virtio_ring.c           | 601 +++++++++++++++++++++++++
 include/linux/kvm_host.h               |  11 +
 include/linux/perf_event.h             |  12 +
 include/uapi/linux/perf_event.h        |   7 +
 include/uapi/linux/virtio_ring.h       |  28 ++
 kernel/cpu.c                           |  11 +
 kernel/events/core.c                   | 243 ++++++++++
 kernel/events/internal.h               |  31 ++
 kernel/events/ring_buffer.c            |  71 +++
 mm/huge_memory.c                       |  14 +
 mm/memblock.c                          |   9 +
 mm/memory.c                            |   8 +
 mm/page_alloc.c                        |   6 +
 net/core/sock.c                        |  24 +
 net/ethernet/eth.c                     |  18 +
 security/integrity/ima/ima_efi.c       |  19 +
 security/integrity/ima/ima_main.c      |   4 +
 tools/include/linux/ring_buffer.h      |   7 +
 tools/include/uapi/linux/perf_event.h  |   7 +
 tools/lib/perf/evlist.c                |   6 +
 tools/lib/perf/evsel.c                 |  36 ++
 tools/lib/perf/mmap.c                  |  42 ++
 tools/perf/builtin-record.c            | 465 +++++++++++++++++++
 tools/perf/builtin-report.c            |  27 ++
 tools/perf/builtin-stat.c              |  67 +++
 tools/perf/util/counts.c               |  15 +
 tools/perf/util/data.c                 |  29 ++
 tools/perf/util/data.h                 |  37 ++
 tools/perf/util/evlist.c               |  52 +++
 tools/perf/util/evsel.c                |  69 +++
 tools/perf/util/evsel.h                |  43 ++
 tools/perf/util/hist.c                 |  42 ++
 tools/perf/util/mmap.c                 |  93 ++++
 tools/perf/util/session.c              |  19 +
 tools/perf/util/stat-display.c         |  21 +
 tools/perf/util/stat.c                 |  32 ++
 tools/perf/util/synthetic-events.c     |  26 ++
 virt/kvm/kvm_main.c                    |   5 +
 92 files changed, 4515 insertions(+)

diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index 9ac371841..328c9647f 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -828,6 +828,12 @@ static void amd_pmu_disable_all(void)
 	amd_pmu_check_overflow();
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|938| <<amd_pmu_v2_handle_irq>> amd_pmu_v2_disable_all();
+ *   - arch/x86/events/amd/core.c|1398| <<amd_core_pmu_init>> x86_pmu.disable_all = amd_pmu_v2_disable_all;
+ *   - arch/x86/events/amd/core.c|1513| <<amd_pmu_reload_virt>> amd_pmu_v2_disable_all();
+ */
 static void amd_pmu_v2_disable_all(void)
 {
 	/* Disable all PMCs */
@@ -913,6 +919,10 @@ static int amd_pmu_handle_irq(struct pt_regs *regs)
 	return amd_pmu_adjust_nmi_window(handled);
 }
 
+/*
+ * 在以下使用amd_pmu_v2_handle_irq():
+ *   - arch/x86/events/amd/core.c|1396| <<amd_core_pmu_init>> x86_pmu.handle_irq = amd_pmu_v2_handle_irq;
+ */
 static int amd_pmu_v2_handle_irq(struct pt_regs *regs)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -1348,6 +1358,10 @@ static const struct attribute_group *amd_attr_update[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|1455| <<amd_pmu_init>> ret = amd_core_pmu_init();
+ */
 static int __init amd_core_pmu_init(void)
 {
 	union cpuid_0x80000022_ebx ebx;
@@ -1371,6 +1385,10 @@ static int __init amd_core_pmu_init(void)
 
 	/* Check for Performance Monitoring v2 support */
 	if (boot_cpu_has(X86_FEATURE_PERFMON_V2)) {
+		/*
+		 * AMD Extended Performance Monitoring and Debug cpuid feature detection
+		 * #define EXT_PERFMON_DEBUG_FEATURES 0x80000022
+		 */
 		ebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);
 
 		/* Update PMU version for later usage */
@@ -1442,14 +1460,32 @@ static int __init amd_core_pmu_init(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2105| <<init_hw_perf_events>> err = amd_pmu_init();
+ *   - arch/x86/events/core.c|2108| <<init_hw_perf_events>> err = amd_pmu_init();
+ */
 __init int amd_pmu_init(void)
 {
 	int ret;
 
 	/* Performance-monitoring supported from K7 and later: */
+	/*
+	 * 应该是在以下获得x86:
+	 * 1929 void __init identify_boot_cpu(void)
+	 * 1930 {
+	 * 1931         identify_cpu(&boot_cpu_data);
+	 */
 	if (boot_cpu_data.x86 < 6)
 		return -ENODEV;
 
+	/*
+	 * static __initconst const struct x86_pmu amd_pmu = {
+	 *     .name                   = "AMD",
+	 *     .eventsel               = MSR_K7_EVNTSEL0,
+	 *     .perfctr                = MSR_K7_PERFCTR0,
+	 *     .num_counters           = AMD64_NUM_COUNTERS,
+	 */
 	x86_pmu = amd_pmu;
 
 	ret = amd_core_pmu_init();
diff --git a/arch/x86/events/amd/ibs.c b/arch/x86/events/amd/ibs.c
index c251bc44c..f39459ba7 100644
--- a/arch/x86/events/amd/ibs.c
+++ b/arch/x86/events/amd/ibs.c
@@ -688,6 +688,11 @@ static struct perf_ibs perf_ibs_op = {
 	.get_count		= get_ibs_op_count,
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|833| <<perf_ibs_nmi_handler>> handled += perf_ibs_handle_irq(&perf_ibs_fetch, regs);
+ *   - arch/x86/events/amd/ibs.c|834| <<perf_ibs_nmi_handler>> handled += perf_ibs_handle_irq(&perf_ibs_op, regs);
+ */
 static int perf_ibs_handle_irq(struct perf_ibs *perf_ibs, struct pt_regs *iregs)
 {
 	struct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index f969410d0..1d43f3205 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -106,6 +106,18 @@ u64 __read_mostly hw_cache_extra_regs
  * Can only be executed on the CPU where the event is active.
  * Returns the delta events processed.
  */
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|948| <<amd_pmu_v2_handle_irq>> x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|1623| <<x86_pmu_stop>> x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|1713| <<x86_pmu_handle_irq>> val = x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|2055| <<_x86_pmu_read>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2314| <<intel_pmu_nhm_workaround>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2705| <<intel_pmu_read_event>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2806| <<intel_pmu_save_and_restart>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/p4.c|1038| <<p4_pmu_handle_irq>> val = x86_perf_event_update(event);
+ *   - arch/x86/events/zhaoxin/core.c|394| <<zhaoxin_pmu_handle_irq>> x86_perf_event_update(event);
+ */
 u64 x86_perf_event_update(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -128,6 +140,18 @@ u64 x86_perf_event_update(struct perf_event *event)
 	 */
 again:
 	prev_raw_count = local64_read(&hwc->prev_count);
+	/*
+	 * 在以下使用hw_perf_event->event_base_rdpmc:
+	 *   - arch/x86/events/amd/uncore.c|94| <<amd_uncore_read>> rdpmcl(hwc->event_base_rdpmc, new);
+	 *   - arch/x86/events/amd/uncore.c|158| <<amd_uncore_add>> hwc->event_base_rdpmc = uncore->rdpmc_base + hwc->idx;
+	 *   - arch/x86/events/amd/uncore.c|169| <<amd_uncore_add>> hwc->event_base_rdpmc += NUM_COUNTERS_L3;
+	 *   - arch/x86/events/core.c|131| <<x86_perf_event_update>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+	 *   - arch/x86/events/core.c|1242| <<x86_assign_hw_event>> hwc->event_base_rdpmc = (idx - INTEL_PMC_IDX_FIXED) |
+	 *   - arch/x86/events/core.c|1249| <<x86_assign_hw_event>> hwc->event_base_rdpmc = x86_pmu_rdpmc_index(hwc->idx);
+	 *   - arch/x86/events/core.c|1272| <<x86_perf_rdpmc_index>> return event->hw.event_base_rdpmc;
+	 *   - arch/x86/events/core.c|2576| <<x86_pmu_event_idx>> return hwc->event_base_rdpmc + 1;
+	 *   - arch/x86/events/intel/ds.c|1880| <<intel_pmu_save_and_restart_reload>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+	 *
 	rdpmcl(hwc->event_base_rdpmc, new_raw_count);
 
 	if (local64_cmpxchg(&hwc->prev_count, prev_raw_count,
@@ -247,6 +271,11 @@ static void release_pmc_hardware(void) {}
 
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2113| <<init_hw_perf_events>> if (!check_hw_exists(&pmu, x86_pmu.num_counters, x86_pmu.num_counters_fixed))
+ *   - arch/x86/events/intel/core.c|4504| <<init_hybrid_pmu>> if (!check_hw_exists(&pmu->pmu, pmu->num_counters, pmu->num_counters_fixed))
+ */
 bool check_hw_exists(struct pmu *pmu, int num_counters, int num_counters_fixed)
 {
 	u64 val, val_fail = -1, val_new= ~0;
@@ -1434,6 +1463,11 @@ int x86_perf_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|768| <<amd_pmu_enable_event>> x86_pmu_enable_event(event);
+ *   - arch/x86/events/intel/core.c|4105| <<core_pmu_enable_event>> x86_pmu_enable_event(event);
+ */
 void x86_pmu_enable_event(struct perf_event *event)
 {
 	if (__this_cpu_read(cpu_hw_events.enabled))
@@ -2076,6 +2110,10 @@ void x86_pmu_update_cpu_context(struct pmu *pmu, int cpu)
 	cpuctx->ctx.pmu = pmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2227| <<global>> early_initcall(init_hw_perf_events); 
+ */
 static int __init init_hw_perf_events(void)
 {
 	struct x86_pmu_quirk *quirk;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 266143abc..3574a0001 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1131,6 +1131,16 @@ static inline bool is_counter_pair(struct hw_perf_event *hwc)
 	return hwc->flags & PERF_X86_EVENT_PAIR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1440| <<x86_pmu_enable_event>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2330| <<intel_pmu_nhm_workaround>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ */
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa381ab69..b5b3c9e42 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -489,6 +489,17 @@ struct kvm_pmc {
 	enum pmc_type type;
 	u8 idx;
 	u64 counter;
+	/*
+	 * 在以下设置kvm_pmc->eventsel:
+	 *   - arch/x86/kvm/pmu.c|405| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_OS;
+	 *   - arch/x86/kvm/pmu.c|407| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_USR;
+	 *   - arch/x86/kvm/pmu.c|409| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_INT;
+	 *   - arch/x86/kvm/svm/pmu.c|240| <<amd_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/svm/pmu.c|293| <<amd_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|520| <<intel_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|541| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|700| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 */
 	u64 eventsel;
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
@@ -496,6 +507,16 @@ struct kvm_pmc {
 	 * eventsel value for general purpose counters,
 	 * ctrl value for fixed counters.
 	 */
+	/*
+	 * 在以下使用kvm_pmc->current_config:
+	 *   - arch/x86/kvm/pmu.c|324| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+	 *   - arch/x86/kvm/pmu.c|329| <<reprogram_counter>> pmc->current_config = new_config;
+	 *   - arch/x86/kvm/pmu.c|567| <<cpl_is_matched>> u64 config = pmc->current_config;
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_release_perf_event>> pmc->current_config = 0;
+	 *   - arch/x86/kvm/svm/pmu.c|280| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|624| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|631| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+	 */
 	u64 current_config;
 	bool is_paused;
 	bool intr;
@@ -503,8 +524,27 @@ struct kvm_pmc {
 
 #define KVM_PMC_MAX_FIXED	3
 struct kvm_pmu {
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|335| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|564| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|587| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned nr_arch_gp_counters;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	unsigned nr_arch_fixed_counters;
+	/*
+	 * 在以下使用kvm_pmu->available_event_types:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|120| <<intel_hw_event_available>> if ((i < 7) && !(pmu->available_event_types & (1 << i)))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|594| <<intel_pmu_refresh>> pmu->available_event_types = ~entry->ebx &
+	 */
 	unsigned available_event_types;
 	u64 fixed_ctr_ctrl;
 	u64 fixed_ctr_ctrl_mask;
@@ -519,8 +559,42 @@ struct kvm_pmu {
 	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
 	struct irq_work irq_work;
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
 	DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->all_valid_pmc_idx:
+	 *   - arch/x86/kvm/pmu.c|633| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/pmu.c|705| <<kvm_pmu_trigger_event>> for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/svm/pmu.c|266| <<amd_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|589| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|591| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
+	 */
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	DECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);
 
 	u64 ds_area;
@@ -542,12 +616,31 @@ struct kvm_pmu {
 	 * The gate to release perf_events not marked in
 	 * pmc_in_use only once in a vcpu time slice.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	bool need_cleanup;
 
 	/*
 	 * The total number of programmed perf_events and it helps to avoid
 	 * redundant check before cleanup if guest don't use vPMU at all.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - arch/x86/kvm/pmu.c|218| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|519| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|72| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|246| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|297| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12059| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 */
 	u8 event_count;
 };
 
@@ -677,6 +770,13 @@ struct kvm_vcpu_arch {
 	u64 ia32_xss;
 	u64 microcode_version;
 	u64 arch_capabilities;
+	/*
+	 * 在以下使用kvm_vcpu_arch->perf_capabilities:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|187| <<vcpu_get_perf_capabilities>> return vcpu->arch.perf_capabilities;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|689| <<intel_pmu_init>> vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();
+	 *   - arch/x86/kvm/x86.c|3560| <<kvm_set_msr_common>> vcpu->arch.perf_capabilities = data;
+	 *   - arch/x86/kvm/x86.c|3987| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.perf_capabilities;
+	 */
 	u64 perf_capabilities;
 
 	/*
@@ -758,6 +858,19 @@ struct kvm_vcpu_arch {
 	int halt_request; /* real mode on Intel only */
 
 	int cpuid_nent;
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpuid_entries:
+	 *   - arch/x86/kvm/cpuid.c|180| <<kvm_cpuid_check_equal>> orig = &vcpu->arch.cpuid_entries[i];
+	 *   - arch/x86/kvm/cpuid.c|232| <<kvm_find_kvm_cpuid_features>> return __kvm_find_kvm_cpuid_features(vcpu, vcpu->arch.cpuid_entries,
+	 *   - arch/x86/kvm/cpuid.c|325| <<kvm_update_cpuid_runtime>> __kvm_update_cpuid_runtime(vcpu, vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);
+	 *   - arch/x86/kvm/cpuid.c|349| <<kvm_vcpu_after_set_cpuid>> cpuid_get_supported_xcr0(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);
+	 *   - arch/x86/kvm/cpuid.c|440| <<kvm_set_cpuid>> kvfree(vcpu->arch.cpuid_entries);
+	 *   - arch/x86/kvm/cpuid.c|441| <<kvm_set_cpuid>> vcpu->arch.cpuid_entries = e2;
+	 *   - arch/x86/kvm/cpuid.c|529| <<kvm_vcpu_ioctl_get_cpuid2>> if (copy_to_user(entries, vcpu->arch.cpuid_entries,
+	 *   - arch/x86/kvm/cpuid.c|1389| <<kvm_find_cpuid_entry_index>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 *   - arch/x86/kvm/cpuid.c|1397| <<kvm_find_cpuid_entry>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 *   - arch/x86/kvm/x86.c|11755| <<kvm_arch_vcpu_destroy>> kvfree(vcpu->arch.cpuid_entries);
+	 */
 	struct kvm_cpuid_entry2 *cpuid_entries;
 	u32 kvm_cpuid_base;
 
@@ -911,9 +1024,32 @@ struct kvm_vcpu_arch {
 	bool l1tf_flush_l1d;
 
 	/* Host CPU on which VM-entry was most recently attempted */
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_vmentry_cpu:
+	 *   - arch/x86/kvm/cpuid.c|427| <<kvm_set_cpuid>> if (vcpu->arch.last_vmentry_cpu != -1) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5275| <<kvm_mmu_after_set_cpuid>> KVM_BUG_ON(vcpu->arch.last_vmentry_cpu != -1, vcpu->kvm);
+	 *   - arch/x86/kvm/svm/sev.c|2616| <<pre_sev_run>> svm->vcpu.arch.last_vmentry_cpu == cpu)
+	 *   - arch/x86/kvm/svm/svm.c|3243| <<dump_vmcb>> svm->current_vmcb->ptr, vcpu->arch.last_vmentry_cpu);
+	 *   - arch/x86/kvm/svm/svm.c|3367| <<svm_handle_invalid_exit>> vcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/svm/svm.c|3442| <<svm_handle_exit>> kvm_run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|5129| <<handle_exception_nmi>> vcpu->run->internal.data[3] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6143| <<dump_vmcs>> vmx->loaded_vmcs->vmcs, vcpu->arch.last_vmentry_cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6368| <<__vmx_handle_exit>> vcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6377| <<__vmx_handle_exit>> vcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6406| <<__vmx_handle_exit>> vcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6466| <<__vmx_handle_exit>> vcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/x86.c|10627| <<vcpu_enter_guest>> vcpu->arch.last_vmentry_cpu = vcpu->cpu;
+	 *   - arch/x86/kvm/x86.c|11624| <<kvm_arch_vcpu_create>> vcpu->arch.last_vmentry_cpu = -1;
+	 */
 	int last_vmentry_cpu;
 
 	/* AMD MSRC001_0015 Hardware Configuration */
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_hwcr:
+	 *   - arch/x86/kvm/x86.c|3282| <<can_set_mci_status>> return !!(vcpu->arch.msr_hwcr & BIT_ULL(18));
+	 *   - arch/x86/kvm/x86.c|3623| <<kvm_set_msr_common>> vcpu->arch.msr_hwcr = data;
+	 *   - arch/x86/kvm/x86.c|4249| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_hwcr;
+	 */
 	u64 msr_hwcr;
 
 	/* pv related cpuid info */
@@ -1142,6 +1278,15 @@ struct kvm_arch {
 	unsigned int indirect_shadow_pages;
 	u8 mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
+	/*
+	 * 在以下使用kvm_arch->active_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|2137| <<kvm_mmu_alloc_shadow_page>> list_add(&sp->link, &kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|2552| <<kvm_mmu_zap_oldest_mmu_pages>> if (list_empty(&kvm->arch.active_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|2556| <<kvm_mmu_zap_oldest_mmu_pages>> list_for_each_entry_safe_reverse(sp, tmp, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5866| <<kvm_zap_obsolete_pages>> &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5991| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6525| <<kvm_mmu_zap_all>> list_for_each_entry_safe(sp, node, &kvm->arch.active_mmu_pages, link) {
+	 */
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
 	struct list_head lpage_disallowed_mmu_pages;
@@ -1290,6 +1435,17 @@ struct kvm_arch {
 	 * count to zero should removed the root from the list and clean
 	 * it up, freeing the root after an RCU grace period.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|30| <<kvm_mmu_init_tdp_mmu>> INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|58| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|213| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|217| <<tdp_mmu_next_root>> next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|225| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|269| <<for_each_tdp_mmu_root>> list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|335| <<kvm_tdp_mmu_get_vcpu_root_hpa>> list_add_rcu(&root->link, &kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1038| <<kvm_tdp_mmu_invalidate_all_roots>> list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link) {
+	 */
 	struct list_head tdp_mmu_roots;
 
 	/*
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..84c03635c 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -23,6 +23,29 @@ extern int unknown_nmi_panic;
 
 #define NMI_FLAG_FIRST	1
 
+/*
+ * 注册了NMI_LOCAL的:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|101| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|850| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *
+ * 注册了NMI_UNKNOWN的:
+ *   - rch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 enum {
 	NMI_LOCAL=0,
 	NMI_UNKNOWN,
@@ -44,6 +67,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index c936ce9f0..143fe1e1d 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -311,6 +311,13 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 		: "memory");
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1496| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1355| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|437| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|448| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
 	u64 val = PRED_CMD_IBPB;
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f6fc8dd51..c781e2ec6 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -28,6 +28,29 @@
 #define ARCH_PERFMON_EVENTSEL_PIN_CONTROL		(1ULL << 19)
 #define ARCH_PERFMON_EVENTSEL_INT			(1ULL << 20)
 #define ARCH_PERFMON_EVENTSEL_ANY			(1ULL << 21)
+/*
+ * 在以下使用ARCH_PERFMON_EVENTSEL_ENABLE:
+ *   - arch/x86/events/amd/core.c|21| <<AMD_MERGE_EVENT_ENABLE>> #define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/amd/uncore.c|109| <<amd_uncore_start>> wrmsrl(hwc->config_base, (hwc->config | ARCH_PERFMON_EVENTSEL_ENABLE));
+ *   - arch/x86/events/core.c|266| <<check_hw_exists>> if (val & ARCH_PERFMON_EVENTSEL_ENABLE) {
+ *   - arch/x86/events/core.c|687| <<x86_pmu_disable_all>> if (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))
+ *   - arch/x86/events/core.c|689| <<x86_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1441| <<x86_pmu_enable_event>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2331| <<intel_pmu_nhm_workaround>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4090| <<core_guest_get_msrs>> event->hw.config | ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4093| <<core_guest_get_msrs>> arr[idx].host &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4095| <<core_guest_get_msrs>> arr[idx].guest &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/knc.c|183| <<knc_pmu_disable_event>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/knc.c|194| <<knc_pmu_enable_event>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|144| <<p6_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|154| <<p6_pmu_enable_all>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/kvm/pmu.h|158| <<pmc_speculative_in_use>> return pmc->eventsel & ARCH_PERFMON_EVENTSEL_ENABLE;
+ */
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 5393babc0..dab2a3d90 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -23,6 +23,10 @@ extern void x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bo
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4009| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
@@ -37,6 +41,10 @@ void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4036| <<svm_vcpu_run>> x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_restore_host(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c
index 907cc98b1..263a58be2 100644
--- a/arch/x86/kernel/acpi/boot.c
+++ b/arch/x86/kernel/acpi/boot.c
@@ -163,6 +163,38 @@ static int __init acpi_parse_madt(struct acpi_table_header *table)
  *
  * Returns the logic cpu number which maps to the local apic
  */
+/*
+ * boot的时候
+ * [0] acpi_register_lapic
+ * [0] acpi_parse_lapic
+ * [0] acpi_table_parse_entries_array
+ * [0] acpi_boot_init
+ * [0] acpi_parse_x2apic
+ * [0] acpi_parse_x2apic_nmi
+ * [0] setup_arch
+ * [0] start_kernel
+ * [0] load_ucode_bsp
+ * [0] secondary_startup_64_no_verify
+ *
+ * cpu hotadd的时候
+ * [0] acpi_register_lapic
+ * [0] acpi_map_cpu
+ * [0] acpi_processor_add
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kernel/acpi/boot.c|228| <<acpi_parse_x2apic>> acpi_register_lapic(apic_id, processor->uid, enabled);
+ *   - arch/x86/kernel/acpi/boot.c|265| <<acpi_parse_lapic>> acpi_register_lapic(processor->id,
+ *   - arch/x86/kernel/acpi/boot.c|284| <<acpi_parse_sapic>> acpi_register_lapic((processor->id << 8) | processor->eid,
+ *   - arch/x86/kernel/acpi/boot.c|821| <<acpi_map_cpu>> cpu = acpi_register_lapic(physid, acpi_id, ACPI_MADT_ENABLED);
+ */
 static int acpi_register_lapic(int id, u32 acpiid, u8 enabled)
 {
 	unsigned int ver = 0;
diff --git a/arch/x86/kernel/check.c b/arch/x86/kernel/check.c
index 5136e6818..c6fbe09bf 100644
--- a/arch/x86/kernel/check.c
+++ b/arch/x86/kernel/check.c
@@ -24,6 +24,13 @@ static int __read_mostly memory_corruption_check = -1;
 static unsigned __read_mostly corruption_check_size = 64*1024;
 static unsigned __read_mostly corruption_check_period = 60; /* seconds */
 
+/*
+ * 在以下使用scan_areas[MAX_SCAN_AREAS]:
+ *   - arch/x86/kernel/check.c|125| <<setup_bios_corruption_check>> scan_areas[num_scan_areas].addr = start;
+ *   - arch/x86/kernel/check.c|126| <<setup_bios_corruption_check>> scan_areas[num_scan_areas].size = end - start;
+ *   - arch/x86/kernel/check.c|149| <<check_for_bios_corruption>> unsigned long *addr = __va(scan_areas[i].addr);
+ *   - arch/x86/kernel/check.c|150| <<check_for_bios_corruption>> unsigned long size = scan_areas[i].size;
+ */
 static struct scan_area {
 	u64 addr;
 	u64 size;
@@ -137,6 +144,10 @@ void __init setup_bios_corruption_check(void)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kernel/check.c|169| <<check_corruption>> check_for_bios_corruption();
+ */
 static void check_for_bios_corruption(void)
 {
 	int i;
@@ -164,6 +175,10 @@ static void check_for_bios_corruption(void)
 static void check_corruption(struct work_struct *dummy);
 static DECLARE_DELAYED_WORK(bios_check_work, check_corruption);
 
+/*
+ * 在以下使用check_corruption():
+ *   - arch/x86/kernel/check.c|176| <<global>> static DECLARE_DELAYED_WORK(bios_check_work, check_corruption);
+ */
 static void check_corruption(struct work_struct *dummy)
 {
 	check_for_bios_corruption();
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index da7c361f4..de0172cbe 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -199,6 +199,18 @@ void __init check_bugs(void)
  * NOTE: This function is *only* called for SVM.  VMX spec_ctrl handling is
  * done in vmenter.S.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/spec-ctrl.h|29| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, true);
+ *   - arch/x86/include/asm/spec-ctrl.h|43| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, false);
+ *
+ * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+ * the guest has, while on VMEXIT we restore the host view. This
+ * would be easier if SPEC_CTRL were architecturally maskable or
+ * shadowable for guests but this is not (currently) the case.
+ * Takes the guest view of SPEC_CTRL MSR as a parameter and also
+ * the guest's version of VIRT_SPEC_CTRL, if emulated.
+ */
 void
 x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
 {
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3e508f239..36f106795 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -67,6 +67,14 @@
 u32 elf_hwcap2 __read_mostly;
 
 /* all of these masks are initialized in setup_cpu_local_masks() */
+/*
+ * 在以下使用cpu_initialized_mask:
+ *   - arch/x86/kernel/cpu/common.c|173| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+ *   - arch/x86/kernel/cpu/common.c|2138| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+ *   - arch/x86/kernel/smpboot.c|1122| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+ *   - arch/x86/kernel/smpboot.c|1147| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+ *   - arch/x86/kernel/smpboot.c|1644| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+ */
 cpumask_var_t cpu_initialized_mask;
 cpumask_var_t cpu_callout_mask;
 cpumask_var_t cpu_callin_mask;
@@ -942,6 +950,19 @@ void cpu_detect(struct cpuinfo_x86 *c)
 		u32 junk, tfms, cap0, misc;
 
 		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
+		/*
+		 *  6 unsigned int x86_family(unsigned int sig)
+		 *  7 {
+		 *  8         unsigned int x86;
+		 *  9
+		 * 10         x86 = (sig >> 8) & 0xf;
+		 * 11
+		 * 12         if (x86 == 0xf)
+		 * 13                 x86 += (sig >> 20) & 0xff;
+		 * 14
+		 * 15         return x86;
+		 * 16 }
+		 */
 		c->x86		= x86_family(tfms);
 		c->x86_model	= x86_model(tfms);
 		c->x86_stepping	= x86_stepping(tfms);
@@ -2122,6 +2143,14 @@ static void wait_for_master_cpu(int cpu)
 	 * wait for ACK from master CPU before continuing
 	 * with AP initialization
 	 */
+	/*
+	 * 在以下使用cpu_initialized_mask:
+	 *   - arch/x86/kernel/cpu/common.c|173| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+	 *   - arch/x86/kernel/cpu/common.c|2138| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+	 *   - arch/x86/kernel/smpboot.c|1122| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 *   - arch/x86/kernel/smpboot.c|1147| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+	 *   - arch/x86/kernel/smpboot.c|1644| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 */
 	WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
 	while (!cpumask_test_cpu(cpu, cpu_callout_mask))
 		cpu_relax();
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index cec0bfa3b..97e14c23c 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -73,6 +73,12 @@ struct nmi_stats {
 
 static DEFINE_PER_CPU(struct nmi_stats, nmi_stats);
 
+/*
+ * 在以下使用ignoe_nmis:
+ *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+ *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+ *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+ */
 static int ignore_nmis __read_mostly;
 
 int unknown_nmi_panic;
@@ -101,6 +107,10 @@ static int __init nmi_warning_debugfs(void)
 }
 fs_initcall(nmi_warning_debugfs);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|145| <<nmi_handle>> nmi_check_duration(a, delta);
+ */
 static void nmi_check_duration(struct nmiaction *action, u64 duration)
 {
 	int remainder_ns, decimal_msecs;
@@ -118,6 +128,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|234| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|261| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|310| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|359| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -152,6 +169,33 @@ static int nmi_handle(unsigned int type, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_handle);
 
+/*
+ * 5.4上看到这两个NMI LOCAL的.
+ *
+ * crash> nmiaction ffffffffa6617ec0
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6641a80,
+ *     prev = 0xffffffffa6634368
+ *   },
+ *   handler = 0xffffffffa4c07aa0, --> perf_event_nmi_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa60b4811 "PMI"
+ * }
+ * crash> nmiaction ffffffffa6641a80
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6634368,
+ *     prev = 0xffffffffa6617ec0
+ *   },
+ *   handler = 0xffffffffa4c7aeb0, --> nmi_cpu_backtrace_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa6041a34 "arch_bt"
+ * }
+ */
+
 int __register_nmi_handler(unsigned int type, struct nmiaction *action)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -213,6 +257,10 @@ void unregister_nmi_handler(unsigned int type, const char *name)
 }
 EXPORT_SYMBOL_GPL(unregister_nmi_handler);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|370| <<default_do_nmi>> pci_serr_error(reason, regs);
+ */
 static void
 pci_serr_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -234,6 +282,10 @@ pci_serr_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(pci_serr_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|372| <<default_do_nmi>> io_check_error(reason, regs);
+ */
 static void
 io_check_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -274,11 +326,25 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(io_check_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|419| <<default_do_nmi>> unknown_nmi_error(reason, regs);
+ */
 static void
 unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 {
 	int handled;
 
+	/*
+	 * 注册了NMI_UNKNOWN的:
+	 *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+	 *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+	 *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+	 *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+	 *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+	 *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+	 */
+
 	/*
 	 * Use 'false' as back-to-back NMIs are dealt with one level up.
 	 * Of course this makes having multiple 'unknown' handlers useless
@@ -304,8 +370,19 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 NOKPROBE_SYMBOL(unknown_nmi_error);
 
 static DEFINE_PER_CPU(bool, swallow_nmi);
+/*
+ * 在以下使用last_nmi_rip:
+ *   - arch/x86/kernel/nmi.c|329| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+ *   - arch/x86/kernel/nmi.c|350| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+ *   - arch/x86/kernel/nmi.c|355| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+ *   - arch/x86/kernel/nmi.c|575| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+ */
 static DEFINE_PER_CPU(unsigned long, last_nmi_rip);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|535| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> default_do_nmi(regs);
+ */
 static noinstr void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;
@@ -472,6 +549,14 @@ enum nmi_states {
 	NMI_EXECUTING,
 	NMI_LATCHED,
 };
+/*
+ * 在以下使用nmi_state:
+ *   - arch/x86/kernel/nmi.c|497| <<global>> static DEFINE_PER_CPU(enum nmi_states, nmi_state);
+ *   - arch/x86/kernel/nmi.c|514| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_read(nmi_state) != NMI_NOT_RUNNING) {
+ *   - arch/x86/kernel/nmi.c|515| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_LATCHED);
+ *   - arch/x86/kernel/nmi.c|518| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_EXECUTING);
+ *   - arch/x86/kernel/nmi.c|545| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_dec_return(nmi_state))
+ */
 static DEFINE_PER_CPU(enum nmi_states, nmi_state);
 static DEFINE_PER_CPU(unsigned long, nmi_cr2);
 static DEFINE_PER_CPU(unsigned long, nmi_dr7);
@@ -509,6 +594,12 @@ DEFINE_IDTENTRY_RAW(exc_nmi)
 
 	inc_irq_stat(__nmi_count);
 
+	/*
+	 * 在以下使用ignoe_nmis:
+	 *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+	 *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+	 *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+	 */
 	if (!ignore_nmis)
 		default_do_nmi(regs);
 
@@ -548,6 +639,10 @@ void restart_nmi(void)
 }
 
 /* reset the back-to-back NMI logic */
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|709| <<arch_cpu_idle_enter>> local_touch_nmi();
+ */
 void local_touch_nmi(void)
 {
 	__this_cpu_write(last_nmi_rip, 0);
diff --git a/arch/x86/kernel/nmi_selftest.c b/arch/x86/kernel/nmi_selftest.c
index a1a96df3d..d1a391bb2 100644
--- a/arch/x86/kernel/nmi_selftest.c
+++ b/arch/x86/kernel/nmi_selftest.c
@@ -23,6 +23,15 @@
 #define FAILURE		1
 #define TIMEOUT		2
 
+/*
+ * 在以下使用nmi_fail:
+ *   - arch/x86/kernel/nmi_selftest.c|71| <<test_nmi_ipi>> nmi_fail = FAILURE;
+ *   - arch/x86/kernel/nmi_selftest.c|89| <<test_nmi_ipi>> nmi_fail = TIMEOUT;
+ *   - arch/x86/kernel/nmi_selftest.c|110| <<reset_nmi>> nmi_fail = 0;
+ *   - arch/x86/kernel/nmi_selftest.c|119| <<dotest>> if (nmi_fail != expected) {
+ *   - arch/x86/kernel/nmi_selftest.c|122| <<dotest>> if (nmi_fail == FAILURE)
+ *   - arch/x86/kernel/nmi_selftest.c|124| <<dotest>> else if (nmi_fail == TIMEOUT)
+ */
 static int __initdata nmi_fail;
 
 /* check to see if NMI IPIs work on this machine */
@@ -62,6 +71,29 @@ static int __init test_nmi_ipi_callback(unsigned int val, struct pt_regs *regs)
         return NMI_DONE;
 }
 
+/*
+ * called by:
+ 49  *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ 50  *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ 51  *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ 52  *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ 53  *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ 54  *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ 55  *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ 56  *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ 57  *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ 58  *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ 59  *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ 60  *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ 61  *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ 62  *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ 63  *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ 64  *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ 65  *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ 66  *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ 67  *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 static void __init test_nmi_ipi(struct cpumask *mask)
 {
 	unsigned long timeout;
diff --git a/arch/x86/kernel/sev.c b/arch/x86/kernel/sev.c
index a428c6233..b267bb976 100644
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@ -1485,6 +1485,10 @@ static enum es_result vc_do_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt,
  * rare operation. If it turns out to be a performance problem the split
  * operations can be moved to memcpy_fromio() and memcpy_toio().
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1604| <<vc_handle_mmio>> ret = vc_handle_mmio_movs(ctxt, bytes);
+ */
 static enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,
 					  unsigned int bytes)
 {
@@ -1533,6 +1537,10 @@ static enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,
 		return ES_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1805| <<vc_handle_exitcode>> result = vc_handle_mmio(ghcb, ctxt);
+ */
 static enum es_result vc_handle_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt)
 {
 	struct insn *insn = &ctxt->insn;
@@ -1753,6 +1761,11 @@ static enum es_result vc_handle_trap_ac(struct ghcb *ghcb,
 	return ES_EXCEPTION;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1878| <<vc_raw_handle_exception>> result = vc_handle_exitcode(&ctxt, ghcb, error_code);
+ *   - arch/x86/kernel/sev.c|2017| <<handle_vc_boot_ghcb>> result = vc_handle_exitcode(&ctxt, boot_ghcb, exit_code);
+ */
 static enum es_result vc_handle_exitcode(struct es_em_ctxt *ctxt,
 					 struct ghcb *ghcb,
 					 unsigned long exit_code)
@@ -1861,6 +1874,11 @@ static __always_inline bool vc_from_invalid_context(struct pt_regs *regs)
 	return is_vc2_stack(sp) && !is_vc2_stack(prev_sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1962| <<DEFINE_IDTENTRY_VC_KERNEL(exc_vmm_communication)>> if (!vc_raw_handle_exception(regs, error_code)) {
+ *   - arch/x86/kernel/sev.c|1994| <<DEFINE_IDTENTRY_VC_USER(exc_vmm_communication)>> if (!vc_raw_handle_exception(regs, error_code)) {
+ */
 static bool vc_raw_handle_exception(struct pt_regs *regs, unsigned long error_code)
 {
 	struct ghcb_state state;
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f24227bc3..f76f806ac 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1144,6 +1144,14 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		boot_error = -1;
 		timeout = jiffies + 10*HZ;
 		while (time_before(jiffies, timeout)) {
+			/*
+			 * 在以下使用cpu_initialized_mask:
+			 *   - arch/x86/kernel/cpu/common.c|173| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+			 *   - arch/x86/kernel/cpu/common.c|2138| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+			 *   - arch/x86/kernel/smpboot.c|1122| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+			 *   - arch/x86/kernel/smpboot.c|1147| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+			 *   - arch/x86/kernel/smpboot.c|1644| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+			 */
 			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
 				/*
 				 * Tell AP to proceed with initialization
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 2796dde06..49c0104ea 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -30,6 +30,21 @@
  * Unlike "struct cpuinfo_x86.x86_capability", kvm_cpu_caps doesn't need to be
  * aligned to sizeof(unsigned long) because it's not accessed via bitops.
  */
+/*
+ * 在以下使用kvm_cpu_caps[NR_KVM_CPU_CAPS]:
+ *   - arch/x86/kvm/cpuid.c|535| <<__kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);
+ *   - arch/x86/kvm/cpuid.c|544| <<kvm_cpu_cap_init_scattered>> kvm_cpu_caps[leaf] = mask;
+ *   - arch/x86/kvm/cpuid.c|554| <<kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= mask;
+ *   - arch/x86/kvm/cpuid.c|570| <<kvm_set_cpu_caps>> memset(kvm_cpu_caps, 0, sizeof(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.c|572| <<kvm_set_cpu_caps>> BUILD_BUG_ON(sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)) >
+ *   - arch/x86/kvm/cpuid.c|575| <<kvm_set_cpu_caps>> memcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,
+ *   - arch/x86/kvm/cpuid.c|576| <<kvm_set_cpu_caps>> sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)));
+ *   - arch/x86/kvm/cpuid.h|90| <<cpuid_entry_override>> BUILD_BUG_ON(leaf >= ARRAY_SIZE(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.h|91| <<cpuid_entry_override>> *reg = kvm_cpu_caps[leaf];
+ *   - arch/x86/kvm/cpuid.h|215| <<kvm_cpu_cap_clear>> kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|223| <<kvm_cpu_cap_set>> kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|231| <<kvm_cpu_cap_get>> return kvm_cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+ */
 u32 kvm_cpu_caps[NR_KVM_CPU_CAPS] __read_mostly;
 EXPORT_SYMBOL_GPL(kvm_cpu_caps);
 
@@ -311,6 +326,10 @@ void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_update_cpuid_runtime);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|421| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -382,6 +401,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|462| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|488| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
@@ -513,6 +537,11 @@ int kvm_vcpu_ioctl_get_cpuid2(struct kvm_vcpu *vcpu,
 }
 
 /* Mask kvm_cpu_caps for @leaf with the raw CPUID capabilities of this CPU. */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|561| <<kvm_cpu_cap_init_scattered>> __kvm_cpu_cap_mask(leaf);
+ *   - arch/x86/kvm/cpuid.c|571| <<kvm_cpu_cap_mask>> __kvm_cpu_cap_mask(leaf);
+ */
 static __always_inline void __kvm_cpu_cap_mask(unsigned int leaf)
 {
 	const struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);
@@ -823,6 +852,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1270| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -1232,6 +1265,11 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1285| <<get_cpuid_func>> r = do_cpuid_func(array, func, type);
+ *   - arch/x86/kvm/cpuid.c|1291| <<get_cpuid_func>> r = do_cpuid_func(array, func, type);
+ */
 static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			 unsigned int type)
 {
@@ -1243,6 +1281,10 @@ static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 
 #define CENTAUR_CPUID_SIGNATURE 0xC0000000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1354| <<kvm_dev_ioctl_get_cpuid>> r = get_cpuid_func(&array, funcs[i], type);
+ */
 static int get_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			  unsigned int type)
 {
@@ -1294,6 +1336,10 @@ static bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,
 	return false;
 }
 
+/*
+ * called by:处理KVM_GET_SUPPORTED_CPUID和KVM_GET_EMULATED_CPUID
+ *   - arch/x86/kvm/x86.c|4635| <<kvm_arch_dev_ioctl>> r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,
+ */
 int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 			    struct kvm_cpuid_entry2 __user *entries,
 			    unsigned int type)
@@ -1421,6 +1467,11 @@ get_out_of_range_cpuid_entry(struct kvm_vcpu *vcpu, u32 *fn_ptr, u32 index)
 	return kvm_find_cpuid_entry_index(vcpu, basic->eax, index);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1528| <<kvm_emulate_cpuid>> kvm_cpuid(vcpu, &eax, &ebx, &ecx, &edx, false);
+ *   - arch/x86/kvm/x86.c|8078| <<emulator_get_cpuid>> return kvm_cpuid(emul_to_vcpu(ctxt), eax, ebx, ecx, edx, exact_only);
+ */
 bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 	       u32 *ecx, u32 *edx, bool exact_only)
 {
@@ -1470,6 +1521,11 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 }
 EXPORT_SYMBOL_GPL(kvm_cpuid);
 
+/*
+ * 在以下使用kvm_emulate_cpuid():
+ *   - arch/x86/kvm/svm/svm.c|3184| <<global>> [SVM_EXIT_CPUID] = kvm_emulate_cpuid,
+ *   - arch/x86/kvm/vmx/vmx.c|5975| <<global>> [EXIT_REASON_CPUID] = kvm_emulate_cpuid,
+ */
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
 {
 	u32 eax, ebx, ecx, edx;
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index b1658c0de..95bee1025 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,11 +42,30 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|52| <<kvm_vcpu_is_illegal_gpa>> return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|58| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/svm/nested.c|261| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|262| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|805| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|324| <<__nested_vmcb_check_save>> CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))
+ *   - arch/x86/kvm/svm/nested.c|522| <<nested_svm_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))
+ *   - arch/x86/kvm/vmx/nested.c|1113| <<nested_vmx_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3))) {
+ *   - arch/x86/kvm/vmx/nested.c|2695| <<nested_vmx_check_eptp>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/nested.c|2890| <<nested_vmx_check_host_state>> CC(kvm_vcpu_is_illegal_gpa(vcpu, vmcs12->host_cr3)))
+ *   - arch/x86/kvm/vmx/vmx.c|5669| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/x86.c|1239| <<kvm_set_cr3>> if (kvm_vcpu_is_illegal_gpa(vcpu, cr3))
+ *   - arch/x86/kvm/x86.c|11183| <<kvm_is_valid_sregs>> if (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))
+ */
 static inline bool kvm_vcpu_is_illegal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
@@ -63,6 +82,23 @@ static inline bool page_address_valid(struct kvm_vcpu *vcpu, gpa_t gpa)
 	return kvm_vcpu_is_legal_aligned_gpa(vcpu, gpa, PAGE_SIZE);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|875| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_1_EDX);
+ *   - arch/x86/kvm/cpuid.c|876| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_1_ECX);
+ *   - arch/x86/kvm/cpuid.c|918| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_0_EBX);
+ *   - arch/x86/kvm/cpuid.c|919| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_ECX);
+ *   - arch/x86/kvm/cpuid.c|920| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_EDX);
+ *   - arch/x86/kvm/cpuid.c|928| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_1_EAX);
+ *   - arch/x86/kvm/cpuid.c|993| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_D_1_EAX);
+ *   - arch/x86/kvm/cpuid.c|1049| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_12_EAX);
+ *   - arch/x86/kvm/cpuid.c|1149| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_0001_EDX);
+ *   - arch/x86/kvm/cpuid.c|1150| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_0001_ECX);
+ *   - arch/x86/kvm/cpuid.c|1184| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_0008_EBX);
+ *   - arch/x86/kvm/cpuid.c|1196| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_000A_EDX);
+ *   - arch/x86/kvm/cpuid.c|1208| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_001F_EAX);
+ *   - arch/x86/kvm/cpuid.c|1244| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_C000_0001_EDX);
+ */
 static __always_inline void cpuid_entry_override(struct kvm_cpuid_entry2 *entry,
 						 unsigned int leaf)
 {
@@ -196,6 +232,39 @@ static __always_inline void kvm_cpu_cap_clear(unsigned int x86_feature)
 	kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|608| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_X2APIC);
+ *   - arch/x86/kvm/cpuid.c|639| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_LA57);
+ *   - arch/x86/kvm/cpuid.c|657| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_TSC_ADJUST);
+ *   - arch/x86/kvm/cpuid.c|658| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_ARCH_CAPABILITIES);
+ *   - arch/x86/kvm/cpuid.c|661| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL);
+ *   - arch/x86/kvm/cpuid.c|663| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_INTEL_STIBP);
+ *   - arch/x86/kvm/cpuid.c|665| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL_SSBD);
+ *   - arch/x86/kvm/cpuid.c|699| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_GBPAGES);
+ *   - arch/x86/kvm/cpuid.c|714| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_IBPB);
+ *   - arch/x86/kvm/cpuid.c|716| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_IBRS);
+ *   - arch/x86/kvm/cpuid.c|718| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_STIBP);
+ *   - arch/x86/kvm/cpuid.c|720| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_SSBD);
+ *   - arch/x86/kvm/cpuid.c|722| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_SSB_NO);
+ *   - arch/x86/kvm/cpuid.c|729| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);
+ *   - arch/x86/kvm/cpuid.h|242| <<kvm_cpu_cap_check_and_set>> kvm_cpu_cap_set(x86_feature);
+ *   - arch/x86/kvm/svm/svm.c|4921| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SVM);
+ *   - arch/x86/kvm/svm/svm.c|4922| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VMCBCLEAN);
+ *   - arch/x86/kvm/svm/svm.c|4925| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_NRIPS);
+ *   - arch/x86/kvm/svm/svm.c|4928| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_NPT);
+ *   - arch/x86/kvm/svm/svm.c|4931| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_TSCRATEMSR);
+ *   - arch/x86/kvm/svm/svm.c|4934| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_V_VMSAVE_VMLOAD);
+ *   - arch/x86/kvm/svm/svm.c|4936| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_LBRV);
+ *   - arch/x86/kvm/svm/svm.c|4939| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_PAUSEFILTER);
+ *   - arch/x86/kvm/svm/svm.c|4942| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_PFTHRESHOLD);
+ *   - arch/x86/kvm/svm/svm.c|4945| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VGIF);
+ *   - arch/x86/kvm/svm/svm.c|4948| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SVME_ADDR_CHK);
+ *   - arch/x86/kvm/svm/svm.c|4954| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);
+ *   - arch/x86/kvm/svm/svm.c|4958| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_PERFCTR_CORE);
+ *   - arch/x86/kvm/vmx/vmx.c|7686| <<vmx_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VMX);
+ *   - arch/x86/kvm/vmx/vmx.c|7711| <<vmx_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_UMIP);
+ */
 static __always_inline void kvm_cpu_cap_set(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
@@ -204,6 +273,11 @@ static __always_inline void kvm_cpu_cap_set(unsigned int x86_feature)
 	kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|236| <<kvm_cpu_cap_has>> return !!kvm_cpu_cap_get(x86_feature);
+ *   - arch/x86/kvm/vmx/evmcs.c|338| <<nested_get_evmcs_version>> if (kvm_cpu_cap_get(X86_FEATURE_VMX) &&
+ */
 static __always_inline u32 kvm_cpu_cap_get(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 3febc3423..2f8f859ec 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -183,6 +183,13 @@ static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|966| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1175| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3496| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4565| <<nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9dda989a1..2db3ed547 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -2524,6 +2524,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1712| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2547| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|432| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|4958| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 6bdaacb6f..ffce2f742 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -120,6 +120,10 @@ void kvm_mmu_free_obsolete_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10506| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu->root.hpa != INVALID_PAGE))
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 3552e6af3..6c2043e08 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -921,6 +921,12 @@ pte_list_desc_remove_entry(struct kvm_rmap_head *rmap_head,
 	mmu_free_pte_list_desc(desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|964| <<kvm_zap_one_rmap_spte>> pte_list_remove(sptep, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1054| <<rmap_remove>> pte_list_remove(spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1698| <<mmu_page_remove_parent_pte>> pte_list_remove(parent_pte, &sp->parent_ptes);
+ */
 static void pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
@@ -1819,6 +1825,11 @@ static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 	return __mmu_unsync_walk(sp, pvec);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1984| <<mmu_sync_children>> kvm_unlink_unsync_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/mmu.c|2447| <<__kvm_mmu_prepare_zap_page>> kvm_unlink_unsync_page(kvm, sp);
+ */
 static void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	WARN_ON(!sp->unsync);
@@ -2424,6 +2435,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2496| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2546| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|5871| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|6505| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2522,6 +2540,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2596| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2621| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|6607| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2568,6 +2592,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3562| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3695| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4276| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|877| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -3514,6 +3545,13 @@ static int mmu_check_root(struct kvm_vcpu *vcpu, gfn_t root_gfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3583| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+ *   - arch/x86/kvm/mmu/mmu.c|3594| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0,
+ *   - arch/x86/kvm/mmu/mmu.c|3717| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0,
+ *   - arch/x86/kvm/mmu/mmu.c|3771| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+ */
 static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 			    u8 level)
 {
@@ -3532,6 +3570,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5337| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4050,6 +4092,11 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4257| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|822| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 struct kvm_page_fault *fault)
 {
@@ -4208,6 +4255,11 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_invalidate_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4278| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4338| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@ -4434,6 +4486,14 @@ static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
 		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5117| <<kvm_init_shadow_npt_mmu>> kvm_mmu_new_pgd(vcpu, nested_cr3);
+ *   - arch/x86/kvm/mmu/mmu.c|5172| <<kvm_init_shadow_ept_mmu>> kvm_mmu_new_pgd(vcpu, new_eptp);
+ *   - arch/x86/kvm/svm/nested.c|535| <<nested_svm_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/vmx/nested.c|1135| <<nested_vmx_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/x86.c|1246| <<kvm_set_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ */
 void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -5274,6 +5334,10 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|128| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -6319,6 +6383,10 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 }
 
 /* Must be called with the mmu_lock held in write-mode. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1334| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_try_split_huge_pages(kvm, slot, start, end, PG_LEVEL_4K);
+ */
 void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot,
 				   u64 start, u64 end,
diff --git a/arch/x86/kvm/mmu/page_track.c b/arch/x86/kvm/mmu/page_track.c
index 2e09d1b62..aabca6a33 100644
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@ -35,6 +35,10 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *slot)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12476| <<kvm_alloc_memslot_metadata>> if (kvm_page_track_create_memslot(kvm, slot, npages))
+ */
 int kvm_page_track_create_memslot(struct kvm *kvm,
 				  struct kvm_memory_slot *slot,
 				  unsigned long npages)
@@ -174,6 +178,11 @@ EXPORT_SYMBOL_GPL(kvm_slot_page_track_remove_page);
 /*
  * check if the corresponding access on the specified guest page is tracked.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2707| <<mmu_try_to_unsync_pages>> if (kvm_slot_page_track_is_active(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE))
+ *   - arch/x86/kvm/mmu/mmu.c|4097| <<page_fault_handle_page_track>> if (kvm_slot_page_track_is_active(vcpu->kvm, fault->slot, fault->gfn, KVM_PAGE_TRACK_WRITE))
+ */
 bool kvm_slot_page_track_is_active(struct kvm *kvm,
 				   const struct kvm_memory_slot *slot,
 				   gfn_t gfn, enum kvm_page_track_mode mode)
@@ -255,6 +264,11 @@ EXPORT_SYMBOL_GPL(kvm_page_track_unregister_notifier);
  * The node should figure out if the written page is the one that node is
  * interested in by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7410| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|7677| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ */
 void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			  int bytes)
 {
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bf2ccf9de..23ad080a6 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -308,6 +308,10 @@ static void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,
 	tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3580| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role = vcpu->arch.mmu->root_role;
@@ -1112,6 +1116,11 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Returns: 0 if the new page table was installed. Non-0 if the page table
  *          could not be installed (e.g. the atomic compare-exchange failed).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1196| <<kvm_tdp_mmu_map>> if (tdp_mmu_link_sp(vcpu->kvm, &iter, sp, account_nx, true)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1491| <<tdp_mmu_split_huge_page>> ret = tdp_mmu_link_sp(kvm, iter, sp, false, shared);
+ */
 static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 			   struct kvm_mmu_page *sp, bool account_nx,
 			   bool shared)
@@ -1460,6 +1469,10 @@ static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1547| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))
+ */
 static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 				   struct kvm_mmu_page *sp, bool shared)
 {
@@ -1500,6 +1513,10 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1581| <<kvm_tdp_mmu_try_split_huge_pages>> r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
+ */
 static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 					 struct kvm_mmu_page *root,
 					 gfn_t start, gfn_t end,
@@ -1567,6 +1584,11 @@ static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 /*
  * Try to split all huge pages mapped by the TDP MMU down to the target level.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6341| <<kvm_mmu_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, false);
+ *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_slot_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);
+ */
 void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      const struct kvm_memory_slot *slot,
 				      gfn_t start, gfn_t end,
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 02f9e4f24..a7a3c64d1 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,8 +23,36 @@
 #include "pmu.h"
 
 /* This is enough to filter the vast majority of currently defined events. */
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - arch/x86/kvm/pmu.c|583| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/cpuid.c|914| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|915| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|916| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|917| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len; 
+ *   - arch/x86/kvm/cpuid.c|918| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|919| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|921| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|927| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/pmu.h|167| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|173| <<kvm_init_pmu_capability>> if ((is_intel && !kvm_pmu_cap.version) || !kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/pmu.h|177| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|181| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|182| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed,
+ *   - arch/x86/kvm/vmx/capabilities.h|401| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|540| <<intel_pmu_refresh>> kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|542| <<intel_pmu_refresh>> kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|545| <<intel_pmu_refresh>> kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> (size_t)kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|557| <<intel_pmu_refresh>> kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|6944| <<kvm_init_msr_list>> min(INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))
+ *   - arch/x86/kvm/x86.c|6949| <<kvm_init_msr_list>> min(INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
@@ -70,6 +98,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11967| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -85,9 +117,19 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 
 static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
 {
+	/*
+	 * intel_pmc_is_enabled()
+	 * amd_pmc_is_enabled()
+	 */
 	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);
 }
 
+/*
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|485| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ *
+ * 在__kvm_perf_overflow()被调用
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
@@ -96,11 +138,19 @@ static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|144| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|554| <<kvm_pmu_incr_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 	bool skip_pmi = false;
 
+	/*
+	 * Set a bit and return its old value
+	 */
 	/* Ignore counters that have been reprogrammed already. */
 	if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
 		return;
@@ -117,6 +167,12 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 	if (!pmc->intr || skip_pmi)
 		return;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|178| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8161| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10337| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 	/*
 	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
 	 * can be ejected on a guest mode re-entry. Otherwise we can't
@@ -131,6 +187,10 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|201| <<pmc_reprogram_counter>> kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -140,6 +200,10 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	__kvm_perf_overflow(pmc, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|326| <<reprogram_counter>> pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ */
 static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 				  u64 config, bool exclude_user,
 				  bool exclude_kernel, bool intr)
@@ -217,6 +281,10 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|374| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -247,6 +315,10 @@ static int cmp_u64(const void *pa, const void *pb)
 	return (a > b) - (a < b);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|356| <<reprogram_counter>> if (!check_pmu_event_filter(pmc))
+ */
 static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu_event_filter *filter;
@@ -283,9 +355,37 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 	return allow_event;
 }
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_pmu pmu;
+ *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+ *          -> u8 idx; 
+ *          -> u64 counter;
+ *          -> u64 eventsel;
+ *          -> struct perf_event *perf_event;
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> u64 current_config;
+ *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|359| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|561| <<kvm_pmu_incr_counter>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|241| <<amd_pmu_set_msr>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|55| <<reprogram_fixed_counters>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|83| <<global_ctrl_changed>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|480| <<intel_pmu_set_msr>> reprogram_counter(pmc);
+ */
 void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
+	/*
+	 * 在以下设置kvm_pmc->eventsel:
+	 *   - arch/x86/kvm/svm/pmu.c|240| <<amd_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|485| <<intel_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 */
 	u64 eventsel = pmc->eventsel;
 	u64 new_config = eventsel;
 	u8 fixed_ctr_ctrl;
@@ -327,6 +427,10 @@ void reprogram_counter(struct kvm_pmc *pmc)
 }
 EXPORT_SYMBOL_GPL(reprogram_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10336| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -342,6 +446,16 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 		reprogram_counter(pmc);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	/*
 	 * Unused perf_events are only released if the corresponding MSRs
 	 * weren't accessed during the last vCPU time slice. kvm_arch_sched_in
@@ -417,9 +531,22 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|178| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8161| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10337| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|96| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10338| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
@@ -427,27 +554,72 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
+	/*
+	 * intel_msr_idx_to_pmc()
+	 * amd_msr_idx_to_pmc()
+	 *
+	 * intel_is_valid_msr()
+	 * amd_is_valid_msr()
+	 */
 	return static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr) ||
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|591| <<kvm_pmu_set_msr>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 */
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	struct kvm_pmc *pmc = static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr);
 
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	if (pmc)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3957| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3971| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4219| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ */
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3768| <<kvm_set_msr_common(MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3854| <<kvm_set_msr_common(MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3862| <<kvm_set_msr_common(default: kvm_pmu_is_valid_msr)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+	/*
+	 * intel_pmu_set_msr
+	 * amd_pmu_set_msr
+	 */
 	return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);
 }
 
@@ -455,11 +627,22 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|349| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|626| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3561| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|522| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11767| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -481,6 +664,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 }
 
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|368| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -488,12 +675,46 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	DECLARE_BITMAP(bitmask, X86_PMC_IDX_MAX);
 	int i;
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	pmu->need_cleanup = false;
 
+	/*
+	 * 在以下使用kvm_pmu->all_valid_pmc_idx:
+	 *   - arch/x86/kvm/pmu.c|633| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/pmu.c|705| <<kvm_pmu_trigger_event>> for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/svm/pmu.c|266| <<amd_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|589| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|591| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
+	 *
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
 		      pmu->pmc_in_use, X86_PMC_IDX_MAX);
 
 	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {
+		/*
+		 * amd_pmc_idx_to_pmc()
+		 */
 		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);
 
 		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
@@ -505,11 +726,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11689| <<kvm_arch_vcpu_destroy>> kvm_pmu_destroy(vcpu);
+ */
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
 	u64 prev_count;
@@ -545,6 +774,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3532| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8535| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8806| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8808| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 5cc5721f2..662e5e111 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -4,6 +4,20 @@
 
 #include <linux/nospec.h>
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_pmu pmu;
+ *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+ *          -> u8 idx; 
+ *          -> u64 counter;
+ *          -> u64 eventsel;
+ *          -> struct perf_event *perf_event;
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> u64 current_config;
+ *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+ */
+
 #define vcpu_to_pmu(vcpu) (&(vcpu)->arch.pmu)
 #define pmu_to_vcpu(pmu)  (container_of((pmu), struct kvm_vcpu, arch.pmu))
 #define pmc_to_pmu(pmc)   (&(pmc)->vcpu->arch.pmu)
@@ -63,8 +77,34 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * [0] pmc_release_perf_event
+ * [0] kvm_pmu_cleanup
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|336| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|80| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
+	/*
+	 * struct kvm_pmc {
+	 *     enum pmc_type type;
+	 *     u8 idx;
+	 *     u64 counter;
+	 *     u64 eventsel;
+	 *     struct perf_event *perf_event;
+	 *     struct kvm_vcpu *vcpu;
+	 *     u64 current_config;
+	 *     bool is_paused;
+	 *     bool intr;
+	 * };
+	 */
 	if (pmc->perf_event) {
 		perf_event_release_kernel(pmc->perf_event);
 		pmc->perf_event = NULL;
@@ -73,6 +113,13 @@ static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|541| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|292| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|649| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -101,6 +148,21 @@ static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
  */
+/*
+ * 只被intel调用:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|79| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|200| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|249| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|268| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|269| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|411| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|412| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|422| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|497| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|498| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|512| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 					 u32 base)
 {
@@ -138,6 +200,12 @@ static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|553| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|557| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ */
 static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event || pmc->is_paused)
@@ -147,6 +215,13 @@ static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|392| <<reprogram_counter>> if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))
+ *   - arch/x86/kvm/pmu.c|639| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/pmu.c|708| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|791| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -160,10 +235,26 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11990| <<kvm_arch_hardware_setup>> kvm_init_pmu_capability();
+ */
 static inline void kvm_init_pmu_capability(void)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
 
+	/*
+	 * struct x86_pmu_capability {
+	 *     int             version;
+	 *     int             num_counters_gp;
+	 *     int             num_counters_fixed;
+	 *     int             bit_width_gp;
+	 *     int             bit_width_fixed;
+	 *     unsigned int    events_mask;
+	 *     int             events_mask_len;
+	 *     unsigned int    pebs_ept        :1;
+	 * };
+	 */
 	perf_get_x86_pmu_capability(&kvm_pmu_cap);
 
 	 /*
diff --git a/arch/x86/kvm/reverse_cpuid.h b/arch/x86/kvm/reverse_cpuid.h
index a19d473d0..08d1f76e5 100644
--- a/arch/x86/kvm/reverse_cpuid.h
+++ b/arch/x86/kvm/reverse_cpuid.h
@@ -30,6 +30,12 @@ struct cpuid_reg {
 	int reg;
 };
 
+/*
+ * 在以下使用reverse_cpuid[]:
+ *   - arch/x86/kvm/reverse_cpuid.h|67| <<reverse_cpuid_check>> BUILD_BUG_ON(x86_leaf >= ARRAY_SIZE(reverse_cpuid));
+ *   - arch/x86/kvm/reverse_cpuid.h|68| <<reverse_cpuid_check>> BUILD_BUG_ON(reverse_cpuid[x86_leaf].function == 0);
+ *   - arch/x86/kvm/reverse_cpuid.h|111| <<x86_feature_cpuid>> return reverse_cpuid[x86_leaf];
+ */
 static const struct cpuid_reg reverse_cpuid[] = {
 	[CPUID_1_EDX]         = {         1, 0, CPUID_EDX},
 	[CPUID_8000_0001_EDX] = {0x80000001, 0, CPUID_EDX},
@@ -103,6 +109,12 @@ static __always_inline u32 __feature_bit(int x86_feature)
 
 #define feature_bit(name)  __feature_bit(X86_FEATURE_##name)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|527| <<__kvm_cpu_cap_mask>> const struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);
+ *   - arch/x86/kvm/cpuid.h|97| <<guest_cpuid_get_register>> const struct cpuid_reg cpuid = x86_feature_cpuid(x86_feature);
+ *   - arch/x86/kvm/reverse_cpuid.h|135| <<cpuid_entry_get_reg>> const struct cpuid_reg cpuid = x86_feature_cpuid(x86_feature);
+ */
 static __always_inline struct cpuid_reg x86_feature_cpuid(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index f24613a10..f163b22d9 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -33,10 +33,20 @@ enum index {
 	INDEX_ERROR,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|144| <<amd_pmc_idx_to_pmc>> unsigned int base = get_msr_base(pmu, PMU_TYPE_COUNTER);
+ *
+ * 对于extention,是MSR_F15H_PERF_CTR和MSR_F15H_PERF_CTL
+ * 对于非extention,是MSR_K7_PERFCTR0和MSR_K7_EVNTSEL0
+ */
 static unsigned int get_msr_base(struct kvm_pmu *pmu, enum pmu_type type)
 {
 	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
 
+	/*
+	 * X86_FEATURE_PERFCTR_CORE是Core performance counter extensions
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE)) {
 		if (type == PMU_TYPE_COUNTER)
 			return MSR_F15H_PERF_CTR;
@@ -50,6 +60,37 @@ static unsigned int get_msr_base(struct kvm_pmu *pmu, enum pmu_type type)
 	}
 }
 
+/*
+ * INDEX_ZERO
+ *   - MSR_F15H_PERF_CTL0 : MSR_F15H_PERF_CTL = 0xc0010200
+ *   - MSR_F15H_PERF_CTR0 : MSR_F15H_PERF_CTR = 0xc0010201
+ *   - MSR_K7_EVNTSEL0    : 0xc0010000
+ *   - MSR_K7_PERFCTR0    : 0xc0010004
+ * INDEX_ONE
+ *   - MSR_F15H_PERF_CTL1 : MSR_F15H_PERF_CTL + 2 = 0xc0010202
+ *   - MSR_F15H_PERF_CTR1 : MSR_F15H_PERF_CTR + 2 = 0xc0010203
+ *   - MSR_K7_EVNTSEL1    : 0xc0010001
+ *   - MSR_K7_PERFCTR1    : 0xc0010005
+ * INDEX_TWO
+ *   - MSR_F15H_PERF_CTL2 : MSR_F15H_PERF_CTL + 4 = 0xc0010204
+ *   - MSR_F15H_PERF_CTR2 : MSR_F15H_PERF_CTR + 4 = 0xc0010205
+ *   - MSR_K7_EVNTSEL2    : 0xc0010002
+ *   - MSR_K7_PERFCTR2    : 0xc0010006
+ * INDEX_THREE
+ *   - MSR_F15H_PERF_CTL3 : MSR_F15H_PERF_CTL + 6 = 0xc0010206
+ *   - MSR_F15H_PERF_CTR3 : MSR_F15H_PERF_CTR + 6 = 0xc0010207
+ *   - MSR_K7_EVNTSEL3    : 0xc0010003
+ *   - MSR_K7_PERFCTR3    : 0xc0010007
+ * INDEX_FOUR
+ *   - MSR_F15H_PERF_CTL4 : MSR_F15H_PERF_CTL + 8 = 0xc0010208
+ *   - MSR_F15H_PERF_CTR4 : MSR_F15H_PERF_CTR + 8 = 0xc0010209
+ * INDEX_FIVE
+ *   - MSR_F15H_PERF_CTL5 : MSR_F15H_PERF_CTL + 10 = 0xc001020a
+ *   - MSR_F15H_PERF_CTR5 : MSR_F15H_PERF_CTR + 10 = 0xc001020b
+ *
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|126| <<get_gp_pmc_amd>> return &pmu->gp_counters[msr_to_index(msr)];
+ */
 static enum index msr_to_index(u32 msr)
 {
 	switch (msr) {
@@ -84,6 +125,43 @@ static enum index msr_to_index(u32 msr)
 	}
 }
 
+/*
+ * INDEX_ZERO
+ *   - MSR_F15H_PERF_CTL0 : MSR_F15H_PERF_CTL = 0xc0010200
+ *   - MSR_F15H_PERF_CTR0 : MSR_F15H_PERF_CTR = 0xc0010201
+ *   - MSR_K7_EVNTSEL0    : 0xc0010000
+ *   - MSR_K7_PERFCTR0    : 0xc0010004
+ * INDEX_ONE
+ *   - MSR_F15H_PERF_CTL1 : MSR_F15H_PERF_CTL + 2 = 0xc0010202
+ *   - MSR_F15H_PERF_CTR1 : MSR_F15H_PERF_CTR + 2 = 0xc0010203
+ *   - MSR_K7_EVNTSEL1    : 0xc0010001
+ *   - MSR_K7_PERFCTR1    : 0xc0010005
+ * INDEX_TWO
+ *   - MSR_F15H_PERF_CTL2 : MSR_F15H_PERF_CTL + 4 = 0xc0010204
+ *   - MSR_F15H_PERF_CTR2 : MSR_F15H_PERF_CTR + 4 = 0xc0010205
+ *   - MSR_K7_EVNTSEL2    : 0xc0010002
+ *   - MSR_K7_PERFCTR2    : 0xc0010006
+ * INDEX_THREE
+ *   - MSR_F15H_PERF_CTL3 : MSR_F15H_PERF_CTL + 6 = 0xc0010206
+ *   - MSR_F15H_PERF_CTR3 : MSR_F15H_PERF_CTR + 6 = 0xc0010207
+ *   - MSR_K7_EVNTSEL3    : 0xc0010003
+ *   - MSR_K7_PERFCTR3    : 0xc0010007
+ * INDEX_FOUR
+ *   - MSR_F15H_PERF_CTL4 : MSR_F15H_PERF_CTL + 8 = 0xc0010208
+ *   - MSR_F15H_PERF_CTR4 : MSR_F15H_PERF_CTR + 8 = 0xc0010209
+ * INDEX_FIVE
+ *   - MSR_F15H_PERF_CTL5 : MSR_F15H_PERF_CTL + 10 = 0xc001020a
+ *   - MSR_F15H_PERF_CTR5 : MSR_F15H_PERF_CTR + 10 = 0xc001020b
+ *
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|155| <<amd_pmc_idx_to_pmc>> return get_gp_pmc_amd(pmu, base + pmc_idx, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|193| <<amd_msr_idx_to_pmc>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|194| <<amd_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|206| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|212| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|229| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|236| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ */
 static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 					     enum pmu_type type)
 {
@@ -126,6 +204,9 @@ static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 	return &pmu->gp_counters[msr_to_index(msr)];
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.hw_event_available = amd_hw_event_available()
+ */
 static bool amd_hw_event_available(struct kvm_pmc *pmc)
 {
 	return true;
@@ -134,11 +215,17 @@ static bool amd_hw_event_available(struct kvm_pmc *pmc)
 /* check if a PMC is enabled by comparing it against global_ctrl bits. Because
  * AMD CPU doesn't have global_ctrl MSR, all PMCs are enabled (return TRUE).
  */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_is_enabled = amd_pmc_is_enabled()
+ */
 static bool amd_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	unsigned int base = get_msr_base(pmu, PMU_TYPE_COUNTER);
@@ -155,6 +242,9 @@ static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 	return get_gp_pmc_amd(pmu, base + pmc_idx, PMU_TYPE_COUNTER);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_rdpmc_ecx = amd_id_valid_rdpmc_ecx()
+ */
 static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -165,6 +255,9 @@ static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 }
 
 /* idx is the ECX register of RDPMC instruction */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc()
+ */
 static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	unsigned int idx, u64 *mask)
 {
@@ -179,12 +272,18 @@ static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[idx];
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_msr = amd_is_valid_msr()
+ */
 static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	/* All MSRs refer to exactly one PMC, so msr_idx_to_pmc is enough.  */
 	return false;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.msr_idx_to_pmc = amd_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -196,6 +295,9 @@ static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 	return pmc;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.get_msr = amd_pmu_get_msr()
+ */
 static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -218,6 +320,9 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.set_msr = amd_pmu_set_msr()
+ */
 static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -246,10 +351,20 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.refresh = amd_pmu_refresh()
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|335| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|564| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|587| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE))
 		pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
 	else
@@ -261,11 +376,21 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 	pmu->version = 1;
 	/* not applicable to AMD; but clean them to prevent any fall out */
 	pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	pmu->nr_arch_fixed_counters = 0;
 	pmu->global_status = 0;
 	bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.init = amd_pmu_init()
+ */
 static void amd_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -281,6 +406,9 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.reset = amd_pmu_reset()
+ */
 static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index f3813dbac..cb18fbff3 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -744,6 +744,11 @@ static bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)
 	return !!test_bit(bit_write,  &tmp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|788| <<set_msr_interception>> set_msr_interception_bitmap(vcpu, msrpm, msr, read, write);
+ *   - arch/x86/kvm/svm/svm.c|861| <<svm_msr_filter_changed>> set_msr_interception_bitmap(vcpu, svm->msrpm, msr, read, write);
+ */
 static void set_msr_interception_bitmap(struct kvm_vcpu *vcpu, u32 *msrpm,
 					u32 msr, int read, int write)
 {
@@ -1351,6 +1356,13 @@ static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 		__svm_vcpu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|804| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/nested.c|1021| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1177| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1667| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ */
 void svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)
 {
 	svm->current_vmcb = target_vmcb;
@@ -3067,6 +3079,11 @@ static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 	return 0;
 }
 
+/*
+ * 在以下使用msr_interception():
+ *   - arch/x86/kvm/svm/svm.c|3197| <<global>> [SVM_EXIT_MSR] = msr_interception,
+ *   - arch/x86/kvm/svm/svm.c|3373| <<svm_invoke_exit_handler>> return msr_interception(vcpu);
+ */
 static int msr_interception(struct kvm_vcpu *vcpu)
 {
 	if (to_svm(vcpu)->vmcb->control.exit_info_1)
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index c5e5dfef6..3fdeef0b9 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -21,6 +21,28 @@ extern int __read_mostly pt_mode;
 #define PT_MODE_SYSTEM		0
 #define PT_MODE_HOST_GUEST	1
 
+/*
+ * version 1:
+ *
+ * IA32_PMCx MSRs 从 0x0c1开始
+ * IA32_PERFEVTSELx MSRs 从0x186开始
+ *
+ * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+ * IA32_PMCx从0x4c1开始
+ *
+ * kvm-unit-tests中:
+ *
+ * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+ * 821                 gp_counter_base = MSR_IA32_PMC0;
+ * 822                 report_prefix_push("full-width writes");
+ * 823                 check_counters();
+ * 824                 check_gp_counters_write_width();
+ * 825         }
+ *
+ * 在以下使用PMU_CAP_FW_WRITES:
+ *   - arch/x86/kvm/vmx/capabilities.h|406| <<vmx_get_perf_capabilities>> u64 perf_cap = PMU_CAP_FW_WRITES;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+ */
 #define PMU_CAP_FW_WRITES	(1ULL << 13)
 #define PMU_CAP_LBR_FMT		0x3f
 
@@ -403,6 +425,28 @@ static inline bool vmx_pebs_supported(void)
 
 static inline u64 vmx_get_perf_capabilities(void)
 {
+	/*
+	 * version 1:
+	 *
+	 * IA32_PMCx MSRs 从 0x0c1开始
+	 * IA32_PERFEVTSELx MSRs 从0x186开始
+	 *
+	 * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+	 * IA32_PMCx从0x4c1开始
+	 *
+	 * kvm-unit-tests中:
+	 *
+	 * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+	 * 821                 gp_counter_base = MSR_IA32_PMC0;
+	 * 822                 report_prefix_push("full-width writes");
+	 * 823                 check_counters();
+	 * 824                 check_gp_counters_write_width();
+	 * 825         }
+	 *
+	 * 在以下使用PMU_CAP_FW_WRITES:
+	 *   - arch/x86/kvm/vmx/capabilities.h|406| <<vmx_get_perf_capabilities>> u64 perf_cap = PMU_CAP_FW_WRITES;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+	 */
 	u64 perf_cap = PMU_CAP_FW_WRITES;
 	u64 host_perf_cap = 0;
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index c399637a3..d87a86893 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -18,8 +18,21 @@
 #include "nested.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用MSR_PMC_FULL_WIDTH_BIT:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_set_msr>> if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|507| <<intel_pmu_set_msr>> !(msr & MSR_PMC_FULL_WIDTH_BIT))
+ */
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|100| <<intel_hw_event_available>> for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|101| <<intel_hw_event_available>> if (intel_arch_events[i].eventsel != event_select ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|102| <<intel_hw_event_available>> intel_arch_events[i].unit_mask != unit_mask)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+ *   - arch/x86/kvm/vmx/pmu_intel.c|507| <<setup_fixed_pmc_eventsel>> intel_arch_events[event].eventsel;
+ */
 static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
 	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
@@ -33,8 +46,18 @@ static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 };
 
 /* mapping between fixed pmc index and intel_arch_events array */
+/*
+ * 在以下使用fixed_pmc_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|492| <<setup_fixed_pmc_eventsel>> size_t size = ARRAY_SIZE(fixed_pmc_events);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|499| <<setup_fixed_pmc_eventsel>> event = fixed_pmc_events[array_index_nospec(i, size)];
+ *   - arch/x86/kvm/vmx/pmu_intel.c|553| <<intel_pmu_refresh>> min3(ARRAY_SIZE(fixed_pmc_events),
+ */
 static int fixed_pmc_events[] = {1, 0, 7};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|480| <<intel_pmu_set_msr(MSR_CORE_PERF_FIXED_CTR_CTRL)>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	struct kvm_pmc *pmc;
@@ -42,6 +65,13 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 	int i;
 
 	pmu->fixed_ctr_ctrl = data;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	for (i = 0; i < pmu->nr_arch_fixed_counters; i++) {
 		u8 new_ctrl = fixed_ctrl_field(data, i);
 		u8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);
@@ -56,6 +86,9 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_idx_to_pmc = intel_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	if (pmc_idx < INTEL_PMC_IDX_FIXED) {
@@ -84,6 +117,9 @@ static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.hw_event_available = 
+ */
 static bool intel_hw_event_available(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -107,6 +143,9 @@ static bool intel_hw_event_available(struct kvm_pmc *pmc)
 }
 
 /* check if a PMC is enabled by comparing it with globl_ctrl bits. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_is_enabled = 
+ */
 static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -117,6 +156,9 @@ static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_rdpmc_ecx =
+ */
 static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -128,6 +170,9 @@ static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 		     : idx < pmu->nr_arch_gp_counters;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.rdpmc_ecx_to_pmc = 
+ */
 static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 					    unsigned int idx, u64 *mask)
 {
@@ -150,6 +195,13 @@ static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[array_index_nospec(idx, num_counters)];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|224| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|270| <<intel_is_valid_msr>> ret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|276| <<intel_is_valid_msr>> perf_capabilities = vcpu_get_perf_capabilities(vcpu);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|672| <<intel_pmu_refresh>> perf_capabilities = vcpu_get_perf_capabilities(vcpu);
+ */
 static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
 {
 	if (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))
@@ -158,6 +210,26 @@ static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
 	return vcpu->arch.perf_capabilities;
 }
 
+/*
+ * version 1:
+ *
+ * IA32_PMCx MSRs 从 0x0c1开始
+ * IA32_PERFEVTSELx MSRs 从0x186开始
+ *
+ * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+ * IA32_PMCx从0x4c1开始
+ *
+ * 在kvm-unit-tests
+ *
+ * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+ * 821                 gp_counter_base = MSR_IA32_PMC0;822                 report_prefix_push("full-width writes");
+ * 823                 check_counters();
+ * 824                 check_gp_counters_write_width();
+ * 825         }
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|197| <<get_fw_gp_pmc>> if (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))
+ */
 static inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)
 {
 	return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
@@ -189,6 +261,9 @@ static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr =
+ */
 static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -224,6 +299,9 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.msr_idx_to_pmc =
+ */
 static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -341,6 +419,9 @@ static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.get_msr = intel_pmu_get_msr()
+ */
 static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -391,6 +472,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.set_msr = intel_pmu_set_msr()
+ */
 static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -453,6 +537,10 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		}
 		break;
 	default:
+		/*
+		 * MSR_IA32_PERFCTR0 0x000000c1
+		 * MSR_IA32_PMC0     0x000004c1
+		 */
 		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
 		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
 			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
@@ -487,6 +575,10 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|604| <<intel_pmu_refresh>> setup_fixed_pmc_eventsel(pmu);
+ */
 static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 {
 	size_t size = ARRAY_SIZE(fixed_pmc_events);
@@ -502,6 +594,9 @@ static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.refresh = intel_pmu_refresh()
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -611,6 +706,9 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.init = intel_pmu_init()
+ */
 static void intel_pmu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -637,6 +735,9 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 	lbr_desc->msr_passthrough = false;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.reset = intel_pmu_reset()
+ */
 static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -680,6 +781,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.deliver_pmi = intel_pmu_deliver_pmi()
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
@@ -767,6 +871,9 @@ void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
 		vcpu->vcpu_id);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.cleanup = intel_pmu_cleanup()
+ */
 static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c9b49a09e..fba3cbfdc 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -2006,6 +2006,10 @@ static u64 nested_vmx_truncate_sysenter_addr(struct kvm_vcpu *vcpu,
 	return (unsigned long)data;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2091| <<vmx_set_msr>> u64 invalid = data & ~vcpu_supported_debugctl(vcpu);
+ */
 static u64 vcpu_supported_debugctl(struct kvm_vcpu *vcpu)
 {
 	u64 debugctl = vmx_supported_debugctl();
@@ -6764,6 +6768,11 @@ static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 
 void vmx_do_interrupt_nmi_irqoff(unsigned long entry);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6817| <<handle_exception_nmi_irqoff>> handle_interrupt_nmi_irqoff(&vmx->vcpu, nmi_entry);
+ *   - arch/x86/kvm/vmx/vmx.c|6830| <<handle_external_interrupt_irqoff>> handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));
+ */
 static void handle_interrupt_nmi_irqoff(struct kvm_vcpu *vcpu,
 					unsigned long entry)
 {
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index b0c47b41c..2acae6c65 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -742,6 +742,14 @@ void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 EXPORT_SYMBOL_GPL(kvm_inject_page_fault);
 
 /* Returns true if the page fault was immediately morphed into a VM-Exit. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|814| <<FNAME>> kvm_inject_emulated_page_fault(vcpu, &walker.fault);
+ *   - arch/x86/kvm/vmx/sgx.c|83| <<sgx_gva_to_gpa>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/vmx/sgx.c|228| <<handle_encls_ecreate>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/x86.c|8252| <<inject_emulated_exception>> return kvm_inject_emulated_page_fault(vcpu, &ctxt->exception);
+ *   - arch/x86/kvm/x86.c|13229| <<kvm_handle_memory_failure>> kvm_inject_emulated_page_fault(vcpu, e);
+ */
 bool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 				    struct x86_exception *fault)
 {
@@ -1399,6 +1407,16 @@ EXPORT_SYMBOL_GPL(kvm_emulate_rdpmc);
  * may depend on host virtualization features rather than host cpu features.
  */
 
+/*
+ * 在以下使用msrs_to_save_all[]:
+ *   - arch/x86/kvm/x86.c|6897| <<kvm_init_msr_list>> for (i = 0; i < ARRAY_SIZE(msrs_to_save_all); i++) {
+ *   - arch/x86/kvm/x86.c|6898| <<kvm_init_msr_list>> if (rdmsr_safe(msrs_to_save_all[i], &dummy[0], &dummy[1]) < 0)
+ *   - arch/x86/kvm/x86.c|6905| <<kvm_init_msr_list>> switch (msrs_to_save_all[i]) {
+ *   - arch/x86/kvm/x86.c|6938| <<kvm_init_msr_list>> msrs_to_save_all[i] - MSR_IA32_RTIT_ADDR0_A >=
+ *   - arch/x86/kvm/x86.c|6943| <<kvm_init_msr_list>> if (msrs_to_save_all[i] - MSR_ARCH_PERFMON_PERFCTR0 >=
+ *   - arch/x86/kvm/x86.c|6948| <<kvm_init_msr_list>> if (msrs_to_save_all[i] - MSR_ARCH_PERFMON_EVENTSEL0 >=
+ *   - arch/x86/kvm/x86.c|6961| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static const u32 msrs_to_save_all[] = {
 	MSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,
 	MSR_STAR,
@@ -1449,7 +1467,20 @@ static const u32 msrs_to_save_all[] = {
 	MSR_IA32_XFD, MSR_IA32_XFD_ERR,
 };
 
+/*
+ * 在以下使用msrs_to_save[ARRAY_SIZE(msrs_to_save_all)]:
+ *   - arch/x86/kvm/x86.c|4582| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6975| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];
+/*
+ * 在以下使用num_msrs_to_save:
+ *   - arch/x86/kvm/x86.c|4580| <<kvm_arch_dev_ioctl>> msr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;
+ *   - arch/x86/kvm/x86.c|4588| <<kvm_arch_dev_ioctl>> num_msrs_to_save * sizeof(u32)))
+ *   - arch/x86/kvm/x86.c|4590| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices + num_msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6912| <<kvm_init_msr_list>> num_msrs_to_save = 0;
+ *   - arch/x86/kvm/x86.c|6980| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static unsigned num_msrs_to_save;
 
 static const u32 emulated_msrs_all[] = {
@@ -1524,6 +1555,13 @@ static unsigned num_emulated_msrs;
  * List of msr numbers which are used to expose MSR-based features that
  * can be used by a hypervisor to validate requested CPU features.
  */
+/*
+ * 在以下使用msr_based_features_all[]:
+ *   - arch/x86/kvm/x86.c|1568| <<global>> static u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];
+ *   - arch/x86/kvm/x86.c|6990| <<kvm_init_msr_list>> for (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {
+ *   - arch/x86/kvm/x86.c|6993| <<kvm_init_msr_list>> msr.index = msr_based_features_all[i];
+ *   - arch/x86/kvm/x86.c|6997| <<kvm_init_msr_list>> msr_based_features[num_msr_based_features++] = msr_based_features_all[i];
+ */
 static const u32 msr_based_features_all[] = {
 	MSR_IA32_VMX_BASIC,
 	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
@@ -1733,6 +1771,15 @@ void kvm_enable_efer_bits(u64 mask)
 }
 EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|762| <<set_msr_interception_bitmap>> if (read && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/svm/svm.c|765| <<set_msr_interception_bitmap>> if (write && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE))
+ *   - arch/x86/kvm/vmx/vmx.c|3922| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {
+ *   - arch/x86/kvm/vmx/vmx.c|3928| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {
+ *   - arch/x86/kvm/x86.c|1939| <<kvm_get_msr_with_filter>> if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/x86.c|1946| <<kvm_set_msr_with_filter>> if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_WRITE))
+ */
 bool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)
 {
 	struct kvm_x86_msr_filter *msr_filter;
@@ -1782,6 +1829,11 @@ EXPORT_SYMBOL_GPL(kvm_msr_allowed);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1893| <<kvm_set_msr_ignored_check>> int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
+ *   - arch/x86/kvm/x86.c|11848| <<kvm_vcpu_reset>> __kvm_set_msr(vcpu, MSR_IA32_XSS, 0, true);
+ */
 static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 			 bool host_initiated)
 {
@@ -1844,6 +1896,12 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	return static_call(kvm_x86_set_msr)(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1957| <<kvm_set_msr_with_filter>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|1968| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2207| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
@@ -3570,6 +3628,12 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 
 		/* Handle McStatusWrEn */
 		if (data == BIT_ULL(18)) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->msr_hwcr:
+			 *   - arch/x86/kvm/x86.c|3282| <<can_set_mci_status>> return !!(vcpu->arch.msr_hwcr & BIT_ULL(18));
+			 *   - arch/x86/kvm/x86.c|3623| <<kvm_set_msr_common>> vcpu->arch.msr_hwcr = data;
+			 *   - arch/x86/kvm/x86.c|4249| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_hwcr;
+			 */
 			vcpu->arch.msr_hwcr = data;
 		} else if (data != 0) {
 			vcpu_unimpl(vcpu, "unimplemented HWCR wrmsr: 0x%llx\n",
@@ -4316,6 +4380,10 @@ static int kvm_ioctl_get_supported_hv_cpuid(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4489| <<kvm_vm_ioctl_check_extension_generic>> return kvm_vm_ioctl_check_extension(kvm, arg);
+ */
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r = 0;
@@ -4489,6 +4557,10 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	}
 	case KVM_CAP_PMU_CAPABILITY:
+		/*
+		 * 没见到QEMU用KVM_PMU_CAP_DISABLE
+		 * 也没见到QEMU用KVM_CAP_PMU_CAPABILITY
+		 */
 		r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
 		break;
 	case KVM_CAP_DISABLE_QUIRKS2:
@@ -4554,6 +4626,12 @@ long kvm_arch_dev_ioctl(struct file *filp,
 
 	switch (ioctl) {
 	case KVM_GET_MSR_INDEX_LIST: {
+		/*
+		 * struct kvm_msr_list {
+		 *     __u32 nmsrs; // number of msrs in entries
+		 *     __u32 indices[];
+		 * };
+		 */
 		struct kvm_msr_list __user *user_msr_list = argp;
 		struct kvm_msr_list msr_list;
 		unsigned n;
@@ -6264,11 +6342,21 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		break;
 	case KVM_CAP_PMU_CAPABILITY:
 		r = -EINVAL;
+		/*
+		 * QEMU-7.1没有KVM_CAP_PMU_VALID_MASK
+		 *
+		 * 如果global disable了,返回error
+		 * 如果想修改不允许修改的bit,返回error
+		 */
 		if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
 			break;
 
 		mutex_lock(&kvm->lock);
 		if (!kvm->created_vcpus) {
+			/*
+			 * 没见到QEMU用KVM_PMU_CAP_DISABLE
+			 * 也没见到QEMU用KVM_CAP_PMU_CAPABILITY
+			 */
 			kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
 			r = 0;
 		}
@@ -6882,6 +6970,10 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12019| <<kvm_arch_hardware_setup>> kvm_init_msr_list();
+ */
 static void kvm_init_msr_list(void)
 {
 	u32 dummy[2];
@@ -10250,6 +10342,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10777| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10428,6 +10524,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 
 	preempt_disable();
 
+	/*
+	 * vmx_prepare_switch_to_guest()
+	 */
 	static_call(kvm_x86_prepare_switch_to_guest)(vcpu);
 
 	/*
@@ -10510,6 +10609,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
+		/*
+		 * vmx_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -10560,6 +10662,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * vmx_handle_exit_irqoff()
+	 */
 	static_call(kvm_x86_handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -10606,6 +10711,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/*
+	 * vmx_handle_exit()
+	 */
 	r = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -11699,6 +11807,12 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		static_branch_dec(&kvm_has_noapic_vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3077| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2126| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11633| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -11949,6 +12063,10 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11992| <<kvm_arch_hardware_setup>> kvm_ops_update(ops);
+ */
 static inline void kvm_ops_update(struct kvm_x86_init_ops *ops)
 {
 	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
@@ -12051,6 +12169,16 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
+		/*
+		 * 在以下使用kvm_pmu->need_cleanup:
+		 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+		 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+		 *
+		 * The gate to release perf_events not marked in
+		 * pmc_in_use only once in a vcpu time slice.
+		 */
 		pmu->need_cleanup = true;
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
@@ -13102,6 +13230,17 @@ EXPORT_SYMBOL_GPL(kvm_fixup_and_inject_pf_error);
  * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value
  * indicates whether exit to userspace is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3818| <<vmx_complete_nested_posted_interrupt>> kvm_handle_memory_failure(vcpu, X86EMUL_IO_NEEDED, NULL);
+ *   - arch/x86/kvm/vmx/nested.c|4843| <<nested_vmx_get_vmptr>> *ret = kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5175| <<handle_vmread>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5248| <<handle_vmwrite>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5418| <<handle_vmptrst>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5464| <<handle_invept>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5547| <<handle_invvpid>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/x86.c|13258| <<kvm_handle_invpcid>> return kvm_handle_memory_failure(vcpu, r, &e);
+ */
 int kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,
 			      struct x86_exception *e)
 {
diff --git a/arch/x86/mm/extable.c b/arch/x86/mm/extable.c
index 60814e110..94a24fa5d 100644
--- a/arch/x86/mm/extable.c
+++ b/arch/x86/mm/extable.c
@@ -144,6 +144,13 @@ static bool ex_handler_copy(const struct exception_table_entry *fixup,
 	return ex_handler_fault(fixup, regs, trapnr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/extable.c|251| <<fixup_exception>> return ex_handler_msr(e, regs, true, false, reg);
+ *   - arch/x86/mm/extable.c|253| <<fixup_exception>> return ex_handler_msr(e, regs, false, false, reg);
+ *   - arch/x86/mm/extable.c|255| <<fixup_exception>> return ex_handler_msr(e, regs, true, true, reg);
+ *   - arch/x86/mm/extable.c|257| <<fixup_exception>> return ex_handler_msr(e, regs, false, true, reg);
+ */
 static bool ex_handler_msr(const struct exception_table_entry *fixup,
 			   struct pt_regs *regs, bool wrmsr, bool safe, int reg)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index c96c8c4f7..2506ba904 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -3301,6 +3301,14 @@ static inline bool blk_mq_last_cpu_in_hctx(unsigned int cpu,
 	return true;
 }
 
+/*
+ * [0] blk_mq_hctx_notify_offline
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,
diff --git a/drivers/acpi/cppc_acpi.c b/drivers/acpi/cppc_acpi.c
index 1e15a9f25..1851110d4 100644
--- a/drivers/acpi/cppc_acpi.c
+++ b/drivers/acpi/cppc_acpi.c
@@ -44,8 +44,21 @@
 
 struct cppc_pcc_data {
 	struct pcc_mbox_chan *pcc_channel;
+	/*
+	 * 在以下使用cpcc_acpi.c的cppc_pcc_data->pcc_comm_addr:
+	 *   - drivers/acpi/cppc_acpi.c|95| <<GET_PCC_VADDR>> #define GET_PCC_VADDR(offs, pcc_ss_id) (pcc_data[pcc_ss_id]->pcc_comm_addr + \
+	 *   - drivers/acpi/cppc_acpi.c|206| <<check_pcc_chan>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|241| <<send_pcc_cmd>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|551| <<register_pcc_channel>> pcc_data[pcc_ss_idx]->pcc_comm_addr = acpi_os_ioremap(pcc_chan->shmem_base_addr, pcc_chan->shmem_size);
+	 *   - drivers/acpi/cppc_acpi.c|554| <<register_pcc_channel>> if (!pcc_data[pcc_ss_idx]->pcc_comm_addr) {
+	 */
 	void __iomem *pcc_comm_addr;
 	bool pcc_channel_acquired;
+	/*
+	 * 在以下使用cpcc_acpi.c的cppc_pcc_data->deadline_us:
+	 *   - drivers/acpi/cppc_acpi.c|217| <<check_pcc_chan>> pcc_ss_data->deadline_us);
+	 *   - drivers/acpi/cppc_acpi.c|546| <<register_pcc_channel>> pcc_data[pcc_ss_idx]->deadline_us = usecs_lat;
+	 */
 	unsigned int deadline_us;
 	unsigned int pcc_mpar, pcc_mrtt, pcc_nominal;
 
@@ -198,10 +211,24 @@ static struct kobj_type cppc_ktype = {
 	.default_groups = cppc_groups,
 };
 
+/*
+ * called by:
+ *   - drivers/acpi/cppc_acpi.c|257| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, false);
+ *   - drivers/acpi/cppc_acpi.c|319| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, true);
+ *   - drivers/acpi/cppc_acpi.c|1405| <<cppc_set_perf>> ret = check_pcc_chan(pcc_ss_id, false);
+ */
 static int check_pcc_chan(int pcc_ss_id, bool chk_err_bit)
 {
 	int ret, status;
 	struct cppc_pcc_data *pcc_ss_data = pcc_data[pcc_ss_id];
+	/*
+	 * 在以下使用cpcc_acpi.c的cppc_pcc_data->pcc_comm_addr:
+	 *   - drivers/acpi/cppc_acpi.c|95| <<GET_PCC_VADDR>> #define GET_PCC_VADDR(offs, pcc_ss_id) (pcc_data[pcc_ss_id]->pcc_comm_addr + \
+	 *   - drivers/acpi/cppc_acpi.c|206| <<check_pcc_chan>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|241| <<send_pcc_cmd>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|551| <<register_pcc_channel>> pcc_data[pcc_ss_idx]->pcc_comm_addr = acpi_os_ioremap(pcc_chan->shmem_base_addr, pcc_chan->shmem_size);
+	 *   - drivers/acpi/cppc_acpi.c|554| <<register_pcc_channel>> if (!pcc_data[pcc_ss_idx]->pcc_comm_addr) {
+	 */
 	struct acpi_pcct_shared_memory __iomem *generic_comm_base =
 		pcc_ss_data->pcc_comm_addr;
 
@@ -233,6 +260,15 @@ static int check_pcc_chan(int pcc_ss_id, bool chk_err_bit)
  * This function transfers the ownership of the PCC to the platform
  * So it must be called while holding write_lock(pcc_lock)
  */
+/*
+ * called by:
+ *   - drivers/acpi/cppc_acpi.c|255| <<send_pcc_cmd>> send_pcc_cmd(pcc_ss_id, CMD_WRITE);
+ *   - drivers/acpi/cppc_acpi.c|1113| <<cppc_get_perf>> if (send_pcc_cmd(pcc_ss_id, CMD_READ) >= 0)
+ *   - drivers/acpi/cppc_acpi.c|1196| <<cppc_get_perf_caps>> if (send_pcc_cmd(pcc_ss_id, CMD_READ) < 0) {
+ *   - drivers/acpi/cppc_acpi.c|1288| <<cppc_get_perf_ctrs>> if (send_pcc_cmd(pcc_ss_id, CMD_READ) < 0) {
+ *   - drivers/acpi/cppc_acpi.c|1359| <<cppc_set_enable>> ret = send_pcc_cmd(pcc_ss_id, CMD_WRITE);
+ *   - drivers/acpi/cppc_acpi.c|1478| <<cppc_set_perf>> send_pcc_cmd(pcc_ss_id, CMD_WRITE);
+ */
 static int send_pcc_cmd(int pcc_ss_id, u16 cmd)
 {
 	int ret = -EIO, i;
@@ -316,6 +352,12 @@ static int send_pcc_cmd(int pcc_ss_id, u16 cmd)
 	}
 
 	/* wait for completion and check for PCC error bit */
+	/*
+	 * called by:
+	 *   - drivers/acpi/cppc_acpi.c|257| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, false);
+	 *   - drivers/acpi/cppc_acpi.c|319| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, true);
+	 *   - drivers/acpi/cppc_acpi.c|1405| <<cppc_set_perf>> ret = check_pcc_chan(pcc_ss_id, false);
+	 */
 	ret = check_pcc_chan(pcc_ss_id, true);
 
 	if (pcc_ss_data->pcc_mrtt)
diff --git a/drivers/acpi/utils.c b/drivers/acpi/utils.c
index 5a7b8065e..9e13b1f4e 100644
--- a/drivers/acpi/utils.c
+++ b/drivers/acpi/utils.c
@@ -610,6 +610,23 @@ EXPORT_SYMBOL(acpi_execute_simple_method);
  *
  * Evaluate device's _EJ0 method for hotplug operations.
  */
+/*
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_evaluate_ej0
+ * [0] acpiphp_disable_and_eject_slot
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/dock.c|404| <<handle_eject_request>> acpi_evaluate_ej0(ds->handle);
+ *   - drivers/acpi/scan.c|272| <<acpi_scan_hot_remove>> status = acpi_evaluate_ej0(handle);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|1010| <<acpiphp_disable_and_eject_slot>> if (ACPI_FAILURE(acpi_evaluate_ej0(handle)))
+ */
 acpi_status acpi_evaluate_ej0(acpi_handle handle)
 {
 	acpi_status status;
diff --git a/drivers/block/ublk_drv.c b/drivers/block/ublk_drv.c
index 6a4a94b4c..fb480fea4 100644
--- a/drivers/block/ublk_drv.c
+++ b/drivers/block/ublk_drv.c
@@ -561,6 +561,10 @@ static inline bool ubq_daemon_is_dying(struct ublk_queue *ubq)
 }
 
 /* todo: handle partial completion */
+/*
+ * called by:
+ *   - drivers/block/ublk_drv.c|892| <<ublk_commit_completion>> ublk_complete_rq(req);
+ */
 static void ublk_complete_rq(struct request *req)
 {
 	struct ublk_queue *ubq = req->mq_hctx->driver_data;
@@ -873,6 +877,10 @@ static int ublk_ch_mmap(struct file *filp, struct vm_area_struct *vma)
 	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
 }
 
+/*
+ * 处理UBLK_IO_COMMIT_AND_FETCH_REQ:
+ *   - drivers/block/ublk_drv.c|1113| <<ublk_ch_uring_cmd>> ublk_commit_completion(ub, ub_cmd);
+ */
 static void ublk_commit_completion(struct ublk_device *ub,
 		struct ublksrv_io_cmd *ub_cmd)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index dd9a05174..0ac47ec87 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -322,6 +322,11 @@ static blk_status_t virtblk_prep_rq(struct blk_mq_hw_ctx *hctx,
 	if (unlikely(status))
 		return status;
 
+	/*
+	 * struct virtblk_req *vbr:
+	 * -> struct sg_table sg_table;
+	 *    -> unsigned int nents;
+	 */
 	vbr->sg_table.nents = virtblk_map_data(hctx, req, vbr);
 	if (unlikely(vbr->sg_table.nents < 0)) {
 		virtblk_cleanup_cmd(req);
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 9cce7dec7..f6dac1e65 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -2746,6 +2746,10 @@ static int virtnet_get_link_ksettings(struct net_device *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2819| <<virtnet_set_coalesce>> ret = virtnet_send_notf_coal_cmds(vi, ec);
+ */
 static int virtnet_send_notf_coal_cmds(struct virtnet_info *vi,
 				       struct ethtool_coalesce *ec)
 {
@@ -3961,6 +3965,29 @@ static void remove_vq_common(struct virtnet_info *vi)
 	virtnet_del_vqs(vi);
 }
 
+/*
+ * [0] virtnet_remove
+ * [0] virtio_dev_remove
+ * [0] device_release_driver_internal
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] unregister_virtio_device
+ * [0] virtio_pci_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] pci_stop_bus_device
+ * [0] pci_stop_and_remove_bus_device
+ * [0] disable_slot
+ * [0] acpiphp_disable_and_eject_slot
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void virtnet_remove(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 27a11cc08..659e32ffc 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -877,6 +877,13 @@ static int xennet_close(struct net_device *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|899| <<xennet_uninit>> xennet_destroy_queues(np);
+ *   - drivers/net/xen-netfront.c|2292| <<talk_to_netback>> xennet_destroy_queues(info);
+ *   - drivers/net/xen-netfront.c|2402| <<talk_to_netback>> xennet_destroy_queues(info);
+ *   - drivers/net/xen-netfront.c|2656| <<xennet_remove>> xennet_destroy_queues(info);
+ */
 static void xennet_destroy_queues(struct netfront_info *info)
 {
 	unsigned int i;
diff --git a/drivers/pci/iov.c b/drivers/pci/iov.c
index 952217572..8c0f8b2a8 100644
--- a/drivers/pci/iov.c
+++ b/drivers/pci/iov.c
@@ -1201,6 +1201,13 @@ EXPORT_SYMBOL_GPL(pci_sriov_get_totalvfs);
  * before enabling SR-IOV.  Return value is negative on error, or number of
  * VFs allocated on success.
  */
+/*
+ * 在以下使用pci_sriov_configure_simple():
+ *   - drivers/misc/pci_endpoint_test.c|990| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/net/ethernet/amazon/ena/ena_netdev.c|4543| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/nvme/host/pci.c|3562| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/pci/pci-pf-stub.c|38| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ */
 int pci_sriov_configure_simple(struct pci_dev *dev, int nr_virtfn)
 {
 	int rc;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 578c4b6d0..d85a90741 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -599,6 +599,11 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|653| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|711| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+ */
 static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 {
 	DECLARE_COMPLETION_ONSTACK(comp);
@@ -708,6 +713,11 @@ static int virtscsi_abort(struct scsi_cmnd *sc)
 		.lun[3] = sc->device->lun & 0xff,
 		.tag = cpu_to_virtio64(vscsi->vdev, (unsigned long)sc),
 	};
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|653| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+	 *   - drivers/scsi/virtio_scsi.c|711| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+	 */
 	return virtscsi_tmf(vscsi, cmd);
 }
 
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index 8351c974c..1cd7507a3 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -327,6 +327,10 @@ static void iblock_complete_cmd(struct se_cmd *cmd)
 	kfree(ibr);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|367| <<iblock_get_bio>> bio->bi_end_io = &iblock_bio_done;
+ */
 static void iblock_bio_done(struct bio *bio)
 {
 	struct se_cmd *cmd = bio->bi_private;
@@ -346,6 +350,13 @@ static void iblock_bio_done(struct bio *bio)
 	iblock_complete_cmd(cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|525| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+ *   - drivers/target/target_core_iblock.c|538| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+ *   - drivers/target/target_core_iblock.c|766| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+ *   - drivers/target/target_core_iblock.c|799| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+ */
 static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 				  blk_opf_t opf)
 {
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 7838dc20f..149477d3d 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -870,6 +870,11 @@ static bool target_cmd_interrupted(struct se_cmd *cmd)
 }
 
 /* May be called from interrupt context so must not sleep. */
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|917| <<target_complete_cmd>> target_complete_cmd_with_sense(cmd, scsi_status, scsi_status ?
+ *   - drivers/target/target_core_xcopy.c|781| <<target_xcopy_do_work>> target_complete_cmd_with_sense(ec_cmd, SAM_STAT_CHECK_CONDITION, sense_rc);
+ */
 void target_complete_cmd_with_sense(struct se_cmd *cmd, u8 scsi_status,
 				    sense_reason_t sense_reason)
 {
@@ -1941,6 +1946,17 @@ static void target_complete_tmr_failure(struct work_struct *work)
  * Callable from all contexts.
  **/
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1599| <<srpt_handle_tsk_mgmt>> rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,
+ *   - drivers/scsi/elx/efct/efct_lio.c|1462| <<efct_scsi_recv_tmf>> rc = target_submit_tmr(&ocp->cmd, se_sess, NULL, lun, ocp, tmr_func,
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2799| <<ibmvscsis_parse_task>> rc = target_submit_tmr(&cmd->se_cmd, nexus->se_sess, NULL,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|627| <<tcm_qla2xxx_handle_tmr>> return target_submit_tmr(se_cmd, sess->se_sess, NULL, lun, mcmd,
+ *   - drivers/target/loopback/tcm_loop.c|223| <<tcm_loop_issue_tmr>> rc = target_submit_tmr(se_cmd, se_sess, tl_cmd->tl_sense_buf, lun,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|370| <<ft_send_tm>> rc = target_submit_tmr(&cmd->se_cmd, cmd->sess->se_sess,
+ *   - drivers/vhost/scsi.c|1297| <<vhost_scsi_handle_tmf>> if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
+ *   - drivers/xen/xen-scsiback.c|638| <<scsiback_device_action>> rc = target_submit_tmr(&pending_req->se_cmd, nexus->tvn_se_sess,
+ */
 int target_submit_tmr(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *sense, u64 unpacked_lun,
 		void *fabric_tmr_ptr, unsigned char tm_type,
@@ -2238,6 +2254,28 @@ static bool target_handle_task_attr(struct se_cmd *cmd)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1656| <<isert_rdma_read_done>> target_execute_cmd(se_cmd);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1340| <<srpt_rdma_read_done>> target_execute_cmd(&ioctx->cmd);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|2749| <<srpt_write_pending>> target_execute_cmd(&ioctx->cmd);
+ *   - drivers/scsi/elx/efct/efct_lio.c|632| <<efct_lio_datamove_done>> target_execute_cmd(&io->tgt_io.cmd);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|3764| <<ibmvscsis_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|549| <<tcm_qla2xxx_handle_data_work>> return target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target.c|1723| <<iscsit_check_dataout_payload>> target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_erl1.c|924| <<iscsit_execute_cmd>> target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_tmr.c|256| <<iscsit_task_reassign_complete_write>> target_execute_cmd(se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_tmr.c|256| <<iscsit_task_reassign_complete_write>> target_execute_cmd(se_cmd);
+ *   - drivers/target/loopback/tcm_loop.c|547| <<tcm_loop_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/target/sbp/sbp_target.c|1732| <<sbp_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/target/target_core_transport.c|2809| <<transport_generic_new_cmd>> target_execute_cmd(cmd);
+ *   - drivers/target/target_core_xcopy.c|570| <<target_xcopy_issue_pt_cmd>> target_execute_cmd(se_cmd);
+ *   - drivers/target/tcm_fc/tfc_io.c|189| <<ft_execute_work>> target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/usb/gadget/function/f_tcm.c|283| <<bot_send_write_request>> target_execute_cmd(se_cmd);
+ *   - drivers/usb/gadget/function/f_tcm.c|714| <<uasp_send_write_request>> target_execute_cmd(se_cmd);
+ *   - drivers/vhost/scsi.c|416| <<vhost_scsi_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/xen/xen-scsiback.c|1446| <<scsiback_write_pending>> target_execute_cmd(se_cmd);
+ */
 void target_execute_cmd(struct se_cmd *cmd)
 {
 	/*
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 7ebf106d5..4e23a4617 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -179,8 +179,37 @@ struct vhost_scsi_virtqueue {
 	 * Writers must also take dev mutex and flush under it.
 	 */
 	int inflight_idx;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|639| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|1482| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|1486| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|1494| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|1495| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|1505| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|1513| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1514| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|1520| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	struct vhost_scsi_cmd *scsi_cmds;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	struct sbitmap scsi_tags;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->max_cmds:
+	 *   - drivers/vhost/scsi.c|1546| <<vhost_scsi_destroy_vq_cmds>> for (i = 0; i < svq->max_cmds; i++) {
+	 *   - drivers/vhost/scsi.c|1563| <<vhost_scsi_setup_vq_cmds>> static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1576| <<vhost_scsi_setup_vq_cmds>> svq->max_cmds = max_cmds;
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_setup_vq_cmds>> for (i = 0; i < max_cmds; i++) {
+	 */
 	int max_cmds;
 };
 
@@ -191,16 +220,67 @@ struct vhost_scsi {
 
 	struct vhost_dev dev;
 	struct vhost_scsi_virtqueue *vqs;
+	/*
+	 * 在以下使用vhost_scsi->compl_bitmap:
+	 *   - drivers/vhost/scsi.c|549| <<vhost_scsi_complete_cmd_work>> bitmap_zero(vs->compl_bitmap, vs->dev.nvqs);
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_complete_cmd_work>> __set_bit(vq, vs->compl_bitmap);
+	 *   - drivers/vhost/scsi.c|582| <<vhost_scsi_complete_cmd_work>> while ((vq = find_next_bit(vs->compl_bitmap, vs->dev.nvqs, vq + 1))
+	 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_open>> vs->compl_bitmap = bitmap_alloc(nvqs, GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_open>> if (!vs->compl_bitmap)
+	 *   - drivers/vhost/scsi.c|1844| <<vhost_scsi_open>> bitmap_free(vs->compl_bitmap);
+	 *   - drivers/vhost/scsi.c|1865| <<vhost_scsi_release>> bitmap_free(vs->compl_bitmap);
+	 *
+	 * 和number of queues有关
+	 */
 	unsigned long *compl_bitmap;
+	/*
+	 * 在以下使用vhost_scsi->old_inflight:
+	 *   - drivers/vhost/scsi.c|270| <<vhost_scsi_init_inflight>> struct vhost_scsi_inflight *old_inflight[])
+	 *   - drivers/vhost/scsi.c|283| <<vhost_scsi_init_inflight>> if (old_inflight)
+	 *   - drivers/vhost/scsi.c|284| <<vhost_scsi_init_inflight>> old_inflight[i] = &vs->vqs[i].inflights[idx];
+	 *   - drivers/vhost/scsi.c|1483| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+	 *   - drivers/vhost/scsi.c|1491| <<vhost_scsi_flush>> kref_put(&vs->old_inflight[i]->kref, vhost_scsi_done_inflight);
+	 *   - drivers/vhost/scsi.c|1498| <<vhost_scsi_flush>> wait_for_completion(&vs->old_inflight[i]->comp);
+	 *   - drivers/vhost/scsi.c|1856| <<vhost_scsi_open>> vs->old_inflight = kmalloc_array(nvqs, sizeof(*vs->old_inflight),
+	 *   - drivers/vhost/scsi.c|1858| <<vhost_scsi_open>> if (!vs->old_inflight)
+	 *   - drivers/vhost/scsi.c|1895| <<vhost_scsi_open>> kfree(vs->old_inflight);
+	 *   - drivers/vhost/scsi.c|1917| <<vhost_scsi_release>> kfree(vs->old_inflight);
+	 *
+	 * 二维数组, 每个queue一个*old_inflight.
+	 */
 	struct vhost_scsi_inflight **old_inflight;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_completion_work:
+	 *   - drivers/vhost/scsi.c|464| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+	 *   - drivers/vhost/scsi.c|628| <<vhost_scsi_complete_cmd_work>> vs_completion_work);
+	 *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 */
 	struct vhost_work vs_completion_work; /* cmd completion work item */
+	/*
+	 * 在以下使用vhost_scsi->vs_completion_list:
+	 *   - drivers/vhost/scsi.c|416| <<vhost_scsi_release_cmd>> llist_add(&cmd->tvc_completion_list, &vs->vs_completion_list);
+	 *   - drivers/vhost/scsi.c|590| <<vhost_scsi_complete_cmd_work>> llnode = llist_del_all(&vs->vs_completion_list);
+	 */
 	struct llist_head vs_completion_list; /* cmd completion queue */
 
 	struct vhost_work vs_event_work; /* evt injection work item */
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_evt_work>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1461| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	struct llist_head vs_event_list; /* evt injection queue */
 
 	bool vs_events_missed; /* any missed events, protected by vq->mutex */
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|1807| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|1886| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	int vs_events_nr; /* num of pending events, protected by vq->mutex */
 };
 
@@ -271,6 +351,11 @@ static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|654| <<vhost_scsi_get_cmd>> cmd->inflight = vhost_scsi_get_inflight(vq);
+ *   - drivers/vhost/scsi.c|1238| <<vhost_scsi_handle_tmf>> tmf->inflight = vhost_scsi_get_inflight(vq);
+ */
 static struct vhost_scsi_inflight *
 vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 {
@@ -284,6 +369,11 @@ vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|437| <<vhost_scsi_release_cmd_res>> vhost_scsi_put_inflight(inflight);
+ *   - drivers/vhost/scsi.c|448| <<vhost_scsi_release_tmf_res>> vhost_scsi_put_inflight(inflight);
+ */
 static void vhost_scsi_put_inflight(struct vhost_scsi_inflight *inflight)
 {
 	kref_put(&inflight->kref, vhost_scsi_done_inflight);
@@ -328,6 +418,11 @@ static u32 vhost_scsi_tpg_get_inst_index(struct se_portal_group *se_tpg)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|682| <<vhost_scsi_complete_cmd_work>> vhost_scsi_release_cmd_res(se_cmd);
+ *   - drivers/vhost/scsi.c|1233| <<vhost_scsi_handle_vq>> vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
+ */
 static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *tv_cmd = container_of(se_cmd,
@@ -346,10 +441,23 @@ static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 			put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
 	}
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * 在以下使用vhost_scsi_release_tmf_res():
+ *   - drivers/vhost/scsi.c|1330| <<vhost_scsi_tmf_resp_work>> vhost_scsi_release_tmf_res(tmf);
+ *   - drivers/vhost/scsi.c|1376| <<vhost_scsi_handle_tmf>> vhost_scsi_release_tmf_res(tmf);
+ */
 static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 {
 	struct vhost_scsi_tpg *tpg = tmf->tpg;
@@ -361,6 +469,9 @@ static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.release_cmd = vhost_scsi_release_cmd()
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	if (se_cmd->se_cmd_flags & SCF_SCSI_TMR_CDB) {
@@ -374,6 +485,12 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 		struct vhost_scsi *vs = cmd->tvc_vhost;
 
 		llist_add(&cmd->tvc_completion_list, &vs->vs_completion_list);
+		/*
+		 * 在以下使用vhost_scsi->vs_completion_work:
+		 *   - drivers/vhost/scsi.c|464| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+		 *   - drivers/vhost/scsi.c|628| <<vhost_scsi_complete_cmd_work>> vs_completion_work);
+		 *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+		 */
 		vhost_work_queue(&vs->dev, &vs->vs_completion_work);
 	}
 }
@@ -535,6 +652,15 @@ static void vhost_scsi_evt_work(struct vhost_work *work)
  * This is scheduled in the vhost work queue so we are called with the owner
  * process mm and can access the vring.
  */
+/*
+ * 在以下使用vhost_scsi->vs_completion_work:
+ *   - drivers/vhost/scsi.c|464| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|628| <<vhost_scsi_complete_cmd_work>> vs_completion_work);
+ *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *
+ * 在以下使用vhost_scsi_complete_cmd_work():
+ *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ */
 static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 {
 	struct vhost_scsi *vs = container_of(work, struct vhost_scsi,
@@ -568,9 +694,26 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		ret = copy_to_iter(&v_rsp, sizeof(v_rsp), &iov_iter);
 		if (likely(ret == sizeof(v_rsp))) {
 			struct vhost_scsi_virtqueue *q;
+			/*
+			 * called by:
+			 *   - drivers/vhost/scsi.c|571| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+			 *   - drivers/vhost/vhost.c|2538| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+			 *   - drivers/vhost/vsock.c|222| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+			 *   - drivers/vhost/vsock.c|554| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+			 */
 			vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
 			q = container_of(cmd->tvc_vq, struct vhost_scsi_virtqueue, vq);
 			vq = q - vs->vqs;
+			/*
+			 * 在以下使用vhost_scsi->compl_bitmap:
+			 *   - drivers/vhost/scsi.c|549| <<vhost_scsi_complete_cmd_work>> bitmap_zero(vs->compl_bitmap, vs->dev.nvqs);
+			 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_complete_cmd_work>> __set_bit(vq, vs->compl_bitmap);
+			 *   - drivers/vhost/scsi.c|582| <<vhost_scsi_complete_cmd_work>> while ((vq = find_next_bit(vs->compl_bitmap, vs->dev.nvqs, vq + 1))
+			 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_open>> vs->compl_bitmap = bitmap_alloc(nvqs, GFP_KERNEL);
+			 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_open>> if (!vs->compl_bitmap)
+			 *   - drivers/vhost/scsi.c|1844| <<vhost_scsi_open>> bitmap_free(vs->compl_bitmap);
+			 *   - drivers/vhost/scsi.c|1865| <<vhost_scsi_release>> bitmap_free(vs->compl_bitmap);
+			 */
 			__set_bit(vq, vs->compl_bitmap);
 		} else
 			pr_err("Faulted on virtio_scsi_cmd_resp\n");
@@ -584,6 +727,10 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_signal(&vs->dev, &vs->vqs[vq].vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1169| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -603,6 +750,14 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		return ERR_PTR(-EIO);
 	}
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	tag = sbitmap_get(&svq->scsi_tags);
 	if (tag < 0) {
 		pr_err("Unable to obtain tag for vhost_scsi_cmd\n");
@@ -889,6 +1044,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1015| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1362| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
@@ -924,6 +1084,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1509| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1155,6 +1319,10 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		pr_err("Faulted on virtio_scsi_ctrl_tmf_resp\n");
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf_resp_work():
+ *   - drivers/vhost/scsi.c|2212| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ */
 static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
@@ -1208,6 +1376,11 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 	tmf->resp_iov = vq->iov[vc->out];
 	tmf->vq_desc = vc->head;
 	tmf->in_iovs = vc->in;
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|654| <<vhost_scsi_get_cmd>> cmd->inflight = vhost_scsi_get_inflight(vq);
+	 *   - drivers/vhost/scsi.c|1238| <<vhost_scsi_handle_tmf>> tmf->inflight = vhost_scsi_get_inflight(vq);
+	 */
 	tmf->inflight = vhost_scsi_get_inflight(vq);
 
 	if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
@@ -1247,6 +1420,10 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_ctrl_an_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1393| <<vhost_scsi_ctl_handle_kick>> vhost_scsi_ctl_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1468,6 +1645,10 @@ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 	svq->scsi_cmds = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1679| <<vhost_scsi_set_endpoint>> ret = vhost_scsi_setup_vq_cmds(vq, vq->num);
+ */
 static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1478,9 +1659,26 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 	if (svq->scsi_cmds)
 		return 0;
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
 			      NUMA_NO_NODE, false, true))
 		return -ENOMEM;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->max_cmds:
+	 *   - drivers/vhost/scsi.c|1546| <<vhost_scsi_destroy_vq_cmds>> for (i = 0; i < svq->max_cmds; i++) {
+	 *   - drivers/vhost/scsi.c|1563| <<vhost_scsi_setup_vq_cmds>> static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1576| <<vhost_scsi_setup_vq_cmds>> svq->max_cmds = max_cmds;
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_setup_vq_cmds>> for (i = 0; i < max_cmds; i++) {
+	 */
 	svq->max_cmds = max_cmds;
 
 	svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 40097826c..a2762b9b5 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -46,6 +46,11 @@ enum {
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * VIRTIO_RING_F_EVENT_IDX:
+ *   后端用了used ring的最后一个元素,告诉前端驱动后端处理到哪个avail ring上的元素了,
+ *   同时前端使用avail ring的最后一个元素告诉后端,处理到那个used ring了
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
@@ -1026,6 +1031,10 @@ static inline int vhost_get_avail_flags(struct vhost_virtqueue *vq,
 	return vhost_get_avail(vq, *flags, &vq->avail->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2563| <<vhost_notify>> if (vhost_get_used_event(vq, &event)) {
+ */
 static inline int vhost_get_used_event(struct vhost_virtqueue *vq,
 				       __virtio16 *event)
 {
@@ -2011,6 +2020,14 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1537| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|1622| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|197| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/test.c|290| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/vsock.c|600| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -2358,6 +2375,13 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|571| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2538| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|222| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|554| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2369,6 +2393,11 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2414| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2420| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2390,12 +2419,39 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|968| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2048| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2393| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2406| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2407| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2511| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->last_used_idx;
 	new = (vq->last_used_idx += count);
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2028| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2466| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	/* If the driver never bothers to signal in a very long while,
 	 * used index might wrap around. If that happens, invalidate
 	 * signalled_used index we stored. TODO: make sure driver
 	 * signals at least once in 2^16 and remove this. */
+	/*
+	 * 下面这个也不可能成立
+	 * new = 2
+	 * vq->signalled_used = 65530
+	 * old = 65533
+	 *
+	 * new - old 永远都是count !!!
+	 */
 	if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
 		vq->signalled_used_valid = false;
 	return 0;
@@ -2403,6 +2459,12 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2368| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2502| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ *   - drivers/vhost/vhost.h|194| <<vhost_add_used_and_signal_n>> int vhost_add_used_n(struct vhost_virtqueue *, struct vring_used_elem *heads,
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
@@ -2438,6 +2500,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2482| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2458,24 +2524,77 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 			vq_err(vq, "Failed to get flags");
 			return true;
 		}
+		/*
+		 * 如果设置了VRING_AVAIL_F_NO_INTERRUPT, vhost_notify()返回false
+		 */
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->vhost_signal:
+	 *   - drivers/vhost/vhost.c|315| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2399| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2463| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2465| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->signalled_used;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2028| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2466| <<vhost_notify>> vq->signalled_used_valid = true;
+	 *
+	 * 除了初始化,
+	 * If the driver never bothers to signal in a very long while,
+	 * used index might wrap around. If that happens, invalidate
+	 * signalled_used index we stored. TODO: make sure driver
+	 * signals at least once in 2^16 and remove this.
+	 */
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|968| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2048| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2393| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2406| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2407| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2511| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
 	if (unlikely(!v))
 		return true;
 
+	/*
+	 * VIRTIO_RING_F_EVENT_IDX:
+	 *   后端用了used ring的最后一个元素,告诉前端驱动后端处理到哪个avail ring上的元素了,
+	 *   同时前端使用avail ring的最后一个元素告诉后端,处理到那个used ring了
+	 */
 	if (vhost_get_used_event(vq, &event)) {
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 *
+	 * new - event <= new - old
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|584| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2539| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2549| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|260| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|560| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
@@ -2485,6 +2604,15 @@ void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_signal);
 
 /* And here's the combo meal deal.  Supersize me! */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|952| <<handle_tx_zerocopy>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|511| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|819| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1153| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+ *   - drivers/vhost/scsi.c|1245| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|87| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
@@ -2495,6 +2623,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
 /* multi-buffer version of vhost_add_used_and_signal */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|377| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|456| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+ */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index d9109107a..3b2b883d9 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -92,15 +92,41 @@ struct vhost_virtqueue {
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|968| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2048| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2393| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2406| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2407| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2511| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->vhost_signal:
+	 *   - drivers/vhost/vhost.c|315| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2399| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2463| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2465| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2028| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2466| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index 828ced060..1771c84c2 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -246,9 +246,29 @@ static int virtio_dev_probe(struct device *_d)
 	/* We have a driver! */
 	virtio_add_status(dev, VIRTIO_CONFIG_S_DRIVER);
 
+	/*
+	 * legacy: vp_get_features()
+	 * modern: vp_get_features()
+	 *
+	 * 关于modern, 写入device_feature_select, 读取device_feature
+	 */
 	/* Figure out what features the device supports. */
 	device_features = dev->config->get_features(dev);
 
+	/*
+	 * virtio-scsi的feature的例子
+	 * 969 static unsigned int features[] = {
+	 * 970         VIRTIO_SCSI_F_HOTPLUG,
+	 * 971         VIRTIO_SCSI_F_CHANGE,
+	 * 972 #ifdef CONFIG_BLK_DEV_INTEGRITY
+	 * 973         VIRTIO_SCSI_F_T10_PI,
+	 * 974 #endif
+	 * 975 };
+	 * 976
+	 * 977 static struct virtio_driver virtio_scsi_driver = {
+	 * 978         .feature_table = features,
+	 * 979         .feature_table_size = ARRAY_SIZE(features),
+	 */
 	/* Figure out what features the driver supports. */
 	driver_features = 0;
 	for (i = 0; i < drv->feature_table_size; i++) {
@@ -269,16 +289,29 @@ static int virtio_dev_probe(struct device *_d)
 		driver_features_legacy = driver_features;
 	}
 
+	/*
+	 * device_features是上面用vp_get_features()读取
+	 * 关于modern, 写入device_feature_select, 读取device_feature
+	 */
 	if (device_features & (1ULL << VIRTIO_F_VERSION_1))
 		dev->features = driver_features & device_features;
 	else
 		dev->features = driver_features_legacy & device_features;
 
+	/*
+	 * #define VIRTIO_TRANSPORT_F_START        28
+	 * #define VIRTIO_TRANSPORT_F_END          41
+	 * #define VIRTIO_F_VERSION_1              32
+	 */
 	/* Transport features always preserved to pass to finalize_features. */
 	for (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++)
 		if (device_features & (1ULL << i))
 			__virtio_set_bit(dev, i);
 
+	/*
+	 * legacy: vp_finalize_features()
+	 * modern: vp_finalize_features()
+	 */
 	err = dev->config->finalize_features(dev);
 	if (err)
 		goto err;
@@ -415,6 +448,16 @@ static int virtio_device_of_init(struct virtio_device *dev)
  *
  * Returns: 0 on suceess, -error on failure
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1218| <<virtio_uml_probe>> rc = register_virtio_device(&vu_dev->vdev);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1097| <<mlxbf_tmfifo_create_vdev>> ret = register_virtio_device(&tm_vdev->vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|430| <<rproc_add_virtio_dev>> ret = register_virtio_device(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1345| <<virtio_ccw_online>> ret = register_virtio_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|667| <<virtio_mmio_probe>> rc = register_virtio_device(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|559| <<virtio_pci_probe>> rc = register_virtio_device(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|379| <<virtio_vdpa_probe>> ret = register_virtio_device(&vd_dev->vdev);
+ */
 int register_virtio_device(struct virtio_device *dev)
 {
 	int err;
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index 3f78a3a1e..ad9f21a05 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -447,6 +447,13 @@ static void virtballoon_changed(struct virtio_device *vdev)
 	spin_unlock_irqrestore(&vb->stop_update_lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_balloon.c|484| <<update_balloon_size_func>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|843| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1015| <<remove_common>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1078| <<virtballoon_restore>> update_balloon_size(vb);
+ */
 static void update_balloon_size(struct virtio_balloon *vb)
 {
 	u32 actual = vb->num_pages;
diff --git a/drivers/virtio/virtio_pci_common.c b/drivers/virtio/virtio_pci_common.c
index ad258a9d3..97299b608 100644
--- a/drivers/virtio/virtio_pci_common.c
+++ b/drivers/virtio/virtio_pci_common.c
@@ -16,6 +16,13 @@
 
 #include "virtio_pci_common.h"
 
+/*
+ * 在以下使用force_legacy:
+ *   - drivers/virtio/virtio_pci_common.c|19| <<global>> static bool force_legacy = false;
+ *   - drivers/virtio/virtio_pci_common.c|22| <<global>> module_param(force_legacy, bool, 0444);
+ *   - drivers/virtio/virtio_pci_common.c|23| <<global>> MODULE_PARM_DESC(force_legacy,
+ *   - drivers/virtio/virtio_pci_common.c|540| <<virtio_pci_probe>> if (force_legacy) {
+ */
 static bool force_legacy = false;
 
 #if IS_ENABLED(CONFIG_VIRTIO_PCI_LEGACY)
@@ -537,6 +544,13 @@ static int virtio_pci_probe(struct pci_dev *pci_dev,
 	if (rc)
 		goto err_enable_device;
 
+	/*
+	 * 在以下使用force_legacy:
+	 *   - drivers/virtio/virtio_pci_common.c|19| <<global>> static bool force_legacy = false;
+	 *   - divers/virtio/virtio_pci_common.c|22| <<global>> module_param(force_legacy, bool, 0444);
+	 *   - drivers/virtio/virtio_pci_common.c|23| <<global>> MODULE_PARM_DESC(force_legacy,
+	 *   - drivers/virtio/virtio_pci_common.c|540| <<virtio_pci_probe>> if (force_legacy) {
+	 */
 	if (force_legacy) {
 		rc = virtio_pci_legacy_probe(vp_dev);
 		/* Also try modern mode if we can't map BAR0 (no IO space). */
@@ -554,6 +568,13 @@ static int virtio_pci_probe(struct pci_dev *pci_dev,
 
 	pci_set_master(pci_dev);
 
+	/*
+	 * struct virtio_pci_device *vp_dev:
+	 * -> struct virtio_pci_legacy_device ldev;
+	 *    -> u8 __iomem *isr;
+	 *    -> void __iomem *ioaddr;
+	 * -> struct virtio_pci_modern_device mdev;
+	 */
 	vp_dev->is_legacy = vp_dev->ldev.ioaddr ? true : false;
 
 	rc = register_virtio_device(&vp_dev->vdev);
@@ -603,6 +624,25 @@ static void virtio_pci_remove(struct pci_dev *pci_dev)
 	put_device(dev);
 }
 
+/*
+ * commit cfecc2918d2b3c5e86ff1a6c95eabbbb17bb8fd3
+ * Author: Tiwei Bie <tiwei.bie@intel.com>
+ * Date:   Fri Jun 1 12:02:39 2018 +0800
+ *
+ * virtio_pci: support enabling VFs
+ *
+ * There is a new feature bit allocated in virtio spec to
+ * support SR-IOV (Single Root I/O Virtualization):
+ *
+ * https://github.com/oasis-tcs/virtio-spec/issues/11
+ *
+ * This patch enables the support for this feature bit in
+ * virtio driver.
+ *
+ * Signed-off-by: Tiwei Bie <tiwei.bie@intel.com>
+ * Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
+ */
+
 static int virtio_pci_sriov_configure(struct pci_dev *pci_dev, int num_vfs)
 {
 	struct virtio_pci_device *vp_dev = pci_get_drvdata(pci_dev);
diff --git a/drivers/virtio/virtio_pci_modern.c b/drivers/virtio/virtio_pci_modern.c
index c3b9f2761..6892dc5b2 100644
--- a/drivers/virtio/virtio_pci_modern.c
+++ b/drivers/virtio/virtio_pci_modern.c
@@ -26,6 +26,10 @@ static u64 vp_get_features(struct virtio_device *vdev)
 	return vp_modern_get_features(&vp_dev->mdev);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|52| <<vp_finalize_features>> vp_transport_features(vdev, features);
+ */
 static void vp_transport_features(struct virtio_device *vdev, u64 features)
 {
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
@@ -45,6 +49,15 @@ static int vp_finalize_features(struct virtio_device *vdev)
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
 	u64 features = vdev->features;
 
+	/*
+	 * 清空virtio_device->features中在VIRTIO_TRANSPORT_F_START->VIRTIO_TRANSPORT_F_END并且不在以下的bit
+	 * - VIRTIO_RING_F_INDIRECT_DESC
+	 * - VIRTIO_RING_F_EVENT_IDX
+	 * - VIRTIO_F_VERSION_1:
+	 * - VIRTIO_F_ACCESS_PLATFORM:
+	 * - VIRTIO_F_RING_PACKED:
+	 * - VIRTIO_F_ORDER_PLATFORM:
+	 */
 	/* Give virtio_ring a chance to accept features. */
 	vring_transport_features(vdev);
 
diff --git a/drivers/virtio/virtio_pci_modern_dev.c b/drivers/virtio/virtio_pci_modern_dev.c
index 869cb46be..486067f8b 100644
--- a/drivers/virtio/virtio_pci_modern_dev.c
+++ b/drivers/virtio/virtio_pci_modern_dev.c
@@ -111,6 +111,13 @@ vp_modern_map_capability(struct virtio_pci_modern_device *mdev, int off,
  *
  * Returns offset of the capability, or 0.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern_dev.c|240| <<vp_modern_probe>> common = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_COMMON_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|250| <<vp_modern_probe>> isr = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_ISR_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|253| <<vp_modern_probe>> notify = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_NOTIFY_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|273| <<vp_modern_probe>> device = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_DEVICE_CFG,
+ */
 static inline int virtio_pci_find_capability(struct pci_dev *dev, u8 cfg_type,
 					     u32 ioresource_types, int *bars)
 {
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 4620e9d79..872632ddf 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -32,6 +32,16 @@
 	} while (0)
 #define END_USE(_vq) \
 	do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while(0)
+/*
+ * 在以下使用vring_virtqueue->last_add_time:
+ *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+ *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+ *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+ *
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|535| <<virtqueue_add_split>> LAST_ADD_TIME_UPDATE(vq);
+ *   - drivers/virtio/virtio_ring.c|1363| <<virtqueue_add_packed>> LAST_ADD_TIME_UPDATE(vq);
+ */
 #define LAST_ADD_TIME_UPDATE(_vq)				\
 	do {							\
 		ktime_t now = ktime_get();			\
@@ -43,6 +53,11 @@
 		(_vq)->last_add_time = now;			\
 		(_vq)->last_add_time_valid = true;		\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|707| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_CHECK(vq);
+ *   - drivers/virtio/virtio_ring.c|1515| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_CHECK(vq);
+ */
 #define LAST_ADD_TIME_CHECK(_vq)				\
 	do {							\
 		if ((_vq)->last_add_time_valid) {		\
@@ -50,6 +65,13 @@
 				      (_vq)->last_add_time)) > 100); \
 		}						\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|765| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|888| <<virtqueue_get_buf_ctx_split>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|1612| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|1766| <<virtqueue_get_buf_ctx_packed>> LAST_ADD_TIME_INVALID(vq);
+ */
 #define LAST_ADD_TIME_INVALID(_vq)				\
 	((_vq)->last_add_time_valid = false)
 #else
@@ -90,6 +112,22 @@ struct vring_virtqueue_split {
 	struct vring vring;
 
 	/* Last written value to avail->flags */
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1068| <<virtqueue_get_buf_ctx_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|1083| <<virtqueue_disable_cb_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|1084| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1091| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow); 
+	 *   - drivers/virtio/virtio_ring.c|1111| <<virtqueue_enable_cb_prepare_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1112| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1116| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1148| <<virtqueue_enable_cb_delayed_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1149| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1153| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1215| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1218| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow);
+	 */
 	u16 avail_flags_shadow;
 
 	/*
@@ -100,6 +138,10 @@ struct vring_virtqueue_split {
 
 	/* Per-descriptor state. */
 	struct vring_desc_state_split *desc_state;
+	/*
+	 * 在以下分配vring_virtqueue_split->desc_extra, num个:
+	 *   - drivers/virtio/virtio_ring.c|1096| <<vring_alloc_state_extra_split>> vring_split->desc_extra = extra;
+	 */
 	struct vring_desc_extra *desc_extra;
 
 	/* DMA address and size information */
@@ -111,9 +153,22 @@ struct vring_virtqueue_split {
 	 * vring.
 	 */
 	u32 vring_align;
+	/*
+	 * 在以下使用vring_virtqueue_split->may_reduce_num:
+	 *   - drivers/virtio/virtio_ring.c|1159| <<vring_alloc_queue_split>> vring_split->may_reduce_num = may_reduce_num;
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_resize_split>> vq->split.may_reduce_num);
+	 */
 	bool may_reduce_num;
 };
 
+/*
+ * struct vring_packed_desc_event {
+ *     // Descriptor Ring Change Event Offset/Wrap Counter.
+ *     __le16 off_wrap;
+ *     // Descriptor Ring Change Event Flags.
+ *     __le16 flags;
+ * };
+ */
 struct vring_virtqueue_packed {
 	/* Actual memory layout for this queue. */
 	struct {
@@ -124,18 +179,80 @@ struct vring_virtqueue_packed {
 	} vring;
 
 	/* Driver ring wrap counter. */
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_wrap_counter:
+	 *   - drivers/virtio/virtio_ring.c|1417| <<virtqueue_add_indirect_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1655| <<virtqueue_kick_prepare_packed>> if (wrap_counter != vq->packed.avail_wrap_counter)
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_vring_init_packed>> vring_packed->avail_wrap_counter = 1;
+	 *
+	 * 主要和vq->packed.vring.device.off_wrap比较,
+	 * 在add的时候当avail index wrap回0的时候会flip
+	 */
 	bool avail_wrap_counter;
 
 	/* Avail used flags. */
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_used_flags:
+	 *   - drivers/virtio/virtio_ring.c|1466| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags);
+	 *   - drivers/virtio/virtio_ring.c|1486| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags ^=
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> avail_used_flags = vq->packed.avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|1602| <<virtqueue_add_packed>> flags = cpu_to_le16(vq->packed.avail_used_flags |
+	 *   - drivers/virtio/virtio_ring.c|1625| <<virtqueue_add_packed>> vq->packed.avail_used_flags ^=
+	 *   - drivers/virtio/virtio_ring.c|1667| <<virtqueue_add_packed>> vq->packed.avail_used_flags = avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|2143| <<virtqueue_vring_init_packed>> vring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+	 *
+	 * 表示在add的时候在desc[i].flags中应该用什么标记avail/used
+	 */
 	u16 avail_used_flags;
 
+	/*
+	 * 在以下使用vring_virtqueue_packed->next_avail_idx:
+	 *   - drivers/virtio/virtio_ring.c|1429| <<virtqueue_add_indirect_packed>> head = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|1501| <<virtqueue_add_indirect_packed>> vq->packed.next_avail_idx = n;
+	 *   - drivers/virtio/virtio_ring.c|1575| <<virtqueue_add_packed>> head = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|1669| <<virtqueue_add_packed>> vq->packed.next_avail_idx = i;
+	 *   - drivers/virtio/virtio_ring.c|1734| <<virtqueue_kick_prepare_packed>> old = vq->packed.next_avail_idx - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|1735| <<virtqueue_kick_prepare_packed>> new = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|2189| <<virtqueue_vring_init_packed>> vring_packed->next_avail_idx = 0;
+	 */
 	/* Index of the next avail descriptor. */
+	/*
+	 * 用来索引vq->packed.vring.desc[i]
+	 */
 	u16 next_avail_idx;
 
 	/*
 	 * Last written value to driver->flags in
 	 * guest byte order.
 	 */
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_FLAG_DESC:
+	 *   - drivers/virtio/virtio_ring.c|1822| <<virtqueue_kick_prepare_packed>> if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+	 *   - drivers/virtio/virtio_ring.c|1997| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+	 *   - drivers/virtio/virtio_ring.c|2042| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2101| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *
+	 * 在以下使用vring_virtqueue_pack->event_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1916| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+	 *   - drivers/virtio/virtio_ring.c|1931| <<virtqueue_disable_cb_packed>> if (vq->packed.event_flags_shadow != VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|1932| <<virtqueue_disable_cb_packed>> vq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+	 *   - drivers/virtio/virtio_ring.c|1934| <<virtqueue_disable_cb_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1959| <<virtqueue_enable_cb_prepare_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|1960| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ?
+	 *   - drivers/virtio/virtio_ring.c|1964| <<virtqueue_enable_cb_prepare_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|2018| <<virtqueue_enable_cb_delayed_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|2019| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ?
+	 *   - drivers/virtio/virtio_ring.c|2023| <<virtqueue_enable_cb_delayed_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|2191| <<virtqueue_vring_init_packed>> vring_packed->event_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2196| <<virtqueue_vring_init_packed>> vring_packed->event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+	 *   - drivers/virtio/virtio_ring.c|2198| <<virtqueue_vring_init_packed>> cpu_to_le16(vring_packed->event_flags_shadow);
+	 *
+	 * 设置为VRING_PACKED_EVENT_FLAG_DESC或者VRING_PACKED_EVENT_FLAG_ENABLE/VRING_PACKED_EVENT_FLAG_DISABLE
+	 * 用来控制virtio是否想接受QEMU的中断
+	 * 比如说: 也可以判断是否使用event idx告诉backend前端的progress
+	 */
 	u16 event_flags_shadow;
 
 	/* Per-descriptor state. */
@@ -144,9 +261,35 @@ struct vring_virtqueue_packed {
 
 	/* DMA address and size information */
 	dma_addr_t ring_dma_addr;
+	/*
+	 * 在以下使用vring_virtqueue_packed->driver_event_dma_addr:
+	 *   - drivers/virtio/virtio_ring.c|1942| <<vring_free_packed>> vring_packed->driver_event_dma_addr);
+	 *   - drivers/virtio/virtio_ring.c|1984| <<vring_alloc_queue_packed>> vring_packed->driver_event_dma_addr = driver_event_dma_addr;
+	 *   - drivers/virtio/virtio_ring.c|2754| <<vring_free>> vq->packed.driver_event_dma_addr);
+	 *   - drivers/virtio/virtio_ring.c|2926| <<virtqueue_get_avail_addr>> return vq->packed.driver_event_dma_addr;
+	 *
+	 * 代表了avail
+	 */
 	dma_addr_t driver_event_dma_addr;
 	dma_addr_t device_event_dma_addr;
+	/*
+	 * 在以下使用vring_virtqueue_packed->ring_size_in_bytes:
+	 *   - drivers/virtio/virtio_ring.c|2174| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->ring_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2211| <<vring_alloc_queue_packed>> vring_packed->ring_size_in_bytes = ring_size_in_bytes;
+	 *   - drivers/virtio/virtio_ring.c|2301| <<virtqueue_reinit_packed>> memset(vq->packed.vring.desc, 0, vq->packed.ring_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|3100| <<vring_free>> vq->packed.ring_size_in_bytes,
+	 */
 	size_t ring_size_in_bytes;
+	/*
+	 * 在以下使用vring_virtqueue_packed->event_size_in_bytes:
+	 *   - drivers/virtio/virtio_ring.c|2179| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2184| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2222| <<vring_alloc_queue_packed>> vring_packed->event_size_in_bytes = event_size_in_bytes;
+	 *   - drivers/virtio/virtio_ring.c|2297| <<virtqueue_reinit_packed>> memset(vq->packed.vring.device, 0, vq->packed.event_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|2298| <<virtqueue_reinit_packed>> memset(vq->packed.vring.driver, 0, vq->packed.event_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|3105| <<vring_free>> vq->packed.event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|3110| <<vring_free>> vq->packed.event_size_in_bytes,
+	 */
 	size_t event_size_in_bytes;
 };
 
@@ -154,6 +297,11 @@ struct vring_virtqueue {
 	struct virtqueue vq;
 
 	/* Is this a packed ring? */
+	/*
+	 * 在以下设置vring_virtqueue->packed_ring:
+	 *   - drivers/virtio/virtio_ring.c|2008| <<bool>> vq->packed_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2488| <<bool>> vq->packed_ring = false;
+	 */
 	bool packed_ring;
 
 	/* Is DMA API used? */
@@ -169,6 +317,22 @@ struct vring_virtqueue {
 	bool indirect;
 
 	/* Host publishes avail event idx */
+	/*
+	 * 在以下设置vring_virtqueue->event:
+	 *   - drivers/virtio/virtio_ring.c|2377| <<vring_create_virtqueue_packed>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+	 *   - drivers/virtio/virtio_ring.c|2973| <<__vring_new_virtqueue>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+	 * 在以下使用vring_virtqueue->event:
+	 *   - drivers/virtio/virtio_ring.c|928| <<virtqueue_kick_prepare_split>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|1070| <<virtqueue_disable_cb_split>> if (vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1098| <<virtqueue_enable_cb_prepare_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1135| <<virtqueue_enable_cb_delayed_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1201| <<virtqueue_vring_init_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|2062| <<virtqueue_enable_cb_prepare_packed>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2109| <<virtqueue_enable_cb_delayed_packed>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|2132| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2919| <<vring_interrupt>> if (vq->event)
+	 */
 	bool event;
 
 	/* Head of free buffer list. */
@@ -185,6 +349,16 @@ struct vring_virtqueue {
 	u16 last_used_idx;
 
 	/* Hint for event idx: already triggered no need to disable. */
+	/*
+	 * 在以下使用vring_virtqueue->event_triggered:
+	 *   - drivers/virtio/virtio_ring.c|500| <<virtqueue_init>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2465| <<virtqueue_disable_cb>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2491| <<virtqueue_enable_cb_prepare>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2492| <<virtqueue_enable_cb_prepare>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2557| <<virtqueue_enable_cb_delayed>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2558| <<virtqueue_enable_cb_delayed>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2616| <<vring_interrupt>> vq->event_triggered = true;
+	 */
 	bool event_triggered;
 
 	union {
@@ -199,14 +373,47 @@ struct vring_virtqueue {
 	bool (*notify)(struct virtqueue *vq);
 
 	/* DMA, allocation, and size information */
+	/*
+	 * 在以下使用vring_virtqueue->we_own_ring:
+	 *   - drivers/virtio/virtio_ring.c|1213| <<vring_create_virtqueue_split>> to_vvq(vq)->we_own_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2141| <<vring_create_virtqueue_packed>> vq->we_own_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2641| <<__vring_new_virtqueue>> vq->we_own_ring = false;
+	 *   - drivers/virtio/virtio_ring.c|2731| <<virtqueue_resize>> if (!vq->we_own_ring)
+	 *   - drivers/virtio/virtio_ring.c|2795| <<vring_free>> if (vq->we_own_ring) {
+	 *   - drivers/virtio/virtio_ring.c|2961| <<virtqueue_get_desc_addr>> BUG_ON(!vq->we_own_ring);
+	 *   - drivers/virtio/virtio_ring.c|2974| <<virtqueue_get_avail_addr>> BUG_ON(!vq->we_own_ring);
+	 *   - drivers/virtio/virtio_ring.c|2988| <<virtqueue_get_used_addr>> BUG_ON(!vq->we_own_ring);
+	 */
 	bool we_own_ring;
 
 #ifdef DEBUG
 	/* They're supposed to lock for us. */
+	/*
+	 * 在以下使用vring_virtqueue->in_use:
+	 *   - drivers/virtio/virtio_ring.c|28| <<START_USE>> if ((_vq)->in_use) \
+	 *   - drivers/virtio/virtio_ring.c|30| <<START_USE>> (_vq)->vq.name, (_vq)->in_use); \
+	 *   - drivers/virtio/virtio_ring.c|31| <<START_USE>> (_vq)->in_use = __LINE__; \
+	 *   - drivers/virtio/virtio_ring.c|34| <<END_USE>> do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while (0)
+	 *   - drivers/virtio/virtio_ring.c|401| <<virtqueue_init>> vq->in_use = false;
+	 */
 	unsigned int in_use;
 
 	/* Figure out if their kicks are too delayed. */
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|40| <<LAST_ADD_TIME_UPDATE>> if ((_vq)->last_add_time_valid) \
+	 *   - drivers/virtio/virtio_ring.c|44| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time_valid = true; \
+	 *   - drivers/virtio/virtio_ring.c|48| <<LAST_ADD_TIME_CHECK>> if ((_vq)->last_add_time_valid) { \
+	 *   - drivers/virtio/virtio_ring.c|54| <<LAST_ADD_TIME_INVALID>> ((_vq)->last_add_time_valid = false)
+	 *   - drivers/virtio/virtio_ring.c|402| <<virtqueue_init>> vq->last_add_time_valid = false;
+	 */
 	bool last_add_time_valid;
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time:
+	 *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+	 *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+	 *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+	 */
 	ktime_t last_add_time;
 #endif
 };
@@ -284,6 +491,10 @@ static bool vring_use_dma_api(struct virtio_device *vdev)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|997| <<virtblk_probe>> max_size = virtio_max_dma_size(vdev);
+ */
 size_t virtio_max_dma_size(struct virtio_device *vdev)
 {
 	size_t max_segment_size = SIZE_MAX;
@@ -295,6 +506,16 @@ size_t virtio_max_dma_size(struct virtio_device *vdev)
 }
 EXPORT_SYMBOL_GPL(virtio_max_dma_size);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1054| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1068| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1868| <<vring_alloc_queue_packed>> ring = vring_alloc_queue(vdev, ring_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1880| <<vring_alloc_queue_packed>> driver = vring_alloc_queue(vdev, event_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1890| <<vring_alloc_queue_packed>> device = vring_alloc_queue(vdev, event_size_in_bytes,
+ *
+ * 分配size
+ */
 static void *vring_alloc_queue(struct virtio_device *vdev, size_t size,
 			      dma_addr_t *dma_handle, gfp_t flag)
 {
@@ -342,6 +563,18 @@ static void vring_free_queue(struct virtio_device *vdev, size_t size,
  * making all of the arch DMA ops work on the vring device itself
  * is a mess.  For now, we use the parent device for DMA ops.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|363| <<vring_map_one_sg>> return dma_map_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|375| <<vring_map_single>> return dma_map_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|385| <<vring_mapping_error>> return dma_mapping_error(vring_dma_dev(vq), addr);
+ *   - drivers/virtio/virtio_ring.c|421| <<vring_unmap_one_split_indirect>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|440| <<vring_unmap_one_split>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|446| <<vring_unmap_one_split>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1176| <<vring_unmap_extra_packed>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1181| <<vring_unmap_extra_packed>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1198| <<vring_unmap_desc_packed>> dma_unmap_page(vring_dma_dev(vq),
+ */
 static inline struct device *vring_dma_dev(const struct vring_virtqueue *vq)
 {
 	return vq->vq.vdev->dev.parent;
@@ -385,6 +618,15 @@ static int vring_mapping_error(const struct vring_virtqueue *vq,
 	return dma_mapping_error(vring_dma_dev(vq), addr);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1097| <<virtqueue_reinit_split>> virtqueue_init(vq, num);
+ *   - drivers/virtio/virtio_ring.c|1254| <<virtqueue_resize_split>> virtqueue_init(vq, vring_split.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2120| <<virtqueue_reinit_packed>> virtqueue_init(vq, vq->packed.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2176| <<vring_create_virtqueue_packed>> virtqueue_init(vq, num);
+ *   - drivers/virtio/virtio_ring.c|2210| <<virtqueue_resize_packed>> virtqueue_init(vq, vring_packed.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2677| <<__vring_new_virtqueue>> virtqueue_init(vq, vring_split->vring.num);
+ */
 static void virtqueue_init(struct vring_virtqueue *vq, u32 num)
 {
 	vq->vq.num_free = num;
@@ -506,6 +748,10 @@ static inline unsigned int virtqueue_add_desc_split(struct virtqueue *vq,
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2240| <<virtqueue_add>> virtqueue_add_split(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_split(struct virtqueue *_vq,
 				      struct scatterlist *sgs[],
 				      unsigned int total_sg,
@@ -689,6 +935,10 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2363| <<virtqueue_kick_prepare>> virtqueue_kick_prepare_split(_vq);
+ */
 static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -720,6 +970,11 @@ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 	return needs_kick;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|928| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+ */
 static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
 			     void **ctx)
 {
@@ -776,6 +1031,10 @@ static inline bool more_used_split(const struct vring_virtqueue *vq)
 			vq->split.vring.used->idx);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2378| <<virtqueue_get_buf_ctx>> virtqueue_get_buf_ctx_split(_vq, len, ctx);
+ */
 static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 					 unsigned int *len,
 					 void **ctx)
@@ -838,6 +1097,22 @@ static void virtqueue_disable_cb_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1068| <<virtqueue_get_buf_ctx_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|1083| <<virtqueue_disable_cb_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|1084| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1091| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1111| <<virtqueue_enable_cb_prepare_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1112| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1116| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1148| <<virtqueue_enable_cb_delayed_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1149| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1153| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1215| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1218| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow);
+	 */
 	if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
 		vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
 		if (vq->event)
@@ -850,6 +1125,10 @@ static void virtqueue_disable_cb_split(struct virtqueue *_vq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2495| <<virtqueue_enable_cb_prepare>> virtqueue_enable_cb_prepare_split(_vq);
+ */
 static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -875,6 +1154,10 @@ static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 	return last_used_idx;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2359| <<virtqueue_poll>> virtqueue_poll_split(_vq, last_used_idx);
+ */
 static bool virtqueue_poll_split(struct virtqueue *_vq, unsigned int last_used_idx)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -919,6 +1202,10 @@ static bool virtqueue_enable_cb_delayed_split(struct virtqueue *_vq)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2578| <<virtqueue_detach_unused_buf>> virtqueue_detach_unused_buf_split(_vq);
+ */
 static void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1082,6 +1369,10 @@ static int vring_alloc_queue_split(struct vring_virtqueue_split *vring_split,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2704| <<vring_create_virtqueue>> return vring_create_virtqueue_split(index, num, vring_align,
+ */
 static struct virtqueue *vring_create_virtqueue_split(
 	unsigned int index,
 	unsigned int num,
@@ -1115,6 +1406,10 @@ static struct virtqueue *vring_create_virtqueue_split(
 	return vq;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2612| <<virtqueue_resize>> err = virtqueue_resize_split(_vq, num);
+ */
 static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 {
 	struct vring_virtqueue_split vring_split = {};
@@ -1152,8 +1447,27 @@ static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 /*
  * Packed ring specific functions - *_packed().
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1854| <<more_used_packed>> used_wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|1884| <<virtqueue_get_buf_ctx_packed>> used_wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|2000| <<virtqueue_enable_cb_delayed_packed>> wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|2033| <<virtqueue_enable_cb_delayed_packed>> wrap_counter = packed_used_wrap_counter(last_used_idx);
+ */
 static inline bool packed_used_wrap_counter(u16 last_used_idx)
 {
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+	 *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 */
 	return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
@@ -1162,6 +1476,11 @@ static inline u16 packed_last_used(u16 last_used_idx)
 	return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_packed>> vring_unmap_extra_packed(vq, &vq->packed.desc_extra[curr]);
+ *   - drivers/virtio/virtio_ring.c|1555| <<detach_buf_packed>> vring_unmap_extra_packed(vq,
+ */
 static void vring_unmap_extra_packed(const struct vring_virtqueue *vq,
 				     struct vring_desc_extra *extra)
 {
@@ -1219,6 +1538,10 @@ static struct vring_packed_desc *alloc_indirect_packed(unsigned int total_sg,
 	return desc;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1675| <<virtqueue_add_packed>> err = virtqueue_add_indirect_packed(vq, sgs, total_sg, out_sgs,
+ */
 static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 					 struct scatterlist *sgs[],
 					 unsigned int total_sg,
@@ -1333,6 +1656,10 @@ static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2168| <<virtqueue_add>> return vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_packed(struct virtqueue *_vq,
 				       struct scatterlist *sgs[],
 				       unsigned int total_sg,
@@ -1380,7 +1707,20 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 
 	WARN_ON_ONCE(total_sg > vq->packed.vring.num && !vq->indirect);
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *        struct {
+	 *            unsigned int num;
+	 *            struct vring_packed_desc *desc;
+	 *            struct vring_packed_desc_event *driver;
+	 *            struct vring_packed_desc_event *device;
+	 *        } vring;
+	 */
 	desc = vq->packed.vring.desc;
+	/*
+	 * i用来索引desc
+	 */
 	i = head;
 	descs_used = total_sg;
 
@@ -1403,6 +1743,22 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 			if (vring_mapping_error(vq, addr))
 				goto unmap_release;
 
+			/*
+			 * 在以下使用vring_virtqueue_packed->avail_used_flags:
+			 *   - drivers/virtio/virtio_ring.c|1466| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags);
+			 *   - drivers/virtio/virtio_ring.c|1486| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags ^=
+			 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> avail_used_flags = vq->packed.avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|1602| <<virtqueue_add_packed>> flags = cpu_to_le16(vq->packed.avail_used_flags |
+			 *   - drivers/virtio/virtio_ring.c|1625| <<virtqueue_add_packed>> vq->packed.avail_used_flags ^=
+			 *   - drivers/virtio/virtio_ring.c|1667| <<virtqueue_add_packed>> vq->packed.avail_used_flags = avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|2143| <<virtqueue_vring_init_packed>> vring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+			 *
+			 * flags分成三部分:
+			 * - vq->packed.avail_used_flags
+			 * - (++c == total_sg ? 0 : VRING_DESC_F_NEXT)
+			 * - (n < out_sgs ? 0 : VRING_DESC_F_WRITE)
+			 */
 			flags = cpu_to_le16(vq->packed.avail_used_flags |
 				    (++c == total_sg ? 0 : VRING_DESC_F_NEXT) |
 				    (n < out_sgs ? 0 : VRING_DESC_F_WRITE));
@@ -1433,6 +1789,15 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 		}
 	}
 
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_wrap_counter:
+	 *   - drivers/virtio/virtio_ring.c|1417| <<virtqueue_add_indirect_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1655| <<virtqueue_kick_prepare_packed>> if (wrap_counter != vq->packed.avail_wrap_counter)
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_vring_init_packed>> vring_packed->avail_wrap_counter = 1;
+	 *
+	 * 似乎只用在kick的判断上
+	 */
 	if (i < head)
 		vq->packed.avail_wrap_counter ^= 1;
 
@@ -1484,6 +1849,10 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 	return -EIO;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2503| <<virtqueue_kick_prepare>> return vq->packed_ring ? virtqueue_kick_prepare_packed(_vq) :
+ */
 static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1505,16 +1874,43 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 	 */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 用来索引desc = vq->packed.vring.desc
+	 */
 	old = vq->packed.next_avail_idx - vq->num_added;
 	new = vq->packed.next_avail_idx;
 	vq->num_added = 0;
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *    -> struct {
+	 *           unsigned int num;
+	 *           struct vring_packed_desc *desc;
+	 *           struct vring_packed_desc_event *driver;
+	 *           struct vring_packed_desc_event *device;
+	 *       } vring;
+	 */
 	snapshot.u32 = *(u32 *)vq->packed.vring.device;
 	flags = le16_to_cpu(snapshot.flags);
 
 	LAST_ADD_TIME_CHECK(vq);
 	LAST_ADD_TIME_INVALID(vq);
 
+	/*
+	 * 有这样类似的代码
+	 * 2012                 vq->packed.event_flags_shadow = vq->event ?
+	 * 2013                                 VRING_PACKED_EVENT_FLAG_DESC :
+	 * 2014                                 VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *
+	 * 在QEMU side, 设置handle_request (avai)是否想接收中断.
+	 *
+	 * disable: 设置VRING_PACKED_EVENT_FLAG_DISABLE
+	 * enable:  设置VRING_PACKED_EVENT_FLAG_DESC或者VRING_PACKED_EVENT_FLAG_ENABLE
+	 *
+	 * 如果设置VRING_PACKED_EVENT_FLAG_DESC, 直接跳过, 说明是ENABLE
+	 * 否则有可能是DISABLE
+	 */
 	if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
 		needs_kick = (flags != VRING_PACKED_EVENT_FLAG_DISABLE);
 		goto out;
@@ -1527,12 +1923,21 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 	if (wrap_counter != vq->packed.avail_wrap_counter)
 		event_idx -= vq->packed.vring.num;
 
+	/*
+	 * event_idx应该相当于现在otherend处理到的,比如vring_used_event(&ring)
+	 */
 	needs_kick = vring_need_event(event_idx, new, old);
 out:
 	END_USE(vq);
 	return needs_kick;
 }
 
+
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1745| <<virtqueue_get_buf_ctx_packed>> detach_buf_packed(vq, id, ctx);
+ *   - drivers/virtio/virtio_ring.c|1902| <<virtqueue_detach_unused_buf_packed>> detach_buf_packed(vq, i, NULL);
+ */
 static void detach_buf_packed(struct vring_virtqueue *vq,
 			      unsigned int id, void **ctx)
 {
@@ -1579,16 +1984,35 @@ static void detach_buf_packed(struct vring_virtqueue *vq,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1806| <<more_used_packed>> return is_used_desc_packed(vq, last_used, used_wrap_counter);
+ *   - drivers/virtio/virtio_ring.c|1931| <<virtqueue_poll_packed>> return is_used_desc_packed(vq, used_idx, wrap_counter);
+ *   - drivers/virtio/virtio_ring.c|1986| <<virtqueue_enable_cb_delayed_packed>> if (is_used_desc_packed(vq, used_idx, wrap_counter)) {
+ */
 static inline bool is_used_desc_packed(const struct vring_virtqueue *vq,
 				       u16 idx, bool used_wrap_counter)
 {
 	bool avail, used;
 	u16 flags;
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *    -> struct {
+	 *           unsigned int num;
+	 *           struct vring_packed_desc *desc;
+	 *           struct vring_packed_desc_event *driver;
+	 *           struct vring_packed_desc_event *device;
+	 *       } vring;
+	 */
 	flags = le16_to_cpu(vq->packed.vring.desc[idx].flags);
 	avail = !!(flags & (1 << VRING_PACKED_DESC_F_AVAIL));
 	used = !!(flags & (1 << VRING_PACKED_DESC_F_USED));
 
+	/*
+	 * 如果只设置了avail, 没设置used, 就返回false
+	 */
 	return avail == used && used == used_wrap_counter;
 }
 
@@ -1673,6 +2097,10 @@ static void *virtqueue_get_buf_ctx_packed(struct virtqueue *_vq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2755| <<virtqueue_disable_cb>> virtqueue_disable_cb_packed(_vq);
+ */
 static void virtqueue_disable_cb_packed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2203,6 +2631,27 @@ EXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);
  * This is sometimes useful because the virtqueue_kick_prepare() needs
  * to be serialized, but the actual virtqueue_notify() call does not.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|307| <<virtio_commit_rqs>> kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/block/virtio_blk.c|379| <<virtio_queue_rq>> if (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))
+ *   - drivers/block/virtio_blk.c|419| <<virtblk_add_req_batch>> kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|429| <<virtio_gpu_notify>> notify = virtqueue_kick_prepare(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|471| <<virtio_gpu_queue_cursor>> notify = virtqueue_kick_prepare(vq);
+ *   - drivers/net/virtio_net.c|682| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1461| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1682| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1885| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> notify = virtqueue_kick_prepare(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|468| <<virtscsi_kick_vq>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|495| <<virtscsi_add_cmd>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2546| <<virtqueue_kick>> if (virtqueue_kick_prepare(vq))
+ *   - fs/fuse/virtio_fs.c|457| <<send_forget_request>> notify = virtqueue_kick_prepare(vq);
+ *   - fs/fuse/virtio_fs.c|1203| <<virtio_fs_enqueue_req>> notify = virtqueue_kick_prepare(vq);
+ *   - sound/virtio/virtio_card.c|44| <<virtsnd_event_send>> if (virtqueue_kick_prepare(vqueue))
+ *   - sound/virtio/virtio_ctl_msg.c|154| <<virtsnd_ctl_msg_send>> notify = virtqueue_kick_prepare(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|245| <<virtsnd_pcm_msg_send>> notify = virtqueue_kick_prepare(vqueue);
+ */
 bool virtqueue_kick_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2220,6 +2669,27 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|311| <<virtio_commit_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/block/virtio_blk.c|384| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/block/virtio_blk.c|445| <<virtio_queue_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|433| <<virtio_gpu_notify>> virtqueue_notify(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|477| <<virtio_gpu_queue_cursor>> virtqueue_notify(vq);
+ *   - drivers/net/virtio_net.c|682| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1461| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1682| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1885| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|995| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|472| <<virtscsi_kick_vq>> virtqueue_notify(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|500| <<virtscsi_add_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2547| <<virtqueue_kick>> return virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|461| <<send_forget_request>> virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|1208| <<virtio_fs_enqueue_req>> virtqueue_notify(vq);
+ *   - sound/virtio/virtio_card.c|45| <<virtsnd_event_send>> virtqueue_notify(vqueue);
+ *   - sound/virtio/virtio_ctl_msg.c|174| <<virtsnd_ctl_msg_send>> virtqueue_notify(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|248| <<virtsnd_pcm_msg_send>> virtqueue_notify(vqueue);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2273,6 +2743,12 @@ EXPORT_SYMBOL_GPL(virtqueue_kick);
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1128| <<receive_mergeable>> buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx);
+ *   - drivers/net/virtio_net.c|1552| <<virtnet_receive>> (buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx))) {
+ *   - drivers/virtio/virtio_ring.c|2384| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -2326,6 +2802,14 @@ EXPORT_SYMBOL_GPL(virtqueue_disable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/firmware/arm_scmi/virtio.c|520| <<virtio_send_message>> msg->poll_idx = virtqueue_enable_cb_prepare(vioch->vqueue);
+ *   - drivers/firmware/arm_scmi/virtio.c|771| <<virtio_poll_done>> msg->poll_idx = virtqueue_enable_cb_prepare(vioch->vqueue);
+ *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+ *   - drivers/net/virtio_net.c|1747| <<virtnet_poll_tx>> opaque = virtqueue_enable_cb_prepare(sq->vq);
+ *   - drivers/virtio/virtio_ring.c|2584| <<virtqueue_enable_cb>> unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);
+ */
 unsigned int virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2371,6 +2855,29 @@ EXPORT_SYMBOL_GPL(virtqueue_poll);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|292| <<virtblk_done>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|47| <<virtcrypto_ctrlq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|94| <<virtcrypto_dataq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/firmware/arm_scmi/virtio.c|293| <<scmi_vio_complete_cb>> if (virtqueue_enable_cb(vqueue)) {
+ *   - drivers/firmware/arm_scmi/virtio.c|769| <<virtio_poll_done>> pending = !virtqueue_enable_cb(vioch->vqueue);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|212| <<virtio_gpu_dequeue_ctrl_func>> } while (!virtqueue_enable_cb(vgdev->ctrlq.vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|260| <<virtio_gpu_dequeue_cursor_func>> } while (!virtqueue_enable_cb(vgdev->cursorq.vq));
+ *   - drivers/net/caif/caif_virtio.c|565| <<cfv_netdev_tx>> virtqueue_enable_cb(cfv->vq_tx);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|486| <<rpmsg_upref_sleepers>> virtqueue_enable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|187| <<virtscsi_vq_done>> } while (!virtqueue_enable_cb(vq));
+ *   - fs/fuse/virtio_fs.c|348| <<virtio_fs_hiprio_done_work>> } while (!virtqueue_enable_cb(vq) && likely(!virtqueue_is_broken(vq)));
+ *   - fs/fuse/virtio_fs.c|630| <<virtio_fs_requests_done_work>> } while (!virtqueue_enable_cb(vq) && likely(!virtqueue_is_broken(vq)));
+ *   - net/vmw_vsock/virtio_transport.c|310| <<virtio_transport_tx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|418| <<virtio_transport_event_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|561| <<virtio_transport_rx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - sound/virtio/virtio_card.c|96| <<virtsnd_event_notify_cb>> } while (!virtqueue_enable_cb(vqueue));
+ *   - sound/virtio/virtio_card.c|166| <<virtsnd_enable_event_vq>> if (!virtqueue_enable_cb(queue->vqueue))
+ *   - sound/virtio/virtio_ctl_msg.c|308| <<virtsnd_ctl_notify_cb>> } while (!virtqueue_enable_cb(vqueue));
+ *   - sound/virtio/virtio_pcm_msg.c|350| <<virtsnd_pcm_notify_cb>> } while (!virtqueue_enable_cb(queue->vqueue));
+ *   - tools/virtio/virtio_test.c|266| <<run_test>> if (virtqueue_enable_cb(vq->vq))
+ */
 bool virtqueue_enable_cb(struct virtqueue *_vq)
 {
 	unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);
@@ -2392,6 +2899,15 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1651| <<virtnet_poll_cleantx>> } while (unlikely(!virtqueue_enable_cb_delayed(sq->vq)));
+ *   - drivers/net/virtio_net.c|1835| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq)));
+ *   - drivers/net/virtio_net.c|1874| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {
+ *   - tools/virtio/virtio_test.c|263| <<run_test>> if (virtqueue_enable_cb_delayed(vq->vq))
+ *   - tools/virtio/vringh_test.c|388| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ *   - tools/virtio/vringh_test.c|422| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ */
 bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2453,6 +2969,16 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 #endif
 	}
 
+	/*
+	 * 在以下使用vring_virtqueue->event_triggered:
+	 *   - drivers/virtio/virtio_ring.c|500| <<virtqueue_init>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2465| <<virtqueue_disable_cb>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2491| <<virtqueue_enable_cb_prepare>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2492| <<virtqueue_enable_cb_prepare>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2557| <<virtqueue_enable_cb_delayed>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2558| <<virtqueue_enable_cb_delayed>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2616| <<vring_interrupt>> vq->event_triggered = true;
+	 */
 	/* Just a hint for performance: so it's ok that this can be racy! */
 	if (vq->event)
 		vq->event_triggered = true;
@@ -2466,6 +2992,11 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 EXPORT_SYMBOL_GPL(vring_interrupt);
 
 /* Only available for split ring */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1267| <<vring_create_virtqueue_split>> vq = __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,
+ *   - drivers/virtio/virtio_ring.c|2852| <<vring_new_virtqueue>> return __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,
+ */
 static struct virtqueue *__vring_new_virtqueue(unsigned int index,
 					       struct vring_virtqueue_split *vring_split,
 					       struct virtio_device *vdev,
@@ -2525,6 +3056,15 @@ static struct virtqueue *__vring_new_virtqueue(unsigned int index,
 	return &vq->vq;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|954| <<vu_setup_vq>> vq = vring_create_virtqueue(index, num, PAGE_SIZE, vdev, true, true,
+ *   - drivers/s390/virtio/virtio_ccw.c|525| <<virtio_ccw_setup_vq>> vq = vring_create_virtqueue(i, info->num, KVM_VIRTIO_CCW_RING_ALIGN,
+ *   - drivers/virtio/virtio_mmio.c|399| <<vm_setup_vq>> vq = vring_create_virtqueue(index, num, VIRTIO_MMIO_VRING_ALIGN, vdev,
+ *   - drivers/virtio/virtio_pci_legacy.c|131| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_pci_modern.c|334| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_vdpa.c|178| <<virtio_vdpa_setup_vq>> vq = vring_create_virtqueue(index, max_num, align, vdev,
+ */
 struct virtqueue *vring_create_virtqueue(
 	unsigned int index,
 	unsigned int num,
@@ -2573,6 +3113,11 @@ EXPORT_SYMBOL_GPL(vring_create_virtqueue);
  * -EPERM: Operation not permitted
  *
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1906| <<virtnet_rx_resize>> err = virtqueue_resize(rq->vq, ring_num, virtnet_rq_free_unused_buf);
+ *   - drivers/net/virtio_net.c|1945| <<virtnet_tx_resize>> err = virtqueue_resize(sq->vq, ring_num, virtnet_sq_free_unused_buf);
+ */
 int virtqueue_resize(struct virtqueue *_vq, u32 num,
 		     void (*recycle)(struct virtqueue *vq, void *buf))
 {
@@ -2641,6 +3186,12 @@ struct virtqueue *vring_new_virtqueue(unsigned int index,
 }
 EXPORT_SYMBOL_GPL(vring_new_virtqueue);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1300| <<virtqueue_resize_split>> vring_free(&vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2256| <<virtqueue_resize_packed>> vring_free(&vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2901| <<vring_del_virtqueue>> vring_free(_vq);
+ */
 static void vring_free(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2692,6 +3243,24 @@ void vring_del_virtqueue(struct virtqueue *_vq)
 EXPORT_SYMBOL_GPL(vring_del_virtqueue);
 
 /* Manipulates transport-specific feature bits. */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1077| <<vu_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|248| <<rproc_virtio_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|821| <<virtio_ccw_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_mmio.c|128| <<vm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_legacy.c|36| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_modern.c|49| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_vdpa.c|320| <<virtio_vdpa_finalize_features>> vring_transport_features(vdev);
+ *
+ * 清空virtio_device->features中在VIRTIO_TRANSPORT_F_START->VIRTIO_TRANSPORT_F_END并且不在以下的bit
+ * - VIRTIO_RING_F_INDIRECT_DESC
+ * - VIRTIO_RING_F_EVENT_IDX
+ * - VIRTIO_F_VERSION_1:
+ * - VIRTIO_F_ACCESS_PLATFORM:
+ * - VIRTIO_F_RING_PACKED:
+ * - VIRTIO_F_ORDER_PLATFORM:
+ */
 void vring_transport_features(struct virtio_device *vdev)
 {
 	unsigned int i;
@@ -2749,6 +3318,10 @@ EXPORT_SYMBOL_GPL(__virtqueue_break);
 /*
  * This function should only be called by the core, not directly by the driver.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|277| <<vp_modern_enable_vq_after_reset>> __virtqueue_unbreak(vq);
+ */
 void __virtqueue_unbreak(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2770,6 +3343,17 @@ EXPORT_SYMBOL_GPL(virtqueue_is_broken);
  * This should prevent the device from being used, allowing drivers to
  * recover.  You may need to grab appropriate locks to flush.
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|171| <<vhost_user_check_reset>> virtio_break_device(&vu_dev->vdev);
+ *   - drivers/char/virtio_console.c|1961| <<virtcons_remove>> virtio_break_device(vdev);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|269| <<virtcrypto_update_status>> virtio_break_device(vcrypto->vdev);
+ *   - drivers/firmware/arm_scmi/virtio.c|164| <<scmi_vio_channel_cleanup_sync>> virtio_break_device(vioch->vqueue->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1233| <<virtio_ccw_remove>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1250| <<virtio_ccw_offline>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio.c|229| <<virtio_reset_device>> virtio_break_device(dev);
+ *   - drivers/virtio/virtio_pci_common.c|591| <<virtio_pci_remove>> virtio_break_device(&vp_dev->vdev);
+ */
 void virtio_break_device(struct virtio_device *dev)
 {
 	struct virtqueue *_vq;
@@ -2813,6 +3397,9 @@ dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * num个sizeof(struct vring_packed_desc)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.ring_dma_addr;
 
@@ -2820,12 +3407,23 @@ dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_desc_addr);
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|989| <<vu_setup_vq>> virtqueue_get_avail_addr(vq),
+ *   - drivers/s390/virtio/virtio_ccw.c|553| <<virtio_ccw_setup_vq>> info->info_block->s.avail = (__u64)virtqueue_get_avail_addr(vq);
+ *   - drivers/virtio/virtio_mmio.c|436| <<vm_setup_vq>> addr = virtqueue_get_avail_addr(vq);
+ *   - drivers/virtio/virtio_pci_modern.c|193| <<vp_active_vq>> virtqueue_get_avail_addr(vq),
+ *   - drivers/virtio/virtio_vdpa.c|195| <<virtio_vdpa_setup_vq>> driver_addr = virtqueue_get_avail_addr(vq);
+ */
 dma_addr_t virtqueue_get_avail_addr(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * 分配的时候只有sizeof(struct vring_packed_desc_event)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.driver_event_dma_addr;
 
@@ -2840,6 +3438,9 @@ dma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * 分配的时候只有sizeof(struct vring_packed_desc_event)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.device_event_dma_addr;
 
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4519d368..eab1ab16f 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -745,6 +745,17 @@ struct kvm {
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|876| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|955| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1176| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1337| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1347| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1353| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1367| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ee8b9ecdc..54fc0f7e5 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -151,6 +151,18 @@ struct hw_perf_event {
 			u64		last_tag;
 			unsigned long	config_base;
 			unsigned long	event_base;
+			/*
+			 * 在以下使用hw_perf_event->event_base_rdpmc:
+			 *   - arch/x86/events/amd/uncore.c|94| <<amd_uncore_read>> rdpmcl(hwc->event_base_rdpmc, new);
+			 *   - arch/x86/events/amd/uncore.c|158| <<amd_uncore_add>> hwc->event_base_rdpmc = uncore->rdpmc_base + hwc->idx;
+			 *   - arch/x86/events/amd/uncore.c|169| <<amd_uncore_add>> hwc->event_base_rdpmc += NUM_COUNTERS_L3;
+			 *   - arch/x86/events/core.c|131| <<x86_perf_event_update>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+			 *   - arch/x86/events/core.c|1242| <<x86_assign_hw_event>> hwc->event_base_rdpmc = (idx - INTEL_PMC_IDX_FIXED) |
+			 *   - arch/x86/events/core.c|1249| <<x86_assign_hw_event>> hwc->event_base_rdpmc = x86_pmu_rdpmc_index(hwc->idx);
+			 *   - arch/x86/events/core.c|1272| <<x86_perf_rdpmc_index>> return event->hw.event_base_rdpmc;
+			 *   - arch/x86/events/core.c|2576| <<x86_pmu_event_idx>> return hwc->event_base_rdpmc + 1;
+			 *   - arch/x86/events/intel/ds.c|1880| <<intel_pmu_save_and_restart_reload>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+			 */
 			int		event_base_rdpmc;
 			int		idx;
 			int		last_cpu;
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 03b370062..e1a5ba8a3 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -673,6 +673,13 @@ struct perf_event_mmap_page {
 	 * data_{offset,size} indicate the location and size of the perf record
 	 * buffer within the mmapped area.
 	 */
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 	__u64   data_head;		/* head in the data section */
 	__u64	data_tail;		/* user-space written tail */
 	__u64	data_offset;		/* where the buffer starts */
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index f8c20d3de..7bd9cd285 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -64,6 +64,13 @@
 #define VRING_PACKED_EVENT_FLAG_ENABLE	0x0
 /* Disable events in packed ring. */
 #define VRING_PACKED_EVENT_FLAG_DISABLE	0x1
+/*
+ * 在以下使用VRING_PACKED_EVENT_FLAG_DESC:
+ *   - drivers/virtio/virtio_ring.c|1822| <<virtqueue_kick_prepare_packed>> if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+ *   - drivers/virtio/virtio_ring.c|1997| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+ *   - drivers/virtio/virtio_ring.c|2042| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+ *   - drivers/virtio/virtio_ring.c|2101| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+ */
 /*
  * Enable events for a specific descriptor in packed ring.
  * (as specified by Descriptor Ring Change Event Offset/Wrap Counter).
@@ -75,6 +82,18 @@
  * Wrap counter bit shift in event suppression structure
  * of packed ring.
  */
+/*
+ * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+ *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ */
 #define VRING_PACKED_EVENT_F_WRAP_CTR	15
 
 /* We support indirect buffer descriptors */
@@ -219,6 +238,15 @@ static inline unsigned vring_size(unsigned int num, unsigned long align)
 /* Assuming a given event_idx value from the other side, if
  * we have just incremented index from old to new_idx,
  * should we trigger an event? */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2475| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+ *   - drivers/vhost/vringh.c|525| <<__vringh_need_notify>> notify = vring_need_event(used_event,
+ *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_kick_prepare_split>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,
+ *   - drivers/virtio/virtio_ring.c|1798| <<virtqueue_kick_prepare_packed>> needs_kick = vring_need_event(event_idx, new, old);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 static inline int vring_need_event(__u16 event_idx, __u16 new_idx, __u16 old)
 {
 	/* Note: Xen has similar logic for notification hold-off
diff --git a/kernel/cpu.c b/kernel/cpu.c
index bbad5e375..3ef3da5e3 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1123,6 +1123,17 @@ static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 }
 
 /* Requires cpu_add_remove_lock to be held */
+/*
+ * [0] _cpu_down
+ * [0] cpu_device_down
+ * [0] device_offline
+ * [0] online_store
+ * [0] kernfs_fop_write_iter
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
 			   enum cpuhp_state target)
 {
diff --git a/kernel/events/core.c b/kernel/events/core.c
index ff4bffc50..5e15a277e 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -2186,6 +2186,10 @@ static void perf_group_detach(struct perf_event *event)
 
 static void sync_child_event(struct perf_event *child_event);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2339| <<__perf_remove_from_context>> perf_child_detach(event);
+ */
 static void perf_child_detach(struct perf_event *event)
 {
 	struct perf_event *parent_event = event->parent;
@@ -2319,6 +2323,12 @@ group_sched_out(struct perf_event *group_event,
  * We disable the event on the hardware level first. After that we
  * remove it from the context list.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2382| <<perf_remove_from_context>> __perf_remove_from_context(event, __get_cpu_context(ctx),
+ *   - kernel/events/core.c|2389| <<perf_remove_from_context>> event_function_call(event, __perf_remove_from_context, (void *)flags);
+ *   - kernel/events/core.c|13392| <<__perf_event_exit_context>> __perf_remove_from_context(event, cpuctx, ctx, (void *)DETACH_GROUP);
+ */
 static void
 __perf_remove_from_context(struct perf_event *event,
 			   struct perf_cpu_context *cpuctx,
@@ -2612,6 +2622,14 @@ static int group_can_go_on(struct perf_event *event,
 	return can_add_hw;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2768| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2771| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2825| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2895| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|13060| <<inherit_event>> add_event_to_ctx(child_event, child_ctx);
+ */
 static void add_event_to_ctx(struct perf_event *event,
 			       struct perf_event_context *ctx)
 {
@@ -4381,8 +4399,25 @@ static void __perf_event_read(void *info)
 	raw_spin_unlock(&ctx->lock);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5207| <<__perf_event_read_value>> total += perf_event_count(event);
+ *   - kernel/events/core.c|5216| <<__perf_event_read_value>> total += perf_event_count(child);
+ *   - kernel/events/core.c|5271| <<__perf_read_group_add>> values[n++] += perf_event_count(leader);
+ *   - kernel/events/core.c|5278| <<__perf_read_group_add>> values[n++] += perf_event_count(sub);
+ *   - kernel/events/core.c|5861| <<perf_event_update_userpage>> userpg->offset = perf_event_count(event);
+ *   - kernel/events/core.c|6881| <<perf_output_read_one>> values[n++] = perf_event_count(event);
+ *   - kernel/events/core.c|6926| <<perf_output_read_group>> values[n++] = perf_event_count(leader);
+ *   - kernel/events/core.c|6941| <<perf_output_read_group>> values[n++] = perf_event_count(sub);
+ *   - kernel/events/core.c|12706| <<sync_child_event>> child_val = perf_event_count(child_event);
+ */
 static inline u64 perf_event_count(struct perf_event *event)
 {
+	/*
+	 * struct perf_event *event:
+	 * -> local64_t count;
+	 * -> atomic64_t child_count;
+	 */
 	return local64_read(&event->count) + atomic64_read(&event->child_count);
 }
 
@@ -4471,6 +4506,14 @@ int perf_event_read_local(struct perf_event *event, u64 *value,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5206| <<__perf_event_read_value>> (void )perf_event_read(event, false);
+ *   - kernel/events/core.c|5215| <<__perf_event_read_value>> (void )perf_event_read(child, false);
+ *   - kernel/events/core.c|5247| <<__perf_read_group_add>> ret = perf_event_read(leader, true);
+ *   - kernel/events/core.c|5443| <<_perf_event_reset>> (void )perf_event_read(event, false);
+ *   - security/security.c|2644| <<security_perf_event_read>> return call_int_hook(perf_event_read, 0, event);
+ */
 static int perf_event_read(struct perf_event *event, bool group)
 {
 	enum perf_event_state state = READ_ONCE(event->state);
@@ -5185,6 +5228,11 @@ static int perf_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5231| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+ *   - kernel/events/core.c|5342| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+ */
 static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event *child;
@@ -5324,6 +5372,10 @@ static int perf_read_group(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5395| <<__perf_read>> ret = perf_read_one(event, read_format, buf);
+ */
 static int perf_read_one(struct perf_event *event,
 				 u64 read_format, char __user *buf)
 {
@@ -5490,6 +5542,10 @@ static void perf_event_for_each(struct perf_event *event,
 		perf_event_for_each_child(sibling, func);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5604| <<_perf_event_period>> event_function_call(event, __perf_event_period, &value);
+ */
 static void __perf_event_period(struct perf_event *event,
 				struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
@@ -5532,6 +5588,11 @@ static int perf_event_check_period(struct perf_event *event, u64 value)
 	return event->pmu->check_period(event, value);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5615| <<perf_event_period>> ret = _perf_event_period(event, value);
+ *   - kernel/events/core.c|5670| <<_perf_ioctl(PERF_EVENT_IOC_PERIOD)>> return _perf_event_period(event, value);
+ */
 static int _perf_event_period(struct perf_event *event, u64 value)
 {
 	if (!is_sampling_event(event))
@@ -5554,6 +5615,11 @@ static int _perf_event_period(struct perf_event *event, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|294| <<pmc_resume_counter>> if (perf_event_period(pmc->perf_event,
+ *   - arch/x86/kvm/pmu.h|208| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+ */
 int perf_event_period(struct perf_event *event, u64 value)
 {
 	struct perf_event_context *ctx;
@@ -6152,6 +6218,21 @@ static const struct vm_operations_struct perf_mmap_vmops = {
 	.page_mkwrite	= perf_mmap_fault,
 };
 
+/*
+ * struct perf_event *event = file->private_data;
+ * -> struct perf_buffer *rb;
+ *    -> atomic_t mmap_count;
+ *    -> unsigned long mmap_locked;
+ *    -> struct user_struct *mmap_user;
+ *    -> struct perf_event_mmap_page *user_page;
+ *       -> __u64   data_head;              // head in the data section
+ *       -> __u64   data_tail;              // user-space written tail
+ *       -> __u64   data_offset;            // where the buffer starts
+ *       -> __u64   data_size;              // data buffer size
+ *    -> void *data_pages[];
+ *
+ * 猜测在perf_event_overflow()更新数据
+ */
 static int perf_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	struct perf_event *event = file->private_data;
@@ -6260,6 +6341,19 @@ static int perf_mmap(struct file *file, struct vm_area_struct *vma)
 			goto unlock;
 		}
 
+		/*
+		 * struct perf_event *event = file->private_data;
+		 * -> struct perf_buffer *rb;
+		 *    -> atomic_t mmap_count;
+		 *    -> unsigned long mmap_locked;
+		 *    -> struct user_struct *mmap_user;
+		 *    -> struct perf_event_mmap_page *user_page;
+		 *       -> __u64   data_head;              // head in the data section
+		 *       -> __u64   data_tail;              // user-space written tail
+		 *       -> __u64   data_offset;            // where the buffer starts
+		 *       -> __u64   data_size;              // data buffer size
+		 *    -> void *data_pages[];
+		 */
 		if (!atomic_inc_not_zero(&event->rb->mmap_count)) {
 			/*
 			 * Raced against perf_mmap_close(); remove the
@@ -6981,6 +7075,13 @@ static inline bool perf_sample_save_hw_index(struct perf_event *event)
 	return event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_HW_INDEX;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1343| <<dump_trace_imc_data>> perf_output_sample(&handle, &header, &data, event);
+ *   - arch/s390/kernel/perf_cpum_sf.c|682| <<cpumsf_output_event_pid>> perf_output_sample(&handle, &header, data, event);
+ *   - arch/x86/events/intel/ds.c|777| <<intel_pmu_drain_bts_buffer>> perf_output_sample(&handle, &header, &data, event);
+ *   - kernel/events/core.c|7558| <<__perf_event_output>> perf_output_sample(&handle, &header, data, event);
+ */
 void perf_output_sample(struct perf_output_handle *handle,
 			struct perf_event_header *header,
 			struct perf_sample_data *data,
@@ -7306,6 +7407,12 @@ perf_callchain(struct perf_event *event, struct pt_regs *regs)
 	return callchain ?: &__empty_callchain;
 }
 
+/*
+ * called by:
+ *   - arch/s390/kernel/perf_cpum_sf.c|674| <<cpumsf_output_event_pid>> perf_prepare_sample(&header, data, event, regs);
+ *   - arch/x86/events/intel/ds.c|762| <<intel_pmu_drain_bts_buffer>> perf_prepare_sample(&header, &data, event, &regs);
+ *   - kernel/events/core.c|7565| <<__perf_event_output>> perf_prepare_sample(&header, data, event, regs);
+ */
 void perf_prepare_sample(struct perf_event_header *header,
 			 struct perf_sample_data *data,
 			 struct perf_event *event,
@@ -7481,6 +7588,12 @@ void perf_prepare_sample(struct perf_event_header *header,
 	WARN_ON_ONCE(header->size & 7);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7572| <<perf_event_output_forward>> __perf_event_output(event, data, regs, perf_output_begin_forward);
+ *   - kernel/events/core.c|7580| <<perf_event_output_backward>> __perf_event_output(event, data, regs, perf_output_begin_backward);
+ *   - kernel/events/core.c|7588| <<perf_event_output>> return __perf_event_output(event, data, regs, perf_output_begin);
+ */
 static __always_inline int
 __perf_event_output(struct perf_event *event,
 		    struct perf_sample_data *data,
@@ -7512,6 +7625,11 @@ __perf_event_output(struct perf_event *event,
 	return err;
 }
 
+/*
+ * called by:
+ *   - include/linux/perf_event.h|1122| <<is_default_overflow_handler>> if (likely(event->overflow_handler == perf_event_output_forward))
+ *   - kernel/events/core.c|11730| <<perf_event_alloc>> event->overflow_handler = perf_event_output_forward;
+ */
 void
 perf_event_output_forward(struct perf_event *event,
 			 struct perf_sample_data *data,
@@ -7547,6 +7665,10 @@ struct perf_read_event {
 	u32				tid;
 };
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12737| <<sync_child_event>> perf_event_read_event(child_event, task);
+ */
 static void
 perf_event_read_event(struct perf_event *event,
 			struct task_struct *task)
@@ -7820,6 +7942,10 @@ static int perf_event_task_match(struct perf_event *event)
 	       event->attr.task;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7936| <<perf_event_task>> perf_iterate_sb(perf_event_task_output,
+ */
 static void perf_event_task_output(struct perf_event *event,
 				   void *data)
 {
@@ -7863,6 +7989,12 @@ static void perf_event_task_output(struct perf_event *event,
 	task_event->event_id.header.size = size;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7943| <<perf_event_fork>> perf_event_task(task, NULL, 1);
+ *   - kernel/events/core.c|12853| <<perf_event_exit_task_context>> perf_event_task(child, child_ctx, 0);
+ *   - kernel/events/core.c|12897| <<perf_event_exit_task>> perf_event_task(child, NULL, 0);
+ */
 static void perf_event_task(struct task_struct *task,
 			      struct perf_event_context *task_ctx,
 			      int new)
@@ -7924,6 +8056,10 @@ static int perf_event_comm_match(struct perf_event *event)
 	return event->attr.comm;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8016| <<perf_event_comm_event>> perf_iterate_sb(perf_event_comm_output,
+ */
 static void perf_event_comm_output(struct perf_event *event,
 				   void *data)
 {
@@ -7957,6 +8093,10 @@ static void perf_event_comm_output(struct perf_event *event,
 	comm_event->event_id.header.size = size;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8043| <<perf_event_comm>> perf_event_comm_event(&comm_event);
+ */
 static void perf_event_comm_event(struct perf_comm_event *comm_event)
 {
 	char comm[TASK_COMM_LEN];
@@ -7976,6 +8116,10 @@ static void perf_event_comm_event(struct perf_comm_event *comm_event)
 		       NULL);
 }
 
+/*
+ * called by:
+ *   - fs/exec.c|1234| <<__set_task_comm>> perf_event_comm(tsk, exec);
+ */
 void perf_event_comm(struct task_struct *task, bool exec)
 {
 	struct perf_comm_event comm_event;
@@ -8268,6 +8412,10 @@ static int perf_event_mmap_match(struct perf_event *event,
 	       (executable && (event->attr.mmap || event->attr.mmap2));
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8492| <<perf_event_mmap_event>> perf_iterate_sb(perf_event_mmap_output,
+ */
 static void perf_event_mmap_output(struct perf_event *event,
 				   void *data)
 {
@@ -8335,6 +8483,10 @@ static void perf_event_mmap_output(struct perf_event *event,
 	mmap_event->event_id.header.type = type;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8633| <<perf_event_mmap>> perf_event_mmap_event(&mmap_event);
+ */
 static void perf_event_mmap_event(struct perf_mmap_event *mmap_event)
 {
 	struct vm_area_struct *vma = mmap_event->vma;
@@ -8556,6 +8708,15 @@ static void perf_addr_filters_adjust(struct vm_area_struct *vma)
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - mm/mmap.c|1819| <<mmap_region>> perf_event_mmap(vma);
+ *   - mm/mmap.c|2427| <<expand_upwards>> perf_event_mmap(vma);
+ *   - mm/mmap.c|2505| <<expand_downwards>> perf_event_mmap(vma);
+ *   - mm/mmap.c|3032| <<do_brk_flags>> perf_event_mmap(vma);
+ *   - mm/mmap.c|3401| <<__install_special_mapping>> perf_event_mmap(vma);
+ *   - mm/mprotect.c|651| <<mprotect_fixup>> perf_event_mmap(vma);
+ */
 void perf_event_mmap(struct vm_area_struct *vma)
 {
 	struct perf_mmap_event mmap_event;
@@ -8678,6 +8839,10 @@ static int perf_event_switch_match(struct perf_event *event)
 	return event->attr.context_switch;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8841| <<perf_event_switch>> perf_iterate_sb(perf_event_switch_output, &switch_event, NULL);
+ */
 static void perf_event_switch_output(struct perf_event *event, void *data)
 {
 	struct perf_switch_event *se = data;
@@ -8707,6 +8872,17 @@ static void perf_event_switch_output(struct perf_event *event, void *data)
 	if (ret)
 		return;
 
+	/*
+	 * struct perf_switch_event *se = data;
+	 *     struct task_struct      *task;
+	 *     struct task_struct      *next_prev;
+	 *
+	 *     struct {
+	 *         struct perf_event_header        header;
+	 *         u32                             next_prev_pid;
+	 *         u32                             next_prev_tid;
+	 *     } event_id;
+	 */
 	if (event->ctx->task)
 		perf_output_put(&handle, se->event_id.header);
 	else
@@ -8717,6 +8893,11 @@ static void perf_event_switch_output(struct perf_event *event, void *data)
 	perf_output_end(&handle);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3588| <<__perf_event_task_sched_out>> perf_event_switch(task, next, false);
+ *   - kernel/events/core.c|3922| <<__perf_event_task_sched_in>> perf_event_switch(task, prev, true);
+ */
 static void perf_event_switch(struct task_struct *task,
 			      struct task_struct *next_prev, bool sched_in)
 {
@@ -9187,6 +9368,12 @@ int perf_event_account_interrupt(struct perf_event *event)
  * Generic event overflow handling, sampling.
  */
 
+/*
+ * called by:
+ *   - kernel/events/core.c|9399| <<perf_event_overflow>> return __perf_event_overflow(event, 1, data, regs);
+ *   - kernel/events/core.c|9461| <<perf_swevent_overflow>> if (__perf_event_overflow(event, throttle,
+ *   - kernel/events/core.c|10783| <<perf_swevent_hrtimer>> if (__perf_event_overflow(event, 1, &data, regs))
+ */
 static int __perf_event_overflow(struct perf_event *event,
 				   int throttle, struct perf_sample_data *data,
 				   struct pt_regs *regs)
@@ -9227,6 +9414,36 @@ static int __perf_event_overflow(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/perf_event.c|860| <<alpha_perf_event_irq_handler>> if (perf_event_overflow(event, &data, regs)) {
+ *   - arch/arc/kernel/perf_event.c|603| <<arc_pmu_intr>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_v6.c|347| <<armv6pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_v7.c|994| <<armv7pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_xscale.c|189| <<xscale1pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_xscale.c|535| <<xscale2pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm64/kernel/perf_event.c|882| <<armv8pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/csky/kernel/perf_event.c|1142| <<csky_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/mips/kernel/perf_event_mipsxx.c|794| <<handle_associated_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/powerpc/perf/core-book3s.c|2310| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/powerpc/perf/core-fsl-emb.c|637| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/s390/kernel/perf_cpum_cf.c|669| <<cfdiag_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/s390/kernel/perf_cpum_sf.c|1138| <<perf_push_sample>> if (perf_event_overflow(event, &data, &regs)) {
+ *   - arch/s390/kernel/perf_pai_crypto.c|371| <<paicrypt_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/sparc/kernel/perf_event.c|1671| <<perf_event_nmi_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/core.c|961| <<amd_pmu_v2_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/ibs.c|804| <<perf_ibs_handle_irq>> throttle = perf_event_overflow(event, &data, &regs);
+ *   - arch/x86/events/core.c|1754| <<x86_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/core.c|2898| <<x86_pmu_handle_guest_pebs>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/core.c|3010| <<handle_pmi_common>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/ds.c|1976| <<__intel_pmu_pebs_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/knc.c|255| <<knc_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/p4.c|1051| <<p4_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/zhaoxin/core.c|400| <<zhaoxin_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/xtensa/kernel/perf_event.c|382| <<xtensa_pmu_irq_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/apple_m1_cpu_pmu.c|415| <<m1_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/riscv_pmu_sbi.c|635| <<pmu_sbi_ovf_handler>> perf_event_overflow(event, &data, regs);
+ */
 int perf_event_overflow(struct perf_event *event,
 			  struct perf_sample_data *data,
 			  struct pt_regs *regs)
@@ -12525,6 +12742,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+ *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+ *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+ *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+ *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
@@ -12670,6 +12900,10 @@ void perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)
 }
 EXPORT_SYMBOL_GPL(perf_pmu_migrate_context);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2203| <<perf_child_detach>> sync_child_event(event);
+ */
 static void sync_child_event(struct perf_event *child_event)
 {
 	struct perf_event *parent_event = child_event->parent;
@@ -12744,6 +12978,10 @@ perf_event_exit_event(struct perf_event *event, struct perf_event_context *ctx)
 	perf_event_wakeup(event);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12889| <<perf_event_exit_task>> perf_event_exit_task_context(child, ctxn);
+ */
 static void perf_event_exit_task_context(struct task_struct *child, int ctxn)
 {
 	struct perf_event_context *child_ctx, *clone_ctx = NULL;
@@ -12811,6 +13049,11 @@ static void perf_event_exit_task_context(struct task_struct *child, int ctxn)
  * Can be called with exec_update_lock held when called from
  * setup_new_exec().
  */
+/*
+ * called by:
+ *   - fs/exec.c|1385| <<begin_new_exec>> perf_event_exit_task(me);
+ *   - kernel/exit.c|804| <<do_exit>> perf_event_exit_task(tsk);
+ */
 void perf_event_exit_task(struct task_struct *child)
 {
 	struct perf_event *event, *tmp;
diff --git a/kernel/events/internal.h b/kernel/events/internal.h
index 5150d5f84..173e75585 100644
--- a/kernel/events/internal.h
+++ b/kernel/events/internal.h
@@ -54,7 +54,38 @@ struct perf_buffer {
 	void				**aux_pages;
 	void				*aux_priv;
 
+	/*
+	 * 在以下使用perf_buffer->user_page:
+	 *   - kernel/events/core.c|5852| <<perf_event_init_userpage>> userpg = rb->user_page;
+	 *   - kernel/events/core.c|5896| <<perf_event_update_userpage>> userpg = rb->user_page;
+	 *   - kernel/events/core.c|6258| <<perf_mmap>> aux_offset = READ_ONCE(rb->user_page->aux_offset);
+	 *   - kernel/events/core.c|6259| <<perf_mmap>> aux_size = READ_ONCE(rb->user_page->aux_size);
+	 *   - kernel/events/ring_buffer.c|122| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - kernel/events/ring_buffer.c|213| <<__perf_output_begin>> tail = READ_ONCE(rb->user_page->data_tail);
+	 *   - kernel/events/ring_buffer.c|492| <<perf_aux_output_begin>> aux_tail = READ_ONCE(rb->user_page->aux_tail);
+	 *   - kernel/events/ring_buffer.c|582| <<perf_aux_output_end>> WRITE_ONCE(rb->user_page->aux_head, rb->aux_head);
+	 *   - kernel/events/ring_buffer.c|614| <<perf_aux_output_skip>> WRITE_ONCE(rb->user_page->aux_head, rb->aux_head);
+	 *   - kernel/events/ring_buffer.c|849| <<__perf_mmap_to_page>> return virt_to_page(rb->user_page);
+	 *   - kernel/events/ring_buffer.c|892| <<rb_alloc>> rb->user_page = perf_mmap_alloc_page(cpu);
+	 *   - kernel/events/ring_buffer.c|893| <<rb_alloc>> if (!rb->user_page)
+	 *   - kernel/events/ring_buffer.c|912| <<rb_alloc>> perf_mmap_free_page(rb->user_page);
+	 *   - kernel/events/ring_buffer.c|925| <<rb_free>> perf_mmap_free_page(rb->user_page);
+	 *   - kernel/events/ring_buffer.c|939| <<__perf_mmap_to_page>> return vmalloc_to_page((void *)rb->user_page + pgoff * PAGE_SIZE);
+	 *   - kernel/events/ring_buffer.c|958| <<rb_free_work>> base = rb->user_page;
+	 *   - kernel/events/ring_buffer.c|993| <<rb_alloc>> rb->user_page = all_buf;
+	 */
 	struct perf_event_mmap_page	*user_page;
+	/*
+	 * 在以下使用perf_buffer->data_pages[]:
+	 *   - kernel/events/internal.h|153| <<__DEFINE_OUTPUT_COPY_BODY(advance_buf)>> handle->addr = rb->data_pages[handle->page]; \
+	 *   - kernel/events/ring_buffer.c|257| <<__perf_output_begin>> handle->addr = rb->data_pages[handle->page] + offset;
+	 *   - kernel/events/ring_buffer.c|851| <<__perf_mmap_to_page>> return virt_to_page(rb->data_pages[pgoff - 1]);
+	 *   - kernel/events/ring_buffer.c|897| <<rb_alloc>> rb->data_pages[i] = perf_mmap_alloc_page(cpu);
+	 *   - kernel/events/ring_buffer.c|898| <<rb_alloc>> if (!rb->data_pages[i])
+	 *   - kernel/events/ring_buffer.c|910| <<rb_alloc>> perf_mmap_free_page(rb->data_pages[i]);
+	 *   - kernel/events/ring_buffer.c|927| <<rb_free>> perf_mmap_free_page(rb->data_pages[i]);
+	 *   - kernel/events/ring_buffer.c|994| <<rb_alloc>> rb->data_pages[0] = all_buf + PAGE_SIZE;
+	 */
 	void				*data_pages[];
 };
 
diff --git a/kernel/events/ring_buffer.c b/kernel/events/ring_buffer.c
index 726132039..155b85f9f 100644
--- a/kernel/events/ring_buffer.c
+++ b/kernel/events/ring_buffer.c
@@ -47,6 +47,11 @@ static void perf_output_get_handle(struct perf_output_handle *handle)
 	handle->wakeup = local_read(&rb->wakeup);
 }
 
+/*
+ * called by:
+ *   - kernel/events/ring_buffer.c|260| <<__perf_output_begin>> perf_output_put_handle(handle);
+ *   - kernel/events/ring_buffer.c|304| <<perf_output_end>> perf_output_put_handle(handle);
+ */
 static void perf_output_put_handle(struct perf_output_handle *handle)
 {
 	struct perf_buffer *rb = handle->rb;
@@ -106,6 +111,13 @@ static void perf_output_put_handle(struct perf_output_handle *handle)
 	 *
 	 * See perf_output_begin().
 	 */
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 	smp_wmb(); /* B, matches C */
 	WRITE_ONCE(rb->user_page->data_head, head);
 
@@ -145,6 +157,12 @@ ring_buffer_has_space(unsigned long head, unsigned long tail,
 		return CIRC_SPACE(tail, head, data_size) >= size;
 }
 
+/*
+ * called by:
+ *   - kernel/events/ring_buffer.c|271| <<perf_output_begin_forward>> return __perf_output_begin(handle, data, event, size, false);
+ *   - kernel/events/ring_buffer.c|278| <<perf_output_begin_backward>> return __perf_output_begin(handle, data, event, size, true);
+ *   - kernel/events/ring_buffer.c|286| <<perf_output_begin>> return __perf_output_begin(handle, data, event, size,
+ */
 static __always_inline int
 __perf_output_begin(struct perf_output_handle *handle,
 		    struct perf_sample_data *data,
@@ -278,6 +296,28 @@ int perf_output_begin_backward(struct perf_output_handle *handle,
 	return __perf_output_begin(handle, data, event, size, true);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1340| <<dump_trace_imc_data>> if (perf_output_begin(&handle, &data, event, header.size))
+ *   - arch/s390/kernel/perf_cpum_sf.c|675| <<cpumsf_output_event_pid>> if (perf_output_begin(&handle, data, event, header.size))
+ *   - arch/x86/events/intel/ds.c|764| <<intel_pmu_drain_bts_buffer>> if (perf_output_begin(&handle, &data, event,
+ *   - kernel/events/core.c|7578| <<perf_event_output>> return __perf_event_output(event, data, regs, perf_output_begin);
+ *   - kernel/events/core.c|7610| <<perf_event_read_event>> ret = perf_output_begin(&handle, &sample, event, read_event.header.size);
+ *   - kernel/events/core.c|7879| <<perf_event_task_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|7982| <<perf_event_comm_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8082| <<perf_event_namespaces_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8209| <<perf_event_cgroup_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8338| <<perf_event_mmap_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8659| <<perf_event_aux_event>> ret = perf_output_begin(&handle, &sample, event, rec.header.size);
+ *   - kernel/events/core.c|8693| <<perf_log_lost_samples>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8748| <<perf_event_switch_output>> ret = perf_output_begin(&handle, &sample, event, se->event_id.header.size);
+ *   - kernel/events/core.c|8822| <<perf_log_throttle>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8865| <<perf_event_ksymbol_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8955| <<perf_event_bpf_output>> ret = perf_output_begin(&handle, data, event,
+ *   - kernel/events/core.c|9064| <<perf_event_text_poke_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|9146| <<perf_log_itrace_start>> ret = perf_output_begin(&handle, &sample, event, rec.header.size);
+ *   - kernel/events/core.c|9176| <<perf_report_aux_output_id>> ret = perf_output_begin(&handle, &sample, event, rec.header.size);
+ */
 int perf_output_begin(struct perf_output_handle *handle,
 		      struct perf_sample_data *data,
 		      struct perf_event *event, unsigned int size)
@@ -299,12 +339,39 @@ unsigned int perf_output_skip(struct perf_output_handle *handle,
 	return __output_skip(handle, NULL, len);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1344| <<dump_trace_imc_data>> perf_output_end(&handle);
+ *   - arch/s390/kernel/perf_cpum_sf.c|683| <<cpumsf_output_event_pid>> perf_output_end(&handle);
+ *   - arch/x86/events/intel/ds.c|780| <<intel_pmu_drain_bts_buffer>> perf_output_end(&handle);
+ *   - kernel/events/core.c|7579| <<__perf_event_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|7656| <<perf_event_read_event>> perf_output_end(&handle);
+ *   - kernel/events/core.c|7945| <<perf_event_task_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8049| <<perf_event_comm_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8156| <<perf_event_namespaces_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8279| <<perf_event_cgroup_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8438| <<perf_event_mmap_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8744| <<perf_event_aux_event>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8777| <<perf_log_lost_samples>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8851| <<perf_event_switch_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8926| <<perf_log_throttle>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8971| <<perf_event_ksymbol_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9060| <<perf_event_bpf_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9178| <<perf_event_text_poke_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9251| <<perf_log_itrace_start>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9281| <<perf_report_aux_output_id>> perf_output_end(&handle);
+ */
 void perf_output_end(struct perf_output_handle *handle)
 {
 	perf_output_put_handle(handle);
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - kernel/events/ring_buffer.c|877| <<rb_alloc>> ring_buffer_init(rb, watermark, flags);
+ *   - kernel/events/ring_buffer.c|973| <<rb_alloc>> ring_buffer_init(rb, watermark, flags);
+ */
 static void
 ring_buffer_init(struct perf_buffer *rb, long watermark, int flags)
 {
@@ -943,6 +1010,10 @@ struct perf_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)
 
 #endif
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5945| <<perf_mmap_fault>> vmf->page = perf_mmap_to_page(rb, vmf->pgoff);
+ */
 struct page *
 perf_mmap_to_page(struct perf_buffer *rb, unsigned long pgoff)
 {
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index f42bb51e0..60b2623c2 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -70,6 +70,16 @@ static atomic_t huge_zero_refcount;
 struct page *huge_zero_page __read_mostly;
 unsigned long huge_zero_pfn __read_mostly = ~0UL;
 
+/*
+ * called by:
+ *   - fs/proc/task_mmu.c|867| <<show_smap>> hugepage_vma_check(vma, vma->vm_flags, true, false));
+ *   - mm/khugepaged.c|469| <<khugepaged_enter_vma>> if (hugepage_vma_check(vma, vm_flags, false, false))
+ *   - mm/khugepaged.c|923| <<hugepage_vma_revalidate>> if (!hugepage_vma_check(vma, vma->vm_flags, false, false))
+ *   - mm/khugepaged.c|1410| <<collapse_pte_mapped_thp>> if (!hugepage_vma_check(vma, vma->vm_flags | VM_HUGEPAGE, false, false))
+ *   - mm/khugepaged.c|2100| <<__acquires>> if (!hugepage_vma_check(vma, vma->vm_flags, false, false)) {
+ *   - mm/memory.c|5002| <<__handle_mm_fault>> hugepage_vma_check(vma, vm_flags, false, true)) {
+ *   - mm/memory.c|5036| <<__handle_mm_fault>> hugepage_vma_check(vma, vm_flags, false, true)) {
+ */
 bool hugepage_vma_check(struct vm_area_struct *vma,
 			unsigned long vm_flags,
 			bool smaps, bool in_pf)
@@ -778,6 +788,10 @@ static void set_huge_zero_page(pgtable_t pgtable, struct mm_struct *mm,
 	mm_inc_nr_ptes(mm);
 }
 
+/*
+ * called by:
+ *   - mm/memory.c|4782| <<create_huge_pmd>> return do_huge_pmd_anonymous_page(vmf);
+ */
 vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 {
 	struct vm_area_struct *vma = vmf->vma;
diff --git a/mm/memblock.c b/mm/memblock.c
index b5d302697..8baf56377 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -1539,6 +1539,10 @@ static void * __init memblock_alloc_internal(
  * Return:
  * Virtual address of allocated memory block on success, NULL on failure.
  */
+/*
+ * called by:
+ *   - mm/page_alloc.c|7009| <<memmap_alloc>> ptr = memblock_alloc_exact_nid_raw(size, align, min_addr,
+ */
 void * __init memblock_alloc_exact_nid_raw(
 			phys_addr_t size, phys_addr_t align,
 			phys_addr_t min_addr, phys_addr_t max_addr,
@@ -2124,6 +2128,11 @@ void __init reset_all_zones_managed_pages(void)
 /**
  * memblock_free_all - release free pages to the buddy allocator
  */
+/*
+ * x86在以下使用:
+ *   - arch/x86/mm/init_32.c|749| <<mem_init>> memblock_free_all();
+ *   - arch/x86/mm/init_64.c|1337| <<mem_init>> memblock_free_all();
+ */
 void __init memblock_free_all(void)
 {
 	unsigned long pages;
diff --git a/mm/memory.c b/mm/memory.c
index a78814413..d78816abf 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4776,6 +4776,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	goto out;
 }
 
+/*
+ * called by:
+ *   - mm/memory.c|5029| <<__handle_mm_fault>> ret = create_huge_pmd(&vmf);
+ */
 static inline vm_fault_t create_huge_pmd(struct vm_fault *vmf)
 {
 	if (vma_is_anonymous(vmf->vma))
@@ -4964,6 +4968,10 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * The mmap_lock may have been released depending on flags and our
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
+/*
+ * called by:
+ *   - mm/memory.c|5157| <<handle_mm_fault>> ret = __handle_mm_fault(vma, address, flags);
+ */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		unsigned long address, unsigned int flags)
 {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d04211f0e..ead24fd52 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -7000,6 +7000,12 @@ static void __init memmap_init(void)
 		init_unavailable_range(hole_pfn, end_pfn, zone_id, nid);
 }
 
+/*
+ * called by:
+ *   - mm/page_alloc.c|7823| <<alloc_node_mem_map>> map = memmap_alloc(size, SMP_CACHE_BYTES, MEMBLOCK_LOW_LIMIT,
+ *   - mm/sparse.c|440| <<__populate_section_memmap>> map = memmap_alloc(size, size, addr, nid, false);
+ *   - mm/sparse.c|467| <<sparse_buffer_init>> sparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);
+ */
 void __init *memmap_alloc(phys_addr_t size, phys_addr_t align,
 			  phys_addr_t min_addr, int nid, bool exact_nid)
 {
diff --git a/net/core/sock.c b/net/core/sock.c
index 788c13726..577f63e2d 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -2770,6 +2770,20 @@ DEFINE_STATIC_KEY_FALSE(net_high_order_alloc_disable_key);
  * no guarantee that allocations succeed. Therefore, @sz MUST be
  * less or equal than PAGE_SIZE.
  */
+/*
+ * called by:
+ *   - drivers/net/tun.c|1668| <<tun_build_skb>> if (unlikely(!skb_page_frag_refill(buflen, alloc_frag, GFP_KERNEL)))
+ *   - drivers/net/virtio_net.c|1313| <<add_recvbuf_small>> if (unlikely(!skb_page_frag_refill(len, alloc_frag, gfp)))
+ *   - drivers/net/virtio_net.c|1410| <<add_recvbuf_mergeable>> if (unlikely(!skb_page_frag_refill(len + room, alloc_frag, gfp)))
+ *   - net/core/sock.c|2809| <<sk_page_frag_refill>> if (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))
+ *   - net/ipv4/esp4.c|481| <<esp_output_head>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/ipv4/esp4.c|590| <<esp_output_tail>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/ipv6/esp6.c|516| <<esp6_output_head>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/ipv6/esp6.c|627| <<esp6_output_tail>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/mptcp/protocol.c|1091| <<mptcp_page_frag_refill>> if (likely(skb_page_frag_refill(32U + sizeof(struct mptcp_data_frag),
+ *   - net/tls/tls_device.c|331| <<tls_device_record_close>> if (likely(skb_page_frag_refill(prot->tag_size, pfrag,
+ *   - net/tls/tls_device.c|381| <<tls_do_allocation>> if (unlikely(!skb_page_frag_refill(prepend_size, pfrag,
+ */
 bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)
 {
 	if (pfrag->page) {
@@ -2804,6 +2818,16 @@ bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)
 }
 EXPORT_SYMBOL(skb_page_frag_refill);
 
+/*
+ * called by:
+ *   - net/ipv4/ip_output.c|1216| <<global>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/ipv6/ip6_output.c|1770| <<global>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/core/skbuff.c|2494| <<linear_to_page>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/core/skmsg.c|39| <<sk_msg_alloc>> if (!sk_page_frag_refill(sk, pfrag)) {
+ *   - net/ipv4/tcp.c|1352| <<tcp_sendmsg_locked>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/kcm/kcmsock.c|960| <<kcm_sendmsg>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/tls/tls_device.c|396| <<tls_do_allocation>> if (!sk_page_frag_refill(sk, pfrag))
+ */
 bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag)
 {
 	if (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))
diff --git a/net/ethernet/eth.c b/net/ethernet/eth.c
index 62b89d6f5..9d2c11376 100644
--- a/net/ethernet/eth.c
+++ b/net/ethernet/eth.c
@@ -301,6 +301,24 @@ EXPORT_SYMBOL(eth_prepare_mac_addr_change);
  * @dev: network device
  * @p: socket address
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_netdev.c|151| <<opa_vnic_process_vema_config>> eth_commit_mac_addr_change(netdev, &saddr);
+ *   - drivers/net/ethernet/broadcom/bgmac.c|1247| <<bgmac_set_mac_address>> eth_commit_mac_addr_change(net_dev, addr);
+ *   - drivers/net/ethernet/davicom/dm9051.c|1086| <<dm9051_set_mac_address>> eth_commit_mac_addr_change(ndev, p);
+ *   - drivers/net/ethernet/faraday/ftgmac100.c|220| <<ftgmac100_set_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/ethernet/marvell/mvneta.c|3890| <<mvneta_set_mac_addr>> eth_commit_mac_addr_change(dev, addr);
+ *   - drivers/net/ethernet/netronome/nfp/nfp_net_common.c|1920| <<nfp_net_set_mac_address>> eth_commit_mac_addr_change(netdev, addr);
+ *   - drivers/net/ethernet/pensando/ionic/ionic_lif.c|1654| <<ionic_set_mac_address>> eth_commit_mac_addr_change(netdev, addr);
+ *   - drivers/net/ethernet/pensando/ionic/ionic_lif.c|3288| <<ionic_station_set>> eth_commit_mac_addr_change(netdev, &addr);
+ *   - drivers/net/ethernet/ti/am65-cpsw-nuss.c|1222| <<am65_cpsw_nuss_ndo_slave_set_mac_address>> eth_commit_mac_addr_change(ndev, sockaddr);
+ *   - drivers/net/hyperv/netvsc_drv.c|1420| <<netvsc_set_mac_addr>> eth_commit_mac_addr_change(ndev, p);
+ *   - drivers/net/usb/qmi_wwan.c|636| <<qmi_wwan_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/virtio_net.c|2048| <<virtnet_set_mac_address>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/wireless/microchip/wilc1000/netdev.c|649| <<wilc_set_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/wireless/microchip/wilc1000/netdev.c|673| <<wilc_set_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - net/ethernet/eth.c|329| <<eth_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ */
 void eth_commit_mac_addr_change(struct net_device *dev, void *p)
 {
 	struct sockaddr *addr = p;
diff --git a/security/integrity/ima/ima_efi.c b/security/integrity/ima/ima_efi.c
index 9db66fe31..205bd10ba 100644
--- a/security/integrity/ima/ima_efi.c
+++ b/security/integrity/ima/ima_efi.c
@@ -11,8 +11,20 @@
 #define arch_ima_efi_boot_mode efi_secureboot_mode_unset
 #endif
 
+/*
+ * called by:
+ *   - security/integrity/ima/ima_efi.c|42| <<arch_ima_get_secureboot>> sb_mode = get_sb_mode();
+ */
 static enum efi_secureboot_mode get_sb_mode(void)
 {
+	/*
+	 * enum efi_secureboot_mode {
+	 *     efi_secureboot_mode_unset,
+	 *     efi_secureboot_mode_unknown,
+	 *     efi_secureboot_mode_disabled,
+	 *     efi_secureboot_mode_enabled,
+	 * };
+	 */
 	enum efi_secureboot_mode mode;
 
 	if (!efi_rt_services_supported(EFI_RT_SUPPORTED_GET_VARIABLE)) {
@@ -30,6 +42,13 @@ static enum efi_secureboot_mode get_sb_mode(void)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - security/integrity/ima/ima_appraise.c|29| <<ima_appraise_parse_cmdline>> bool sb_state = arch_ima_get_secureboot();
+ *   - security/integrity/ima/ima_efi.c|67| <<arch_get_ima_policy>> if (IS_ENABLED(CONFIG_IMA_ARCH_POLICY) && arch_ima_get_secureboot()) {
+ *   - security/integrity/ima/ima_main.c|803| <<ima_load_data>> && arch_ima_get_secureboot()) {
+ *   - security/integrity/platform_certs/load_uefi.c|214| <<load_uefi_certs>> if (!arch_ima_get_secureboot())
+ */
 bool arch_ima_get_secureboot(void)
 {
 	static enum efi_secureboot_mode sb_mode;
diff --git a/security/integrity/ima/ima_main.c b/security/integrity/ima/ima_main.c
index 040b03ddc..389d11546 100644
--- a/security/integrity/ima/ima_main.c
+++ b/security/integrity/ima/ima_main.c
@@ -790,6 +790,10 @@ int ima_post_read_file(struct file *file, void *buf, loff_t size,
  *
  * For permission return 0, otherwise return -EACCES.
  */
+/*
+ * called by:
+ *   - security/security.c|1777| <<security_kernel_load_data>> return ima_load_data(id, contents);
+ */
 int ima_load_data(enum kernel_load_data_id id, bool contents)
 {
 	bool ima_enforce, sig_enforce;
diff --git a/tools/include/linux/ring_buffer.h b/tools/include/linux/ring_buffer.h
index 6c0261737..0a6488b9d 100644
--- a/tools/include/linux/ring_buffer.h
+++ b/tools/include/linux/ring_buffer.h
@@ -50,6 +50,13 @@
 
 static inline u64 ring_buffer_read_head(struct perf_event_mmap_page *base)
 {
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 /*
  * Architectures where smp_load_acquire() does not fallback to
  * READ_ONCE() + smp_mb() pair.
diff --git a/tools/include/uapi/linux/perf_event.h b/tools/include/uapi/linux/perf_event.h
index 581ed4bdc..8f0bd7c60 100644
--- a/tools/include/uapi/linux/perf_event.h
+++ b/tools/include/uapi/linux/perf_event.h
@@ -673,6 +673,13 @@ struct perf_event_mmap_page {
 	 * data_{offset,size} indicate the location and size of the perf record
 	 * buffer within the mmapped area.
 	 */
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 	__u64   data_head;		/* head in the data section */
 	__u64	data_tail;		/* user-space written tail */
 	__u64	data_offset;		/* where the buffer starts */
diff --git a/tools/lib/perf/evlist.c b/tools/lib/perf/evlist.c
index 8ec5b9f34..dcf1cc742 100644
--- a/tools/lib/perf/evlist.c
+++ b/tools/lib/perf/evlist.c
@@ -430,6 +430,12 @@ static void perf_evlist__set_mmap_first(struct perf_evlist *evlist, struct perf_
 		evlist->mmap_first = map;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evlist.c|542| <<mmap_per_thread>> if (mmap_per_evsel(evlist, ops, idx, mp, 0, thread, &output,
+ *   - tools/lib/perf/evlist.c|552| <<mmap_per_thread>> if (mmap_per_evsel(evlist, ops, idx, mp, cpu, 0, &output,
+ *   - tools/lib/perf/evlist.c|583| <<mmap_per_cpu>> if (mmap_per_evsel(evlist, ops, cpu, mp, cpu,
+ */
 static int
 mmap_per_evsel(struct perf_evlist *evlist, struct perf_evlist_mmap_ops *ops,
 	       int idx, struct perf_mmap_param *mp, int cpu_idx,
diff --git a/tools/lib/perf/evsel.c b/tools/lib/perf/evsel.c
index 8ce5bbd09..cca5460aa 100644
--- a/tools/lib/perf/evsel.c
+++ b/tools/lib/perf/evsel.c
@@ -242,6 +242,11 @@ void perf_evsel__munmap(struct perf_evsel *evsel)
 	evsel->mmap = NULL;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/tests/test-evsel.c|153| <<test_stat_user_read>> err = perf_evsel__mmap(evsel, 0);
+ *   - tools/perf/tests/mmap-basic.c|204| <<test_stat_user_read>> err = perf_evsel__mmap(evsel, 0);
+ */
 int perf_evsel__mmap(struct perf_evsel *evsel, int pages)
 {
 	int ret, idx, thread;
@@ -384,6 +389,23 @@ static void perf_evsel__adjust_values(struct perf_evsel *evsel, u64 *buf,
 		count->lost = buf[n++];
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_evsel__read (evsel=evsel@entry=0x7c6130, cpu_map_idx=cpu_map_idx@entry=0, thread=thread@entry=0, count=0x7c63c8) at evsel.c:389
+ * #1  0x00000000004c42ff in evsel__read_one (thread=0, cpu_map_idx=0, evsel=0x7c6130) at util/evsel.c:1540
+ * #2  evsel__read_counter (evsel=evsel@entry=0x7c6130, cpu_map_idx=cpu_map_idx@entry=0, thread=thread@entry=0) at util/evsel.c:1629
+ * #3  0x000000000042b0b1 in read_single_counter (rs=0x7fffffff8fb0, thread=0, cpu_map_idx=0, counter=0x7c6130) at builtin-stat.c:367
+ * #4  read_counter_cpu (cpu_map_idx=0, rs=0x7fffffff8fb0, counter=0x7c6130) at builtin-stat.c:397
+ * #5  read_affinity_counters (rs=0x7fffffff8fb0) at builtin-stat.c:449
+ * #6  read_counters (rs=0x7fffffff8fb0) at builtin-stat.c:483
+ * #7  0x000000000042e01c in __run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1046
+ * #8  run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1071
+ * #9  cmd_stat (argc=2, argv=<optimized out>) at builtin-stat.c:2577
+ * #10 0x00000000004a544b in run_builtin (p=p@entry=0x719650 <commands+336>, argc=argc@entry=6, argv=argv@entry=0x7fffffffdf70) at perf.c:316
+ * #11 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #12 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #13 main (argc=6, argv=0x7fffffffdf70) at perf.c:544
+ */
 int perf_evsel__read(struct perf_evsel *evsel, int cpu_map_idx, int thread,
 		     struct perf_counts_values *count)
 {
@@ -405,6 +427,20 @@ int perf_evsel__read(struct perf_evsel *evsel, int cpu_map_idx, int thread,
 	    !perf_mmap__read_self(MMAP(evsel, cpu_map_idx, thread), count))
 		return 0;
 
+	/*
+	 * 15 struct perf_counts_values {
+	 * 16         union {
+	 * 17                 struct {
+	 * 18                         uint64_t val;
+	 * 19                         uint64_t ena;
+	 * 20                         uint64_t run;
+	 * 21                         uint64_t id;
+	 * 22                         uint64_t lost;
+	 * 23                 };
+	 * 24                 uint64_t values[5];
+	 * 25         };
+	 * 26 };
+	 */
 	if (readn(*fd, buf.values, size) <= 0)
 		return -errno;
 
diff --git a/tools/lib/perf/mmap.c b/tools/lib/perf/mmap.c
index 0d1634ced..9446bd8c6 100644
--- a/tools/lib/perf/mmap.c
+++ b/tools/lib/perf/mmap.c
@@ -32,6 +32,44 @@ size_t perf_mmap__mmap_len(struct perf_mmap *map)
 	return map->mask + 1 + page_size;
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7ffff7eeb010, mp=mp@entry=0x7fffffff8840, fd=fd@entry=5, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7ffff7eeb010, mp=0x7fffffff8840, fd=5, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7a7480, ops=0x7fffffff8820, idx=0, mp=0x7fffffff8840, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87d0, _output_overwrite=0x7fffffff87d4,
+ *     nr_mmaps=0x7fffffff87cc) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8840, ops=<optimized out>, evlist=0x7a7480) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7a7480, ops=ops@entry=0x7fffffff8820, mp=mp@entry=0x7fffffff8840) at evlist.c:642
+ * #5  0x00000000004bcecc in evlist__mmap_ex (evlist=evlist@entry=0x7a7480, pages=<optimized out>, auxtrace_pages=0, auxtrace_overwrite=auxtrace_overwrite@entry=false, nr_cblocks=<optimized out>,
+ *     affinity=<optimized out>, flush=1, comp_level=0) at util/evlist.c:1001
+ * #6  0x0000000000422a28 in record__mmap_evlist (rec=0x707020 <record>, evlist=0x7a7480) at builtin-record.c:1159
+ * #7  record__mmap (rec=0x707020 <record>) at builtin-record.c:1208
+ * #8  record__open (rec=0x707020 <record>) at builtin-record.c:1292
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2304
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7fffec363010, mp=mp@entry=0x7fffffff8820, fd=fd@entry=21, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7fffec363010, mp=0x7fffffff8820, fd=21, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7c9850, ops=0x7fffffff8800, idx=0, mp=0x7fffffff8820, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87b0, _output_overwrite=0x7fffffff87b4,
+ *     nr_mmaps=0x7fffffff87ac) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8820, ops=<optimized out>, evlist=0x7c9850) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7c9850, ops=ops@entry=0x7fffffff8800, mp=mp@entry=0x7fffffff8820) at evlist.c:642
+ * #5  0x00000000004bcfeb in evlist__mmap_ex (comp_level=0, flush=1, affinity=0, nr_cblocks=0, auxtrace_overwrite=false, auxtrace_pages=0, pages=4294967295, evlist=0x7c9850) at util/evlist.c:1001
+ * #6  evlist__mmap (evlist=evlist@entry=0x7c9850, pages=pages@entry=4294967295) at util/evlist.c:1006
+ * #7  0x00000000004c0745 in evlist__start_sb_thread (evlist=0x7c9850, target=target@entry=0x707160 <record+320>) at util/sideband_evlist.c:122
+ * #8  0x0000000000422578 in record__setup_sb_evlist (rec=0x707020 <record>) at builtin-record.c:1984
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2359
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ */
 int perf_mmap__mmap(struct perf_mmap *map, struct perf_mmap_param *mp,
 		    int fd, struct perf_cpu cpu)
 {
@@ -138,6 +176,10 @@ static int overwrite_rb_find_range(void *buf, int mask, u64 *start, u64 *end)
 /*
  * Report the start and end of the available data in ringbuffer
  */
+/*
+ * called by:
+ *   - tools/lib/perf/mmap.c|221| <<perf_mmap__read_init>> return __perf_mmap__read_init(map);
+ */
 static int __perf_mmap__read_init(struct perf_mmap *md)
 {
 	u64 head = perf_mmap__read_head(md);
diff --git a/tools/perf/builtin-record.c b/tools/perf/builtin-record.c
index 0f711f888..8a89d6959 100644
--- a/tools/perf/builtin-record.c
+++ b/tools/perf/builtin-record.c
@@ -110,6 +110,10 @@ struct record_thread {
 	struct mmap		**maps;
 	struct mmap		**overwrite_maps;
 	struct record		*rec;
+	/*
+	 * 似乎只在这一处增加record_thread->samples:
+	 * - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+	 */
 	unsigned long long	samples;
 	unsigned long		waking;
 	u64			bytes_written;
@@ -232,6 +236,32 @@ static bool record__output_max_size_exceeded(struct record *rec)
 	       (record__bytes_written(rec) >= rec->output_max_size);
 }
 
+/*
+ * (gdb) bt
+ * #0  record__write (size=528, bf=0x7ca940, map=0x0, rec=0x707020 <record>) at builtin-record.c:608
+ * #1  process_synthesized_event (tool=tool@entry=0x707020 <record>, event=event@entry=0x7ca940, sample=sample@entry=0x0, machine=machine@entry=0x7a8148) at builtin-record.c:608
+ * #2  0x000000000054e929 in __perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148,
+ *     from=from@entry=0) at util/synthetic-events.c:1873
+ * #3  0x000000000054eb2f in perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148)
+ *     at util/synthetic-events.c:1883
+ * #4  0x0000000000421225 in record__synthesize (tail=tail@entry=false, rec=0x707020 <record>) at builtin-record.c:1853
+ * #5  0x0000000000422519 in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2356
+ * #6  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #7  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #8  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #9  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #10 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/builtin-record.c|661| <<process_synthesized_event>> return record__write(rec, NULL, event, event->header.size);
+ *   - tools/perf/builtin-record.c|688| <<record__pushfn>> return record__write(rec, map, bf, size);
+ *   - tools/perf/builtin-record.c|768| <<record__process_auxtrace>> record__write(rec, map, event, event->header.size);
+ *   - tools/perf/builtin-record.c|769| <<record__process_auxtrace>> record__write(rec, map, data1, len1);
+ *   - tools/perf/builtin-record.c|771| <<record__process_auxtrace>> record__write(rec, map, data2, len2);
+ *   - tools/perf/builtin-record.c|772| <<record__process_auxtrace>> record__write(rec, map, &pad, padding);
+ *   - tools/perf/builtin-record.c|1607| <<record__mmap_read_evlist>> rc = record__write(rec, NULL, &finished_round_event, sizeof(finished_round_event));
+ *   - tools/perf/builtin-record.c|1792| <<write_finished_init>> return record__write(rec, NULL, &finished_init_event, sizeof(finished_init_event));
+ */
 static int record__write(struct record *rec, struct mmap *map __maybe_unused,
 			 void *bf, size_t size)
 {
@@ -426,6 +456,10 @@ static int record__aio_pushfn(struct mmap *map, void *to, void *buf, size_t size
 	return size;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1615| <<record__mmap_read_evlist>> if (record__aio_push(rec, map, &off) < 0) {
+ */
 static int record__aio_push(struct record *rec, struct mmap *map, off_t *off)
 {
 	int ret, idx;
@@ -439,6 +473,12 @@ static int record__aio_push(struct record *rec, struct mmap *map, off_t *off)
 
 	idx = record__aio_sync(map, false);
 	aio.data = map->aio.data[idx];
+	/*
+	 * called by:
+	 *   - 定义tools/perf/util/mmap.c|324| <<global>> int perf_mmap__push(struct mmap *md, void *to,
+	 *   - tools/perf/builtin-record.c|468| <<record__aio_push>> ret = perf_mmap__push(map, &aio, record__aio_pushfn);
+	 *   - tools/perf/builtin-record.c|1608| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+	 */
 	ret = perf_mmap__push(map, &aio, record__aio_pushfn);
 	if (ret != 0) /* ret > 0 - no data, ret < 0 - error */
 		return ret;
@@ -599,6 +639,43 @@ static int record__comp_enabled(struct record *rec)
 	return rec->opts.comp_level > 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  process_synthesized_event (tool=tool@entry=0x707020 <record>, event=event@entry=0x7ca940, sample=sample@entry=0x0, machine=machine@entry=0x7a8148) at builtin-record.c:606
+ * #1  0x000000000054e859 in __perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148, 
+ *     from=from@entry=0) at util/synthetic-events.c:1873
+ * #2  0x000000000054ea5f in perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148)
+ *     at util/synthetic-events.c:1883
+ * #3  0x0000000000421225 in record__synthesize (tail=tail@entry=false, rec=0x707020 <record>) at builtin-record.c:1853
+ * #4  0x000000000042251c in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2355
+ * #5  0x00000000004255d5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4079
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * callled by:
+ *   - tools/perf/builtin-record.c|620| <<process_locked_synthesized_event>> ret = process_synthesized_event(tool, event, sample, machine);
+ *   - tools/perf/builtin-record.c|1369| <<perf_event__synthesize_guest_os>> err = perf_event__synthesize_modules(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1379| <<perf_event__synthesize_guest_os>> err = perf_event__synthesize_kernel_mmap(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1696| <<record__synthesize_workload>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1833| <<record__synthesize>> event_op f = process_synthesized_event;
+ *   - tools/perf/builtin-record.c|1840| <<record__synthesize>> process_synthesized_event);
+ *   - tools/perf/builtin-record.c|1848| <<record__synthesize>> process_synthesized_event, machine);
+ *   - tools/perf/builtin-record.c|1854| <<record__synthesize>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1861| <<record__synthesize>> session, process_synthesized_event);
+ *   - tools/perf/builtin-record.c|1867| <<record__synthesize>> err = perf_event__synthesize_kernel_mmap(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1873| <<record__synthesize>> err = perf_event__synthesize_modules(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1887| <<record__synthesize>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1893| <<record__synthesize>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1901| <<record__synthesize>> process_synthesized_event, NULL);
+ *   - tools/perf/builtin-record.c|1907| <<record__synthesize>> err = perf_event__synthesize_bpf_events(session, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1915| <<record__synthesize>> err = perf_event__synthesize_cgroups(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2407| <<__cmd_record>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2427| <<__cmd_record>> tgid, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2407| <<__cmd_record>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2427| <<__cmd_record>> tgid, process_synthesized_event,
+ */
 static int process_synthesized_event(struct perf_tool *tool,
 				     union perf_event *event,
 				     struct perf_sample *sample __maybe_unused,
@@ -622,16 +699,44 @@ static int process_locked_synthesized_event(struct perf_tool *tool,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1599| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+ *
+ * __cmd_record() or record__thread()
+ * -> record__mmap_read_all()
+ *    -> record__mmap_read_evlist()
+ *       -> perf_mmap__push()
+ *          -> record__pushfn()
+ */
 static int record__pushfn(struct mmap *map, void *to, void *bf, size_t size)
 {
 	struct record *rec = to;
 
+	/*
+	 * 似乎默认是0
+	 */
 	if (record__comp_enabled(rec)) {
 		size = zstd_compress(rec->session, map, map->data, mmap__mmap_len(map), bf, size);
 		bf   = map->data;
 	}
 
+	/*
+	 * 似乎只在这一处增加record_thread->samples:
+	 * - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+	 */
 	thread->samples++;
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|661| <<process_synthesized_event>> return record__write(rec, NULL, event, event->header.size);
+	 *   - tools/perf/builtin-record.c|688| <<record__pushfn>> return record__write(rec, map, bf, size);
+	 *   - tools/perf/builtin-record.c|768| <<record__process_auxtrace>> record__write(rec, map, event, event->header.size);
+	 *   - tools/perf/builtin-record.c|769| <<record__process_auxtrace>> record__write(rec, map, data1, len1);
+	 *   - tools/perf/builtin-record.c|771| <<record__process_auxtrace>> record__write(rec, map, data2, len2);
+	 *   - tools/perf/builtin-record.c|772| <<record__process_auxtrace>> record__write(rec, map, &pad, padding);
+	 *   - tools/perf/builtin-record.c|1607| <<record__mmap_read_evlist>> rc = record__write(rec, NULL, &finished_round_event, sizeof(finished_round_event));
+	 *   - tools/perf/builtin-record.c|1792| <<write_finished_init>> return record__write(rec, NULL, &finished_init_event, sizeof(finished_init_event));
+	 */
 	return record__write(rec, map, bf, size);
 }
 
@@ -683,6 +788,11 @@ static void record__sig_exit(void)
 
 #ifdef HAVE_AUXTRACE_SUPPORT
 
+/*
+ * 在以下使用record__process_auxtrace():
+ *   - tools/perf/builtin-record.c|793| <<record__auxtrace_mmap_read>> record__process_auxtrace);
+ *   - tools/perf/builtin-record.c|809| <<record__auxtrace_mmap_read_snapshot>> record__process_auxtrace,
+ */
 static int record__process_auxtrace(struct perf_tool *tool,
 				    struct mmap *map,
 				    union perf_event *event, void *data1,
@@ -1144,6 +1254,15 @@ static int record__alloc_thread_data(struct record *rec, struct evlist *evlist)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1261| <<record__mmap>> return record__mmap_evlist(rec, rec->evlist);
+ *
+ * __cmd_record()
+ * -> record__open()
+ *    -> record__mmap()
+ *       -> record__mmap_evlist()
+ */
 static int record__mmap_evlist(struct record *rec,
 			       struct evlist *evlist)
 {
@@ -1203,11 +1322,29 @@ static int record__mmap_evlist(struct record *rec,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1344| <<record__open>> rc = record__mmap(rec);
+ *
+ * __cmd_record()
+ * -> record__open()
+ *    -> record__mmap()
+ *       -> record__mmap_evlist()
+ */
 static int record__mmap(struct record *rec)
 {
 	return record__mmap_evlist(rec, rec->evlist);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2366| <<__cmd_record>> if (record__open(rec) != 0) {
+ *
+ * __cmd_record()
+ * -> record__open()
+ *    -> record__mmap()
+ *       -> record__mmap_evlist()
+ */
 static int record__open(struct record *rec)
 {
 	char msg[BUFSIZ];
@@ -1248,6 +1385,19 @@ static int record__open(struct record *rec)
 
 	evlist__for_each_entry(evlist, pos) {
 try_again:
+		/*
+		 * called by:
+		 *   - tools/perf/builtin-record.c|1251| <<record__open>> if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
+		 *   - tools/perf/builtin-top.c|1024| <<perf_top__start_counters>> if (evsel__open(counter, top->evlist->core.user_requested_cpus,
+		 *   - tools/perf/tests/mmap-basic.c|98| <<test__basic_mmap>> if (evsel__open(evsels[i], cpus, threads) < 0) {
+		 *   - tools/perf/tests/openat-syscall-all-cpus.c|56| <<test__openat_syscall_event_on_all_cpus>> if (evsel__open(evsel, cpus, threads) < 0) {
+		 *   - tools/perf/util/evlist.c|1361| <<evlist__open>> err = evsel__open(evsel, evsel->core.cpus, evsel->core.threads);
+		 *   - tools/perf/util/evsel.c|2207| <<evsel__open_per_thread>> return evsel__open(evsel, NULL, threads);
+		 *   - tools/perf/util/parse-events.c|178| <<is_event_supported>> open_return = evsel__open(evsel, NULL, tmap);
+		 *   - tools/perf/util/parse-events.c|190| <<is_event_supported>> ret = evsel__open(evsel, NULL, tmap) >= 0;
+		 *   - tools/perf/util/python.c|904| <<pyrf_evsel__open>> if (evsel__open(evsel, cpus, threads) < 0) {
+		 *   - tools/perf/util/sideband_evlist.c|117| <<evlist__start_sb_thread>> if (evsel__open(counter, evlist->core.user_requested_cpus,
+		 */
 		if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
 			if (evsel__fallback(pos, errno, msg, sizeof(msg))) {
 				if (verbose > 0)
@@ -1288,6 +1438,15 @@ static int record__open(struct record *rec)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1344| <<record__open>> rc = record__mmap(rec);
+	 *
+	 * __cmd_record()
+	 * -> record__open()
+	 *    -> record__mmap()
+	 *       -> record__mmap_evlist()
+	 */
 	rc = record__mmap(rec);
 	if (rc)
 		goto out;
@@ -1307,6 +1466,57 @@ static void set_timestamp_boundary(struct record *rec, u64 sample_time)
 		rec->evlist->last_sample_time = sample_time;
 }
 
+/*
+ * 3433 static struct record record = {
+ * 3434         .opts = {
+ * 3435                 .sample_time         = true,
+ * 3436                 .mmap_pages          = UINT_MAX,
+ * 3437                 .user_freq           = UINT_MAX,
+ * 3438                 .user_interval       = ULLONG_MAX,
+ * 3439                 .freq                = 4000,
+ * 3440                 .target              = {
+ * 3441                         .uses_mmap   = true,
+ * 3442                         .default_per_cpu = true,
+ * 3443                 },
+ * 3444                 .mmap_flush          = MMAP_FLUSH_DEFAULT,
+ * 3445                 .nr_threads_synthesize = 1,
+ * 3446                 .ctl_fd              = -1,
+ * 3447                 .ctl_fd_ack          = -1,
+ * 3448                 .synth               = PERF_SYNTH_ALL,
+ * 3449         },
+ * 3450         .tool = {
+ * 3451                 .sample         = process_sample_event,
+ * 3452                 .fork           = perf_event__process_fork,
+ * 3453                 .exit           = perf_event__process_exit,
+ * 3454                 .comm           = perf_event__process_comm,
+ * 3455                 .namespaces     = perf_event__process_namespaces,
+ * 3456                 .mmap           = build_id__process_mmap,
+ * 3457                 .mmap2          = build_id__process_mmap2,
+ * 3458                 .itrace_start   = process_timestamp_boundary,
+ * 3459                 .aux            = process_timestamp_boundary,
+ * 3460                 .ordered_events = true,
+ * 3461         },
+ * 3461 };
+ *
+ * (gdb) bt
+ * #0  process_sample_event (tool=0x707020 <record>, event=0x7ffff7e69da8, sample=0x7fffffff8100, evsel=0x7c6130, machine=0x7a8148) at builtin-record.c:1318
+ * #1  0x0000000000508e80 in perf_session__deliver_event (session=0x7a7f40, event=0x7ffff7e69da8, tool=0x707020 <record>, file_offset=7592, file_path=0x7c6640 "perf.data") at util/session.c:1635
+ * #2  0x000000000050dbb6 in do_flush (show_progress=true, oe=0x7aeac0) at util/ordered-events.c:245
+ * #3  __ordered_events__flush (oe=0x7aeac0, oe@entry=0x0, how=how@entry=OE_FLUSH__FINAL, timestamp=timestamp@entry=0) at util/ordered-events.c:324
+ * #4  0x000000000050e3e5 in __ordered_events__flush (timestamp=<optimized out>, how=<optimized out>, oe=<optimized out>) at util/ordered-events.c:340
+ * #5  ordered_events__flush (how=OE_FLUSH__FINAL, oe=0x0) at util/ordered-events.c:342
+ * #6  ordered_events__flush (oe=oe@entry=0x7aeac0, how=how@entry=OE_FLUSH__FINAL) at util/ordered-events.c:340
+ * #7  0x000000000050b38d in __perf_session__process_events (session=0x7a7f40) at util/session.c:2456
+ * #8  perf_session__process_events (session=session@entry=0x7a7f40) at util/session.c:2618
+ * #9  0x000000000041f1f2 in process_buildids (rec=0x707020 <record>) at builtin-record.c:1357
+ * #10 record__finish_output (rec=0x707020 <record>) at builtin-record.c:1675
+ * #11 0x00000000004223c0 in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2649
+ * #12 0x00000000004256c5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4089
+ * #13 0x00000000004a547b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #14 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #15 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #16 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ */
 static int process_sample_event(struct perf_tool *tool,
 				union perf_event *event,
 				struct perf_sample *sample,
@@ -1450,6 +1660,23 @@ static size_t zstd_compress(struct perf_session *session, struct mmap *map,
 	return compressed;
 }
 
+/*
+ * (gdb) bt
+ * #0  record__mmap_read_evlist (rec=rec@entry=0x707020 <record>, evlist=0x7a7480, overwrite=overwrite@entry=false, 
+ *     synch=synch@entry=true) at builtin-record.c:1456
+ * #1  0x000000000042352e in record__mmap_read_all (synch=true, rec=0x707020 <record>) at builtin-record.c:2620
+ * #2  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2620
+ * #3  0x0000000000425705 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4097
+ * #4  0x00000000004a55bb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, 
+ *     argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #5  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #6  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #7  main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/builtin-record.c|1619| <<record__mmap_read_all>> err = record__mmap_read_evlist(rec, rec->evlist, false, synch);
+ *   - tools/perf/builtin-record.c|1623| <<record__mmap_read_all>> return record__mmap_read_evlist(rec, rec->evlist, true, synch);
+ */
 static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 				    bool overwrite, bool synch)
 {
@@ -1458,12 +1685,28 @@ static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 	int rc = 0;
 	int nr_mmaps;
 	struct mmap **maps;
+	/*
+	 * struct record *rec:
+	 * -> struct perf_data data;
+	 *    -> const char *path;
+	 *    -> struct perf_data_file file;
+	 *         char            *path;
+	 *         union {                      
+	 *             int      fd;
+	 *             FILE    *fptr;
+	 *         };
+	 *         unsigned long    size;
+	 */
 	int trace_fd = rec->data.file.fd;
 	off_t off = 0;
 
 	if (!evlist)
 		return 0;
 
+	/*
+	 * 文件的开头声明:
+	 * static __thread struct record_thread *thread;
+	 */
 	nr_mmaps = thread->nr_mmaps;
 	maps = overwrite ? thread->overwrite_maps : thread->maps;
 
@@ -1487,6 +1730,12 @@ static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 				map->core.flush = 1;
 			}
 			if (!record__aio_enabled(rec)) {
+				/*
+				 * called by:
+				 *   - 定义tools/perf/util/mmap.c|324| <<global>> int perf_mmap__push(struct mmap *md, void *to,
+				 *   - tools/perf/builtin-record.c|468| <<record__aio_push>> ret = perf_mmap__push(map, &aio, record__aio_pushfn);
+				 *   - tools/perf/builtin-record.c|1608| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+				 */
 				if (perf_mmap__push(map, rec, record__pushfn) < 0) {
 					if (synch)
 						map->core.flush = flush;
@@ -1534,6 +1783,13 @@ static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1658| <<record__thread>> if (record__mmap_read_all(thread->rec, false) < 0 || terminate)
+ *   - tools/perf/builtin-record.c|1687| <<record__thread>> record__mmap_read_all(thread->rec, true);
+ *   - tools/perf/builtin-record.c|2601| <<__cmd_record>> if (record__mmap_read_all(rec, false) < 0) {
+ *   - tools/perf/builtin-record.c|2738| <<__cmd_record>> record__mmap_read_all(rec, true);
+ */
 static int record__mmap_read_all(struct record *rec, bool synch)
 {
 	int err;
@@ -1542,6 +1798,11 @@ static int record__mmap_read_all(struct record *rec, bool synch)
 	if (err)
 		return err;
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1619| <<record__mmap_read_all>> err = record__mmap_read_evlist(rec, rec->evlist, false, synch);
+	 *   - tools/perf/builtin-record.c|1623| <<record__mmap_read_all>> return record__mmap_read_evlist(rec, rec->evlist, true, synch);
+	 */
 	return record__mmap_read_evlist(rec, rec->evlist, true, synch);
 }
 
@@ -1554,6 +1815,10 @@ static void record__thread_munmap_filtered(struct fdarray *fda, int fd,
 		perf_mmap__put(map);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2282| <<record__start_threads>> if (pthread_create(&handle, &attrs, record__thread, &thread_data[t])) {
+ */
 static void *record__thread(void *arg)
 {
 	enum thread_msg msg = THREAD_MSG__READY;
@@ -1577,6 +1842,13 @@ static void *record__thread(void *arg)
 	for (;;) {
 		unsigned long long hits = thread->samples;
 
+		/*
+		 * called by:
+		 *   - tools/perf/builtin-record.c|1658| <<record__thread>> if (record__mmap_read_all(thread->rec, false) < 0 || terminate)
+		 *   - tools/perf/builtin-record.c|1687| <<record__thread>> record__mmap_read_all(thread->rec, true);
+		 *   - tools/perf/builtin-record.c|2601| <<__cmd_record>> if (record__mmap_read_all(rec, false) < 0) {
+		 *   - tools/perf/builtin-record.c|2738| <<__cmd_record>> record__mmap_read_all(rec, true);
+		 */
 		if (record__mmap_read_all(thread->rec, false) < 0 || terminate)
 			break;
 
@@ -1651,6 +1923,11 @@ static void record__init_features(struct record *rec)
 	perf_header__clear_feat(&session->header, HEADER_STAT);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1791| <<record__switch_output>> record__finish_output(rec);
+ *   - tools/perf/builtin-record.c|2714| <<__cmd_record>> record__finish_output(rec);
+ */
 static void
 record__finish_output(struct record *rec)
 {
@@ -1679,6 +1956,12 @@ record__finish_output(struct record *rec)
 	return;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2002| <<record__switch_output>> record__synthesize_workload(rec, true);
+ *   - tools/perf/builtin-record.c|2053| <<record__switch_output>> record__synthesize_workload(rec, false);
+ *   - tools/perf/builtin-record.c|2965| <<__cmd_record>> record__synthesize_workload(rec, true);
+ */
 static int record__synthesize_workload(struct record *rec, bool tail)
 {
 	int err;
@@ -1701,6 +1984,13 @@ static int record__synthesize_workload(struct record *rec, bool tail)
 	return err;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1814| <<record__switch_output>> write_finished_init(rec, true);
+ *   - tools/perf/builtin-record.c|1870| <<record__switch_output>> write_finished_init(rec, false);
+ *   - tools/perf/builtin-record.c|2580| <<__cmd_record>> err = write_finished_init(rec, false);
+ *   - tools/perf/builtin-record.c|2731| <<__cmd_record>> write_finished_init(rec, true);
+ */
 static int write_finished_init(struct record *rec, bool tail)
 {
 	if (rec->opts.tail_synthesize != tail)
@@ -1711,6 +2001,11 @@ static int write_finished_init(struct record *rec, bool tail)
 
 static int record__synthesize(struct record *rec, bool tail);
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2578| <<__cmd_record>> fd = record__switch_output(rec, false);
+ *   - tools/perf/builtin-record.c|2711| <<__cmd_record>> fd = record__switch_output(rec, true);
+ */
 static int
 record__switch_output(struct record *rec, bool at_exit)
 {
@@ -1822,6 +2117,13 @@ static const struct perf_event_mmap_page *record__pick_pc(struct record *rec)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1765| <<record__switch_output>> record__synthesize(rec, true);
+ *   - tools/perf/builtin-record.c|1806| <<record__switch_output>> record__synthesize(rec, false);
+ *   - tools/perf/builtin-record.c|2396| <<__cmd_record>> err = record__synthesize(rec, false);
+ *   - tools/perf/builtin-record.c|2674| <<__cmd_record>> record__synthesize(rec, true);
+ */
 static int record__synthesize(struct record *rec, bool tail)
 {
 	struct perf_session *session = rec->session;
@@ -1948,6 +2250,10 @@ static int record__process_signal_event(union perf_event *event __maybe_unused,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2587| <<__cmd_record>> err = record__setup_sb_evlist(rec);
+ */
 static int record__setup_sb_evlist(struct record *rec)
 {
 	struct record_opts *opts = &rec->opts;
@@ -2033,6 +2339,10 @@ static void hit_auxtrace_snapshot_trigger(struct record *rec)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2364| <<__cmd_record>> record__uniquify_name(rec);
+ */
 static void record__uniquify_name(struct record *rec)
 {
 	struct evsel *pos;
@@ -2040,6 +2350,9 @@ static void record__uniquify_name(struct record *rec)
 	char *new_name;
 	int ret;
 
+	/*
+	 * 一般不用hybrid
+	 */
 	if (!perf_pmu__has_hybrid())
 		return;
 
@@ -2077,6 +2390,10 @@ static int record__terminate_thread(struct record_thread *thread_data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2593| <<__cmd_record>> if (record__start_threads(rec))
+ */
 static int record__start_threads(struct record *rec)
 {
 	int t, tt, err, ret = 0, nr_threads = rec->nr_threads;
@@ -2140,6 +2457,10 @@ static int record__start_threads(struct record *rec)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2913| <<__cmd_record>> record__stop_threads(rec);
+ */
 static int record__stop_threads(struct record *rec)
 {
 	int t;
@@ -2178,6 +2499,32 @@ static unsigned long record__waking(struct record *rec)
 	return waking;
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at ut    il/ses          sion.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ *
+ * called by:
+ *   - tools/perf/builtin-record.c|4079| <<cmd_record>> err = __cmd_record(&record, argc, argv);
+ *
+ * struct record:
+ * -> struct evlist *evlist;
+ * -> struct perf_session *session;
+ */
 static int __cmd_record(struct record *rec, int argc, const char **argv)
 {
 	int err;
@@ -2237,6 +2584,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		}
 	}
 
+	/*
+	 * 在open_file_write()创建
+	 * (gdb) p data->file.path
+	 * $1 = 0x7c6640 "perf.data"
+	 *
+	 * int fd;
+	 */
 	fd = perf_data__fd(data);
 	rec->session = session;
 
@@ -2294,6 +2648,14 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 
 	record__uniquify_name(rec);
 
+	/*
+	 * __cmd_record()
+	 * -> record__open()
+	 *    -> record__mmap()
+	 *       -> record__mmap_evlist()
+	 *
+	 * 这里打开那些fd
+	 */
 	if (record__open(rec) != 0) {
 		err = -1;
 		goto out_free_threads;
@@ -2330,11 +2692,21 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	if (!rec->evlist->core.nr_groups)
 		perf_header__clear_feat(&session->header, HEADER_GROUP_DESC);
 
+	/*
+	 * 一般的record的is_pipe是zero
+	 */
 	if (data->is_pipe) {
 		err = perf_header__write_pipe(fd);
 		if (err < 0)
 			goto out_free_threads;
 	} else {
+		/*
+		 * 在open_file_write()创建
+		 * (gdb) p data->file.path
+		 * $1 = 0x7c6640 "perf.data"
+		 *
+		 * int fd;
+		 */
 		err = perf_session__write_header(session, rec->evlist, fd, false);
 		if (err < 0)
 			goto out_free_threads;
@@ -2352,6 +2724,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	if (err)
 		goto out_free_threads;
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1765| <<record__switch_output>> record__synthesize(rec, true);
+	 *   - tools/perf/builtin-record.c|1806| <<record__switch_output>> record__synthesize(rec, false);
+	 *   - tools/perf/builtin-record.c|2396| <<__cmd_record>> err = record__synthesize(rec, false);
+	 *   - tools/perf/builtin-record.c|2674| <<__cmd_record>> record__synthesize(rec, true);
+	 */
 	err = record__synthesize(rec, false);
 	if (err < 0)
 		goto out_free_threads;
@@ -2367,8 +2746,17 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		}
 	}
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|2593| <<__cmd_record>> if (record__start_threads(rec))
+	 *
+	 * 每个thread的函数是record__thread() !!!!!!!!!!
+	 */
 	if (record__start_threads(rec))
 		goto out_free_threads;
+	/*
+	 * 这里还没有hang
+	 */
 
 	/*
 	 * When perf is starting the traced process, all the events
@@ -2386,6 +2774,15 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		union perf_event *event;
 		pid_t tgid;
 
+		/*
+		 * union perf_event {
+		 *     struct perf_event_header                header;
+		 *     struct perf_record_mmap                 mmap;
+		 *     struct perf_record_mmap2                mmap2;
+		 *     struct perf_record_comm                 comm;
+		 *     struct perf_record_namespaces           namespaces;
+		 *     ... ...
+		 */
 		event = malloc(sizeof(event->comm) + machine->id_hdr_size);
 		if (event == NULL) {
 			err = -ENOMEM;
@@ -2440,6 +2837,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	trigger_ready(&switch_output_trigger);
 	perf_hooks__invoke_record_start();
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1814| <<record__switch_output>> write_finished_init(rec, true);
+	 *   - tools/perf/builtin-record.c|1870| <<record__switch_output>> write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2580| <<__cmd_record>> err = write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2731| <<__cmd_record>> write_finished_init(rec, true);
+	 */
 	/*
 	 * Must write FINISHED_INIT so it will be seen after all other
 	 * synthesized user events, but before any regular events.
@@ -2448,7 +2852,19 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	if (err < 0)
 		goto out_child;
 
+	/*
+	 * 时间都花在for loop
+	 */
 	for (;;) {
+		/*
+		 * __cmd_record() or record__thread()
+		 * -> record__mmap_read_all()
+		 *    -> record__mmap_read_evlist()
+		 *       -> record__pushfn()
+		 *
+		 * 似乎只在这一处增加record_thread->samples:
+		 *   - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+		 */
 		unsigned long long hits = thread->samples;
 
 		/*
@@ -2462,6 +2878,20 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		if (trigger_is_hit(&switch_output_trigger) || done || draining)
 			evlist__toggle_bkw_mmap(rec->evlist, BKW_MMAP_DATA_PENDING);
 
+		/*
+		 * __cmd_record() or record__thread()
+		 * -> record__mmap_read_all()
+		 *    -> record__mmap_read_evlist()
+		 *       -> perf_mmap__push()
+		 *          -> record__pushfn()
+		 *
+		 * record__mmap_read_all()
+		 * -> record__mmap_read_evlist()
+		 *    -> perf_mmap__push(map, rec, record__pushfn)
+		 *       -> perf_mmap__read_init()
+		 *       -> push = record__pushfn()
+		 *       -> perf_mmap__consume()
+		 */
 		if (record__mmap_read_all(rec, false) < 0) {
 			trigger_error(&auxtrace_snapshot_trigger);
 			trigger_error(&switch_output_trigger);
@@ -2518,7 +2948,16 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 				alarm(rec->switch_output.time);
 		}
 
+		/*
+		 * 似乎只在这一处增加record_thread->samples:
+		 *   - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+		 *
+		 * struct record_thread
+		 */
 		if (hits == thread->samples) {
+			/*
+			 * failed的测试是在sig_handler()设置的done, 可能进程被杀死了
+			 */
 			if (done || draining)
 				break;
 			err = fdarray__poll(&thread->pollfd, -1);
@@ -2592,6 +3031,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		fprintf(stderr, "[ perf record: Woken up %ld times to write data ]\n",
 			record__waking(rec));
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1814| <<record__switch_output>> write_finished_init(rec, true);
+	 *   - tools/perf/builtin-record.c|1870| <<record__switch_output>> write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2580| <<__cmd_record>> err = write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2731| <<__cmd_record>> write_finished_init(rec, true);
+	 */
 	write_finished_init(rec, true);
 
 	if (target__none(&rec->opts.target))
@@ -3765,6 +4211,25 @@ static int record__init_thread_masks(struct record *rec)
 	return ret;
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at ut    il/ses     sion.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ */
 int cmd_record(int argc, const char **argv)
 {
 	int err;
diff --git a/tools/perf/builtin-report.c b/tools/perf/builtin-report.c
index 91ed41cc7..65fbb6551 100644
--- a/tools/perf/builtin-report.c
+++ b/tools/perf/builtin-report.c
@@ -463,6 +463,14 @@ static size_t hists__fprintf_nr_sample_events(struct hists *hists, struct report
 	size_t ret;
 	char unit;
 	unsigned long nr_samples = hists->stats.nr_samples;
+	/*
+	 * struct hists *hists:
+	 * -> struct hists_stats stats;
+	 *    -> u64 total_period;
+	 *    -> u64 total_non_filtered_period;
+	 *    -> u32 nr_samples;
+	 *    -> u32 nr_non_filtered_samples;
+	 */
 	u64 nr_events = hists->stats.total_period;
 	struct evsel *evsel = hists_to_evsel(hists);
 	char buf[512];
@@ -726,6 +734,10 @@ static int hists__resort_cb(struct hist_entry *he, void *arg)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-report.c|1017| <<__cmd_report>> report__output_resort(rep);
+ */
 static void report__output_resort(struct report *rep)
 {
 	struct ui_progress prog;
@@ -920,6 +932,21 @@ static int tasks_print(struct report *rep, FILE *fp)
 	return 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  hists__inc_stats (h=0xda58e0, hists=0x7ae5e0) at util/hist.c:1933
+ * #1  output_resort (hists=hists@entry=0x7ae5e0, prog=<optimized out>, use_callchain=<optimized out>, cb=cb@entry=0x426bb0 <hists__resort_cb>, cb_arg=<optimized out>)
+ *     at util/hist.c:1933
+ * #2  0x0000000000533bd5 in evsel__output_resort_cb (evsel=evsel@entry=0x7ae380, prog=prog@entry=0x7fffffffb640, cb=cb@entry=0x426bb0 <hists__resort_cb>,
+ *     cb_arg=cb_arg@entry=0x7fffffffb7f0) at util/hist.h:246
+ * #3  0x00000000004295cb in report__output_resort (rep=0x7fffffffb7f0) at builtin-report.c:739
+ * #4  __cmd_report (rep=0x7fffffffb7f0) at builtin-report.c:1019
+ * #5  cmd_report (argc=<optimized out>, argv=<optimized out>) at builtin-report.c:1659
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719620 <commands+288>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdfe0) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=2, argv=0x7fffffffdfe0) at perf.c:544
+ */
 static int __cmd_report(struct report *rep)
 {
 	int ret;
diff --git a/tools/perf/builtin-stat.c b/tools/perf/builtin-stat.c
index 0b4a62e4f..7ac57c2ea 100644
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@ -336,6 +336,10 @@ static int evsel__write_stat_event(struct evsel *counter, int cpu_map_idx, u32 t
 					   process_synthesized_event, NULL);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|395| <<read_counter_cpu>> read_single_counter(counter, cpu_map_idx, thread, rs)) {
+ */
 static int read_single_counter(struct evsel *counter, int cpu_map_idx,
 			       int thread, struct timespec *rs)
 {
@@ -374,6 +378,10 @@ static int read_single_counter(struct evsel *counter, int cpu_map_idx,
  * Read out the results of a single counter:
  * do not aggregate counts across CPUs in system-wide mode
  */
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|450| <<read_affinity_counters>> counter->err = read_counter_cpu(counter, rs,
+ */
 static int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_map_idx)
 {
 	int nthreads = perf_thread_map__nr(evsel_list->core.threads);
@@ -421,6 +429,10 @@ static int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|487| <<read_counters>> read_affinity_counters(rs))
+ */
 static int read_affinity_counters(struct timespec *rs)
 {
 	struct evlist_cpu_iterator evlist_cpu_itr;
@@ -469,6 +481,11 @@ static int read_bpf_map_counters(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|540| <<process_interval>> read_counters(&rs);
+ *   - tools/perf/builtin-stat.c|1075| <<__run_perf_stat>> read_counters(&(struct timespec) { .tv_nsec = t1-t0 });
+ */
 static void read_counters(struct timespec *rs)
 {
 	struct evsel *counter;
@@ -760,6 +777,13 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 		 * errored is a sticky flag that means one of the counter's
 		 * cpu event had a problem and needs to be reexamined.
 		 */
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		counter->errored = true;
 
 		if ((evsel__leader(counter) != counter) ||
@@ -791,6 +815,10 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 	return COUNTER_FATAL;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|1100| <<run_perf_stat>> ret = __run_perf_stat(argc, argv, run_idx);
+ */
 static int __run_perf_stat(int argc, const char **argv, int run_idx)
 {
 	int interval = stat_config.interval;
@@ -826,6 +854,16 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	}
 
 	evlist__for_each_entry(evsel_list, counter) {
+		/*
+		 * 在以下使用evsel->reset_group:
+		 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+		 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true;
+		 */
 		counter->reset_group = false;
 		if (bpf_counter__load(counter, &target))
 			return -1;
@@ -843,6 +881,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		if (target.use_bpf)
 			break;
 
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		if (counter->reset_group || counter->errored)
 			continue;
 		if (evsel__is_bpf(counter))
@@ -892,6 +937,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 
@@ -901,6 +953,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 			if (!counter->reset_group)
@@ -1042,6 +1101,10 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	return WEXITSTATUS(status);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|2606| <<cmd_stat>> status = run_perf_stat(argc, argv, run_idx);
+ */
 static int run_perf_stat(int argc, const char **argv, int run_idx)
 {
 	int ret;
@@ -2233,6 +2296,10 @@ static void setup_system_wide(int forks)
 	}
 }
 
+/*
+ * 在以下使用cmd_stat():
+ *   - tools/perf/perf.c|72| <<global>> { "stat", cmd_stat, 0 },
+ */
 int cmd_stat(int argc, const char **argv)
 {
 	const char * const stat_usage[] = {
diff --git a/tools/perf/util/counts.c b/tools/perf/util/counts.c
index 7a447d918..9b2cbc1b1 100644
--- a/tools/perf/util/counts.c
+++ b/tools/perf/util/counts.c
@@ -7,6 +7,11 @@
 #include <perf/threadmap.h>
 #include <linux/zalloc.h>
 
+/*
+ * called by:
+ *   - tools/perf/util/counts.c|64| <<evsel__alloc_counts>> evsel->counts = perf_counts__new(perf_cpu_map__nr(cpus), nthreads);
+ *   - tools/perf/util/stat.c|164| <<evsel__alloc_prev_raw_counts>> counts = perf_counts__new(cpu_map_nr, nthreads);
+ */
 struct perf_counts *perf_counts__new(int ncpus, int nthreads)
 {
 	struct perf_counts *counts = zalloc(sizeof(*counts));
@@ -51,11 +56,21 @@ void perf_counts__reset(struct perf_counts *counts)
 	memset(&counts->aggr, 0, sizeof(struct perf_counts_values));
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|1640| <<__evsel__read_on_cpu>> if (evsel->counts == NULL && evsel__alloc_counts(evsel) < 0)
+ *   - tools/perf/util/stat.c|186| <<evsel__alloc_stats>> evsel__alloc_counts(evsel) < 0 ||
+ */
 void evsel__reset_counts(struct evsel *evsel)
 {
 	perf_counts__reset(evsel->counts);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|1640| <<__evsel__read_on_cpu>> if (evsel->counts == NULL && evsel__alloc_counts(evsel) < 0)
+ *   - tools/perf/util/stat.c|190| <<evsel__alloc_stats>> evsel__alloc_counts(evsel) < 0 ||
+ */
 int evsel__alloc_counts(struct evsel *evsel)
 {
 	struct perf_cpu_map *cpus = evsel__cpus(evsel);
diff --git a/tools/perf/util/data.c b/tools/perf/util/data.c
index a7f68c309..3ae98ddca 100644
--- a/tools/perf/util/data.c
+++ b/tools/perf/util/data.c
@@ -284,11 +284,34 @@ static int open_file_read(struct perf_data *data)
 	return -1;
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at util/session.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ */
 static int open_file_write(struct perf_data *data)
 {
 	int fd;
 	char sbuf[STRERR_BUFSIZE];
 
+	/*
+	 * (gdb) p data->file.path
+	 * $1 = 0x7c6640 "perf.data"
+	 */
 	fd = open(data->file.path, O_CREAT|O_RDWR|O_TRUNC|O_CLOEXEC,
 		  S_IRUSR|S_IWUSR);
 
@@ -348,6 +371,12 @@ static int open_dir(struct perf_data *data)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-inject.c|2202| <<cmd_inject>> if (perf_data__open(&inject.output)) {
+ *   - tools/perf/util/data.c|436| <<perf_data__switch>> ret = perf_data__open(data);
+ *   - tools/perf/util/session.c|213| <<__perf_session__new>> ret = perf_data__open(data);
+ */
 int perf_data__open(struct perf_data *data)
 {
 	if (check_pipe(data))
diff --git a/tools/perf/util/data.h b/tools/perf/util/data.h
index effcc195d..9b2c535e0 100644
--- a/tools/perf/util/data.h
+++ b/tools/perf/util/data.h
@@ -68,6 +68,43 @@ static inline bool perf_data__is_single_file(struct perf_data *data)
 	return data->dir.version == PERF_DIR_SINGLE_FILE;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-inject.c|260| <<perf_event__repipe_auxtrace>> ret = copy_bytes(inject, perf_data__fd(session->data),
+ *   - tools/perf/builtin-inject.c|1737| <<save_section_info>> int fd = perf_data__fd(inject->session->data);
+ *   - tools/perf/builtin-inject.c|1797| <<feat_copy>> int fd = perf_data__fd(inject->session->data);
+ *   - tools/perf/builtin-inject.c|1871| <<output_fd>> return inject->in_place_update ? -1 : perf_data__fd(&inject->output);
+ *   - tools/perf/builtin-inject.c|2237| <<cmd_inject>> ret = perf_header__write_pipe(perf_data__fd(&inject.output));
+ *   - tools/perf/builtin-record.c|698| <<record__process_auxtrace>> int fd = perf_data__fd(data);
+ *   - tools/perf/builtin-record.c|1659| <<record__finish_output>> int fd = perf_data__fd(data);
+ *   - tools/perf/builtin-record.c|1665| <<record__finish_output>> data->file.size = lseek(perf_data__fd(data), 0, SEEK_CUR);
+ *   - tools/perf/builtin-record.c|2240| <<__cmd_record>> fd = perf_data__fd(data);
+ *   - tools/perf/builtin-stat.c|1011| <<__run_perf_stat>> int fd = perf_data__fd(&perf_stat.data);
+ *   - tools/perf/builtin-stat.c|1014| <<__run_perf_stat>> err = perf_header__write_pipe(perf_data__fd(&perf_stat.data));
+ *   - tools/perf/builtin-stat.c|2656| <<cmd_stat>> int fd = perf_data__fd(&perf_stat.data);
+ *   - tools/perf/builtin-timechart.c|1612| <<__cmd_timechart>> perf_data__fd(session->data),
+ *   - tools/perf/util/arm-spe.c|170| <<arm_spe_get_trace>> int fd = perf_data__fd(speq->spe->session->data);
+ *   - tools/perf/util/arm-spe.c|938| <<arm_spe_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/auxtrace.c|264| <<auxtrace_copy_data>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/cs-etm.c|1076| <<cs_etm__get_trace>> int fd = perf_data__fd(etmq->etm->session->data);
+ *   - tools/perf/util/cs-etm.c|2451| <<cs_etm__process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/data-convert-json.c|229| <<output_headers>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/header.c|3446| <<perf_header__fprintf_info>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/header.c|4137| <<perf_session__read_header>> int fd = perf_data__fd(data);
+ *   - tools/perf/util/header.c|4427| <<perf_event__process_tracing_data>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/intel-bts.c|484| <<intel_bts_process_queue>> int fd = perf_data__fd(btsq->bts->session->data);
+ *   - tools/perf/util/intel-bts.c|648| <<intel_bts_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/intel-pt.c|401| <<intel_pt_get_buffer>> int fd = perf_data__fd(ptq->pt->session->data);
+ *   - tools/perf/util/intel-pt.c|3519| <<intel_pt_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/s390-cpumsf.c|730| <<s390_cpumsf_run_decoder>> int fd = perf_data__fd(sfq->sf->session->data);
+ *   - tools/perf/util/s390-cpumsf.c|968| <<s390_cpumsf_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/session.c|397| <<process_event_auxtrace_stub>> skipn(perf_data__fd(session->data), event->auxtrace.size);
+ *   - tools/perf/util/session.c|1652| <<perf_session__process_user_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/session.c|1769| <<perf_session__peek_event>> fd = perf_data__fd(session->data);
+ *   - tools/perf/util/session.c|2433| <<__perf_session__process_events>> .fd = perf_data__fd(session->data),
+ *   - tools/perf/util/session.c|2514| <<__perf_session__process_dir_events>> .fd = perf_data__fd(session->data),
+ *   - tools/perf/util/synthetic-events.c|2342| <<perf_event__synthesize_for_pipe>> int fd = perf_data__fd(data);
+ */
 static inline int perf_data__fd(struct perf_data *data)
 {
 	if (data->use_stdio)
diff --git a/tools/perf/util/evlist.c b/tools/perf/util/evlist.c
index 48167f394..4f06ed23e 100644
--- a/tools/perf/util/evlist.c
+++ b/tools/perf/util/evlist.c
@@ -579,6 +579,29 @@ static void __evlist__enable(struct evlist *evlist, char *evsel_name)
 	evlist->enabled = true;
 }
 
+/*
+ * called by:
+ *   - tools/perf/bench/evlist-open-close.c|131| <<bench__do_evlist_open_close>> evlist__enable(evlist);
+ *   - tools/perf/builtin-kvm.c|983| <<kvm_events_live_report>> evlist__enable(kvm->evlist);
+ *   - tools/perf/builtin-record.c|2512| <<__cmd_record>> evlist__enable(rec->evlist);
+ *   - tools/perf/builtin-record.c|2567| <<__cmd_record>> evlist__enable(rec->evlist);
+ *   - tools/perf/builtin-stat.c|610| <<enable_counters>> evlist__enable(evsel_list);
+ *   - tools/perf/builtin-top.c|1305| <<__cmd_top>> evlist__enable(top->evlist);
+ *   - tools/perf/builtin-trace.c|4094| <<trace__run>> evlist__enable(evlist);
+ *   - tools/perf/builtin-trace.c|4101| <<trace__run>> evlist__enable(evlist);
+ *   - tools/perf/tests/backward-ring-buffer.c|75| <<do_test>> evlist__enable(evlist);
+ *   - tools/perf/tests/bpf.c|170| <<do_test>> evlist__enable(evlist);
+ *   - tools/perf/tests/code-reading.c|689| <<do_test_code_reading>> evlist__enable(evlist);
+ *   - tools/perf/tests/keep-tracking.c|116| <<test__keep_tracking>> evlist__enable(evlist);
+ *   - tools/perf/tests/keep-tracking.c|134| <<test__keep_tracking>> evlist__enable(evlist);
+ *   - tools/perf/tests/openat-syscall-tp-fields.c|82| <<test__syscall_openat_tp_fields>> evlist__enable(evlist);
+ *   - tools/perf/tests/perf-record.c|159| <<test__PERF_RECORD>> evlist__enable(evlist);
+ *   - tools/perf/tests/perf-time-to-tsc.c|135| <<test__perf_time_to_tsc>> evlist__enable(evlist);
+ *   - tools/perf/tests/sw-clock.c|91| <<__test__sw_clock_freq>> evlist__enable(evlist);
+ *   - tools/perf/tests/switch-tracking.c|483| <<test__switch_tracking>> evlist__enable(evlist);
+ *   - tools/perf/util/evlist.c|594| <<evlist__toggle_enable>> (evlist->enabled ? evlist__disable : evlist__enable)(evlist);
+ *   - tools/perf/util/evlist.c|2058| <<evlist__ctlfd_enable>> evlist__enable(evlist);
+ */
 void evlist__enable(struct evlist *evlist)
 {
 	__evlist__enable(evlist, NULL);
@@ -989,6 +1012,9 @@ int evlist__mmap_ex(struct evlist *evlist, unsigned int pages,
 	struct perf_evlist_mmap_ops ops = {
 		.idx  = perf_evlist__mmap_cb_idx,
 		.get  = perf_evlist__mmap_cb_get,
+		/*
+		 * 核心是这个!!!
+		 */
 		.mmap = perf_evlist__mmap_cb_mmap,
 	};
 
@@ -1490,6 +1516,18 @@ int evlist__prepare_workload(struct evlist *evlist, struct target *target, const
 	return -1;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-ftrace.c|633| <<__cmd_ftrace>> evlist__start_workload(ftrace->evlist);
+ *   - tools/perf/builtin-ftrace.c|908| <<__cmd_latency>> evlist__start_workload(ftrace->evlist);
+ *   - tools/perf/builtin-lock.c|1686| <<__cmd_contention>> evlist__start_workload(con.evlist);
+ *   - tools/perf/builtin-record.c|2560| <<__cmd_record>> evlist__start_workload(rec->evlist);
+ *   - tools/perf/builtin-stat.c|1035| <<__run_perf_stat>> evlist__start_workload(evsel_list);
+ *   - tools/perf/builtin-trace.c|4097| <<trace__run>> evlist__start_workload(evlist);
+ *   - tools/perf/tests/event-times.c|50| <<attach__enable_on_exec>> return evlist__start_workload(evlist) == 1 ? TEST_OK : TEST_FAIL;
+ *   - tools/perf/tests/perf-record.c|164| <<test__PERF_RECORD>> evlist__start_workload(evlist);
+ *   - tools/perf/tests/task-exit.c|115| <<test__task_exit>> evlist__start_workload(evlist);
+ */
 int evlist__start_workload(struct evlist *evlist)
 {
 	if (evlist->workload.cork_fd > 0) {
@@ -1889,6 +1927,11 @@ void evlist__close_control(int ctl_fd, int ctl_fd_ack, bool *ctl_fd_close)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1281| <<record__mmap_evlist>> if (evlist__initialize_ctlfd(evlist, opts->ctl_fd, opts->ctl_fd_ack))
+ *   - tools/perf/builtin-stat.c|2617| <<cmd_stat>> if (evlist__initialize_ctlfd(evsel_list, stat_config.ctl_fd, stat_config.ctl_fd_ack))
+ */
 int evlist__initialize_ctlfd(struct evlist *evlist, int fd, int ack)
 {
 	if (fd == -1) {
@@ -2100,6 +2143,11 @@ static int evlist__ctlfd_list(struct evlist *evlist, char *cmd_data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2846| <<__cmd_record>> if (evlist__ctlfd_process(rec->evlist, &cmd) > 0) {
+ *   - tools/perf/builtin-stat.c|678| <<process_evlist>> if (evlist__ctlfd_process(evlist, &cmd) > 0) {
+ */
 int evlist__ctlfd_process(struct evlist *evlist, enum evlist_ctl_cmd *cmd)
 {
 	int err = 0;
@@ -2147,6 +2195,10 @@ int evlist__ctlfd_process(struct evlist *evlist, enum evlist_ctl_cmd *cmd)
 	return err;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2825| <<__cmd_record>> evlist__ctlfd_update(rec->evlist,
+ */
 int evlist__ctlfd_update(struct evlist *evlist, struct pollfd *update)
 {
 	int ctlfd_pos = evlist->ctl_fd.pos;
diff --git a/tools/perf/util/evsel.c b/tools/perf/util/evsel.c
index 18c3eb864..2467e31cc 100644
--- a/tools/perf/util/evsel.c
+++ b/tools/perf/util/evsel.c
@@ -1429,6 +1429,10 @@ int evsel__append_addr_filter(struct evsel *evsel, const char *filter)
 }
 
 /* Caller has to clear disabled after going through all CPUs. */
+/*
+ * called by:
+ *   - tools/perf/util/evlist.c|563| <<__evlist__enable>> evsel__enable_cpu(pos, evlist_cpu_itr.cpu_map_idx);
+ */
 int evsel__enable_cpu(struct evsel *evsel, int cpu_map_idx)
 {
 	return perf_evsel__enable_cpu(&evsel->core, cpu_map_idx);
@@ -1533,6 +1537,27 @@ void evsel__compute_deltas(struct evsel *evsel, int cpu_map_idx, int thread,
 	count->run = count->run - tmp.run;
 }
 
+/*
+ * (gdb) bt
+ * #0  evsel__read_one (thread=0, cpu_map_idx=0, evsel=0x7c6130) at util/evsel.c:1538
+ * #1  evsel__read_counter (evsel=evsel@entry=0x7c6130, cpu_map_idx=cpu_map_idx@entry=0, thread=thread@entry=0) at util/evsel.c:1629
+ * #2  0x000000000042b0b1 in read_single_counter (rs=0x7fffffff8fb0, thread=0, cpu_map_idx=0, counter=0x7c6130) at builtin-stat.c:367
+ * #3  read_counter_cpu (cpu_map_idx=0, rs=0x7fffffff8fb0, counter=0x7c6130) at builtin-stat.c:397
+ * #4  read_affinity_counters (rs=0x7fffffff8fb0) at builtin-stat.c:449
+ * #5  read_counters (rs=0x7fffffff8fb0) at builtin-stat.c:483
+ * #6  0x000000000042e01c in __run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1046
+ * #7  run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1071
+ * #8  cmd_stat (argc=2, argv=<optimized out>) at builtin-stat.c:2577
+ * #9  0x00000000004a544b in run_builtin (p=p@entry=0x719650 <commands+336>, argc=argc@entry=6, argv=argv@entry=0x7fffffffdf70) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=6, argv=0x7fffffffdf70) at perf.c:544
+ * (gdb) p evsel->name
+ * $1 = 0x7c5e90 "cycles"
+ *
+ * called by:
+ *   - tools/perf/util/evsel.c|1629| <<evsel__read_counter>> return evsel__read_one(evsel, cpu_map_idx, thread);
+ */
 static int evsel__read_one(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	struct perf_counts_values *count = perf_counts(evsel->counts, cpu_map_idx, thread);
@@ -1619,6 +1644,10 @@ static int evsel__read_group(struct evsel *leader, int cpu_map_idx, int thread)
 	return evsel__process_group_data(leader, cpu_map_idx, thread, data);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|366| <<read_single_counter>> return evsel__read_counter(counter, cpu_map_idx, thread);
+ */
 int evsel__read_counter(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	u64 read_format = evsel->core.attr.read_format;
@@ -1629,6 +1658,11 @@ int evsel__read_counter(struct evsel *evsel, int cpu_map_idx, int thread)
 	return evsel__read_one(evsel, cpu_map_idx, thread);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.h|402| <<evsel__read_on_cpu>> return __evsel__read_on_cpu(evsel, cpu_map_idx, thread, false);
+ *   - tools/perf/util/evsel.h|414| <<evsel__read_on_cpu_scaled>> return __evsel__read_on_cpu(evsel, cpu_map_idx, thread, true);
+ */
 int __evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread, bool scale)
 {
 	struct perf_counts_values count;
@@ -2031,6 +2065,12 @@ bool evsel__increase_rlimit(enum rlimit_action *set_rlimit)
 	return false;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|2188| <<evsel__open>> return evsel__open_cpu(evsel, cpus, threads, 0, perf_cpu_map__nr(cpus));
+ *   - tools/perf/util/evsel.c|2200| <<evsel__open_per_cpu>> return evsel__open_cpu(evsel, cpus, NULL, 0, perf_cpu_map__nr(cpus));
+ *   - tools/perf/util/evsel.c|2202| <<evsel__open_per_cpu>> return evsel__open_cpu(evsel, cpus, NULL, cpu_map_idx, cpu_map_idx + 1);
+ */
 static int evsel__open_cpu(struct evsel *evsel, struct perf_cpu_map *cpus,
 		struct perf_thread_map *threads,
 		int start_cpu_map_idx, int end_cpu_map_idx)
@@ -2177,6 +2217,19 @@ static int evsel__open_cpu(struct evsel *evsel, struct perf_cpu_map *cpus,
 	return err;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1251| <<record__open>> if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
+ *   - tools/perf/builtin-top.c|1024| <<perf_top__start_counters>> if (evsel__open(counter, top->evlist->core.user_requested_cpus,
+ *   - tools/perf/tests/mmap-basic.c|98| <<test__basic_mmap>> if (evsel__open(evsels[i], cpus, threads) < 0) {
+ *   - tools/perf/tests/openat-syscall-all-cpus.c|56| <<test__openat_syscall_event_on_all_cpus>> if (evsel__open(evsel, cpus, threads) < 0) {
+ *   - tools/perf/util/evlist.c|1361| <<evlist__open>> err = evsel__open(evsel, evsel->core.cpus, evsel->core.threads);
+ *   - tools/perf/util/evsel.c|2207| <<evsel__open_per_thread>> return evsel__open(evsel, NULL, threads);
+ *   - tools/perf/util/parse-events.c|178| <<is_event_supported>> open_return = evsel__open(evsel, NULL, tmap);
+ *   - tools/perf/util/parse-events.c|190| <<is_event_supported>> ret = evsel__open(evsel, NULL, tmap) >= 0;
+ *   - tools/perf/util/python.c|904| <<pyrf_evsel__open>> if (evsel__open(evsel, cpus, threads) < 0) {
+ *   - tools/perf/util/sideband_evlist.c|117| <<evlist__start_sb_thread>> if (evsel__open(counter, evlist->core.user_requested_cpus,
+ */
 int evsel__open(struct evsel *evsel, struct perf_cpu_map *cpus,
 		struct perf_thread_map *threads)
 {
@@ -2189,6 +2242,15 @@ void evsel__close(struct evsel *evsel)
 	perf_evsel__free_id(&evsel->core);
 }
 
+/*
+ * called by:
+ *   - tools/perf/tests/event-times.c|128| <<attach__cpu_disabled>> err = evsel__open_per_cpu(evsel, cpus, -1);
+ *   - tools/perf/tests/event-times.c|155| <<attach__cpu_enabled>> err = evsel__open_per_cpu(evsel, cpus, -1);
+ *   - tools/perf/util/bpf_counter.c|454| <<bperf_reload_leader_program>> evsel__open_per_cpu(evsel, all_cpu_map, -1);
+ *   - tools/perf/util/bpf_counter_cgroup.c|91| <<bperf_load_program>> if (evsel__open_per_cpu(cgrp_switch, evlist->core.all_cpus, -1) < 0) {
+ *   - tools/perf/util/bpf_counter_cgroup.c|118| <<bperf_load_program>> err = evsel__open_per_cpu(evsel, evsel->core.cpus, -1);
+ *   - tools/perf/util/stat.c|629| <<create_perf_stat_counter>> return evsel__open_per_cpu(evsel, evsel__cpus(evsel), cpu_map_idx);
+ */
 int evsel__open_per_cpu(struct evsel *evsel, struct perf_cpu_map *cpus, int cpu_map_idx)
 {
 	if (cpu_map_idx == -1)
@@ -2197,6 +2259,13 @@ int evsel__open_per_cpu(struct evsel *evsel, struct perf_cpu_map *cpus, int cpu_
 	return evsel__open_cpu(evsel, cpus, NULL, cpu_map_idx, cpu_map_idx + 1);
 }
 
+/*
+ * called by:
+ *   - tools/perf/tests/event-times.c|75| <<attach__current_disabled>> err = evsel__open_per_thread(evsel, threads);
+ *   - tools/perf/tests/event-times.c|99| <<attach__current_enabled>> err = evsel__open_per_thread(evsel, threads);
+ *   - tools/perf/tests/openat-syscall.c|39| <<test__openat_syscall_event>> if (evsel__open_per_thread(evsel, threads) < 0) {
+ *   - tools/perf/util/stat.c|631| <<create_perf_stat_counter>> return evsel__open_per_thread(evsel, evsel->core.threads);
+ */
 int evsel__open_per_thread(struct evsel *evsel, struct perf_thread_map *threads)
 {
 	return evsel__open(evsel, NULL, threads);
diff --git a/tools/perf/util/evsel.h b/tools/perf/util/evsel.h
index d927713b5..675f8d049 100644
--- a/tools/perf/util/evsel.h
+++ b/tools/perf/util/evsel.h
@@ -118,6 +118,27 @@ struct evsel {
 	void			*priv;
 	u64			db_id;
 	bool			uniquified_name;
+	/*
+	 * 在以下使用counter->supported:
+	 *   - tools/perf/builtin-record.c|1269| <<record__open>> pos->supported = true;
+	 *   - tools/perf/builtin-stat.c|382| <<read_counter_cpu>> if (!counter->supported)
+	 *   - tools/perf/builtin-stat.c|758| <<stat_handle_error>> counter->supported = false;
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|962| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|968| <<__run_perf_stat>> if (!counter->supported) {
+	 *   - tools/perf/util/bpf_counter_cgroup.c|139| <<bperf_load_program>> evsel->supported = true;
+	 *   - tools/perf/util/mem-events.c|136| <<perf_mem_events__init>> e->supported = perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|141| <<perf_mem_events__init>> e->supported |= perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|145| <<perf_mem_events__init>> if (e->supported)
+	 *   - tools/perf/util/mem-events.c|163| <<perf_mem_events__list>> e->supported ? ": available" : "");
+	 *   - tools/perf/util/mem-events.c|198| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/mem-events.c|207| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/stat-display.c|587| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED);
+	 *   - tools/perf/util/stat-display.c|591| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED,
+	 *   - tools/perf/util/stat-display.c|595| <<printout>> if (counter->supported) {
+	 *   - tools/perf/util/stat.c|502| <<perf_event__process_stat_event>> counter->supported = true;
+	 *   - tools/perf/util/synthetic-events.c|2066| <<perf_event__synthesize_extra_attr>> if (!evsel->supported)
+	 */
 	bool 			supported;
 	bool 			needs_swap;
 	bool 			disabled;
@@ -128,7 +149,24 @@ struct evsel {
 	bool			forced_leader;
 	bool			cmdline_group_boundary;
 	bool			merged_stat;
+	/*
+	 * 在以下使用evsel->reset_group:
+	 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+	 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true; 
+	 */
 	bool			reset_group;
+	/*
+	 * 在以下使用evsel->errored:
+	 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 */
 	bool			errored;
 	bool			needs_auxtrace_mmap;
 	struct hashmap		*per_pkg_mask;
@@ -359,6 +397,11 @@ int __evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread, bool
  * @cpu_map_idx - CPU of interest
  * @thread - thread of interest
  */
+/*
+ * called by:
+ *   - tools/perf/tests/openat-syscall-all-cpus.c|101| <<test__openat_syscall_event_on_all_cpus>> if (evsel__read_on_cpu(evsel, idx, 0) < 0) {
+ *   - tools/perf/tests/openat-syscall.c|52| <<test__openat_syscall_event>> if (evsel__read_on_cpu(evsel, 0, 0) < 0) {
+ */
 static inline int evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	return __evsel__read_on_cpu(evsel, cpu_map_idx, thread, false);
diff --git a/tools/perf/util/hist.c b/tools/perf/util/hist.c
index 1c085ab56..3d844ca5b 100644
--- a/tools/perf/util/hist.c
+++ b/tools/perf/util/hist.c
@@ -1720,6 +1720,27 @@ static void hists__inc_filter_stats(struct hists *hists, struct hist_entry *h)
 	hists->stats.total_non_filtered_period += h->stat.period;
 }
 
+/*
+ * (gdb) bt
+ * #0  hists__inc_stats (h=0xda58e0, hists=0x7ae5e0) at util/hist.c:1933
+ * #1  output_resort (hists=hists@entry=0x7ae5e0, prog=<optimized out>, use_callchain=<optimized out>, cb=cb@entry=0x426bb0 <hists__resort_cb>, cb_arg=<optimized out>)
+ *     at util/hist.c:1933
+ * #2  0x0000000000533bd5 in evsel__output_resort_cb (evsel=evsel@entry=0x7ae380, prog=prog@entry=0x7fffffffb640, cb=cb@entry=0x426bb0 <hists__resort_cb>, 
+ *     cb_arg=cb_arg@entry=0x7fffffffb7f0) at util/hist.h:246
+ * #3  0x00000000004295cb in report__output_resort (rep=0x7fffffffb7f0) at builtin-report.c:739
+ * #4  __cmd_report (rep=0x7fffffffb7f0) at builtin-report.c:1019
+ * #5  cmd_report (argc=<optimized out>, argv=<optimized out>) at builtin-report.c:1659
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719620 <commands+288>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdfe0) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=2, argv=0x7fffffffdfe0) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/util/hist.c|1933| <<output_resort>> hists__inc_stats(hists, n);
+ *   - tools/perf/util/hist.c|2380| <<hists__add_dummy_entry>> hists__inc_stats(hists, he);
+ *   - tools/perf/util/hist.c|2428| <<add_dummy_hierarchy_entry>> hists__inc_stats(hists, he);
+ *   - tools/perf/util/hist.h|201| <<add_dummy_hierarchy_entry>> void hists__inc_stats(struct hists *hists, struct hist_entry *h);
+ */
 void hists__inc_stats(struct hists *hists, struct hist_entry *h)
 {
 	if (!h->filtered)
@@ -1885,6 +1906,12 @@ static void __hists__insert_output_entry(struct rb_root_cached *entries,
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/hist.c|1955| <<evsel__output_resort_cb>> output_resort(evsel__hists(evsel), prog, use_callchain, cb, cb_arg);
+ *   - tools/perf/util/hist.c|1965| <<hists__output_resort>> output_resort(hists, prog, symbol_conf.use_callchain, NULL, NULL);
+ *   - tools/perf/util/hist.c|1971| <<hists__output_resort_cb>> output_resort(hists, prog, symbol_conf.use_callchain, cb, NULL);
+ */
 static void output_resort(struct hists *hists, struct ui_progress *prog,
 			  bool use_callchain, hists__resort_cb_t cb,
 			  void *cb_arg)
@@ -1940,6 +1967,21 @@ static void output_resort(struct hists *hists, struct ui_progress *prog,
 	}
 }
 
+/*
+ * (gdb) bt
+ * #0  evsel__output_resort_cb (evsel=evsel@entry=0x7ae380, prog=prog@entry=0x7fffffffb640, cb=cb@entry=0x426bb0 <hists__resort_cb>, cb_arg=cb_arg@entry=0x7fffffffb7f0) at util/hist.c:1945
+ * #1  0x00000000004295cb in report__output_resort (rep=0x7fffffffb7f0) at builtin-report.c:739
+ * #2  __cmd_report (rep=0x7fffffffb7f0) at builtin-report.c:1019
+ * #3  cmd_report (argc=<optimized out>, argv=<optimized out>) at builtin-report.c:1659
+ * #4  0x00000000004a548b in run_builtin (p=p@entry=0x719620 <commands+288>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdfe0) at perf.c:316
+ * #5  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #6  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #7  main (argc=2, argv=0x7fffffffdfe0) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/builtin-report.c|737| <<report__output_resort>> evsel__output_resort_cb(pos, &prog, hists__resort_cb, rep);
+ *   - tools/perf/util/hist.c|1960| <<evsel__output_resort>> return evsel__output_resort_cb(evsel, prog, NULL, NULL);
+ */
 void evsel__output_resort_cb(struct evsel *evsel, struct ui_progress *prog,
 			     hists__resort_cb_t cb, void *cb_arg)
 {
diff --git a/tools/perf/util/mmap.c b/tools/perf/util/mmap.c
index a4dff881b..d89950a6c 100644
--- a/tools/perf/util/mmap.c
+++ b/tools/perf/util/mmap.c
@@ -321,9 +321,78 @@ int mmap__mmap(struct mmap *map, struct mmap_params *mp, int fd, struct perf_cpu
 	return perf_mmap__aio_mmap(map, mp);
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7ffff7eeb010, mp=mp@entry=0x7fffffff8840, fd=fd@entry=5, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7ffff7eeb010, mp=0x7fffffff8840, fd=5, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7a7480, ops=0x7fffffff8820, idx=0, mp=0x7fffffff8840, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87d0, _output_overwrite=0x7fffffff87d4,
+ *     nr_mmaps=0x7fffffff87cc) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8840, ops=<optimized out>, evlist=0x7a7480) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7a7480, ops=ops@entry=0x7fffffff8820, mp=mp@entry=0x7fffffff8840) at evlist.c:642
+ * #5  0x00000000004bcecc in evlist__mmap_ex (evlist=evlist@entry=0x7a7480, pages=<optimized out>, auxtrace_pages=0, auxtrace_overwrite=auxtrace_overwrite@entry=false, nr_cblocks=<optimized out>,
+ *     affinity=<optimized out>, flush=1, comp_level=0) at util/evlist.c:1001
+ * #6  0x0000000000422a28 in record__mmap_evlist (rec=0x707020 <record>, evlist=0x7a7480) at builtin-record.c:1159
+ * #7  record__mmap (rec=0x707020 <record>) at builtin-record.c:1208
+ * #8  record__open (rec=0x707020 <record>) at builtin-record.c:1292
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2304
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7fffec363010, mp=mp@entry=0x7fffffff8820, fd=fd@entry=21, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7fffec363010, mp=0x7fffffff8820, fd=21, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7c9850, ops=0x7fffffff8800, idx=0, mp=0x7fffffff8820, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87b0, _output_overwrite=0x7fffffff87b4,
+ *     nr_mmaps=0x7fffffff87ac) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8820, ops=<optimized out>, evlist=0x7c9850) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7c9850, ops=ops@entry=0x7fffffff8800, mp=mp@entry=0x7fffffff8820) at evlist.c:642
+ * #5  0x00000000004bcfeb in evlist__mmap_ex (comp_level=0, flush=1, affinity=0, nr_cblocks=0, auxtrace_overwrite=false, auxtrace_pages=0, pages=4294967295, evlist=0x7c9850) at util/evlist.c:1001
+ * #6  evlist__mmap (evlist=evlist@entry=0x7c9850, pages=pages@entry=4294967295) at util/evlist.c:1006
+ * #7  0x00000000004c0745 in evlist__start_sb_thread (evlist=0x7c9850, target=target@entry=0x707160 <record+320>) at util/sideband_evlist.c:122
+ * #8  0x0000000000422578 in record__setup_sb_evlist (rec=0x707020 <record>) at builtin-record.c:1984
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2359
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * 上面是md->core.base的来源
+ *
+ * called by:
+ *   - 定义tools/perf/util/mmap.c|324| <<global>> int perf_mmap__push(struct mmap *md, void *to,
+ *   - tools/perf/builtin-record.c|468| <<record__aio_push>> ret = perf_mmap__push(map, &aio, record__aio_pushfn);
+ *   - tools/perf/builtin-record.c|1608| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+ */
 int perf_mmap__push(struct mmap *md, void *to,
 		    int push(struct mmap *map, void *to, void *buf, size_t size))
 {
+	/*
+	 * struct mmap *md:
+	 * -> struct perf_mmap core:
+	 *        void                    *base;
+	 *        int                      mask;
+	 *        int                      fd;
+	 *        struct perf_cpu          cpu;
+	 *        refcount_t               refcnt;
+	 *        u64                      prev;
+	 *        u64                      start;
+	 *        u64                      end;
+	 *        bool                     overwrite;
+	 *        u64                      flush;
+	 *        libperf_unmap_cb_t       unmap_cb;
+	 *        char                     event_copy[PERF_SAMPLE_MAX_SIZE] __aligned(8);
+	 *        struct perf_mmap        *next;
+	 *
+	 *
+	 * struct perf_event_mmap_page *base;
+	 *   -> __u64   data_head;              // head in the data section
+	 *   -> __u64   data_tail;              // user-space written tail
+	 *   -> __u64   data_offset;            // where the buffer starts
+	 *   -> __u64   data_size;              // data buffer size
+	 */
 	u64 head = perf_mmap__read_head(&md->core);
 	unsigned char *data = md->core.base + page_size;
 	unsigned long size;
@@ -334,6 +403,24 @@ int perf_mmap__push(struct mmap *md, void *to,
 	if (rc < 0)
 		return (rc == -EAGAIN) ? 1 : -1;
 
+	/*
+	 * struct mmap *md:
+	 * -> struct perf_mmap        core;
+	 *        void                    *base;
+	 *        int                      mask;
+	 *        int                      fd;
+	 *        struct perf_cpu          cpu;
+	 *        refcount_t               refcnt;
+	 *        u64                      prev;
+	 *        u64                      start;
+	 *        u64                      end;
+	 *        bool                     overwrite;
+	 *        u64                      flush;
+	 *        libperf_unmap_cb_t       unmap_cb;
+	 *        char                     event_copy[PERF_SAMPLE_MAX_SIZE] __aligned(8);
+	 *        struct perf_mmap        *next;
+	 * -> struct auxtrace_mmap auxtrace_mmap;
+	 */
 	size = md->core.end - md->core.start;
 
 	if ((md->core.start & md->core.mask) + size != (md->core.end & md->core.mask)) {
@@ -341,6 +428,9 @@ int perf_mmap__push(struct mmap *md, void *to,
 		size = md->core.mask + 1 - (md->core.start & md->core.mask);
 		md->core.start += size;
 
+		/*
+		 * 一个例子是record__pushfn()
+		 */
 		if (push(md, to, buf, size) < 0) {
 			rc = -1;
 			goto out;
@@ -351,6 +441,9 @@ int perf_mmap__push(struct mmap *md, void *to,
 	size = md->core.end - md->core.start;
 	md->core.start += size;
 
+	/*
+	 * 一个例子是record__pushfn()
+	 */
 	if (push(md, to, buf, size) < 0) {
 		rc = -1;
 		goto out;
diff --git a/tools/perf/util/session.c b/tools/perf/util/session.c
index 192c9274f..33ca0c491 100644
--- a/tools/perf/util/session.c
+++ b/tools/perf/util/session.c
@@ -189,6 +189,25 @@ static int ordered_events__deliver_event(struct ordered_events *oe,
 					   event->file_path);
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at ut    il/session.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ */
 struct perf_session *__perf_session__new(struct perf_data *data,
 					 bool repipe, int repipe_fd,
 					 struct perf_tool *tool)
diff --git a/tools/perf/util/stat-display.c b/tools/perf/util/stat-display.c
index b82844cb0..fa66885c7 100644
--- a/tools/perf/util/stat-display.c
+++ b/tools/perf/util/stat-display.c
@@ -531,6 +531,15 @@ static bool is_mixed_hw_group(struct evsel *counter)
 	return false;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|862| <<print_counter_aggrdata>> printout(config, id, nr, counter, uval,
+ *   - tools/perf/util/stat-display.c|983| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|987| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|1031| <<print_counter_aggr>> printout(config, aggr_cpu_id__empty(), 0, counter, uval, prefix, cd.avg_running,
+ *   - tools/perf/util/stat-display.c|1076| <<print_counter>> printout(config, id, 0, counter, uval, prefix,
+ *   - tools/perf/util/stat-display.c|1115| <<print_no_aggr_metric>> printout(config, id, 0, counter, uval, prefix,
+ */
 static void printout(struct perf_stat_config *config, struct aggr_cpu_id id, int nr,
 		     struct evsel *counter, double uval,
 		     char *prefix, u64 run, u64 ena, double noise,
@@ -1049,6 +1058,10 @@ static void counter_cb(struct perf_stat_config *config __maybe_unused,
  * Print out the results of a single counter:
  * does not use aggregated count in system-wide
  */
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1519| <<evlist__print_counters>> print_counter(config, counter, prefix);
+ */
 static void print_counter(struct perf_stat_config *config,
 			  struct evsel *counter, char *prefix)
 {
@@ -1268,6 +1281,10 @@ static void print_interval(struct perf_stat_config *config,
 		num_print_interval = 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1471| <<evlist__print_counters>> print_header(config, _target, argc, argv);
+ */
 static void print_header(struct perf_stat_config *config,
 			 struct target *_target,
 			 int argc, const char **argv)
@@ -1445,6 +1462,10 @@ static void print_percore(struct perf_stat_config *config,
 		fputc('\n', output);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|1117| <<print_counters>> evlist__print_counters(evsel_list, &stat_config, &target, ts, argc, argv);
+ */
 void evlist__print_counters(struct evlist *evlist, struct perf_stat_config *config,
 			    struct target *_target, struct timespec *ts, int argc, const char **argv)
 {
diff --git a/tools/perf/util/stat.c b/tools/perf/util/stat.c
index 0882b4754..290a80f5c 100644
--- a/tools/perf/util/stat.c
+++ b/tools/perf/util/stat.c
@@ -180,6 +180,10 @@ static void evsel__reset_prev_raw_counts(struct evsel *evsel)
 		perf_counts__reset(evsel->prev_raw_counts);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|198| <<evlist__alloc_stats>> if (evsel__alloc_stats(evsel, alloc_raw))
+ */
 static int evsel__alloc_stats(struct evsel *evsel, bool alloc_raw)
 {
 	if (evsel__alloc_stat_priv(evsel) < 0 ||
@@ -190,6 +194,15 @@ static int evsel__alloc_stats(struct evsel *evsel, bool alloc_raw)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-script.c|2052| <<perf_sample__fprint_metric>> evlist__alloc_stats(script->session->evlist, false);
+ *   - tools/perf/builtin-script.c|3638| <<set_maps>> if (evlist__alloc_stats(evlist, true))
+ *   - tools/perf/builtin-stat.c|2138| <<set_maps>> if (evlist__alloc_stats(evsel_list, true))
+ *   - tools/perf/builtin-stat.c|2570| <<cmd_stat>> if (evlist__alloc_stats(evsel_list, interval))
+ *   - tools/perf/tests/parse-metric.c|106| <<__compute_metric>> err = evlist__alloc_stats(evlist, false);
+ *   - tools/perf/tests/pmu-events.c|892| <<test__parsing_callback>> err = evlist__alloc_stats(evlist, false);
+ */
 int evlist__alloc_stats(struct evlist *evlist, bool alloc_raw)
 {
 	struct evsel *evsel;
@@ -355,6 +368,10 @@ static int check_per_pkg(struct evsel *counter, struct perf_counts_values *vals,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|424| <<process_counter_maps>> if (process_counter_values(config, counter, idx, thread,
+ */
 static int
 process_counter_values(struct perf_stat_config *config, struct evsel *evsel,
 		       int cpu_map_idx, int thread,
@@ -409,6 +426,10 @@ process_counter_values(struct perf_stat_config *config, struct evsel *evsel,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|446| <<perf_stat_process_counter>> ret = process_counter_maps(config, counter);
+ */
 static int process_counter_maps(struct perf_stat_config *config,
 				struct evsel *counter)
 {
@@ -430,6 +451,12 @@ static int process_counter_maps(struct perf_stat_config *config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-script.c|3611| <<process_stat_round_event>> perf_stat_process_counter(&stat_config, counter);
+ *   - tools/perf/builtin-stat.c|485| <<read_counters>> if (counter->err == 0 && perf_stat_process_counter(&stat_config, counter))
+ *   - tools/perf/builtin-stat.c|2087| <<process_stat_round_event>> perf_stat_process_counter(&stat_config, counter);
+ */
 int perf_stat_process_counter(struct perf_stat_config *config,
 			      struct evsel *counter)
 {
@@ -542,6 +569,11 @@ size_t perf_event__fprintf_stat_config(union perf_event *event, FILE *fp)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|879| <<__run_perf_stat>> if (create_perf_stat_counter(counter, &stat_config, &target,
+ *   - tools/perf/builtin-stat.c|952| <<__run_perf_stat>> if (create_perf_stat_counter(counter, &stat_config, &target,
+ */
 int create_perf_stat_counter(struct evsel *evsel,
 			     struct perf_stat_config *config,
 			     struct target *target,
diff --git a/tools/perf/util/synthetic-events.c b/tools/perf/util/synthetic-events.c
index 538790758..ed8ed2a4b 100644
--- a/tools/perf/util/synthetic-events.c
+++ b/tools/perf/util/synthetic-events.c
@@ -47,6 +47,19 @@
 
 unsigned int proc_map_timeout = DEFAULT_PROC_MAP_PARSE_TIMEOUT;
 
+/*
+ * (gdb) bt
+ * #0  perf_tool__process_synth_event (tool=0x707020 <record>, event=0x7ca940, machine=0x7a8148, process=0x420aa0 <process_synthesized_event>) at util/synthetic-events.c:54
+ * #1  0x000000000054d98f in __perf_event__synthesize_kernel_mmap (machine=0x7a8148, process=0x420aa0 <process_synthesized_event>, tool=0x707020 <record>) at util/synthetic-events.c:1143
+ * #2  perf_event__synthesize_kernel_mmap (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, machine=machine@entry=0x7a8148) at util/synthetic-events.c:1155
+ * #3  0x0000000000421412 in record__synthesize (tail=tail@entry=false, rec=0x707020 <record>) at builtin-record.c:1867
+ * #4  0x000000000042251c in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2355
+ * #5  0x00000000004255d5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4079
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ */
 int perf_tool__process_synth_event(struct perf_tool *tool,
 				   union perf_event *event,
 				   struct machine *machine,
@@ -218,6 +231,12 @@ static void perf_event__get_ns_link_info(pid_t pid, const char *ns,
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2554| <<__cmd_record>> perf_event__synthesize_namespaces(tool, event,
+ *   - tools/perf/util/synthetic-events.c|759| <<__event__synthesize_thread>> if (perf_event__synthesize_namespaces(tool, namespaces_event, pid,
+ *   - tools/perf/util/synthetic-events.c|804| <<__event__synthesize_thread>> if (perf_event__synthesize_namespaces(tool, namespaces_event, _pid,
+ */
 int perf_event__synthesize_namespaces(struct perf_tool *tool,
 				      union perf_event *event,
 				      pid_t pid, pid_t tgid,
@@ -815,6 +834,13 @@ static int __event__synthesize_thread(union perf_event *comm_event,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1748| <<record__synthesize_workload>> err = perf_event__synthesize_thread_map(&rec->tool, thread_map,
+ *   - tools/perf/tests/code-reading.c|608| <<do_test_code_reading>> ret = perf_event__synthesize_thread_map(NULL, threads,
+ *   - tools/perf/tests/mmap-thread-lookup.c|148| <<synth_process>> err = perf_event__synthesize_thread_map(NULL, map,
+ *   - tools/perf/util/synthetic-events.c|1924| <<__machine__synthesize_threads>> return perf_event__synthesize_thread_map(tool, threads, process, machine,
+ */
 int perf_event__synthesize_thread_map(struct perf_tool *tool,
 				      struct perf_thread_map *threads,
 				      perf_event__handler_t process,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 584a5bab3..5541a5083 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -4435,6 +4435,11 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4802| <<kvm_vm_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(kvm, arg);
+ *   - virt/kvm/kvm_main.c|4957| <<kvm_dev_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
+ */
 static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 {
 	switch (arg) {
-- 
2.34.1

