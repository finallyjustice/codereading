From f16b13183627987b1b9adb24dbcd739a69a23e21 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 31 Oct 2023 07:16:16 -0700
Subject: [PATCH 1/1] linux v6.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/kvm_asm.h       |   10 +
 arch/arm64/include/asm/kvm_host.h      |    6 +
 arch/arm64/include/asm/kvm_pgtable.h   |    4 +
 arch/arm64/kernel/perf_event.c         |   18 +
 arch/arm64/kvm/arch_timer.c            |    8 +
 arch/arm64/kvm/arm.c                   |   66 ++
 arch/arm64/kvm/guest.c                 |   12 +
 arch/arm64/kvm/handle_exit.c           |    4 +
 arch/arm64/kvm/hyp/exception.c         |    8 +
 arch/arm64/kvm/hyp/nvhe/hyp-main.c     |    5 +
 arch/arm64/kvm/hyp/pgtable.c           |   20 +
 arch/arm64/kvm/hyp/vgic-v3-sr.c        |    5 +
 arch/arm64/kvm/hyp/vhe/debug-sr.c      |   13 +
 arch/arm64/kvm/hyp/vhe/switch.c        |    4 +
 arch/arm64/kvm/hyp/vhe/sysreg-sr.c     |    4 +
 arch/arm64/kvm/hyp/vhe/tlb.c           |   34 +
 arch/arm64/kvm/hypercalls.c            |    4 +
 arch/arm64/kvm/inject_fault.c          |    7 +
 arch/arm64/kvm/mmu.c                   |    8 +
 arch/arm64/kvm/pmu-emul.c              |  605 +++++++++++
 arch/arm64/kvm/pmu.c                   |   42 +
 arch/arm64/kvm/reset.c                 |    6 +
 arch/arm64/kvm/sys_regs.c              |   53 +
 arch/arm64/kvm/vgic/vgic-mmio-v3.c     |    4 +
 arch/arm64/kvm/vgic/vgic-v3.c          |    4 +
 arch/arm64/kvm/vgic/vgic.c             |   12 +
 arch/arm64/kvm/vmid.c                  |    4 +
 arch/arm64/mm/context.c                |    4 +
 arch/x86/events/amd/core.c             |   79 ++
 arch/x86/events/amd/ibs.c              |    5 +
 arch/x86/events/core.c                 |   38 +
 arch/x86/events/intel/core.c           |   11 +
 arch/x86/events/intel/lbr.c            |   21 +
 arch/x86/events/intel/p6.c             |    3 +
 arch/x86/events/perf_event.h           |   27 +
 arch/x86/hyperv/hv_init.c              |   17 +
 arch/x86/include/asm/apic.h            |    7 +
 arch/x86/include/asm/fpu/api.h         |   13 +
 arch/x86/include/asm/fpu/types.h       |   32 +
 arch/x86/include/asm/fpu/xcr.h         |    8 +
 arch/x86/include/asm/hardirq.h         |    7 +
 arch/x86/include/asm/hyperv-tlfs.h     |    7 +
 arch/x86/include/asm/kvm_host.h        |  502 ++++++++++
 arch/x86/include/asm/kvmclock.h        |    5 +
 arch/x86/include/asm/mshyperv.h        |    8 +
 arch/x86/include/asm/msr-index.h       |   22 +
 arch/x86/include/asm/nmi.h             |   45 +
 arch/x86/include/asm/nospec-branch.h   |    7 +
 arch/x86/include/asm/perf_event.h      |   23 +
 arch/x86/include/asm/pvclock-abi.h     |    9 +
 arch/x86/include/asm/pvclock.h         |   18 +
 arch/x86/include/asm/realmode.h        |    7 +
 arch/x86/include/asm/spec-ctrl.h       |    8 +
 arch/x86/include/asm/svm.h             |    6 +
 arch/x86/kernel/acpi/boot.c            |   32 +
 arch/x86/kernel/apic/apic.c            |    4 +
 arch/x86/kernel/check.c                |   15 +
 arch/x86/kernel/cpu/bugs.c             |   12 +
 arch/x86/kernel/cpu/common.c           |   33 +
 arch/x86/kernel/cpu/mtrr/mtrr.c        |    8 +
 arch/x86/kernel/fpu/context.h          |    5 +
 arch/x86/kernel/fpu/core.c             |  166 +++
 arch/x86/kernel/fpu/init.c             |    8 +
 arch/x86/kernel/fpu/signal.c           |    9 +
 arch/x86/kernel/fpu/xstate.c           |   74 ++
 arch/x86/kernel/fpu/xstate.h           |    5 +
 arch/x86/kernel/kvm.c                  |   21 +
 arch/x86/kernel/kvmclock.c             |   93 ++
 arch/x86/kernel/nmi.c                  |   95 ++
 arch/x86/kernel/nmi_selftest.c         |   32 +
 arch/x86/kernel/pvclock.c              |    6 +
 arch/x86/kernel/sev.c                  |   18 +
 arch/x86/kernel/smp.c                  |   14 +
 arch/x86/kernel/smpboot.c              |   27 +
 arch/x86/kernel/tsc.c                  |  118 +++
 arch/x86/kvm/cpuid.c                   |  114 +++
 arch/x86/kvm/cpuid.h                   |   77 ++
 arch/x86/kvm/emulate.c                 |   11 +
 arch/x86/kvm/hyperv.c                  |    4 +
 arch/x86/kvm/irq_comm.c                |    8 +
 arch/x86/kvm/kvm_cache_regs.h          |    7 +
 arch/x86/kvm/lapic.c                   |   53 +
 arch/x86/kvm/lapic.h                   |   11 +
 arch/x86/kvm/mmu.h                     |   28 +
 arch/x86/kvm/mmu/mmu.c                 |  154 +++
 arch/x86/kvm/mmu/mmu_internal.h        |   12 +
 arch/x86/kvm/mmu/page_track.c          |   14 +
 arch/x86/kvm/mmu/spte.h                |   19 +
 arch/x86/kvm/mmu/tdp_mmu.c             |   95 ++
 arch/x86/kvm/mmu/tdp_mmu.h             |   14 +
 arch/x86/kvm/pmu.c                     |  405 ++++++++
 arch/x86/kvm/pmu.h                     |  119 +++
 arch/x86/kvm/reverse_cpuid.h           |   15 +
 arch/x86/kvm/svm/avic.c                |   21 +
 arch/x86/kvm/svm/pmu.c                 |  132 +++
 arch/x86/kvm/svm/svm.c                 |   52 +
 arch/x86/kvm/svm/svm.h                 |    8 +
 arch/x86/kvm/vmx/capabilities.h        |   52 +
 arch/x86/kvm/vmx/evmcs.c               |   57 ++
 arch/x86/kvm/vmx/evmcs.h               |   22 +
 arch/x86/kvm/vmx/pmu_intel.c           |  170 ++++
 arch/x86/kvm/vmx/posted_intr.c         |   27 +
 arch/x86/kvm/vmx/vmcs.h                |   38 +
 arch/x86/kvm/vmx/vmx.c                 |  385 +++++++
 arch/x86/kvm/vmx/vmx.h                 |   36 +
 arch/x86/kvm/vmx/vmx_ops.h             |    7 +
 arch/x86/kvm/x86.c                     | 1273 ++++++++++++++++++++++++
 arch/x86/kvm/x86.h                     |  110 ++
 arch/x86/mm/extable.c                  |    7 +
 arch/x86/realmode/init.c               |    4 +
 arch/x86/xen/enlighten_hvm.c           |    7 +
 arch/x86/xen/time.c                    |    7 +
 block/blk-mq.c                         |   16 +
 drivers/acpi/cppc_acpi.c               |   42 +
 drivers/acpi/utils.c                   |   17 +
 drivers/block/ublk_drv.c               |    8 +
 drivers/block/virtio_blk.c             |    5 +
 drivers/block/xen-blkback/blkback.c    |  417 ++++++++
 drivers/block/xen-blkback/common.h     |  141 +++
 drivers/block/xen-blkback/xenbus.c     |   38 +
 drivers/iommu/amd/iommu.c              |   10 +
 drivers/iommu/intel/irq_remapping.c    |    7 +
 drivers/irqchip/irq-gic-v3.c           |   23 +
 drivers/net/virtio_net.c               |   55 +
 drivers/net/xen-netfront.c             |    7 +
 drivers/pci/hotplug/pciehp.h           |   25 +
 drivers/pci/hotplug/pciehp_core.c      |   15 +
 drivers/pci/hotplug/pciehp_ctrl.c      |  129 +++
 drivers/pci/hotplug/pciehp_hpc.c       |   85 ++
 drivers/pci/hotplug/pciehp_pci.c       |    4 +
 drivers/pci/iov.c                      |    7 +
 drivers/pci/pci.c                      |   37 +
 drivers/pci/probe.c                    |    5 +
 drivers/pci/setup-res.c                |   13 +
 drivers/perf/arm_dmc620_pmu.c          |    3 +
 drivers/perf/arm_pmu.c                 |   23 +
 drivers/perf/arm_pmu_acpi.c            |    4 +
 drivers/scsi/virtio_scsi.c             |   10 +
 drivers/target/target_core_file.c      |   14 +
 drivers/target/target_core_hba.c       |    8 +
 drivers/target/target_core_iblock.c    |   36 +
 drivers/target/target_core_sbc.c       |    6 +
 drivers/target/target_core_transport.c |   79 ++
 drivers/vfio/pci/vfio_pci_core.c       |   20 +
 drivers/vfio/pci/vfio_pci_intrs.c      |  122 +++
 drivers/vhost/net.c                    |    5 +
 drivers/vhost/scsi.c                   |  302 ++++++
 drivers/vhost/vhost.c                  |  150 +++
 drivers/vhost/vhost.h                  |   26 +
 drivers/virtio/virtio.c                |   43 +
 drivers/virtio/virtio_balloon.c        |    7 +
 drivers/virtio/virtio_pci_common.c     |   40 +
 drivers/virtio/virtio_pci_modern.c     |   13 +
 drivers/virtio/virtio_pci_modern_dev.c |    7 +
 drivers/virtio/virtio_ring.c           |  636 ++++++++++++
 drivers/xen/events/events_base.c       |   10 +
 fs/proc/interrupts.c                   |    4 +
 fs/select.c                            |    5 +
 include/kvm/arm_pmu.h                  |   39 +
 include/kvm/arm_vgic.h                 |   17 +
 include/kvm/iodev.h                    |    6 +
 include/linux/clocksource.h            |   39 +
 include/linux/irqbypass.h              |   24 +
 include/linux/irqdesc.h                |   33 +
 include/linux/irqnr.h                  |   18 +
 include/linux/kvm_host.h               |   70 ++
 include/linux/pci.h                    |    5 +
 include/linux/perf_event.h             |   61 ++
 include/linux/sched.h                  |    9 +
 include/linux/time_namespace.h         |    7 +
 include/linux/timekeeper_internal.h    |  164 +++
 include/linux/timekeeping.h            |    9 +
 include/linux/timex.h                  |   14 +
 include/linux/vfio_pci_core.h          |   13 +
 include/uapi/linux/kvm.h               |    5 +
 include/uapi/linux/perf_event.h        |    7 +
 include/uapi/linux/virtio_ring.h       |   28 +
 kernel/cpu.c                           |   99 ++
 kernel/events/core.c                   |  575 +++++++++++
 kernel/events/internal.h               |   31 +
 kernel/events/ring_buffer.c            |   71 ++
 kernel/irq/manage.c                    |   18 +
 kernel/sched/core.c                    |   17 +
 kernel/sched/cputime.c                 |    9 +
 kernel/sched/fair.c                    |    5 +
 kernel/smpboot.c                       |   18 +
 kernel/stop_machine.c                  |  317 ++++++
 kernel/time/clocksource.c              |  220 ++++
 kernel/time/ntp.c                      |   51 +
 kernel/time/posix-stubs.c              |    5 +
 kernel/time/tick-common.c              |   34 +
 kernel/time/tick-legacy.c              |   20 +
 kernel/time/tick-oneshot.c             |   13 +
 kernel/time/tick-sched.c               |   11 +
 kernel/time/timekeeping.c              | 1268 +++++++++++++++++++++++
 kernel/time/timekeeping_debug.c        |    4 +
 kernel/time/timekeeping_internal.h     |   15 +
 kernel/watchdog.c                      |  114 +++
 kernel/watchdog_hld.c                  |   38 +
 lib/iov_iter.c                         |   16 +
 mm/huge_memory.c                       |   14 +
 mm/memblock.c                          |    9 +
 mm/memory.c                            |    8 +
 mm/page_alloc.c                        |    6 +
 net/core/sock.c                        |   24 +
 net/ethernet/eth.c                     |   18 +
 security/integrity/ima/ima_efi.c       |   19 +
 security/integrity/ima/ima_main.c      |    4 +
 tools/include/linux/ring_buffer.h      |    7 +
 tools/include/uapi/linux/perf_event.h  |    7 +
 tools/lib/perf/evlist.c                |    6 +
 tools/lib/perf/evsel.c                 |   36 +
 tools/lib/perf/mmap.c                  |   42 +
 tools/perf/builtin-record.c            |  465 +++++++++
 tools/perf/builtin-report.c            |   27 +
 tools/perf/builtin-stat.c              |   77 ++
 tools/perf/builtin-top.c               |   13 +
 tools/perf/util/counts.c               |   15 +
 tools/perf/util/data.c                 |   29 +
 tools/perf/util/data.h                 |   37 +
 tools/perf/util/evlist.c               |   52 +
 tools/perf/util/evsel.c                |   75 ++
 tools/perf/util/evsel.h                |   43 +
 tools/perf/util/hist.c                 |   42 +
 tools/perf/util/mmap.c                 |   93 ++
 tools/perf/util/session.c              |   19 +
 tools/perf/util/stat-display.c         |   21 +
 tools/perf/util/stat.c                 |   32 +
 tools/perf/util/synthetic-events.c     |   26 +
 virt/kvm/eventfd.c                     |   67 ++
 virt/kvm/irqchip.c                     |    9 +
 virt/kvm/kvm_main.c                    |  148 +++
 virt/lib/irqbypass.c                   |   51 +
 233 files changed, 14036 insertions(+)

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 53035763e..3fe77c8d5 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -11,6 +11,16 @@
 #include <asm/insn.h>
 #include <asm/virt.h>
 
+/*
+ * 在以下调用ARM_EXCEPTION_CODE():
+ *   - arch/arm64/include/asm/kvm_asm.h|16| <<ARM_EXCEPTION_IS_TRAP>> #define ARM_EXCEPTION_IS_TRAP(x) (ARM_EXCEPTION_CODE((x)) == ARM_EXCEPTION_TRAP)
+ *   - arch/arm64/kvm/arm.c|990| <<kvm_arch_vcpu_ioctl_run>> if (ARM_EXCEPTION_CODE(ret) == ARM_EXCEPTION_IRQ) {
+ *   - arch/arm64/kvm/handle_exit.c|270| <<handle_exit>> exception_index = ARM_EXCEPTION_CODE(exception_index);
+ *   - arch/arm64/kvm/handle_exit.c|316| <<handle_exit_early>> exception_index = ARM_EXCEPTION_CODE(exception_index);
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|432| <<fixup_guest_exit>> if (ARM_EXCEPTION_CODE(*exit_code) != ARM_EXCEPTION_IRQ)
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|436| <<fixup_guest_exit>> ARM_EXCEPTION_CODE(*exit_code) != ARM_EXCEPTION_IRQ) {
+ *   - arch/arm64/kvm/trace_arm.h|40| <<__field>> __entry->ret = ARM_EXCEPTION_CODE(ret);
+ */
 #define ARM_EXIT_WITH_SERROR_BIT  31
 #define ARM_EXCEPTION_CODE(x)	  ((x) & ~(1U << ARM_EXIT_WITH_SERROR_BIT))
 #define ARM_EXCEPTION_IS_TRAP(x)  (ARM_EXCEPTION_CODE((x)) == ARM_EXCEPTION_TRAP)
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e9c9388cc..8e686b37f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -595,6 +595,12 @@ struct kvm_vcpu_arch {
 
 #define ctxt_sys_reg(c,r)	(*__ctxt_sys_reg(c,r))
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_cpu_context ctxt;
+ *       -> u64 sys_regs[NR_SYS_REGS];
+ */
 #define __vcpu_sys_reg(v,r)	(ctxt_sys_reg(&(v)->arch.ctxt, (r)))
 
 u64 vcpu_read_sys_reg(const struct kvm_vcpu *vcpu, int reg);
diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 9f339dffb..8936af77b 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
@@ -304,6 +304,10 @@ int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
 			      enum kvm_pgtable_stage2_flags flags,
 			      kvm_pgtable_force_pte_cb_t force_pte_cb);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|663| <<kvm_init_stage2_mmu>> err = kvm_pgtable_stage2_init(pgt, mmu, &kvm_s2_mm_ops);
+ */
 #define kvm_pgtable_stage2_init(pgt, mmu, mm_ops) \
 	__kvm_pgtable_stage2_init(pgt, mmu, mm_ops, 0, NULL)
 
diff --git a/arch/arm64/kernel/perf_event.c b/arch/arm64/kernel/perf_event.c
index cb69ff1e6..f8586c2a7 100644
--- a/arch/arm64/kernel/perf_event.c
+++ b/arch/arm64/kernel/perf_event.c
@@ -403,6 +403,15 @@ static inline bool armv8pmu_event_has_user_read(struct perf_event *event)
  * except when we have allocated the 64bit cycle counter (for CPU
  * cycles event) or when user space counter access is enabled.
  */
+/*
+ * called by:
+ *   - arch/arm64/kernel/perf_event.c|528| <<armv8pmu_read_hw_counter>> if (armv8pmu_event_is_chained(event))
+ *   - arch/arm64/kernel/perf_event.c|597| <<armv8pmu_write_hw_counter>> if (armv8pmu_event_is_chained(event)) {
+ *   - arch/arm64/kernel/perf_event.c|636| <<armv8pmu_write_event_type>> if (armv8pmu_event_is_chained(event)) {
+ *   - arch/arm64/kernel/perf_event.c|655| <<armv8pmu_event_cnten_mask>> if (armv8pmu_event_is_chained(event))
+ *   - arch/arm64/kernel/perf_event.c|943| <<armv8pmu_get_event_idx>> if (armv8pmu_event_is_chained(event))
+ *   - arch/arm64/kernel/perf_event.c|955| <<armv8pmu_clear_event_idx>> if (armv8pmu_event_is_chained(event))
+ */
 static inline bool armv8pmu_event_is_chained(struct perf_event *event)
 {
 	int idx = event->hw.idx;
@@ -1050,6 +1059,15 @@ static void armv8pmu_reset(void *info)
 	armv8pmu_pmcr_write(pmcr);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/perf_event.c|1111| <<armv8_pmuv3_map_event>> return __armv8_pmuv3_map_event(event, NULL, NULL);
+ *   - arch/arm64/kernel/perf_event.c|1116| <<armv8_a53_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a53_perf_cache_map);
+ *   - arch/arm64/kernel/perf_event.c|1121| <<armv8_a57_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a57_perf_cache_map);
+ *   - arch/arm64/kernel/perf_event.c|1126| <<armv8_a73_map_event>> return __armv8_pmuv3_map_event(event, NULL, &armv8_a73_perf_cache_map);
+ *   - arch/arm64/kernel/perf_event.c|1131| <<armv8_thunder_map_event>> return __armv8_pmuv3_map_event(event, NULL,
+ *   - arch/arm64/kernel/perf_event.c|1137| <<armv8_vulcan_map_event>> return __armv8_pmuv3_map_event(event, NULL,
+ */
 static int __armv8_pmuv3_map_event(struct perf_event *event,
 				   const unsigned (*extra_event_map)
 						  [PERF_COUNT_HW_MAX],
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index bb24a76b4..80aaf1024 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -1025,6 +1025,10 @@ static void timer_irq_ack(struct irq_data *d)
 		d->chip->irq_ack(d);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1045| <<timer_irq_domain_alloc>> &timer_chip, NULL);
+ */
 static struct irq_chip timer_chip = {
 	.name			= "KVM",
 	.irq_ack		= timer_irq_ack,
@@ -1295,6 +1299,10 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
  * The host kernel runs at EL2 with HCR_EL2.TGE == 1,
  * and this makes those bits have no effect for the host kernel execution.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1660| <<cpu_hyp_init_features>> kvm_timer_init_vhe();
+ */
 void kvm_timer_init_vhe(void)
 {
 	/* When HCR_EL2.E2H ==1, EL1PCEN and EL1PCTEN are shifted by 10 */
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index 917086be5..b95d5d640 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -56,6 +56,17 @@ DECLARE_KVM_NVHE_PER_CPU(struct kvm_nvhe_init_params, kvm_init_params);
 static bool vgic_present;
 
 static DEFINE_PER_CPU(unsigned char, kvm_arm_hardware_enabled);
+/*
+ * 在以下使用userspace_irqchip_in_use:
+ *   - arch/arm64/include/asm/kvm_host.h|67| <<global>> DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
+ *   - arch/arm64/kvm/arm.c|59| <<global>> DEFINE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
+ *   - arch/arm64/kvm/arch_timer.c|164| <<userspace_irqchip>> return static_branch_unlikely(&userspace_irqchip_in_use) &&
+ *   - arch/arm64/kvm/arm.c|368| <<kvm_arch_vcpu_destroy>> static_branch_dec(&userspace_irqchip_in_use);
+ *   - arch/arm64/kvm/arm.c|581| <<kvm_arch_vcpu_run_pid_change>> static_branch_inc(&userspace_irqchip_in_use);
+ *   - arch/arm64/kvm/arm.c|792| <<kvm_vcpu_exit_request>> if (static_branch_unlikely(&userspace_irqchip_in_use)) {
+ *   - arch/arm64/kvm/arm.c|911| <<kvm_arch_vcpu_ioctl_run>> if (static_branch_unlikely(&userspace_irqchip_in_use))
+ *   - arch/arm64/kvm/arm.c|957| <<kvm_arch_vcpu_ioctl_run>> if (static_branch_unlikely(&userspace_irqchip_in_use))
+ */
 DEFINE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
@@ -134,6 +145,10 @@ static void set_default_spectre(struct kvm *kvm)
  * kvm_arch_init_vm - initializes a VM data structure
  * @kvm:	pointer to the KVM struct
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1203| <<kvm_create_vm>> r = kvm_arch_init_vm(kvm, type);
+ */
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int ret;
@@ -531,6 +546,9 @@ static int kvm_vcpu_initialized(struct kvm_vcpu *vcpu)
  * run for the first time, as well as the updates that must be
  * performed each time we get a new thread dealing with this vcpu.
  */
+/*
+ * virt/kvm/kvm_main.c|4085| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_run_pid_change(vcpu);
+ */
 int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -565,6 +583,10 @@ int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|583| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_arm_pmu_v3_enable(vcpu);
+	 */
 	ret = kvm_arm_pmu_v3_enable(vcpu);
 	if (ret)
 		return ret;
@@ -741,6 +763,12 @@ static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 			preempt_enable();
 		}
 
+		/*
+		 * struct kvm_vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct kvm_cpu_context ctxt;
+		 *       -> u64 sys_regs[NR_SYS_REGS];
+		 */
 		if (kvm_check_request(KVM_REQ_RELOAD_PMU, vcpu))
 			kvm_pmu_handle_pmcr(vcpu,
 					    __vcpu_sys_reg(vcpu, PMCR_EL0));
@@ -834,6 +862,10 @@ static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
  * return with return value 0 and with the kvm_run structure filled in with the
  * required data for the requested emulation.
  */
+/*
+ * called by (KVM_RUN):
+ *   - virt/kvm/kvm_main.c|4095| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -884,6 +916,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		 */
 		kvm_arm_vmid_update(&vcpu->arch.hw_mmu->vmid);
 
+		/*
+		 * 只在此处调用:
+		 *   - arch/arm64/kvm/arm.c|906| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_flush_hwstate(vcpu);
+		 */
 		kvm_pmu_flush_hwstate(vcpu);
 
 		local_irq_disable();
@@ -903,6 +939,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		if (ret <= 0 || kvm_vcpu_exit_request(vcpu, &ret)) {
 			vcpu->mode = OUTSIDE_GUEST_MODE;
 			isb(); /* Ensure work in x_flush_hwstate is committed */
+			/*
+			 * called by:
+			 *   - arch/arm64/kvm/arm.c|925| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+			 *   - arch/arm64/kvm/arm.c|958| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+			 */
 			kvm_pmu_sync_hwstate(vcpu);
 			if (static_branch_unlikely(&userspace_irqchip_in_use))
 				kvm_timer_sync_user(vcpu);
@@ -936,6 +977,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		 * that the vgic can properly sample the updated state of the
 		 * interrupt line.
 		 */
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arm.c|925| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+		 *   - arch/arm64/kvm/arm.c|958| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+		 */
 		kvm_pmu_sync_hwstate(vcpu);
 
 		/*
@@ -1208,6 +1254,10 @@ static int kvm_arch_vcpu_ioctl_vcpu_init(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1373| <<kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)>> r = kvm_arm_vcpu_set_attr(vcpu, &attr);
+ */
 static int kvm_arm_vcpu_set_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -1222,6 +1272,10 @@ static int kvm_arm_vcpu_set_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1380| <<kvm_arch_vcpu_ioctl(KVM_GET_DEVICE_ATTR)>> r = kvm_arm_vcpu_get_attr(vcpu, &attr);
+ */
 static int kvm_arm_vcpu_get_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -1236,6 +1290,10 @@ static int kvm_arm_vcpu_get_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1387| <<kvm_arch_vcpu_ioctl(KVM_HAS_DEVICE_ATTR)>> r = kvm_arm_vcpu_has_attr(vcpu, &attr);
+ */
 static int kvm_arm_vcpu_has_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -1568,6 +1626,10 @@ static void cpu_prepare_hyp_mode(int cpu)
 	kvm_flush_dcache_to_poc(params, sizeof(*params));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1597| <<cpu_init_hyp_mode>> hyp_install_host_vector();
+ */
 static void hyp_install_host_vector(void)
 {
 	struct kvm_nvhe_init_params *params;
@@ -1901,6 +1963,10 @@ static int kvm_hyp_init_protection(u32 hyp_va_bits)
 /**
  * Inits Hyp-mode on all online CPUs
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2224| <<kvm_arch_init>> err = init_hyp_mode();
+ */
 static int init_hyp_mode(void)
 {
 	u32 hyp_va_bits;
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index f802a3b3f..f7376f8a5 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -943,6 +943,10 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1237| <<kvm_arm_vcpu_set_attr>> ret = kvm_arm_vcpu_arch_set_attr(vcpu, attr);
+ */
 int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr)
 {
@@ -966,6 +970,10 @@ int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1251| <<kvm_arm_vcpu_get_attr>> ret = kvm_arm_vcpu_arch_get_attr(vcpu, attr);
+ */
 int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr)
 {
@@ -989,6 +997,10 @@ int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1265| <<kvm_arm_vcpu_has_attr>> ret = kvm_arm_vcpu_arch_has_attr(vcpu, attr);
+ */
 int kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr)
 {
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index bbe5b393d..84f59793c 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -255,6 +255,10 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1026| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/hyp/exception.c b/arch/arm64/kvm/hyp/exception.c
index b7557b25e..f3e730e65 100644
--- a/arch/arm64/kvm/hyp/exception.c
+++ b/arch/arm64/kvm/hyp/exception.c
@@ -18,6 +18,14 @@
 #error Hypervisor code only!
 #endif
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/exception.c|99| <<enter_exception64>> vbar = __vcpu_read_sys_reg(vcpu, VBAR_EL1);
+ *   - arch/arm64/kvm/hyp/exception.c|100| <<enter_exception64>> sctlr = __vcpu_read_sys_reg(vcpu, SCTLR_EL1);
+ *   - arch/arm64/kvm/hyp/exception.c|178| <<get_except32_cpsr>> u32 sctlr = __vcpu_read_sys_reg(vcpu, SCTLR_EL1);
+ *   - arch/arm64/kvm/hyp/exception.c|274| <<enter_exception32>> u32 sctlr = __vcpu_read_sys_reg(vcpu, SCTLR_EL1);
+ *   - arch/arm64/kvm/hyp/exception.c|298| <<enter_exception32>> vect_offset += __vcpu_read_sys_reg(vcpu, VBAR_EL1);
+ */
 static inline u64 __vcpu_read_sys_reg(const struct kvm_vcpu *vcpu, int reg)
 {
 	u64 val;
diff --git a/arch/arm64/kvm/hyp/nvhe/hyp-main.c b/arch/arm64/kvm/hyp/nvhe/hyp-main.c
index 3cea4b6ac..f441dff4b 100644
--- a/arch/arm64/kvm/hyp/nvhe/hyp-main.c
+++ b/arch/arm64/kvm/hyp/nvhe/hyp-main.c
@@ -195,6 +195,11 @@ typedef void (*hcall_t)(struct kvm_cpu_context *);
 
 #define HANDLE_FUNC(x)	[__KVM_HOST_SMCCC_FUNC_##x] = (hcall_t)handle_##x
 
+/*
+ * 在以下使用host_hcall[]:
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|245| <<handle_host_hcall>> if (unlikely(id < hcall_min || id >= ARRAY_SIZE(host_hcall)))
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|248| <<handle_host_hcall>> hfn = host_hcall[id];
+ */
 static const hcall_t host_hcall[] = {
 	/* ___kvm_hyp_init */
 	HANDLE_FUNC(__kvm_get_mdcr_el2),
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 2cb3867eb..a4d257352 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -682,6 +682,12 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
 	return !!pte;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|747| <<stage2_map_walker_try_leaf>> stage2_put_pte(ptep, data->mmu, addr, level, mm_ops);
+ *   - arch/arm64/kvm/hyp/pgtable.c|823| <<stage2_map_walk_leaf>> stage2_put_pte(ptep, data->mmu, addr, level, mm_ops);
+ *   - arch/arm64/kvm/hyp/pgtable.c|984| <<stage2_unmap_walker>> stage2_put_pte(ptep, mmu, addr, level, mm_ops);
+ */
 static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
 {
@@ -786,6 +792,11 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|846| <<stage2_map_walk_table_post>> ret = stage2_map_walk_leaf(addr, end, level, ptep, data);
+ *   - arch/arm64/kvm/hyp/pgtable.c|885| <<stage2_map_walker>> return stage2_map_walk_leaf(addr, end, level, ptep, data);
+ */
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 				struct stage2_map_data *data)
 {
@@ -1115,6 +1126,10 @@ bool kvm_pgtable_stage2_is_young(struct kvm_pgtable *pgt, u64 addr)
 	return pte & KVM_PTE_LEAF_ATTR_LO_S2_AF;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1296| <<user_mem_abort>> ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
+ */
 int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
 				   enum kvm_pgtable_prot prot)
 {
@@ -1172,6 +1187,11 @@ int kvm_pgtable_stage2_flush(struct kvm_pgtable *pgt, u64 addr, u64 size)
 }
 
 
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_pgtable.h|308| <<kvm_pgtable_stage2_init>> __kvm_pgtable_stage2_init(pgt, mmu, mm_ops, 0, NULL)
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|133| <<kvm_host_prepare_stage2>> ret = __kvm_pgtable_stage2_init(&host_kvm.pgt, mmu,
+ */
 int __kvm_pgtable_stage2_init(struct kvm_pgtable *pgt, struct kvm_s2_mmu *mmu,
 			      struct kvm_pgtable_mm_ops *mm_ops,
 			      enum kvm_pgtable_stage2_flags flags,
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..aaa746c88 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -1013,6 +1013,11 @@ static void __vgic_v3_write_ctlr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 	write_gicreg(vmcr, ICH_VMCR_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|316| <<kvm_hyp_handle_sysreg>> __vgic_v3_perform_cpuif_access(vcpu) == 1)
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|328| <<kvm_hyp_handle_cp15_32>> __vgic_v3_perform_cpuif_access(vcpu) == 1)
+ */
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu)
 {
 	int rt;
diff --git a/arch/arm64/kvm/hyp/vhe/debug-sr.c b/arch/arm64/kvm/hyp/vhe/debug-sr.c
index 289689b26..6e0bb67fe 100644
--- a/arch/arm64/kvm/hyp/vhe/debug-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/debug-sr.c
@@ -10,16 +10,29 @@
 
 #include <asm/kvm_hyp.h>
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|167| <<__kvm_vcpu_run_vhe>> __debug_switch_to_guest(vcpu);
+ */
 void __debug_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	__debug_switch_to_guest_common(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|185| <<__kvm_vcpu_run_vhe>> __debug_switch_to_host(vcpu);
+ */
 void __debug_switch_to_host(struct kvm_vcpu *vcpu)
 {
 	__debug_switch_to_host_common(vcpu);
 }
 
+/*
+ * 在以下使用__kvm_get_mdcr_el2():
+ *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|205| <<host_hcall[]>> HANDLE_FUNC(__kvm_get_mdcr_el2),
+ *   - arch/arm64/kvm/debug.c|68| <<kvm_arm_init_debug>> __this_cpu_write(mdcr_el2, kvm_call_hyp_ret(__kvm_get_mdcr_el2));
+ */
 u64 __kvm_get_mdcr_el2(void)
 {
 	return read_sysreg(mdcr_el2);
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 7acb87eaa..19d228eb8 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -131,6 +131,10 @@ static void early_exit_filter(struct kvm_vcpu *vcpu, u64 *exit_code)
 }
 
 /* Switch to the guest for VHE systems running in EL2 */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|204| <<__kvm_vcpu_run>> ret = __kvm_vcpu_run_vhe(vcpu);
+ */
 static int __kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *host_ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
index 7b44f6b3b..3e8c2799f 100644
--- a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
@@ -95,6 +95,10 @@ void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
  * and deferring saving system register state until we're no longer running the
  * VCPU avoids having to save them on every exit from the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|434| <<kvm_arch_vcpu_put>> kvm_vcpu_put_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_put_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/tlb.c b/arch/arm64/kvm/hyp/vhe/tlb.c
index 24cef9b87..a8185ca51 100644
--- a/arch/arm64/kvm/hyp/vhe/tlb.c
+++ b/arch/arm64/kvm/hyp/vhe/tlb.c
@@ -16,6 +16,12 @@ struct tlb_inv_context {
 	u64		sctlr;
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|90| <<__kvm_tlb_flush_vmid_ipa>> __tlb_switch_to_guest(mmu, &cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|121| <<__kvm_tlb_flush_vmid>> __tlb_switch_to_guest(mmu, &cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|135| <<__kvm_flush_cpu_context>> __tlb_switch_to_guest(mmu, &cxt);
+ */
 static void __tlb_switch_to_guest(struct kvm_s2_mmu *mmu,
 				  struct tlb_inv_context *cxt)
 {
@@ -60,6 +66,12 @@ static void __tlb_switch_to_guest(struct kvm_s2_mmu *mmu,
 	isb();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|117| <<__kvm_tlb_flush_vmid_ipa>> __tlb_switch_to_host(&cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|133| <<__kvm_tlb_flush_vmid>> __tlb_switch_to_host(&cxt);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|148| <<__kvm_flush_cpu_context>> __tlb_switch_to_host(&cxt);
+ */
 static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
 {
 	/*
@@ -79,6 +91,11 @@ static void __tlb_switch_to_host(struct tlb_inv_context *cxt)
 	local_irq_restore(cxt->flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|694| <<stage2_put_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, addr, level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1139| <<kvm_pgtable_stage2_relax_perms>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, pgt->mmu, addr, level);
+ */
 void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,
 			      phys_addr_t ipa, int level)
 {
@@ -111,6 +128,11 @@ void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/pgtable.c|784| <<stage2_map_walk_table_pre>> kvm_call_hyp(__kvm_tlb_flush_vmid, data->mmu);
+ *   - arch/arm64/kvm/mmu.c|84| <<kvm_flush_remote_tlbs>> kvm_call_hyp(__kvm_tlb_flush_vmid, &kvm->arch.mmu);
+ */
 void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
 {
 	struct tlb_inv_context cxt;
@@ -127,6 +149,10 @@ void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|416| <<kvm_arch_vcpu_load>> kvm_call_hyp(__kvm_flush_cpu_context, mmu);
+ */
 void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu)
 {
 	struct tlb_inv_context cxt;
@@ -142,6 +168,14 @@ void __kvm_flush_cpu_context(struct kvm_s2_mmu *mmu)
 	__tlb_switch_to_host(&cxt);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vmid.c|69| <<flush_context>> kvm_call_hyp(__kvm_flush_vm_context);
+ *
+ * new_vmid()
+ * -> flush_context()
+ *    -> __kvm_flush_vm_context()
+ */
 void __kvm_flush_vm_context(void)
 {
 	dsb(ishst);
diff --git a/arch/arm64/kvm/hypercalls.c b/arch/arm64/kvm/hypercalls.c
index c9f401fa0..83820a110 100644
--- a/arch/arm64/kvm/hypercalls.c
+++ b/arch/arm64/kvm/hypercalls.c
@@ -121,6 +121,10 @@ static bool kvm_hvc_call_allowed(struct kvm_vcpu *vcpu, u32 func_id)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|44| <<handle_hvc>> ret = kvm_hvc_call_handler(vcpu);
+ */
 int kvm_hvc_call_handler(struct kvm_vcpu *vcpu)
 {
 	struct kvm_smccc_features *smccc_feat = &vcpu->kvm->arch.smccc_feat;
diff --git a/arch/arm64/kvm/inject_fault.c b/arch/arm64/kvm/inject_fault.c
index f32f4a2a3..4008591da 100644
--- a/arch/arm64/kvm/inject_fault.c
+++ b/arch/arm64/kvm/inject_fault.c
@@ -199,6 +199,13 @@ void kvm_set_sei_esr(struct kvm_vcpu *vcpu, u64 esr)
  * uncategorized RAS error. Without the RAS Extensions we can't specify an ESR
  * value, so the CPU generates an imp-def value.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|837| <<__kvm_arm_vcpu_set_events>> kvm_inject_vabt(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|33| <<kvm_handle_guest_serror>> kvm_inject_vabt(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|310| <<handle_exit_early>> kvm_inject_vabt(vcpu);
+ *   - arch/arm64/kvm/mmu.c|1392| <<kvm_handle_guest_abort>> kvm_inject_vabt(vcpu);
+ */
 void kvm_inject_vabt(struct kvm_vcpu *vcpu)
 {
 	kvm_set_sei_esr(vcpu, ESR_ELx_ISV);
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index c9a13e487..bd379811d 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -645,6 +645,10 @@ static struct kvm_pgtable_mm_ops kvm_s2_mm_ops = {
  * Note we don't need locking here as this is only called when the VM is
  * created, which can only be done once.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|145| <<kvm_arch_init_vm>> ret = kvm_init_stage2_mmu(kvm, &kvm->arch.mmu);
+ */
 int kvm_init_stage2_mmu(struct kvm *kvm, struct kvm_s2_mmu *mmu)
 {
 	int cpu, err;
@@ -1084,6 +1088,10 @@ static int sanitise_mte_tags(struct kvm *kvm, kvm_pfn_t pfn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1470| <<kvm_handle_guest_abort>> ret = user_mem_abort(vcpu, fault_ipa, memslot, hva, fault_status);
+ */
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  struct kvm_memory_slot *memslot, unsigned long hva,
 			  unsigned long fault_status)
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 11c43bed5..347661a6e 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -15,21 +15,120 @@
 #include <kvm/arm_pmu.h>
 #include <kvm/arm_vgic.h>
 
+/*
+ * 调度进去时:
+ *
+ * prepare_task_switch()
+ * -> perf_event_task_sched_out()
+ *    -> __perf_event_task_sched_out()
+ *       -> perf_pmu_sched_task()
+ *          -> pmu->sched_task(cpuctx->task_ctx, sched_in)
+ *
+ * 调度出去时:
+ *
+ * finish_task_switch()
+ * -> perf_event_task_sched_in()
+ *    -> perf_event_context_sched_in()
+ *       -> perf_event_sched_in()
+ */
+
+/*
+ * From bead02204e9806807bb290137b1ccabfcb4b16fd Mon Sep 17 00:00:00 2001
+ * From: Marc Zyngier <maz@kernel.org>
+ * Date: Sun, 13 Nov 2022 16:38:18 +0000
+ * Subject: KVM: arm64: PMU: Align chained counter implementation with
+ *  architecture pseudocode
+ *
+ * Ricardo recently pointed out that the PMU chained counter emulation
+ * in KVM wasn't quite behaving like the one on actual hardware, in
+ * the sense that a chained counter would expose an overflow on
+ * both halves of a chained counter, while KVM would only expose the
+ * overflow on the top half.
+ *
+ * The difference is subtle, but significant. What does the architecture
+ * say (DDI0087 H.a):
+ *
+ * - Up to PMUv3p4, all counters but the cycle counter are 32bit
+ *
+ * - A 32bit counter that overflows generates a CHAIN event on the
+ *   adjacent counter after exposing its own overflow status
+ * 
+ * - The CHAIN event is accounted if the counter is correctly
+ *   configured (CHAIN event selected and counter enabled)
+ *
+ * This all means that our current implementation (which uses 64bit
+ * perf events) prevents us from emulating this overflow on the lower half.
+ *
+ * How to fix this? By implementing the above, to the letter.
+ *
+ * This largely results in code deletion, removing the notions of
+ * "counter pair", "chained counters", and "canonical counter".
+ * The code is further restructured to make the CHAIN handling similar
+ * to SWINC, as the two are now extremely similar in behaviour.
+ *
+ * Reported-by: Ricardo Koller <ricarkol@google.com>
+ * Signed-off-by: Marc Zyngier <maz@kernel.org>
+ * Reviewed-by: Reiji Watanabe <reijiw@google.com>
+ * Link: https://lore.kernel.org/r/20221113163832.3154370-3-maz@kernel.org
+ */
+
+/*
+ * 在以下使用kvm_arm_pmu_available:
+ *   - arch/arm64/kernel/image-vars.h|116| <<global>> KVM_NVHE_ALIAS(kvm_arm_pmu_available);
+ *   - arch/arm64/kvm/pmu-emul.c|18| <<global>> DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+ *   - include/kvm/arm_pmu.h|43| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+ *   - arch/arm64/kvm/pmu-emul.c|848| <<kvm_host_pmu_init>> static_branch_enable(&kvm_arm_pmu_available);
+ *   - include/kvm/arm_pmu.h|47| <<kvm_arm_support_pmu_v3>> return static_branch_likely(&kvm_arm_pmu_available);
+ */
 DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
 
+/*
+ * 在以下使用arm_pmus:
+ *   - arch/arm64/kvm/pmu-emul.c|26| <<global>> static LIST_HEAD(arm_pmus);
+ *   - arch/arm64/kvm/pmu-emul.c|845| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+ *   - arch/arm64/kvm/pmu-emul.c|847| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+ *   - arch/arm64/kvm/pmu-emul.c|1068| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+ */
 static LIST_HEAD(arm_pmus);
+/*
+ * 在以下使用arm_pmus_lock:
+ *   - arch/arm64/kvm/pmu-emul.c|21| <<global>> static DEFINE_MUTEX(arm_pmus_lock);
+ *   - arch/arm64/kvm/pmu-emul.c|838| <<kvm_host_pmu_init>> mutex_lock(&arm_pmus_lock);
+ *   - arch/arm64/kvm/pmu-emul.c|851| <<kvm_host_pmu_init>> mutex_unlock(&arm_pmus_lock);
+ *   - arch/arm64/kvm/pmu-emul.c|1066| <<kvm_arm_pmu_v3_set_pmu>> mutex_lock(&arm_pmus_lock);
+ *   - arch/arm64/kvm/pmu-emul.c|1084| <<kvm_arm_pmu_v3_set_pmu>> mutex_unlock(&arm_pmus_lock);
+ */
 static DEFINE_MUTEX(arm_pmus_lock);
 
 static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx);
 static void kvm_pmu_update_pmc_chained(struct kvm_vcpu *vcpu, u64 select_idx);
 static void kvm_pmu_stop_counter(struct kvm_vcpu *vcpu, struct kvm_pmc *pmc);
 
+/*
+ * 在以下使用PERF_ATTR_CFG1_KVM_PMU_CHAINED:
+ *   - arch/arm64/kvm/pmu-emul.c|1048| <<kvm_pmu_create_perf_event>> attr.config1 |= PERF_ATTR_CFG1_KVM_PMU_CHAINED;
+ */
 #define PERF_ATTR_CFG1_KVM_PMU_CHAINED 0x1
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|139| <<kvm_pmu_idx_has_chain_evtype>> eventsel = __vcpu_sys_reg(vcpu, reg) & kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|575| <<kvm_pmu_software_increment>> type &= kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|672| <<kvm_pmu_create_perf_event>> eventsel = data & kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|816| <<kvm_pmu_set_counter_event_type>> mask |= kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|946| <<kvm_pmu_get_pmceid>> nr_events = kvm_pmu_event_mask(vcpu->kvm) + 1;
+ *   - arch/arm64/kvm/pmu-emul.c|1144| <<kvm_arm_pmu_v3_set_attr>> nr_events = kvm_pmu_event_mask(kvm) + 1;
+ */
 static u32 kvm_pmu_event_mask(struct kvm *kvm)
 {
 	unsigned int pmuver;
 
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct arm_pmu *arm_pmu;
+	 *       -> int pmuver;
+	 */
 	pmuver = kvm->arch.arm_pmu->pmuver;
 
 	switch (pmuver) {
@@ -51,8 +150,16 @@ static u32 kvm_pmu_event_mask(struct kvm *kvm)
  * @vcpu: The vcpu pointer
  * @select_idx: The counter index
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|522| <<kvm_pmu_perf_overflow>> if (!kvm_pmu_idx_is_64bit(vcpu, pmc->idx))
+ *   - arch/arm64/kvm/pmu-emul.c|726| <<kvm_pmu_create_perf_event>> if (kvm_pmu_idx_is_64bit(vcpu, pmc->idx))
+ */
 static bool kvm_pmu_idx_is_64bit(struct kvm_vcpu *vcpu, u64 select_idx)
 {
+	/*
+	 * ARMV8_PMU_CYCLE_IDX似乎是31
+	 */
 	return (select_idx == ARMV8_PMU_CYCLE_IDX &&
 		__vcpu_sys_reg(vcpu, PMCR_EL0) & ARMV8_PMU_PMCR_LC);
 }
@@ -72,10 +179,32 @@ static struct kvm_vcpu *kvm_pmc_to_vcpu(struct kvm_pmc *pmc)
  * kvm_pmu_pmc_is_chained - determine if the pmc is chained
  * @pmc: The PMU counter pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|100| <<kvm_pmu_get_canonical_pmc>> if (kvm_pmu_pmc_is_chained(pmc) &&
+ *   - arch/arm64/kvm/pmu-emul.c|144| <<kvm_pmu_get_pair_counter_value>> if (kvm_pmu_pmc_is_chained(pmc)) {
+ *   - arch/arm64/kvm/pmu-emul.c|185| <<kvm_pmu_get_counter_value>> if (kvm_pmu_pmc_is_chained(pmc) &&
+ *   - arch/arm64/kvm/pmu-emul.c|255| <<kvm_pmu_stop_counter>> if (kvm_pmu_pmc_is_chained(pmc))
+ *   - arch/arm64/kvm/pmu-emul.c|577| <<kvm_pmu_software_increment>> if (kvm_pmu_pmc_is_chained(&pmu->pmc[i])) {
+ *   - arch/arm64/kvm/pmu-emul.c|689| <<kvm_pmu_create_perf_event>> if (kvm_pmu_pmc_is_chained(pmc)) {
+ *   - arch/arm64/kvm/pmu-emul.c|735| <<kvm_pmu_update_pmc_chained>> old_state = kvm_pmu_pmc_is_chained(pmc);
+ */
 static bool kvm_pmu_pmc_is_chained(struct kvm_pmc *pmc)
 {
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
 
+	/*
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct irq_work overflow_work;
+	 *       -> struct kvm_pmu_events events;
+	 *       -> struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
+	 *       -> DECLARE_BITMAP(chained, ARMV8_PMU_MAX_COUNTER_PAIRS);
+	 *       -> int irq_num;
+	 *       -> bool created;
+	 *       -> bool irq_level;
+	 */
 	return test_bit(pmc->idx >> 1, vcpu->arch.pmu.chained);
 }
 
@@ -83,6 +212,12 @@ static bool kvm_pmu_pmc_is_chained(struct kvm_pmc *pmc)
  * kvm_pmu_idx_is_high_counter - determine if select_idx is a high/low counter
  * @select_idx: The counter index
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|111| <<kvm_pmu_get_canonical_pmc>> kvm_pmu_idx_is_high_counter(pmc->idx))
+ *   - arch/arm64/kvm/pmu-emul.c|118| <<kvm_pmu_get_alternate_pmc>> if (kvm_pmu_idx_is_high_counter(pmc->idx))
+ *   - arch/arm64/kvm/pmu-emul.c|196| <<kvm_pmu_get_counter_value>> kvm_pmu_idx_is_high_counter(select_idx))
+ */
 static bool kvm_pmu_idx_is_high_counter(u64 select_idx)
 {
 	return select_idx & 0x1;
@@ -95,14 +230,36 @@ static bool kvm_pmu_idx_is_high_counter(u64 select_idx)
  * When a pair of PMCs are chained together we use the low counter (canonical)
  * to hold the underlying perf event.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|155| <<kvm_pmu_get_pair_counter_value>> pmc = kvm_pmu_get_canonical_pmc(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|231| <<kvm_pmu_release_perf_event>> pmc = kvm_pmu_get_canonical_pmc(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|249| <<kvm_pmu_stop_counter>> pmc = kvm_pmu_get_canonical_pmc(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|662| <<kvm_pmu_create_perf_event>> pmc = kvm_pmu_get_canonical_pmc(&pmu->pmc[select_idx]);
+ *   - arch/arm64/kvm/pmu-emul.c|778| <<kvm_pmu_update_pmc_chained>> canonical_pmc = kvm_pmu_get_canonical_pmc(pmc);
+ *
+ * When a pair of PMCs are chained together we use the low counter (canonical)
+ * to hold the underlying perf event.
+ */
 static struct kvm_pmc *kvm_pmu_get_canonical_pmc(struct kvm_pmc *pmc)
 {
+	/*
+	 * When a pair of PMCs are chained together we use the low counter (canonical)
+	 * to hold the underlying perf event.
+	 */
 	if (kvm_pmu_pmc_is_chained(pmc) &&
 	    kvm_pmu_idx_is_high_counter(pmc->idx))
 		return pmc - 1;
 
 	return pmc;
 }
+
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|785| <<kvm_pmu_update_pmc_chained>> kvm_pmu_stop_counter(vcpu, kvm_pmu_get_alternate_pmc(pmc));
+ *
+ * 返回pair中的另一个, 如果是high就返回low, 如果是low就返回high
+ */
 static struct kvm_pmc *kvm_pmu_get_alternate_pmc(struct kvm_pmc *pmc)
 {
 	if (kvm_pmu_idx_is_high_counter(pmc->idx))
@@ -116,6 +273,10 @@ static struct kvm_pmc *kvm_pmu_get_alternate_pmc(struct kvm_pmc *pmc)
  * @vcpu: The vcpu pointer
  * @select_idx: The counter index
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|772| <<kvm_pmu_update_pmc_chained>> new_state = kvm_pmu_idx_has_chain_evtype(vcpu, pmc->idx) &&
+ */
 static bool kvm_pmu_idx_has_chain_evtype(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	u64 eventsel, reg;
@@ -128,6 +289,12 @@ static bool kvm_pmu_idx_has_chain_evtype(struct kvm_vcpu *vcpu, u64 select_idx)
 	reg = PMEVTYPER0_EL0 + select_idx;
 	eventsel = __vcpu_sys_reg(vcpu, reg) & kvm_pmu_event_mask(vcpu->kvm);
 
+	/*
+	 * 在以下使用ARMV8_PMUV3_PERFCTR_CHAIN:
+	 *   - arch/arm64/include/asm/perf_event.h|48| <<global>> #define ARMV8_PMUV3_PERFCTR_CHAIN 0x001E
+	 *   - arch/arm64/kernel/perf_event.c|646| <<armv8pmu_write_event_type>> u32 chain_evt = ARMV8_PMUV3_PERFCTR_CHAIN |
+	 *   - arch/arm64/kernel/perf_event.c|1034| <<armv8pmu_filter_match>> return evtype != ARMV8_PMUV3_PERFCTR_CHAIN;
+	 */
 	return eventsel == ARMV8_PMUV3_PERFCTR_CHAIN;
 }
 
@@ -136,12 +303,23 @@ static bool kvm_pmu_idx_has_chain_evtype(struct kvm_vcpu *vcpu, u64 select_idx)
  * @vcpu: The vcpu pointer
  * @pmc: The PMU counter pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|265| <<kvm_pmu_get_counter_value>> counter = kvm_pmu_get_pair_counter_value(vcpu, pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|332| <<kvm_pmu_stop_counter>> counter = kvm_pmu_get_pair_counter_value(vcpu, pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|781| <<kvm_pmu_create_perf_event>> counter = kvm_pmu_get_pair_counter_value(vcpu, pmc);
+ */
 static u64 kvm_pmu_get_pair_counter_value(struct kvm_vcpu *vcpu,
 					  struct kvm_pmc *pmc)
 {
 	u64 counter, counter_high, reg, enabled, running;
 
 	if (kvm_pmu_pmc_is_chained(pmc)) {
+		/*
+		 * 关于kvm_pmu_get_canonical_pmc():
+		 * When a pair of PMCs are chained together we use the low counter (canonical)
+		 * to hold the underlying perf event.
+		 */
 		pmc = kvm_pmu_get_canonical_pmc(pmc);
 		reg = PMEVCNTR0_EL0 + pmc->idx;
 
@@ -171,6 +349,13 @@ static u64 kvm_pmu_get_pair_counter_value(struct kvm_vcpu *vcpu,
  * @vcpu: The vcpu pointer
  * @select_idx: The counter index
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|291| <<kvm_pmu_set_counter_value>> __vcpu_sys_reg(vcpu, reg) += (s64)val - kvm_pmu_get_counter_value(vcpu, select_idx);
+ *   - arch/arm64/kvm/sys_regs.c|817| <<access_pmu_evcntr>> p->regval = kvm_pmu_get_counter_value(vcpu, idx);
+ *
+ * 返回某一个counter寄存器的值
+ */
 u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	u64 counter;
@@ -180,6 +365,9 @@ u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 	if (!kvm_vcpu_has_pmu(vcpu))
 		return 0;
 
+	/*
+	 * kvm_pmu_get_pair_counter_value - get PMU counter value
+	 */
 	counter = kvm_pmu_get_pair_counter_value(vcpu, pmc);
 
 	if (kvm_pmu_pmc_is_chained(pmc) &&
@@ -197,6 +385,12 @@ u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
  * @select_idx: The counter index
  * @val: The counter value
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|711| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_counter_value(vcpu, ARMV8_PMU_CYCLE_IDX, 0);
+ *   - arch/arm64/kvm/pmu-emul.c|717| <<kvm_pmu_handle_pmcr>> kvm_pmu_set_counter_value(vcpu, i, 0);
+ *   - arch/arm64/kvm/sys_regs.c|815| <<access_pmu_evcntr>> kvm_pmu_set_counter_value(vcpu, idx, p->regval);
+ */
 void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
 {
 	u64 reg;
@@ -208,6 +402,13 @@ void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
 	      ? PMCCNTR_EL0 : PMEVCNTR0_EL0 + select_idx;
 	__vcpu_sys_reg(vcpu, reg) += (s64)val - kvm_pmu_get_counter_value(vcpu, select_idx);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|314| <<kvm_pmu_set_counter_value>> kvm_pmu_create_perf_event(vcpu, select_idx);
+	 *   - arch/arm64/kvm/pmu-emul.c|506| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|546| <<kvm_pmu_disable_counter_mask>> kvm_pmu_create_perf_event(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|1030| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(vcpu, select_idx);
+	 */
 	/* Recreate the perf event to reflect the updated sample_period */
 	kvm_pmu_create_perf_event(vcpu, select_idx);
 }
@@ -216,8 +417,20 @@ void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
  * kvm_pmu_release_perf_event - remove the perf event
  * @pmc: The PMU counter pointer
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|347| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|392| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(&pmu->pmc[i]);
+ */
 static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc)
 {
+	/*
+	 * kvm_pmu_get_canonical_pmc - obtain the canonical pmc
+	 * @pmc: The PMU counter pointer
+	 *
+	 * When a pair of PMCs are chained together we use the low counter (canonical)
+	 * to hold the underlying perf event.
+	 */
 	pmc = kvm_pmu_get_canonical_pmc(pmc);
 	if (pmc->perf_event) {
 		perf_event_disable(pmc->perf_event);
@@ -232,10 +445,24 @@ static void kvm_pmu_release_perf_event(struct kvm_pmc *pmc)
  *
  * If this counter has been configured to monitor some event, release it here.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|297| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(vcpu, &pmu->pmc[i]);
+ *   - arch/arm64/kvm/pmu-emul.c|668| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(vcpu, pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|779| <<kvm_pmu_update_pmc_chained>> kvm_pmu_stop_counter(vcpu, canonical_pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|785| <<kvm_pmu_update_pmc_chained>> kvm_pmu_stop_counter(vcpu, kvm_pmu_get_alternate_pmc(pmc));
+ */
 static void kvm_pmu_stop_counter(struct kvm_vcpu *vcpu, struct kvm_pmc *pmc)
 {
 	u64 counter, reg, val;
 
+	/*
+	 * kvm_pmu_get_canonical_pmc - obtain the canonical pmc
+	 * @pmc: The PMU counter pointer
+	 *
+	 * When a pair of PMCs are chained together we use the low counter (canonical)
+	 * to hold the underlying perf event.
+	 */
 	pmc = kvm_pmu_get_canonical_pmc(pmc);
 	if (!pmc->perf_event)
 		return;
@@ -255,6 +482,11 @@ static void kvm_pmu_stop_counter(struct kvm_vcpu *vcpu, struct kvm_pmc *pmc)
 	if (kvm_pmu_pmc_is_chained(pmc))
 		__vcpu_sys_reg(vcpu, reg + 1) = upper_32_bits(counter);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|347| <<kvm_pmu_stop_counter>> kvm_pmu_release_perf_event(pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|392| <<kvm_pmu_vcpu_destroy>> kvm_pmu_release_perf_event(&pmu->pmc[i]);
+	 */
 	kvm_pmu_release_perf_event(pmc);
 }
 
@@ -263,6 +495,10 @@ static void kvm_pmu_stop_counter(struct kvm_vcpu *vcpu, struct kvm_pmc *pmc)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|357| <<kvm_arch_vcpu_create>> kvm_pmu_vcpu_init(vcpu);
+ */
 void kvm_pmu_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -277,8 +513,21 @@ void kvm_pmu_vcpu_init(struct kvm_vcpu *vcpu)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/reset.c|268| <<kvm_reset_vcpu>> kvm_pmu_vcpu_reset(vcpu);
+ */
 void kvm_pmu_vcpu_reset(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|418| <<kvm_pmu_vcpu_reset>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|761| <<kvm_pmu_handle_pmcr>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|868| <<access_pmcnten>> mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|891| <<access_pminten>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|915| <<access_pmovs>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|945| <<access_pmswinc>> mask = kvm_pmu_valid_counter_mask(vcpu);
+	 */
 	unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
 	int i;
@@ -294,6 +543,10 @@ void kvm_pmu_vcpu_reset(struct kvm_vcpu *vcpu)
  * @vcpu: The vcpu pointer
  *
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|383| <<kvm_arch_vcpu_destroy>> kvm_pmu_vcpu_destroy(vcpu);
+ */
 void kvm_pmu_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -301,9 +554,25 @@ void kvm_pmu_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 	for (i = 0; i < ARMV8_PMU_MAX_COUNTERS; i++)
 		kvm_pmu_release_perf_event(&pmu->pmc[i]);
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|440| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|626| <<kvm_pmu_perf_overflow_notify_vcpu>> pmu = container_of(work, struct kvm_pmu, overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|668| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1198| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	irq_work_sync(&vcpu->arch.pmu.overflow_work);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|418| <<kvm_pmu_vcpu_reset>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|761| <<kvm_pmu_handle_pmcr>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|868| <<access_pmcnten>> mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|891| <<access_pminten>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|915| <<access_pmovs>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|945| <<access_pmswinc>> mask = kvm_pmu_valid_counter_mask(vcpu);
+ */
 u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
 {
 	u64 val = __vcpu_sys_reg(vcpu, PMCR_EL0) >> ARMV8_PMU_PMCR_N_SHIFT;
@@ -322,6 +591,11 @@ u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
  *
  * Call perf_event_enable to start counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_handle_pmcr>> kvm_pmu_enable_counter_mask(vcpu,
+ *   - arch/arm64/kvm/sys_regs.c|874| <<access_pmcnten>> kvm_pmu_enable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -360,6 +634,11 @@ void kvm_pmu_enable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
  *
  * Call perf_event_disable to stop counting the perf event
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|753| <<kvm_pmu_handle_pmcr>> kvm_pmu_disable_counter_mask(vcpu, __vcpu_sys_reg(vcpu, PMCNTENSET_EL0));
+ *   - arch/arm64/kvm/sys_regs.c|879| <<access_pmcnten>> kvm_pmu_disable_counter_mask(vcpu, val);
+ */
 void kvm_pmu_disable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -385,6 +664,11 @@ void kvm_pmu_disable_counter_mask(struct kvm_vcpu *vcpu, u64 val)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|550| <<kvm_pmu_update_state>> overflow = !!kvm_pmu_overflow_status(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|662| <<kvm_pmu_perf_overflow>> if (kvm_pmu_overflow_status(vcpu)) {
+ */
 static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 {
 	u64 reg = 0;
@@ -398,6 +682,11 @@ static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 	return reg;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|601| <<kvm_pmu_flush_hwstate>> kvm_pmu_update_state(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|613| <<kvm_pmu_sync_hwstate>> kvm_pmu_update_state(vcpu);
+ */
 static void kvm_pmu_update_state(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -410,15 +699,34 @@ static void kvm_pmu_update_state(struct kvm_vcpu *vcpu)
 	if (pmu->irq_level == overflow)
 		return;
 
+	/*
+	 * 在以下使用kvm_pmu->irq_level:
+	 *   - arch/arm64/kvm/pmu-emul.c|586| <<kvm_pmu_update_state>> if (pmu->irq_level == overflow)
+	 *   - arch/arm64/kvm/pmu-emul.c|589| <<kvm_pmu_update_state>> pmu->irq_level = overflow;
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_should_notify_user>> return pmu->irq_level != run_level;
+	 *   - arch/arm64/kvm/pmu-emul.c|635| <<kvm_pmu_update_run>> if (vcpu->arch.pmu.irq_level)
+	 */
 	pmu->irq_level = overflow;
 
 	if (likely(irqchip_in_kernel(vcpu->kvm))) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|406| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/arm.c|1130| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1138| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|557| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+		 */
 		int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
 					      pmu->irq_num, overflow, pmu);
 		WARN_ON(ret);
 	}
 }
 
+/*
+ * called by:
+ *  - arch/arm64/kvm/arm.c|805| <<kvm_vcpu_exit_request>> kvm_pmu_should_notify_user(vcpu)) {
+ */
 bool kvm_pmu_should_notify_user(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -434,6 +742,12 @@ bool kvm_pmu_should_notify_user(struct kvm_vcpu *vcpu)
 /*
  * Reflect the PMU overflow interrupt output level into the kvm_run structure
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1029| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_update_run(vcpu);
+ *
+ * 当unlikely(!irqchip_in_kernel(vcpu->kvm))的时候调用的
+ */
 void kvm_pmu_update_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_sync_regs *regs = &vcpu->run->s.regs;
@@ -451,8 +765,17 @@ void kvm_pmu_update_run(struct kvm_vcpu *vcpu)
  * Check if the PMU has overflowed while we were running in the host, and inject
  * an interrupt if that was the case.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|906| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_flush_hwstate(vcpu);
+ */
 void kvm_pmu_flush_hwstate(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|601| <<kvm_pmu_flush_hwstate>> kvm_pmu_update_state(vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|613| <<kvm_pmu_sync_hwstate>> kvm_pmu_update_state(vcpu);
+	 */
 	kvm_pmu_update_state(vcpu);
 }
 
@@ -463,6 +786,11 @@ void kvm_pmu_flush_hwstate(struct kvm_vcpu *vcpu)
  * Check if the PMU has overflowed while we were running in the guest, and
  * inject an interrupt if that was the case.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|925| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+ *   - arch/arm64/kvm/arm.c|958| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_sync_hwstate(vcpu);
+ */
 void kvm_pmu_sync_hwstate(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_update_state(vcpu);
@@ -473,6 +801,13 @@ void kvm_pmu_sync_hwstate(struct kvm_vcpu *vcpu)
  * to the event.
  * This is why we need a callback to do it once outside of the NMI context.
  */
+/*
+ * 在以下使用kvm_pmu->overflow_work:
+ *   - arch/arm64/kvm/pmu-emul.c|440| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+ *   - arch/arm64/kvm/pmu-emul.c|626| <<kvm_pmu_perf_overflow_notify_vcpu>> pmu = container_of(work, struct kvm_pmu, overflow_work);
+ *   - arch/arm64/kvm/pmu-emul.c|668| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+ *   - arch/arm64/kvm/pmu-emul.c|1198| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+ */
 static void kvm_pmu_perf_overflow_notify_vcpu(struct irq_work *work)
 {
 	struct kvm_vcpu *vcpu;
@@ -487,6 +822,12 @@ static void kvm_pmu_perf_overflow_notify_vcpu(struct irq_work *work)
 /**
  * When the perf event overflows, set the overflow status and inform the vcpu.
  */
+/*
+ * 在以下使用kvm_pmu_perf_overflow():
+ *   - arch/arm64/kvm/pmu-emul.c|909| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc + 1);
+ *   - arch/arm64/kvm/pmu-emul.c|932| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, pmc);
+ *   - arch/arm64/kvm/pmu-emul.c|1124| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_pmu_perf_overflow, &attr);
+ */
 static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 				  struct perf_sample_data *data,
 				  struct pt_regs *regs)
@@ -509,14 +850,27 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 		period &= GENMASK(31, 0);
 
 	local64_set(&perf_event->hw.period_left, 0);
+	/*
+	 * 这里填写的似乎是上一次overflow后的次数, 应该是user配置的
+	 */
 	perf_event->attr.sample_period = period;
 	perf_event->hw.sample_period = period;
 
+	/*
+	 * 在kvm_pmu_overflow_status()会检查
+	 */
 	__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(idx);
 
 	if (kvm_pmu_overflow_status(vcpu)) {
 		kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 
+		/*
+		 * 在以下使用kvm_pmu->overflow_work:
+		 *   - arch/arm64/kvm/pmu-emul.c|440| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|626| <<kvm_pmu_perf_overflow_notify_vcpu>> pmu = container_of(work, struct kvm_pmu, overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|668| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+		 *   - arch/arm64/kvm/pmu-emul.c|1198| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+		 */
 		if (!in_nmi())
 			kvm_vcpu_kick(vcpu);
 		else
@@ -531,6 +885,26 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
  * @vcpu: The vcpu pointer
  * @val: the value guest writes to PMSWINC register
  */
+/*
+ * Software Increment Register (PMSWINC)
+ * The Software Increment Register, PMSWINC, increments a counter that is
+ * configured to count the Software count event, event 0x00.
+ *
+ * The PMSWINC Register is:
+ *
+ * a 32-bit write-only CP15 register
+ *
+ * accessible in:
+ *
+ * - privileged modes
+ * - User mode only when the PMUSERENR.EN bit is set to 1
+ * - when the Security Extensions are implemented, a Common register
+ * - accessed using an MCR command with <CRn> set to c9, <opc1> set to 0, <CRm>
+ *   set to c12, and <opc2> set to 4.
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|946| <<access_pmswinc>> kvm_pmu_software_increment(vcpu, p->regval & mask);
+ */
 void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -557,6 +931,9 @@ void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
 		if (type != ARMV8_PMUV3_PERFCTR_SW_INCR)
 			continue;
 
+		/*
+		 * 在这里增加1 !!!!!
+		 */
 		/* increment this even SW_INC counter */
 		reg = __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) + 1;
 		reg = lower_32_bits(reg);
@@ -566,6 +943,9 @@ void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
 			continue;
 
 		if (kvm_pmu_pmc_is_chained(&pmu->pmc[i])) {
+			/*
+			 * 也在这里增加1 !!!!
+			 */
 			/* increment the high counter */
 			reg = __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i + 1) + 1;
 			reg = lower_32_bits(reg);
@@ -584,6 +964,13 @@ void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
  * @vcpu: The vcpu pointer
  * @val: the value guest writes to PMCR register
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|760| <<check_vcpu_requests(KVM_REQ_RELOAD_PMU)>> kvm_pmu_handle_pmcr(vcpu,
+ *   - arch/arm64/kvm/sys_regs.c|707| <<access_pmcr>> kvm_pmu_handle_pmcr(vcpu, val);
+ *
+ * Performance Monitor Control Register (PMCR)
+ */
 void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 {
 	int i;
@@ -602,6 +989,9 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	if (val & ARMV8_PMU_PMCR_C)
 		kvm_pmu_set_counter_value(vcpu, ARMV8_PMU_CYCLE_IDX, 0);
 
+	/*
+	 * Reset all counters
+	 */
 	if (val & ARMV8_PMU_PMCR_P) {
 		unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
 		mask &= ~BIT(ARMV8_PMU_CYCLE_IDX);
@@ -610,6 +1000,11 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|905| <<kvm_pmu_create_perf_event>> attr.disabled = !kvm_pmu_counter_is_enabled(vcpu, pmc->idx);
+ *   - arch/arm64/kvm/pmu-emul.c|994| <<kvm_pmu_update_pmc_chained>> kvm_pmu_counter_is_enabled(vcpu, pmc->idx | 0x1);
+ */
 static bool kvm_pmu_counter_is_enabled(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	return (__vcpu_sys_reg(vcpu, PMCR_EL0) & ARMV8_PMU_PMCR_E) &&
@@ -621,6 +1016,13 @@ static bool kvm_pmu_counter_is_enabled(struct kvm_vcpu *vcpu, u64 select_idx)
  * @vcpu: The vcpu pointer
  * @select_idx: The number of selected counter
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|314| <<kvm_pmu_set_counter_value>> kvm_pmu_create_perf_event(vcpu, select_idx);
+ *   - arch/arm64/kvm/pmu-emul.c|506| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|546| <<kvm_pmu_disable_counter_mask>> kvm_pmu_create_perf_event(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|1030| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(vcpu, select_idx);
+ */
 static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	struct arm_pmu *arm_pmu = vcpu->kvm->arch.arm_pmu;
@@ -635,18 +1037,35 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
 	 * obtained from the low/even counter. We also use this counter to
 	 * determine if the event is enabled/disabled.
 	 */
+	/*
+	 * When a pair of PMCs are chained together we use the low counter (canonical)
+	 * to hold the underlying perf event.
+	 */
 	pmc = kvm_pmu_get_canonical_pmc(&pmu->pmc[select_idx]);
 
 	reg = (pmc->idx == ARMV8_PMU_CYCLE_IDX)
 	      ? PMCCFILTR_EL0 : PMEVTYPER0_EL0 + pmc->idx;
 	data = __vcpu_sys_reg(vcpu, reg);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|297| <<kvm_pmu_vcpu_reset>> kvm_pmu_stop_counter(vcpu, &pmu->pmc[i]);
+	 *   - arch/arm64/kvm/pmu-emul.c|668| <<kvm_pmu_create_perf_event>> kvm_pmu_stop_counter(vcpu, pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|779| <<kvm_pmu_update_pmc_chained>> kvm_pmu_stop_counter(vcpu, canonical_pmc);
+	 *   - arch/arm64/kvm/pmu-emul.c|785| <<kvm_pmu_update_pmc_chained>> kvm_pmu_stop_counter(vcpu, kvm_pmu_get_alternate_pmc(pmc));
+	 */
 	kvm_pmu_stop_counter(vcpu, pmc);
 	if (pmc->idx == ARMV8_PMU_CYCLE_IDX)
 		eventsel = ARMV8_PMUV3_PERFCTR_CPU_CYCLES;
 	else
 		eventsel = data & kvm_pmu_event_mask(vcpu->kvm);
 
+	/*
+	 * 在以下使用ARMV8_PMUV3_PERFCTR_SW_INCR:
+	 *   - arch/arm64/include/asm/perf_event.h|18| <<global>> #define ARMV8_PMUV3_PERFCTR_SW_INCR 0x0000
+	 *   - arch/arm64/kvm/pmu-emul.c|781| <<kvm_pmu_software_increment>> if (type != ARMV8_PMUV3_PERFCTR_SW_INCR)
+	 *   - arch/arm64/kvm/pmu-emul.c|890| <<kvm_pmu_create_perf_event>> if (eventsel == ARMV8_PMUV3_PERFCTR_SW_INCR)
+	 */
 	/* Software increment event doesn't need to be backed by a perf event */
 	if (eventsel == ARMV8_PMUV3_PERFCTR_SW_INCR)
 		return;
@@ -659,6 +1078,9 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
 	    !test_bit(eventsel, vcpu->kvm->arch.pmu_filter))
 		return;
 
+	/*
+	 * struct perf_event_attr attr;
+	 */
 	memset(&attr, 0, sizeof(struct perf_event_attr));
 	attr.type = arm_pmu->pmu.type;
 	attr.size = sizeof(attr);
@@ -679,8 +1101,29 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
 		 * high counter.
 		 */
 		attr.sample_period = (-counter) & GENMASK(63, 0);
+		/*
+		 *   - arch/arm64/kernel/perf_event.c|304| <<armv8pmu_event_is_64bit>> return event->attr.config1 & 0x1;
+		 *   - arch/arm64/kernel/perf_event.c|309| <<armv8pmu_event_want_user_access>> return event->attr.config1 & 0x2;
+		 *   - arch/arm64/kvm/pmu-emul.c|1048| <<kvm_pmu_create_perf_event>> attr.config1 |= PERF_ATTR_CFG1_KVM_PMU_CHAINED;
+		 */
 		attr.config1 |= PERF_ATTR_CFG1_KVM_PMU_CHAINED;
 
+		/*
+		 * 似乎都是在下面触发的:
+		 *   - kernel/events/core.c|9407| <<__perf_event_overflow>> READ_ONCE(event->overflow_handler)(event, data, regs);
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+		 *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+		 *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+		 *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+		 *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+		 */
 		event = perf_event_create_kernel_counter(&attr, -1, current,
 							 kvm_pmu_perf_overflow,
 							 pmc + 1);
@@ -691,6 +1134,22 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
 		else
 			attr.sample_period = (-counter) & GENMASK(31, 0);
 
+		/*
+		 * 似乎都是在下面触发的:
+		 *   - kernel/events/core.c|9407| <<__perf_event_overflow>> READ_ONCE(event->overflow_handler)(event, data, regs);
+		 *
+		 * called by:
+		 *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+		 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+		 *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+		 *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+		 *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+		 *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+		 */
 		event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_pmu_perf_overflow, pmc);
 	}
@@ -712,6 +1171,12 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
  * Update the chained bitmap based on the event type written in the
  * typer register and the enable state of the odd register.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|354| <<kvm_pmu_enable_counter_mask>> kvm_pmu_update_pmc_chained(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|389| <<kvm_pmu_disable_counter_mask>> kvm_pmu_update_pmc_chained(vcpu, i);
+ *   - arch/arm64/kvm/pmu-emul.c|823| <<kvm_pmu_set_counter_event_type>> kvm_pmu_update_pmc_chained(vcpu, select_idx);
+ */
 static void kvm_pmu_update_pmc_chained(struct kvm_vcpu *vcpu, u64 select_idx)
 {
 	struct kvm_pmu *pmu = &vcpu->arch.pmu;
@@ -749,6 +1214,10 @@ static void kvm_pmu_update_pmc_chained(struct kvm_vcpu *vcpu, u64 select_idx)
  * event with given hardware event number. Here we call perf_event API to
  * emulate this action and create a kernel perf event for it.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|850| <<access_pmu_evtyper>> kvm_pmu_set_counter_event_type(vcpu, p->regval, idx);
+ */
 void kvm_pmu_set_counter_event_type(struct kvm_vcpu *vcpu, u64 data,
 				    u64 select_idx)
 {
@@ -767,9 +1236,24 @@ void kvm_pmu_set_counter_event_type(struct kvm_vcpu *vcpu, u64 data,
 	__vcpu_sys_reg(vcpu, reg) = data & mask;
 
 	kvm_pmu_update_pmc_chained(vcpu, select_idx);
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|314| <<kvm_pmu_set_counter_value>> kvm_pmu_create_perf_event(vcpu, select_idx);
+	 *   - arch/arm64/kvm/pmu-emul.c|506| <<kvm_pmu_enable_counter_mask>> kvm_pmu_create_perf_event(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|546| <<kvm_pmu_disable_counter_mask>> kvm_pmu_create_perf_event(vcpu, i);
+	 *   - arch/arm64/kvm/pmu-emul.c|1030| <<kvm_pmu_set_counter_event_type>> kvm_pmu_create_perf_event(vcpu, select_idx);
+	 */
 	kvm_pmu_create_perf_event(vcpu, select_idx);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu.c|955| <<armpmu_register>> kvm_host_pmu_init(pmu);
+ *
+ * arm_pmu_acpi_probe() or arm_pmu_device_probe()
+ * -> armpmu_register()
+ *    -> kvm_host_pmu_init()
+ */
 void kvm_host_pmu_init(struct arm_pmu *pmu)
 {
 	struct arm_pmu_entry *entry;
@@ -777,15 +1261,47 @@ void kvm_host_pmu_init(struct arm_pmu *pmu)
 	if (pmu->pmuver == 0 || pmu->pmuver == ID_AA64DFR0_PMUVER_IMP_DEF)
 		return;
 
+	/*
+	 * 在以下使用arm_pmus_lock:
+	 *   - arch/arm64/kvm/pmu-emul.c|21| <<global>> static DEFINE_MUTEX(arm_pmus_lock);
+	 *   - arch/arm64/kvm/pmu-emul.c|838| <<kvm_host_pmu_init>> mutex_lock(&arm_pmus_lock);
+	 *   - arch/arm64/kvm/pmu-emul.c|851| <<kvm_host_pmu_init>> mutex_unlock(&arm_pmus_lock);
+	 *   - arch/arm64/kvm/pmu-emul.c|1066| <<kvm_arm_pmu_v3_set_pmu>> mutex_lock(&arm_pmus_lock);
+	 *   - arch/arm64/kvm/pmu-emul.c|1084| <<kvm_arm_pmu_v3_set_pmu>> mutex_unlock(&arm_pmus_lock);
+	 */
 	mutex_lock(&arm_pmus_lock);
 
+	/*
+	 * struct arm_pmu_entry {
+	 *     struct list_head entry;
+	 *     struct arm_pmu *arm_pmu;
+	 * };
+	 */
 	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
 	if (!entry)
 		goto out_unlock;
 
+	/*
+	 * 在以下使用arm_pmus:
+	 *   - arch/arm64/kvm/pmu-emul.c|26| <<global>> static LIST_HEAD(arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|845| <<kvm_host_pmu_init>> list_add_tail(&entry->entry, &arm_pmus);
+	 *   - arch/arm64/kvm/pmu-emul.c|847| <<kvm_host_pmu_init>> if (list_is_singular(&arm_pmus))
+	 *   - arch/arm64/kvm/pmu-emul.c|1068| <<kvm_arm_pmu_v3_set_pmu>> list_for_each_entry(entry, &arm_pmus, entry) {
+	 */
 	entry->arm_pmu = pmu;
 	list_add_tail(&entry->entry, &arm_pmus);
 
+	/*
+	 * 在以下使用kvm_arm_pmu_available:
+	 *   - arch/arm64/kernel/image-vars.h|116| <<global>> KVM_NVHE_ALIAS(kvm_arm_pmu_available);
+	 *   - arch/arm64/kvm/pmu-emul.c|18| <<global>> DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+	 *   - include/kvm/arm_pmu.h|43| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+	 *   - arch/arm64/kvm/pmu-emul.c|848| <<kvm_host_pmu_init>> static_branch_enable(&kvm_arm_pmu_available);
+	 *   - include/kvm/arm_pmu.h|47| <<kvm_arm_support_pmu_v3>> return static_branch_likely(&kvm_arm_pmu_available);
+	 *
+	 * list_is_singular - tests whether a list has just one entry.
+	 * 所以只enable一次
+	 */
 	if (list_is_singular(&arm_pmus))
 		static_branch_enable(&kvm_arm_pmu_available);
 
@@ -793,6 +1309,10 @@ void kvm_host_pmu_init(struct arm_pmu *pmu)
 	mutex_unlock(&arm_pmus_lock);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1059| <<kvm_arm_pmu_v3_set_attr>> kvm->arch.arm_pmu = kvm_pmu_probe_armpmu();
+ */
 static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 {
 	struct perf_event_attr attr = { };
@@ -816,6 +1336,22 @@ static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 	attr.config = ARMV8_PMUV3_PERFCTR_CPU_CYCLES;
 	attr.sample_period = GENMASK(63, 0);
 
+	/*
+	 * 似乎都是在下面触发的:
+	 *   - kernel/events/core.c|9407| <<__perf_event_overflow>> READ_ONCE(event->overflow_handler)(event, data, regs);
+	 *
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+	 *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+	 *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+	 *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+	 *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_pmu_perf_overflow, &attr);
 
@@ -838,6 +1374,10 @@ static struct arm_pmu *kvm_pmu_probe_armpmu(void)
 	return pmu;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|747| <<access_pmceid>> pmceid = kvm_pmu_get_pmceid(vcpu, (p->Op2 & 1));
+ */
 u64 kvm_pmu_get_pmceid(struct kvm_vcpu *vcpu, bool pmceid1)
 {
 	unsigned long *bmap = vcpu->kvm->arch.pmu_filter;
@@ -880,6 +1420,14 @@ u64 kvm_pmu_get_pmceid(struct kvm_vcpu *vcpu, bool pmceid1)
 	return val & mask;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|583| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_arm_pmu_v3_enable(vcpu);
+ *
+ * kvm_vcpu_ioctl(KVM_RUN)
+ * -> kvm_arch_vcpu_run_pid_change()
+ *    -> kvm_arm_pmu_v3_enable()
+ */
 int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_vcpu_has_pmu(vcpu))
@@ -913,6 +1461,16 @@ int kvm_arm_pmu_v3_enable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1201| <<kvm_arm_pmu_v3_set_attr>> return kvm_arm_pmu_v3_init(vcpu);
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)
+ * -> kvm_arm_vcpu_set_attr()
+ *    -> kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ *       -> kvm_arm_pmu_v3_set_attr()
+ *          -> kvm_arm_pmu_v3_init()
+ */
 static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
 {
 	if (irqchip_in_kernel(vcpu->kvm)) {
@@ -947,6 +1505,16 @@ static int kvm_arm_pmu_v3_init(struct kvm_vcpu *vcpu)
  * As a PPI, the interrupt number is the same for all vcpus,
  * while as an SPI it must be a separate number per vcpu.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1467| <<kvm_arm_pmu_v3_set_attr>> if (!pmu_irq_is_valid(kvm, irq))
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)
+ * -> kvm_arm_vcpu_set_attr()
+ *    -> kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ *       -> kvm_arm_pmu_v3_set_attr()
+ *          -> pmu_irq_is_valid()
+ */
 static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
 {
 	unsigned long i;
@@ -968,6 +1536,16 @@ static bool pmu_irq_is_valid(struct kvm *kvm, int irq)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|1151| <<kvm_arm_pmu_v3_set_attr>> return kvm_arm_pmu_v3_set_pmu(vcpu, pmu_id);
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)
+ * -> kvm_arm_vcpu_set_attr()
+ *    -> kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ *       -> kvm_arm_pmu_v3_set_attr()
+ *          -> kvm_arm_pmu_v3_set_pmu()
+ */
 static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -999,6 +1577,15 @@ static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|953| <<kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)>> ret = kvm_arm_pmu_v3_set_attr(vcpu, attr);
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_DEVICE_ATTR)
+ * -> kvm_arm_vcpu_set_attr()
+ *    -> kvm_arm_vcpu_arch_set_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ *       -> kvm_arm_pmu_v3_set_attr()
+ */
 int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -1113,6 +1700,15 @@ int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|976| <<kvm_arm_vcpu_arch_get_attr(KVM_ARM_VCPU_PMU_V3_CTRL)>> ret = kvm_arm_pmu_v3_get_attr(vcpu, attr);
+ *
+ * kvm_arch_vcpu_ioctl(KVM_GET_DEVICE_ATTR)
+ * -> kvm_arm_vcpu_get_attr()
+ *    -> kvm_arm_vcpu_arch_get_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ *       -> kvm_arm_pmu_v3_get_attr()
+ */
 int kvm_arm_pmu_v3_get_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	switch (attr->attr) {
@@ -1137,6 +1733,15 @@ int kvm_arm_pmu_v3_get_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|999| <<kvm_arm_vcpu_arch_has_attr(KVM_ARM_VCPU_PMU_V3_CTRL)>> ret = kvm_arm_pmu_v3_has_attr(vcpu, attr);
+ *
+ * kvm_arch_vcpu_ioctl(KVM_HAS_DEVICE_ATTR)
+ * -> kvm_arm_vcpu_has_attr()
+ *    -> kvm_arm_vcpu_arch_has_attr(KVM_ARM_VCPU_PMU_V3_CTRL)
+ *       -> kvm_arm_pmu_v3_has_attr()
+ */
 int kvm_arm_pmu_v3_has_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 {
 	switch (attr->attr) {
diff --git a/arch/arm64/kvm/pmu.c b/arch/arm64/kvm/pmu.c
index 7887133d1..383e90c68 100644
--- a/arch/arm64/kvm/pmu.c
+++ b/arch/arm64/kvm/pmu.c
@@ -6,12 +6,21 @@
 #include <linux/kvm_host.h>
 #include <linux/perf_event.h>
 
+/*
+ * 在以下使用kvm_pmu_events:
+ *   - arch/arm64/kvm/pmu.c|9| <<global>> static DEFINE_PER_CPU(struct kvm_pmu_events, kvm_pmu_events);
+ *   - arch/arm64/kvm/pmu.c|43| <<kvm_get_pmu_events>> return this_cpu_ptr(&kvm_pmu_events);
+ */
 static DEFINE_PER_CPU(struct kvm_pmu_events, kvm_pmu_events);
 
 /*
  * Given the perf event attributes and system type, determine
  * if we are going to need to switch counters at guest entry/exit.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu.c|46| <<kvm_set_pmu_events>> if (!kvm_arm_support_pmu_v3() || !pmu || !kvm_pmu_switch_needed(attr))
+ */
 static bool kvm_pmu_switch_needed(struct perf_event_attr *attr)
 {
 	/**
@@ -26,6 +35,14 @@ static bool kvm_pmu_switch_needed(struct perf_event_attr *attr)
 	return (attr->exclude_host != attr->exclude_guest);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu.c|44| <<kvm_set_pmu_events>> struct kvm_pmu_events *pmu = kvm_get_pmu_events();
+ *   - arch/arm64/kvm/pmu.c|60| <<kvm_clr_pmu_events>> struct kvm_pmu_events *pmu = kvm_get_pmu_events();
+ *   - arch/arm64/kvm/pmu.c|196| <<kvm_vcpu_pmu_restore_guest>> pmu = kvm_get_pmu_events();
+ *   - arch/arm64/kvm/pmu.c|220| <<kvm_vcpu_pmu_restore_host>> pmu = kvm_get_pmu_events();
+ *   - include/kvm/arm_pmu.h|91| <<kvm_pmu_update_vcpu_events>> vcpu->arch.pmu.events = *kvm_get_pmu_events(); \
+ */
 struct kvm_pmu_events *kvm_get_pmu_events(void)
 {
 	return this_cpu_ptr(&kvm_pmu_events);
@@ -35,6 +52,10 @@ struct kvm_pmu_events *kvm_get_pmu_events(void)
  * Add events to track that we may want to switch at guest entry/exit
  * time.
  */
+/*
+ * called by:
+ *   - arch/arm64/kernel/perf_event.c|675| <<armv8pmu_enable_event_counter>> kvm_set_pmu_events(mask, attr);
+ */
 void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr)
 {
 	struct kvm_pmu_events *pmu = kvm_get_pmu_events();
@@ -51,6 +72,11 @@ void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr)
 /*
  * Stop tracking events
  */
+/*
+ * called by:
+ *   - arch/arm64/kernel/perf_event.c|706| <<armv8pmu_disable_event_counter>> kvm_clr_pmu_events(mask);
+ *   - arch/arm64/kernel/perf_event.c|1047| <<armv8pmu_reset>> kvm_clr_pmu_events(U32_MAX);
+ */
 void kvm_clr_pmu_events(u32 clr)
 {
 	struct kvm_pmu_events *pmu = kvm_get_pmu_events();
@@ -125,6 +151,11 @@ static u64 kvm_vcpu_pmu_read_evtype_direct(int idx)
  * Write a value direct to PMEVTYPER<idx> where idx is 0-30
  * or PMCCFILTR_EL0 where idx is ARMV8_PMU_CYCLE_IDX (31).
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu.c|155| <<kvm_vcpu_pmu_enable_el0>> kvm_vcpu_pmu_write_evtype_direct(counter, typer);
+ *   - arch/arm64/kvm/pmu.c|170| <<kvm_vcpu_pmu_disable_el0>> kvm_vcpu_pmu_write_evtype_direct(counter, typer);
+ */
 static void kvm_vcpu_pmu_write_evtype_direct(int idx, u32 val)
 {
 	switch (idx) {
@@ -173,6 +204,13 @@ static void kvm_vcpu_pmu_disable_el0(unsigned long events)
  * Since the latter is preemptible, special care must be taken to
  * disable preemption.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|427| <<kvm_arch_vcpu_load>> kvm_vcpu_pmu_restore_guest(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|708| <<access_pmcr>> kvm_vcpu_pmu_restore_guest(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|852| <<access_pmu_evtyper>> kvm_vcpu_pmu_restore_guest(vcpu);
+ *   - arch/arm64/kvm/sys_regs.c|875| <<access_pmcnten>> kvm_vcpu_pmu_restore_guest(vcpu);
+ */
 void kvm_vcpu_pmu_restore_guest(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu_events *pmu;
@@ -194,6 +232,10 @@ void kvm_vcpu_pmu_restore_guest(struct kvm_vcpu *vcpu)
 /*
  * On VHE ensure that only host events have EL0 counting enabled
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|452| <<kvm_arch_vcpu_put>> kvm_vcpu_pmu_restore_host(vcpu);
+ */
 void kvm_vcpu_pmu_restore_host(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu_events *pmu;
diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 0e08fbe68..3d5f4e578 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -246,6 +246,12 @@ static int kvm_set_vm_width(struct kvm_vcpu *vcpu)
  * disable preemption around the vcpu reset as we would otherwise race with
  * preempt notifiers which also call put/load.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|740| <<check_vcpu_requests>> kvm_reset_vcpu(vcpu);
+ *   - arch/arm64/kvm/arm.c|1182| <<kvm_vcpu_set_target>> ret = kvm_reset_vcpu(vcpu);
+ *   - arch/arm64/kvm/arm.c|1347| <<kvm_arch_vcpu_ioctl>> kvm_reset_vcpu(vcpu);
+ */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_reset_state reset_state;
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 3234f50b8..a8625f472 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -194,6 +194,15 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
  * The cp15_64 code makes sure this automatically works
  * for both AArch64 and AArch32 accesses.
  */
+/*
+ * 在以下使用access_gic_sgi():
+ *   - arch/arm64/kvm/sys_regs.c|1623| <<global>> { SYS_DESC(SYS_ICC_SGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|1624| <<global>> { SYS_DESC(SYS_ICC_ASGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|1625| <<global>> { SYS_DESC(SYS_ICC_SGI0R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2147| <<global>> { Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2149| <<global>> { Op1( 1), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2150| <<global>> { Op1( 2), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ */
 static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
@@ -688,6 +697,14 @@ static bool pmu_access_event_counter_el0_disabled(struct kvm_vcpu *vcpu)
 	return check_pmu_access_disabled(vcpu, ARMV8_PMU_USERENR_ER | ARMV8_PMU_USERENR_EN);
 }
 
+/*
+ * Performance Monitor Control Register (PMCR)
+ */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1645| <<global>> { PMU_SYS_REG(SYS_PMCR_EL0), .access = access_pmcr,
+ *   - arch/arm64/kvm/sys_regs.c|2034| <<global>> { CP15_PMU_SYS_REG(DIRECT, 0, 9, 12, 0), .access = access_pmcr },
+ */
 static bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 			const struct sys_reg_desc *r)
 {
@@ -704,6 +721,11 @@ static bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 		if (!kvm_supports_32bit_el0())
 			val |= ARMV8_PMU_PMCR_LC;
 		__vcpu_sys_reg(vcpu, PMCR_EL0) = val;
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arm.c|760| <<check_vcpu_requests(KVM_REQ_RELOAD_PMU)>> kvm_pmu_handle_pmcr(vcpu,
+		 *   - arch/arm64/kvm/sys_regs.c|707| <<access_pmcr>> kvm_pmu_handle_pmcr(vcpu, val);
+		 */
 		kvm_pmu_handle_pmcr(vcpu, val);
 		kvm_vcpu_pmu_restore_guest(vcpu);
 	} else {
@@ -716,6 +738,10 @@ static bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 	return true;
 }
 
+/*
+ * The Event Counter Selection Register, PMSELR, selects the current event
+ * counter, PMNx. When a particular event counter is selected:
+ */
 static bool access_pmselr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 			  const struct sys_reg_desc *r)
 {
@@ -942,6 +968,15 @@ static bool access_pmswinc(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 	if (pmu_write_swinc_el0_disabled(vcpu))
 		return false;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|418| <<kvm_pmu_vcpu_reset>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|761| <<kvm_pmu_handle_pmcr>> unsigned long mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|868| <<access_pmcnten>> mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|891| <<access_pminten>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|915| <<access_pmovs>> u64 mask = kvm_pmu_valid_counter_mask(vcpu);
+	 *   - arch/arm64/kvm/sys_regs.c|945| <<access_pmswinc>> mask = kvm_pmu_valid_counter_mask(vcpu);
+	 */
 	mask = kvm_pmu_valid_counter_mask(vcpu);
 	kvm_pmu_software_increment(vcpu, p->regval & mask);
 	return true;
@@ -1402,6 +1437,18 @@ static unsigned int mte_visibility(const struct kvm_vcpu *vcpu,
  * This should be revisited if we ever encounter a more demanding
  * guest...
  */
+/*
+ * 在以下使用sys_reg_descs[]:
+ *   - arch/arm64/kvm/sys_regs.c|2506| <<emulate_sys_reg>> r = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|2535| <<kvm_reset_sys_regs>> for (i = 0; i < ARRAY_SIZE(sys_reg_descs); i++)
+ *   - arch/arm64/kvm/sys_regs.c|2536| <<kvm_reset_sys_regs>> if (sys_reg_descs[i].reset)
+ *   - arch/arm64/kvm/sys_regs.c|2537| <<kvm_reset_sys_regs>> sys_reg_descs[i].reset(vcpu, &sys_reg_descs[i]);
+ *   - arch/arm64/kvm/sys_regs.c|2815| <<kvm_arm_sys_reg_get_reg>> sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|2856| <<kvm_arm_sys_reg_set_reg>> sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|2938| <<walk_sys_regs>> i2 = sys_reg_descs;
+ *   - arch/arm64/kvm/sys_regs.c|2939| <<walk_sys_regs>> end2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);
+ *   - arch/arm64/kvm/sys_regs.c|2983| <<kvm_sys_reg_table_init>> valid &= check_sysreg_table(sys_reg_descs, ARRAY_SIZE(sys_reg_descs), false);
+ */
 static const struct sys_reg_desc sys_reg_descs[] = {
 	{ SYS_DESC(SYS_DC_ISW), access_dcsw },
 	{ SYS_DESC(SYS_DC_CSW), access_dcsw },
@@ -2477,6 +2524,12 @@ static bool is_imp_def_sys_reg(struct sys_reg_params *params)
  *
  * Return: true if the system register access was successful, false otherwise.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|2377| <<kvm_handle_cp10_id>> if (emulate_sys_reg(vcpu, &params))
+ *   - arch/arm64/kvm/sys_regs.c|2420| <<kvm_emulate_cp15_id_reg>> else if (!emulate_sys_reg(vcpu, params))
+ *   - arch/arm64/kvm/sys_regs.c|2555| <<kvm_handle_sys_reg>> if (!emulate_sys_reg(vcpu, &params))
+ */
 static bool emulate_sys_reg(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *params)
 {
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index 91201f743..e2e87aca6 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -1055,6 +1055,10 @@ static int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)
  * check for matching ones. If this bit is set, we signal all, but not the
  * calling VCPU.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|237| <<access_gic_sgi>> vgic_v3_dispatch_sgi(vcpu, p->regval, g1);
+ */
 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/arm64/kvm/vgic/vgic-v3.c b/arch/arm64/kvm/vgic/vgic-v3.c
index 826ff6f2a..ba222d3d5 100644
--- a/arch/arm64/kvm/vgic/vgic-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-v3.c
@@ -720,6 +720,10 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|931| <<kvm_vgic_load>> vgic_v3_load(vcpu);
+ */
 void vgic_v3_load(struct kvm_vcpu *vcpu)
 {
 	struct vgic_v3_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v3;
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index d97e6080b..1b636cce3 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -434,6 +434,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|406| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/arm.c|1130| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1138| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|557| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 			bool level, void *owner)
 {
@@ -887,6 +895,10 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 }
 
 /* Flush our emulation state into the GIC hardware before entering the guest. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|913| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
 	/*
diff --git a/arch/arm64/kvm/vmid.c b/arch/arm64/kvm/vmid.c
index d78ae63d7..2db12bed5 100644
--- a/arch/arm64/kvm/vmid.c
+++ b/arch/arm64/kvm/vmid.c
@@ -42,6 +42,10 @@ static DEFINE_PER_CPU(u64, reserved_vmids);
 #define vmid_gen_match(vmid) \
 	(!(((vmid) ^ atomic64_read(&vmid_generation)) >> kvm_arm_vmid_bits))
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vmid.c|119| <<new_vmid>> flush_context();
+ */
 static void flush_context(void)
 {
 	int cpu;
diff --git a/arch/arm64/mm/context.c b/arch/arm64/mm/context.c
index b8b4cf0bc..114b5d189 100644
--- a/arch/arm64/mm/context.c
+++ b/arch/arm64/mm/context.c
@@ -101,6 +101,10 @@ static void set_reserved_asid_bits(void)
 #define asid_gen_match(asid) \
 	(!(((asid) ^ atomic64_read(&asid_generation)) >> asid_bits))
 
+/*
+ * called by:
+ *   - arch/arm64/mm/context.c|204| <<new_context>> flush_context();
+ */
 static void flush_context(void)
 {
 	int i;
diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index 9ac371841..4e67b1f0e 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -14,6 +14,11 @@
 #include "../perf_event.h"
 
 static DEFINE_PER_CPU(unsigned long, perf_nmi_tstamp);
+/*
+ * 在以下使用static的perf_nmi_window:
+ *   - arch/x86/events/amd/core.c|881| <<amd_pmu_adjust_nmi_window>> this_cpu_write(perf_nmi_tstamp, jiffies + perf_nmi_window);
+ *   - arch/x86/events/amd/core.c|1375| <<amd_core_pmu_init>> perf_nmi_window = msecs_to_jiffies(100);
+ */
 static unsigned long perf_nmi_window;
 
 /* AMD Event 0xFFF: Merge.  Used with Large Increment per Cycle events */
@@ -21,6 +26,14 @@ static unsigned long perf_nmi_window;
 #define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)
 
 /* PMC Enable and Overflow bits for PerfCntrGlobal* registers */
+/*
+ * 在以下使用amd_pmu_global_cntr_mask:
+ *   - arch/x86/events/amd/core.c|594| <<amd_pmu_cpu_reset>> wrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR, amd_pmu_global_cntr_mask);
+ *   - arch/x86/events/amd/core.c|680| <<amd_pmu_get_global_status>> return status & amd_pmu_global_cntr_mask;
+ *   - arch/x86/events/amd/core.c|692| <<amd_pmu_ack_global_status>> status &= amd_pmu_global_cntr_mask;
+ *   - arch/x86/events/amd/core.c|804| <<amd_pmu_v2_enable_all>> amd_pmu_set_global_ctl(amd_pmu_global_cntr_mask);
+ *   - arch/x86/events/amd/core.c|1406| <<amd_core_pmu_init>> amd_pmu_global_cntr_mask = (1ULL << x86_pmu.num_counters) - 1;
+ */
 static u64 amd_pmu_global_cntr_mask __read_mostly;
 
 static __initconst const u64 amd_hw_cache_event_ids
@@ -665,6 +678,11 @@ static void amd_pmu_cpu_dead(int cpu)
 	amd_pmu_cpu_reset(cpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|804| <<amd_pmu_v2_enable_all>> amd_pmu_set_global_ctl(amd_pmu_global_cntr_mask);
+ *   - arch/x86/events/amd/core.c|840| <<amd_pmu_v2_disable_all>> amd_pmu_set_global_ctl(0);
+ */
 static inline void amd_pmu_set_global_ctl(u64 ctl)
 {
 	wrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_CTL, ctl);
@@ -828,6 +846,12 @@ static void amd_pmu_disable_all(void)
 	amd_pmu_check_overflow();
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|938| <<amd_pmu_v2_handle_irq>> amd_pmu_v2_disable_all();
+ *   - arch/x86/events/amd/core.c|1398| <<amd_core_pmu_init>> x86_pmu.disable_all = amd_pmu_v2_disable_all;
+ *   - arch/x86/events/amd/core.c|1513| <<amd_pmu_reload_virt>> amd_pmu_v2_disable_all();
+ */
 static void amd_pmu_v2_disable_all(void)
 {
 	/* Disable all PMCs */
@@ -913,6 +937,10 @@ static int amd_pmu_handle_irq(struct pt_regs *regs)
 	return amd_pmu_adjust_nmi_window(handled);
 }
 
+/*
+ * 在以下使用amd_pmu_v2_handle_irq():
+ *   - arch/x86/events/amd/core.c|1396| <<amd_core_pmu_init>> x86_pmu.handle_irq = amd_pmu_v2_handle_irq;
+ */
 static int amd_pmu_v2_handle_irq(struct pt_regs *regs)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -1348,18 +1376,32 @@ static const struct attribute_group *amd_attr_update[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|1455| <<amd_pmu_init>> ret = amd_core_pmu_init();
+ */
 static int __init amd_core_pmu_init(void)
 {
 	union cpuid_0x80000022_ebx ebx;
 	u64 even_ctr_mask = 0ULL;
 	int i;
 
+	/*
+	 * // More extended AMD flags: CPUID level 0x80000001, ECX, word 6
+	 * #define X86_FEATURE_PERFCTR_CORE        ( 6*32+23) // Core performance counter extensions
+	 */
 	if (!boot_cpu_has(X86_FEATURE_PERFCTR_CORE))
 		return 0;
 
 	/* Avoid calculating the value each time in the NMI handler */
 	perf_nmi_window = msecs_to_jiffies(100);
 
+	/*
+	 * #define AMD64_NUM_COUNTERS                              4
+	 * #define AMD64_NUM_COUNTERS_CORE                         6
+	 * #define AMD64_NUM_COUNTERS_NB                           4
+	 */
+
 	/*
 	 * If core performance counter extensions exists, we must use
 	 * MSR_F15H_PERF_CTL/MSR_F15H_PERF_CTR msrs. See also
@@ -1371,6 +1413,25 @@ static int __init amd_core_pmu_init(void)
 
 	/* Check for Performance Monitoring v2 support */
 	if (boot_cpu_has(X86_FEATURE_PERFMON_V2)) {
+		/*
+		 * CPUID Fn8000_0022_EAX[PerfMonV2] = 1 支持下面的:
+		 *
+		 * - PerfCntGlobalCtl MSR
+		 * - PerfCntGlobalStatus MSR
+		 * - PerfCntGlobalStatusClr MSR
+		 * - PerfCntGlobalStatusSet MSR
+		 *
+		 * #define MSR_AMD64_PERF_CNTR_GLOBAL_STATUS       0xc0000300
+		 * #define MSR_AMD64_PERF_CNTR_GLOBAL_CTL          0xc0000301
+		 * #define MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR   0xc0000302
+		 */
+		/*
+		 * AMD Extended Performance Monitoring and Debug cpuid feature detection
+		 * #define EXT_PERFMON_DEBUG_FEATURES 0x80000022
+		 */
+		/*
+		 * 返回的是ebx的值
+		 */
 		ebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);
 
 		/* Update PMU version for later usage */
@@ -1442,14 +1503,32 @@ static int __init amd_core_pmu_init(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2105| <<init_hw_perf_events>> err = amd_pmu_init();
+ *   - arch/x86/events/core.c|2108| <<init_hw_perf_events>> err = amd_pmu_init();
+ */
 __init int amd_pmu_init(void)
 {
 	int ret;
 
 	/* Performance-monitoring supported from K7 and later: */
+	/*
+	 * 应该是在以下获得x86:
+	 * 1929 void __init identify_boot_cpu(void)
+	 * 1930 {
+	 * 1931         identify_cpu(&boot_cpu_data);
+	 */
 	if (boot_cpu_data.x86 < 6)
 		return -ENODEV;
 
+	/*
+	 * static __initconst const struct x86_pmu amd_pmu = {
+	 *     .name                   = "AMD",
+	 *     .eventsel               = MSR_K7_EVNTSEL0,
+	 *     .perfctr                = MSR_K7_PERFCTR0,
+	 *     .num_counters           = AMD64_NUM_COUNTERS,
+	 */
 	x86_pmu = amd_pmu;
 
 	ret = amd_core_pmu_init();
diff --git a/arch/x86/events/amd/ibs.c b/arch/x86/events/amd/ibs.c
index c251bc44c..f39459ba7 100644
--- a/arch/x86/events/amd/ibs.c
+++ b/arch/x86/events/amd/ibs.c
@@ -688,6 +688,11 @@ static struct perf_ibs perf_ibs_op = {
 	.get_count		= get_ibs_op_count,
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|833| <<perf_ibs_nmi_handler>> handled += perf_ibs_handle_irq(&perf_ibs_fetch, regs);
+ *   - arch/x86/events/amd/ibs.c|834| <<perf_ibs_nmi_handler>> handled += perf_ibs_handle_irq(&perf_ibs_op, regs);
+ */
 static int perf_ibs_handle_irq(struct perf_ibs *perf_ibs, struct pt_regs *iregs)
 {
 	struct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index f969410d0..1d43f3205 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -106,6 +106,18 @@ u64 __read_mostly hw_cache_extra_regs
  * Can only be executed on the CPU where the event is active.
  * Returns the delta events processed.
  */
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|948| <<amd_pmu_v2_handle_irq>> x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|1623| <<x86_pmu_stop>> x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|1713| <<x86_pmu_handle_irq>> val = x86_perf_event_update(event);
+ *   - arch/x86/events/core.c|2055| <<_x86_pmu_read>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2314| <<intel_pmu_nhm_workaround>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2705| <<intel_pmu_read_event>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/core.c|2806| <<intel_pmu_save_and_restart>> x86_perf_event_update(event);
+ *   - arch/x86/events/intel/p4.c|1038| <<p4_pmu_handle_irq>> val = x86_perf_event_update(event);
+ *   - arch/x86/events/zhaoxin/core.c|394| <<zhaoxin_pmu_handle_irq>> x86_perf_event_update(event);
+ */
 u64 x86_perf_event_update(struct perf_event *event)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -128,6 +140,18 @@ u64 x86_perf_event_update(struct perf_event *event)
 	 */
 again:
 	prev_raw_count = local64_read(&hwc->prev_count);
+	/*
+	 * 在以下使用hw_perf_event->event_base_rdpmc:
+	 *   - arch/x86/events/amd/uncore.c|94| <<amd_uncore_read>> rdpmcl(hwc->event_base_rdpmc, new);
+	 *   - arch/x86/events/amd/uncore.c|158| <<amd_uncore_add>> hwc->event_base_rdpmc = uncore->rdpmc_base + hwc->idx;
+	 *   - arch/x86/events/amd/uncore.c|169| <<amd_uncore_add>> hwc->event_base_rdpmc += NUM_COUNTERS_L3;
+	 *   - arch/x86/events/core.c|131| <<x86_perf_event_update>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+	 *   - arch/x86/events/core.c|1242| <<x86_assign_hw_event>> hwc->event_base_rdpmc = (idx - INTEL_PMC_IDX_FIXED) |
+	 *   - arch/x86/events/core.c|1249| <<x86_assign_hw_event>> hwc->event_base_rdpmc = x86_pmu_rdpmc_index(hwc->idx);
+	 *   - arch/x86/events/core.c|1272| <<x86_perf_rdpmc_index>> return event->hw.event_base_rdpmc;
+	 *   - arch/x86/events/core.c|2576| <<x86_pmu_event_idx>> return hwc->event_base_rdpmc + 1;
+	 *   - arch/x86/events/intel/ds.c|1880| <<intel_pmu_save_and_restart_reload>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+	 *
 	rdpmcl(hwc->event_base_rdpmc, new_raw_count);
 
 	if (local64_cmpxchg(&hwc->prev_count, prev_raw_count,
@@ -247,6 +271,11 @@ static void release_pmc_hardware(void) {}
 
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2113| <<init_hw_perf_events>> if (!check_hw_exists(&pmu, x86_pmu.num_counters, x86_pmu.num_counters_fixed))
+ *   - arch/x86/events/intel/core.c|4504| <<init_hybrid_pmu>> if (!check_hw_exists(&pmu->pmu, pmu->num_counters, pmu->num_counters_fixed))
+ */
 bool check_hw_exists(struct pmu *pmu, int num_counters, int num_counters_fixed)
 {
 	u64 val, val_fail = -1, val_new= ~0;
@@ -1434,6 +1463,11 @@ int x86_perf_event_set_period(struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|768| <<amd_pmu_enable_event>> x86_pmu_enable_event(event);
+ *   - arch/x86/events/intel/core.c|4105| <<core_pmu_enable_event>> x86_pmu_enable_event(event);
+ */
 void x86_pmu_enable_event(struct perf_event *event)
 {
 	if (__this_cpu_read(cpu_hw_events.enabled))
@@ -2076,6 +2110,10 @@ void x86_pmu_update_cpu_context(struct pmu *pmu, int cpu)
 	cpuctx->ctx.pmu = pmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2227| <<global>> early_initcall(init_hw_perf_events); 
+ */
 static int __init init_hw_perf_events(void)
 {
 	struct x86_pmu_quirk *quirk;
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index c20d8cd47..f766c15f7 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -5723,6 +5723,17 @@ __init int intel_pmu_init(void)
 	char *name;
 	struct x86_hybrid_pmu *pmu;
 
+	/*
+	 * 在init_intel()设置的. 依靠cpuid 0xa
+	 *   - arch/x86/kernel/cpu/intel.c|675| <<init_intel>> set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
+	 *
+	 * 671         if (c->cpuid_level > 9) {
+	 * 672                 unsigned eax = cpuid_eax(10);
+	 * 673                 // Check for version and the number of counters
+	 * 674                 if ((eax & 0xff) && (((eax>>8) & 0xff) > 1))
+	 * 675                         set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
+	 * 676         }
+	 */
 	if (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {
 		switch (boot_cpu_data.x86) {
 		case 0x6:
diff --git a/arch/x86/events/intel/lbr.c b/arch/x86/events/intel/lbr.c
index 47fca6a7a..4137fa1e1 100644
--- a/arch/x86/events/intel/lbr.c
+++ b/arch/x86/events/intel/lbr.c
@@ -354,6 +354,14 @@ static __always_inline void wrlbr_info(unsigned int idx, u64 val)
 	wrmsrl(x86_pmu.lbr_info + idx, val);
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/intel/lbr.c|405| <<rdlbr_all>> u64 from = rdlbr_from(idx, NULL);
+ *   - arch/x86/events/intel/lbr.c|479| <<lbr_is_reset_in_cstate>> return x86_pmu.lbr_deep_c_reset && !rdlbr_from(0, NULL);
+ *   - arch/x86/events/intel/lbr.c|481| <<lbr_is_reset_in_cstate>> return !rdlbr_from(((struct x86_perf_task_context *)ctx)->tos, NULL);
+ *   - arch/x86/events/intel/lbr.c|824| <<intel_pmu_lbr_read_64>> from = rdlbr_from(lbr_idx, NULL);
+ *   - arch/x86/events/intel/lbr.c|941| <<intel_pmu_store_lbr>> from = rdlbr_from(i, lbr);
+ */
 static __always_inline u64 rdlbr_from(unsigned int idx, struct lbr_entry *lbr)
 {
 	u64 val;
@@ -399,6 +407,11 @@ wrlbr_all(struct lbr_entry *lbr, unsigned int idx, bool need_info)
 		wrlbr_info(idx, lbr->info);
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/intel/lbr.c|524| <<intel_pmu_lbr_save>> if (!rdlbr_all(&task_ctx->lbr[i], lbr_idx, need_info))
+ *   - arch/x86/events/intel/lbr.c|541| <<intel_pmu_arch_lbr_save>> if (!rdlbr_all(&entries[i], i, true))
+ */
 static inline bool
 rdlbr_all(struct lbr_entry *lbr, unsigned int idx, bool need_info)
 {
@@ -800,6 +813,10 @@ void intel_pmu_lbr_read_32(struct cpu_hw_events *cpuc)
  * is the same as the linear address, allowing us to merge the LIP and EIP
  * LBR formats.
  */
+/*
+ * struct x86_pmu core_pmu.lbr_read = intel_pmu_lbr_read_64()
+ * struct x86_pmu intel_pmu.lbr_read = intel_pmu_lbr_read_64()
+ */
 void intel_pmu_lbr_read_64(struct cpu_hw_events *cpuc)
 {
 	bool need_info = false, call_stack = false;
@@ -1162,6 +1179,10 @@ int intel_pmu_setup_lbr_filter(struct perf_event *event)
  * decoded (e.g., text page not present), then X86_BR_NONE is
  * returned.
  */
+/*
+ * called by:
+ *   - arch/x86/events/intel/lbr.c|1424| <<intel_pmu_lbr_filter>> type = branch_type(from, to, cpuc->lbr_entries[i].abort);
+ */
 static int branch_type(unsigned long from, unsigned long to, int abort)
 {
 	struct insn insn;
diff --git a/arch/x86/events/intel/p6.c b/arch/x86/events/intel/p6.c
index 408879b0c..e2f8cae9f 100644
--- a/arch/x86/events/intel/p6.c
+++ b/arch/x86/events/intel/p6.c
@@ -269,6 +269,9 @@ __init int p6_pmu_init(void)
 		break;
 
 	default:
+		/*
+		 * qemu host的x86_model是79
+		 */
 		pr_cont("unsupported p6 CPU model %d ", boot_cpu_data.x86_model);
 		return -ENODEV;
 	}
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 266143abc..8f3d83c8b 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -1083,6 +1083,23 @@ extern u64 __read_mostly hw_cache_extra_regs
 
 u64 x86_perf_event_update(struct perf_event *event);
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|238| <<reserve_pmc_hardware>> if (!reserve_evntsel_nmi(x86_pmu_config_addr(i)))
+ *   - arch/x86/events/core.c|246| <<reserve_pmc_hardware>> release_evntsel_nmi(x86_pmu_config_addr(i));
+ *   - arch/x86/events/core.c|263| <<release_pmc_hardware>> release_evntsel_nmi(x86_pmu_config_addr(i));
+ *   - arch/x86/events/core.c|291| <<check_hw_exists>> reg = x86_pmu_config_addr(i);
+ *   - arch/x86/events/core.c|715| <<x86_pmu_disable_all>> rdmsrl(x86_pmu_config_addr(idx), val);
+ *   - arch/x86/events/core.c|719| <<x86_pmu_disable_all>> wrmsrl(x86_pmu_config_addr(idx), val);
+ *   - arch/x86/events/core.c|721| <<x86_pmu_disable_all>> wrmsrl(x86_pmu_config_addr(idx + 1), 0);
+ *   - arch/x86/events/core.c|1271| <<x86_assign_hw_event>> hwc->config_base = x86_pmu_config_addr(hwc->idx);
+ *   - arch/x86/events/core.c|1606| <<perf_event_print_debug>> rdmsrl(x86_pmu_config_addr(idx), pmc_ctrl);
+ *   - arch/x86/events/intel/core.c|2838| <<intel_pmu_reset>> wrmsrl_safe(x86_pmu_config_addr(idx), 0ull);
+ *   - arch/x86/events/intel/core.c|4083| <<core_guest_get_msrs>> arr[idx].msr = x86_pmu_config_addr(idx);
+ *   - arch/x86/events/intel/p4.c|1382| <<p4_pmu_init>> reg = x86_pmu_config_addr(i);
+ *   - arch/x86/events/perf_event.h|1157| <<__x86_pmu_enable_event>> wrmsrl(x86_pmu_config_addr(hwc->idx + 1), x86_pmu.perf_ctr_pair_en);
+ *   - arch/x86/events/perf_event.h|1178| <<x86_pmu_disable_event>> wrmsrl(x86_pmu_config_addr(hwc->idx + 1), 0);
+ */
 static inline unsigned int x86_pmu_config_addr(int index)
 {
 	return x86_pmu.eventsel + (x86_pmu.addr_offset ?
@@ -1131,6 +1148,16 @@ static inline bool is_counter_pair(struct hw_perf_event *hwc)
 	return hwc->flags & PERF_X86_EVENT_PAIR;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1440| <<x86_pmu_enable_event>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2330| <<intel_pmu_nhm_workaround>> __x86_pmu_enable_event(&event->hw,
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ */
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
diff --git a/arch/x86/hyperv/hv_init.c b/arch/x86/hyperv/hv_init.c
index 3de6d8b53..09d4a1ef5 100644
--- a/arch/x86/hyperv/hv_init.c
+++ b/arch/x86/hyperv/hv_init.c
@@ -43,6 +43,23 @@ union hv_ghcb * __percpu *hv_ghcb_pg;
 /* Storage to save the hypercall page temporarily for hibernation */
 static void *hv_hypercall_pg_saved;
 
+/*
+ * 在以下使用hv_vp_assist_page:
+ *   - arch/x86/hyperv/hv_init.c|46| <<global>> struct hv_vp_assist_page **hv_vp_assist_page;
+ *   - arch/x86/hyperv/hv_apic.c|91| <<hv_apic_eoi_write>> struct hv_vp_assist_page *hvp = hv_vp_assist_page[smp_processor_id()];
+ *   - arch/x86/hyperv/hv_init.c|80| <<hv_cpu_init>> struct hv_vp_assist_page **hvp = &hv_vp_assist_page[smp_processor_id()];
+ *   - arch/x86/hyperv/hv_init.c|87| <<hv_cpu_init>> if (!hv_vp_assist_page)
+ *   - arch/x86/hyperv/hv_init.c|229| <<hv_cpu_die>> if (hv_vp_assist_page && hv_vp_assist_page[cpu]) {
+ *   - arch/x86/hyperv/hv_init.c|238| <<hv_cpu_die>> memunmap(hv_vp_assist_page[cpu]);
+ *   - arch/x86/hyperv/hv_init.c|239| <<hv_cpu_die>> hv_vp_assist_page[cpu] = NULL;
+ *   - arch/x86/hyperv/hv_init.c|401| <<hyperv_init>> hv_vp_assist_page = kcalloc(num_possible_cpus(),
+ *   - arch/x86/hyperv/hv_init.c|402| <<hyperv_init>> sizeof(*hv_vp_assist_page), GFP_KERNEL);
+ *   - arch/x86/hyperv/hv_init.c|403| <<hyperv_init>> if (!hv_vp_assist_page) {
+ *   - arch/x86/hyperv/hv_init.c|528| <<hyperv_init>> kfree(hv_vp_assist_page);
+ *   - arch/x86/hyperv/hv_init.c|529| <<hyperv_init>> hv_vp_assist_page = NULL;
+ *   - arch/x86/include/asm/mshyperv.h|146| <<hv_get_vp_assist_page>> if (!hv_vp_assist_page)
+ *   - arch/x86/include/asm/mshyperv.h|149| <<hv_get_vp_assist_page>> return hv_vp_assist_page[cpu];
+ */
 struct hv_vp_assist_page **hv_vp_assist_page;
 EXPORT_SYMBOL_GPL(hv_vp_assist_page);
 
diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 3415321c8..e2be21d8c 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -327,6 +327,13 @@ struct apic {
 	/* wakeup_secondary_cpu */
 	int	(*wakeup_secondary_cpu)(int apicid, unsigned long start_eip);
 	/* wakeup secondary CPU using 64-bit wakeup point */
+	/*
+	 * 在以下使用wakeup_secondary_cpu_64:
+	 *   - arch/x86/kernel/apic/apic.c|2563| <<acpi_wake_cpu_handler_update>> (*drv)->wakeup_secondary_cpu_64 = handler;
+	 *   - arch/x86/kernel/smpboot.c|1083| <<do_boot_cpu>> if (apic->wakeup_secondary_cpu_64)
+	 *   - arch/x86/kernel/smpboot.c|1132| <<do_boot_cpu>> if (apic->wakeup_secondary_cpu_64)
+	 *   - arch/x86/kernel/smpboot.c|1133| <<do_boot_cpu>> boot_error = apic->wakeup_secondary_cpu_64(apicid, start_ip);
+	 */
 	int	(*wakeup_secondary_cpu_64)(int apicid, unsigned long start_eip);
 
 	void	(*inquire_remote_apic)(int apicid);
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index 503a57781..e21df2653 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -158,6 +158,19 @@ static inline void fpstate_set_confidential(struct fpu_guest *gfpu)
 
 static inline bool fpstate_is_confidential(struct fpu_guest *gfpu)
 {
+	/*
+	 * @is_confidential:    Indicator for KVM confidential mode.
+	 *                      The FPU registers are restored by the
+	 *                      vmentry firmware from encrypted guest
+	 *                      memory. On vmexit the FPU registers are
+	 *                      saved by firmware to encrypted guest memory
+	 *                      and the registers are scrubbed before
+	 *                      returning to the host. So there is no
+	 *                      content which is worth saving and restoring.
+	 *                      The fpstate has to be there so that
+	 *                      preemption and softirq FPU usage works
+	 *                      without special casing.
+	 */
 	return gfpu->fpstate->is_confidential;
 }
 
diff --git a/arch/x86/include/asm/fpu/types.h b/arch/x86/include/asm/fpu/types.h
index eb7cd1139..102a06dfd 100644
--- a/arch/x86/include/asm/fpu/types.h
+++ b/arch/x86/include/asm/fpu/types.h
@@ -142,6 +142,21 @@ enum xfeature {
 #define XFEATURE_MASK_XTILE_CFG		(1 << XFEATURE_XTILE_CFG)
 #define XFEATURE_MASK_XTILE_DATA	(1 << XFEATURE_XTILE_DATA)
 
+/*
+ * 在以下使用XFEATURE_MASK_FPSSE:
+ *   - arch/x86/include/asm/fpu/xstate.h|13| <<XFEATURE_MASK_EXTEND>> #define XFEATURE_MASK_EXTEND (~(XFEATURE_MASK_FPSSE | (1ULL << 63)))
+ *   - arch/x86/kernel/fpu/core.c|384| <<fpu_copy_guest_fpstate_to_uabi>> ustate->xsave.header.xfeatures = XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/core.c|398| <<fpu_copy_uabi_to_guest_fpstate>> if (ustate->xsave.header.xfeatures & ~XFEATURE_MASK_FPSSE)
+ *   - arch/x86/kernel/fpu/init.c|200| <<fpu__init_system_xstate_size_legacy>> fpu_user_cfg.legacy_features = XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/regset.c|124| <<xfpregs_set>> fpu->fpstate->regs.xsave.header.xfeatures |= XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/signal.c|62| <<check_xstate_in_sigframe>> fx_sw->xfeatures = XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/signal.c|152| <<save_xstate_epilog>> xfeatures |= XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/signal.c|359| <<__fpu_restore_sig>> user_xfeatures = XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/signal.c|421| <<__fpu_restore_sig>> fpregs->xsave.header.xfeatures |= XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kernel/fpu/xstate.c|801| <<fpu__init_system_xstate>> if ((fpu_kernel_cfg.max_features & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
+ *   - arch/x86/kvm/cpuid.c|319| <<__kvm_update_cpuid_runtime>> best->ecx |= XFEATURE_MASK_FPSSE;
+ *   - arch/x86/kvm/cpuid.c|365| <<kvm_vcpu_after_set_cpuid>> XFEATURE_MASK_FPSSE;
+ */
 #define XFEATURE_MASK_FPSSE		(XFEATURE_MASK_FP | XFEATURE_MASK_SSE)
 #define XFEATURE_MASK_AVX512		(XFEATURE_MASK_OPMASK \
 					 | XFEATURE_MASK_ZMM_Hi256 \
@@ -352,6 +367,23 @@ struct fpstate {
 	u64			xfeatures;
 
 	/* @user_xfeatures:	xfeatures valid in UABI buffers */
+	/*
+	 * 在以下使用fpstate->user_xfeatures:
+	 *   - arch/x86/kernel/fpu/core.c|527| <<__fpstate_reset>> fpstate->user_xfeatures = fpu_user_cfg.default_features;
+	 *   - arch/x86/kernel/fpu/signal.c|110| <<save_sw_bytes>> sw_bytes->xfeatures = fpstate->user_xfeatures;
+	 *   - arch/x86/kernel/fpu/signal.c|288| <<restore_fpregs_from_user>> ret = __restore_fpregs_from_user(buf, fpu->fpstate->user_xfeatures,
+	 *   - arch/x86/kernel/fpu/signal.c|343| <<__fpu_restore_sig>> u64 user_xfeatures = 0;
+	 *   - arch/x86/kernel/fpu/signal.c|353| <<__fpu_restore_sig>> user_xfeatures = fx_sw_user.xfeatures;
+	 *   - arch/x86/kernel/fpu/signal.c|355| <<__fpu_restore_sig>> user_xfeatures = XFEATURE_MASK_FPSSE;
+	 *   - arch/x86/kernel/fpu/signal.c|361| <<__fpu_restore_sig>> return restore_fpregs_from_user(buf_fx, user_xfeatures, fx_only,
+	 *   - arch/x86/kernel/fpu/signal.c|434| <<__fpu_restore_sig>> u64 mask = user_xfeatures | xfeatures_mask_supervisor();
+	 *   - arch/x86/kernel/fpu/xstate.c|403| <<validate_user_xstate_header>> if (hdr->xfeatures & ~fpstate->user_xfeatures)
+	 *   - arch/x86/kernel/fpu/xstate.c|1088| <<__copy_xstate_to_uabi_buf>> header.xfeatures &= fpstate->user_xfeatures;
+	 *   - arch/x86/kernel/fpu/xstate.c|1131| <<__copy_xstate_to_uabi_buf>> mask = fpstate->user_xfeatures;
+	 *   - arch/x86/kernel/fpu/xstate.c|1494| <<fpstate_realloc>> newfps->user_xfeatures = curfps->user_xfeatures | xfeatures;
+	 *   - arch/x86/kernel/fpu/xstate.h|264| <<xsave_to_user_sigframe>> u64 mask = fpstate->user_xfeatures;
+	 *   - arch/x86/kvm/cpuid.c|356| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
+	 */
 	u64			user_xfeatures;
 
 	/* @xfd:		xfeatures disabled to trap userspace use. */
diff --git a/arch/x86/include/asm/fpu/xcr.h b/arch/x86/include/asm/fpu/xcr.h
index 9656a5bc6..1429923d2 100644
--- a/arch/x86/include/asm/fpu/xcr.h
+++ b/arch/x86/include/asm/fpu/xcr.h
@@ -5,6 +5,14 @@
 #define XCR_XFEATURE_ENABLED_MASK	0x00000000
 #define XCR_XFEATURE_IN_USE_MASK	0x00000001
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/fpu/xcr.h|32| <<xfeatures_in_use>> return xgetbv(XCR_XFEATURE_IN_USE_MASK);
+ *   - arch/x86/kernel/sev-shared.c|433| <<snp_cpuid_postprocess>> xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
+ *   - arch/x86/kernel/sev-shared.c|885| <<vc_handle_cpuid>> ghcb_set_xcr0(ghcb, xgetbv(XCR_XFEATURE_ENABLED_MASK));
+ *   - arch/x86/kvm/svm/sev.c|3023| <<sev_es_prepare_switch_to_guest>> hostsa->xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
+ *   - arch/x86/kvm/x86.c|10121| <<kvm_arch_init>> host_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
+ */
 static inline u64 xgetbv(u32 index)
 {
 	u32 eax, edx;
diff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h
index 275e7fd20..e5f038a64 100644
--- a/arch/x86/include/asm/hardirq.h
+++ b/arch/x86/include/asm/hardirq.h
@@ -25,6 +25,13 @@ typedef struct {
 	unsigned int apic_irq_work_irqs;
 #ifdef CONFIG_SMP
 	unsigned int irq_resched_count;
+	/*
+	 * 在以下增加irq_call_count:
+	 *   - arch/x86/kernel/smp.c|238| <<DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/kernel/smp.c|247| <<DEFINE_IDTENTRY_SYSVEC(sysvec_call_function_single)>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/xen/smp.c|248| <<xen_call_function_interrupt>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/xen/smp.c|256| <<xen_call_function_single_interrupt>> inc_irq_stat(irq_call_count);
+	 */
 	unsigned int irq_call_count;
 #endif
 	unsigned int irq_tlb_count;
diff --git a/arch/x86/include/asm/hyperv-tlfs.h b/arch/x86/include/asm/hyperv-tlfs.h
index 0a9407dc0..7dd46c3e6 100644
--- a/arch/x86/include/asm/hyperv-tlfs.h
+++ b/arch/x86/include/asm/hyperv-tlfs.h
@@ -375,6 +375,13 @@ struct hv_vp_assist_page {
 	struct hv_nested_enlightenments_control nested_control;
 	__u8 enlighten_vmentry;
 	__u8 reserved2[7];
+	/*
+	 * 在以下使用hv_vp_assist_page->current_nested_vmcs:
+	 *   - arch/x86/kvm/vmx/evmcs.c|362| <<nested_enlightened_vmentry>> if (unlikely(!evmptr_is_valid(assist_page.current_nested_vmcs)))
+	 *   - arch/x86/kvm/vmx/evmcs.c|365| <<nested_enlightened_vmentry>> *evmcs_gpa = assist_page.current_nested_vmcs;
+	 *   - arch/x86/kvm/vmx/evmcs.h|211| <<evmcs_load>> vp_ap->current_nested_vmcs = phys_addr;
+	 *   - arch/x86/kvm/vmx/vmx.c|8471| <<vmx_exit>> vp_ap->current_nested_vmcs = 0;
+	 */
 	__u64 current_nested_vmcs;
 } __packed;
 
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index aa381ab69..116b57660 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -73,20 +73,88 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2999| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3155| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3246| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3255| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3673| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4773| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5397| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9140| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10396| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10718| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11998| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|567| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|584| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * 处理kvm_guest_time_update()
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|165| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/x86.c|11323| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|13107| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ * 处理的函数: kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|194| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8172| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11325| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ * 处理的函数: kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1357| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2348| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2527| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9354| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10519| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12179| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|106| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理kvm_update_masterclock()
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
+/*
+ * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+ *   - arch/x86/kvm/x86.c|3092| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|3122| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *
+ * 应该是在vcpu_enter_guest()的下面的代码判断的.
+ * 11312         if (kvm_vcpu_exit_request(vcpu)) {
+ * 11313                 vcpu->mode = OUTSIDE_GUEST_MODE;
+ * 11314                 smp_wmb();
+ * 11315                 local_irq_enable();
+ * 11316                 preempt_enable();
+ * 11317                 kvm_vcpu_srcu_read_lock(vcpu);
+ * 11318                 r = 1;
+ * 11319                 goto cancel_injection;
+ * 11320         }
+ */
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2505| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5364| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11044| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理函数是kvm_gen_kvmclock_update(vcpu)
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -489,6 +557,17 @@ struct kvm_pmc {
 	enum pmc_type type;
 	u8 idx;
 	u64 counter;
+	/*
+	 * 在以下设置kvm_pmc->eventsel:
+	 *   - arch/x86/kvm/pmu.c|405| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_OS;
+	 *   - arch/x86/kvm/pmu.c|407| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_USR;
+	 *   - arch/x86/kvm/pmu.c|409| <<reprogram_counter>> eventsel |= ARCH_PERFMON_EVENTSEL_INT;
+	 *   - arch/x86/kvm/svm/pmu.c|240| <<amd_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/svm/pmu.c|293| <<amd_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|520| <<intel_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|541| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|700| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 */
 	u64 eventsel;
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
@@ -496,31 +575,177 @@ struct kvm_pmc {
 	 * eventsel value for general purpose counters,
 	 * ctrl value for fixed counters.
 	 */
+	/*
+	 * 在以下使用kvm_pmc->current_config:
+	 *   - arch/x86/kvm/pmu.c|324| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+	 *   - arch/x86/kvm/pmu.c|329| <<reprogram_counter>> pmc->current_config = new_config;
+	 *   - arch/x86/kvm/pmu.c|567| <<cpl_is_matched>> u64 config = pmc->current_config;
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_release_perf_event>> pmc->current_config = 0;
+	 *   - arch/x86/kvm/svm/pmu.c|280| <<amd_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|624| <<intel_pmu_init>> pmu->gp_counters[i].current_config = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|631| <<intel_pmu_init>> pmu->fixed_counters[i].current_config = 0;
+	 */
 	u64 current_config;
 	bool is_paused;
+	/*
+	 * 在以下使用kvm_pmc->intr:
+	 *   - arch/x86/kvm/pmu.c|174| <<__kvm_perf_overflow>> if (!pmc->intr || skip_pmi)
+	 *   - arch/x86/kvm/pmu.c|275| <<pmc_reprogram_counter>> pmc->intr = intr || pebs;
+	 */
 	bool intr;
 };
 
 #define KVM_PMC_MAX_FIXED	3
 struct kvm_pmu {
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|335| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|564| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|587| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	unsigned nr_arch_gp_counters;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	unsigned nr_arch_fixed_counters;
+	/*
+	 * 在以下使用kvm_pmu->available_event_types:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|120| <<intel_hw_event_available>> if ((i < 7) && !(pmu->available_event_types & (1 << i)))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|594| <<intel_pmu_refresh>> pmu->available_event_types = ~entry->ebx &
+	 */
 	unsigned available_event_types;
+	/*
+	 * 在以下使用kvm_pmu->fixed_ctr_ctrl:
+	 *   - arch/x86/kvm/pmu.c|538| <<reprogram_counter>> fixed_ctr_ctrl = fixed_ctrl_field(pmu->fixed_ctr_ctrl,
+	 *   - arch/x86/kvm/pmu.h|258| <<pmc_speculative_in_use>> return fixed_ctrl_field(pmu->fixed_ctr_ctrl,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|64| <<reprogram_fixed_counters>> u8 old_fixed_ctr_ctrl = pmu->fixed_ctr_ctrl;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|67| <<reprogram_fixed_counters>> pmu->fixed_ctr_ctrl = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|433| <<intel_pmu_get_msr(MSR_CORE_PERF_FIXED_CTR_CTRL)>> msr_info->data = pmu->fixed_ctr_ctrl;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|488| <<intel_pmu_set_msr(MSR_CORE_PERF_FIXED_CTR_CTRL)>> if (pmu->fixed_ctr_ctrl == data)
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|761| <<intel_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *
+	 * global的sel, 对每一个event的配置(MSR_CORE_PERF_FIXED_CTR_CTRL)
+	 */
 	u64 fixed_ctr_ctrl;
+	/*
+	 * 在以下使用kvm_pmu->fixed_ctr_ctrl_mask:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|490| <<intel_pmu_set_msr>> if (!(data & pmu->fixed_ctr_ctrl_mask)) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|620| <<intel_pmu_refresh>> pmu->fixed_ctr_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|659| <<intel_pmu_refresh>> pmu->fixed_ctr_ctrl_mask &= ~(0xbull << (i * 4));
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|698| <<intel_pmu_refresh>> pmu->fixed_ctr_ctrl_mask &= ~(1ULL << (INTEL_PMC_IDX_FIXED + i * 4));
+	 *
+	 * 似乎是记录被支持的bit
+	 */
 	u64 fixed_ctr_ctrl_mask;
+	/*
+	 * 在以下使用kvm_pmu->global_ctrl:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|108| <<global_ctrl_changed>> u64 diff = pmu->global_ctrl ^ data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|111| <<global_ctrl_changed>> pmu->global_ctrl = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|156| <<intel_pmc_is_enabled>> return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|439| <<intel_pmu_get_msr(MSR_CORE_PERF_GLOBAL_CTRL)>> msr_info->data = pmu->global_ctrl;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|502| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_CTRL)>> if (pmu->global_ctrl == data)
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|761| <<intel_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|888| <<intel_pmu_cross_mapped_check>> for_each_set_bit(bit, (unsigned long *)&pmu->global_ctrl,
+	 *   - arch/x86/kvm/vmx/vmx.c|7000| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+	 *
+	 * 记录MSR_CORE_PERF_GLOBAL_CTRL当前的值(哪个fixed被enable/disable了)
+	 */
 	u64 global_ctrl;
+	/*
+	 * 在以下使用global_status:
+	 *   - arch/x86/kvm/pmu.c|221| <<__kvm_perf_overflow>> (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/pmu.c|223| <<__kvm_perf_overflow>> __set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
+	 *   - arch/x86/kvm/svm/pmu.c|391| <<amd_pmu_refresh>> pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|436| <<intel_pmu_get_msr(MSR_CORE_PERF_GLOBAL_STATUS)>> msr_info->data = pmu->global_status;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|497| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_STATUS, host only!)>> pmu->global_status = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|512| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL)>> pmu->global_status &= ~data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|761| <<intel_pmu_reset>> pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8234| <<vmx_handle_intel_pt_intr>> (unsigned long *)&vcpu->arch.pmu.global_status);
+	 *
+	 * 记录哪个正在overflow
+	 * 只能被MSR_CORE_PERF_GLOBAL_OVF_CTRL修改
+	 * 如果是MSR_CORE_PERF_GLOBAL_STATUS的话, 不是read就是from QEMU
+	 */
 	u64 global_status;
+	/*
+	 * 在以下设置kvm_pmu->counter_bitmasks[2]:
+	 *   - arch/x86/kvm/svm/pmu.c|373| <<amd_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << 48) - 1;
+	 *   - arch/x86/kvm/svm/pmu.c|378| <<amd_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|613| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_GP] = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|614| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|638| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|653| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_FIXED] = ((u64)1 << edx.split.bit_width_fixed) - 1;
+	 */
 	u64 counter_bitmask[2];
+	/*
+	 * 在以下使用kvm_pmu->global_ctrl_mask:
+	 *   - arch/x86/kvm/pmu.h|161| <<kvm_valid_perf_global_ctrl>> return !(pmu->global_ctrl_mask & data);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|618| <<intel_pmu_refresh>> pmu->global_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|662| <<intel_pmu_refresh>> pmu->global_ctrl_mask = counter_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|663| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+	 */
 	u64 global_ctrl_mask;
+	/*
+	 * 只在intel使用kvm_pmu->global_ovf_ctrl_mask:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|510| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL)>> if (!(data & pmu->global_ovf_ctrl_mask)) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|619| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|663| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|667| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+	 */
 	u64 global_ovf_ctrl_mask;
 	u64 reserved_bits;
 	u64 raw_event_mask;
 	u8 version;
 	struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+	/*
+	 * 在以下使用kvm_pmu->irq_work:
+	 *   - arch/x86/kvm/pmu.c|192| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+	 *   - arch/x86/kvm/pmu.c|663| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+	 *   - arch/x86/kvm/pmu.c|673| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+	 */
 	struct irq_work irq_work;
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
 	DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->all_valid_pmc_idx:
+	 *   - arch/x86/kvm/pmu.c|633| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/pmu.c|705| <<kvm_pmu_trigger_event>> for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/svm/pmu.c|266| <<amd_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|589| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|591| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
+	 */
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	DECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);
 
 	u64 ds_area;
@@ -542,12 +767,31 @@ struct kvm_pmu {
 	 * The gate to release perf_events not marked in
 	 * pmc_in_use only once in a vcpu time slice.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	bool need_cleanup;
 
 	/*
 	 * The total number of programmed perf_events and it helps to avoid
 	 * redundant check before cleanup if guest don't use vPMU at all.
 	 */
+	/*
+	 * 在以下使用kvm_pmu->event_count:
+	 *   - arch/x86/kvm/pmu.c|218| <<pmc_reprogram_counter>> pmc_to_pmu(pmc)->event_count++;
+	 *   - arch/x86/kvm/pmu.c|519| <<kvm_pmu_init>> pmu->event_count = 0;
+	 *   - arch/x86/kvm/pmu.h|72| <<pmc_release_perf_event>> pmc_to_pmu(pmc)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|246| <<intel_pmu_release_guest_lbr_event>> vcpu_to_pmu(vcpu)->event_count--;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|297| <<intel_pmu_create_guest_lbr_event>> pmu->event_count++;
+	 *   - arch/x86/kvm/x86.c|12059| <<kvm_arch_sched_in>> if (pmu->version && unlikely(pmu->event_count)) {
+	 */
 	u8 event_count;
 };
 
@@ -645,6 +889,16 @@ struct kvm_vcpu_arch {
 	 * kvm_{register,rip}_{read,write} functions.
 	 */
 	unsigned long regs[NR_VCPU_REGS];
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_avail:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|56| <<kvm_register_is_available>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|68| <<kvm_register_mark_available>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|74| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/svm/svm.c|4093| <<svm_vcpu_run>> vcpu->arch.regs_avail &= ~SVM_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/nested.c|273| <<vmx_switch_vmcs>> vcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/vmx.c|7316| <<vmx_vcpu_run>> vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/x86.c|12636| <<kvm_arch_vcpu_create>> vcpu->arch.regs_avail = ~0;
+	 */
 	u32 regs_avail;
 	u32 regs_dirty;
 
@@ -677,6 +931,13 @@ struct kvm_vcpu_arch {
 	u64 ia32_xss;
 	u64 microcode_version;
 	u64 arch_capabilities;
+	/*
+	 * 在以下使用kvm_vcpu_arch->perf_capabilities:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|187| <<vcpu_get_perf_capabilities>> return vcpu->arch.perf_capabilities;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|689| <<intel_pmu_init>> vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();
+	 *   - arch/x86/kvm/x86.c|3560| <<kvm_set_msr_common>> vcpu->arch.perf_capabilities = data;
+	 *   - arch/x86/kvm/x86.c|3987| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.perf_capabilities;
+	 */
 	u64 perf_capabilities;
 
 	/*
@@ -726,9 +987,55 @@ struct kvm_vcpu_arch {
 	 * "guest_fpstate" state here contains the guest FPU context, with the
 	 * host PRKU bits.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->guest_fpu:
+	 *   - arch/x86/kernel/fpu/core.c|332| <<fpu_swap_kvm_fpstate>> struct fpstate *guest_fps = guest_fpu->fpstate;
+	 *   - arch/x86/kvm/cpuid.c|166| <<kvm_check_cpuid>> return fpu_enable_guest_xfd_features(&vcpu->arch.guest_fpu, xfeatures);
+	 *   - arch/x86/kvm/cpuid.c|356| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
+	 *   - arch/x86/kvm/svm/svm.c|1402| <<svm_vcpu_create>> fpstate_set_confidential(&vcpu->arch.guest_fpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6808| <<handle_nm_fault_irqoff>> if (vcpu->arch.guest_fpu.fpstate->xfd) 
+	 *   - arch/x86/kvm/vmx/vmx.c|6809| <<handle_nm_fault_irqoff>> rdmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);
+	 *   - arch/x86/kvm/x86.c|4446| <<kvm_set_msr_common>> fpu_update_guest_xfd(&vcpu->arch.guest_fpu, data);
+	 *   - arch/x86/kvm/x86.c|4456| <<kvm_set_msr_common>> vcpu->arch.guest_fpu.xfd_err = data;
+	 *   - arch/x86/kvm/x86.c|4817| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.guest_fpu.fpstate->xfd;
+	 *   - arch/x86/kvm/x86.c|4824| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.guest_fpu.xfd_err;
+	 *   - arch/x86/kvm/x86.c|5860| <<kvm_vcpu_ioctl_x86_get_xsave>> if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
+	 *   - arch/x86/kvm/x86.c|5863| <<kvm_vcpu_ioctl_x86_get_xsave>> fpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu,
+	 *   - arch/x86/kvm/x86.c|5872| <<kvm_vcpu_ioctl_x86_get_xsave2>> if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
+	 *   - arch/x86/kvm/x86.c|5875| <<kvm_vcpu_ioctl_x86_get_xsave2>> fpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu,
+	 *   - arch/x86/kvm/x86.c|5882| <<kvm_vcpu_ioctl_x86_set_xsave>> if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
+	 *   - arch/x86/kvm/x86.c|5885| <<kvm_vcpu_ioctl_x86_set_xsave>> return fpu_copy_uabi_to_guest_fpstate(&vcpu->arch.guest_fpu,
+	 *   - arch/x86/kvm/x86.c|6315| <<kvm_arch_vcpu_ioctl>> if (vcpu->arch.guest_fpu.uabi_size > sizeof(struct kvm_xsave))
+	 *   - arch/x86/kvm/x86.c|6332| <<kvm_arch_vcpu_ioctl>> int size = vcpu->arch.guest_fpu.uabi_size;
+	 *   - arch/x86/kvm/x86.c|6345| <<kvm_arch_vcpu_ioctl>> int size = vcpu->arch.guest_fpu.uabi_size;
+	 *   - arch/x86/kvm/x86.c|11217| <<vcpu_enter_guest>> if (vcpu->arch.guest_fpu.xfd_err)
+	 *   - arch/x86/kvm/x86.c|11218| <<vcpu_enter_guest>> wrmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);
+	 *   - arch/x86/kvm/x86.c|11300| <<vcpu_enter_guest>> if (vcpu->arch.guest_fpu.xfd_err)
+	 *   - arch/x86/kvm/x86.c|11555| <<kvm_load_guest_fpu>> fpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, true);
+	 *   - arch/x86/kvm/x86.c|11562| <<kvm_put_guest_fpu>> fpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, false);
+	 *   - arch/x86/kvm/x86.c|12189| <<kvm_arch_vcpu_ioctl_get_fpu>> if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
+	 *   - arch/x86/kvm/x86.c|12194| <<kvm_arch_vcpu_ioctl_get_fpu>> fxsave = &vcpu->arch.guest_fpu.fpstate->regs.fxsave;
+	 *   - arch/x86/kvm/x86.c|12212| <<kvm_arch_vcpu_ioctl_set_fpu>> if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
+	 *   - arch/x86/kvm/x86.c|12217| <<kvm_arch_vcpu_ioctl_set_fpu>> fxsave = &vcpu->arch.guest_fpu.fpstate->regs.fxsave;
+	 *   - arch/x86/kvm/x86.c|12345| <<kvm_arch_vcpu_create>> if (!fpu_alloc_guest_fpstate(&vcpu->arch.guest_fpu)) {
+	 *   - arch/x86/kvm/x86.c|12381| <<kvm_arch_vcpu_create>> fpu_free_guest_fpstate(&vcpu->arch.guest_fpu);
+	 *   - arch/x86/kvm/x86.c|12427| <<kvm_arch_vcpu_destroy>> fpu_free_guest_fpstate(&vcpu->arch.guest_fpu);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_vcpu_reset>> if (vcpu->arch.guest_fpu.fpstate && kvm_mpx_supported()) {
+	 *   - arch/x86/kvm/x86.c|12498| <<kvm_vcpu_reset>> struct fpstate *fpstate = vcpu->arch.guest_fpu.fpstate;
+	 */
 	struct fpu_guest guest_fpu;
 
 	u64 xcr0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->guest_supported_xcr0:
+	 *   - arch/x86/kvm/cpuid.c|267| <<__kvm_update_cpuid_runtime>> u64 guest_supported_xcr0 = cpuid_get_supported_xcr0(entries, nent);
+	 *   - arch/x86/kvm/cpuid.c|317| <<__kvm_update_cpuid_runtime>> best->ecx &= guest_supported_xcr0 & 0xffffffff;
+	 *   - arch/x86/kvm/cpuid.c|318| <<__kvm_update_cpuid_runtime>> best->edx &= guest_supported_xcr0 >> 32;
+	 *   - arch/x86/kvm/cpuid.c|348| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_supported_xcr0 =
+	 *   - arch/x86/kvm/cpuid.c|356| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
+	 *   - arch/x86/kvm/x86.c|1025| <<kvm_guest_supported_xfd>> return vcpu->arch.guest_supported_xcr0 & XFEATURE_MASK_USER_DYNAMIC;
+	 *   - arch/x86/kvm/x86.c|1048| <<__kvm_set_xcr>> valid_bits = vcpu->arch.guest_supported_xcr0 | XFEATURE_MASK_FP;
+	 */
 	u64 guest_supported_xcr0;
 
 	struct kvm_pio_request pio;
@@ -758,6 +1065,19 @@ struct kvm_vcpu_arch {
 	int halt_request; /* real mode on Intel only */
 
 	int cpuid_nent;
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpuid_entries:
+	 *   - arch/x86/kvm/cpuid.c|180| <<kvm_cpuid_check_equal>> orig = &vcpu->arch.cpuid_entries[i];
+	 *   - arch/x86/kvm/cpuid.c|232| <<kvm_find_kvm_cpuid_features>> return __kvm_find_kvm_cpuid_features(vcpu, vcpu->arch.cpuid_entries,
+	 *   - arch/x86/kvm/cpuid.c|325| <<kvm_update_cpuid_runtime>> __kvm_update_cpuid_runtime(vcpu, vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);
+	 *   - arch/x86/kvm/cpuid.c|349| <<kvm_vcpu_after_set_cpuid>> cpuid_get_supported_xcr0(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);
+	 *   - arch/x86/kvm/cpuid.c|440| <<kvm_set_cpuid>> kvfree(vcpu->arch.cpuid_entries);
+	 *   - arch/x86/kvm/cpuid.c|441| <<kvm_set_cpuid>> vcpu->arch.cpuid_entries = e2;
+	 *   - arch/x86/kvm/cpuid.c|529| <<kvm_vcpu_ioctl_get_cpuid2>> if (copy_to_user(entries, vcpu->arch.cpuid_entries,
+	 *   - arch/x86/kvm/cpuid.c|1389| <<kvm_find_cpuid_entry_index>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 *   - arch/x86/kvm/cpuid.c|1397| <<kvm_find_cpuid_entry>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 *   - arch/x86/kvm/x86.c|11755| <<kvm_arch_vcpu_destroy>> kvfree(vcpu->arch.cpuid_entries);
+	 */
 	struct kvm_cpuid_entry2 *cpuid_entries;
 	u32 kvm_cpuid_base;
 
@@ -773,7 +1093,22 @@ struct kvm_vcpu_arch {
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3601| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3605| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	unsigned int hw_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2399| <<kvm_write_system_time>> kvm_gfn_to_pfn_cache_init(vcpu->kvm, &vcpu->arch.pv_time, vcpu,
+	 *   - arch/x86/kvm/x86.c|2403| <<kvm_write_system_time>> kvm_gfn_to_pfn_cache_destroy(vcpu->kvm, &vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3457| <<kvm_guest_time_update>> if (vcpu->pv_time.active)
+	 *   - arch/x86/kvm/x86.c|3458| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|3683| <<kvmclock_reset>> kvm_gfn_to_pfn_cache_destroy(vcpu->kvm, &vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|5642| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|6807| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time.active)
+	 */
 	struct gfn_to_pfn_cache pv_time;
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
@@ -785,8 +1120,31 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache cache;
 	} st;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2604| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = l1_offset;
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|680| <<nested_vmcb02_prepare_control>> vcpu->arch.l1_tsc_offset,
+	 *   - arch/x86/kvm/svm/nested.c|1038| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/svm/svm.c|1115| <<svm_write_tsc_offset>> svm->vmcb01.ptr->control.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2551| <<prepare_vmcs02>> vcpu->arch.l1_tsc_offset,
+	 *   - arch/x86/kvm/vmx/nested.c|4571| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2568| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset +
+	 *   - arch/x86/kvm/x86.c|2601| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset,
+	 *   - arch/x86/kvm/x86.c|2764| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3714| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4073| <<kvm_get_msr_common>> offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|5429| <<kvm_arch_tsc_get_attr>> if (put_user(vcpu->arch.l1_tsc_offset, uaddr))
+	 */
 	u64 l1_tsc_offset;
 	u64 tsc_offset; /* current tsc offset */
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2830| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3567| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|5158| <<kvm_arch_vcpu_load>> vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|11096| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
@@ -794,9 +1152,31 @@ struct kvm_vcpu_arch {
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2694| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10396| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|11685| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2730| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2753| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|385| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	s8 virtual_tsc_shift;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2730| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2753| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|385| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	u32 virtual_tsc_mult;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2470| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 */
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
@@ -911,9 +1291,33 @@ struct kvm_vcpu_arch {
 	bool l1tf_flush_l1d;
 
 	/* Host CPU on which VM-entry was most recently attempted */
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_vmentry_cpu:
+	 *   - arch/x86/kvm/x86.c|10627| <<vcpu_enter_guest>> vcpu->arch.last_vmentry_cpu = vcpu->cpu;
+	 *   - arch/x86/kvm/x86.c|11624| <<kvm_arch_vcpu_create>> vcpu->arch.last_vmentry_cpu = -1;
+	 * 在以下使用kvm_vcpu_arch->last_vmentry_cpu:
+	 *   - arch/x86/kvm/cpuid.c|427| <<kvm_set_cpuid>> if (vcpu->arch.last_vmentry_cpu != -1) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5275| <<kvm_mmu_after_set_cpuid>> KVM_BUG_ON(vcpu->arch.last_vmentry_cpu != -1, vcpu->kvm);
+	 *   - arch/x86/kvm/svm/sev.c|2616| <<pre_sev_run>> svm->vcpu.arch.last_vmentry_cpu == cpu)
+	 *   - arch/x86/kvm/svm/svm.c|3243| <<dump_vmcb>> svm->current_vmcb->ptr, vcpu->arch.last_vmentry_cpu);
+	 *   - arch/x86/kvm/svm/svm.c|3367| <<svm_handle_invalid_exit>> vcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/svm/svm.c|3442| <<svm_handle_exit>> kvm_run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|5129| <<handle_exception_nmi>> vcpu->run->internal.data[3] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6143| <<dump_vmcs>> vmx->loaded_vmcs->vmcs, vcpu->arch.last_vmentry_cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6368| <<__vmx_handle_exit>> vcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6377| <<__vmx_handle_exit>> vcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6406| <<__vmx_handle_exit>> vcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;
+	 *   - arch/x86/kvm/vmx/vmx.c|6466| <<__vmx_handle_exit>> vcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;
+	 */
 	int last_vmentry_cpu;
 
 	/* AMD MSRC001_0015 Hardware Configuration */
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_hwcr:
+	 *   - arch/x86/kvm/x86.c|3282| <<can_set_mci_status>> return !!(vcpu->arch.msr_hwcr & BIT_ULL(18));
+	 *   - arch/x86/kvm/x86.c|3623| <<kvm_set_msr_common>> vcpu->arch.msr_hwcr = data;
+	 *   - arch/x86/kvm/x86.c|4249| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_hwcr;
+	 */
 	u64 msr_hwcr;
 
 	/* pv related cpuid info */
@@ -1142,6 +1546,15 @@ struct kvm_arch {
 	unsigned int indirect_shadow_pages;
 	u8 mmu_valid_gen;
 	struct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];
+	/*
+	 * 在以下使用kvm_arch->active_mmu_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|2137| <<kvm_mmu_alloc_shadow_page>> list_add(&sp->link, &kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|2552| <<kvm_mmu_zap_oldest_mmu_pages>> if (list_empty(&kvm->arch.active_mmu_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|2556| <<kvm_mmu_zap_oldest_mmu_pages>> list_for_each_entry_safe_reverse(sp, tmp, &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5866| <<kvm_zap_obsolete_pages>> &kvm->arch.active_mmu_pages, link) {
+	 *   - arch/x86/kvm/mmu/mmu.c|5991| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6525| <<kvm_mmu_zap_all>> list_for_each_entry_safe(sp, node, &kvm->arch.active_mmu_pages, link) {
+	 */
 	struct list_head active_mmu_pages;
 	struct list_head zapped_obsolete_pages;
 	struct list_head lpage_disallowed_mmu_pages;
@@ -1184,29 +1597,107 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下设置kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/x86.c|12668| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 *   - arch/x86/kvm/x86.c|7015| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|498| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3381| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3387| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3566| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 */
 	s64 kvmclock_offset;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2923| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3024| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3120| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3417| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3465| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3495| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|6069| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|6079| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12922| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|12924| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|12927| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12929| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	/*
 	 * This also protects nr_vcpus_matched_tsc which is read from a
 	 * preemption-disabled region, so it must be a raw spinlock.
 	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2740| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2792| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|12146| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2857| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|3018| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|5889| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 */
 	u32 last_tsc_khz;
 	u64 last_tsc_offset;
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2877| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|3020| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 */
 	u64 cur_tsc_offset;
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2510| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2525| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2763| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2765| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3024| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	int nr_vcpus_matched_tsc;
 
 	u32 default_tsc_khz;
 
 	seqcount_raw_spinlock_t pvclock_sc;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3077| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 * 在以下使用kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2525| <<kvm_track_tsc_matching>> if (ka->use_master_clock ||
+	 *   - arch/x86/kvm/x86.c|2531| <<kvm_track_tsc_matching>> ka->use_master_clock, gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|3081| <<pvclock_update_vm_gtod_copy>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|3085| <<pvclock_update_vm_gtod_copy>> trace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,
+	 *   - arch/x86/kvm/x86.c|3147| <<__get_kvmclock>> if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3266| <<kvm_guest_time_update>> use_master_clock = ka->use_master_clock;
+	 *   - arch/x86/kvm/x86.c|4923| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+	 *   - arch/x86/kvm/x86.c|6761| <<kvm_vm_ioctl_set_clock>> if (ka->use_master_clock)
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3106| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3212| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3321| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6814| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3808| <<kvmclock_update_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3823| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3839| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12902| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12943| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
 	struct delayed_work kvmclock_sync_work;
 
@@ -1290,6 +1781,17 @@ struct kvm_arch {
 	 * count to zero should removed the root from the list and clean
 	 * it up, freeing the root after an RCU grace period.
 	 */
+	/*
+	 * 在以下使用kvm_arch->tdp_mmu_roots:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|30| <<kvm_mmu_init_tdp_mmu>> INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|58| <<kvm_mmu_uninit_tdp_mmu>> WARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|213| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|217| <<tdp_mmu_next_root>> next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|225| <<tdp_mmu_next_root>> next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|269| <<for_each_tdp_mmu_root>> list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link) \
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|335| <<kvm_tdp_mmu_get_vcpu_root_hpa>> list_add_rcu(&root->link, &kvm->arch.tdp_mmu_roots);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1038| <<kvm_tdp_mmu_invalidate_all_roots>> list_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link) {
+	 */
 	struct list_head tdp_mmu_roots;
 
 	/*
diff --git a/arch/x86/include/asm/kvmclock.h b/arch/x86/include/asm/kvmclock.h
index 6c5765192..74baf239e 100644
--- a/arch/x86/include/asm/kvmclock.h
+++ b/arch/x86/include/asm/kvmclock.h
@@ -13,6 +13,11 @@ static inline struct pvclock_vcpu_time_info *this_cpu_pvti(void)
 	return &this_cpu_read(hv_clock_per_cpu)->pvti;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|181| <<kvm_check_and_clear_guest_paused>> struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
+ *   - arch/x86/kernel/kvmclock.c|213| <<kvm_register_clock>> struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
+ */
 static inline struct pvclock_vsyscall_time_info *this_cpu_hvclock(void)
 {
 	return this_cpu_read(hv_clock_per_cpu);
diff --git a/arch/x86/include/asm/mshyperv.h b/arch/x86/include/asm/mshyperv.h
index 61f0c206b..08838bcbb 100644
--- a/arch/x86/include/asm/mshyperv.h
+++ b/arch/x86/include/asm/mshyperv.h
@@ -141,6 +141,14 @@ static inline u64 hv_do_fast_hypercall16(u16 code, u64 input1, u64 input2)
 
 extern struct hv_vp_assist_page **hv_vp_assist_page;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm_onhyperv.h|47| <<svm_hv_hardware_setup>> hv_get_vp_assist_page(cpu);
+ *   - arch/x86/kvm/vmx/evmcs.h|207| <<evmcs_load>> hv_get_vp_assist_page(smp_processor_id());
+ *   - arch/x86/kvm/vmx/vmx.c|2453| <<vmx_hardware_enable>> !hv_get_vp_assist_page(cpu))
+ *   - arch/x86/kvm/vmx/vmx.c|8465| <<vmx_exit>> vp_ap = hv_get_vp_assist_page(cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8501| <<vmx_init>> if (!hv_get_vp_assist_page(cpu)) {
+ */
 static inline struct hv_vp_assist_page *hv_get_vp_assist_page(unsigned int cpu)
 {
 	if (!hv_vp_assist_page)
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 6674bdb09..e209206e5 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -581,8 +581,30 @@
 #define AMD_CPPC_ENERGY_PERF_PREF(x)	(((x) & 0xff) << 24)
 
 /* AMD Performance Counter Global Status and Control MSRs */
+/*
+ * CPUID Fn8000_0022_EAX[PerfMonV2] = 1 支持下面的:
+ *
+ * - PerfCntGlobalCtl MSR
+ * - PerfCntGlobalStatus MSR
+ * - PerfCntGlobalStatusClr MSR
+ * - PerfCntGlobalStatusSet MSR
+ */
+/*
+ * 在以下使用MSR_AMD64_PERF_CNTR_GLOBAL_STATUS:
+ *   - arch/x86/events/amd/core.c|678| <<amd_pmu_get_global_status>> rdmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS, status);
+ */
 #define MSR_AMD64_PERF_CNTR_GLOBAL_STATUS	0xc0000300
+/*
+ * 在以下使用MSR_AMD64_PERF_CNTR_GLOBAL_CTL:
+ *   - arch/x86/events/amd/core.c|591| <<amd_pmu_cpu_reset>> wrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_CTL, 0);
+ *   - arch/x86/events/amd/core.c|670| <<amd_pmu_set_global_ctl>> wrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_CTL, ctl);
+ */
 #define MSR_AMD64_PERF_CNTR_GLOBAL_CTL		0xc0000301
+/*
+ * 在以下使用MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR:
+ *   - arch/x86/events/amd/core.c|594| <<amd_pmu_cpu_reset>> wrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR, amd_pmu_global_cntr_mask);
+ *   - arch/x86/events/amd/core.c|693| <<amd_pmu_ack_global_status>> wrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR, status);
+ */
 #define MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR	0xc0000302
 
 /* Fam 17h MSRs */
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..84c03635c 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -23,6 +23,29 @@ extern int unknown_nmi_panic;
 
 #define NMI_FLAG_FIRST	1
 
+/*
+ * 注册了NMI_LOCAL的:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|101| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/reboot.c|850| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *
+ * 注册了NMI_UNKNOWN的:
+ *   - rch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 enum {
 	NMI_LOCAL=0,
 	NMI_UNKNOWN,
@@ -44,6 +67,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index c936ce9f0..143fe1e1d 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -311,6 +311,13 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 		: "memory");
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1496| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1355| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|437| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|448| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
 	u64 val = PRED_CMD_IBPB;
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index f6fc8dd51..c781e2ec6 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -28,6 +28,29 @@
 #define ARCH_PERFMON_EVENTSEL_PIN_CONTROL		(1ULL << 19)
 #define ARCH_PERFMON_EVENTSEL_INT			(1ULL << 20)
 #define ARCH_PERFMON_EVENTSEL_ANY			(1ULL << 21)
+/*
+ * 在以下使用ARCH_PERFMON_EVENTSEL_ENABLE:
+ *   - arch/x86/events/amd/core.c|21| <<AMD_MERGE_EVENT_ENABLE>> #define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)
+ *   - arch/x86/events/amd/core.c|799| <<amd_pmu_v2_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/amd/uncore.c|109| <<amd_uncore_start>> wrmsrl(hwc->config_base, (hwc->config | ARCH_PERFMON_EVENTSEL_ENABLE));
+ *   - arch/x86/events/core.c|266| <<check_hw_exists>> if (val & ARCH_PERFMON_EVENTSEL_ENABLE) {
+ *   - arch/x86/events/core.c|687| <<x86_pmu_disable_all>> if (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))
+ *   - arch/x86/events/core.c|689| <<x86_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/core.c|743| <<x86_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/core.c|1441| <<x86_pmu_enable_event>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2331| <<intel_pmu_nhm_workaround>> ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|2772| <<intel_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/core.c|4090| <<core_guest_get_msrs>> event->hw.config | ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4093| <<core_guest_get_msrs>> arr[idx].host &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4095| <<core_guest_get_msrs>> arr[idx].guest &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/core.c|4120| <<core_pmu_enable_all>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/events/intel/knc.c|183| <<knc_pmu_disable_event>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/knc.c|194| <<knc_pmu_enable_event>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|144| <<p6_pmu_disable_all>> val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/intel/p6.c|154| <<p6_pmu_enable_all>> val |= ARCH_PERFMON_EVENTSEL_ENABLE;
+ *   - arch/x86/events/zhaoxin/core.c|347| <<zhaoxin_pmu_enable_event>> __x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);
+ *   - arch/x86/kvm/pmu.h|158| <<pmc_speculative_in_use>> return pmc->eventsel & ARCH_PERFMON_EVENTSEL_ENABLE;
+ */
 #define ARCH_PERFMON_EVENTSEL_ENABLE			(1ULL << 22)
 #define ARCH_PERFMON_EVENTSEL_INV			(1ULL << 23)
 #define ARCH_PERFMON_EVENTSEL_CMASK			0xFF000000ULL
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226ef..b4fb6a024 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -41,6 +41,15 @@ struct pvclock_wall_clock {
 } __attribute__((__packed__));
 
 #define PVCLOCK_TSC_STABLE_BIT	(1 << 0)
+/*
+ * 在以下使用PVCLOCK_GUEST_STOPPED:
+ *   - arch/x86/kernel/kvmclock.c|187| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+ *   - arch/x86/kernel/kvmclock.c|188| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kernel/pvclock.c|86| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+ *   - arch/x86/kernel/pvclock.c|87| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kvm/x86.c|3427| <<kvm_setup_guest_pvclock>> vcpu->hv_clock.flags |= (guest_hv_clock->flags & PVCLOCK_GUEST_STOPPED);
+ *   - arch/x86/kvm/x86.c|3430| <<kvm_setup_guest_pvclock>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+ */
 #define PVCLOCK_GUEST_STOPPED	(1 << 1)
 /* PVCLOCK_COUNTS_FROM_ZERO broke ABI and can't be used anymore. */
 #define PVCLOCK_COUNTS_FROM_ZERO (1 << 2)
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2..bcf2d7d3e 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -35,12 +35,23 @@ bool pvclock_read_retry(const struct pvclock_vcpu_time_info *src,
 	return unlikely(version != src->version);
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/pvclock.h|97| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
+ *   - arch/x86/kvm/x86.c|2713| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.h|366| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+ */
 /*
  * Scale a 64-bit delta by scaling and multiplying by a 32-bit fraction,
  * yielding a 64-bit result.
  */
 static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 {
+	/*
+	 * 例子:
+	 * shift = -1
+	 * multi = 2532094943
+	 */
 	u64 product;
 #ifdef __i386__
 	u32 tmp1, tmp2;
@@ -78,6 +89,13 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 	return product;
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+ *   - arch/x86/kernel/pvclock.c|82| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+ *   - arch/x86/kvm/x86.c|3358| <<__get_kvmclock>> data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
+ *   - drivers/ptp/ptp_kvm_x86.c|86| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair.tsc);
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
diff --git a/arch/x86/include/asm/realmode.h b/arch/x86/include/asm/realmode.h
index fd6f6e5b7..af6cc715e 100644
--- a/arch/x86/include/asm/realmode.h
+++ b/arch/x86/include/asm/realmode.h
@@ -19,6 +19,13 @@ struct real_mode_header {
 	u32	text_start;
 	u32	ro_end;
 	/* SMP trampoline */
+	/*
+	 * 在以下使用real_mode_header->trampoline_start:
+	 *   - arch/x86/kernel/sev.c|1007| <<wakeup_cpu_via_vmgexit>> if (WARN_ONCE(start_ip != real_mode_header->trampoline_start,
+	 *   - arch/x86/kernel/sev.c|1166| <<sev_es_setup_ap_jump_table>> startup_cs = (u16)(rmh->trampoline_start >> 4);
+	 *   - arch/x86/kernel/sev.c|1168| <<sev_es_setup_ap_jump_table>> rmh->trampoline_start);
+	 *   - arch/x86/kernel/smpboot.c|1076| <<do_boot_cpu>> unsigned long start_ip = real_mode_header->trampoline_start;
+	 */
 	u32	trampoline_start;
 	u32	trampoline_header;
 #ifdef CONFIG_AMD_MEM_ENCRYPT
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 5393babc0..dab2a3d90 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -23,6 +23,10 @@ extern void x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bo
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4009| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
@@ -37,6 +41,10 @@ void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4036| <<svm_vcpu_run>> x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_restore_host(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/include/asm/svm.h b/arch/x86/include/asm/svm.h
index 036162684..c54a4d30c 100644
--- a/arch/x86/include/asm/svm.h
+++ b/arch/x86/include/asm/svm.h
@@ -237,6 +237,12 @@ struct __attribute__ ((__packed__)) vmcb_control_area {
 
 #define AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK	GENMASK_ULL(11, 0)
 #define AVIC_PHYSICAL_ID_ENTRY_BACKING_PAGE_MASK	(0xFFFFFFFFFFULL << 12)
+/*
+ * 在以下使用AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK:
+ *   - arch/x86/kvm/svm/avic.c|1070| <<avic_vcpu_load>> entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ *   - arch/x86/kvm/svm/avic.c|1086| <<avic_vcpu_put>> if (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))
+ *   - arch/x86/kvm/svm/avic.c|1091| <<avic_vcpu_put>> entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ */
 #define AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK		(1ULL << 62)
 #define AVIC_PHYSICAL_ID_ENTRY_VALID_MASK		(1ULL << 63)
 #define AVIC_PHYSICAL_ID_TABLE_SIZE_MASK		(0xFFULL)
diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c
index 907cc98b1..263a58be2 100644
--- a/arch/x86/kernel/acpi/boot.c
+++ b/arch/x86/kernel/acpi/boot.c
@@ -163,6 +163,38 @@ static int __init acpi_parse_madt(struct acpi_table_header *table)
  *
  * Returns the logic cpu number which maps to the local apic
  */
+/*
+ * boot的时候
+ * [0] acpi_register_lapic
+ * [0] acpi_parse_lapic
+ * [0] acpi_table_parse_entries_array
+ * [0] acpi_boot_init
+ * [0] acpi_parse_x2apic
+ * [0] acpi_parse_x2apic_nmi
+ * [0] setup_arch
+ * [0] start_kernel
+ * [0] load_ucode_bsp
+ * [0] secondary_startup_64_no_verify
+ *
+ * cpu hotadd的时候
+ * [0] acpi_register_lapic
+ * [0] acpi_map_cpu
+ * [0] acpi_processor_add
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - arch/x86/kernel/acpi/boot.c|228| <<acpi_parse_x2apic>> acpi_register_lapic(apic_id, processor->uid, enabled);
+ *   - arch/x86/kernel/acpi/boot.c|265| <<acpi_parse_lapic>> acpi_register_lapic(processor->id,
+ *   - arch/x86/kernel/acpi/boot.c|284| <<acpi_parse_sapic>> acpi_register_lapic((processor->id << 8) | processor->eid,
+ *   - arch/x86/kernel/acpi/boot.c|821| <<acpi_map_cpu>> cpu = acpi_register_lapic(physid, acpi_id, ACPI_MADT_ENABLED);
+ */
 static int acpi_register_lapic(int id, u32 acpiid, u8 enabled)
 {
 	unsigned int ver = 0;
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 6d303d1d2..9cbff21da 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -1064,6 +1064,10 @@ void setup_secondary_APIC_clock(void)
 /*
  * The guts of the apic timer interrupt
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1112| <<DEFINE_IDTENTRY_SYSVEC(sysvec_apic_timer_interrupt)>> local_apic_timer_interrupt();
+ */
 static void local_apic_timer_interrupt(void)
 {
 	struct clock_event_device *evt = this_cpu_ptr(&lapic_events);
diff --git a/arch/x86/kernel/check.c b/arch/x86/kernel/check.c
index 5136e6818..c6fbe09bf 100644
--- a/arch/x86/kernel/check.c
+++ b/arch/x86/kernel/check.c
@@ -24,6 +24,13 @@ static int __read_mostly memory_corruption_check = -1;
 static unsigned __read_mostly corruption_check_size = 64*1024;
 static unsigned __read_mostly corruption_check_period = 60; /* seconds */
 
+/*
+ * 在以下使用scan_areas[MAX_SCAN_AREAS]:
+ *   - arch/x86/kernel/check.c|125| <<setup_bios_corruption_check>> scan_areas[num_scan_areas].addr = start;
+ *   - arch/x86/kernel/check.c|126| <<setup_bios_corruption_check>> scan_areas[num_scan_areas].size = end - start;
+ *   - arch/x86/kernel/check.c|149| <<check_for_bios_corruption>> unsigned long *addr = __va(scan_areas[i].addr);
+ *   - arch/x86/kernel/check.c|150| <<check_for_bios_corruption>> unsigned long size = scan_areas[i].size;
+ */
 static struct scan_area {
 	u64 addr;
 	u64 size;
@@ -137,6 +144,10 @@ void __init setup_bios_corruption_check(void)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kernel/check.c|169| <<check_corruption>> check_for_bios_corruption();
+ */
 static void check_for_bios_corruption(void)
 {
 	int i;
@@ -164,6 +175,10 @@ static void check_for_bios_corruption(void)
 static void check_corruption(struct work_struct *dummy);
 static DECLARE_DELAYED_WORK(bios_check_work, check_corruption);
 
+/*
+ * 在以下使用check_corruption():
+ *   - arch/x86/kernel/check.c|176| <<global>> static DECLARE_DELAYED_WORK(bios_check_work, check_corruption);
+ */
 static void check_corruption(struct work_struct *dummy)
 {
 	check_for_bios_corruption();
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index da7c361f4..de0172cbe 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -199,6 +199,18 @@ void __init check_bugs(void)
  * NOTE: This function is *only* called for SVM.  VMX spec_ctrl handling is
  * done in vmenter.S.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/spec-ctrl.h|29| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, true);
+ *   - arch/x86/include/asm/spec-ctrl.h|43| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_spec_ctrl, guest_virt_spec_ctrl, false);
+ *
+ * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+ * the guest has, while on VMEXIT we restore the host view. This
+ * would be easier if SPEC_CTRL were architecturally maskable or
+ * shadowable for guests but this is not (currently) the case.
+ * Takes the guest view of SPEC_CTRL MSR as a parameter and also
+ * the guest's version of VIRT_SPEC_CTRL, if emulated.
+ */
 void
 x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
 {
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3e508f239..e8d6793be 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -67,6 +67,14 @@
 u32 elf_hwcap2 __read_mostly;
 
 /* all of these masks are initialized in setup_cpu_local_masks() */
+/*
+ * 在以下使用cpu_initialized_mask:
+ *   - arch/x86/kernel/cpu/common.c|173| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+ *   - arch/x86/kernel/cpu/common.c|2138| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+ *   - arch/x86/kernel/smpboot.c|1122| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+ *   - arch/x86/kernel/smpboot.c|1147| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+ *   - arch/x86/kernel/smpboot.c|1644| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+ */
 cpumask_var_t cpu_initialized_mask;
 cpumask_var_t cpu_callout_mask;
 cpumask_var_t cpu_callin_mask;
@@ -942,6 +950,19 @@ void cpu_detect(struct cpuinfo_x86 *c)
 		u32 junk, tfms, cap0, misc;
 
 		cpuid(0x00000001, &tfms, &misc, &junk, &cap0);
+		/*
+		 *  6 unsigned int x86_family(unsigned int sig)
+		 *  7 {
+		 *  8         unsigned int x86;
+		 *  9
+		 * 10         x86 = (sig >> 8) & 0xf;
+		 * 11
+		 * 12         if (x86 == 0xf)
+		 * 13                 x86 += (sig >> 20) & 0xff;
+		 * 14
+		 * 15         return x86;
+		 * 16 }
+		 */
 		c->x86		= x86_family(tfms);
 		c->x86_model	= x86_model(tfms);
 		c->x86_stepping	= x86_stepping(tfms);
@@ -1941,6 +1962,10 @@ void __init identify_boot_cpu(void)
 	tsx_init();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|399| <<smp_store_cpu_info>> identify_secondary_cpu(c);
+ */
 void identify_secondary_cpu(struct cpuinfo_x86 *c)
 {
 	BUG_ON(c == &boot_cpu_data);
@@ -2122,6 +2147,14 @@ static void wait_for_master_cpu(int cpu)
 	 * wait for ACK from master CPU before continuing
 	 * with AP initialization
 	 */
+	/*
+	 * 在以下使用cpu_initialized_mask:
+	 *   - arch/x86/kernel/cpu/common.c|173| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+	 *   - arch/x86/kernel/cpu/common.c|2138| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+	 *   - arch/x86/kernel/smpboot.c|1122| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 *   - arch/x86/kernel/smpboot.c|1147| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+	 *   - arch/x86/kernel/smpboot.c|1644| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 */
 	WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
 	while (!cpumask_test_cpu(cpu, cpu_callout_mask))
 		cpu_relax();
diff --git a/arch/x86/kernel/cpu/mtrr/mtrr.c b/arch/x86/kernel/cpu/mtrr/mtrr.c
index 2746cac9d..0947febbb 100644
--- a/arch/x86/kernel/cpu/mtrr/mtrr.c
+++ b/arch/x86/kernel/cpu/mtrr/mtrr.c
@@ -248,6 +248,10 @@ static void set_mtrr_cpuslocked(unsigned int reg, unsigned long base,
 	stop_machine_cpuslocked(mtrr_rendezvous_handler, &data, cpu_online_mask);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|808| <<mtrr_ap_init>> set_mtrr_from_inactive_cpu(~0U, 0, 0, 0);
+ */
 static void set_mtrr_from_inactive_cpu(unsigned int reg, unsigned long base,
 				      unsigned long size, mtrr_type type)
 {
@@ -784,6 +788,10 @@ void __init mtrr_bp_init(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|1972| <<identify_secondary_cpu>> mtrr_ap_init();
+ */
 void mtrr_ap_init(void)
 {
 	if (!mtrr_enabled())
diff --git a/arch/x86/kernel/fpu/context.h b/arch/x86/kernel/fpu/context.h
index 958accf2c..8bdba203f 100644
--- a/arch/x86/kernel/fpu/context.h
+++ b/arch/x86/kernel/fpu/context.h
@@ -45,6 +45,11 @@ static inline void fpregs_deactivate(struct fpu *fpu)
 	trace_x86_fpu_regs_deactivated(fpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/context.h|77| <<fpregs_restore_userregs>> fpregs_activate(fpu);
+ *   - arch/x86/kernel/fpu/core.c|850| <<fpregs_mark_activate>> fpregs_activate(fpu);
+ */
 static inline void fpregs_activate(struct fpu *fpu)
 {
 	__this_cpu_write(fpu_fpregs_owner_ctx, fpu);
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 3b28c5b25..c1295927b 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -37,6 +37,33 @@ DEFINE_PER_CPU(u64, xfd_state);
 struct fpu_state_config	fpu_kernel_cfg __ro_after_init;
 struct fpu_state_config fpu_user_cfg __ro_after_init;
 
+/*
+ * 在以下使用init_fpstate:
+ *   - arch/x86/kernel/fpu/core.c|44| <<global>> struct fpstate init_fpstate __ro_after_init;
+ *   - arch/x86/kernel/fpu/core.c|192| <<fpu_reset_from_exception_fixup>> restore_fpregs_from_fpstate(&init_fpstate, XFEATURE_MASK_FPSTATE);
+ *   - arch/x86/kernel/fpu/core.c|481| <<init_fpstate_copy_size>> return sizeof(init_fpstate.regs.xsave);
+ *   - arch/x86/kernel/fpu/core.c|535| <<fpstate_reset>> __fpstate_reset(fpu->fpstate, init_fpstate.xfd);
+ *   - arch/x86/kernel/fpu/core.c|584| <<fpu_clone>> memcpy(&dst_fpu->fpstate->regs, &init_fpstate.regs,
+ *   - arch/x86/kernel/fpu/core.c|668| <<restore_fpregs_from_init_fpstate>> os_xrstor(&init_fpstate, features_mask);
+ *   - arch/x86/kernel/fpu/core.c|670| <<restore_fpregs_from_init_fpstate>> fxrstor(&init_fpstate.regs.fxsave);
+ *   - arch/x86/kernel/fpu/core.c|672| <<restore_fpregs_from_init_fpstate>> frstor(&init_fpstate.regs.fsave);
+ *   - arch/x86/kernel/fpu/core.c|699| <<fpu_reset_fpregs>> memcpy(&fpu->fpstate->regs, &init_fpstate.regs, init_fpstate_copy_size());
+ *   - arch/x86/kernel/fpu/init.c|131| <<fpu__init_system_generic>> fpstate_init_user(&init_fpstate);
+ *   - arch/x86/kernel/fpu/init.c|216| <<fpu__init_init_fpstate>> init_fpstate.size = fpu_kernel_cfg.max_size;
+ *   - arch/x86/kernel/fpu/init.c|217| <<fpu__init_init_fpstate>> init_fpstate.xfeatures = fpu_kernel_cfg.max_features;
+ *   - arch/x86/kernel/fpu/signal.c|264| <<__restore_fpregs_from_user>> os_xrstor(&init_fpstate, init_bv);
+ *   - arch/x86/kernel/fpu/xstate.c|183| <<fpu__init_cpu_xstate>> wrmsrl(MSR_IA32_XFD, init_fpstate.xfd);
+ *   - arch/x86/kernel/fpu/xstate.c|363| <<setup_init_fpu_buf>> xstate_init_xcomp_bv(&init_fpstate.regs.xsave, fpu_kernel_cfg.max_features);
+ *   - arch/x86/kernel/fpu/xstate.c|368| <<setup_init_fpu_buf>> os_xrstor_booting(&init_fpstate.regs.xsave);
+ *   - arch/x86/kernel/fpu/xstate.c|386| <<setup_init_fpu_buf>> fxsave(&init_fpstate.regs.fxsave);
+ *   - arch/x86/kernel/fpu/xstate.c|687| <<is_supported_xstate_size>> if (test_xstate_size <= sizeof(init_fpstate.regs))
+ *   - arch/x86/kernel/fpu/xstate.c|691| <<is_supported_xstate_size>> sizeof(init_fpstate.regs), test_xstate_size);
+ *   - arch/x86/kernel/fpu/xstate.c|757| <<fpu__init_disable_system_xstate>> init_fpstate.xfd = 0;
+ *   - arch/x86/kernel/fpu/xstate.c|851| <<fpu__init_system_xstate>> init_fpstate.xfd = fpu_user_cfg.max_features & XFEATURE_MASK_USER_DYNAMIC;
+ *   - arch/x86/kernel/fpu/xstate.c|1072| <<__copy_xstate_to_uabi_buf>> struct xregs_state *xinit = &init_fpstate.regs.xsave;
+ *   - arch/x86/kernel/fpu/xstate.c|1387| <<xstate_op_valid>> if (fpstate == &init_fpstate)
+ *   - arch/x86/kernel/fpu/xstate.c|1426| <<xfd_update_static_branch>> if (init_fpstate.xfd)
+ */
 /*
  * Represents the initial FPU state. It's mostly (but not completely) zeroes,
  * depending on the FPU hardware format:
@@ -133,6 +160,12 @@ void save_fpregs_to_fpstate(struct fpu *fpu)
 	frstor(&fpu->fpstate->regs.fsave);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/context.h|80| <<fpregs_restore_userregs>> restore_fpregs_from_fpstate(fpu->fpstate, XFEATURE_MASK_FPSTATE);
+ *   - arch/x86/kernel/fpu/core.c|219| <<fpu_reset_from_exception_fixup>> restore_fpregs_from_fpstate(&init_fpstate, XFEATURE_MASK_FPSTATE);
+ *   - arch/x86/kernel/fpu/core.c|415| <<fpu_swap_kvm_fpstate>> restore_fpregs_from_fpstate(cur_fps, XFEATURE_MASK_FPSTATE);
+ */
 void restore_fpregs_from_fpstate(struct fpstate *fpstate, u64 mask)
 {
 	/*
@@ -215,6 +248,18 @@ static void fpu_init_guest_permissions(struct fpu_guest *gfpu)
 	gfpu->perm = perm & ~FPU_GUEST_PERM_LOCKED;
 }
 
+/*
+ * struct task_struct:
+ * -> struct thread_struct thread;
+ *    -> struct fpu fpu;
+ *       -> struct fpstate *fpstate;
+ *          -> u64 xfeatures;
+ *          -> u64 user_xfeatures;
+ *       -> struct fpstate *__task_fpstate;
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|12528| <<kvm_arch_vcpu_create>> if (!fpu_alloc_guest_fpstate(&vcpu->arch.guest_fpu)) {
+ */
 bool fpu_alloc_guest_fpstate(struct fpu_guest *gfpu)
 {
 	struct fpstate *fpstate;
@@ -314,6 +359,10 @@ EXPORT_SYMBOL_GPL(fpu_update_guest_xfd);
  * Note: It can be invoked unconditionally even when write emulation is
  * enabled for the price of a then pointless MSR read.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11615| <<vcpu_enter_guest>> fpu_sync_guest_vmexit_xfd_state();
+ */
 void fpu_sync_guest_vmexit_xfd_state(void)
 {
 	struct fpstate *fps = current->thread.fpu.fpstate;
@@ -327,9 +376,48 @@ void fpu_sync_guest_vmexit_xfd_state(void)
 EXPORT_SYMBOL_GPL(fpu_sync_guest_vmexit_xfd_state);
 #endif /* CONFIG_X86_64 */
 
+/*
+ * 如果在enter的时候执行下面的.
+ * cur_fps->xfeatures |= 0x200;
+ * cur_fps->regs.xsave.header.xfeatures |= 0x200;
+ *
+ * [   61.026395] Bad FPU state detected at restore_fpregs_from_fpstate+0x4e/0xc0, reinitializing FPU registers.
+ * [   61.026413] WARNING: CPU: 7 PID: 847 at arch/x86/mm/extable.c:126 fixup_exception+0x2e2/0x300
+ * [   61.026421] Modules linked in:
+ * [   61.026428] CPU: 7 PID: 847 Comm: qemu-system-x86 Not tainted 6.0.0 #7
+ * [   61.026434] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS rel-1.16.0-0-gd239552ce722-prebuilt.qemu.org 04/01/2014
+ * [   61.026437] RIP: 0010:fixup_exception+0x2e2/0x300
+ * ... ...
+ * [   61.027514]  exc_general_protection+0x132/0x480
+ * [   61.027524]  asm_exc_general_protection+0x22/0x30
+ * [   61.027532] RIP: 0010:restore_fpregs_from_fpstate+0x4e/0xc0
+ * ... ...
+ * ... ...
+ * [   61.027543]  ? restore_fpregs_from_fpstate+0x3c/0xc0
+ * [   61.027546]  fpu_swap_kvm_fpstate+0x6f/0x140
+ * [   61.027564]  kvm_arch_vcpu_ioctl_run+0x51/0x1af0
+ * [   61.027568]  kvm_vcpu_ioctl+0x270/0x680
+ * [   61.027572]  __x64_sys_ioctl+0x86/0xc0
+ * [   61.027578]  do_syscall_64+0x3b/0x90
+ * [   61.027597]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|11721| <<kvm_load_guest_fpu>> fpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, true);
+ *   - arch/x86/kvm/x86.c|11728| <<kvm_put_guest_fpu>> fpu_swap_kvm_fpstate(&vcpu->arch.guest_fpu, false);
+ */
 int fpu_swap_kvm_fpstate(struct fpu_guest *guest_fpu, bool enter_guest)
 {
 	struct fpstate *guest_fps = guest_fpu->fpstate;
+	/*
+	 * called by:
+	 *   - struct task_struct:
+	 *     -> struct thread_struct thread;
+	 *        -> struct fpu fpu;
+	 *           -> struct fpstate *fpstate;
+	 *              -> u64 xfeatures;
+	 *              -> u64 user_xfeatures;
+	 *           -> struct fpstate *__task_fpstate;
+	 */
 	struct fpu *fpu = &current->thread.fpu;
 	struct fpstate *cur_fps = fpu->fpstate;
 
@@ -339,6 +427,12 @@ int fpu_swap_kvm_fpstate(struct fpu_guest *guest_fpu, bool enter_guest)
 
 	/* Swap fpstate */
 	if (enter_guest) {
+		/*
+		 * @__task_fpstate:
+		 *
+		 * Pointer to an inactive struct fpstate. Initialized to NULL. Is
+		 * used only for KVM support to swap out the regular task fpstate.
+		 */
 		fpu->__task_fpstate = cur_fps;
 		fpu->fpstate = guest_fps;
 		guest_fps->in_use = true;
@@ -368,14 +462,45 @@ int fpu_swap_kvm_fpstate(struct fpu_guest *guest_fpu, bool enter_guest)
 }
 EXPORT_SYMBOL_GPL(fpu_swap_kvm_fpstate);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6010| <<kvm_vcpu_ioctl_x86_get_xsave>> fpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu, guest_xsave->region, sizeof(guest_xsave->region), vcpu->arch.pkru);
+ *   - arch/x86/kvm/x86.c|6022| <<kvm_vcpu_ioctl_x86_get_xsave2>> fpu_copy_guest_fpstate_to_uabi(&vcpu->arch.guest_fpu, state, size, vcpu->arch.pkru);
+ */
 void fpu_copy_guest_fpstate_to_uabi(struct fpu_guest *gfpu, void *buf,
 				    unsigned int size, u32 pkru)
 {
+	/*
+	 * &vcpu->arch.guest_fpu.
+	 *
+	 * struct kvm_vcpu vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct fpu_guest guest_fpu;
+	 *       -> struct fpstate *fpstate;
+	 *          -> u64 xfeatures;
+	 *          -> u64 user_xfeatures;
+	 */
 	struct fpstate *kstate = gfpu->fpstate;
+	/*
+	 * union fpregs_state {
+	 *     struct fregs_state              fsave;
+	 *     struct fxregs_state             fxsave;
+	 *     struct swregs_state             soft;
+	 *     struct xregs_state              xsave;
+	 *     u8 __padding[PAGE_SIZE];
+	 * };
+	 */
 	union fpregs_state *ustate = buf;
 	struct membuf mb = { .p = buf, .left = size };
 
 	if (cpu_feature_enabled(X86_FEATURE_XSAVE)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kernel/fpu/core.c|379| <<fpu_copy_guest_fpstate_to_uabi>> __copy_xstate_to_uabi_buf(mb, kstate, pkru, XSTATE_COPY_XSAVE);
+		 *   - arch/x86/kernel/fpu/xstate.c|1182| <<copy_xstate_to_uabi_buf>> __copy_xstate_to_uabi_buf(to, tsk->thread.fpu.fpstate,
+		 *
+		 * Copy kernel saved xstate to a UABI buffer
+		 */
 		__copy_xstate_to_uabi_buf(mb, kstate, pkru, XSTATE_COPY_XSAVE);
 	} else {
 		memcpy(&ustate->fxsave, &kstate->regs.fxsave,
@@ -386,6 +511,10 @@ void fpu_copy_guest_fpstate_to_uabi(struct fpu_guest *gfpu, void *buf,
 }
 EXPORT_SYMBOL_GPL(fpu_copy_guest_fpstate_to_uabi);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6045| <<kvm_vcpu_ioctl_x86_set_xsave>> return fpu_copy_uabi_to_guest_fpstate(&vcpu->arch.guest_fpu,
+ */
 int fpu_copy_uabi_to_guest_fpstate(struct fpu_guest *gfpu, const void *buf,
 				   u64 xcr0, u32 *vpkru)
 {
@@ -518,6 +647,11 @@ void fpstate_init_user(struct fpstate *fpstate)
 		fpstate_init_fstate(fpstate);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/core.c|256| <<fpu_alloc_guest_fpstate>> __fpstate_reset(fpstate, 0);
+ *   - arch/x86/kernel/fpu/core.c|603| <<fpstate_reset>> __fpstate_reset(fpu->fpstate, init_fpstate.xfd);
+ */
 static void __fpstate_reset(struct fpstate *fpstate, u64 xfd)
 {
 	/* Initialize sizes and feature masks */
@@ -528,6 +662,15 @@ static void __fpstate_reset(struct fpstate *fpstate, u64 xfd)
 	fpstate->xfd		= xfd;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/core.c|630| <<fpu_clone>> fpstate_reset(dst_fpu);
+ *   - arch/x86/kernel/fpu/core.c|807| <<fpu_flush_thread>> fpstate_reset(&current->thread.fpu);
+ *   - arch/x86/kernel/fpu/init.c|210| <<fpu__init_system_xstate_size_legacy>> fpstate_reset(&current->thread.fpu);
+ *   - arch/x86/kernel/fpu/init.c|226| <<fpu__init_system>> fpstate_reset(&current->thread.fpu);
+ *   - arch/x86/kernel/fpu/xstate.c|759| <<fpu__init_disable_system_xstate>> fpstate_reset(&current->thread.fpu);
+ *   - arch/x86/kernel/fpu/xstate.c|869| <<fpu__init_system_xstate>> fpstate_reset(&current->thread.fpu);
+ */
 void fpstate_reset(struct fpu *fpu)
 {
 	/* Set the fpstate pointer to the default fpstate */
@@ -747,6 +890,12 @@ void fpu_flush_thread(void)
 /*
  * Load FPU context before returning to userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/entry-common.h|58| <<arch_exit_to_user_mode_prepare>> switch_fpu_return();
+ *   - arch/x86/kvm/fpu.h|104| <<kvm_fpu_get>> switch_fpu_return();
+ *   - arch/x86/kvm/x86.c|11215| <<vcpu_enter_guest>> switch_fpu_return();
+ */
 void switch_fpu_return(void)
 {
 	if (!static_cpu_has(X86_FEATURE_FPU))
@@ -774,8 +923,25 @@ void fpregs_assert_state_consistent(void)
 EXPORT_SYMBOL_GPL(fpregs_assert_state_consistent);
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/core.c|406| <<fpu_swap_kvm_fpstate>> fpregs_mark_activate();
+ *   - arch/x86/kernel/fpu/core.c|801| <<fpu__clear_user_states>> fpregs_mark_activate();
+ *   - arch/x86/kernel/fpu/signal.c|329| <<restore_fpregs_from_user>> fpregs_mark_activate();
+ *   - arch/x86/kernel/fpu/signal.c|448| <<__fpu_restore_sig>> fpregs_mark_activate();
+ */
 void fpregs_mark_activate(void)
 {
+	/*
+	 * called by:
+	 *   - struct task_struct:
+	 *     -> struct thread_struct thread;
+	 *        -> struct fpu fpu;
+	 *           -> struct fpstate *fpstate;
+	 *              -> u64 xfeatures;
+	 *              -> u64 user_xfeatures;
+	 *           -> struct fpstate *__task_fpstate;
+	 */
 	struct fpu *fpu = &current->thread.fpu;
 
 	fpregs_activate(fpu);
diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c
index 621f4b6ca..222bb1a64 100644
--- a/arch/x86/kernel/fpu/init.c
+++ b/arch/x86/kernel/fpu/init.c
@@ -185,6 +185,10 @@ static void __init fpu__init_task_struct_size(void)
  * We set this up first, and later it will be overwritten by
  * fpu__init_system_xstate() if the CPU knows about xstates.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/init.c|236| <<fpu__init_system>> fpu__init_system_xstate_size_legacy();
+ */
 static void __init fpu__init_system_xstate_size_legacy(void)
 {
 	unsigned int size;
@@ -221,6 +225,10 @@ static void __init fpu__init_init_fpstate(void)
  * Called on the boot CPU once per system bootup, to set up the initial
  * FPU state that is later cloned into all processes:
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|1590| <<early_identify_cpu>> fpu__init_system(c);
+ */
 void __init fpu__init_system(struct cpuinfo_x86 *c)
 {
 	fpstate_reset(&current->thread.fpu);
diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
index 91d4b6de5..2ec728c4a 100644
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@ -331,6 +331,10 @@ static bool restore_fpregs_from_user(void __user *buf, u64 xrestore,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/signal.c|496| <<fpu__restore_sig>> success = __fpu_restore_sig(buf, buf_fx, ia32_fxstate);
+ */
 static bool __fpu_restore_sig(void __user *buf, void __user *buf_fx,
 			      bool ia32_fxstate)
 {
@@ -457,6 +461,11 @@ static inline unsigned int xstate_sigframe_size(struct fpstate *fpstate)
 /*
  * Restore FPU state from a sigframe:
  */
+/*
+ * called by:
+ *   - arch/x86/ia32/ia32_signal.c|96| <<ia32_restore_sigcontext>> return fpu__restore_sig(compat_ptr(sc.fpstate), 1);
+ *   - arch/x86/kernel/signal.c|140| <<restore_sigcontext>> return fpu__restore_sig((void __user *)sc.fpstate,
+ */
 bool fpu__restore_sig(void __user *buf, int ia32_frame)
 {
 	struct fpu *fpu = &current->thread.fpu;
diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
index c8340156b..4270cf41b 100644
--- a/arch/x86/kernel/fpu/xstate.c
+++ b/arch/x86/kernel/fpu/xstate.c
@@ -349,6 +349,10 @@ static __init void os_xrstor_booting(struct xregs_state *xstate)
 /*
  * setup the xstate image representing the init state
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/xstate.c|878| <<fpu__init_system_xstate>> setup_init_fpu_buf();
+ */
 static void __init setup_init_fpu_buf(void)
 {
 	BUILD_BUG_ON((XFEATURE_MASK_USER_SUPPORTED |
@@ -396,6 +400,10 @@ int xfeature_size(int xfeature_nr)
 }
 
 /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/xstate.c|1274| <<copy_uabi_to_xstate>> if (validate_user_xstate_header(&hdr, fpstate))
+ */
 static int validate_user_xstate_header(const struct xstate_header *hdr,
 				       struct fpstate *fpstate)
 {
@@ -692,6 +700,10 @@ static bool __init is_supported_xstate_size(unsigned int test_xstate_size)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/xstate.c|872| <<fpu__init_system_xstate>> err = init_xstate_size();
+ */
 static int __init init_xstate_size(void)
 {
 	/* Recompute the context size for enabled features: */
@@ -738,6 +750,10 @@ static int __init init_xstate_size(void)
  * We enabled the XSAVE hardware, but something went wrong and
  * we can not use it.  Disable it.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/xstate.c|907| <<fpu__init_system_xstate>> fpu__init_disable_system_xstate(legacy_size);
+ */
 static void __init fpu__init_disable_system_xstate(unsigned int legacy_size)
 {
 	fpu_kernel_cfg.max_features = 0;
@@ -1044,6 +1060,9 @@ int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
 static void copy_feature(bool from_xstate, struct membuf *to, void *xstate,
 			 void *init_xstate, unsigned int size)
 {
+	/*
+	 * 从第二个参数拷贝到第一个参数(to)
+	 */
 	membuf_write(to, from_xstate ? xstate : init_xstate, size);
 }
 
@@ -1060,6 +1079,32 @@ static void copy_feature(bool from_xstate, struct membuf *to, void *xstate,
  *
  * It supports partial copy but @to.pos always starts from zero.
  */
+/*
+ * 如果是从下面的情况来的
+ *
+ * kvm_vcpu_ioctl_x86_get_xsave()或者kvm_vcpu_ioctl_x86_get_xsave2()
+ * -> fpu_copy_guest_fpstate_to_uabi()
+ *    -> __copy_xstate_to_uabi_buf()
+ *
+ * &vcpu->arch.guest_fpu.
+ *
+ * struct kvm_vcpu vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct fpu_guest guest_fpu;
+ *       -> struct fpstate *fpstate;
+ *          -> u64 xfeatures;
+ *          -> u64 user_xfeatures;
+ *          -> union fpregs_state regs;
+ *             -> struct fregs_state   fsave;
+ *             -> struct fxregs_state  fxsave;
+ *             -> struct swregs_state  soft;
+ *             -> struct xregs_state   xsave;
+ *             -> u8 __padding[PAGE_SIZE];
+ *
+ * called by:
+ *   - arch/x86/kernel/fpu/core.c|379| <<fpu_copy_guest_fpstate_to_uabi>> __copy_xstate_to_uabi_buf(mb, kstate, pkru, XSTATE_COPY_XSAVE);
+ *   - arch/x86/kernel/fpu/xstate.c|1182| <<copy_xstate_to_uabi_buf>> __copy_xstate_to_uabi_buf(to, tsk->thread.fpu.fpstate,
+ */
 void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
 			       u32 pkru_val, enum xstate_copy_mode copy_mode)
 {
@@ -1085,6 +1130,23 @@ void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
 		break;
 
 	case XSTATE_COPY_XSAVE:
+		/*
+		 * 在以下使用fpstate->user_xfeatures:
+		 *   - arch/x86/kernel/fpu/core.c|527| <<__fpstate_reset>> fpstate->user_xfeatures = fpu_user_cfg.default_features;
+		 *   - arch/x86/kernel/fpu/signal.c|110| <<save_sw_bytes>> sw_bytes->xfeatures = fpstate->user_xfeatures;
+		 *   - arch/x86/kernel/fpu/signal.c|288| <<restore_fpregs_from_user>> ret = __restore_fpregs_from_user(buf, fpu->fpstate->user_xfeatures,
+		 *   - arch/x86/kernel/fpu/signal.c|343| <<__fpu_restore_sig>> u64 user_xfeatures = 0;
+		 *   - arch/x86/kernel/fpu/signal.c|353| <<__fpu_restore_sig>> user_xfeatures = fx_sw_user.xfeatures;
+		 *   - arch/x86/kernel/fpu/signal.c|355| <<__fpu_restore_sig>> user_xfeatures = XFEATURE_MASK_FPSSE;
+		 *   - arch/x86/kernel/fpu/signal.c|361| <<__fpu_restore_sig>> return restore_fpregs_from_user(buf_fx, user_xfeatures, fx_only,
+		 *   - arch/x86/kernel/fpu/signal.c|434| <<__fpu_restore_sig>> u64 mask = user_xfeatures | xfeatures_mask_supervisor();
+		 *   - arch/x86/kernel/fpu/xstate.c|403| <<validate_user_xstate_header>> if (hdr->xfeatures & ~fpstate->user_xfeatures)
+		 *   - arch/x86/kernel/fpu/xstate.c|1088| <<__copy_xstate_to_uabi_buf>> header.xfeatures &= fpstate->user_xfeatures;
+		 *   - arch/x86/kernel/fpu/xstate.c|1131| <<__copy_xstate_to_uabi_buf>> mask = fpstate->user_xfeatures;
+		 *   - arch/x86/kernel/fpu/xstate.c|1494| <<fpstate_realloc>> newfps->user_xfeatures = curfps->user_xfeatures | xfeatures;
+		 *   - arch/x86/kernel/fpu/xstate.h|264| <<xsave_to_user_sigframe>> u64 mask = fpstate->user_xfeatures;
+		 *   - arch/x86/kvm/cpuid.c|356| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
+		 */
 		header.xfeatures &= fpstate->user_xfeatures;
 		break;
 	}
@@ -1130,6 +1192,12 @@ void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
 	 */
 	mask = fpstate->user_xfeatures;
 
+	/*
+	 * 从FIRST_EXTENDED_XFEATURE开始, 应该不包括0和1
+	 * #define for_each_extended_xfeature(bit, mask)                           \
+	 *	   (bit) = FIRST_EXTENDED_XFEATURE;                                \
+	 *	   for_each_set_bit_from(bit, (unsigned long *)&(mask), 8 * sizeof(mask))
+	 */
 	for_each_extended_xfeature(i, mask) {
 		/*
 		 * If there was a feature or alignment gap, zero the space
@@ -1176,6 +1244,12 @@ void __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,
  *
  * It supports partial copy but @to.pos always starts from zero.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/regset.c|85| <<xfpregs_get>> copy_xstate_to_uabi_buf(to, target, XSTATE_COPY_FX);
+ *   - arch/x86/kernel/fpu/regset.c|137| <<xstateregs_get>> copy_xstate_to_uabi_buf(to, target, XSTATE_COPY_XSAVE);
+ *   - arch/x86/kernel/fpu/regset.c|340| <<fpregs_get>> copy_xstate_to_uabi_buf(mb, target, XSTATE_COPY_FP);
+ */
 void copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,
 			     enum xstate_copy_mode copy_mode)
 {
diff --git a/arch/x86/kernel/fpu/xstate.h b/arch/x86/kernel/fpu/xstate.h
index 5ad470313..10966dc0a 100644
--- a/arch/x86/kernel/fpu/xstate.h
+++ b/arch/x86/kernel/fpu/xstate.h
@@ -174,6 +174,11 @@ static inline int __xfd_enable_feature(u64 which, struct fpu_guest *guest_fpu) {
  * Uses either XSAVE or XSAVEOPT or XSAVES depending on the CPU features
  * and command line options. The choice is permanent until the next reboot.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/fpu/core.c|145| <<save_fpregs_to_fpstate>> os_xsave(fpu->fpstate);
+ *   - arch/x86/kernel/fpu/signal.c|394| <<__fpu_restore_sig>> os_xsave(fpu->fpstate);
+ */
 static inline void os_xsave(struct fpstate *fpstate)
 {
 	u64 mask = fpstate->xfeatures;
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index d4e48b4a4..cd37373d4 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -346,6 +346,24 @@ static notrace void kvm_guest_apic_eoi_write(u32 reg, u32 val)
 	apic->native_eoi_write(APIC_EOI, APIC_EOI_ACK);
 }
 
+/*
+ * [  252.313028] CPU: 4 PID: 1113 Comm: cpuhp/4 Not tainted 6.0.0 #2
+ * ... ...
+ * [  252.313033] Call Trace:
+ * [  252.313047]  <TASK>
+ * [  252.313049]  dump_stack_lvl+0x45/0x5e
+ * [  252.313064]  kvm_guest_cpu_init+0x1a/0x180
+ * [  252.313069]  kvm_cpu_online+0x15/0x30
+ * [  252.313071]  cpuhp_invoke_callback+0x2cb/0x470
+ * [  252.313075]  ? blk_mq_quiesce_queue_nowait+0x60/0x60
+ * [  252.313080]  cpuhp_thread_fun+0x8d/0x150
+ * [  252.313083]  smpboot_thread_fn+0x183/0x220
+ * [  252.313089]  ? sort_range+0x20/0x20
+ * [  252.313091]  kthread+0xe2/0x110
+ * [  252.313095]  ? kthread_complete_and_exit+0x20/0x20
+ * [  252.313098]  ret_from_fork+0x22/0x30
+ * [  252.313102]  </TASK>
+ */
 static void kvm_guest_cpu_init(void)
 {
 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
@@ -934,6 +952,9 @@ static void kvm_sev_hc_page_enc_status(unsigned long pfn, int npages, bool enc)
 			   KVM_MAP_GPA_RANGE_ENC_STAT(enc) | KVM_MAP_GPA_RANGE_PAGE_SZ_4K);
 }
 
+/*
+ * struct hypervisor_x86 x86_hyper_kvm.init.init_platform = kvm_init_platform()
+ */
 static void __init kvm_init_platform(void)
 {
 	if (cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT) &&
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 16333ba19..a361540f7 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -26,6 +26,13 @@ static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+/*
+ * 在以下使用kvm_sched_clock_offset:
+ *   - arch/x86/kernel/kvmclock.c|91| <<kvm_sched_clock_read>> return kvm_clock_read() - kvm_sched_clock_offset;
+ *   - arch/x86/kernel/kvmclock.c|98| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+ *   - arch/x86/kernel/kvmclock.c|102| <<kvm_sched_clock_init>> kvm_sched_clock_offset);
+ *   - arch/x86/kernel/kvmclock.c|104| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
+ */
 static u64 kvm_sched_clock_offset __ro_after_init;
 
 static int __init parse_no_kvmclock(char *arg)
@@ -71,30 +78,67 @@ static int kvm_set_wallclock(const struct timespec64 *now)
 	return -ENODEV;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|86| <<kvm_clock_get_cycles>> return kvm_clock_read();
+ *   - arch/x86/kernel/kvmclock.c|91| <<kvm_sched_clock_read>> return kvm_clock_read() - kvm_sched_clock_offset;
+ *   - arch/x86/kernel/kvmclock.c|98| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+ */
 static u64 kvm_clock_read(void)
 {
 	u64 ret;
 
 	preempt_disable_notrace();
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/kvmclock.c|79| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+	 *   - arch/x86/kernel/pvclock.c|137| <<pvclock_read_wallclock>> delta = pvclock_clocksource_read(vcpu_time);
+	 *   - arch/x86/xen/time.c|53| <<xen_clocksource_read>> ret = pvclock_clocksource_read(src);
+	 */
 	ret = pvclock_clocksource_read(this_cpu_pvti());
 	preempt_enable_notrace();
 	return ret;
 }
 
+/*
+ * struct clocksource kvm_clock.read = kvm_clock_get_cycles()
+ */
 static u64 kvm_clock_get_cycles(struct clocksource *cs)
 {
 	return kvm_clock_read();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|99| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ */
 static u64 kvm_sched_clock_read(void)
 {
+	/*
+	 * 在以下使用kvm_sched_clock_offset:
+	 *   - arch/x86/kernel/kvmclock.c|91| <<kvm_sched_clock_read>> return kvm_clock_read() - kvm_sched_clock_offset;
+	 *   - arch/x86/kernel/kvmclock.c|98| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+	 *   - arch/x86/kernel/kvmclock.c|102| <<kvm_sched_clock_init>> kvm_sched_clock_offset);
+	 *   - arch/x86/kernel/kvmclock.c|104| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
+	 */
 	return kvm_clock_read() - kvm_sched_clock_offset;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|317| <<kvmclock_init>> kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+ */
 static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable)
 		clear_sched_clock_stable();
+	/*
+	 * 在以下使用kvm_sched_clock_offset:
+	 *   - arch/x86/kernel/kvmclock.c|91| <<kvm_sched_clock_read>> return kvm_clock_read() - kvm_sched_clock_offset;
+	 *   - arch/x86/kernel/kvmclock.c|98| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+	 *   - arch/x86/kernel/kvmclock.c|102| <<kvm_sched_clock_init>> kvm_sched_clock_offset);
+	 *   - arch/x86/kernel/kvmclock.c|104| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) >
+	 */
 	kvm_sched_clock_offset = kvm_clock_read();
 	paravirt_set_sched_clock(kvm_sched_clock_read);
 
@@ -154,6 +198,42 @@ static int kvm_cs_enable(struct clocksource *cs)
 	return 0;
 }
 
+/*
+ * 5.4的例子
+ * crash> kvm_clock
+ * kvm_clock = $2 = {
+ *   read = 0xffffffff8f6846b0,
+ *   mask = 18446744073709551615,
+ *   mult = 8388608,
+ *   shift = 23,
+ *   max_idle_ns = 881590591483,
+ *   maxadj = 922746,
+ *   archdata = {
+ *     vclock_mode = 2
+ *   },
+ *   max_cycles = 1981102219259,
+ *   name = 0xffffffff90a40655 "kvm-clock",
+ *   list = {
+ *     next = 0xffffffff910356d8,
+ *     prev = 0xffffffff910b94c0
+ *   },
+ *   rating = 400,
+ *   enable = 0x0,
+ *   disable = 0x0,
+ *   flags = 33,
+ *   suspend = 0x0,
+ *   resume = 0x0,
+ *   mark_unstable = 0x0,
+ *   tick_stable = 0x0,
+ *   wd_list = {
+ *     next = 0xffffffff91042548,
+ *     prev = 0xffffffff91042548
+ *   },
+ *   cs_last = 0,
+ *   wd_last = 0,
+ *   owner = 0x0
+ * }
+ */
 struct clocksource kvm_clock = {
 	.name	= "kvm-clock",
 	.read	= kvm_clock_get_cycles,
@@ -166,6 +246,11 @@ EXPORT_SYMBOL_GPL(kvm_clock);
 
 static void kvm_register_clock(char *txt)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/kvmclock.c|181| <<kvm_check_and_clear_guest_paused>> struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
+	 *   - arch/x86/kernel/kvmclock.c|213| <<kvm_register_clock>> struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
+	 */
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
 	u64 pa;
 
@@ -260,6 +345,10 @@ static int __init kvm_setup_vsyscall_timeinfo(void)
 }
 early_initcall(kvm_setup_vsyscall_timeinfo);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|391| <<kvmclock_init>> kvmclock_setup_percpu, NULL) < 0) {
+ */
 static int kvmclock_setup_percpu(unsigned int cpu)
 {
 	struct pvclock_vsyscall_time_info *p = per_cpu(hv_clock_per_cpu, cpu);
@@ -284,6 +373,10 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 	return p ? 0 : -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|987| <<kvm_init_platform>> kvmclock_init();
+ */
 void __init kvmclock_init(void)
 {
 	u8 flags;
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index cec0bfa3b..97e14c23c 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -73,6 +73,12 @@ struct nmi_stats {
 
 static DEFINE_PER_CPU(struct nmi_stats, nmi_stats);
 
+/*
+ * 在以下使用ignoe_nmis:
+ *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+ *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+ *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+ */
 static int ignore_nmis __read_mostly;
 
 int unknown_nmi_panic;
@@ -101,6 +107,10 @@ static int __init nmi_warning_debugfs(void)
 }
 fs_initcall(nmi_warning_debugfs);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|145| <<nmi_handle>> nmi_check_duration(a, delta);
+ */
 static void nmi_check_duration(struct nmiaction *action, u64 duration)
 {
 	int remainder_ns, decimal_msecs;
@@ -118,6 +128,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|234| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|261| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|310| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|359| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -152,6 +169,33 @@ static int nmi_handle(unsigned int type, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_handle);
 
+/*
+ * 5.4上看到这两个NMI LOCAL的.
+ *
+ * crash> nmiaction ffffffffa6617ec0
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6641a80,
+ *     prev = 0xffffffffa6634368
+ *   },
+ *   handler = 0xffffffffa4c07aa0, --> perf_event_nmi_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa60b4811 "PMI"
+ * }
+ * crash> nmiaction ffffffffa6641a80
+ * struct nmiaction {
+ *   list = {
+ *     next = 0xffffffffa6634368,
+ *     prev = 0xffffffffa6617ec0
+ *   },
+ *   handler = 0xffffffffa4c7aeb0, --> nmi_cpu_backtrace_handler()
+ *   max_duration = 0,
+ *   flags = 0,
+ *   name = 0xffffffffa6041a34 "arch_bt"
+ * }
+ */
+
 int __register_nmi_handler(unsigned int type, struct nmiaction *action)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -213,6 +257,10 @@ void unregister_nmi_handler(unsigned int type, const char *name)
 }
 EXPORT_SYMBOL_GPL(unregister_nmi_handler);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|370| <<default_do_nmi>> pci_serr_error(reason, regs);
+ */
 static void
 pci_serr_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -234,6 +282,10 @@ pci_serr_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(pci_serr_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|372| <<default_do_nmi>> io_check_error(reason, regs);
+ */
 static void
 io_check_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -274,11 +326,25 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(io_check_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|419| <<default_do_nmi>> unknown_nmi_error(reason, regs);
+ */
 static void
 unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 {
 	int handled;
 
+	/*
+	 * 注册了NMI_UNKNOWN的:
+	 *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+	 *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+	 *   - arch/x86/kernel/nmi_selftest.c|55| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+	 *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+	 *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+	 *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+	 */
+
 	/*
 	 * Use 'false' as back-to-back NMIs are dealt with one level up.
 	 * Of course this makes having multiple 'unknown' handlers useless
@@ -304,8 +370,19 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 NOKPROBE_SYMBOL(unknown_nmi_error);
 
 static DEFINE_PER_CPU(bool, swallow_nmi);
+/*
+ * 在以下使用last_nmi_rip:
+ *   - arch/x86/kernel/nmi.c|329| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+ *   - arch/x86/kernel/nmi.c|350| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+ *   - arch/x86/kernel/nmi.c|355| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+ *   - arch/x86/kernel/nmi.c|575| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+ */
 static DEFINE_PER_CPU(unsigned long, last_nmi_rip);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|535| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> default_do_nmi(regs);
+ */
 static noinstr void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;
@@ -472,6 +549,14 @@ enum nmi_states {
 	NMI_EXECUTING,
 	NMI_LATCHED,
 };
+/*
+ * 在以下使用nmi_state:
+ *   - arch/x86/kernel/nmi.c|497| <<global>> static DEFINE_PER_CPU(enum nmi_states, nmi_state);
+ *   - arch/x86/kernel/nmi.c|514| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_read(nmi_state) != NMI_NOT_RUNNING) {
+ *   - arch/x86/kernel/nmi.c|515| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_LATCHED);
+ *   - arch/x86/kernel/nmi.c|518| <<DEFINE_IDTENTRY_RAW>> this_cpu_write(nmi_state, NMI_EXECUTING);
+ *   - arch/x86/kernel/nmi.c|545| <<DEFINE_IDTENTRY_RAW>> if (this_cpu_dec_return(nmi_state))
+ */
 static DEFINE_PER_CPU(enum nmi_states, nmi_state);
 static DEFINE_PER_CPU(unsigned long, nmi_cr2);
 static DEFINE_PER_CPU(unsigned long, nmi_dr7);
@@ -509,6 +594,12 @@ DEFINE_IDTENTRY_RAW(exc_nmi)
 
 	inc_irq_stat(__nmi_count);
 
+	/*
+	 * 在以下使用ignoe_nmis:
+	 *   - arch/x86/kernel/nmi.c|512| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (!ignore_nmis)
+	 *   - arch/x86/kernel/nmi.c|542| <<stop_nmi>> ignore_nmis++;
+	 *   - arch/x86/kernel/nmi.c|547| <<restart_nmi>> ignore_nmis--;
+	 */
 	if (!ignore_nmis)
 		default_do_nmi(regs);
 
@@ -548,6 +639,10 @@ void restart_nmi(void)
 }
 
 /* reset the back-to-back NMI logic */
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|709| <<arch_cpu_idle_enter>> local_touch_nmi();
+ */
 void local_touch_nmi(void)
 {
 	__this_cpu_write(last_nmi_rip, 0);
diff --git a/arch/x86/kernel/nmi_selftest.c b/arch/x86/kernel/nmi_selftest.c
index a1a96df3d..d1a391bb2 100644
--- a/arch/x86/kernel/nmi_selftest.c
+++ b/arch/x86/kernel/nmi_selftest.c
@@ -23,6 +23,15 @@
 #define FAILURE		1
 #define TIMEOUT		2
 
+/*
+ * 在以下使用nmi_fail:
+ *   - arch/x86/kernel/nmi_selftest.c|71| <<test_nmi_ipi>> nmi_fail = FAILURE;
+ *   - arch/x86/kernel/nmi_selftest.c|89| <<test_nmi_ipi>> nmi_fail = TIMEOUT;
+ *   - arch/x86/kernel/nmi_selftest.c|110| <<reset_nmi>> nmi_fail = 0;
+ *   - arch/x86/kernel/nmi_selftest.c|119| <<dotest>> if (nmi_fail != expected) {
+ *   - arch/x86/kernel/nmi_selftest.c|122| <<dotest>> if (nmi_fail == FAILURE)
+ *   - arch/x86/kernel/nmi_selftest.c|124| <<dotest>> else if (nmi_fail == TIMEOUT)
+ */
 static int __initdata nmi_fail;
 
 /* check to see if NMI IPIs work on this machine */
@@ -62,6 +71,29 @@ static int __init test_nmi_ipi_callback(unsigned int val, struct pt_regs *regs)
         return NMI_DONE;
 }
 
+/*
+ * called by:
+ 49  *   - arch/x86/events/amd/ibs.c|918| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ 50  *   - arch/x86/events/core.c|2136| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ 51  *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ 52  *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ 53  *   - arch/x86/kernel/cpu/mshyperv.c|369| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ 54  *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ 55  *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ 56  *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ 57  *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ 58  *   - arch/x86/kernel/smp.c|143| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ 59  *   - arch/x86/kernel/smpboot.c|1026| <<wakeup_cpu_via_init_nmi>> boot_error = register_nmi_handler(NMI_LOCAL,
+ 60  *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ 61  *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ 62  *   - drivers/acpi/apei/ghes.c|1179| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ 63  *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ 64  *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ 65  *   - drivers/watchdog/hpwdt.c|247| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ 66  *   - drivers/watchdog/hpwdt.c|250| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ 67  *   - drivers/watchdog/hpwdt.c|253| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
+
 static void __init test_nmi_ipi(struct cpumask *mask)
 {
 	unsigned long timeout;
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index eda37df01..10c3136dc 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -64,6 +64,12 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|79| <<kvm_clock_read>> ret = pvclock_clocksource_read(this_cpu_pvti());
+ *   - arch/x86/kernel/pvclock.c|137| <<pvclock_read_wallclock>> delta = pvclock_clocksource_read(vcpu_time);
+ *   - arch/x86/xen/time.c|53| <<xen_clocksource_read>> ret = pvclock_clocksource_read(src);
+ */
 u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
diff --git a/arch/x86/kernel/sev.c b/arch/x86/kernel/sev.c
index a428c6233..b267bb976 100644
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@ -1485,6 +1485,10 @@ static enum es_result vc_do_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt,
  * rare operation. If it turns out to be a performance problem the split
  * operations can be moved to memcpy_fromio() and memcpy_toio().
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1604| <<vc_handle_mmio>> ret = vc_handle_mmio_movs(ctxt, bytes);
+ */
 static enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,
 					  unsigned int bytes)
 {
@@ -1533,6 +1537,10 @@ static enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,
 		return ES_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1805| <<vc_handle_exitcode>> result = vc_handle_mmio(ghcb, ctxt);
+ */
 static enum es_result vc_handle_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt)
 {
 	struct insn *insn = &ctxt->insn;
@@ -1753,6 +1761,11 @@ static enum es_result vc_handle_trap_ac(struct ghcb *ghcb,
 	return ES_EXCEPTION;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1878| <<vc_raw_handle_exception>> result = vc_handle_exitcode(&ctxt, ghcb, error_code);
+ *   - arch/x86/kernel/sev.c|2017| <<handle_vc_boot_ghcb>> result = vc_handle_exitcode(&ctxt, boot_ghcb, exit_code);
+ */
 static enum es_result vc_handle_exitcode(struct es_em_ctxt *ctxt,
 					 struct ghcb *ghcb,
 					 unsigned long exit_code)
@@ -1861,6 +1874,11 @@ static __always_inline bool vc_from_invalid_context(struct pt_regs *regs)
 	return is_vc2_stack(sp) && !is_vc2_stack(prev_sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/sev.c|1962| <<DEFINE_IDTENTRY_VC_KERNEL(exc_vmm_communication)>> if (!vc_raw_handle_exception(regs, error_code)) {
+ *   - arch/x86/kernel/sev.c|1994| <<DEFINE_IDTENTRY_VC_USER(exc_vmm_communication)>> if (!vc_raw_handle_exception(regs, error_code)) {
+ */
 static bool vc_raw_handle_exception(struct pt_regs *regs, unsigned long error_code)
 {
 	struct ghcb_state state;
diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index 06db901fa..63bf81a81 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -235,6 +235,13 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)
 {
 	ack_APIC_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
+	/*
+	 * 在以下增加irq_call_count:
+	 *   - arch/x86/kernel/smp.c|238| <<DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/kernel/smp.c|247| <<DEFINE_IDTENTRY_SYSVEC(sysvec_call_function_single)>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/xen/smp.c|248| <<xen_call_function_interrupt>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/xen/smp.c|256| <<xen_call_function_single_interrupt>> inc_irq_stat(irq_call_count);
+	 */
 	inc_irq_stat(irq_call_count);
 	generic_smp_call_function_interrupt();
 	trace_call_function_exit(CALL_FUNCTION_VECTOR);
@@ -244,6 +251,13 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_call_function_single)
 {
 	ack_APIC_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
+	/*
+	 * 在以下增加irq_call_count:
+	 *   - arch/x86/kernel/smp.c|238| <<DEFINE_IDTENTRY_SYSVEC(sysvec_call_function)>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/kernel/smp.c|247| <<DEFINE_IDTENTRY_SYSVEC(sysvec_call_function_single)>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/xen/smp.c|248| <<xen_call_function_interrupt>> inc_irq_stat(irq_call_count);
+	 *   - arch/x86/xen/smp.c|256| <<xen_call_function_single_interrupt>> inc_irq_stat(irq_call_count);
+	 */
 	inc_irq_stat(irq_call_count);
 	generic_smp_call_function_single_interrupt();
 	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index f24227bc3..b5cf6ba1a 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -151,6 +151,10 @@ static inline void smpboot_restore_warm_reset_vector(void)
  * Report back to the Boot Processor during boot time or to the caller processor
  * during CPU online.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|233| <<start_secondary>> smp_callin();
+ */
 static void smp_callin(void)
 {
 	int cpuid;
@@ -226,6 +230,9 @@ static void notrace start_secondary(void *unused)
 #endif
 	cpu_init_secondary();
 	rcu_cpu_starting(raw_smp_processor_id());
+	/*
+	 * kvm_setup_secondary_clock()
+	 */
 	x86_cpuinit.early_percpu_clock_init();
 	smp_callin();
 
@@ -381,6 +388,11 @@ void __init smp_store_boot_cpu_info(void)
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|178| <<smp_callin>> smp_store_cpu_info(cpuid);
+ *   - arch/x86/xen/smp_pv.c|74| <<cpu_bringup>> smp_store_cpu_info(cpu);
+ */
 void smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
@@ -1073,6 +1085,13 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		       int *cpu0_nmi_registered)
 {
 	/* start_ip had better be page-aligned! */
+	/*
+	 * 在以下使用real_mode_header->trampoline_start:
+	 *   - arch/x86/kernel/sev.c|1007| <<wakeup_cpu_via_vmgexit>> if (WARN_ONCE(start_ip != real_mode_header->trampoline_start,
+	 *   - arch/x86/kernel/sev.c|1166| <<sev_es_setup_ap_jump_table>> startup_cs = (u16)(rmh->trampoline_start >> 4);
+	 *   - arch/x86/kernel/sev.c|1168| <<sev_es_setup_ap_jump_table>> rmh->trampoline_start);
+	 *   - arch/x86/kernel/smpboot.c|1076| <<do_boot_cpu>> unsigned long start_ip = real_mode_header->trampoline_start;
+	 */
 	unsigned long start_ip = real_mode_header->trampoline_start;
 
 	unsigned long boot_error = 0;
@@ -1144,6 +1163,14 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		boot_error = -1;
 		timeout = jiffies + 10*HZ;
 		while (time_before(jiffies, timeout)) {
+			/*
+			 * 在以下使用cpu_initialized_mask:
+			 *   - arch/x86/kernel/cpu/common.c|173| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+			 *   - arch/x86/kernel/cpu/common.c|2138| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+			 *   - arch/x86/kernel/smpboot.c|1122| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+			 *   - arch/x86/kernel/smpboot.c|1147| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+			 *   - arch/x86/kernel/smpboot.c|1644| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+			 */
 			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
 				/*
 				 * Tell AP to proceed with initialization
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index cafacb2e5..bce0606af 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -42,10 +42,32 @@ EXPORT_SYMBOL(tsc_khz);
  * TSC can be unstable due to cpufreq or due to unsynced TSCs
  */
 static int __read_mostly tsc_unstable;
+/*
+ * 在以下使用tsc_early_khz:
+ *   - arch/x86/kernel/tsc.c|84| <<tsc_early_khz_setup>> return kstrtouint(buf, 0, &tsc_early_khz);
+ *   - arch/x86/kernel/tsc.c|1524| <<determine_cpu_tsc_frequencies>> if (tsc_early_khz)
+ *   - arch/x86/kernel/tsc.c|1525| <<determine_cpu_tsc_frequencies>> tsc_khz = tsc_early_khz;
+ */
 static unsigned int __initdata tsc_early_khz;
 
+/*
+ * 在以下使用__use_tsc:
+ *   - arch/x86/kernel/tsc.c|47| <<global>> static DEFINE_STATIC_KEY_FALSE(__use_tsc);
+ *   - arch/x86/kernel/tsc.c|261| <<native_sched_clock>> if (static_branch_likely(&__use_tsc)) {
+ *   - arch/x86/kernel/tsc.c|1575| <<tsc_enable_sched_clock>> static_branch_enable(&__use_tsc);
+ */
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
+/*
+ * 在以下使用tsc_clocksource_reliable:
+ *   - arch/x86/kernel/tsc.c|340| <<tsc_setup>> tsc_clocksource_reliable = 1;
+ *   - arch/x86/kernel/tsc.c|1279| <<check_system_tsc_reliable>> tsc_clocksource_reliable = 1;
+ *   - arch/x86/kernel/tsc.c|1283| <<check_system_tsc_reliable>> tsc_clocksource_reliable = 1;
+ *   - arch/x86/kernel/tsc.c|1320| <<unsynchronized_tsc>> if (tsc_clocksource_reliable)
+ *   - arch/x86/kernel/tsc.c|1628| <<tsc_init>> if (tsc_clocksource_reliable || no_tsc_watchdog)
+ *   - arch/x86/kernel/tsc_sync.c|110| <<start_sync_check_timer>> if (!cpu_feature_enabled(X86_FEATURE_TSC_ADJUST) || tsc_clocksource_reliable)
+ *   - arch/x86/kernel/tsc_sync.c|460| <<check_tsc_sync_target>> if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {
+ */
 int tsc_clocksource_reliable;
 
 static u32 art_to_tsc_numerator;
@@ -59,6 +81,24 @@ struct cyc2ns {
 
 }; /* fits one cacheline */
 
+/*
+ * 在以下使用percpu的cyc2ns:
+ *   - arch/x86/kernel/tsc.c|79| <<global>> static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
+ *   - arch/x86/kernel/tsc.c|77| <<cyc2ns_read_begin>> seq = this_cpu_read(cyc2ns.seq.seqcount.sequence);
+ *   - arch/x86/kernel/tsc.c|80| <<cyc2ns_read_begin>> data->cyc2ns_offset = this_cpu_read(cyc2ns.data[idx].cyc2ns_offset);
+ *   - arch/x86/kernel/tsc.c|81| <<cyc2ns_read_begin>> data->cyc2ns_mul = this_cpu_read(cyc2ns.data[idx].cyc2ns_mul);
+ *   - arch/x86/kernel/tsc.c|82| <<cyc2ns_read_begin>> data->cyc2ns_shift = this_cpu_read(cyc2ns.data[idx].cyc2ns_shift);
+ *   - arch/x86/kernel/tsc.c|84| <<cyc2ns_read_begin>> } while (unlikely(seq != this_cpu_read(cyc2ns.seq.seqcount.sequence)));
+ *   - arch/x86/kernel/tsc.c|146| <<__set_cyc2ns_scale>> struct cyc2ns *c2n;
+ *   - arch/x86/kernel/tsc.c|172| <<__set_cyc2ns_scale>> c2n = per_cpu_ptr(&cyc2ns, cpu);
+ *   - arch/x86/kernel/tsc.c|204| <<cyc2ns_init_boot_cpu>> struct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);
+ *   - arch/x86/kernel/tsc.c|218| <<cyc2ns_init_secondary_cpus>> struct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);
+ *   - arch/x86/kernel/tsc.c|224| <<cyc2ns_init_secondary_cpus>> c2n = per_cpu_ptr(&cyc2ns, cpu);
+ *   - arch/x86/kernel/tsc.c|970| <<tsc_restore_sched_clock_state>> this_cpu_write(cyc2ns.data[0].cyc2ns_offset, 0);
+ *   - arch/x86/kernel/tsc.c|971| <<tsc_restore_sched_clock_state>> this_cpu_write(cyc2ns.data[1].cyc2ns_offset, 0);
+ *   - arch/x86/kernel/tsc.c|976| <<tsc_restore_sched_clock_state>> per_cpu(cyc2ns.data[0].cyc2ns_offset, cpu) = offset;
+ *   - arch/x86/kernel/tsc.c|977| <<tsc_restore_sched_clock_state>> per_cpu(cyc2ns.data[1].cyc2ns_offset, cpu) = offset;
+ */
 static DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);
 
 static int __init tsc_early_khz_setup(char *buf)
@@ -67,6 +107,11 @@ static int __init tsc_early_khz_setup(char *buf)
 }
 early_param("tsc_early_khz", tsc_early_khz_setup);
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2783| <<arch_perf_update_userpage>> cyc2ns_read_begin(&data);
+ *   - arch/x86/kernel/tsc.c|152| <<cycles_2_ns>> cyc2ns_read_begin(&data);
+ */
 __always_inline void cyc2ns_read_begin(struct cyc2ns_data *data)
 {
 	int seq, idx;
@@ -113,8 +158,21 @@ __always_inline void cyc2ns_read_end(void)
  *                      -johnstul@us.ibm.com "math is hard, lets go shopping!"
  */
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|137| <<__set_cyc2ns_scale>> ns_now = cycles_2_ns(tsc_now);
+ *   - arch/x86/kernel/tsc.c|224| <<native_sched_clock>> return cycles_2_ns(tsc_now);
+ *   - arch/x86/kernel/tsc.c|245| <<native_sched_clock_from_tsc>> return cycles_2_ns(tsc);
+ */
 static __always_inline unsigned long long cycles_2_ns(unsigned long long cyc)
 {
+	/*
+	 * struct cyc2ns_data {
+	 *     u32 cyc2ns_mul;
+	 *     u32 cyc2ns_shift;
+	 *     u64 cyc2ns_offset;
+	 * };
+	 */
 	struct cyc2ns_data data;
 	unsigned long long ns;
 
@@ -128,6 +186,11 @@ static __always_inline unsigned long long cycles_2_ns(unsigned long long cyc)
 	return ns;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|183| <<set_cyc2ns_scale>> __set_cyc2ns_scale(khz, cpu, tsc_now);
+ *   - arch/x86/kernel/tsc.c|197| <<cyc2ns_init_boot_cpu>> __set_cyc2ns_scale(tsc_khz, smp_processor_id(), rdtsc());
+ */
 static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
 {
 	unsigned long long ns_now;
@@ -166,6 +229,11 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 	c2n->data[1] = data;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1014| <<time_cpufreq_notifier>> set_cyc2ns_scale(tsc_khz, freq->policy->cpu, rdtsc());
+ *   - arch/x86/kernel/tsc.c|1432| <<tsc_refine_calibration_work>> set_cyc2ns_scale(tsc_khz, cpu, tsc_stop);
+ */
 static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
 {
 	unsigned long flags;
@@ -934,6 +1002,9 @@ void tsc_save_sched_clock_state(void)
  * that sched_clock() continues from the point where it was left off during
  * suspend.
  */
+/*
+ * struct x86_platform_ops x86_platform.restore_sched_clock_state = tsc_restore_sched_clock_state()
+ */
 void tsc_restore_sched_clock_state(void)
 {
 	unsigned long long offset;
@@ -1141,11 +1212,58 @@ static struct clocksource clocksource_tsc_early = {
 	.list			= LIST_HEAD_INIT(clocksource_tsc_early.list),
 };
 
+/*
+ * crash> clocksource_tsc
+ * clocksource_tsc = $3 = {
+ *   read = 0xffffffff8f63f620,
+ *   mask = 18446744073709551615,
+ *   mult = 4204163,
+ *   shift = 23,
+ *   max_idle_ns = 881590799062,
+ *   maxadj = 462457,
+ *   archdata = {
+ *     vclock_mode = 1
+ *   },
+ *   max_cycles = 3952913259213,
+ *   name = 0xffffffff90a5c529 "tsc",
+ *   list = {
+ *     next = 0xffffffff91042398,
+ *     prev = 0xffffffff910424f8
+ *   },
+ *   rating = 300,
+ *   enable = 0x0,
+ *   disable = 0x0,
+ *   flags = 51,
+ *   suspend = 0x0,
+ *   resume = 0xffffffff8f63f810,
+ *   mark_unstable = 0xffffffff8f63f980,
+ *   tick_stable = 0xffffffff8f63f950,
+ *   wd_list = {
+ *     next = 0xffffffff910b9480,
+ *     prev = 0xffffffff910b9480
+ *   },
+ *   cs_last = 5837572713660022,
+ *   wd_last = 2925638053814702,
+ *   owner = 0x0
+ * }
+ */
 /*
  * Must mark VALID_FOR_HRES early such that when we unregister tsc_early
  * this one will immediately take over. We will only register if TSC has
  * been found good.
  */
+/*
+ * 在以下使用clocksource_tsc:
+ *   - arch/x86/kernel/tsc.c|1228| <<global>> static struct clocksource clocksource_tsc = {
+ *   - arch/x86/kernel/tsc.c|1242| <<global>> .list = LIST_HEAD_INIT(clocksource_tsc.list),
+ *   - arch/x86/kernel/tsc.c|1257| <<mark_tsc_unstable>> clocksource_mark_unstable(&clocksource_tsc);
+ *   - arch/x86/kernel/tsc.c|1265| <<tsc_disable_clocksource_watchdog>> clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
+ *   - arch/x86/kernel/tsc.c|1477| <<tsc_refine_calibration_work>> art_related_clocksource = &clocksource_tsc;
+ *   - arch/x86/kernel/tsc.c|1478| <<tsc_refine_calibration_work>> clocksource_register_khz(&clocksource_tsc, tsc_khz);
+ *   - arch/x86/kernel/tsc.c|1493| <<init_tsc_clocksource>> clocksource_tsc.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
+ *   - arch/x86/kernel/tsc.c|1501| <<init_tsc_clocksource>> art_related_clocksource = &clocksource_tsc;
+ *   - arch/x86/kernel/tsc.c|1502| <<init_tsc_clocksource>> clocksource_register_khz(&clocksource_tsc, tsc_khz);
+ */
 static struct clocksource clocksource_tsc = {
 	.name			= "tsc",
 	.rating			= 300,
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 2796dde06..ec895fd77 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -30,6 +30,21 @@
  * Unlike "struct cpuinfo_x86.x86_capability", kvm_cpu_caps doesn't need to be
  * aligned to sizeof(unsigned long) because it's not accessed via bitops.
  */
+/*
+ * 在以下使用kvm_cpu_caps[NR_KVM_CPU_CAPS]:
+ *   - arch/x86/kvm/cpuid.c|535| <<__kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);
+ *   - arch/x86/kvm/cpuid.c|544| <<kvm_cpu_cap_init_scattered>> kvm_cpu_caps[leaf] = mask;
+ *   - arch/x86/kvm/cpuid.c|554| <<kvm_cpu_cap_mask>> kvm_cpu_caps[leaf] &= mask;
+ *   - arch/x86/kvm/cpuid.c|570| <<kvm_set_cpu_caps>> memset(kvm_cpu_caps, 0, sizeof(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.c|572| <<kvm_set_cpu_caps>> BUILD_BUG_ON(sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)) >
+ *   - arch/x86/kvm/cpuid.c|575| <<kvm_set_cpu_caps>> memcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,
+ *   - arch/x86/kvm/cpuid.c|576| <<kvm_set_cpu_caps>> sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)));
+ *   - arch/x86/kvm/cpuid.h|90| <<cpuid_entry_override>> BUILD_BUG_ON(leaf >= ARRAY_SIZE(kvm_cpu_caps));
+ *   - arch/x86/kvm/cpuid.h|91| <<cpuid_entry_override>> *reg = kvm_cpu_caps[leaf];
+ *   - arch/x86/kvm/cpuid.h|215| <<kvm_cpu_cap_clear>> kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|223| <<kvm_cpu_cap_set>> kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+ *   - arch/x86/kvm/cpuid.h|231| <<kvm_cpu_cap_get>> return kvm_cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+ */
 u32 kvm_cpu_caps[NR_KVM_CPU_CAPS] __read_mostly;
 EXPORT_SYMBOL_GPL(kvm_cpu_caps);
 
@@ -234,6 +249,11 @@ void kvm_update_pv_runtime(struct kvm_vcpu *vcpu)
  * Calculate guest's supported XCR0 taking into account guest CPUID data and
  * KVM's supported XCR0 (comprised of host's XCR0 and KVM_SUPPORTED_XCR0).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|267| <<__kvm_update_cpuid_runtime>> u64 guest_supported_xcr0 = cpuid_get_supported_xcr0(entries, nent);
+ *   - arch/x86/kvm/cpuid.c|357| <<kvm_vcpu_after_set_cpuid>> cpuid_get_supported_xcr0(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);
+ */
 static u64 cpuid_get_supported_xcr0(struct kvm_cpuid_entry2 *entries, int nent)
 {
 	struct kvm_cpuid_entry2 *best;
@@ -311,6 +331,10 @@ void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_update_cpuid_runtime);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|421| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -326,6 +350,23 @@ static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 		kvm_apic_set_version(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->guest_supported_xcr0:
+	 *   - arch/x86/kvm/cpuid.c|267| <<__kvm_update_cpuid_runtime>> u64 guest_supported_xcr0 = cpuid_get_supported_xcr0(entries, nent);
+	 *   - arch/x86/kvm/cpuid.c|317| <<__kvm_update_cpuid_runtime>> best->ecx &= guest_supported_xcr0 & 0xffffffff;
+	 *   - arch/x86/kvm/cpuid.c|318| <<__kvm_update_cpuid_runtime>> best->edx &= guest_supported_xcr0 >> 32;
+	 *   - arch/x86/kvm/cpuid.c|348| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_supported_xcr0 =
+	 *   - arch/x86/kvm/cpuid.c|356| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
+	 *   - arch/x86/kvm/x86.c|1025| <<kvm_guest_supported_xfd>> return vcpu->arch.guest_supported_xcr0 & XFEATURE_MASK_USER_DYNAMIC;
+	 *   - arch/x86/kvm/x86.c|1048| <<__kvm_set_xcr>> valid_bits = vcpu->arch.guest_supported_xcr0 | XFEATURE_MASK_FP;
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct fpu_guest guest_fpu;
+	 *       -> struct fpstate *fpstate;
+	 *    -> u64 xcr0;
+	 *    -> u64 guest_supported_xcr0;
+	 */
 	vcpu->arch.guest_supported_xcr0 =
 		cpuid_get_supported_xcr0(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);
 
@@ -334,6 +375,23 @@ static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	 * XSAVE/XCRO are not exposed to the guest, and even if XSAVE isn't
 	 * supported by the host.
 	 */
+	/*
+	 * 在以下使用fpstate->user_xfeatures:
+	 *   - arch/x86/kernel/fpu/core.c|527| <<__fpstate_reset>> fpstate->user_xfeatures = fpu_user_cfg.default_features;
+	 *   - arch/x86/kernel/fpu/signal.c|110| <<save_sw_bytes>> sw_bytes->xfeatures = fpstate->user_xfeatures;
+	 *   - arch/x86/kernel/fpu/signal.c|288| <<restore_fpregs_from_user>> ret = __restore_fpregs_from_user(buf, fpu->fpstate->user_xfeatures,
+	 *   - arch/x86/kernel/fpu/signal.c|343| <<__fpu_restore_sig>> u64 user_xfeatures = 0;
+	 *   - arch/x86/kernel/fpu/signal.c|353| <<__fpu_restore_sig>> user_xfeatures = fx_sw_user.xfeatures;
+	 *   - arch/x86/kernel/fpu/signal.c|355| <<__fpu_restore_sig>> user_xfeatures = XFEATURE_MASK_FPSSE;
+	 *   - arch/x86/kernel/fpu/signal.c|361| <<__fpu_restore_sig>> return restore_fpregs_from_user(buf_fx, user_xfeatures, fx_only,
+	 *   - arch/x86/kernel/fpu/signal.c|434| <<__fpu_restore_sig>> u64 mask = user_xfeatures | xfeatures_mask_supervisor();
+	 *   - arch/x86/kernel/fpu/xstate.c|403| <<validate_user_xstate_header>> if (hdr->xfeatures & ~fpstate->user_xfeatures)
+	 *   - arch/x86/kernel/fpu/xstate.c|1088| <<__copy_xstate_to_uabi_buf>> header.xfeatures &= fpstate->user_xfeatures;
+	 *   - arch/x86/kernel/fpu/xstate.c|1131| <<__copy_xstate_to_uabi_buf>> mask = fpstate->user_xfeatures;
+	 *   - arch/x86/kernel/fpu/xstate.c|1494| <<fpstate_realloc>> newfps->user_xfeatures = curfps->user_xfeatures | xfeatures;
+	 *   - arch/x86/kernel/fpu/xstate.h|264| <<xsave_to_user_sigframe>> u64 mask = fpstate->user_xfeatures;
+	 *   - arch/x86/kvm/cpuid.c|356| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
+	 */
 	vcpu->arch.guest_fpu.fpstate->user_xfeatures = vcpu->arch.guest_supported_xcr0 |
 						       XFEATURE_MASK_FPSSE;
 
@@ -382,6 +440,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|462| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|488| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
@@ -513,6 +576,11 @@ int kvm_vcpu_ioctl_get_cpuid2(struct kvm_vcpu *vcpu,
 }
 
 /* Mask kvm_cpu_caps for @leaf with the raw CPUID capabilities of this CPU. */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|561| <<kvm_cpu_cap_init_scattered>> __kvm_cpu_cap_mask(leaf);
+ *   - arch/x86/kvm/cpuid.c|571| <<kvm_cpu_cap_mask>> __kvm_cpu_cap_mask(leaf);
+ */
 static __always_inline void __kvm_cpu_cap_mask(unsigned int leaf)
 {
 	const struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);
@@ -526,6 +594,10 @@ static __always_inline void __kvm_cpu_cap_mask(unsigned int leaf)
 	kvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|719| <<kvm_set_cpu_caps>> kvm_cpu_cap_init_scattered(CPUID_12_EAX, SF(SGX1) | SF(SGX2));
+ */
 static __always_inline
 void kvm_cpu_cap_init_scattered(enum kvm_only_cpuid_leafs leaf, u32 mask)
 {
@@ -743,6 +815,18 @@ struct kvm_cpuid_array {
 	int nent;
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|869| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, 0);
+ *   - arch/x86/kvm/cpuid.c|908| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, i);
+ *   - arch/x86/kvm/cpuid.c|928| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, 1);
+ *   - arch/x86/kvm/cpuid.c|977| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, i);
+ *   - arch/x86/kvm/cpuid.c|993| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, 1);
+ *   - arch/x86/kvm/cpuid.c|1017| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, i);
+ *   - arch/x86/kvm/cpuid.c|1056| <<__do_cpuid_func>> entry = do_host_cpuid(array, function, 1);
+ *   - arch/x86/kvm/cpuid.c|1080| <<__do_cpuid_func>> if (!do_host_cpuid(array, function, i))
+ *   - arch/x86/kvm/cpuid.c|1092| <<__do_cpuid_func>> if (!do_host_cpuid(array, function, i))
+ */
 static struct kvm_cpuid_entry2 *do_host_cpuid(struct kvm_cpuid_array *array,
 					      u32 function, u32 index)
 {
@@ -823,6 +907,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1270| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -1232,6 +1320,11 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1285| <<get_cpuid_func>> r = do_cpuid_func(array, func, type);
+ *   - arch/x86/kvm/cpuid.c|1291| <<get_cpuid_func>> r = do_cpuid_func(array, func, type);
+ */
 static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			 unsigned int type)
 {
@@ -1243,6 +1336,10 @@ static int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 
 #define CENTAUR_CPUID_SIGNATURE 0xC0000000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1354| <<kvm_dev_ioctl_get_cpuid>> r = get_cpuid_func(&array, funcs[i], type);
+ */
 static int get_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 			  unsigned int type)
 {
@@ -1259,6 +1356,9 @@ static int get_cpuid_func(struct kvm_cpuid_array *array, u32 func,
 
 	limit = array->entries[array->nent - 1].eax;
 	for (func = func + 1; func <= limit; ++func) {
+		/*
+		 * 这里才是重要的
+		 */
 		r = do_cpuid_func(array, func, type);
 		if (r)
 			break;
@@ -1294,6 +1394,10 @@ static bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,
 	return false;
 }
 
+/*
+ * called by:处理KVM_GET_SUPPORTED_CPUID和KVM_GET_EMULATED_CPUID
+ *   - arch/x86/kvm/x86.c|4635| <<kvm_arch_dev_ioctl>> r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,
+ */
 int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 			    struct kvm_cpuid_entry2 __user *entries,
 			    unsigned int type)
@@ -1421,6 +1525,11 @@ get_out_of_range_cpuid_entry(struct kvm_vcpu *vcpu, u32 *fn_ptr, u32 index)
 	return kvm_find_cpuid_entry_index(vcpu, basic->eax, index);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1528| <<kvm_emulate_cpuid>> kvm_cpuid(vcpu, &eax, &ebx, &ecx, &edx, false);
+ *   - arch/x86/kvm/x86.c|8078| <<emulator_get_cpuid>> return kvm_cpuid(emul_to_vcpu(ctxt), eax, ebx, ecx, edx, exact_only);
+ */
 bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 	       u32 *ecx, u32 *edx, bool exact_only)
 {
@@ -1470,6 +1579,11 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 }
 EXPORT_SYMBOL_GPL(kvm_cpuid);
 
+/*
+ * 在以下使用kvm_emulate_cpuid():
+ *   - arch/x86/kvm/svm/svm.c|3184| <<global>> [SVM_EXIT_CPUID] = kvm_emulate_cpuid,
+ *   - arch/x86/kvm/vmx/vmx.c|5975| <<global>> [EXIT_REASON_CPUID] = kvm_emulate_cpuid,
+ */
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
 {
 	u32 eax, ebx, ecx, edx;
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index b1658c0de..1ea2dba5d 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,11 +42,30 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|52| <<kvm_vcpu_is_illegal_gpa>> return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|58| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/svm/nested.c|261| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|262| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|805| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|324| <<__nested_vmcb_check_save>> CC(kvm_vcpu_is_illegal_gpa(vcpu, save->cr3)))
+ *   - arch/x86/kvm/svm/nested.c|522| <<nested_svm_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3)))
+ *   - arch/x86/kvm/vmx/nested.c|1113| <<nested_vmx_load_cr3>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3))) {
+ *   - arch/x86/kvm/vmx/nested.c|2695| <<nested_vmx_check_eptp>> if (CC(kvm_vcpu_is_illegal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/nested.c|2890| <<nested_vmx_check_host_state>> CC(kvm_vcpu_is_illegal_gpa(vcpu, vmcs12->host_cr3)))
+ *   - arch/x86/kvm/vmx/vmx.c|5669| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/x86.c|1239| <<kvm_set_cr3>> if (kvm_vcpu_is_illegal_gpa(vcpu, cr3))
+ *   - arch/x86/kvm/x86.c|11183| <<kvm_is_valid_sregs>> if (kvm_vcpu_is_illegal_gpa(vcpu, sregs->cr3))
+ */
 static inline bool kvm_vcpu_is_illegal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !kvm_vcpu_is_legal_gpa(vcpu, gpa);
@@ -63,6 +82,23 @@ static inline bool page_address_valid(struct kvm_vcpu *vcpu, gpa_t gpa)
 	return kvm_vcpu_is_legal_aligned_gpa(vcpu, gpa, PAGE_SIZE);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|875| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_1_EDX);
+ *   - arch/x86/kvm/cpuid.c|876| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_1_ECX);
+ *   - arch/x86/kvm/cpuid.c|918| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_0_EBX);
+ *   - arch/x86/kvm/cpuid.c|919| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_ECX);
+ *   - arch/x86/kvm/cpuid.c|920| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_EDX);
+ *   - arch/x86/kvm/cpuid.c|928| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_7_1_EAX);
+ *   - arch/x86/kvm/cpuid.c|993| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_D_1_EAX);
+ *   - arch/x86/kvm/cpuid.c|1049| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_12_EAX);
+ *   - arch/x86/kvm/cpuid.c|1149| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_0001_EDX);
+ *   - arch/x86/kvm/cpuid.c|1150| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_0001_ECX);
+ *   - arch/x86/kvm/cpuid.c|1184| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_0008_EBX);
+ *   - arch/x86/kvm/cpuid.c|1196| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_000A_EDX);
+ *   - arch/x86/kvm/cpuid.c|1208| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_8000_001F_EAX);
+ *   - arch/x86/kvm/cpuid.c|1244| <<__do_cpuid_func>> cpuid_entry_override(entry, CPUID_C000_0001_EDX);
+ */
 static __always_inline void cpuid_entry_override(struct kvm_cpuid_entry2 *entry,
 						 unsigned int leaf)
 {
@@ -196,14 +232,55 @@ static __always_inline void kvm_cpu_cap_clear(unsigned int x86_feature)
 	kvm_cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|608| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_X2APIC);
+ *   - arch/x86/kvm/cpuid.c|639| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_LA57);
+ *   - arch/x86/kvm/cpuid.c|657| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_TSC_ADJUST);
+ *   - arch/x86/kvm/cpuid.c|658| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_ARCH_CAPABILITIES);
+ *   - arch/x86/kvm/cpuid.c|661| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL);
+ *   - arch/x86/kvm/cpuid.c|663| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_INTEL_STIBP);
+ *   - arch/x86/kvm/cpuid.c|665| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL_SSBD);
+ *   - arch/x86/kvm/cpuid.c|699| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_GBPAGES);
+ *   - arch/x86/kvm/cpuid.c|714| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_IBPB);
+ *   - arch/x86/kvm/cpuid.c|716| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_IBRS);
+ *   - arch/x86/kvm/cpuid.c|718| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_STIBP);
+ *   - arch/x86/kvm/cpuid.c|720| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_SSBD);
+ *   - arch/x86/kvm/cpuid.c|722| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_AMD_SSB_NO);
+ *   - arch/x86/kvm/cpuid.c|729| <<kvm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);
+ *   - arch/x86/kvm/cpuid.h|242| <<kvm_cpu_cap_check_and_set>> kvm_cpu_cap_set(x86_feature);
+ *   - arch/x86/kvm/svm/svm.c|4921| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SVM);
+ *   - arch/x86/kvm/svm/svm.c|4922| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VMCBCLEAN);
+ *   - arch/x86/kvm/svm/svm.c|4925| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_NRIPS);
+ *   - arch/x86/kvm/svm/svm.c|4928| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_NPT);
+ *   - arch/x86/kvm/svm/svm.c|4931| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_TSCRATEMSR);
+ *   - arch/x86/kvm/svm/svm.c|4934| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_V_VMSAVE_VMLOAD);
+ *   - arch/x86/kvm/svm/svm.c|4936| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_LBRV);
+ *   - arch/x86/kvm/svm/svm.c|4939| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_PAUSEFILTER);
+ *   - arch/x86/kvm/svm/svm.c|4942| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_PFTHRESHOLD);
+ *   - arch/x86/kvm/svm/svm.c|4945| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VGIF);
+ *   - arch/x86/kvm/svm/svm.c|4948| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_SVME_ADDR_CHK);
+ *   - arch/x86/kvm/svm/svm.c|4954| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);
+ *   - arch/x86/kvm/svm/svm.c|4958| <<svm_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_PERFCTR_CORE);
+ *   - arch/x86/kvm/vmx/vmx.c|7686| <<vmx_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_VMX);
+ *   - arch/x86/kvm/vmx/vmx.c|7711| <<vmx_set_cpu_caps>> kvm_cpu_cap_set(X86_FEATURE_UMIP);
+ */
 static __always_inline void kvm_cpu_cap_set(unsigned int x86_feature)
 {
+	/*
+	 * leaf的意思就是除以32
+	 */
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
 
 	reverse_cpuid_check(x86_leaf);
 	kvm_cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|236| <<kvm_cpu_cap_has>> return !!kvm_cpu_cap_get(x86_feature);
+ *   - arch/x86/kvm/vmx/evmcs.c|338| <<nested_get_evmcs_version>> if (kvm_cpu_cap_get(X86_FEATURE_VMX) &&
+ */
 static __always_inline u32 kvm_cpu_cap_get(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index aacb28c83..43c31fa94 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -4114,6 +4114,9 @@ static int em_fxrstor(struct x86_emulate_ctxt *ctxt)
 			goto out;
 	}
 
+	/*
+	 * MXCSR Register State
+	 */
 	if (fx_state.mxcsr >> 16) {
 		rc = emulate_gp(ctxt, 0);
 		goto out;
@@ -5060,6 +5063,10 @@ static int decode_operand(struct x86_emulate_ctxt *ctxt, struct operand *op,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9447| <<x86_decode_emulated_instruction>> r = x86_decode_insn(ctxt, insn, insn_len, emulation_type);
+ */
 int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len, int emulation_type)
 {
 	int rc = X86EMUL_CONTINUE;
@@ -5437,6 +5444,10 @@ void init_decode_cache(struct x86_emulate_ctxt *ctxt)
 	ctxt->mem_read.end = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9568| <<x86_emulate_instruction>> r = x86_emulate_insn(ctxt);
+ */
 int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index ed8044475..af0dcf82e 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1125,6 +1125,10 @@ static inline bool tsc_page_update_unsafe(struct kvm_hv *hv)
 		hv->hv_tsc_emulation_control;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3764| <<kvm_guest_time_update>> kvm_hv_setup_tsc_page(v->kvm, &vcpu->hv_clock);
+ */
 void kvm_hv_setup_tsc_page(struct kvm *kvm,
 			   struct pvclock_vcpu_time_info *hv_clock)
 {
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 0687162c4..2e87d6561 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -100,6 +100,14 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|141| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|173| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|426| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/svm/avic.c|868| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/vmx/posted_intr.c|332| <<vmx_pi_update_irte>> kvm_set_msi_irq(kvm, e, &irq);
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 3febc3423..2f8f859ec 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -183,6 +183,13 @@ static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|966| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1175| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3496| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4565| <<nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 9dda989a1..6e22ef63c 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -552,6 +552,11 @@ static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 	return result;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|572| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|2682| <<kvm_get_apic_interrupt>> apic_clear_irr(vector, apic);
+ */
 static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 {
 	if (unlikely(apic->apicv_active)) {
@@ -567,6 +572,10 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3602| <<nested_vmx_run>> kvm_apic_clear_irr(vcpu, vmx->nested.posted_intr_nv);
+ */
 void kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)
 {
 	apic_clear_irr(vec, vcpu->arch.apic);
@@ -652,6 +661,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|820| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|1911| <<kvm_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|76| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|98| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|686| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1066| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1079| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|14010| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1121,6 +1141,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|660| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+ *   - arch/x86/kvm/lapic.c|2543| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1621,6 +1646,10 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1700| <<__kvm_wait_lapic_expire>> __wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
+ */
 static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
@@ -1695,6 +1724,12 @@ static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 		__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4014| <<svm_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ *   - arch/x86/kvm/trace.h|1009| <<__field>> TRACE_EVENT(kvm_wait_lapic_expire,
+ *   - arch/x86/kvm/vmx/vmx.c|7283| <<vmx_vcpu_run>> kvm_wait_lapic_expire(vcpu);
+ */
 void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu) &&
@@ -2432,6 +2467,10 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_apicv);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12466| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2524,6 +2563,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1712| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2547| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|432| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|4958| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -3020,6 +3066,13 @@ int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10501| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|10766| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+ *   - arch/x86/kvm/x86.c|10964| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+ *   - arch/x86/kvm/x86.c|11197| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+ */
 int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 117a46df5..0d655d0e5 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -63,6 +63,17 @@ struct kvm_lapic {
 	struct kvm_vcpu *vcpu;
 	bool apicv_active;
 	bool sw_enabled;
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|546| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|563| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|566| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2426| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2873| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|166| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|443| <<avic_kick_target_vcpus_fast>> target_vcpu->arch.apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|478| <<avic_kick_target_vcpus>> vcpu->arch.apic->irr_pending = true;
+	 */
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
 	/* Number of bits set in ISR. */
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 6bdaacb6f..dc857488c 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -120,6 +120,10 @@ void kvm_mmu_free_obsolete_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu);
 void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10506| <<vcpu_enter_guest>> r = kvm_mmu_reload(vcpu);
+ */
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
 	if (likely(vcpu->arch.mmu->root.hpa != INVALID_PAGE))
@@ -247,6 +251,12 @@ static inline gfn_t gfn_to_index(gfn_t gfn, gfn_t base_gfn, int level)
 		(base_gfn >> KVM_HPAGE_GFN_SHIFT(level));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|265| <<kvm_mmu_slot_lpages>> return __kvm_mmu_slot_lpages(slot, slot->npages, level)
+ *   - arch/x86/kvm/x86.c|13487| <<memslot_rmap_alloc>> int lpages = __kvm_mmu_slot_lpages(slot, npages, level);
+ *   - arch/x86/kvm/x86.c|13527| <<kvm_alloc_memslot_metadata>> lpages = __kvm_mmu_slot_lpages(slot, npages, level);
+ */
 static inline unsigned long
 __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 		      int level)
@@ -255,12 +265,23 @@ __kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, unsigned long npages,
 			    slot->base_gfn, level) + 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|119| <<kvm_mmu_rmaps_stat_show>> lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ */
 static inline unsigned long
 kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 {
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|548| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|1606| <<__rmap_add>> kvm_update_page_stats(kvm, sp->role.level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|611| <<__handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1556| <<tdp_mmu_split_huge_page>> kvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
@@ -269,6 +290,13 @@ static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 gpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u64 access,
 			   struct x86_exception *exception);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4024| <<nonpaging_gva_to_gpa>> return kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|368| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(table_gfn),
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|433| <<FNAME(walk_addr_generic)>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(gfn), access, &walker->fault);
+ *   - arch/x86/kvm/x86.c|881| <<load_pdptrs>> real_gpa = kvm_translate_gpa(vcpu, mmu, gfn_to_gpa(pdpt_gfn),
+ */
 static inline gpa_t kvm_translate_gpa(struct kvm_vcpu *vcpu,
 				      struct kvm_mmu *mmu,
 				      gpa_t gpa, u64 access,
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 3552e6af3..b552d1f7f 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -608,6 +608,12 @@ static bool mmu_spte_age(u64 *sptep)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3344| <<fast_page_fault>> walk_shadow_page_lockless_begin(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4031| <<get_mmio_spte>> walk_shadow_page_lockless_begin(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4136| <<shadow_page_table_clear_flood>> walk_shadow_page_lockless_begin(vcpu);
+ */
 static void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)
 {
 	if (is_tdp_mmu(vcpu->arch.mmu)) {
@@ -921,6 +927,12 @@ pte_list_desc_remove_entry(struct kvm_rmap_head *rmap_head,
 	mmu_free_pte_list_desc(desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|964| <<kvm_zap_one_rmap_spte>> pte_list_remove(sptep, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1054| <<rmap_remove>> pte_list_remove(spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1698| <<mmu_page_remove_parent_pte>> pte_list_remove(parent_pte, &sp->parent_ptes);
+ */
 static void pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
@@ -1819,6 +1831,11 @@ static int mmu_unsync_walk(struct kvm_mmu_page *sp,
 	return __mmu_unsync_walk(sp, pvec);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1984| <<mmu_sync_children>> kvm_unlink_unsync_page(vcpu->kvm, sp);
+ *   - arch/x86/kvm/mmu/mmu.c|2447| <<__kvm_mmu_prepare_zap_page>> kvm_unlink_unsync_page(kvm, sp);
+ */
 static void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	WARN_ON(!sp->unsync);
@@ -2424,6 +2441,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2496| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2546| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|5871| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|6505| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2522,6 +2546,12 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2596| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2621| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
+ *   - arch/x86/kvm/mmu/mmu.c|6607| <<mmu_shrink_scan>> freed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2568,6 +2598,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3562| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3695| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4276| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|877| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -3184,6 +3221,24 @@ static int handle_abnormal_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fau
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * #PF can be fast if:
+ *
+ * 1. The shadow page table entry is not present and A/D bits are
+ *    disabled _by KVM_, which could mean that the fault is potentially
+ *    caused by access tracking (if enabled).  If A/D bits are enabled
+ *    by KVM, but disabled by L1 for L2, KVM is forced to disable A/D
+ *    bits for L2 and employ access tracking, but the fast page fault
+ *    mechanism only supports direct MMUs.
+ * 2. The shadow page table entry is present, the access is a write,
+ *    and no reserved bits are set (MMIO SPTEs cannot be "fixed"), i.e.
+ *    the fault was caused by a write-protection violation.  If the
+ *    SPTE is MMU-writable (determined later), the fault can be fixed
+ *    by setting the Writable bit, which can be done out of mmu_lock.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3341| <<fast_page_fault>> if (!page_fault_can_be_fast(fault))
+ */
 static bool page_fault_can_be_fast(struct kvm_page_fault *fault)
 {
 	/*
@@ -3287,6 +3342,18 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * volatile bits: bits that can be set outside of mmu_lock. The Writable bit
+ * can be set by KVM's fast page fault handler, and Accessed and Dirty bits
+ * can be set by the CPU.
+ *
+ * 也就是说,volatile bits是那些可以在lock外修改的bit,比如在fast page fault中
+ * 的writable,或者硬件支持的access/dirty bit.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4276| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmutrace.h|255| <<__field>> fast_page_fault,
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3295,6 +3362,21 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	u64 *sptep = NULL;
 	uint retry_count = 0;
 
+	/*
+	 * #PF can be fast if:
+	 *
+	 * 1. The shadow page table entry is not present and A/D bits are
+	 *    disabled _by KVM_, which could mean that the fault is potentially
+	 *    caused by access tracking (if enabled).  If A/D bits are enabled
+	 *    by KVM, but disabled by L1 for L2, KVM is forced to disable A/D
+	 *    bits for L2 and employ access tracking, but the fast page fault
+	 *    mechanism only supports direct MMUs.
+	 * 2. The shadow page table entry is present, the access is a write,
+	 *    and no reserved bits are set (MMIO SPTEs cannot be "fixed"), i.e.
+	 *    the fault was caused by a write-protection violation.  If the
+	 *    SPTE is MMU-writable (determined later), the fault can be fixed
+	 *    by setting the Writable bit, which can be done out of mmu_lock.
+	 */
 	if (!page_fault_can_be_fast(fault))
 		return ret;
 
@@ -3380,6 +3462,11 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		 * since the gfn is not stable for indirect shadow page. See
 		 * Documentation/virt/kvm/locking.rst to get more detail.
 		 */
+		/*
+		 * 注释:
+		 * Returns true if the SPTE was fixed successfully. Otherwise,
+		 * someone else modified the SPTE from its original value.
+		 */
 		if (fast_pf_fix_direct_spte(vcpu, fault, sptep, spte, new_spte)) {
 			ret = RET_PF_FIXED;
 			break;
@@ -3514,6 +3601,13 @@ static int mmu_check_root(struct kvm_vcpu *vcpu, gfn_t root_gfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3583| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+ *   - arch/x86/kvm/mmu/mmu.c|3594| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0,
+ *   - arch/x86/kvm/mmu/mmu.c|3717| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0,
+ *   - arch/x86/kvm/mmu/mmu.c|3771| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+ */
 static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 			    u8 level)
 {
@@ -3532,6 +3626,10 @@ static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 	return __pa(sp->spt);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5337| <<kvm_mmu_load>> r = mmu_alloc_direct_roots(vcpu);
+ */
 static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -4050,6 +4148,11 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4257| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|822| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 struct kvm_page_fault *fault)
 {
@@ -4208,6 +4311,30 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_invalidate_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()初始化fault
+ * 267         struct kvm_page_fault fault = {
+ * 268                 .addr = cr2_or_gpa,
+ * 269                 .error_code = err,
+ * 270                 .exec = err & PFERR_FETCH_MASK,
+ * 271                 .write = err & PFERR_WRITE_MASK,
+ * 272                 .present = err & PFERR_PRESENT_MASK,
+ * 273                 .rsvd = err & PFERR_RSVD_MASK,
+ * 274                 .user = err & PFERR_USER_MASK,
+ * 275                 .prefetch = prefetch,
+ * 276                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 277                 .nx_huge_page_workaround_enabled =
+ * 278                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 279 
+ * 280                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 281                 .req_level = PG_LEVEL_4K,
+ * 282                 .goal_level = PG_LEVEL_4K,
+ * 283         };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4278| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4338| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@ -4247,6 +4374,11 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	else
 		write_lock(&vcpu->kvm->mmu_lock);
 
+	/*
+	 * 注释:
+	 * Returns true if the page fault is stale and needs to be retried, i.e. if the
+	 * root was invalidated by a memslot update or a relevant mmu_notifier fired.
+	 */
 	if (is_page_fault_stale(vcpu, fault, mmu_seq))
 		goto out_unlock;
 
@@ -4311,6 +4443,12 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5102| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *   - arch/x86/kvm/mmu/mmu_internal.h|271| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu/mmu_internal.h|290| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	/*
@@ -4434,6 +4572,14 @@ static bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,
 		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5117| <<kvm_init_shadow_npt_mmu>> kvm_mmu_new_pgd(vcpu, nested_cr3);
+ *   - arch/x86/kvm/mmu/mmu.c|5172| <<kvm_init_shadow_ept_mmu>> kvm_mmu_new_pgd(vcpu, new_eptp);
+ *   - arch/x86/kvm/svm/nested.c|535| <<nested_svm_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/vmx/nested.c|1135| <<nested_vmx_load_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ *   - arch/x86/kvm/x86.c|1246| <<kvm_set_cr3>> kvm_mmu_new_pgd(vcpu, cr3);
+ */
 void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -5274,6 +5420,10 @@ void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_reset_context);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|128| <<kvm_mmu_reload>> return kvm_mmu_load(vcpu);
+ */
 int kvm_mmu_load(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -6319,6 +6469,10 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 }
 
 /* Must be called with the mmu_lock held in write-mode. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1334| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_try_split_huge_pages(kvm, slot, start, end, PG_LEVEL_4K);
+ */
 void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot,
 				   u64 start, u64 end,
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index 582def531..3517b28ed 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -54,6 +54,13 @@ struct kvm_mmu_page {
 	struct list_head link;
 	struct hlist_node hash_link;
 
+	/*
+	 * 在以下使用kvm_mmu_page->tdp_mmu_page:
+	 *   - arch/x86/kvm/mmu/mmu.c|1903| <<is_obsolete_sp>> return !sp->tdp_mmu_page && unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|148| <<kvm_tdp_mmu_put_root>> WARN_ON(!root->tdp_mmu_page);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|292| <<tdp_mmu_init_sp>> sp->tdp_mmu_page = true;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.h|71| <<is_tdp_mmu_page>> static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
+	 */
 	bool tdp_mmu_page;
 	bool unsync;
 	u8 mmu_valid_gen;
@@ -256,6 +263,11 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4183| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);
+ *   - arch/x86/kvm/mmu/mmu.c|5613| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, lower_32_bits(error_code), false);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch)
 {
diff --git a/arch/x86/kvm/mmu/page_track.c b/arch/x86/kvm/mmu/page_track.c
index 2e09d1b62..aabca6a33 100644
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@ -35,6 +35,10 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *slot)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12476| <<kvm_alloc_memslot_metadata>> if (kvm_page_track_create_memslot(kvm, slot, npages))
+ */
 int kvm_page_track_create_memslot(struct kvm *kvm,
 				  struct kvm_memory_slot *slot,
 				  unsigned long npages)
@@ -174,6 +178,11 @@ EXPORT_SYMBOL_GPL(kvm_slot_page_track_remove_page);
 /*
  * check if the corresponding access on the specified guest page is tracked.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2707| <<mmu_try_to_unsync_pages>> if (kvm_slot_page_track_is_active(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE))
+ *   - arch/x86/kvm/mmu/mmu.c|4097| <<page_fault_handle_page_track>> if (kvm_slot_page_track_is_active(vcpu->kvm, fault->slot, fault->gfn, KVM_PAGE_TRACK_WRITE))
+ */
 bool kvm_slot_page_track_is_active(struct kvm *kvm,
 				   const struct kvm_memory_slot *slot,
 				   gfn_t gfn, enum kvm_page_track_mode mode)
@@ -255,6 +264,11 @@ EXPORT_SYMBOL_GPL(kvm_page_track_unregister_notifier);
  * The node should figure out if the written page is the one that node is
  * interested in by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7410| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|7677| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ */
 void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			  int bytes)
 {
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index 7670c13ce..54177430a 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -295,6 +295,25 @@ static inline bool is_executable_pte(u64 spte)
 	return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|477| <<mmu_spte_update_no_track>> WARN_ON(spte_to_pfn(old_spte) != spte_to_pfn(new_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|515| <<mmu_spte_update>> kvm_set_pfn_accessed(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|520| <<mmu_spte_update>> kvm_set_pfn_dirty(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/mmu.c|550| <<mmu_spte_clear_track_bits>> pfn = spte_to_pfn(old_spte);
+ *   - arch/x86/kvm/mmu/mmu.c|602| <<mmu_spte_age>> kvm_set_pfn_dirty(spte_to_pfn(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|1220| <<spte_wrprot_for_clear_dirty>> kvm_set_pfn_dirty(spte_to_pfn(*sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|2835| <<mmu_set_spte>> } else if (pfn != spte_to_pfn(*sptep)) {
+ *   - arch/x86/kvm/mmu/mmu.c|2837| <<mmu_set_spte>> spte_to_pfn(*sptep), pfn);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1083| <<FNAME(sync_page)>> spte_to_pfn(spte), spte, true, false,
+ *   - arch/x86/kvm/mmu/tdp_iter.c|74| <<spte_to_child_pt>> return (tdp_ptep_t)__va(spte_to_pfn(spte) << PAGE_SHIFT);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|357| <<handle_changed_spte_acc_track>> spte_to_pfn(old_spte) != spte_to_pfn(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|358| <<handle_changed_spte_acc_track>> kvm_set_pfn_accessed(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|370| <<handle_changed_spte_dirty_log>> pfn_changed = spte_to_pfn(old_spte) != spte_to_pfn(new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|548| <<__handle_changed_spte>> bool pfn_changed = spte_to_pfn(old_spte) != spte_to_pfn(new_spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|615| <<__handle_changed_spte>> kvm_set_pfn_dirty(spte_to_pfn(old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1309| <<age_gfn_range>> kvm_set_pfn_dirty(spte_to_pfn(new_spte));
+ */
 static inline kvm_pfn_t spte_to_pfn(u64 pte)
 {
 	return (pte & SPTE_BASE_ADDR_MASK) >> PAGE_SHIFT;
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index bf2ccf9de..33c88c90c 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -10,9 +10,23 @@
 #include <asm/cmpxchg.h>
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用global的tdp_mmu_enabled:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|13| <<global>> static bool __read_mostly tdp_mmu_enabled = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|14| <<global>> module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|21| <<kvm_mmu_init_tdp_mmu>> if (!tdp_enabled || !READ_ONCE(tdp_mmu_enabled))
+ * 在以下使用kvm->arch.tdp_mmu_enabled:
+ *   - arch/x86/kvm/mmu.h|237| <<is_tdp_mmu_enabled>> static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mmu_enabled; }
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|29| <<kvm_mmu_init_tdp_mmu>> kvm->arch.tdp_mmu_enabled = true;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|51| <<kvm_mmu_uninit_tdp_mmu>> if (!kvm->arch.tdp_mmu_enabled)
+ */
 static bool __read_mostly tdp_mmu_enabled = true;
 module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6115| <<kvm_mmu_init_vm>> r = kvm_mmu_init_tdp_mmu(kvm);
+ */
 /* Initializes the TDP MMU for the VM, if enabled. */
 int kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 {
@@ -34,6 +48,14 @@ int kvm_mmu_init_tdp_mmu(struct kvm *kvm)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|143| <<kvm_tdp_mmu_put_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|251| <<__for_each_tdp_mmu_root_yield_safe>> if (kvm_lockdep_assert_mmu_lock_held(_kvm, _shared) && \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|270| <<for_each_tdp_mmu_root>> if (kvm_lockdep_assert_mmu_lock_held(_kvm, false) && \
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|928| <<tdp_mmu_zap_root>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1647| <<kvm_tdp_mmu_try_split_huge_pages>> kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ */
 /* Arbitrarily returns true so that this may be used in if statements. */
 static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
 							     bool shared)
@@ -308,6 +330,10 @@ static void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,
 	tdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3580| <<mmu_alloc_direct_roots>> root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
+ */
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
 {
 	union kvm_mmu_page_role role = vcpu->arch.mmu->root_role;
@@ -512,6 +538,27 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
  * Handle bookkeeping that might result from the modification of a SPTE.
  * This function must be called for all TDP SPTE modifications.
  */
+/*
+ * 5.15的例子
+ * [0] __handle_changed_spte
+ * [0] tdp_mmu_map_handle_target_level
+ * [0] kvm_tdp_mmu_map
+ * [0] direct_page_fault
+ * [0] kvm_mmu_page_fault
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] vcpu_run
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|611| <<handle_changed_spte>> __handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|658| <<tdp_mmu_set_spte_atomic>> __handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|736| <<__tdp_mmu_set_spte>> __handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+ */
 static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				  u64 old_spte, u64 new_spte, int level,
 				  bool shared)
@@ -600,6 +647,10 @@ static void __handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 		handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|497| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -628,6 +679,15 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
  *            no side-effects other than setting iter->old_spte to the last
  *            known value of the spte.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|697| <<tdp_mmu_zap_spte_atomic>> ret = tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|889| <<__tdp_mmu_zap_root>> else if (tdp_mmu_set_spte_atomic(kvm, &iter, 0))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1099| <<tdp_mmu_map_handle_target_level>> else if (tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<tdp_mmu_link_sp>> ret = tdp_mmu_set_spte_atomic(kvm, iter, spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1409| <<wrprot_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1672| <<clear_dirty_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, new_spte))
+ */
 static inline int tdp_mmu_set_spte_atomic(struct kvm *kvm,
 					  struct tdp_iter *iter,
 					  u64 new_spte)
@@ -712,6 +772,11 @@ static inline int tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  * Returns the old SPTE value, which _may_ be different than @old_spte if the
  * SPTE had voldatile bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|773| <<_tdp_mmu_set_spte>> iter->old_spte = __tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|945| <<kvm_tdp_mmu_zap_sp>> __tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,
+ */
 static u64 __tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 			      u64 old_spte, u64 new_spte, gfn_t gfn, int level,
 			      bool record_acc_track, bool record_dirty_log)
@@ -1048,6 +1113,10 @@ void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)
  * Installs a last-level SPTE to handle a TDP page fault.
  * (NPT/EPT violation/misconfiguration)
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1221| <<kvm_tdp_mmu_map>> ret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);
+ */
 static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 					  struct kvm_page_fault *fault,
 					  struct tdp_iter *iter)
@@ -1112,6 +1181,11 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
  * Returns: 0 if the new page table was installed. Non-0 if the page table
  *          could not be installed (e.g. the atomic compare-exchange failed).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1196| <<kvm_tdp_mmu_map>> if (tdp_mmu_link_sp(vcpu->kvm, &iter, sp, account_nx, true)) {
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1491| <<tdp_mmu_split_huge_page>> ret = tdp_mmu_link_sp(kvm, iter, sp, false, shared);
+ */
 static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 			   struct kvm_mmu_page *sp, bool account_nx,
 			   bool shared)
@@ -1140,6 +1214,10 @@ static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4310| <<direct_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@ -1460,6 +1538,10 @@ static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1547| <<tdp_mmu_split_huge_pages_root>> if (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))
+ */
 static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 				   struct kvm_mmu_page *sp, bool shared)
 {
@@ -1500,6 +1582,10 @@ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1581| <<kvm_tdp_mmu_try_split_huge_pages>> r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
+ */
 static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 					 struct kvm_mmu_page *root,
 					 gfn_t start, gfn_t end,
@@ -1567,6 +1653,11 @@ static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
 /*
  * Try to split all huge pages mapped by the TDP MMU down to the target level.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6341| <<kvm_mmu_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, false);
+ *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_slot_try_split_huge_pages>> kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);
+ */
 void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
 				      const struct kvm_memory_slot *slot,
 				      gfn_t start, gfn_t end,
@@ -1873,6 +1964,10 @@ int kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,
  *
  * WARNING: This function is only intended to be called during fast_page_fault.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3389| <<fast_page_fault>> sptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
+ */
 u64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,
 					u64 *spte)
 {
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index c163f7cc2..763447a1f 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -7,6 +7,12 @@
 
 hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|222| <<tdp_mmu_next_root>> kvm_tdp_mmu_get_root(next_root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|329| <<kvm_tdp_mmu_get_vcpu_root_hpa>> kvm_tdp_mmu_get_root(root))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1083| <<kvm_tdp_mmu_invalidate_all_roots>> !WARN_ON_ONCE(!kvm_tdp_mmu_get_root(root))) {
+ */
 __must_check static inline bool kvm_tdp_mmu_get_root(struct kvm_mmu_page *root)
 {
 	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
@@ -70,6 +76,14 @@ int kvm_mmu_init_tdp_mmu(struct kvm *kvm);
 void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm);
 static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|619| <<walk_shadow_page_lockless_begin>> if (is_tdp_mmu(vcpu->arch.mmu)) {
+ *   - arch/x86/kvm/mmu/mmu.c|638| <<walk_shadow_page_lockless_end>> if (is_tdp_mmu(vcpu->arch.mmu)) {
+ *   - arch/x86/kvm/mmu/mmu.c|3388| <<fast_page_fault>> if (is_tdp_mmu(vcpu->arch.mmu))
+ *   - arch/x86/kvm/mmu/mmu.c|4077| <<get_mmio_spte>> if (is_tdp_mmu(vcpu->arch.mmu))
+ *   - arch/x86/kvm/mmu/mmu.c|4340| <<direct_page_fault>> bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
+ */
 static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
 {
 	struct kvm_mmu_page *sp;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 02f9e4f24..b4c3fe56f 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -22,9 +22,87 @@
 #include "lapic.h"
 #include "pmu.h"
 
+/*
+ * u: 仅统计用户空间程序触发的性能事件
+ * k: 仅统计内核触发的性能事件
+ * h: 仅统计Hypervisor触发的性能事件
+ * G: 在KVM虚拟机中,仅统计Guest系统触发的性能事件
+ * H: 仅统计Host系统触发的性能事件
+ * p: 精度级别
+ *
+ * https://blog.csdn.net/pwl999/article/details/81200439
+ *
+ * https://blog.csdn.net/pwl999/article/details/81362587
+ */
+/*
+ * 在perf_event_context_sched_in()最后加入下面:
+ *
+ * for (c = 0; c < 20; c++) mdelay(10);
+ *
+ * 程序例子:
+ *
+ * ./linux-6.0/tools/perf/perf stat ./testprog 10
+ *
+ *
+ * 然后perf stat的结果的cycles会增加
+ *
+ *
+ * [0] __perf_event_task_sched_in
+ * [0] finish_task_switch
+ * [0] __schedule
+ * [0] schedule
+ * [0] exit_to_user_mode_prepare
+ * [0] irqentry_exit_to_user_mode
+ * [0] asm_sysvec_call_function_single
+ *
+ *
+ * 调度进去时:
+ *
+ * prepare_task_switch()
+ * -> perf_event_task_sched_out()
+ *    -> __perf_event_task_sched_out()
+ *       -> perf_pmu_sched_task()
+ *          -> pmu->sched_task(cpuctx->task_ctx, sched_in)
+ *
+ * 调度出去时:
+ *
+ * finish_task_switch()
+ * -> perf_event_task_sched_in()
+ *    -> perf_event_context_sched_in()
+ *       -> perf_event_sched_in()
+ */
+
 /* This is enough to filter the vast majority of currently defined events. */
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - arch/x86/kvm/pmu.c|583| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/cpuid.c|914| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|915| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|916| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|917| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len; 
+ *   - arch/x86/kvm/cpuid.c|918| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|919| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|921| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|927| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/pmu.h|167| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|173| <<kvm_init_pmu_capability>> if ((is_intel && !kvm_pmu_cap.version) || !kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/pmu.h|177| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|181| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|182| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed,
+ *   - arch/x86/kvm/vmx/capabilities.h|401| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|540| <<intel_pmu_refresh>> kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|542| <<intel_pmu_refresh>> kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|545| <<intel_pmu_refresh>> kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|555| <<intel_pmu_refresh>> (size_t)kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|557| <<intel_pmu_refresh>> kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|6944| <<kvm_init_msr_list>> min(INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))
+ *   - arch/x86/kvm/x86.c|6949| <<kvm_init_msr_list>> min(INTEL_PMC_MAX_GENERIC, kvm_pmu_cap.num_counters_gp))
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
@@ -70,6 +148,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11967| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -83,11 +165,26 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|523| <<reprogram_counter>> if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))
+ *   - arch/x86/kvm/pmu.c|950| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))
+ */
 static inline bool pmc_is_enabled(struct kvm_pmc *pmc)
 {
+	/*
+	 * .pmc_is_enabled = intel_pmc_is_enabled()
+	 * .pmc_is_enabled = amd_pmc_is_enabled()
+	 */
 	return static_call(kvm_x86_pmu_pmc_is_enabled)(pmc);
 }
 
+/*
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|485| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ *
+ * 在__kvm_perf_overflow()被调用
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
@@ -96,11 +193,31 @@ static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|144| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|554| <<kvm_pmu_incr_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 	bool skip_pmi = false;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
+	/*
+	 * Set a bit and return its old value
+	 */
 	/* Ignore counters that have been reprogrammed already. */
 	if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
 		return;
@@ -112,11 +229,34 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 	} else {
 		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
 	}
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|165| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/x86.c|11323| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|13107| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 * 处理的函数: kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 
+	/*
+	 * 在以下使用kvm_pmc->intr:
+	 *   - arch/x86/kvm/pmu.c|174| <<__kvm_perf_overflow>> if (!pmc->intr || skip_pmi)
+	 *   - arch/x86/kvm/pmu.c|275| <<pmc_reprogram_counter>> pmc->intr = intr || pebs;
+	 */
 	if (!pmc->intr || skip_pmi)
 		return;
 
+	/*
+	 * 在以下使用kvm_pmu->irq_work:
+	 *   - arch/x86/kvm/pmu.c|192| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+	 *   - arch/x86/kvm/pmu.c|663| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+	 *   - arch/x86/kvm/pmu.c|673| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+	 *
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|178| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8161| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10337| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 	/*
 	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
 	 * can be ejected on a guest mode re-entry. Otherwise we can't
@@ -129,8 +269,22 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
 	else
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|194| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8172| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11325| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 }
 
+/*
+ * 似乎都是在下面触发的:
+ *   - kernel/events/core.c|9407| <<__perf_event_overflow>> READ_ONCE(event->overflow_handler)(event, data, regs);
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|201| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -140,6 +294,10 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	__kvm_perf_overflow(pmc, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|326| <<reprogram_counter>> pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ */
 static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 				  u64 config, bool exclude_user,
 				  bool exclude_kernel, bool intr)
@@ -189,6 +347,22 @@ static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 			attr.precise_ip = 3;
 	}
 
+	/*
+	 * 似乎都是在下面触发的:
+	 *   - kernel/events/core.c|9407| <<__perf_event_overflow>> READ_ONCE(event->overflow_handler)(event, data, regs);
+	 *
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+	 *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+	 *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+	 *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+	 *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+	 *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+	 */
 	event = perf_event_create_kernel_counter(&attr, -1, current,
 						 kvm_perf_overflow, pmc);
 	if (IS_ERR(event)) {
@@ -199,8 +373,26 @@ static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 
 	pmc->perf_event = event;
 	pmc_to_pmu(pmc)->event_count++;
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
 	clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
 	pmc->is_paused = false;
+	/*
+	 * 在以下使用kvm_pmc->intr:
+	 *   - arch/x86/kvm/pmu.c|174| <<__kvm_perf_overflow>> if (!pmc->intr || skip_pmi)
+	 *   - arch/x86/kvm/pmu.c|216| <<pmc_reprogram_counter>> bool exclude_kernel, bool intr)
+	 *   - arch/x86/kvm/pmu.c|275| <<pmc_reprogram_counter>> pmc->intr = intr || pebs;
+	 */
 	pmc->intr = intr || pebs;
 }
 
@@ -217,6 +409,10 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|374| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -235,6 +431,18 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	perf_event_enable(pmc->perf_event);
 	pmc->is_paused = false;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
 	clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
 	return true;
 }
@@ -247,6 +455,10 @@ static int cmp_u64(const void *pa, const void *pb)
 	return (a > b) - (a < b);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|356| <<reprogram_counter>> if (!check_pmu_event_filter(pmc))
+ */
 static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu_event_filter *filter;
@@ -283,9 +495,37 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 	return allow_event;
 }
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_pmu pmu;
+ *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+ *          -> u8 idx; 
+ *          -> u64 counter;
+ *          -> u64 eventsel;
+ *          -> struct perf_event *perf_event;
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> u64 current_config;
+ *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|359| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/pmu.c|561| <<kvm_pmu_incr_counter>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|241| <<amd_pmu_set_msr>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|55| <<reprogram_fixed_counters>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|83| <<global_ctrl_changed>> reprogram_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|480| <<intel_pmu_set_msr>> reprogram_counter(pmc);
+ */
 void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
+	/*
+	 * 在以下设置kvm_pmc->eventsel:
+	 *   - arch/x86/kvm/svm/pmu.c|240| <<amd_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|485| <<intel_pmu_set_msr>> pmc->eventsel = data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 */
 	u64 eventsel = pmc->eventsel;
 	u64 new_config = eventsel;
 	u8 fixed_ctr_ctrl;
@@ -319,6 +559,10 @@ void reprogram_counter(struct kvm_pmc *pmc)
 	pmc_release_perf_event(pmc);
 
 	pmc->current_config = new_config;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|326| <<reprogram_counter>> pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+	 */
 	pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
 			      (eventsel & pmu->raw_event_mask),
 			      !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
@@ -327,11 +571,33 @@ void reprogram_counter(struct kvm_pmc *pmc)
 }
 EXPORT_SYMBOL_GPL(reprogram_counter);
 
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|165| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/x86.c|11323| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|13107| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ * 处理的函数: kvm_pmu_handle_event()
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10336| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	int bit;
 
+	/*
+	 * 在以下使用kvm_pmu->reprogram_pmi:
+	 *   - arch/x86/kvm/pmu.c|152| <<__kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmu->reprogram_pmi))
+	 *   - arch/x86/kvm/pmu.c|263| <<pmc_reprogram_counter>> clear_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|303| <<pmc_resume_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+	 *   - arch/x86/kvm/pmu.c|436| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/pmu.c|440| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+	 *
+	 * 应该是在overflow的handler不处理, 标记这个bitmap,
+	 * 等到kvm_pmu_handle_event()的时候处理
+	 * kvm_pmu_handle_event()为了响应vcpu_enter_guest(KVM_REQ_PMU)
+	 */
 	for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
 		struct kvm_pmc *pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, bit);
 
@@ -342,6 +608,16 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 		reprogram_counter(pmc);
 	}
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	/*
 	 * Unused perf_events are only released if the corresponding MSRs
 	 * weren't accessed during the last vCPU time slice. kvm_arch_sched_in
@@ -417,9 +693,22 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|178| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8161| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10337| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|96| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10338| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
@@ -427,27 +716,72 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
+	/*
+	 * intel_msr_idx_to_pmc()
+	 * amd_msr_idx_to_pmc()
+	 *
+	 * intel_is_valid_msr()
+	 * amd_is_valid_msr()
+	 */
 	return static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr) ||
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|591| <<kvm_pmu_set_msr>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 */
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	struct kvm_pmc *pmc = static_call(kvm_x86_pmu_msr_idx_to_pmc)(vcpu, msr);
 
+	/*
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	if (pmc)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3957| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3971| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4219| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ */
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	return static_call(kvm_x86_pmu_get_msr)(vcpu, msr_info);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3768| <<kvm_set_msr_common(MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3854| <<kvm_set_msr_common(MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3862| <<kvm_set_msr_common(default: kvm_pmu_is_valid_msr)>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+	/*
+	 * intel_pmu_set_msr
+	 * amd_pmu_set_msr
+	 */
 	return static_call(kvm_x86_pmu_set_msr)(vcpu, msr_info);
 }
 
@@ -455,11 +789,22 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|349| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|626| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3561| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|522| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11767| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -481,6 +826,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 }
 
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|368| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -488,12 +837,46 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	DECLARE_BITMAP(bitmask, X86_PMC_IDX_MAX);
 	int i;
 
+	/*
+	 * 在以下使用kvm_pmu->need_cleanup:
+	 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+	 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+	 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+	 *
+	 * The gate to release perf_events not marked in
+	 * pmc_in_use only once in a vcpu time slice.
+	 */
 	pmu->need_cleanup = false;
 
+	/*
+	 * 在以下使用kvm_pmu->all_valid_pmc_idx:
+	 *   - arch/x86/kvm/pmu.c|633| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/pmu.c|705| <<kvm_pmu_trigger_event>> for_each_set_bit(i, pmu->all_valid_pmc_idx, X86_PMC_IDX_MAX) {
+	 *   - arch/x86/kvm/svm/pmu.c|266| <<amd_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|589| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|591| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_refresh>> bitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);
+	 *
+	 * 在以下使用kvm_pmu->pmc_in_use:
+	 *   - arch/x86/kvm/pmu.c|559| <<kvm_pmu_mark_pmc_in_use>> __set_bit(pmc->idx, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/pmu.c|634| <<kvm_pmu_cleanup>> bitmap_andnot(bitmask, pmu->all_valid_pmc_idx, pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/pmu.c|645| <<kvm_pmu_cleanup>> bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> __set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|291| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|304| <<intel_pmu_create_guest_lbr_event>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|337| <<intel_pmu_handle_lbr_msrs_access>> __set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|341| <<intel_pmu_handle_lbr_msrs_access>> clear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|757| <<vmx_passthrough_lbr_msrs>> if (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|764| <<vmx_passthrough_lbr_msrs>> __clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);
+	 */
 	bitmap_andnot(bitmask, pmu->all_valid_pmc_idx,
 		      pmu->pmc_in_use, X86_PMC_IDX_MAX);
 
 	for_each_set_bit(i, bitmask, X86_PMC_IDX_MAX) {
+		/*
+		 * amd_pmc_idx_to_pmc()
+		 */
 		pmc = static_call(kvm_x86_pmu_pmc_idx_to_pmc)(pmu, i);
 
 		if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
@@ -505,11 +888,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11689| <<kvm_arch_vcpu_destroy>> kvm_pmu_destroy(vcpu);
+ */
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|692| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
 	u64 prev_count;
@@ -545,8 +936,22 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3532| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8535| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8806| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8808| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_pmu pmu;
+	 *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+	 *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+	 */
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 	struct kvm_pmc *pmc;
 	int i;
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 5cc5721f2..cdd32d82c 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -4,6 +4,20 @@
 
 #include <linux/nospec.h>
 
+/*
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_pmu pmu;
+ *       -> struct kvm_pmc gp_counters[INTEL_PMC_MAX_GENERIC];
+ *          -> u8 idx; 
+ *          -> u64 counter;
+ *          -> u64 eventsel;
+ *          -> struct perf_event *perf_event;
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> u64 current_config;
+ *       -> struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+ */
+
 #define vcpu_to_pmu(vcpu) (&(vcpu)->arch.pmu)
 #define pmu_to_vcpu(pmu)  (container_of((pmu), struct kvm_vcpu, arch.pmu))
 #define pmc_to_pmu(pmc)   (&(pmc)->vcpu->arch.pmu)
@@ -44,10 +58,27 @@ struct kvm_pmu_ops {
 
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|280| <<pmc_pause_counter>> pmc->counter = counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_incr_counter>> pmc->counter = (pmc->counter + 1) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|77| <<pmc_read_counter>> return counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|196| <<get_sample_period>> u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|199| <<get_sample_period>> sample_period = pmc_bitmask(pmc) + 1;
+ */
 static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 
+	/*
+	 * 在以下设置kvm_pmu->counter_bitmasks[2]:
+	 *   - arch/x86/kvm/svm/pmu.c|373| <<amd_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << 48) - 1;
+	 *   - arch/x86/kvm/svm/pmu.c|378| <<amd_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|613| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_GP] = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|614| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|638| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|653| <<intel_pmu_refresh>> pmu->counter_bitmask[KVM_PMC_FIXED] = ((u64)1 << edx.split.bit_width_fixed) - 1;
+	 */
 	return pmu->counter_bitmask[pmc->type];
 }
 
@@ -63,8 +94,34 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * [0] pmc_release_perf_event
+ * [0] kvm_pmu_cleanup
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/pmu.c|336| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|80| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
+	/*
+	 * struct kvm_pmc {
+	 *     enum pmc_type type;
+	 *     u8 idx;
+	 *     u64 counter;
+	 *     u64 eventsel;
+	 *     struct perf_event *perf_event;
+	 *     struct kvm_vcpu *vcpu;
+	 *     u64 current_config;
+	 *     bool is_paused;
+	 *     bool intr;
+	 * };
+	 */
 	if (pmc->perf_event) {
 		perf_event_release_kernel(pmc->perf_event);
 		pmc->perf_event = NULL;
@@ -73,6 +130,13 @@ static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|541| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|292| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|649| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|656| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -101,6 +165,21 @@ static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
  */
+/*
+ * 只被intel调用:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|79| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|200| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|249| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|250| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|268| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|269| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|411| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|412| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|422| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|497| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|498| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|512| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 					 u32 base)
 {
@@ -129,6 +208,12 @@ static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|225| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|295| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|215| <<pmc_update_sample_period>> get_sample_period(pmc, pmc->counter));
+ */
 static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
@@ -138,15 +223,33 @@ static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|553| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|557| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ */
 static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event || pmc->is_paused)
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|294| <<pmc_resume_counter>> if (perf_event_period(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.h|208| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+	 */
 	perf_event_period(pmc->perf_event,
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|392| <<reprogram_counter>> if (!pmc_speculative_in_use(pmc) || !pmc_is_enabled(pmc))
+ *   - arch/x86/kvm/pmu.c|639| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/pmu.c|708| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_is_enabled(pmc) || !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|791| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -160,10 +263,26 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11990| <<kvm_arch_hardware_setup>> kvm_init_pmu_capability();
+ */
 static inline void kvm_init_pmu_capability(void)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
 
+	/*
+	 * struct x86_pmu_capability {
+	 *     int             version;
+	 *     int             num_counters_gp;
+	 *     int             num_counters_fixed;
+	 *     int             bit_width_gp;
+	 *     int             bit_width_fixed;
+	 *     unsigned int    events_mask;
+	 *     int             events_mask_len;
+	 *     unsigned int    pebs_ept        :1;
+	 * };
+	 */
 	perf_get_x86_pmu_capability(&kvm_pmu_cap);
 
 	 /*
diff --git a/arch/x86/kvm/reverse_cpuid.h b/arch/x86/kvm/reverse_cpuid.h
index a19d473d0..6e15dbfbe 100644
--- a/arch/x86/kvm/reverse_cpuid.h
+++ b/arch/x86/kvm/reverse_cpuid.h
@@ -30,6 +30,12 @@ struct cpuid_reg {
 	int reg;
 };
 
+/*
+ * 在以下使用reverse_cpuid[]:
+ *   - arch/x86/kvm/reverse_cpuid.h|67| <<reverse_cpuid_check>> BUILD_BUG_ON(x86_leaf >= ARRAY_SIZE(reverse_cpuid));
+ *   - arch/x86/kvm/reverse_cpuid.h|68| <<reverse_cpuid_check>> BUILD_BUG_ON(reverse_cpuid[x86_leaf].function == 0);
+ *   - arch/x86/kvm/reverse_cpuid.h|111| <<x86_feature_cpuid>> return reverse_cpuid[x86_leaf];
+ */
 static const struct cpuid_reg reverse_cpuid[] = {
 	[CPUID_1_EDX]         = {         1, 0, CPUID_EDX},
 	[CPUID_8000_0001_EDX] = {0x80000001, 0, CPUID_EDX},
@@ -82,6 +88,9 @@ static __always_inline u32 __feature_translate(int x86_feature)
 	return x86_feature;
 }
 
+/*
+ * leaf的意思就是除以32
+ */
 static __always_inline u32 __feature_leaf(int x86_feature)
 {
 	return __feature_translate(x86_feature) / 32;
@@ -103,6 +112,12 @@ static __always_inline u32 __feature_bit(int x86_feature)
 
 #define feature_bit(name)  __feature_bit(X86_FEATURE_##name)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|527| <<__kvm_cpu_cap_mask>> const struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);
+ *   - arch/x86/kvm/cpuid.h|97| <<guest_cpuid_get_register>> const struct cpuid_reg cpuid = x86_feature_cpuid(x86_feature);
+ *   - arch/x86/kvm/reverse_cpuid.h|135| <<cpuid_entry_get_reg>> const struct cpuid_reg cpuid = x86_feature_cpuid(x86_feature);
+ */
 static __always_inline struct cpuid_reg x86_feature_cpuid(unsigned int x86_feature)
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index 6919dee69..ce67b35a2 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -217,6 +217,10 @@ int avic_vm_init(struct kvm *kvm)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1313| <<init_vmcb>> avic_init_vmcb(svm, vmcb);
+ */
 void avic_init_vmcb(struct vcpu_svm *svm, struct vmcb *vmcb)
 {
 	struct kvm_svm *kvm_svm = to_kvm_svm(svm->vcpu.kvm);
@@ -799,6 +803,10 @@ static void svm_ir_list_del(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|962| <<avic_pi_update_irte>> svm_ir_list_add(svm, &pi);
+ */
 static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 {
 	int ret = 0;
@@ -888,6 +896,14 @@ get_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|14188| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm,
+ *   - arch/x86/kvm/x86.c|14213| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|14228| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+ *
+ * struct kvm_x86_ops svm_x86_ops.pi_update_irte = avic_pi_update_irte()
+ */
 int avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 			uint32_t guest_irq, bool set)
 {
@@ -1012,6 +1028,11 @@ bool avic_check_apicv_inhibit_reasons(enum kvm_apicv_inhibit reason)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|1073| <<avic_vcpu_load>> avic_update_iommu_vcpu_affinity(vcpu, h_physical_id, true);
+ *   - arch/x86/kvm/svm/avic.c|1089| <<avic_vcpu_put>> avic_update_iommu_vcpu_affinity(vcpu, -1, 0);
+ */
 static inline int
 avic_update_iommu_vcpu_affinity(struct kvm_vcpu *vcpu, int cpu, bool r)
 {
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index f24613a10..9ea73ca4d 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -33,10 +33,20 @@ enum index {
 	INDEX_ERROR,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|144| <<amd_pmc_idx_to_pmc>> unsigned int base = get_msr_base(pmu, PMU_TYPE_COUNTER);
+ *
+ * 对于extention,是MSR_F15H_PERF_CTR和MSR_F15H_PERF_CTL
+ * 对于非extention,是MSR_K7_PERFCTR0和MSR_K7_EVNTSEL0
+ */
 static unsigned int get_msr_base(struct kvm_pmu *pmu, enum pmu_type type)
 {
 	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
 
+	/*
+	 * X86_FEATURE_PERFCTR_CORE是Core performance counter extensions
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE)) {
 		if (type == PMU_TYPE_COUNTER)
 			return MSR_F15H_PERF_CTR;
@@ -50,6 +60,37 @@ static unsigned int get_msr_base(struct kvm_pmu *pmu, enum pmu_type type)
 	}
 }
 
+/*
+ * INDEX_ZERO
+ *   - MSR_F15H_PERF_CTL0 : MSR_F15H_PERF_CTL = 0xc0010200
+ *   - MSR_F15H_PERF_CTR0 : MSR_F15H_PERF_CTR = 0xc0010201
+ *   - MSR_K7_EVNTSEL0    : 0xc0010000
+ *   - MSR_K7_PERFCTR0    : 0xc0010004
+ * INDEX_ONE
+ *   - MSR_F15H_PERF_CTL1 : MSR_F15H_PERF_CTL + 2 = 0xc0010202
+ *   - MSR_F15H_PERF_CTR1 : MSR_F15H_PERF_CTR + 2 = 0xc0010203
+ *   - MSR_K7_EVNTSEL1    : 0xc0010001
+ *   - MSR_K7_PERFCTR1    : 0xc0010005
+ * INDEX_TWO
+ *   - MSR_F15H_PERF_CTL2 : MSR_F15H_PERF_CTL + 4 = 0xc0010204
+ *   - MSR_F15H_PERF_CTR2 : MSR_F15H_PERF_CTR + 4 = 0xc0010205
+ *   - MSR_K7_EVNTSEL2    : 0xc0010002
+ *   - MSR_K7_PERFCTR2    : 0xc0010006
+ * INDEX_THREE
+ *   - MSR_F15H_PERF_CTL3 : MSR_F15H_PERF_CTL + 6 = 0xc0010206
+ *   - MSR_F15H_PERF_CTR3 : MSR_F15H_PERF_CTR + 6 = 0xc0010207
+ *   - MSR_K7_EVNTSEL3    : 0xc0010003
+ *   - MSR_K7_PERFCTR3    : 0xc0010007
+ * INDEX_FOUR
+ *   - MSR_F15H_PERF_CTL4 : MSR_F15H_PERF_CTL + 8 = 0xc0010208
+ *   - MSR_F15H_PERF_CTR4 : MSR_F15H_PERF_CTR + 8 = 0xc0010209
+ * INDEX_FIVE
+ *   - MSR_F15H_PERF_CTL5 : MSR_F15H_PERF_CTL + 10 = 0xc001020a
+ *   - MSR_F15H_PERF_CTR5 : MSR_F15H_PERF_CTR + 10 = 0xc001020b
+ *
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|126| <<get_gp_pmc_amd>> return &pmu->gp_counters[msr_to_index(msr)];
+ */
 static enum index msr_to_index(u32 msr)
 {
 	switch (msr) {
@@ -84,6 +125,43 @@ static enum index msr_to_index(u32 msr)
 	}
 }
 
+/*
+ * INDEX_ZERO
+ *   - MSR_F15H_PERF_CTL0 : MSR_F15H_PERF_CTL = 0xc0010200
+ *   - MSR_F15H_PERF_CTR0 : MSR_F15H_PERF_CTR = 0xc0010201
+ *   - MSR_K7_EVNTSEL0    : 0xc0010000
+ *   - MSR_K7_PERFCTR0    : 0xc0010004
+ * INDEX_ONE
+ *   - MSR_F15H_PERF_CTL1 : MSR_F15H_PERF_CTL + 2 = 0xc0010202
+ *   - MSR_F15H_PERF_CTR1 : MSR_F15H_PERF_CTR + 2 = 0xc0010203
+ *   - MSR_K7_EVNTSEL1    : 0xc0010001
+ *   - MSR_K7_PERFCTR1    : 0xc0010005
+ * INDEX_TWO
+ *   - MSR_F15H_PERF_CTL2 : MSR_F15H_PERF_CTL + 4 = 0xc0010204
+ *   - MSR_F15H_PERF_CTR2 : MSR_F15H_PERF_CTR + 4 = 0xc0010205
+ *   - MSR_K7_EVNTSEL2    : 0xc0010002
+ *   - MSR_K7_PERFCTR2    : 0xc0010006
+ * INDEX_THREE
+ *   - MSR_F15H_PERF_CTL3 : MSR_F15H_PERF_CTL + 6 = 0xc0010206
+ *   - MSR_F15H_PERF_CTR3 : MSR_F15H_PERF_CTR + 6 = 0xc0010207
+ *   - MSR_K7_EVNTSEL3    : 0xc0010003
+ *   - MSR_K7_PERFCTR3    : 0xc0010007
+ * INDEX_FOUR
+ *   - MSR_F15H_PERF_CTL4 : MSR_F15H_PERF_CTL + 8 = 0xc0010208
+ *   - MSR_F15H_PERF_CTR4 : MSR_F15H_PERF_CTR + 8 = 0xc0010209
+ * INDEX_FIVE
+ *   - MSR_F15H_PERF_CTL5 : MSR_F15H_PERF_CTL + 10 = 0xc001020a
+ *   - MSR_F15H_PERF_CTR5 : MSR_F15H_PERF_CTR + 10 = 0xc001020b
+ *
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|155| <<amd_pmc_idx_to_pmc>> return get_gp_pmc_amd(pmu, base + pmc_idx, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|193| <<amd_msr_idx_to_pmc>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|194| <<amd_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|206| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|212| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|229| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|236| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ */
 static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 					     enum pmu_type type)
 {
@@ -126,6 +204,9 @@ static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 	return &pmu->gp_counters[msr_to_index(msr)];
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.hw_event_available = amd_hw_event_available()
+ */
 static bool amd_hw_event_available(struct kvm_pmc *pmc)
 {
 	return true;
@@ -134,11 +215,17 @@ static bool amd_hw_event_available(struct kvm_pmc *pmc)
 /* check if a PMC is enabled by comparing it against global_ctrl bits. Because
  * AMD CPU doesn't have global_ctrl MSR, all PMCs are enabled (return TRUE).
  */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_is_enabled = amd_pmc_is_enabled()
+ */
 static bool amd_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.pmc_idx_to_pmc = amd_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	unsigned int base = get_msr_base(pmu, PMU_TYPE_COUNTER);
@@ -155,6 +242,9 @@ static struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 	return get_gp_pmc_amd(pmu, base + pmc_idx, PMU_TYPE_COUNTER);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_rdpmc_ecx = amd_id_valid_rdpmc_ecx()
+ */
 static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -165,6 +255,9 @@ static bool amd_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 }
 
 /* idx is the ECX register of RDPMC instruction */
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc()
+ */
 static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	unsigned int idx, u64 *mask)
 {
@@ -179,12 +272,18 @@ static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[idx];
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.is_valid_msr = amd_is_valid_msr()
+ */
 static bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	/* All MSRs refer to exactly one PMC, so msr_idx_to_pmc is enough.  */
 	return false;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.msr_idx_to_pmc = amd_msr_idx_to_pmc()
+ */
 static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -196,6 +295,9 @@ static struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 	return pmc;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.get_msr = amd_pmu_get_msr()
+ */
 static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -218,6 +320,9 @@ static int amd_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.set_msr = amd_pmu_set_msr()
+ */
 static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -228,6 +333,10 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	/* MSR_PERFCTRn */
 	pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
 	if (pmc) {
+		/*
+		 * struct kvm_pmc *pmc:
+		 * -> u64 counter;
+		 */
 		pmc->counter += data - pmc_read_counter(pmc);
 		pmc_update_sample_period(pmc);
 		return 0;
@@ -246,10 +355,20 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.refresh = amd_pmu_refresh()
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
 
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_gp_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|335| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
+	 *   - arch/x86/kvm/svm/pmu.c|337| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|564| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|587| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int , eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_PERFCTR_CORE))
 		pmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS_CORE;
 	else
@@ -261,11 +380,21 @@ static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 	pmu->version = 1;
 	/* not applicable to AMD; but clean them to prevent any fall out */
 	pmu->counter_bitmask[KVM_PMC_FIXED] = 0;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	pmu->nr_arch_fixed_counters = 0;
 	pmu->global_status = 0;
 	bitmap_set(pmu->all_valid_pmc_idx, 0, pmu->nr_arch_gp_counters);
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.init = amd_pmu_init()
+ */
 static void amd_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -281,6 +410,9 @@ static void amd_pmu_init(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops amd_pmu_ops.reset = amd_pmu_reset()
+ */
 static void amd_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index f3813dbac..6b7eb11d7 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -744,6 +744,11 @@ static bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)
 	return !!test_bit(bit_write,  &tmp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|788| <<set_msr_interception>> set_msr_interception_bitmap(vcpu, msrpm, msr, read, write);
+ *   - arch/x86/kvm/svm/svm.c|861| <<svm_msr_filter_changed>> set_msr_interception_bitmap(vcpu, svm->msrpm, msr, read, write);
+ */
 static void set_msr_interception_bitmap(struct kvm_vcpu *vcpu, u32 *msrpm,
 					u32 msr, int read, int write)
 {
@@ -781,6 +786,33 @@ static void set_msr_interception_bitmap(struct kvm_vcpu *vcpu, u32 *msrpm,
 	svm->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/sev.c|2977| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_EFER, 1, 1);
+ *   - arch/x86/kvm/svm/sev.c|2978| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_CR_PAT, 1, 1);
+ *   - arch/x86/kvm/svm/sev.c|2979| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);
+ *   - arch/x86/kvm/svm/sev.c|2980| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);
+ *   - arch/x86/kvm/svm/sev.c|2981| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);
+ *   - arch/x86/kvm/svm/sev.c|2982| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 1, 1);
+ *   - arch/x86/kvm/svm/sev.c|2987| <<sev_es_init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_TSC_AUX, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|818| <<svm_vcpu_init_msrpm>> set_msr_interception(vcpu, msrpm, direct_access_msrs[i].index, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|839| <<svm_set_x2apic_msr_interception>> set_msr_interception(&svm->vcpu, svm->msrpm, index,
+ *   - arch/x86/kvm/svm/svm.c|929| <<svm_enable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|930| <<svm_enable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|931| <<svm_enable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|932| <<svm_enable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|944| <<svm_disable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 0, 0);
+ *   - arch/x86/kvm/svm/svm.c|945| <<svm_disable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 0, 0);
+ *   - arch/x86/kvm/svm/svm.c|946| <<svm_disable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 0, 0);
+ *   - arch/x86/kvm/svm/svm.c|947| <<svm_disable_lbrv>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 0, 0);
+ *   - arch/x86/kvm/svm/svm.c|1164| <<init_vmcb_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_EIP, 0, 0);
+ *   - arch/x86/kvm/svm/svm.c|1165| <<init_vmcb_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_ESP, 0, 0);
+ *   - arch/x86/kvm/svm/svm.c|1179| <<init_vmcb_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_EIP, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|1180| <<init_vmcb_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_ESP, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|1310| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2952| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|2967| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0, 1);
+ */
 void set_msr_interception(struct kvm_vcpu *vcpu, u32 *msrpm, u32 msr,
 			  int read, int write)
 {
@@ -1351,6 +1383,13 @@ static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 		__svm_vcpu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|804| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/nested.c|1021| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1177| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1667| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ */
 void svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)
 {
 	svm->current_vmcb = target_vmcb;
@@ -2102,6 +2141,10 @@ static int mc_interception(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用shutdown_interception():
+ *   - arch/x86/kvm/svm/svm.c|3204| <<global>> [SVM_EXIT_SHUTDOWN] = shutdown_interception,
+ */
 static int shutdown_interception(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *kvm_run = vcpu->run;
@@ -3067,6 +3110,11 @@ static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 	return 0;
 }
 
+/*
+ * 在以下使用msr_interception():
+ *   - arch/x86/kvm/svm/svm.c|3197| <<global>> [SVM_EXIT_MSR] = msr_interception,
+ *   - arch/x86/kvm/svm/svm.c|3373| <<svm_invoke_exit_handler>> return msr_interception(vcpu);
+ */
 static int msr_interception(struct kvm_vcpu *vcpu)
 {
 	if (to_svm(vcpu)->vmcb->control.exit_info_1)
@@ -4903,6 +4951,10 @@ static __init void svm_adjust_mmio_mask(void)
 	kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5117| <<svm_hardware_setup>> svm_set_cpu_caps();
+ */
 static __init void svm_set_cpu_caps(void)
 {
 	kvm_set_cpu_caps();
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 6a7686bf6..c6bb84000 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -256,6 +256,14 @@ struct vcpu_svm {
 	u32 ldr_reg;
 	u32 dfr_reg;
 	struct page *avic_backing_page;
+	/*
+	 * 在以下使用vcpu_svm->avic_physical_id_cache:
+	 *   - arch/x86/kvm/svm/avic.c|319| <<avic_init_backing_page>> svm->avic_physical_id_cache = entry;
+	 *   - arch/x86/kvm/svm/avic.c|1066| <<avic_vcpu_load>> entry = READ_ONCE(*(svm->avic_physical_id_cache));
+	 *   - arch/x86/kvm/svm/avic.c|1072| <<avic_vcpu_load>> WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
+	 *   - arch/x86/kvm/svm/avic.c|1083| <<avic_vcpu_put>> entry = READ_ONCE(*(svm->avic_physical_id_cache));
+	 *   - arch/x86/kvm/svm/avic.c|1092| <<avic_vcpu_put>> WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
+	 */
 	u64 *avic_physical_id_cache;
 
 	/*
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index c5e5dfef6..54b32557f 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -21,6 +21,28 @@ extern int __read_mostly pt_mode;
 #define PT_MODE_SYSTEM		0
 #define PT_MODE_HOST_GUEST	1
 
+/*
+ * version 1:
+ *
+ * IA32_PMCx MSRs 从 0x0c1开始
+ * IA32_PERFEVTSELx MSRs 从0x186开始
+ *
+ * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+ * IA32_PMCx从0x4c1开始
+ *
+ * kvm-unit-tests中:
+ *
+ * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+ * 821                 gp_counter_base = MSR_IA32_PMC0;
+ * 822                 report_prefix_push("full-width writes");
+ * 823                 check_counters();
+ * 824                 check_gp_counters_write_width();
+ * 825         }
+ *
+ * 在以下使用PMU_CAP_FW_WRITES:
+ *   - arch/x86/kvm/vmx/capabilities.h|406| <<vmx_get_perf_capabilities>> u64 perf_cap = PMU_CAP_FW_WRITES;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+ */
 #define PMU_CAP_FW_WRITES	(1ULL << 13)
 #define PMU_CAP_LBR_FMT		0x3f
 
@@ -401,8 +423,38 @@ static inline bool vmx_pebs_supported(void)
 	return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/capabilities.h|477| <<vmx_supported_debugctl>> if (vmx_get_perf_capabilities() & PMU_CAP_LBR_FMT)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|774| <<intel_pmu_init>> vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();
+ *   - arch/x86/kvm/vmx/vmx.c|1857| <<vmx_get_msr_feature(MSR_IA32_PERF_CAPABILITIES)>> msr->data = vmx_get_perf_capabilities();
+ *   - arch/x86/kvm/vmx/vmx.c|2350| <<vmx_set_msr(MSR_IA32_PERF_CAPABILITIES)>> (vmx_get_perf_capabilities() & PMU_CAP_LBR_FMT))
+ *   - arch/x86/kvm/vmx/vmx.c|2357| <<vmx_set_msr(MSR_IA32_PERF_CAPABILITIES)>> (vmx_get_perf_capabilities() & PERF_CAP_PEBS_MASK))
+ */
 static inline u64 vmx_get_perf_capabilities(void)
 {
+	/*
+	 * version 1:
+	 *
+	 * IA32_PMCx MSRs 从 0x0c1开始
+	 * IA32_PERFEVTSELx MSRs 从0x186开始
+	 *
+	 * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+	 * IA32_PMCx从0x4c1开始
+	 *
+	 * kvm-unit-tests中:
+	 *
+	 * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+	 * 821                 gp_counter_base = MSR_IA32_PMC0;
+	 * 822                 report_prefix_push("full-width writes");
+	 * 823                 check_counters();
+	 * 824                 check_gp_counters_write_width();
+	 * 825         }
+	 *
+	 * 在以下使用PMU_CAP_FW_WRITES:
+	 *   - arch/x86/kvm/vmx/capabilities.h|406| <<vmx_get_perf_capabilities>> u64 perf_cap = PMU_CAP_FW_WRITES;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+	 */
 	u64 perf_cap = PMU_CAP_FW_WRITES;
 	u64 host_perf_cap = 0;
 
diff --git a/arch/x86/kvm/vmx/evmcs.c b/arch/x86/kvm/vmx/evmcs.c
index 6a61b1ae7..ed0da46a4 100644
--- a/arch/x86/kvm/vmx/evmcs.c
+++ b/arch/x86/kvm/vmx/evmcs.c
@@ -10,6 +10,38 @@
 #include "vmx.h"
 #include "trace.h"
 
+/*
+ * When running nested KVM on Hyper-V it's possible to use so called
+ * 'Enlightened VMCS' and do normal memory reads/writes instead of
+ * doing VMWRITE/VMREAD instructions. In addition, clean field mask
+ * provides a huge room for optimization on L0's side.
+ */
+
+/*
+ * 在以下使用enable_evmcs:
+ *   - arch/x86/kvm/vmx/evmcs.c|13| <<global>> DEFINE_STATIC_KEY_FALSE(enable_evmcs);
+ *   - arch/x86/kvm/vmx/evmcs.h|17| <<global>> DECLARE_STATIC_KEY_FALSE(enable_evmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|2452| <<vmx_hardware_enable>> if (static_branch_unlikely(&enable_evmcs) &&
+ *   - arch/x86/kvm/vmx/vmx.c|2795| <<alloc_vmcs_cpu>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx.c|2890| <<alloc_kvm_area>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx.c|3890| <<vmx_msr_bitmap_l01_changed>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx.c|7211| <<vmx_vcpu_run>> if (static_branch_unlikely(&enable_evmcs)) {
+ *   - arch/x86/kvm/vmx/vmx.c|7344| <<vmx_vcpu_create>> if (IS_ENABLED(CONFIG_HYPERV) && static_branch_unlikely(&enable_evmcs) &&
+ *   - arch/x86/kvm/vmx/vmx.c|8456| <<vmx_exit>> if (static_branch_unlikely(&enable_evmcs)) {
+ *   - arch/x86/kvm/vmx/vmx.c|8475| <<vmx_exit>> static_branch_disable(&enable_evmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|8509| <<vmx_init>> static_branch_enable(&enable_evmcs);
+ *   - arch/x86/kvm/vmx/vmx_ops.h|134| <<vmcs_read16>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|142| <<vmcs_read32>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|150| <<vmcs_read64>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|162| <<vmcs_readl>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|209| <<vmcs_write16>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|218| <<vmcs_write32>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|227| <<vmcs_write64>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|239| <<vmcs_writel>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|249| <<vmcs_clear_bits>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|259| <<vmcs_set_bits>> if (static_branch_unlikely(&enable_evmcs))
+ *   - arch/x86/kvm/vmx/vmx_ops.h|276| <<vmcs_load>> if (static_branch_unlikely(&enable_evmcs))
+ */
 DEFINE_STATIC_KEY_FALSE(enable_evmcs);
 
 #define EVMCS1_OFFSET(x) offsetof(struct hv_enlightened_vmcs, x)
@@ -295,6 +327,10 @@ const struct evmcs_field vmcs_field_to_evmcs_1[] = {
 const unsigned int nr_evmcs_1_fields = ARRAY_SIZE(vmcs_field_to_evmcs_1);
 
 #if IS_ENABLED(CONFIG_HYPERV)
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2776| <<setup_vmcs_config>> evmcs_sanitize_exec_ctrls(vmcs_conf);
+ */
 __init void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf)
 {
 	vmcs_conf->cpu_based_exec_ctrl &= ~EVMCS1_UNSUPPORTED_EXEC_CTRL;
@@ -307,6 +343,11 @@ __init void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1988| <<nested_vmx_handle_enlightened_vmptrld>> if (!nested_enlightened_vmentry(vcpu, &evmcs_gpa)) {
+ *   - arch/x86/kvm/vmx/nested.c|5071| <<handle_vmclear>> !nested_enlightened_vmentry(vcpu, &evmcs_gpa))) {
+ */
 bool nested_enlightened_vmentry(struct kvm_vcpu *vcpu, u64 *evmcs_gpa)
 {
 	struct hv_vp_assist_page assist_page;
@@ -327,6 +368,11 @@ bool nested_enlightened_vmentry(struct kvm_vcpu *vcpu, u64 *evmcs_gpa)
 	return true;
 }
 
+/*
+ * 在以下使用nested_get_evmcs_version():
+ *   - arch/x86/kvm/vmx/nested.c|6826| <<global>> struct kvm_x86_nested_ops vmx_nested_ops.get_evmcs_version = nested_get_evmcs_version()
+ *   - arch/x86/kvm/vmx/evmcs.c|438| <<nested_enable_evmcs>> *vmcs_version = nested_get_evmcs_version(vcpu);
+ */
 uint16_t nested_get_evmcs_version(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -342,6 +388,10 @@ uint16_t nested_get_evmcs_version(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * 处理MSR_IA32_VMX_BASIC..MSR_IA32_VMX_VMFUNC:
+ *   - arch/x86/kvm/vmx/vmx.c|1935| <<vmx_get_msr>> nested_evmcs_filter_control_msr(msr_info->index,
+ */
 void nested_evmcs_filter_control_msr(u32 msr_index, u64 *pdata)
 {
 	u32 ctl_low = (u32)*pdata;
@@ -375,6 +425,10 @@ void nested_evmcs_filter_control_msr(u32 msr_index, u64 *pdata)
 	*pdata = ctl_low | ((u64)ctl_high << 32);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2867| <<nested_vmx_check_controls>> return nested_evmcs_check_controls(vmcs12);
+ */
 int nested_evmcs_check_controls(struct vmcs12 *vmcs12)
 {
 	int ret = 0;
@@ -427,6 +481,9 @@ int nested_evmcs_check_controls(struct vmcs12 *vmcs12)
 	return ret;
 }
 
+/*
+ * struct kvm_x86_nested_ops vmx_nested_ops.enable_evmcs = nested_enable_evmcs()
+ */
 int nested_enable_evmcs(struct kvm_vcpu *vcpu,
 			uint16_t *vmcs_version)
 {
diff --git a/arch/x86/kvm/vmx/evmcs.h b/arch/x86/kvm/vmx/evmcs.h
index f886a8ff0..f9f5d917a 100644
--- a/arch/x86/kvm/vmx/evmcs.h
+++ b/arch/x86/kvm/vmx/evmcs.h
@@ -16,6 +16,24 @@ struct vmcs_config;
 
 DECLARE_STATIC_KEY_FALSE(enable_evmcs);
 
+/*
+ * 在以下使用current_evmcs:
+ *   - arch/x86/kvm/vmx/evmcs.h|135| <<evmcs_write64>> *(u64 *)((char *)current_evmcs + offset) = value;
+ *   - arch/x86/kvm/vmx/evmcs.h|137| <<evmcs_write64>> current_evmcs->hv_clean_fields &= ~clean_field;
+ *   - arch/x86/kvm/vmx/evmcs.h|148| <<evmcs_write32>> *(u32 *)((char *)current_evmcs + offset) = value;
+ *   - arch/x86/kvm/vmx/evmcs.h|149| <<evmcs_write32>> current_evmcs->hv_clean_fields &= ~clean_field;
+ *   - arch/x86/kvm/vmx/evmcs.h|160| <<evmcs_write16>> *(u16 *)((char *)current_evmcs + offset) = value;
+ *   - arch/x86/kvm/vmx/evmcs.h|161| <<evmcs_write16>> current_evmcs->hv_clean_fields &= ~clean_field;
+ *   - arch/x86/kvm/vmx/evmcs.h|171| <<evmcs_read64>> return *(u64 *)((char *)current_evmcs + offset);
+ *   - arch/x86/kvm/vmx/evmcs.h|181| <<evmcs_read32>> return *(u32 *)((char *)current_evmcs + offset);
+ *   - arch/x86/kvm/vmx/evmcs.h|191| <<evmcs_read16>> return *(u16 *)((char *)current_evmcs + offset);
+ *   - arch/x86/kvm/vmx/evmcs.h|196| <<evmcs_touch_msr_bitmap>> if (unlikely(!current_evmcs))
+ *   - arch/x86/kvm/vmx/evmcs.h|199| <<evmcs_touch_msr_bitmap>> if (current_evmcs->hv_enlightenments_control.msr_bitmap)
+ *   - arch/x86/kvm/vmx/evmcs.h|200| <<evmcs_touch_msr_bitmap>> current_evmcs->hv_clean_fields &=
+ *   - arch/x86/kvm/vmx/evmcs.h|209| <<evmcs_load>> if (current_evmcs->hv_enlightenments_control.nested_flush_hypercall)
+ *   - arch/x86/kvm/vmx/vmx.c|7212| <<vmx_vcpu_run>> current_evmcs->hv_clean_fields |=
+ *   - arch/x86/kvm/vmx/vmx.c|7215| <<vmx_vcpu_run>> current_evmcs->hv_vp_id = kvm_hv_get_vpindex(vcpu);
+ */
 #define current_evmcs ((struct hv_enlightened_vmcs *)this_cpu_read(current_vmcs))
 
 #define KVM_EVMCS_VERSION 1
@@ -191,6 +209,10 @@ static inline u16 evmcs_read16(unsigned long field)
 	return *(u16 *)((char *)current_evmcs + offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3901| <<vmx_msr_bitmap_l01_changed>> evmcs_touch_msr_bitmap();
+ */
 static inline void evmcs_touch_msr_bitmap(void)
 {
 	if (unlikely(!current_evmcs))
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index c399637a3..043cab0ba 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -18,8 +18,21 @@
 #include "nested.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用MSR_PMC_FULL_WIDTH_BIT:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|503| <<intel_pmu_set_msr>> if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|507| <<intel_pmu_set_msr>> !(msr & MSR_PMC_FULL_WIDTH_BIT))
+ */
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|100| <<intel_hw_event_available>> for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|101| <<intel_hw_event_available>> if (intel_arch_events[i].eventsel != event_select ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|102| <<intel_hw_event_available>> intel_arch_events[i].unit_mask != unit_mask)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+ *   - arch/x86/kvm/vmx/pmu_intel.c|507| <<setup_fixed_pmc_eventsel>> intel_arch_events[event].eventsel;
+ */
 static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 	[0] = { 0x3c, 0x00, PERF_COUNT_HW_CPU_CYCLES },
 	[1] = { 0xc0, 0x00, PERF_COUNT_HW_INSTRUCTIONS },
@@ -33,8 +46,18 @@ static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 };
 
 /* mapping between fixed pmc index and intel_arch_events array */
+/*
+ * 在以下使用fixed_pmc_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|492| <<setup_fixed_pmc_eventsel>> size_t size = ARRAY_SIZE(fixed_pmc_events);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|499| <<setup_fixed_pmc_eventsel>> event = fixed_pmc_events[array_index_nospec(i, size)];
+ *   - arch/x86/kvm/vmx/pmu_intel.c|553| <<intel_pmu_refresh>> min3(ARRAY_SIZE(fixed_pmc_events),
+ */
 static int fixed_pmc_events[] = {1, 0, 7};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|480| <<intel_pmu_set_msr(MSR_CORE_PERF_FIXED_CTR_CTRL)>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	struct kvm_pmc *pmc;
@@ -42,6 +65,13 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 	int i;
 
 	pmu->fixed_ctr_ctrl = data;
+	/*
+	 * 在以下设置kvm_pmu->nr_arch_fixed_counters:
+	 *   - arch/x86/kvm/svm/pmu.c|345| <<amd_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|598| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|600| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min( edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+	 */
 	for (i = 0; i < pmu->nr_arch_fixed_counters; i++) {
 		u8 new_ctrl = fixed_ctrl_field(data, i);
 		u8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);
@@ -56,6 +86,9 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_idx_to_pmc = intel_pmc_idx_to_pmc()
+ */
 static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	if (pmc_idx < INTEL_PMC_IDX_FIXED) {
@@ -69,6 +102,10 @@ static struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)
 }
 
 /* function is called when global control register has been updated. */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|505| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_CTRL)>> global_ctrl_changed(pmu, data);
+ */
 static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 {
 	int bit;
@@ -84,6 +121,9 @@ static void global_ctrl_changed(struct kvm_pmu *pmu, u64 data)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.hw_event_available = 
+ */
 static bool intel_hw_event_available(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -107,6 +147,9 @@ static bool intel_hw_event_available(struct kvm_pmc *pmc)
 }
 
 /* check if a PMC is enabled by comparing it with globl_ctrl bits. */
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.pmc_is_enabled = 
+ */
 static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -117,6 +160,9 @@ static bool intel_pmc_is_enabled(struct kvm_pmc *pmc)
 	return test_bit(pmc->idx, (unsigned long *)&pmu->global_ctrl);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_rdpmc_ecx =
+ */
 static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -128,6 +174,9 @@ static bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 		     : idx < pmu->nr_arch_gp_counters;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.rdpmc_ecx_to_pmc = 
+ */
 static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 					    unsigned int idx, u64 *mask)
 {
@@ -150,6 +199,13 @@ static struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	return &counters[array_index_nospec(idx, num_counters)];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|224| <<fw_writes_is_enabled>> return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|270| <<intel_is_valid_msr>> ret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|276| <<intel_is_valid_msr>> perf_capabilities = vcpu_get_perf_capabilities(vcpu);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|672| <<intel_pmu_refresh>> perf_capabilities = vcpu_get_perf_capabilities(vcpu);
+ */
 static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
 {
 	if (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))
@@ -158,6 +214,26 @@ static inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)
 	return vcpu->arch.perf_capabilities;
 }
 
+/*
+ * version 1:
+ *
+ * IA32_PMCx MSRs 从 0x0c1开始
+ * IA32_PERFEVTSELx MSRs 从0x186开始
+ *
+ * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+ * IA32_PMCx从0x4c1开始
+ *
+ * 在kvm-unit-tests
+ *
+ * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+ * 821                 gp_counter_base = MSR_IA32_PMC0;822                 report_prefix_push("full-width writes");
+ * 823                 check_counters();
+ * 824                 check_gp_counters_write_width();
+ * 825         }
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|197| <<get_fw_gp_pmc>> if (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))
+ */
 static inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)
 {
 	return (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;
@@ -189,6 +265,9 @@ static bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.is_valid_msr =
+ */
 static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -224,6 +303,9 @@ static bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	return ret;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.msr_idx_to_pmc =
+ */
 static struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -341,6 +423,9 @@ static bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.get_msr = intel_pmu_get_msr()
+ */
 static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -391,6 +476,9 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.set_msr = intel_pmu_set_msr()
+ */
 static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -423,6 +511,17 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		}
 		break;
 	case MSR_CORE_PERF_GLOBAL_OVF_CTRL:
+		/*
+		 * IA32_PERF_GLOBAL_CTRL     : 控制fixed和gp的enable/disable
+		 * IA32_PERF_GLOBAL_STATUS   : 负责query fixed和gp的overflow conditions
+		 * IA32_PERF_GLOBAL_OVF_CTRL : 负责clear fixed和gp的overflow status
+		 *
+		 * 只在intel使用kvm_pmu->global_ovf_ctrl_mask:
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|510| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL)>> if (!(data & pmu->global_ovf_ctrl_mask)) {
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|619| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = ~0ull;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|663| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|667| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+		 */
 		if (!(data & pmu->global_ovf_ctrl_mask)) {
 			if (!msr_info->host_initiated)
 				pmu->global_status &= ~data;
@@ -453,6 +552,27 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		}
 		break;
 	default:
+		/*
+		 * version 1:
+		 *
+		 * IA32_PMCx MSRs 从 0x0c1开始
+		 * IA32_PERFEVTSELx MSRs 从0x186开始
+		 *
+		 * 当IA_PERF_CAPABILITIES.FW_WRITE[bit 13] == 1的时候:
+		 * IA32_PMCx从0x4c1开始
+		 *
+		 * kvm-unit-tests中:
+		 *
+		 * 820         if (rdmsr(MSR_IA32_PERF_CAPABILITIES) & PMU_CAP_FW_WRITES) {
+		 * 821                 gp_counter_base = MSR_IA32_PMC0;
+		 * 822                 report_prefix_push("full-width writes");
+		 * 823                 check_counters();
+		 * 824                 check_gp_counters_write_width();
+		 * 825         }
+		 *
+		 * MSR_IA32_PERFCTR0 0x000000c1
+		 * MSR_IA32_PMC0     0x000004c1
+		 */
 		if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
 		    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
 			if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
@@ -487,6 +607,10 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|604| <<intel_pmu_refresh>> setup_fixed_pmc_eventsel(pmu);
+ */
 static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 {
 	size_t size = ARRAY_SIZE(fixed_pmc_events);
@@ -502,6 +626,9 @@ static void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.refresh = intel_pmu_refresh()
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -520,12 +647,39 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 	pmu->version = 0;
 	pmu->reserved_bits = 0xffffffff00200000ull;
 	pmu->raw_event_mask = X86_RAW_EVENT_MASK;
+	/*
+	 * 在以下使用kvm_pmu->global_ctrl_mask:
+	 *   - arch/x86/kvm/pmu.h|161| <<kvm_valid_perf_global_ctrl>> return !(pmu->global_ctrl_mask & data);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|618| <<intel_pmu_refresh>> pmu->global_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|662| <<intel_pmu_refresh>> pmu->global_ctrl_mask = counter_mask;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|663| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+	 */
 	pmu->global_ctrl_mask = ~0ull;
+	/*
+	 * 只在intel使用kvm_pmu->global_ovf_ctrl_mask:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|510| <<intel_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL)>> if (!(data & pmu->global_ovf_ctrl_mask)) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|619| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|663| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask = pmu->global_ctrl_mask & ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF | MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|667| <<intel_pmu_refresh>> pmu->global_ovf_ctrl_mask &= ~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;
+	 */
 	pmu->global_ovf_ctrl_mask = ~0ull;
+	/*
+	 * 在以下使用kvm_pmu->fixed_ctr_ctrl_mask:
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|490| <<intel_pmu_set_msr>> if (!(data & pmu->fixed_ctr_ctrl_mask)) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|620| <<intel_pmu_refresh>> pmu->fixed_ctr_ctrl_mask = ~0ull;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|659| <<intel_pmu_refresh>> pmu->fixed_ctr_ctrl_mask &= ~(0xbull << (i * 4));
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|698| <<intel_pmu_refresh>> pmu->fixed_ctr_ctrl_mask &= ~(1ULL << (INTEL_PMC_IDX_FIXED + i * 4));
+	 *
+	 * 似乎是记录被支持的bit
+	 */
 	pmu->fixed_ctr_ctrl_mask = ~0ull;
 	pmu->pebs_enable_mask = ~0ull;
 	pmu->pebs_data_cfg_mask = ~0ull;
 
+	/*
+	 * 因为intel vmx的pmu->nr_arch_gp_counters似乎只在这里更新
+	 * 所以在intel_pmu_refresh()判断是否disable pmu是合理的
+	 */
 	entry = kvm_find_cpuid_entry(vcpu, 0xa);
 	if (!entry || !vcpu->kvm->arch.enable_pmu)
 		return;
@@ -557,6 +711,10 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 						  kvm_pmu_cap.bit_width_fixed);
 		pmu->counter_bitmask[KVM_PMC_FIXED] =
 			((u64)1 << edx.split.bit_width_fixed) - 1;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|604| <<intel_pmu_refresh>> setup_fixed_pmc_eventsel(pmu);
+		 */
 		setup_fixed_pmc_eventsel(pmu);
 	}
 
@@ -611,6 +769,9 @@ static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.init = intel_pmu_init()
+ */
 static void intel_pmu_init(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -637,6 +798,9 @@ static void intel_pmu_init(struct kvm_vcpu *vcpu)
 	lbr_desc->msr_passthrough = false;
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.reset = intel_pmu_reset()
+ */
 static void intel_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -680,6 +844,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.deliver_pmi = intel_pmu_deliver_pmi()
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
@@ -767,6 +934,9 @@ void vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)
 		vcpu->vcpu_id);
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.cleanup = intel_pmu_cleanup()
+ */
 static void intel_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
 	if (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index 1b56c5e5c..cbfd02c3d 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -19,6 +19,13 @@
  * wake the target vCPUs.  vCPUs are removed from the list and the notification
  * vector is reset when the vCPU is scheduled in.
  */
+/*
+ * 在以下使用wakeup_vcpus_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|28| <<global>> static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_enable_wakeup_handler>> &per_cpu(wakeup_vcpus_on_cpu, vcpu->cpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|219| <<pi_wakeup_handler>> struct list_head *wakeup_list = &per_cpu(wakeup_vcpus_on_cpu, cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|234| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(wakeup_vcpus_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, wakeup_vcpus_on_cpu);
 /*
  * Protect the per-CPU list with a per-CPU spinlock to handle task migration.
@@ -141,6 +148,10 @@ static bool vmx_can_use_vtd_pi(struct kvm *kvm)
  * Put the vCPU on this pCPU's list of vCPUs that needs to be awakened and set
  * WAKEUP as the notification vector in the PI descriptor.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/posted_intr.c|202| <<vmx_vcpu_pi_put>> pi_enable_wakeup_handler(vcpu);
+ */
 static void pi_enable_wakeup_handler(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -191,6 +202,10 @@ static bool vmx_needs_pi_wakeup(struct kvm_vcpu *vcpu)
 	return vmx_can_use_ipiv(vcpu) || vmx_can_use_vtd_pi(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1402| <<vmx_vcpu_put>> vmx_vcpu_pi_put(vcpu);
+ */
 void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -267,6 +282,12 @@ void vmx_pi_start_assignment(struct kvm *kvm)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|14112| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 1);
+ *   - arch/x86/kvm/x86.c|14137| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_pi_update_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|14152| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_pi_update_irte)(kvm, host_irq, guest_irq, set);
+ */
 int vmx_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 		       uint32_t guest_irq, bool set)
 {
@@ -311,6 +332,9 @@ int vmx_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 		kvm_set_msi_irq(kvm, e, &irq);
 		if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
 		    !kvm_irq_is_postable(&irq)) {
+			/*
+			 * intel_ir_set_vcpu_affinity()
+			 */
 			/*
 			 * Make sure the IRTE is in remapped mode if
 			 * we don't handle it in posted mode.
@@ -332,6 +356,9 @@ int vmx_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 		trace_kvm_pi_irte_update(host_irq, vcpu->vcpu_id, e->gsi,
 				vcpu_info.vector, vcpu_info.pi_desc_addr, set);
 
+		/*
+		 * intel_ir_set_vcpu_affinity()
+		 */
 		if (set)
 			ret = irq_set_vcpu_affinity(host_irq, &vcpu_info);
 		else
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index ac290a44a..f19c5f2f6 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -62,12 +62,41 @@ struct loaded_vmcs {
 	struct vmcs *vmcs;
 	struct vmcs *shadow_vmcs;
 	int cpu;
+	/*
+	 * 在以下使用loaded_vmcs->launched:
+	 *   - arch/x86/kvm/vmx/vmx.c|729| <<__loaded_vmcs_clear>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx/vmx.c|744| <<__loaded_vmcs_clear>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|881| <<__vmx_vcpu_run_flags>> if (vmx->loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx/vmx.c|2876| <<alloc_loaded_vmcs>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7358| <<vmx_vcpu_run>> vmx->loaded_vmcs->launched = 1;
+	 */
 	bool launched;
 	bool nmi_known_unmasked;
 	bool hv_timer_soft_disabled;
 	/* Support for vnmi-less CPUs */
+	/*
+	 * 在以下设置loaded_vmcs->soft_vnmi_blocked:
+	 *   - arch/x86/kvm/vmx/vmx.c|4937| <<vmx_inject_nmi>> vmx->loaded_vmcs->soft_vnmi_blocked = 1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4975| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->soft_vnmi_blocked = masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6510| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 * 在以下使用loaded_vmcs->soft_vnmi_blocked:
+	 *   - arch/x86/kvm/vmx/vmx.c|4961| <<vmx_get_nmi_mask>> return vmx->loaded_vmcs->soft_vnmi_blocked;
+	 *   - arch/x86/kvm/vmx/vmx.c|4974| <<vmx_set_nmi_mask>> if (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4994| <<vmx_nmi_blocked>> if (!enable_vnmi && to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)
+	 *   - arch/x86/kvm/vmx/vmx.c|6496| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6994| <<vmx_recover_nmi_blocking>> } else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
+	 *   - arch/x86/kvm/vmx/vmx.c|7202| <<vmx_vcpu_run>> if (unlikely(!enable_vnmi && vmx->loaded_vmcs->soft_vnmi_blocked))
+	 */
 	int soft_vnmi_blocked;
 	ktime_t entry_time;
+	/*
+	 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|4938| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|4976| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6499| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL && vcpu->arch.nmi_pending) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6995| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time += ktime_to_ns(ktime_sub(ktime_get(), vmx->loaded_vmcs->entry_time));
+	 */
 	s64 vnmi_blocked_time;
 	unsigned long *msr_bitmap;
 	struct list_head loaded_vmcss_on_cpu_link;
@@ -146,6 +175,15 @@ static inline bool is_icebp(u32 intr_info)
 	return is_intr_type(intr_info, INTR_TYPE_PRIV_SW_EXCEPTION);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5915| <<nested_vmx_l0_wants_exit>> if (is_nmi(intr_info))
+ *   - arch/x86/kvm/vmx/nested.c|5987| <<nested_vmx_l1_wants_exit>> if (is_nmi(intr_info))
+ *   - arch/x86/kvm/vmx/vmx.c|5169| <<handle_exception_nmi>> if (is_machine_check(intr_info) || is_nmi(intr_info))
+ *   - arch/x86/kvm/vmx/vmx.c|6914| <<handle_interrupt_nmi_irqoff>> bool is_nmi = entry == (unsigned long )asm_exc_nmi_noist;
+ *   - arch/x86/kvm/vmx/vmx.c|6916| <<handle_interrupt_nmi_irqoff>> kvm_before_interrupt(vcpu, is_nmi ? KVM_HANDLING_NMI : KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/vmx/vmx.c|6956| <<handle_exception_nmi_irqoff>> else if (is_nmi(intr_info))
+ */
 static inline bool is_nmi(u32 intr_info)
 {
 	return is_intr_type(intr_info, INTR_TYPE_NMI_INTR);
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c9b49a09e..382697d54 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -160,6 +160,13 @@ module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
  * List of MSRs that can be directly passed to the guest.
  * In addition to these x2apic and PT MSRs are handled specially.
  */
+/*
+ * 在以下使用vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS]:
+ *   - arch/x86/kvm/vmx/vmx.c|641| <<possible_passthrough_msr_slot>> for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++)
+ *   - arch/x86/kvm/vmx/vmx.c|642| <<possible_passthrough_msr_slot>> if (vmx_possible_passthrough_msrs[i] == msr)
+ *   - arch/x86/kvm/vmx/vmx.c|4175| <<vmx_msr_filter_changed>> for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {
+ *   - arch/x86/kvm/vmx/vmx.c|4176| <<vmx_msr_filter_changed>> u32 msr = vmx_possible_passthrough_msrs[i];
+ */
 static u32 vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS] = {
 	MSR_IA32_SPEC_CTRL,
 	MSR_IA32_PRED_CMD,
@@ -213,6 +220,13 @@ module_param(ple_window_max, uint, 0444);
 int __read_mostly pt_mode = PT_MODE_SYSTEM;
 module_param(pt_mode, int, S_IRUGO);
 
+/*
+ * 在以下使用vmx_l1d_should_flush:
+ *   - arch/x86/kvm/vmx/vmx.c|216| <<global>> static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|311| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|313| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|7315| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
 static DEFINE_MUTEX(vmx_l1d_flush_mutex);
@@ -396,6 +410,10 @@ static __always_inline void vmx_disable_fb_clear(struct vcpu_vmx *vmx)
 	vmx->msr_ia32_mcu_opt_ctrl = msr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7333| <<vmx_vcpu_enter_exit>> vmx_enable_fb_clear(vmx);
+ */
 static __always_inline void vmx_enable_fb_clear(struct vcpu_vmx *vmx)
 {
 	if (!vmx->disable_fb_clear)
@@ -433,6 +451,15 @@ static u32 vmx_segment_access_rights(struct kvm_segment *var);
 
 void vmx_vmexit(void);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|447| <<vmread_error>> vmx_insn_failed("kvm: vmread failed: field=%lx\n", field);
+ *   - arch/x86/kvm/vmx/vmx.c|452| <<vmwrite_error>> vmx_insn_failed("kvm: vmwrite failed: field=%lx val=%lx err=%u\n",
+ *   - arch/x86/kvm/vmx/vmx.c|458| <<vmclear_error>> vmx_insn_failed("kvm: vmclear failed: %p/%llx err=%u\n",
+ *   - arch/x86/kvm/vmx/vmx.c|464| <<vmptrld_error>> vmx_insn_failed("kvm: vmptrld failed: %p/%llx err=%u\n",
+ *   - arch/x86/kvm/vmx/vmx.c|470| <<invvpid_error>> vmx_insn_failed("kvm: invvpid failed: ext=0x%lx vpid=%u gva=0x%lx\n",
+ *   - arch/x86/kvm/vmx/vmx.c|476| <<invept_error>> vmx_insn_failed("kvm: invept failed: ext=0x%lx eptp=%llx gpa=0x%llx\n",
+ */
 #define vmx_insn_failed(fmt...)		\
 do {					\
 	WARN_ONCE(1, fmt);		\
@@ -477,12 +504,38 @@ noinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)
 			ext, eptp, gpa);
 }
 
+/*
+ * 在以下使用percpu的vmxarea:
+ *   - arch/x86/kvm/vmx/vmx.c|480| <<global>> static DEFINE_PER_CPU(struct vmcs *, vmxarea);
+ *   - arch/x86/kvm/vmx/vmx.c|2462| <<vmx_hardware_enable>> u64 phys_addr = __pa(per_cpu(vmxarea, cpu));
+ *   - arch/x86/kvm/vmx/vmx.c|2887| <<free_kvm_area>> free_vmcs(per_cpu(vmxarea, cpu));
+ *   - arch/x86/kvm/vmx/vmx.c|2888| <<free_kvm_area>> per_cpu(vmxarea, cpu) = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|2918| <<alloc_kvm_area>> per_cpu(vmxarea, cpu) = vmcs;
+ */
 static DEFINE_PER_CPU(struct vmcs *, vmxarea);
+/*
+ * 在以下使用percpu的current_vmcs:
+ *   - arch/x86/kvm/vmx/vmcs.h|28| <<global>> DECLARE_PER_CPU(struct vmcs *, current_vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|481| <<global>> DEFINE_PER_CPU(struct vmcs *, current_vmcs);
+ *   - arch/x86/kvm/vmx/evmcs.h|37| <<current_evmcs>> #define current_evmcs ((struct hv_enlightened_vmcs *)this_cpu_read(current_vmcs))
+ *   - arch/x86/kvm/vmx/vmx.c|707| <<__loaded_vmcs_clear>> if (per_cpu(current_vmcs, cpu) == loaded_vmcs->vmcs)
+ *   - arch/x86/kvm/vmx/vmx.c|708| <<__loaded_vmcs_clear>> per_cpu(current_vmcs, cpu) = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|1354| <<vmx_vcpu_load_vmcs>> prev = per_cpu(current_vmcs, cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|1356| <<vmx_vcpu_load_vmcs>> per_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;
+ */
 DEFINE_PER_CPU(struct vmcs *, current_vmcs);
 /*
  * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed
  * when a CPU is brought down, and we need to VMCLEAR all VMCSs loaded on it.
  */
+/*
+ * 在以下使用percpu的loaded_vmcss_on_cpu:
+ *   - arch/x86/kvm/vmx/vmx.c|504| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|712| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|1386| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+ *   - arch/x86/kvm/vmx/vmx.c|2521| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|8830| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
 
 static DECLARE_BITMAP(vmx_vpid_bitmap, VMX_NR_VPIDS);
@@ -523,6 +576,16 @@ static inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)
 static unsigned long host_idt_base;
 
 #if IS_ENABLED(CONFIG_HYPERV)
+/*
+ * 在以下使用enlightened_vmcs:
+ *   - arch/x86/kvm/vmx/vmx.c|530| <<global>> static bool __read_mostly enlightened_vmcs = true;
+ *   - arch/x86/kvm/vmx/vmx.c|531| <<global>> module_param(enlightened_vmcs, bool, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|2779| <<setup_vmcs_config>> if (enlightened_vmcs)
+ *   - arch/x86/kvm/vmx/vmx.c|8498| <<vmx_init>> if (enlightened_vmcs &&
+ *   - arch/x86/kvm/vmx/vmx.c|8506| <<vmx_init>> enlightened_vmcs = false;
+ *   - arch/x86/kvm/vmx/vmx.c|8511| <<vmx_init>> if (enlightened_vmcs) {
+ *   - arch/x86/kvm/vmx/vmx.c|8521| <<vmx_init>> enlightened_vmcs = false;
+ */
 static bool __read_mostly enlightened_vmcs = true;
 module_param(enlightened_vmcs, bool, 0444);
 
@@ -725,6 +788,13 @@ void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs)
 			 __loaded_vmcs_clear, loaded_vmcs, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|775| <<vmx_read_guest_seg_selector>> if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_SEL))
+ *   - arch/x86/kvm/vmx/vmx.c|784| <<vmx_read_guest_seg_base>> if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_BASE))
+ *   - arch/x86/kvm/vmx/vmx.c|793| <<vmx_read_guest_seg_limit>> if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_LIMIT))
+ *   - arch/x86/kvm/vmx/vmx.c|802| <<vmx_read_guest_seg_ar>> if (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_AR))
+ */
 static bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,
 				       unsigned field)
 {
@@ -838,6 +908,10 @@ void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
 /*
  * Check if MSR is intercepted for currently loaded MSR bitmap.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|889| <<__vmx_vcpu_run_flags>> if (unlikely(!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL)))
+ */
 static bool msr_write_intercepted(struct vcpu_vmx *vmx, u32 msr)
 {
 	if (!(exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS))
@@ -846,6 +920,11 @@ static bool msr_write_intercepted(struct vcpu_vmx *vmx, u32 msr)
 	return vmx_test_msr_bitmap_write(vmx->loaded_vmcs->msr_bitmap, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3100| <<nested_vmx_check_vmentry_hw>> __vmx_vcpu_run_flags(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|7453| <<vmx_vcpu_run>> vmx_vcpu_enter_exit(vcpu, vmx, __vmx_vcpu_run_flags(vmx));
+ */
 unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)
 {
 	unsigned int flags = 0;
@@ -1255,6 +1334,17 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 #endif
 
 	vmx_set_host_fs_gs(host_state, fs_sel, gs_sel, fs_base, gs_base);
+	/*
+	 * 在以下设置vcpu_vmx->guest_state_loaded:
+	 *   - arch/x86/kvm/vmx/vmx.c|1286| <<vmx_prepare_switch_to_guest>> vmx->guest_state_loaded = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|1324| <<vmx_prepare_switch_to_host>> vmx->guest_state_loaded = false;
+	 * 在以下使用vcpu_vmx->guest_state_loaded:
+	 *   - arch/x86/kvm/vmx/nested.c|243| <<vmx_sync_vmcs_host_state>> if (unlikely(!vmx->guest_state_loaded))
+	 *   - arch/x86/kvm/vmx/vmx.c|1248| <<vmx_prepare_switch_to_guest>> if (vmx->guest_state_loaded)
+	 *   - arch/x86/kvm/vmx/vmx.c|1293| <<vmx_prepare_switch_to_host>> if (!vmx->guest_state_loaded)
+	 *   - arch/x86/kvm/vmx/vmx.c|1332| <<vmx_read_guest_kernel_gs_base>> if (vmx->guest_state_loaded)
+	 *   - arch/x86/kvm/vmx/vmx.c|1341| <<vmx_write_guest_kernel_gs_base>> if (vmx->guest_state_loaded)
+	 */
 	vmx->guest_state_loaded = true;
 }
 
@@ -1317,6 +1407,13 @@ static void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|269| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu, prev);
+ *   - arch/x86/kvm/vmx/nested.c|4117| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|4122| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/vmx.c|1413| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu, NULL);
+ */
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 			struct loaded_vmcs *buddy)
 {
@@ -2006,6 +2103,10 @@ static u64 nested_vmx_truncate_sysenter_addr(struct kvm_vcpu *vcpu,
 	return (unsigned long)data;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2091| <<vmx_set_msr>> u64 invalid = data & ~vcpu_supported_debugctl(vcpu);
+ */
 static u64 vcpu_supported_debugctl(struct kvm_vcpu *vcpu)
 {
 	u64 debugctl = vmx_supported_debugctl();
@@ -2522,6 +2623,11 @@ static __init u64 adjust_vmx_controls64(u64 ctl_opt, u32 msr)
 	return  ctl_opt & allowed;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7506| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0)
+ *   - arch/x86/kvm/vmx/vmx.c|8310| <<hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+ */
 static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 				    struct vmx_capability *vmx_cap)
 {
@@ -2787,6 +2893,18 @@ struct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags)
 	vmcs = page_address(pages);
 	memset(vmcs, 0, vmcs_config.size);
 
+	/*
+	 * struct vmcs_hdr {
+	 *   u32 revision_id:31;
+	 *   u32 shadow_vmcs:1;
+	 * };
+	 *
+	 * struct vmcs {
+	 *   struct vmcs_hdr hdr;
+	 *   u32 abort;
+	 *   char data[];
+	 * };
+	 */
 	/* KVM supports Enlightened VMCS v1 only */
 	if (static_branch_unlikely(&enable_evmcs))
 		vmcs->hdr.revision_id = KVM_EVMCS_VERSION;
@@ -3876,6 +3994,11 @@ void free_vpid(int vpid)
 	spin_unlock(&vmx_vpid_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3914| <<vmx_disable_intercept_for_msr>> vmx_msr_bitmap_l01_changed(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|3958| <<vmx_enable_intercept_for_msr>> vmx_msr_bitmap_l01_changed(vmx);
+ */
 static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 {
 	/*
@@ -3889,6 +4012,29 @@ static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 	vmx->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2078| <<vmx_set_msr>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD,
+ *   - arch/x86/kvm/vmx/vmx.c|2177| <<vmx_set_msr>> vmx_disable_intercept_for_msr(vcpu,
+ *   - arch/x86/kvm/vmx/vmx.c|2213| <<vmx_set_msr>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4033| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4034| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|4036| <<vmx_update_msr_bitmap_x2apic>> vmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_ICR), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|4093| <<vmx_msr_filter_changed>> vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|4096| <<vmx_msr_filter_changed>> vmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7365| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_TSC, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7367| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_FS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7368| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7369| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7371| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7372| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7373| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|7375| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C1_RES, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7376| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C3_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7377| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C6_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.c|7378| <<vmx_vcpu_create>> vmx_disable_intercept_for_msr(vcpu, MSR_CORE_C7_RESIDENCY, MSR_TYPE_R);
+ *   - arch/x86/kvm/vmx/vmx.h|442| <<vmx_set_intercept_for_msr>> vmx_disable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3933,6 +4079,11 @@ void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 		vmx_clear_msr_bitmap_write(msr_bitmap, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4032| <<vmx_update_msr_bitmap_x2apic>> vmx_enable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.h|440| <<vmx_set_intercept_for_msr>> vmx_enable_intercept_for_msr(vcpu, msr, type);
+ */
 void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4304,6 +4455,11 @@ void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
 	vmcs_writel(CR4_GUEST_HOST_MASK, ~vcpu->arch.cr4_guest_owned_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4413| <<vmx_refresh_apicv_exec_ctrl>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|4665| <<init_vmcs>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+ */
 static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 {
 	u32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;
@@ -6105,6 +6261,12 @@ static void vmx_dump_msrs(char *name, struct vmx_msrs *m)
 		pr_err("  %2d: msr=0x%08x value=0x%016llx\n", i, e->index, e->value);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6364| <<__vmx_handle_exit>> dump_vmcs(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6373| <<__vmx_handle_exit>> dump_vmcs(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6460| <<__vmx_handle_exit>> dump_vmcs(vcpu);
+ */
 void dump_vmcs(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6352,6 +6514,13 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 			return 1;
 	}
 
+	/*
+	 * 在以下设置vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/nested.c|4399| <<load_vmcs12_host_state>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1486| <<vmx_set_rflags>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3305| <<vmx_set_cr0>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3559| <<vmx_set_segment>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 */
 	/* If guest state is invalid, start emulating.  L2 is handled above. */
 	if (vmx->emulation_required)
 		return handle_invalid_guest_state(vcpu);
@@ -6764,6 +6933,44 @@ static void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 
 void vmx_do_interrupt_nmi_irqoff(unsigned long entry);
 
+/*
+ * 331 SYM_FUNC_START(vmx_do_interrupt_nmi_irqoff)
+ * 332         //
+ * 333         // Unconditionally create a stack frame, getting the correct RSP on the
+ * 334         // stack (for x86-64) would take two instructions anyways, and RBP can
+ * 335         // be used to restore RSP to make objtool happy (see below).
+ * 336         //
+ * 337         push %_ASM_BP
+ * 338         mov %_ASM_SP, %_ASM_BP
+ * 339 
+ * 340 #ifdef CONFIG_X86_64
+ * 341         //
+ * 342         // Align RSP to a 16-byte boundary (to emulate CPU behavior) before
+ * 343         // creating the synthetic interrupt stack frame for the IRQ/NMI.
+ * 344         //
+ * 345         and  $-16, %rsp
+ * 346         push $__KERNEL_DS
+ * 347         push %rbp
+ * 348 #endif
+ * 349         pushf
+ * 350         push $__KERNEL_CS
+ * 351         CALL_NOSPEC _ASM_ARG1
+ * 352 
+ * 353         //
+ * 354         // "Restore" RSP from RBP, even though IRET has already unwound RSP to
+ * 355         // the correct value.  objtool doesn't know the callee will IRET and,
+ * 356         // without the explicit restore, thinks the stack is getting walloped.
+ * 357         // Using an unwind hint is problematic due to x86-64's dynamic alignment.
+ * 358         //
+ * 359         mov %_ASM_BP, %_ASM_SP
+ * 360         pop %_ASM_BP
+ * 361         RET
+ * 362 SYM_FUNC_END(vmx_do_interrupt_nmi_irqoff)
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6817| <<handle_exception_nmi_irqoff>> handle_interrupt_nmi_irqoff(&vmx->vcpu, nmi_entry);
+ *   - arch/x86/kvm/vmx/vmx.c|6830| <<handle_external_interrupt_irqoff>> handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));
+ */
 static void handle_interrupt_nmi_irqoff(struct kvm_vcpu *vcpu,
 					unsigned long entry)
 {
@@ -6794,11 +7001,20 @@ static void handle_nm_fault_irqoff(struct kvm_vcpu *vcpu)
 		rdmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6984| <<vmx_handle_exit_irqoff>> handle_exception_nmi_irqoff(vmx);
+ */
 static void handle_exception_nmi_irqoff(struct vcpu_vmx *vmx)
 {
 	const unsigned long nmi_entry = (unsigned long)asm_exc_nmi_noist;
 	u32 intr_info = vmx_get_intr_info(&vmx->vcpu);
 
+	/*
+	 * 在以下调用handle_interrupt_nmi_irqoff():
+	 *   - arch/x86/kvm/vmx/vmx.c|6817| <<handle_exception_nmi_irqoff>> handle_interrupt_nmi_irqoff(&vmx->vcpu, nmi_entry);
+	 *   - arch/x86/kvm/vmx/vmx.c|6830| <<handle_external_interrupt_irqoff>> handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));
+	 */
 	/* if exit due to PF check for async PF */
 	if (is_page_fault(intr_info))
 		vmx->vcpu.arch.apf.host_apf_flags = kvm_read_and_reset_apf_flags();
@@ -6813,6 +7029,10 @@ static void handle_exception_nmi_irqoff(struct vcpu_vmx *vmx)
 		handle_interrupt_nmi_irqoff(&vmx->vcpu, nmi_entry);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6982| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu);
+ */
 static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu)
 {
 	u32 intr_info = vmx_get_intr_info(vcpu);
@@ -6823,10 +7043,18 @@ static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu)
 	    "KVM: unexpected VM-Exit interrupt info: 0x%x", intr_info))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/vmx.c|6817| <<handle_exception_nmi_irqoff>> handle_interrupt_nmi_irqoff(&vmx->vcpu, nmi_entry);
+	 *   - arch/x86/kvm/vmx/vmx.c|6830| <<handle_external_interrupt_irqoff>> handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));
+	 */
 	handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));
 	vcpu->arch.at_instruction_boundary = true;
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.handle_exit_irqoff = vmx_handle_exit_irqoff()
+ */
 static void vmx_handle_exit_irqoff(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6864,6 +7092,10 @@ static bool vmx_has_emulated_msr(struct kvm *kvm, u32 index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7360| <<vmx_vcpu_run>> vmx_recover_nmi_blocking(vmx);
+ */
 static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 {
 	u32 exit_intr_info;
@@ -6899,11 +7131,23 @@ static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 				!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)
 				  & GUEST_INTR_STATE_NMI);
 	} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))
+		/*
+		 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+		 *   - arch/x86/kvm/vmx/vmx.c|4938| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|4976| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+		 *   - arch/x86/kvm/vmx/vmx.c|6499| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL && vcpu->arch.nmi_pending) {
+		 *   - arch/x86/kvm/vmx/vmx.c|6995| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time += ktime_to_ns(ktime_sub(ktime_get(), vmx->loaded_vmcs->entry_time));
+		 */
 		vmx->loaded_vmcs->vnmi_blocked_time +=
 			ktime_to_ns(ktime_sub(ktime_get(),
 					      vmx->loaded_vmcs->entry_time));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7056| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+ *   - arch/x86/kvm/vmx/vmx.c|7063| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu, vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+ */
 static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 				      u32 idt_vectoring_info,
 				      int instr_len_field,
@@ -6927,6 +7171,16 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 	vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
 	type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
 
+	/*
+	 * #define INTR_TYPE_EXT_INTR              (0 << 8) // external interrupt
+	 * #define INTR_TYPE_RESERVED              (1 << 8) // reserved
+	 * #define INTR_TYPE_NMI_INTR              (2 << 8) // NMI
+	 * #define INTR_TYPE_HARD_EXCEPTION        (3 << 8) // processor exception
+	 * #define INTR_TYPE_SOFT_INTR             (4 << 8) // software interrupt
+	 * #define INTR_TYPE_PRIV_SW_EXCEPTION     (5 << 8) // ICE breakpoint - undocumented
+	 * #define INTR_TYPE_SOFT_EXCEPTION        (6 << 8) // software exception
+	 * #define INTR_TYPE_OTHER_EVENT           (7 << 8) // other event
+	 */
 	switch (type) {
 	case INTR_TYPE_NMI_INTR:
 		vcpu->arch.nmi_injected = true;
@@ -6958,15 +7212,36 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7361| <<vmx_vcpu_run>> vmx_complete_interrupts(vmx);
+ */
 static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 {
+	/*
+	 * 在以下修改vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|7306| <<vmx_vcpu_run>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7318| <<vmx_vcpu_run>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/vmx/vmx.c|7056| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|7063| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu, vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+	 */
 	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 				  VM_EXIT_INSTRUCTION_LEN,
 				  IDT_VECTORING_ERROR_CODE);
 }
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.cancel_injection = vmx_cancel_injection()
+ */
 static void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/vmx.c|7056| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|7063| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu, vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+	 */
 	__vmx_complete_interrupts(vcpu,
 				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 				  VM_ENTRY_INSTRUCTION_LEN,
@@ -6998,6 +7273,10 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 					msrs[i].host, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7281| <<vmx_vcpu_run>> vmx_update_hv_timer(vcpu);
+ */
 static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7057,6 +7336,10 @@ void noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,
 	barrier_nospec();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7366| <<vmx_vcpu_run>> return vmx_exit_handlers_fastpath(vcpu);
+ */
 static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 {
 	switch (to_vmx(vcpu)->exit_reason.basic) {
@@ -7069,6 +7352,10 @@ static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7289| <<vmx_vcpu_run>> vmx_vcpu_enter_exit(vcpu, vmx, __vmx_vcpu_run_flags(vmx));
+ */
 static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 					struct vcpu_vmx *vmx,
 					unsigned long flags)
@@ -7099,16 +7386,33 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 	guest_state_exit_irqoff();
 }
 
+/*
+ * 被vcpu_enter_guest()调用
+ */
 static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	unsigned long cr3, cr4;
 
+	/*
+	 * 在以下设置loaded_vmcs->soft_vnmi_blocked:
+	 *   - arch/x86/kvm/vmx/vmx.c|4937| <<vmx_inject_nmi>> vmx->loaded_vmcs->soft_vnmi_blocked = 1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4975| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->soft_vnmi_blocked = masked;
+	 *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6510| <<__vmx_handle_exit>> vmx->loaded_vmcs->soft_vnmi_blocked = 0;
+	 */
 	/* Record the guest's net vcpu time for enforced NMI injections. */
 	if (unlikely(!enable_vnmi &&
 		     vmx->loaded_vmcs->soft_vnmi_blocked))
 		vmx->loaded_vmcs->entry_time = ktime_get();
 
+	/*
+	 * 在以下设置vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/nested.c|4399| <<load_vmcs12_host_state>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1486| <<vmx_set_rflags>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3305| <<vmx_set_cr0>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3559| <<vmx_set_segment>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 */
 	/*
 	 * Don't enter VMX if guest state is invalid, let the exit handler
 	 * start emulation until we arrive back to a valid state.  Synthesize a
@@ -7189,6 +7493,9 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 
 	kvm_wait_lapic_expire(vcpu);
 
+	/*
+	 * 在这里进入!!!!!!!!!!!
+	 */
 	/* The actual VMENTER/EXIT is in the .noinstr.text section. */
 	vmx_vcpu_enter_exit(vcpu, vmx, __vmx_vcpu_run_flags(vmx));
 
@@ -7217,6 +7524,16 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	loadsegment(es, __USER_DS);
 #endif
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_avail:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|56| <<kvm_register_is_available>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|68| <<kvm_register_mark_available>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|74| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	 *   - arch/x86/kvm/svm/svm.c|4093| <<svm_vcpu_run>> vcpu->arch.regs_avail &= ~SVM_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/nested.c|273| <<vmx_switch_vmcs>> vcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/vmx/vmx.c|7316| <<vmx_vcpu_run>> vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;
+	 *   - arch/x86/kvm/x86.c|12636| <<kvm_arch_vcpu_create>> vcpu->arch.regs_avail = ~0;
+	 */
 	vcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;
 
 	pt_guest_exit(vmx);
@@ -7235,6 +7552,11 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 		vmx->nested.nested_run_pending = 0;
 	}
 
+	/*
+	 * 在以下修改vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|7306| <<vmx_vcpu_run>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7318| <<vmx_vcpu_run>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 */
 	vmx->idt_vectoring_info = 0;
 
 	if (unlikely(vmx->fail)) {
@@ -7242,6 +7564,30 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 		return EXIT_FASTPATH_NONE;
 	}
 
+	/*
+	 * union vmx_exit_reason {
+	 *     struct {
+	 *         u32     basic                   : 16;
+	 *         u32     reserved16              : 1;
+	 *         u32     reserved17              : 1;
+	 *         u32     reserved18              : 1;
+	 *         u32     reserved19              : 1;
+	 *         u32     reserved20              : 1;
+	 *         u32     reserved21              : 1;
+	 *         u32     reserved22              : 1;
+	 *         u32     reserved23              : 1;
+	 *         u32     reserved24              : 1;
+	 *         u32     reserved25              : 1;
+	 *         u32     bus_lock_detected       : 1;
+	 *         u32     enclave_mode            : 1;
+	 *         u32     smi_pending_mtf         : 1;
+	 *         u32     smi_from_vmx_root       : 1;
+	 *         u32     reserved30              : 1;
+	 *         u32     failed_vmentry          : 1;
+	 *     };
+	 *     u32 full;
+	 * };
+	 */
 	vmx->exit_reason.full = vmcs_read32(VM_EXIT_REASON);
 	if (unlikely((u16)vmx->exit_reason.basic == EXIT_REASON_MCE_DURING_VMENTRY))
 		kvm_machine_check();
@@ -7254,6 +7600,14 @@ static fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	if (unlikely(vmx->exit_reason.failed_vmentry))
 		return EXIT_FASTPATH_NONE;
 
+	/*
+	 * 在以下使用loaded_vmcs->launched:
+	 *   - arch/x86/kvm/vmx/vmx.c|729| <<__loaded_vmcs_clear>> if (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx/vmx.c|744| <<__loaded_vmcs_clear>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|881| <<__vmx_vcpu_run_flags>> if (vmx->loaded_vmcs->launched)
+	 *   - arch/x86/kvm/vmx/vmx.c|2876| <<alloc_loaded_vmcs>> loaded_vmcs->launched = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7358| <<vmx_vcpu_run>> vmx->loaded_vmcs->launched = 1;
+	 */
 	vmx->loaded_vmcs->launched = 1;
 
 	vmx_recover_nmi_blocking(vmx);
@@ -8009,6 +8363,20 @@ static void vmx_vm_destroy(struct kvm *kvm)
 	free_pages((unsigned long)kvm_vmx->pid_table, vmx_get_pid_table_order(kvm));
 }
 
+/*
+ * 在以下使用vmx_x86_ops:
+ *   - arch/x86/kvm/vmx/vmx.c|8771| <<global>> .runtime_ops = &vmx_x86_ops,
+ *   - arch/x86/kvm/vmx/vmx.c|8637| <<hardware_setup>> vmx_x86_ops.set_apic_access_page_addr = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8640| <<hardware_setup>> vmx_x86_ops.update_cr8_intercept = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8645| <<hardware_setup>> vmx_x86_ops.tlb_remote_flush = hv_remote_flush_tlb;
+ *   - arch/x86/kvm/vmx/vmx.c|8646| <<hardware_setup>> vmx_x86_ops.tlb_remote_flush_with_range =
+ *   - arch/x86/kvm/vmx/vmx.c|8662| <<hardware_setup>> vmx_x86_ops.sync_pir_to_irr = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8698| <<hardware_setup>> vmx_x86_ops.cpu_dirty_log_size = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|8725| <<hardware_setup>> vmx_x86_ops.set_hv_timer = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8726| <<hardware_setup>> vmx_x86_ops.cancel_hv_timer = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8727| <<hardware_setup>> vmx_x86_ops.request_immediate_exit = __kvm_request_immediate_exit;
+ *   - arch/x86/kvm/vmx/vmx.c|8852| <<vmx_init>> vmx_x86_ops.enable_direct_tlbflush
+ */
 static struct kvm_x86_ops vmx_x86_ops __initdata = {
 	.name = "kvm_intel",
 
@@ -8154,6 +8522,13 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|194| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8172| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11325| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
@@ -8400,6 +8775,12 @@ static __init int hardware_setup(void)
 	return r;
 }
 
+/*
+ * 在以下使用vmx_init_ops:
+ *   - arch/x86/kvm/vmx/vmx.c|8738| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;
+ *   - arch/x86/kvm/vmx/vmx.c|8740| <<hardware_setup>> vmx_init_ops.handle_intel_pt_intr = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8860| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 static struct kvm_x86_init_ops vmx_init_ops __initdata = {
 	.cpu_has_kvm_support = cpu_has_kvm_support,
 	.disabled_by_bios = vmx_disabled_by_bios,
@@ -8411,6 +8792,10 @@ static struct kvm_x86_init_ops vmx_init_ops __initdata = {
 	.pmu_ops = &intel_pmu_ops,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8817| <<vmx_exit>> vmx_cleanup_l1d_flush();
+ */
 static void vmx_cleanup_l1d_flush(void)
 {
 	if (vmx_l1d_flush_pages) {
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 24d58c2ff..953e07cc1 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -270,10 +270,26 @@ struct vcpu_vmx {
 	 * values.  If false, host state is loaded in the CPU registers
 	 * and vmx->loaded_vmcs->host_state is invalid.
 	 */
+	/*
+	 * 在以下设置vcpu_vmx->guest_state_loaded:
+	 *   - arch/x86/kvm/vmx/vmx.c|1286| <<vmx_prepare_switch_to_guest>> vmx->guest_state_loaded = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|1324| <<vmx_prepare_switch_to_host>> vmx->guest_state_loaded = false;
+	 * 在以下使用vcpu_vmx->guest_state_loaded:
+	 *   - arch/x86/kvm/vmx/nested.c|243| <<vmx_sync_vmcs_host_state>> if (unlikely(!vmx->guest_state_loaded))
+	 *   - arch/x86/kvm/vmx/vmx.c|1248| <<vmx_prepare_switch_to_guest>> if (vmx->guest_state_loaded)
+	 *   - arch/x86/kvm/vmx/vmx.c|1293| <<vmx_prepare_switch_to_host>> if (!vmx->guest_state_loaded)
+	 *   - arch/x86/kvm/vmx/vmx.c|1332| <<vmx_read_guest_kernel_gs_base>> if (vmx->guest_state_loaded)
+	 *   - arch/x86/kvm/vmx/vmx.c|1341| <<vmx_write_guest_kernel_gs_base>> if (vmx->guest_state_loaded)
+	 */
 	bool		      guest_state_loaded;
 
 	unsigned long         exit_qualification;
 	u32                   exit_intr_info;
+	/*
+	 * 在以下修改vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|7306| <<vmx_vcpu_run>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7318| <<vmx_vcpu_run>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -325,6 +341,21 @@ struct vcpu_vmx {
 		} seg[8];
 	} segment_cache;
 	int vpid;
+	/*
+	 * 在以下设置vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/nested.c|4399| <<load_vmcs12_host_state>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1486| <<vmx_set_rflags>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3305| <<vmx_set_cr0>> vmx->emulation_required = vmx_emulation_required(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|3559| <<vmx_set_segment>> to_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);
+	 * 在以下使用vcpu_vmx->emulation_required:
+	 *   - arch/x86/kvm/vmx/vmx.c|1745| <<vmx_queue_exception>> WARN_ON_ONCE(vmx->emulation_required);
+	 *   - arch/x86/kvm/vmx/vmx.c|5794| <<vmx_emulation_required_with_pending_exception>> return vmx->emulation_required && !vmx->rmode.vm86_active &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5807| <<handle_invalid_guest_state>> while (vmx->emulation_required && count-- != 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6434| <<__vmx_handle_exit>> if (vmx->emulation_required) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6444| <<__vmx_handle_exit>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|6927| <<vmx_handle_exit_irqoff>> if (vmx->emulation_required)
+	 *   - arch/x86/kvm/vmx/vmx.c|7210| <<vmx_vcpu_run>> if (unlikely(vmx->emulation_required)) {
+	 */
 	bool emulation_required;
 
 	union vmx_exit_reason exit_reason;
@@ -581,6 +612,11 @@ int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
 void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs);
 void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4871| <<alloc_shadow_vmcs>> loaded_vmcs->shadow_vmcs = alloc_vmcs(true);
+ *   - arch/x86/kvm/vmx/vmx.c|2890| <<alloc_loaded_vmcs>> loaded_vmcs->vmcs = alloc_vmcs(false);
+ */
 static inline struct vmcs *alloc_vmcs(bool shadow)
 {
 	return alloc_vmcs_cpu(shadow, raw_smp_processor_id(),
diff --git a/arch/x86/kvm/vmx/vmx_ops.h b/arch/x86/kvm/vmx/vmx_ops.h
index 5cfc49ddb..1cf37acbd 100644
--- a/arch/x86/kvm/vmx/vmx_ops.h
+++ b/arch/x86/kvm/vmx/vmx_ops.h
@@ -269,6 +269,13 @@ static inline void vmcs_clear(struct vmcs *vmcs)
 	vmx_asm1(vmclear, "m"(phys_addr), vmcs, phys_addr);
 }
 
+/*
+ * struct vmcs {
+ *     struct vmcs_hdr hdr;
+ *     u32 abort;
+ *     char data[];
+ * };
+ */
 static inline void vmcs_load(struct vmcs *vmcs)
 {
 	u64 phys_addr = __pa(vmcs);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index b0c47b41c..8fd496619 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -112,6 +112,11 @@ static u64 __read_mostly cr4_reserved_bits = CR4_RESERVED_BITS;
 
 #define KVM_EXIT_HYPERCALL_VALID_MASK (1 << KVM_HC_MAP_GPA_RANGE)
 
+/*
+ * 在以下使用KVM_CAP_PMU_VALID_MASK:
+ *   - arch/x86/kvm/x86.c|5308| <<kvm_vm_ioctl_check_extension>> r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
+ *   - arch/x86/kvm/x86.c|7121| <<kvm_vm_ioctl_enable_cap>> if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
+ */
 #define KVM_CAP_PMU_VALID_MASK KVM_PMU_CAP_DISABLE
 
 #define KVM_X2APIC_API_VALID_FLAGS (KVM_X2APIC_API_USE_32BIT_IDS | \
@@ -179,6 +184,21 @@ module_param(force_emulation_prefix, bool, S_IRUGO);
 int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用全局的enable_pmu:
+ *   - arch/x86/kvm/x86.c|183| <<global>> bool __read_mostly enable_pmu = true;
+ *   - arch/x86/kvm/x86.c|185| <<global>> module_param(enable_pmu, bool, 0444);
+ *   - arch/x86/kvm/pmu.h|293| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|295| <<kvm_init_pmu_capability>> if (!enable_pmu) {
+ *   - arch/x86/kvm/svm/svm.c|4971| <<svm_set_cpu_caps>> if (enable_pmu && boot_cpu_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/svm/svm.c|5114| <<svm_hardware_setup>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/capabilities.h|453| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|7711| <<vmx_set_cpu_caps>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|8396| <<hardware_setup>> if (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())
+ *   - arch/x86/kvm/x86.c|5280| <<kvm_vm_ioctl_check_extension>> r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
+ *   - arch/x86/kvm/x86.c|7093| <<kvm_vm_ioctl_enable_cap>> if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
+ *   - arch/x86/kvm/x86.c|13173| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+ */
 /* Enable/disable PMU virtualization */
 bool __read_mostly enable_pmu = true;
 EXPORT_SYMBOL_GPL(enable_pmu);
@@ -715,6 +735,10 @@ int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)
 }
 EXPORT_SYMBOL_GPL(kvm_complete_insn_gp);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2001| <<complete_emulated_msr_access>> return complete_emulated_insn_gp(vcpu, vcpu->run->msr.error);
+ */
 static int complete_emulated_insn_gp(struct kvm_vcpu *vcpu, int err)
 {
 	if (err) {
@@ -742,6 +766,14 @@ void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 EXPORT_SYMBOL_GPL(kvm_inject_page_fault);
 
 /* Returns true if the page fault was immediately morphed into a VM-Exit. */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|814| <<FNAME>> kvm_inject_emulated_page_fault(vcpu, &walker.fault);
+ *   - arch/x86/kvm/vmx/sgx.c|83| <<sgx_gva_to_gpa>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/vmx/sgx.c|228| <<handle_encls_ecreate>> kvm_inject_emulated_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/x86.c|8252| <<inject_emulated_exception>> return kvm_inject_emulated_page_fault(vcpu, &ctxt->exception);
+ *   - arch/x86/kvm/x86.c|13229| <<kvm_handle_memory_failure>> kvm_inject_emulated_page_fault(vcpu, e);
+ */
 bool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 				    struct x86_exception *fault)
 {
@@ -958,6 +990,11 @@ void kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw)
 }
 EXPORT_SYMBOL_GPL(kvm_lmsw);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4012| <<svm_vcpu_run>> kvm_load_guest_xsave_state(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7272| <<vmx_vcpu_run>> kvm_load_guest_xsave_state(vcpu);
+ */
 void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.guest_state_protected)
@@ -1018,6 +1055,12 @@ static inline u64 kvm_guest_supported_xfd(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1082| <<kvm_emulate_xsetbv>> __kvm_set_xcr(vcpu, kvm_rcx_read(vcpu), kvm_read_edx_eax(vcpu))) {
+ *   - arch/x86/kvm/x86.c|6087| <<kvm_vcpu_ioctl_x86_set_xcrs>> r = __kvm_set_xcr(vcpu, XCR_XFEATURE_ENABLED_MASK,
+ *   - arch/x86/kvm/x86.c|8897| <<emulator_set_xcr>> return __kvm_set_xcr(emul_to_vcpu(ctxt), index, xcr);
+ */
 static int __kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr)
 {
 	u64 xcr0 = xcr;
@@ -1399,6 +1442,16 @@ EXPORT_SYMBOL_GPL(kvm_emulate_rdpmc);
  * may depend on host virtualization features rather than host cpu features.
  */
 
+/*
+ * 在以下使用msrs_to_save_all[]:
+ *   - arch/x86/kvm/x86.c|6897| <<kvm_init_msr_list>> for (i = 0; i < ARRAY_SIZE(msrs_to_save_all); i++) {
+ *   - arch/x86/kvm/x86.c|6898| <<kvm_init_msr_list>> if (rdmsr_safe(msrs_to_save_all[i], &dummy[0], &dummy[1]) < 0)
+ *   - arch/x86/kvm/x86.c|6905| <<kvm_init_msr_list>> switch (msrs_to_save_all[i]) {
+ *   - arch/x86/kvm/x86.c|6938| <<kvm_init_msr_list>> msrs_to_save_all[i] - MSR_IA32_RTIT_ADDR0_A >=
+ *   - arch/x86/kvm/x86.c|6943| <<kvm_init_msr_list>> if (msrs_to_save_all[i] - MSR_ARCH_PERFMON_PERFCTR0 >=
+ *   - arch/x86/kvm/x86.c|6948| <<kvm_init_msr_list>> if (msrs_to_save_all[i] - MSR_ARCH_PERFMON_EVENTSEL0 >=
+ *   - arch/x86/kvm/x86.c|6961| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static const u32 msrs_to_save_all[] = {
 	MSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,
 	MSR_STAR,
@@ -1449,7 +1502,20 @@ static const u32 msrs_to_save_all[] = {
 	MSR_IA32_XFD, MSR_IA32_XFD_ERR,
 };
 
+/*
+ * 在以下使用msrs_to_save[ARRAY_SIZE(msrs_to_save_all)]:
+ *   - arch/x86/kvm/x86.c|4582| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6975| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];
+/*
+ * 在以下使用num_msrs_to_save:
+ *   - arch/x86/kvm/x86.c|4580| <<kvm_arch_dev_ioctl>> msr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;
+ *   - arch/x86/kvm/x86.c|4588| <<kvm_arch_dev_ioctl>> num_msrs_to_save * sizeof(u32)))
+ *   - arch/x86/kvm/x86.c|4590| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices + num_msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6912| <<kvm_init_msr_list>> num_msrs_to_save = 0;
+ *   - arch/x86/kvm/x86.c|6980| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static unsigned num_msrs_to_save;
 
 static const u32 emulated_msrs_all[] = {
@@ -1524,6 +1590,13 @@ static unsigned num_emulated_msrs;
  * List of msr numbers which are used to expose MSR-based features that
  * can be used by a hypervisor to validate requested CPU features.
  */
+/*
+ * 在以下使用msr_based_features_all[]:
+ *   - arch/x86/kvm/x86.c|1568| <<global>> static u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];
+ *   - arch/x86/kvm/x86.c|6990| <<kvm_init_msr_list>> for (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {
+ *   - arch/x86/kvm/x86.c|6993| <<kvm_init_msr_list>> msr.index = msr_based_features_all[i];
+ *   - arch/x86/kvm/x86.c|6997| <<kvm_init_msr_list>> msr_based_features[num_msr_based_features++] = msr_based_features_all[i];
+ */
 static const u32 msr_based_features_all[] = {
 	MSR_IA32_VMX_BASIC,
 	MSR_IA32_VMX_TRUE_PINBASED_CTLS,
@@ -1733,6 +1806,15 @@ void kvm_enable_efer_bits(u64 mask)
 }
 EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|762| <<set_msr_interception_bitmap>> if (read && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/svm/svm.c|765| <<set_msr_interception_bitmap>> if (write && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE))
+ *   - arch/x86/kvm/vmx/vmx.c|3922| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {
+ *   - arch/x86/kvm/vmx/vmx.c|3928| <<vmx_disable_intercept_for_msr>> !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {
+ *   - arch/x86/kvm/x86.c|1939| <<kvm_get_msr_with_filter>> if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))
+ *   - arch/x86/kvm/x86.c|1946| <<kvm_set_msr_with_filter>> if (!kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_WRITE))
+ */
 bool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)
 {
 	struct kvm_x86_msr_filter *msr_filter;
@@ -1782,6 +1864,11 @@ EXPORT_SYMBOL_GPL(kvm_msr_allowed);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1893| <<kvm_set_msr_ignored_check>> int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
+ *   - arch/x86/kvm/x86.c|11848| <<kvm_vcpu_reset>> __kvm_set_msr(vcpu, MSR_IA32_XSS, 0, true);
+ */
 static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 			 bool host_initiated)
 {
@@ -1844,6 +1931,12 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	return static_call(kvm_x86_set_msr)(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1957| <<kvm_set_msr_with_filter>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|1968| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2207| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
@@ -2086,6 +2179,11 @@ int kvm_emulate_monitor(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_monitor);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11284| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+ *   - arch/x86/kvm/x86.c|11338| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+ */
 static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
 {
 	xfer_to_guest_mode_prepare();
@@ -2168,13 +2266,51 @@ static int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 	return kvm_set_msr_ignored_check(vcpu, index, *data, true);
 }
 
+/*
+ * 例子:
+ *
+ * old:
+ *
+ * pvclock_clock.vclock_mode = 1
+ * pvclock_clock.cycle_last  = 976271374516564
+ * pvclock_clock.mask        = 18446744073709551615
+ * pvclock_clock.mult        = 4945495
+ * pvclock_clock.shift       = 24
+ * pvclock_clock.base_cycles = 11726012845710097
+ * pvclock_clock.offset      = 734000000000
+ *
+ * new:
+ *
+ * pvclock_clock.vclock_mode = 1
+ * pvclock_clock.cycle_last  = 1093684034224684
+ * pvclock_clock.mask        = 18446744073709551615
+ * pvclock_clock.mult        = 4945495
+ * pvclock_clock.shift       = 24
+ * pvclock_clock.base_cycles = 16001776054629497
+ * pvclock_clock.offset      = 35344000000000
+ */
 #ifdef CONFIG_X86_64
 struct pvclock_clock {
 	int vclock_mode;
+	/*
+	 * 在以下使用pvclock_clock->cycle_last:
+	 *   - arch/x86/kvm/x86.c|2260| <<update_pvclock_gtod>> vdata->clock.cycle_last = tk->tkr_mono.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2268| <<update_pvclock_gtod>> vdata->raw_clock.cycle_last = tk->tkr_raw.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2926| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2956| <<vgettsc>> v = (tsc_pg_val - clock->cycle_last) &
+	 *   - arch/x86/kvm/x86.c|2966| <<vgettsc>> v = (*tsc_timestamp - clock->cycle_last) &
+	 */
 	u64 cycle_last;
 	u64 mask;
 	u32 mult;
 	u32 shift;
+	/*
+	 * 在以下使用pvclock_clock->base_cycles:
+	 *   - arch/x86/kvm/x86.c|2305| <<update_pvclock_gtod>> vdata->clock.base_cycles = tk->tkr_mono.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|2313| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|3060| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+	 *   - arch/x86/kvm/x86.c|3080| <<do_realtime>> ns = gtod->clock.base_cycles;
+	 */
 	u64 base_cycles;
 	u64 offset;
 };
@@ -2183,20 +2319,99 @@ struct pvclock_gtod_data {
 	seqcount_t	seq;
 
 	struct pvclock_clock clock; /* extract of a clocksource struct */
+	/*
+	 * 在以下使用pvclock_clock->raw_clock:
+	 *   - arch/x86/kvm/x86.c|2308| <<update_pvclock_gtod>> vdata->raw_clock.vclock_mode = tk->tkr_raw.clock->vdso_clock_mode;
+	 *   - arch/x86/kvm/x86.c|2309| <<update_pvclock_gtod>> vdata->raw_clock.cycle_last = tk->tkr_raw.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2310| <<update_pvclock_gtod>> vdata->raw_clock.mask = tk->tkr_raw.mask;
+	 *   - arch/x86/kvm/x86.c|2311| <<update_pvclock_gtod>> vdata->raw_clock.mult = tk->tkr_raw.mult;
+	 *   - arch/x86/kvm/x86.c|2312| <<update_pvclock_gtod>> vdata->raw_clock.shift = tk->tkr_raw.shift;
+	 *   - arch/x86/kvm/x86.c|2313| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|2314| <<update_pvclock_gtod>> vdata->raw_clock.offset = tk->tkr_raw.base;
+	 *   - arch/x86/kvm/x86.c|3060| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+	 *   - arch/x86/kvm/x86.c|3061| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+	 *   - arch/x86/kvm/x86.c|3062| <<do_monotonic_raw>> ns >>= gtod->raw_clock.shift;
+	 *   - arch/x86/kvm/x86.c|3063| <<do_monotonic_raw>> ns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));
+	 */
 	struct pvclock_clock raw_clock; /* extract of a clocksource struct */
 
+	/*
+	 * 在以下使用pvclock_gtod_data->offs_boot:
+	 *   - arch/x86/kvm/x86.c|2363| <<update_pvclock_gtod>> vdata->offs_boot = tk->offs_boot;
+	 *   - arch/x86/kvm/x86.c|2371| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+	 *   - arch/x86/kvm/x86.c|3130| <<do_monotonic_raw>> ns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));
+	 */
 	ktime_t		offs_boot;
 	u64		wall_time_sec;
 };
 
+/*
+ * 在以下使用pvclock_gtod_data:
+ *   - arch/x86/kvm/x86.c|2287| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2347| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - arch/x86/kvm/x86.c|2593| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2774| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+ *   - arch/x86/kvm/x86.c|3015| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+ *   - arch/x86/kvm/x86.c|3093| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|3116| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|3143| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|3160| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|3248| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+ *   - arch/x86/kvm/x86.c|9642| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ */
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9459| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
 
 	write_seqcount_begin(&vdata->seq);
 
+	/*
+	 * 很可能用的tk_core.timekeeper
+	 *
+	 * 50 static struct {
+	 * 51         seqcount_raw_spinlock_t seq;
+	 * 52         struct timekeeper       timekeeper;
+	 * 53 } tk_core ____cacheline_aligned = {
+	 * 54         .seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),
+	 * 55 };
+	 *
+	 * @tkr_mono:           The readout base structure for CLOCK_MONOTONIC
+	 * @tkr_raw:            The readout base structure for CLOCK_MONOTONIC_RAW
+	 * @xtime_sec:          Current CLOCK_REALTIME time in seconds
+	 * @ktime_sec:          Current CLOCK_MONOTONIC time in seconds
+	 * @wall_to_monotonic:  CLOCK_REALTIME to CLOCK_MONOTONIC offset
+	 * @offs_real:          Offset clock monotonic -> clock realtime
+	 * @offs_boot:          Offset clock monotonic -> clock boottime
+	 *
+	 * -> CLOCK_REALTIME clock gives the time passed since January 1, 1970.
+	 * This clock is affected by NTP adjustments and can jump forward and
+	 * backward when a system administrator adjusts system time.
+	 * -> CLOCK_MONOTONIC clock gives the time since a fixed starting
+	 * point-usually since you booted the system. This clock is affected
+	 * by NTP, but it can't jump backward.
+	 * -> CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC,
+	 * but this clock is not affected by NTP adjustments.
+	 * -> CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but
+	 * less-accurate variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+	 *
+	 * struct tk_read_base - base structure for timekeeping readout
+	 * @clock:      Current clocksource used for timekeeping.
+	 * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+	 * @cycle_last: @clock cycle value at last update
+	 * @mult:       (NTP adjusted) multiplier for scaled math conversion
+	 * @shift:      Shift value for scaled math conversion
+	 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+	 * @base:       ktime_t (nanoseconds) base time for readout
+	 * @base_real:  Nanoseconds base value for clock REALTIME readout
+	 *
+	 * 除了base_real什么都拷贝了
+	 */
 	/* copy pvclock gtod data */
 	vdata->clock.vclock_mode	= tk->tkr_mono.clock->vdso_clock_mode;
 	vdata->clock.cycle_last		= tk->tkr_mono.cycle_last;
@@ -2214,8 +2429,24 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	vdata->raw_clock.base_cycles	= tk->tkr_raw.xtime_nsec;
 	vdata->raw_clock.offset		= tk->tkr_raw.base;
 
+	/*
+	 * @xtime_sec:          Current CLOCK_REALTIME time in seconds
+	 */
 	vdata->wall_time_sec            = tk->xtime_sec;
 
+	/*
+	 * 在以下使用timekeeper->offs_boot:
+	 *   - kernel/time/timekeeping.c|1124| <<global>> [TK_OFFS_BOOT] = &tk_core.timekeeper.offs_boot,
+	 *   - arch/x86/kvm/x86.c|2377| <<update_pvclock_gtod>> vdata->offs_boot = tk->offs_boot;
+	 *   - kernel/time/timekeeping.c|295| <<tk_update_sleep_time>> tk->offs_boot = ktime_add(tk->offs_boot, delta);
+	 *   - kernel/time/timekeeping.c|300| <<tk_update_sleep_time>> tk->monotonic_to_boot = ktime_to_timespec64(tk->offs_boot);
+	 *   - kernel/time/timekeeping.c|737| <<ktime_get_boot_fast_ns>> return (ktime_get_mono_fast_ns() + ktime_to_ns(data_race(tk->offs_boot)));
+	 *   - kernel/time/timekeeping.c|839| <<ktime_get_fast_timestamps>> snapshot->boot = snapshot->mono + ktime_to_ns(data_race(tk->offs_boot));
+	 *   - kernel/time/timekeeping.c|2580| <<getboottime64>> ktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);
+	 *   - kernel/time/timekeeping.c|2657| <<ktime_get_update_offsets_now>> *offs_boot = tk->offs_boot;
+	 *
+	 * @offs_boot:          Offset clock monotonic -> clock boottime
+	 */
 	vdata->offs_boot		= tk->offs_boot;
 
 	write_seqcount_end(&vdata->seq);
@@ -2234,6 +2465,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4061| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK_NEW)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|4068| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2280,6 +2516,13 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4004| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|4010| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ *
+ * 这里是登记shared memory的地址
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
@@ -2292,9 +2535,33 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 		ka->boot_vcpu_runs_old_kvmclock = old_msr;
 	}
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> gpa_t time;
+	 *    -> struct gfn_to_pfn_cache pv_time;
+	 */
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2505| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5364| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11044| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * 处理函数是kvm_gen_kvmclock_update(vcpu)
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2399| <<kvm_write_system_time>> kvm_gfn_to_pfn_cache_init(vcpu->kvm, &vcpu->arch.pv_time, vcpu,
+	 *   - arch/x86/kvm/x86.c|2403| <<kvm_write_system_time>> kvm_gfn_to_pfn_cache_destroy(vcpu->kvm, &vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3457| <<kvm_guest_time_update>> if (vcpu->pv_time.active)
+	 *   - arch/x86/kvm/x86.c|3458| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|3683| <<kvmclock_reset>> kvm_gfn_to_pfn_cache_destroy(vcpu->kvm, &vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|5642| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|6807| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time.active)
+	 */
 	/* we verify if the enable bit is set... */
 	if (system_time & 1) {
 		kvm_gfn_to_pfn_cache_init(vcpu->kvm, &vcpu->arch.pv_time, vcpu,
@@ -2307,12 +2574,35 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2527| <<kvm_get_time_scale>> *pmultiplier = div_frac(scaled64, tps32);
+ */
 static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 {
+	/*
+	 * Same "calling convention" as do_div:
+	 * - divide (n << 32) by base
+	 * - put result in n
+	 * - return remainder
+	 */
 	do_shl32_div32(dividend, divisor);
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2572| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3382| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3559| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ *
+ * 从base_hz变成scaled_hz
+ *
+ * 对于kvm-clock, 从3392422000变成1000000000
+ *
+ * scaled是1000000000
+ * base是3392422000
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2321,6 +2611,9 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 	uint64_t tps64;
 	uint32_t tps32;
 
+	/*
+	 * 从base_hz变成scaled_hz
+	 */
 	tps64 = base_hz;
 	scaled64 = scaled_hz;
 	while (tps64 > scaled64*2 || tps64 & 0xffffffff00000000ULL) {
@@ -2330,18 +2623,58 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 
 	tps32 = (uint32_t)tps64;
 	while (tps32 <= scaled64 || scaled64 & 0xffffffff00000000ULL) {
+		/*
+		 * 下面scaled64要除以tps32
+		 * 所以"scaled64 >>= 1"等价于"tps32 <<= 1"
+		 */
 		if (scaled64 & 0xffffffff00000000ULL || tps32 & 0x80000000)
 			scaled64 >>= 1;
 		else
 			tps32 <<= 1;
+		/*
+		 * 如果最终的shift是正数, 说明这里又加回来了
+		 * 就是from << shift或者to >> shift
+		 * 也就是:
+		 * from << shift --> tps32 <<= 1
+		 * to >> shift   --> scaled64 >>= 1
+		 *
+		 * 根据pvclock_scale_delta(), 负数是正常的向右, 正数是向左
+		 * if (shift < 0)
+		 *     delta >>= -shift;
+		 * else
+		 *     delta <<= shift;
+		 */
 		shift++;
 	}
 
+	/*
+	 * 比如
+	 * tps64 =    1696211000
+	 * scaled64 = 1000000000
+	 *
+	 * 这里的除法先往左移动32位再除
+	 * Same "calling convention" as do_div:
+	 * - divide (n << 32) by base
+	 * - put result in n
+	 * - return remainder
+	 */
 	*pshift = shift;
 	*pmultiplier = div_frac(scaled64, tps32);
+	/*
+	 * 最后:
+	 * shift = -1
+	 * multi = 2532094943
+	 */
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2403| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+ *   - arch/x86/kvm/x86.c|3100| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|9386| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|9421| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
@@ -2357,6 +2690,10 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2746| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
@@ -2393,6 +2730,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6629| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|12794| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2405,10 +2747,20 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 		return -1;
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2730| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC, &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2753| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|385| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	/* Compute a scale to convert nanoseconds in TSC cycles */
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2470| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2426,6 +2778,10 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3873| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
@@ -2442,6 +2798,10 @@ static inline int gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2773| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2460,6 +2820,23 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3077| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1357| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2348| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2527| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9354| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10519| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12179| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|106| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 *
+	 * 处理kvm_update_masterclock()
+	 *
+	 * 但是!!! 只有online的时候(INIT/SIPI之后)才会执行这个request去更新master clock
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2496,6 +2873,13 @@ u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2790| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2829| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3799| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4868| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2505,6 +2889,21 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|563| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1680| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1691| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1776| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1860| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|1884| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|986| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2092| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|7849| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|3163| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|9462| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|10661| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
 	return vcpu->arch.l1_tsc_offset +
@@ -2537,6 +2936,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2675| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2765| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4785| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
@@ -2561,6 +2966,12 @@ static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 	static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2686| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2712| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+ *   - arch/x86/kvm/x86.c|2724| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ */
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)
 {
 	vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
@@ -2596,6 +3007,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2757| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ *   - arch/x86/kvm/x86.c|5469| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched)
 {
@@ -2643,6 +3059,76 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	kvm_track_tsc_matching(vcpu);
 }
 
+/*
+ * [0] kvm_synchronize_tsc
+ * [0] kvm_arch_vcpu_postcreate
+ * [0] kvm_vm_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] kvm_synchronize_tsc
+ * [0] kvm_set_msr_common
+ * [0] vmx_set_msr
+ * [0] __kvm_set_msr
+ * [0] do_set_msr
+ * [0] msr_io
+ * [0] kvm_arch_vcpu_ioctl
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 在4.14上, 2个vcpu调用6次, 4个vcpu调用12次
+ * /usr/share/bcc/tools/trace -t -C 'kvm_write_tsc'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 3.001404 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.002039 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.002591 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.003144 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.018026 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.018118 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.018204 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.018262 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.031890 1   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.031936 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.031979 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.032021 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ *
+ * 根据分析, 应该是QEMU来的 (不是VM kernel写的MSR TSC)
+ *
+ *
+ * KVM希望各个vCPU的TSC处在同步状态,称之为Master Clock模式,每当vTSC被修改,就有两种可能:
+ *
+ * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+ * 此时TSC进入下一代,体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+ * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offset,
+ * 其中offset即上述L1 TSC Offset.重新同步后，可以以此为基点导出任意vCPU的TSC值.
+ *
+ * vCPU在尝试重新同步TSC，此时不能将L1 offset设置为offset
+ * 此时会设置kvm->arch.nr_vcpus_matched_tsc++,一旦所有vCPU都处于matched状态,就可以重新回到Master Clock模式
+ * 在满足vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz的前提下(vTSCfreq相同是TSC能同步的前提),以下情形视为尝试同步TSC:
+ *
+ * Host Initiated且写入值为0,视为正在初始化
+ * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+ * 按照Host TSC是否同步(即是否stable),会为L1 TSC offset设置不同的值
+ *
+ * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+ * 对于Unstable Host,则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+ *
+ * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差值
+ *
+ * 4.14测试kvm_write_tsc的时候data=0
+ *
+ * 这个kvm_synchronize_tsc()函数最大的目的:
+ * - 计算kvm->arch.nr_vcpus_matched_tsc, 稍后好同步master clock
+ * - 使用kvm_vcpu_write_tsc_offset(vcpu, offset)来计算vcpu->arch.l1_tsc_offset和vcpu->arch.tsc_offset
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3712| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|11779| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2654,8 +3140,28 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 一开始是0, 所以elapsed应该很大
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2470| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 在以下尝试同步TSC:
+	 *   - Host Initiated且写入值为0,视为正在初始化
+	 *   - 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+	 *
+	 * vcpu->arch.virtual_tsc_khz为vCPU的vTSCfreq
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	if (vcpu->arch.virtual_tsc_khz) {
 		if (data == 0) {
 			/*
@@ -2673,6 +3179,9 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 * of virtual cycle time against real time is
 			 * interpreted as an attempt to synchronize the CPU.
 			 */
+			/*
+			 * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+			 */
 			synchronizing = data < tsc_exp + tsc_hz &&
 					data + tsc_hz > tsc_exp;
 		}
@@ -2684,9 +3193,33 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	 * compensation code attempt to catch up if we fall behind, but
 	 * it's better to try to match offsets from the beginning.
          */
+	/*
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 *
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2470| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2857| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|3018| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|5889| <<kvm_arch_tsc_set_attr>> kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *
+	 * !!!! 第一次进来elapsed非常大, 但是不满足"vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz"
+	 */
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2877| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *   - arch/x86/kvm/x86.c|3020| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
@@ -2696,6 +3229,9 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		matched = true;
 	}
 
+	/*
+	 * data是0, ofset是一样的, ns是kvm base
+	 */
 	__kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
@@ -2718,9 +3254,21 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3011| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
+	/*
+	 * 在以下使用pvclock_clock->cycle_last:
+	 *   - arch/x86/kvm/x86.c|2260| <<update_pvclock_gtod>> vdata->clock.cycle_last = tk->tkr_mono.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2268| <<update_pvclock_gtod>> vdata->raw_clock.cycle_last = tk->tkr_raw.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2926| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2956| <<vgettsc>> v = (tsc_pg_val - clock->cycle_last) &
+	 *   - arch/x86/kvm/x86.c|2966| <<vgettsc>> v = (*tsc_timestamp - clock->cycle_last) &
+	 */
 	u64 last = pvclock_gtod_data.clock.cycle_last;
 
 	if (likely(ret >= last))
@@ -2738,6 +3286,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2993| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|3013| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2774,8 +3327,61 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * 例子:
+ *
+ * old:
+ *
+ * pvclock_clock.vclock_mode = 1
+ * pvclock_clock.cycle_last  = 976271374516564
+ * pvclock_clock.mask        = 18446744073709551615
+ * pvclock_clock.mult        = 4945495
+ * pvclock_clock.shift       = 24
+ * pvclock_clock.base_cycles = 11726012845710097
+ * pvclock_clock.offset      = 734000000000
+ *
+ * new:
+ *
+ * pvclock_clock.vclock_mode = 1
+ * pvclock_clock.cycle_last  = 1093684034224684
+ * pvclock_clock.mask        = 18446744073709551615
+ * pvclock_clock.mult        = 4945495
+ * pvclock_clock.shift       = 24
+ * pvclock_clock.base_cycles = 16001776054629497
+ * pvclock_clock.offset      = 35344000000000
+ *
+ * 一些例子.
+ * ktime_get_raw_ts64()
+ * ktime_get_snapshot()
+ *
+ * do {
+ *	seq = read_seqcount_begin(&tk_core.seq);
+ *	now = tk_clock_read(&tk->tkr_mono);
+ *	nsec_raw  = timekeeping_cycles_to_ns(&tk->tkr_raw, now);
+ *	base_raw = tk->tkr_raw.base;
+ *	systime_snapshot->raw = ktime_add_ns(base_raw, nsec_raw);
+ * } while (read_seqcount_retry(&tk_core.seq, seq));
+ */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3026| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
+	/*
+	 * struct pvclock_gtod_data *gtod:
+	 * -> struct pvclock_clock clock;
+	 * -> struct pvclock_clock raw_clock;
+	 *    -> int vclock_mode;
+	 *    -> u64 cycle_last;
+	 *    -> u64 mask;
+	 *    -> u32 mult;
+	 *    -> u32 shift; 
+	 *    -> u64 base_cycles;
+	 *    -> u64 offset;
+	 * -> ktime_t offs_boot;
+	 * -> u64 wall_time_sec;
+	 */
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	unsigned long seq;
 	int mode;
@@ -2783,9 +3389,46 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 
 	do {
 		seq = read_seqcount_begin(&gtod->seq);
+		/*
+		 * struct tk_read_base - base structure for timekeeping readout
+		 * @clock:      Current clocksource used for timekeeping.
+		 * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+		 * @cycle_last: @clock cycle value at last update
+		 * @mult:       (NTP adjusted) multiplier for scaled math conversion
+		 * @shift:      Shift value for scaled math conversion
+		 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+		 * @base:       ktime_t (nanoseconds) base time for readout
+		 * @base_real:  Nanoseconds base value for clock REALTIME readout
+		 *
+		 * vdata->raw_clock.vclock_mode    = tk->tkr_raw.clock->vdso_clock_mode;
+		 * vdata->raw_clock.cycle_last     = tk->tkr_raw.cycle_last;
+		 * vdata->raw_clock.mask           = tk->tkr_raw.mask;
+		 * vdata->raw_clock.mult           = tk->tkr_raw.mult;
+		 * vdata->raw_clock.shift          = tk->tkr_raw.shift;
+		 * vdata->raw_clock.base_cycles    = tk->tkr_raw.xtime_nsec;
+		 * vdata->raw_clock.offset         = tk->tkr_raw.base;
+		 *
+		 * 在以下使用pvclock_clock->base_cycles:
+		 *   - arch/x86/kvm/x86.c|2305| <<update_pvclock_gtod>> vdata->clock.base_cycles = tk->tkr_mono.xtime_nsec;
+		 *   - arch/x86/kvm/x86.c|2313| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+		 *   - arch/x86/kvm/x86.c|3060| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+		 *   - arch/x86/kvm/x86.c|3080| <<do_realtime>> ns = gtod->clock.base_cycles;
+		 *
+		 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+		 * 应该是往左shifted过的
+		 */
 		ns = gtod->raw_clock.base_cycles;
+		/*
+		 * 这里计算tsc_timestamp
+		 */
 		ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
 		ns >>= gtod->raw_clock.shift;
+		/*
+		 * 在4.14的正常机器上, offs_boot一直是0
+		 * @offs_boot:          Offset clock monotonic -> clock boottime
+		 *
+		 * @base:       ktime_t (nanoseconds) base time for readout
+		 */
 		ns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));
 	} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));
 	*t = ns;
@@ -2793,6 +3436,10 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3135| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2815,6 +3462,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3105| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
@@ -2826,6 +3477,11 @@ static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3303| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+ *   - arch/x86/kvm/x86.c|9799| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+ */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
 {
@@ -2878,6 +3534,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3164| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6773| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9237| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12394| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2897,10 +3560,21 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3077| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2403| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3100| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9386| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9421| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2910,17 +3584,63 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3103| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9194| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 static void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+	 *   - arch/x86/kvm/x86.c|3092| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *   - arch/x86/kvm/x86.c|3122| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+	 *
+	 * 应该是在vcpu_enter_guest()的下面的代码判断的.
+	 * 11312         if (kvm_vcpu_exit_request(vcpu)) {
+	 * 11313                 vcpu->mode = OUTSIDE_GUEST_MODE;
+	 * 11314                 smp_wmb();
+	 * 11315                 local_irq_enable();
+	 * 11316                 preempt_enable();
+	 * 11317                 kvm_vcpu_srcu_read_lock(vcpu);
+	 * 11318                 r = 1;
+	 * 11319                 goto cancel_injection;
+	 * 11320         }
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3498| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9709| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+ */
 static void __kvm_start_pvclock_update(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2923| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3024| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3120| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3417| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3465| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3495| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|6069| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|6079| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12922| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|12924| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|12927| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|12929| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
 	write_seqcount_begin(&kvm->arch.pvclock_sc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3511| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7207| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+ */
 static void kvm_start_pvclock_update(struct kvm *kvm)
 {
 	kvm_make_mclock_inprogress_request(kvm);
@@ -2929,6 +3649,12 @@ static void kvm_start_pvclock_update(struct kvm *kvm)
 	__kvm_start_pvclock_update(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3513| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7232| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9673| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+ */
 static void kvm_end_pvclock_update(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2937,6 +3663,9 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 
 	write_seqcount_end(&ka->pvclock_sc);
 	raw_spin_unlock_irq(&ka->tsc_write_lock);
+	/*
+	 * 调用kvm_guest_time_update()
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -2945,6 +3674,18 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10389| <<vcpu_enter_guest(KVM_REQ_MASTERCLOCK_UPDATE)>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_request_tsc_page_update(kvm);
@@ -2954,6 +3695,10 @@ static void kvm_update_masterclock(struct kvm *kvm)
 }
 
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3331| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2962,6 +3707,16 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
 	get_cpu();
 
+	/*
+	 * struct kvm_clock_data {
+	 *     __u64 clock;
+	 *     __u32 flags;
+	 *     __u32 pad0;
+	 *     __u64 realtime;
+	 *     __u64 host_tsc;
+	 *     __u32 pad[4];
+	 * };
+	 */
 	data->flags = 0;
 	if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
 #ifdef CONFIG_X86_64
@@ -2977,6 +3732,10 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 		data->flags |= KVM_CLOCK_TSC_STABLE;
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		/*
+		 * 从base_hz变成scaled_hz
+		 * 从__this_cpu_read(cpu_tsc_khz) * 1000LL变成NSEC_PER_SEC
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
@@ -2988,6 +3747,11 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3339| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6878| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2999,6 +3763,18 @@ static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|560| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2385| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|176| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|636| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|677| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|727| <<kvm_xen_vcpu_set_attr>> get_kvmclock_ns(vcpu->kvm));
+ *   - arch/x86/kvm/xen.c|1158| <<kvm_xen_hcall_vcpu_op>> delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|1188| <<kvm_xen_hcall_set_timer_op>> uint64_t guest_now = get_kvmclock_ns(vcpu->kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
@@ -3007,6 +3783,12 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return data.clock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3458| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3460| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|3463| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 				    struct gfn_to_pfn_cache *gpc,
 				    unsigned int offset)
@@ -3058,6 +3840,10 @@ static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10397| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3126,13 +3912,37 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,
 					    v->arch.l1_tsc_scaling_ratio);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3601| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3605| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 *
+	 * 这里似乎只执行第一次
+	 */
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|2572| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+		 *   - arch/x86/kvm/x86.c|3382| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+		 *   - arch/x86/kvm/x86.c|3559| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+		 *
+		 * 比如3392422 * 1000 = 3392422000
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
 				   &vcpu->hv_clock.tsc_to_system_mul);
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|498| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3381| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3387| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3566| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7015| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|12668| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
 	vcpu->last_guest_tsc = tsc_timestamp;
@@ -3142,8 +3952,24 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	if (use_master_clock)
 		pvclock_flags |= PVCLOCK_TSC_STABLE_BIT;
 
+	/*
+	 * struct kvm_vcpu_arch:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.flags = pvclock_flags;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2399| <<kvm_write_system_time>> kvm_gfn_to_pfn_cache_init(vcpu->kvm, &vcpu->arch.pv_time, vcpu,
+	 *   - arch/x86/kvm/x86.c|2403| <<kvm_write_system_time>> kvm_gfn_to_pfn_cache_destroy(vcpu->kvm, &vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3457| <<kvm_guest_time_update>> if (vcpu->pv_time.active)
+	 *   - arch/x86/kvm/x86.c|3458| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|3683| <<kvmclock_reset>> kvm_gfn_to_pfn_cache_destroy(vcpu->kvm, &vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|5642| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|6807| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time.active)
+	 *
+	 * 更新共享内存
+	 */
 	if (vcpu->pv_time.active)
 		kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
 	if (vcpu->xen.vcpu_info_cache.active)
@@ -3186,11 +4012,30 @@ static void kvmclock_update_fn(struct work_struct *work)
 	}
 }
 
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2505| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5364| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11044| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理函数是kvm_gen_kvmclock_update(vcpu)
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 处理kvm_guest_time_update()
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3808| <<kvmclock_update_fn>> struct kvm_arch *ka = container_of(dwork, struct kvm_arch, kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3823| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3839| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12902| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12943| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
@@ -3519,6 +4364,16 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 	mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2720| <<efer_trap>> ret = kvm_set_msr_common(vcpu, &msr_info);
+ *   - arch/x86/kvm/svm/svm.c|3081| <<svm_set_msr>> return kvm_set_msr_common(vcpu, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2042| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2057| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2218| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2346| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2355| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ */
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
@@ -3550,6 +4405,9 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_IA32_PERF_CAPABILITIES: {
 		struct kvm_msr_entry msr_ent = {.index = msr, .data = 0};
 
+		/*
+		 * 只有host可以修改这个值
+		 */
 		if (!msr_info->host_initiated)
 			return 1;
 		if (kvm_get_msr_feature(&msr_ent))
@@ -3570,6 +4428,12 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 
 		/* Handle McStatusWrEn */
 		if (data == BIT_ULL(18)) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->msr_hwcr:
+			 *   - arch/x86/kvm/x86.c|3282| <<can_set_mci_status>> return !!(vcpu->arch.msr_hwcr & BIT_ULL(18));
+			 *   - arch/x86/kvm/x86.c|3623| <<kvm_set_msr_common>> vcpu->arch.msr_hwcr = data;
+			 *   - arch/x86/kvm/x86.c|4249| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_hwcr;
+			 */
 			vcpu->arch.msr_hwcr = data;
 		} else if (data != 0) {
 			vcpu_unimpl(vcpu, "unimplemented HWCR wrmsr: 0x%llx\n",
@@ -4316,6 +5180,10 @@ static int kvm_ioctl_get_supported_hv_cpuid(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4489| <<kvm_vm_ioctl_check_extension_generic>> return kvm_vm_ioctl_check_extension(kvm, arg);
+ */
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r = 0;
@@ -4489,6 +5357,10 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	}
 	case KVM_CAP_PMU_CAPABILITY:
+		/*
+		 * 没见到QEMU用KVM_PMU_CAP_DISABLE
+		 * 也没见到QEMU用KVM_CAP_PMU_CAPABILITY
+		 */
 		r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
 		break;
 	case KVM_CAP_DISABLE_QUIRKS2:
@@ -4554,6 +5426,12 @@ long kvm_arch_dev_ioctl(struct file *filp,
 
 	switch (ioctl) {
 	case KVM_GET_MSR_INDEX_LIST: {
+		/*
+		 * struct kvm_msr_list {
+		 *     __u32 nmsrs; // number of msrs in entries
+		 *     __u32 indices[];
+		 * };
+		 */
 		struct kvm_msr_list __user *user_msr_list = argp;
 		struct kvm_msr_list msr_list;
 		unsigned n;
@@ -5097,6 +5975,11 @@ static void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,
 
 static void kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5736| <<kvm_arch_vcpu_ioctl(KVM_SET_VCPU_EVENTS)>> r = kvm_vcpu_ioctl_x86_set_vcpu_events(vcpu, &events);
+ *   - arch/x86/kvm/x86.c|11625| <<sync_regs>> if (kvm_vcpu_ioctl_x86_set_vcpu_events(
+ */
 static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 					      struct kvm_vcpu_events *events)
 {
@@ -5225,9 +6108,26 @@ static int kvm_vcpu_ioctl_x86_set_debugregs(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6323| <<kvm_arch_vcpu_ioctl(KVM_GET_XSAVE)>> kvm_vcpu_ioctl_x86_get_xsave(vcpu, u.xsave);
+ */
 static void kvm_vcpu_ioctl_x86_get_xsave(struct kvm_vcpu *vcpu,
 					 struct kvm_xsave *guest_xsave)
 {
+	/*
+	 * @is_confidential:    Indicator for KVM confidential mode.
+	 *                      The FPU registers are restored by the
+	 *                      vmentry firmware from encrypted guest
+	 *                      memory. On vmexit the FPU registers are
+	 *                      saved by firmware to encrypted guest memory
+	 *                      and the registers are scrubbed before
+	 *                      returning to the host. So there is no
+	 *                      content which is worth saving and restoring.
+	 *                      The fpstate has to be there so that
+	 *                      preemption and softirq FPU usage works
+	 *                      without special casing.
+	 */
 	if (fpstate_is_confidential(&vcpu->arch.guest_fpu))
 		return;
 
@@ -5237,6 +6137,10 @@ static void kvm_vcpu_ioctl_x86_get_xsave(struct kvm_vcpu *vcpu,
 				       vcpu->arch.pkru);
 }
 
+/*
+ * 处理KVM_GET_XSAVE2:
+ *   - arch/x86/kvm/x86.c|6512| <<kvm_arch_vcpu_ioctl(KVM_GET_XSAVE2)>> kvm_vcpu_ioctl_x86_get_xsave2(vcpu, u.buffer, size);
+ */
 static void kvm_vcpu_ioctl_x86_get_xsave2(struct kvm_vcpu *vcpu,
 					  u8 *state, unsigned int size)
 {
@@ -6264,11 +7168,28 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		break;
 	case KVM_CAP_PMU_CAPABILITY:
 		r = -EINVAL;
+		/*
+		 * QEMU-7.1没有KVM_CAP_PMU_VALID_MASK
+		 *
+		 * 如果global disable了,返回error
+		 * 如果想修改不允许修改的bit,返回error
+		 */
 		if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
 			break;
 
 		mutex_lock(&kvm->lock);
+		/*
+		 * 注意! 只能在created_vcpus是0的时候改
+		 *
+		 * 在以下修改kvm->created_vcpus:
+		 *   - virt/kvm/kvm_main.c|3921| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus++;
+		 *   - virt/kvm/kvm_main.c|3995| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus--;
+		 */
 		if (!kvm->created_vcpus) {
+			/*
+			 * 没见到QEMU用KVM_PMU_CAP_DISABLE
+			 * 也没见到QEMU用KVM_CAP_PMU_CAPABILITY
+			 */
 			kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
 			r = 0;
 		}
@@ -6486,6 +7407,9 @@ int kvm_arch_pm_notifier(struct kvm *kvm, unsigned long state)
 }
 #endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */
 
+/*
+ * arch/x86/kvm/x86.c|7188| <<kvm_arch_vm_ioctl(KVM_GET_CLOCK)>> r = kvm_vm_ioctl_get_clock(kvm, argp);
+ */
 static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_clock_data data = { 0 };
@@ -6882,6 +7806,10 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12019| <<kvm_arch_hardware_setup>> kvm_init_msr_list();
+ */
 static void kvm_init_msr_list(void)
 {
 	u32 dummy[2];
@@ -8523,6 +9451,52 @@ static int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1597| <<kvm_emulate_cpuid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/hyperv.c|2078| <<kvm_hv_hypercall_complete>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|849| <<nested_svm_vmrun>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|852| <<nested_svm_vmrun>> ret = kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2204| <<vmload_vmsave_interception>> ret = kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2378| <<stgi_interception>> ret = kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2390| <<clgi_interception>> ret = kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2409| <<invlpga_interception>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2506| <<invlpg_interception>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|3131| <<pause_interception>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|156| <<nested_vmx_succeed>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|165| <<nested_vmx_failInvalid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|184| <<nested_vmx_failValid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5574| <<handle_invvpid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5664| <<handle_vmfunc>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/sgx.c|199| <<__handle_encls_ecreate>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/sgx.c|345| <<handle_encls_einit>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5385| <<handle_cr>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5390| <<handle_cr>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5398| <<handle_cr>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5516| <<handle_invlpg>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5535| <<handle_apic_access>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5694| <<handle_ept_misconfig>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5819| <<handle_pause>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|712| <<kvm_complete_insn_gp>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|1083| <<kvm_emulate_xsetbv>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|1394| <<kvm_emulate_rdpmc>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|2107| <<kvm_emulate_as_nop>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|2199| <<handle_fastpath_set_msr_irqoff>> kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|2206| <<handle_fastpath_set_msr_irqoff>> kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|8555| <<kvm_emulate_wbinvd>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|9696| <<complete_fast_pio_out>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|9716| <<kvm_fast_pio_out>> kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|9742| <<complete_fast_pio_in>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|9774| <<kvm_fast_pio>> return ret && kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|10236| <<kvm_emulate_halt>> int ret = kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|10247| <<kvm_emulate_ap_reset_hold>> int ret = kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|10397| <<complete_hypercall_exit>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|10497| <<kvm_emulate_hypercall>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|14150| <<kvm_handle_invpcid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|14159| <<kvm_handle_invpcid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/x86.c|14172| <<kvm_handle_invpcid>> return kvm_skip_emulated_instruction(vcpu);
+ *   - arch/x86/kvm/xen.c|945| <<kvm_xen_hypercall_set_result>> return kvm_skip_emulated_instruction(vcpu);
+ */
 int kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu)
 {
 	unsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);
@@ -8626,6 +9600,10 @@ static bool is_vmware_backdoor_opcode(struct x86_emulate_ctxt *ctxt)
  * [*] Except #MC, which is higher priority, but KVM should never emulate in
  *     response to a machine check.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9494| <<x86_emulate_instruction>> r = x86_decode_emulated_instruction(vcpu, emulation_type,
+ */
 int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 				    void *insn, int insn_len)
 {
@@ -8643,6 +9621,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5639| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|9642| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|9649| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -8827,12 +9811,36 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|711| <<avic_unaccelerated_access_interception>> ret = kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|398| <<__svm_skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/svm/svm.c|2161| <<io_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2324| <<gp_interception>> return kvm_emulate_instruction(vcpu,
+ *   - arch/x86/kvm/svm/svm.c|2503| <<invlpg_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/svm/svm.c|2511| <<emulate_on_interception>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|1630| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP)) 
+ *   - arch/x86/kvm/vmx/vmx.c|5024| <<handle_rmode_exception>> if (kvm_emulate_instruction(vcpu, 0)) {
+ *   - arch/x86/kvm/vmx/vmx.c|5113| <<handle_exception_nmi>> return kvm_emulate_instruction(vcpu, EMULTYPE_VMWARE_GP);
+ *   - arch/x86/kvm/vmx/vmx.c|5251| <<handle_io>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5325| <<handle_desc>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5538| <<handle_apic_access>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5674| <<handle_ept_violation>> return kvm_emulate_instruction(vcpu, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5736| <<handle_invalid_guest_state>> if (!kvm_emulate_instruction(vcpu, 0))
+ *   - arch/x86/kvm/x86.c|725| <<complete_emulated_insn_gp>> return kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE | EMULTYPE_SKIP |
+ *   - arch/x86/kvm/x86.c|8085| <<handle_ud>> return kvm_emulate_instruction(vcpu, emul_type);
+ *   - arch/x86/kvm/x86.c|11653| <<complete_emulated_io>> return kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);
+ */
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
 {
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2516| <<rsm_interception>> return kvm_emulate_instruction_from_buffer(vcpu, rsm_ins_bytes, 2);
+ */
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len)
 {
@@ -9114,6 +10122,15 @@ static void kvm_timer_init(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9433| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9442| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9585| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+ *
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -9121,13 +10138,29 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	unsigned long i;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 处理kvm_update_masterclock()
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2403| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3100| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9386| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9421| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
 
+/*
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9433| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9442| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9585| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
 
 /*
@@ -9135,22 +10168,59 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  * region to prevent possible deadlocks against time accessors which
  * are invoked with work related locks held.
  */
+/*
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9433| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9442| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9585| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+ *
+ * 在以下使用pvclock_irq_work_fn():
+ *   - arch/x86/kvm/x86.c|9445| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
+	/*
+	 * pvclock_gtod_update_fn()
+	 */
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9445| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9468| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9584| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 4.14的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ *
+ * struct notifier_block pvclock_gtod_notifier.notifier_call = pvclock_gtod_notify()
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	struct timekeeper *tk = priv;
 
+	/*
+	 * 只在这里调用
+	 */
 	update_pvclock_gtod(tk);
 
 	/*
@@ -9158,12 +10228,25 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|9445| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|9468| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|9584| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+	 *
+	 * pvclock_irq_work_fn()
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
 	return 0;
 }
 
+/*
+ * 在以下使用pvclock_gtod_notifier:
+ *   - arch/x86/kvm/x86.c|9555| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|9583| <<kvm_arch_exit>> pvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);
+ */
 static struct notifier_block pvclock_gtod_notifier = {
 	.notifier_call = pvclock_gtod_notify,
 };
@@ -9240,6 +10323,11 @@ int kvm_arch_init(void *opaque)
 	kvm_timer_init();
 
 	if (boot_cpu_has(X86_FEATURE_XSAVE)) {
+		/*
+		 * 程序通过访问寄存器XCR0 (eXterned Control Register)
+		 * 可以得到操作系统对SIMD扩展的支持信息.
+		 * 该寄存器通过XSETBV进行设置,通过XGETBV进行读取.
+		 */
 		host_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
 		kvm_caps.supported_xcr0 = host_xcr0 & KVM_SUPPORTED_XCR0;
 	}
@@ -9335,6 +10423,10 @@ int kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10004| <<kvm_emulate_hypercall(KVM_HC_CLOCK_PAIRING)>> ret = kvm_pv_clock_pairing(vcpu, a0, a1);
+ */
 static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 			        unsigned long clock_type)
 {
@@ -9606,6 +10698,10 @@ static int dm_request_for_irq_injection(struct kvm_vcpu *vcpu)
 		likely(!pic_in_kernel(vcpu->kvm));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12047| <<kvm_arch_vcpu_ioctl_run>> post_kvm_run_save(vcpu);
+ */
 /* Called within kvm->srcu read side.  */
 static void post_kvm_run_save(struct kvm_vcpu *vcpu)
 {
@@ -10250,6 +11346,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10777| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -10284,6 +11384,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_mmu_free_obsolete_roots(vcpu);
 		if (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))
 			__kvm_migrate_timers(vcpu);
+		/*
+		 * 但是!!! 只有online的时候(INIT/SIPI之后)才会执行这个request去更新master clock
+		 */
 		if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
 			kvm_update_masterclock(vcpu->kvm);
 		if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
@@ -10421,6 +11524,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 只在这里调用
+	 */
 	r = kvm_mmu_reload(vcpu);
 	if (unlikely(r)) {
 		goto cancel_injection;
@@ -10428,6 +11534,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 
 	preempt_disable();
 
+	/*
+	 * vmx_prepare_switch_to_guest()
+	 */
 	static_call(kvm_x86_prepare_switch_to_guest)(vcpu);
 
 	/*
@@ -10466,6 +11575,9 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (kvm_lapic_enabled(vcpu))
 		static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
 
+	/*
+	 * KVM_REQ_MCLOCK_INPROGRESS应该是在这里判断的
+	 */
 	if (kvm_vcpu_exit_request(vcpu)) {
 		vcpu->mode = OUTSIDE_GUEST_MODE;
 		smp_wmb();
@@ -10510,6 +11622,16 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
+		/*
+		 * enum exit_fastpath_completion {
+		 *     EXIT_FASTPATH_NONE,
+		 *     EXIT_FASTPATH_REENTER_GUEST,
+		 *     EXIT_FASTPATH_EXIT_HANDLED,
+		 * };
+		 * typedef enum exit_fastpath_completion fastpath_t;
+		 *
+		 * vmx_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_vcpu_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
@@ -10517,6 +11639,11 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (kvm_lapic_enabled(vcpu))
 			static_call_cond(kvm_x86_sync_pir_to_irr)(vcpu);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|11284| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11338| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+		 */
 		if (unlikely(kvm_vcpu_exit_request(vcpu))) {
 			exit_fastpath = EXIT_FASTPATH_EXIT_HANDLED;
 			break;
@@ -10546,6 +11673,11 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (hw_breakpoint_active())
 		hw_breakpoint_restore();
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_vmentry_cpu:
+	 *   - arch/x86/kvm/x86.c|10627| <<vcpu_enter_guest>> vcpu->arch.last_vmentry_cpu = vcpu->cpu;
+	 *   - arch/x86/kvm/x86.c|11624| <<kvm_arch_vcpu_create>> vcpu->arch.last_vmentry_cpu = -1;
+	 */
 	vcpu->arch.last_vmentry_cpu = vcpu->cpu;
 	vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
 
@@ -10560,6 +11692,11 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * 只在此处调用
+	 *
+	 * vmx_handle_exit_irqoff()
+	 */
 	static_call(kvm_x86_handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -10572,6 +11709,12 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	 * interrupts on processors that implement an interrupt shadow, the
 	 * stat.exits increment will do nicely.
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/svm.c|4061| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6876| <<handle_interrupt_nmi_irqoff>> kvm_before_interrupt(vcpu, is_nmi ? KVM_HANDLING_NMI : KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/x86.c|11657| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
 	local_irq_enable();
 	++vcpu->stat.exits;
@@ -10600,12 +11743,21 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		profile_hit(KVM_PROFILING, (void *)rip);
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2694| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10396| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|11685| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	if (unlikely(vcpu->arch.tsc_always_catchup))
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/*
+	 * vmx_handle_exit()
+	 */
 	r = static_call(kvm_x86_handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -10620,6 +11772,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 }
 
 /* Called within kvm->srcu read side.  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11365| <<vcpu_run>> r = vcpu_block(vcpu);
+ */
 static inline int vcpu_block(struct kvm_vcpu *vcpu)
 {
 	bool hv_timer;
@@ -10671,6 +11827,11 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 对于不是nested的, 必须:
+ * 1. KVM_MP_STATE_RUNNABLE
+ * 2. !vcpu->arch.apf.halted
+ */
 static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu))
@@ -10680,6 +11841,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12021| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 /* Called within kvm->srcu read side.  */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
@@ -10695,6 +11860,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 		 * this point can start executing an instruction.
 		 */
 		vcpu->arch.at_instruction_boundary = false;
+		/*
+		 * 对于不是nested的, 必须:
+		 * 1. KVM_MP_STATE_RUNNABLE
+		 * 2. !vcpu->arch.apf.halted
+		 */
 		if (kvm_vcpu_running(vcpu)) {
 			r = vcpu_enter_guest(vcpu);
 		} else {
@@ -10731,6 +11901,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11660| <<complete_emulated_pio>> return complete_emulated_io(vcpu);
+ *   - arch/x86/kvm/x86.c|11713| <<complete_emulated_mmio>> return complete_emulated_io(vcpu);
+ */
 static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
 {
 	return kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);
@@ -10806,6 +11981,12 @@ static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11741| <<kvm_arch_vcpu_ioctl_run>> kvm_load_guest_fpu(vcpu);
+ *   - arch/x86/kvm/x86.c|11990| <<kvm_arch_vcpu_ioctl_get_mpstate>> kvm_load_guest_fpu(vcpu);
+ *   - arch/x86/kvm/x86.c|12677| <<kvm_vcpu_reset>> kvm_load_guest_fpu(vcpu);
+ */
 /* Swap (qemu) user FPU context for the guest FPU context. */
 static void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)
 {
@@ -10814,6 +11995,12 @@ static void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11816| <<kvm_arch_vcpu_ioctl_run>> kvm_put_guest_fpu(vcpu);
+ *   - arch/x86/kvm/x86.c|12006| <<kvm_arch_vcpu_ioctl_get_mpstate>> kvm_put_guest_fpu(vcpu);
+ *   - arch/x86/kvm/x86.c|12671| <<kvm_vcpu_reset>> kvm_put_guest_fpu(vcpu);
+ */
 /* When vcpu_run ends, restore user space FPU context. */
 static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 {
@@ -10822,6 +12009,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4117| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *kvm_run = vcpu->run;
@@ -10845,6 +12036,12 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE(kvm_lapic_hv_timer_in_use(vcpu));
 
 		kvm_vcpu_srcu_read_unlock(vcpu);
+		/*
+		 * 注释
+		 * Block the vCPU until the vCPU is runnable, an event arrives, or a signal is
+		 * pending.  This is mostly used when halting a vCPU, but may also be used
+		 * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
+		 */
 		kvm_vcpu_block(vcpu);
 		kvm_vcpu_srcu_read_lock(vcpu);
 
@@ -10898,10 +12095,16 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
+	/*
+	 * vmx_vcpu_pre_run()
+	 */
 	r = static_call(kvm_x86_vcpu_pre_run)(vcpu);
 	if (r <= 0)
 		goto out;
 
+	/*
+	 * 只在此处调用
+	 */
 	r = vcpu_run(vcpu);
 
 out:
@@ -11072,6 +12275,10 @@ int kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4146| <<kvm_vcpu_ioctl(KVM_GET_MP_STATE)>> r = kvm_arch_vcpu_ioctl_get_mpstate(vcpu, &mp_state);
+ */
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state)
 {
@@ -11100,6 +12307,10 @@ int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4161| <<kvm_vcpu_ioctl(KVM_SET_MP_STATE)>> r = kvm_arch_vcpu_ioctl_set_mpstate(vcpu, &mp_state);
+ */
 int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 				    struct kvm_mp_state *mp_state)
 {
@@ -11699,6 +12910,12 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		static_branch_dec(&kvm_has_noapic_vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3077| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2126| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11633| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -11949,6 +13166,10 @@ void kvm_arch_hardware_disable(void)
 	drop_user_return_notifiers();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11992| <<kvm_arch_hardware_setup>> kvm_ops_update(ops);
+ */
 static inline void kvm_ops_update(struct kvm_x86_init_ops *ops)
 {
 	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
@@ -12051,6 +13272,16 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
+		/*
+		 * 在以下使用kvm_pmu->need_cleanup:
+		 *   - arch/x86/kvm/pmu.c|451| <<kvm_pmu_handle_event>> if (unlikely(pmu->need_cleanup))
+		 *   - arch/x86/kvm/pmu.c|615| <<kvm_pmu_init>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/pmu.c|631| <<kvm_pmu_cleanup>> pmu->need_cleanup = false;
+		 *   - arch/x86/kvm/x86.c|12064| <<kvm_arch_sched_in>> pmu->need_cleanup = true;
+		 *
+		 * The gate to release perf_events not marked in
+		 * pmc_in_use only once in a vcpu time slice.
+		 */
 		pmu->need_cleanup = true;
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
@@ -12175,6 +13406,16 @@ void kvm_arch_sync_events(struct kvm *kvm)
  * address, i.e. its accessibility is not guaranteed, and must be
  * accessed via __copy_{to,from}_user().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|271| <<avic_alloc_access_page>> ret = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3825| <<init_rmode_identity_map>> uaddr = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3874| <<alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx/vmx.c|5047| <<vmx_set_tss_addr>> ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|13354| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|13356| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|13358| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 void __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
 				      u32 size)
 {
@@ -12972,11 +14213,24 @@ bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_has_noncoherent_dma);
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|430| <<kvm_irqfd_assign>> if (kvm_arch_has_irq_bypass()) {
+ */
 bool kvm_arch_has_irq_bypass(void)
 {
 	return true;
 }
 
+/*
+ * 在以下使用irq_bypass_consumer->add_producer:
+ *   - virt/kvm/eventfd.c|432| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ *   - virt/lib/irqbypass.c|66| <<__connect>> ret = cons->add_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|217| <<irq_bypass_register_consumer>> !consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用kvm_arch_irq_bypass_add_producer():
+ *   - virt/kvm/eventfd.c|422| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ */
 int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 				      struct irq_bypass_producer *prod)
 {
@@ -12995,6 +14249,10 @@ int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 	return ret;
 }
 
+/*
+ * 在以下使用kvm_arch_irq_bypass_del_producer():
+ *   - virt/kvm/eventfd.c|457| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ */
 void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 				      struct irq_bypass_producer *prod)
 {
@@ -13019,6 +14277,10 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	kvm_arch_end_assignment(irqfd->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|640| <<kvm_irq_routing_update>> int ret = kvm_arch_update_irqfd_routing(irqfd->kvm, irqfd->producer->irq, irqfd->gsi, 1);
+ */
 int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				   uint32_t guest_irq, bool set)
 {
@@ -13102,6 +14364,17 @@ EXPORT_SYMBOL_GPL(kvm_fixup_and_inject_pf_error);
  * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value
  * indicates whether exit to userspace is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3818| <<vmx_complete_nested_posted_interrupt>> kvm_handle_memory_failure(vcpu, X86EMUL_IO_NEEDED, NULL);
+ *   - arch/x86/kvm/vmx/nested.c|4843| <<nested_vmx_get_vmptr>> *ret = kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5175| <<handle_vmread>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5248| <<handle_vmwrite>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5418| <<handle_vmptrst>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5464| <<handle_invept>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5547| <<handle_invvpid>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/x86.c|13258| <<kvm_handle_invpcid>> return kvm_handle_memory_failure(vcpu, r, &e);
+ */
 int kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,
 			      struct x86_exception *e)
 {
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 1926d2cb8..296afc971 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -10,13 +10,56 @@
 
 struct kvm_caps {
 	/* control of guest tsc rate supported? */
+	/*
+	 * 在以下使用kvm_caps->has_tsc_control:
+	 *   - arch/x86/kvm/debugfs.c|69| <<kvm_arch_create_vcpu_debugfs>> if (kvm_caps.has_tsc_control) {
+	 *   - arch/x86/kvm/svm/svm.c|5014| <<svm_hardware_setup>> kvm_caps.has_tsc_control = true;
+	 *   - arch/x86/kvm/vmx/nested.c|2560| <<prepare_vmcs02>> if (kvm_caps.has_tsc_control)
+	 *   - arch/x86/kvm/vmx/nested.c|4611| <<nested_vmx_vmexit>> if (kvm_caps.has_tsc_control)
+	 *   - arch/x86/kvm/vmx/vmx.c|8322| <<hardware_setup>> kvm_caps.has_tsc_control = true;
+	 *   - arch/x86/kvm/x86.c|2614| <<set_tsc_khz>> if (!kvm_caps.has_tsc_control) {
+	 *   - arch/x86/kvm/x86.c|2866| <<kvm_vcpu_write_tsc_multiplier>> if (kvm_caps.has_tsc_control)
+	 *   - arch/x86/kvm/x86.c|3702| <<kvm_guest_time_update>> if (kvm_caps.has_tsc_control)
+	 *   - arch/x86/kvm/x86.c|5069| <<kvm_vm_ioctl_check_extension>> r = kvm_caps.has_tsc_control;
+	 *   - arch/x86/kvm/x86.c|6393| <<kvm_arch_vcpu_ioctl>> if (kvm_caps.has_tsc_control &&
+	 *   - arch/x86/kvm/x86.c|7453| <<kvm_arch_vm_ioctl>> if (kvm_caps.has_tsc_control &&
+	 *   - arch/x86/kvm/x86.c|12749| <<kvm_arch_hardware_setup>> if (kvm_caps.has_tsc_control) {
+	 */
 	bool has_tsc_control;
 	/* maximum supported tsc_khz for guests */
 	u32  max_guest_tsc_khz;
+	/*
+	 * 在以下使用kvm_caps->tsc_scaling_ratio_frac_bits:
+	 *   - arch/x86/kvm/debugfs.c|51| <<vcpu_get_tsc_scaling_frac_bits>> *val = kvm_caps.tsc_scaling_ratio_frac_bits;
+	 *   - arch/x86/kvm/svm/svm.c|5053| <<svm_hardware_setup>> kvm_caps.tsc_scaling_ratio_frac_bits = 32;
+	 *   - arch/x86/kvm/vmx/vmx.c|8207| <<vmx_set_hv_timer>> kvm_caps.tsc_scaling_ratio_frac_bits,
+	 *   - arch/x86/kvm/vmx/vmx.c|8685| <<hardware_setup>> kvm_caps.tsc_scaling_ratio_frac_bits = 48;
+	 *   - arch/x86/kvm/x86.c|2707| <<set_tsc_khz>> ratio = mul_u64_u32_div(1ULL << kvm_caps.tsc_scaling_ratio_frac_bits,
+	 *   - arch/x86/kvm/x86.c|2835| <<__scale_tsc>> return mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
+	 *   - arch/x86/kvm/x86.c|2895| <<kvm_calc_nested_tsc_offset>> kvm_caps.tsc_scaling_ratio_frac_bits);
+	 *   - arch/x86/kvm/x86.c|2906| <<kvm_calc_nested_tsc_multiplier>> kvm_caps.tsc_scaling_ratio_frac_bits);
+	 *   - arch/x86/kvm/x86.c|13202| <<kvm_arch_hardware_setup>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	/* number of bits of the fractional part of the TSC scaling ratio */
 	u8   tsc_scaling_ratio_frac_bits;
 	/* maximum allowed value of TSC scaling ratio */
 	u64  max_tsc_scaling_ratio;
+	/*
+	 * 在以下使用kvm_caps->default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1663| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|686| <<nested_vmcb02_prepare_control>> if (svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|1044| <<nested_svm_vmexit>> if (svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1367| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1894| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8205| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2690| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2733| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2842| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2891| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2904| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3221| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|13202| <<kvm_arch_hardware_setup>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	/* 1ull << kvm_caps.tsc_scaling_ratio_frac_bits */
 	u64  default_tsc_scaling_ratio;
 	/* bus lock detection supported? */
@@ -25,7 +68,27 @@ struct kvm_caps {
 	bool has_notify_vmexit;
 
 	u64 supported_mce_cap;
+	/*
+	 * 在以下使用kvm_caps->supported_xcr0:
+	 *   - arch/x86/kvm/cpuid.c|260| <<cpuid_get_supported_xcr0>> return (best->eax | ((u64)best->edx << 32)) & kvm_caps.supported_xcr0;
+	 *   - arch/x86/kvm/cpuid.c|983| <<__do_cpuid_func>> u64 permitted_xcr0 = kvm_caps.supported_xcr0 & xstate_get_guest_group_perm();
+	 *   - arch/x86/kvm/svm/svm.c|5003| <<svm_hardware_setup>> kvm_caps.supported_xcr0 &= ~(XFEATURE_MASK_BNDREGS |
+	 *   - arch/x86/kvm/vmx/vmx.c|8254| <<hardware_setup>> kvm_caps.supported_xcr0 &= ~(XFEATURE_MASK_BNDREGS |
+	 *   - arch/x86/kvm/x86.c|5100| <<kvm_vm_ioctl_check_extension>> r = xstate_required_size(kvm_caps.supported_xcr0 & guest_perm, false);
+	 *   - arch/x86/kvm/x86.c|5145| <<kvm_x86_dev_get_attr>> if (put_user(kvm_caps.supported_xcr0, uaddr))
+	 *   - arch/x86/kvm/x86.c|5887| <<kvm_vcpu_ioctl_x86_set_xsave>> kvm_caps.supported_xcr0,
+	 *   - arch/x86/kvm/x86.c|9962| <<kvm_arch_init>> kvm_caps.supported_xcr0 = host_xcr0 & KVM_SUPPORTED_XCR0;
+	 *   - arch/x86/kvm/x86.h|315| <<kvm_mpx_supported>> return (kvm_caps.supported_xcr0 & (XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR))
+	 */
 	u64 supported_xcr0;
+	/*
+	 * 在以下使用kvm_caps->supported_xss:
+	 *   - arch/x86/kvm/cpuid.c|984| <<__do_cpuid_func>> u64 permitted_xss = kvm_caps.supported_xss;
+	 *   - arch/x86/kvm/svm/svm.c|4927| <<svm_set_cpu_caps>> kvm_caps.supported_xss = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7725| <<vmx_set_cpu_caps>> kvm_caps.supported_xss = 0;
+	 *   - arch/x86/kvm/x86.c|4270| <<kvm_set_msr_common>> if (data & ~kvm_caps.supported_xss)
+	 *   - arch/x86/kvm/x86.c|12743| <<kvm_arch_hardware_setup>> kvm_caps.supported_xss = 0;
+	 */
 	u64 supported_xss;
 };
 
@@ -247,6 +310,24 @@ static inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2596| <<cr_interception>> val = kvm_register_read(vcpu, reg);
+ *   - arch/x86/kvm/svm/svm.c|2708| <<dr_interception>> val = kvm_register_read(vcpu, reg);
+ *   - arch/x86/kvm/vmx/nested.c|4740| <<get_vmx_mem_address>> off += kvm_register_read(vcpu, base_reg);
+ *   - arch/x86/kvm/vmx/nested.c|4742| <<get_vmx_mem_address>> off += kvm_register_read(vcpu, index_reg) << scaling;
+ *   - arch/x86/kvm/vmx/nested.c|5117| <<handle_vmread>> field = kvm_register_read(vcpu, (((instr_info) >> 28) & 0xf));
+ *   - arch/x86/kvm/vmx/nested.c|5240| <<handle_vmwrite>> value = kvm_register_read(vcpu, (((instr_info) >> 3) & 0xf));
+ *   - arch/x86/kvm/vmx/nested.c|5251| <<handle_vmwrite>> field = kvm_register_read(vcpu, (((instr_info) >> 28) & 0xf));
+ *   - arch/x86/kvm/vmx/nested.c|5449| <<handle_invept>> type = kvm_register_read(vcpu, gpr_index);
+ *   - arch/x86/kvm/vmx/nested.c|5530| <<handle_invvpid>> type = kvm_register_read(vcpu, gpr_index);
+ *   - arch/x86/kvm/vmx/nested.c|5788| <<nested_vmx_exit_handled_cr>> val = kvm_register_read(vcpu, reg);
+ *   - arch/x86/kvm/vmx/nested.c|5874| <<nested_vmx_exit_handled_vmcs_access>> field = kvm_register_read(vcpu, (((vmx_instruction_info) >> 28) & 0xf));
+ *   - arch/x86/kvm/vmx/vmx.c|5430| <<handle_cr>> val = kvm_register_read(vcpu, reg);
+ *   - arch/x86/kvm/vmx/vmx.c|5553| <<handle_dr>> err = kvm_set_dr(vcpu, dr, kvm_register_read(vcpu, reg));
+ *   - arch/x86/kvm/vmx/vmx.c|5934| <<handle_invpcid>> type = kvm_register_read(vcpu, gpr_index);
+ *   - arch/x86/kvm/xen.c|1221| <<kvm_xen_hypercall>> input = (u64)kvm_register_read(vcpu, VCPU_REGS_RAX);
+ */
 static inline unsigned long kvm_register_read(struct kvm_vcpu *vcpu, int reg)
 {
 	unsigned long val = kvm_register_read_raw(vcpu, reg);
@@ -326,8 +407,25 @@ extern bool report_ignored_msrs;
 
 extern bool eager_page_split;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1665| <<__wait_lapic_expire>> nsec_to_cycles(vcpu, timer_advance_ns)));
+ *   - arch/x86/kvm/lapic.c|1851| <<update_target_expiration>> nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+ *   - arch/x86/kvm/lapic.c|1852| <<update_target_expiration>> nsec_to_cycles(apic->vcpu, ns_remaining_old);
+ *   - arch/x86/kvm/lapic.c|1896| <<set_target_expiration>> nsec_to_cycles(apic->vcpu, deadline);
+ *   - arch/x86/kvm/lapic.c|1920| <<advance_periodic_target_expiration>> nsec_to_cycles(apic->vcpu, delta);
+ *   - arch/x86/kvm/vmx/vmx.c|8196| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu,
+ *   - arch/x86/kvm/x86.c|3148| <<kvm_synchronize_tsc>> nsec_to_cycles(vcpu, elapsed);
+ *   - arch/x86/kvm/x86.c|3198| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+ */
 static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/include/asm/pvclock.h|97| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
+	 *   - arch/x86/kvm/x86.c|2713| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.h|366| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+	 */
 	return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
 				   vcpu->arch.virtual_tsc_shift);
 }
@@ -377,12 +475,24 @@ enum kvm_intr_type {
 	KVM_HANDLING_NMI,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4061| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6876| <<handle_interrupt_nmi_irqoff>> kvm_before_interrupt(vcpu, is_nmi ? KVM_HANDLING_NMI : KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/x86.c|11657| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ */
 static inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,
 					enum kvm_intr_type intr)
 {
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4069| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6878| <<handle_interrupt_nmi_irqoff>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/x86.c|11661| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+ */
 static inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 {
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
diff --git a/arch/x86/mm/extable.c b/arch/x86/mm/extable.c
index 60814e110..94a24fa5d 100644
--- a/arch/x86/mm/extable.c
+++ b/arch/x86/mm/extable.c
@@ -144,6 +144,13 @@ static bool ex_handler_copy(const struct exception_table_entry *fixup,
 	return ex_handler_fault(fixup, regs, trapnr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/extable.c|251| <<fixup_exception>> return ex_handler_msr(e, regs, true, false, reg);
+ *   - arch/x86/mm/extable.c|253| <<fixup_exception>> return ex_handler_msr(e, regs, false, false, reg);
+ *   - arch/x86/mm/extable.c|255| <<fixup_exception>> return ex_handler_msr(e, regs, true, true, reg);
+ *   - arch/x86/mm/extable.c|257| <<fixup_exception>> return ex_handler_msr(e, regs, false, true, reg);
+ */
 static bool ex_handler_msr(const struct exception_table_entry *fixup,
 			   struct pt_regs *regs, bool wrmsr, bool safe, int reg)
 {
diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c
index 41d7669a9..e509886a9 100644
--- a/arch/x86/realmode/init.c
+++ b/arch/x86/realmode/init.c
@@ -86,6 +86,10 @@ static void __init sme_sev_setup_real_mode(struct trampoline_header *th)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/realmode/init.c|208| <<init_real_mode>> setup_real_mode();
+ */
 static void __init setup_real_mode(void)
 {
 	u16 real_mode_seg;
diff --git a/arch/x86/xen/enlighten_hvm.c b/arch/x86/xen/enlighten_hvm.c
index 1c1ac4184..7d60c8caf 100644
--- a/arch/x86/xen/enlighten_hvm.c
+++ b/arch/x86/xen/enlighten_hvm.c
@@ -33,6 +33,13 @@
 
 static unsigned long shared_info_pfn;
 
+/*
+ * 在以下使用xen_percpu_upcall:
+ *   - arch/x86/xen/enlighten_hvm.c|134| <<DEFINE_IDTENTRY_SYSVEC>> if (xen_percpu_upcall)
+ *   - arch/x86/xen/enlighten_hvm.c|180| <<xen_cpu_up_prepare_hvm>> if (xen_percpu_upcall) {
+ *   - arch/x86/xen/suspend_hvm.c|18| <<xen_hvm_post_suspend>> if (xen_percpu_upcall) {
+ *   - drivers/xen/events/events_base.c|2217| <<xen_init_setup_upcall_vector>> xen_percpu_upcall = true;
+ */
 __ro_after_init bool xen_percpu_upcall;
 EXPORT_SYMBOL_GPL(xen_percpu_upcall);
 
diff --git a/arch/x86/xen/time.c b/arch/x86/xen/time.c
index 9ef0a5cca..6c9937e99 100644
--- a/arch/x86/xen/time.c
+++ b/arch/x86/xen/time.c
@@ -31,6 +31,13 @@
 /* Minimum amount of time until next clock event fires */
 #define TIMER_SLOP	100000
 
+/*
+ * 在以下使用xen_sched_clock_offset:
+ *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+ *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+ */
 static u64 xen_sched_clock_offset __read_mostly;
 
 /* Get the TSC speed from Xen */
diff --git a/block/blk-mq.c b/block/blk-mq.c
index c96c8c4f7..74bd15d02 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -3301,6 +3301,14 @@ static inline bool blk_mq_last_cpu_in_hctx(unsigned int cpu,
 	return true;
 }
 
+/*
+ * [0] blk_mq_hctx_notify_offline
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,
@@ -3334,6 +3342,14 @@ static int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)
 	return 0;
 }
 
+/*
+ * [0] blk_mq_hctx_notify_online
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_thread_fun
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int blk_mq_hctx_notify_online(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,
diff --git a/drivers/acpi/cppc_acpi.c b/drivers/acpi/cppc_acpi.c
index 1e15a9f25..1851110d4 100644
--- a/drivers/acpi/cppc_acpi.c
+++ b/drivers/acpi/cppc_acpi.c
@@ -44,8 +44,21 @@
 
 struct cppc_pcc_data {
 	struct pcc_mbox_chan *pcc_channel;
+	/*
+	 * 在以下使用cpcc_acpi.c的cppc_pcc_data->pcc_comm_addr:
+	 *   - drivers/acpi/cppc_acpi.c|95| <<GET_PCC_VADDR>> #define GET_PCC_VADDR(offs, pcc_ss_id) (pcc_data[pcc_ss_id]->pcc_comm_addr + \
+	 *   - drivers/acpi/cppc_acpi.c|206| <<check_pcc_chan>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|241| <<send_pcc_cmd>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|551| <<register_pcc_channel>> pcc_data[pcc_ss_idx]->pcc_comm_addr = acpi_os_ioremap(pcc_chan->shmem_base_addr, pcc_chan->shmem_size);
+	 *   - drivers/acpi/cppc_acpi.c|554| <<register_pcc_channel>> if (!pcc_data[pcc_ss_idx]->pcc_comm_addr) {
+	 */
 	void __iomem *pcc_comm_addr;
 	bool pcc_channel_acquired;
+	/*
+	 * 在以下使用cpcc_acpi.c的cppc_pcc_data->deadline_us:
+	 *   - drivers/acpi/cppc_acpi.c|217| <<check_pcc_chan>> pcc_ss_data->deadline_us);
+	 *   - drivers/acpi/cppc_acpi.c|546| <<register_pcc_channel>> pcc_data[pcc_ss_idx]->deadline_us = usecs_lat;
+	 */
 	unsigned int deadline_us;
 	unsigned int pcc_mpar, pcc_mrtt, pcc_nominal;
 
@@ -198,10 +211,24 @@ static struct kobj_type cppc_ktype = {
 	.default_groups = cppc_groups,
 };
 
+/*
+ * called by:
+ *   - drivers/acpi/cppc_acpi.c|257| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, false);
+ *   - drivers/acpi/cppc_acpi.c|319| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, true);
+ *   - drivers/acpi/cppc_acpi.c|1405| <<cppc_set_perf>> ret = check_pcc_chan(pcc_ss_id, false);
+ */
 static int check_pcc_chan(int pcc_ss_id, bool chk_err_bit)
 {
 	int ret, status;
 	struct cppc_pcc_data *pcc_ss_data = pcc_data[pcc_ss_id];
+	/*
+	 * 在以下使用cpcc_acpi.c的cppc_pcc_data->pcc_comm_addr:
+	 *   - drivers/acpi/cppc_acpi.c|95| <<GET_PCC_VADDR>> #define GET_PCC_VADDR(offs, pcc_ss_id) (pcc_data[pcc_ss_id]->pcc_comm_addr + \
+	 *   - drivers/acpi/cppc_acpi.c|206| <<check_pcc_chan>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|241| <<send_pcc_cmd>> pcc_ss_data->pcc_comm_addr;
+	 *   - drivers/acpi/cppc_acpi.c|551| <<register_pcc_channel>> pcc_data[pcc_ss_idx]->pcc_comm_addr = acpi_os_ioremap(pcc_chan->shmem_base_addr, pcc_chan->shmem_size);
+	 *   - drivers/acpi/cppc_acpi.c|554| <<register_pcc_channel>> if (!pcc_data[pcc_ss_idx]->pcc_comm_addr) {
+	 */
 	struct acpi_pcct_shared_memory __iomem *generic_comm_base =
 		pcc_ss_data->pcc_comm_addr;
 
@@ -233,6 +260,15 @@ static int check_pcc_chan(int pcc_ss_id, bool chk_err_bit)
  * This function transfers the ownership of the PCC to the platform
  * So it must be called while holding write_lock(pcc_lock)
  */
+/*
+ * called by:
+ *   - drivers/acpi/cppc_acpi.c|255| <<send_pcc_cmd>> send_pcc_cmd(pcc_ss_id, CMD_WRITE);
+ *   - drivers/acpi/cppc_acpi.c|1113| <<cppc_get_perf>> if (send_pcc_cmd(pcc_ss_id, CMD_READ) >= 0)
+ *   - drivers/acpi/cppc_acpi.c|1196| <<cppc_get_perf_caps>> if (send_pcc_cmd(pcc_ss_id, CMD_READ) < 0) {
+ *   - drivers/acpi/cppc_acpi.c|1288| <<cppc_get_perf_ctrs>> if (send_pcc_cmd(pcc_ss_id, CMD_READ) < 0) {
+ *   - drivers/acpi/cppc_acpi.c|1359| <<cppc_set_enable>> ret = send_pcc_cmd(pcc_ss_id, CMD_WRITE);
+ *   - drivers/acpi/cppc_acpi.c|1478| <<cppc_set_perf>> send_pcc_cmd(pcc_ss_id, CMD_WRITE);
+ */
 static int send_pcc_cmd(int pcc_ss_id, u16 cmd)
 {
 	int ret = -EIO, i;
@@ -316,6 +352,12 @@ static int send_pcc_cmd(int pcc_ss_id, u16 cmd)
 	}
 
 	/* wait for completion and check for PCC error bit */
+	/*
+	 * called by:
+	 *   - drivers/acpi/cppc_acpi.c|257| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, false);
+	 *   - drivers/acpi/cppc_acpi.c|319| <<send_pcc_cmd>> ret = check_pcc_chan(pcc_ss_id, true);
+	 *   - drivers/acpi/cppc_acpi.c|1405| <<cppc_set_perf>> ret = check_pcc_chan(pcc_ss_id, false);
+	 */
 	ret = check_pcc_chan(pcc_ss_id, true);
 
 	if (pcc_ss_data->pcc_mrtt)
diff --git a/drivers/acpi/utils.c b/drivers/acpi/utils.c
index 5a7b8065e..9e13b1f4e 100644
--- a/drivers/acpi/utils.c
+++ b/drivers/acpi/utils.c
@@ -610,6 +610,23 @@ EXPORT_SYMBOL(acpi_execute_simple_method);
  *
  * Evaluate device's _EJ0 method for hotplug operations.
  */
+/*
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_evaluate_ej0
+ * [0] acpiphp_disable_and_eject_slot
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/dock.c|404| <<handle_eject_request>> acpi_evaluate_ej0(ds->handle);
+ *   - drivers/acpi/scan.c|272| <<acpi_scan_hot_remove>> status = acpi_evaluate_ej0(handle);
+ *   - drivers/pci/hotplug/acpiphp_glue.c|1010| <<acpiphp_disable_and_eject_slot>> if (ACPI_FAILURE(acpi_evaluate_ej0(handle)))
+ */
 acpi_status acpi_evaluate_ej0(acpi_handle handle)
 {
 	acpi_status status;
diff --git a/drivers/block/ublk_drv.c b/drivers/block/ublk_drv.c
index 6a4a94b4c..fb480fea4 100644
--- a/drivers/block/ublk_drv.c
+++ b/drivers/block/ublk_drv.c
@@ -561,6 +561,10 @@ static inline bool ubq_daemon_is_dying(struct ublk_queue *ubq)
 }
 
 /* todo: handle partial completion */
+/*
+ * called by:
+ *   - drivers/block/ublk_drv.c|892| <<ublk_commit_completion>> ublk_complete_rq(req);
+ */
 static void ublk_complete_rq(struct request *req)
 {
 	struct ublk_queue *ubq = req->mq_hctx->driver_data;
@@ -873,6 +877,10 @@ static int ublk_ch_mmap(struct file *filp, struct vm_area_struct *vma)
 	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
 }
 
+/*
+ * 处理UBLK_IO_COMMIT_AND_FETCH_REQ:
+ *   - drivers/block/ublk_drv.c|1113| <<ublk_ch_uring_cmd>> ublk_commit_completion(ub, ub_cmd);
+ */
 static void ublk_commit_completion(struct ublk_device *ub,
 		struct ublksrv_io_cmd *ub_cmd)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index dd9a05174..0ac47ec87 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -322,6 +322,11 @@ static blk_status_t virtblk_prep_rq(struct blk_mq_hw_ctx *hctx,
 	if (unlikely(status))
 		return status;
 
+	/*
+	 * struct virtblk_req *vbr:
+	 * -> struct sg_table sg_table;
+	 *    -> unsigned int nents;
+	 */
 	vbr->sg_table.nents = virtblk_map_data(hctx, req, vbr);
 	if (unlikely(vbr->sg_table.nents < 0)) {
 		virtblk_cleanup_cmd(req);
diff --git a/drivers/block/xen-blkback/blkback.c b/drivers/block/xen-blkback/blkback.c
index a5cf7f1e8..002090032 100644
--- a/drivers/block/xen-blkback/blkback.c
+++ b/drivers/block/xen-blkback/blkback.c
@@ -62,6 +62,13 @@
  * IO workloads.
  */
 
+/*
+ * 在以下使用max_buffer_pages:
+ *   - drivers/block/xen-blkback/blkback.c|65| <<global>> static int max_buffer_pages = 1024;
+ *   - rivers/block/xen-blkback/blkback.c|66| <<global>> module_param_named(max_buffer_pages, max_buffer_pages, int , 0644);
+ *   - drivers/block/xen-blkback/blkback.c|67| <<global>> MODULE_PARM_DESC(max_buffer_pages,
+ *   - drivers/block/xen-blkback/blkback.c|635| <<xen_blkif_schedule>> gnttab_page_cache_shrink(&ring->free_pages, max_buffer_pages);
+ */
 static int max_buffer_pages = 1024;
 module_param_named(max_buffer_pages, max_buffer_pages, int, 0644);
 MODULE_PARM_DESC(max_buffer_pages,
@@ -78,6 +85,19 @@ MODULE_PARM_DESC(max_buffer_pages,
  * algorithm.
  */
 
+/*
+ * 在以下使用max_pgrants:
+ *   - drivers/block/xen-blkback/blkback.c|81| <<global>> static int max_pgrants = 1056;
+ *   - drivers/block/xen-blkback/blkback.c|82| <<global>> module_param_named(max_persistent_grants, max_pgrants, int , 0644);
+ *   - drivers/block/xen-blkback/blkback.c|175| <<add_persistent_gnt>> if (ring->persistent_gnt_c >= max_pgrants) {
+ *   - drivers/block/xen-blkback/blkback.c|344| <<purge_persistent_gnt>> if (ring->persistent_gnt_c < max_pgrants ||
+ *   - drivers/block/xen-blkback/blkback.c|345| <<purge_persistent_gnt>> (ring->persistent_gnt_c == max_pgrants &&
+ *   - drivers/block/xen-blkback/blkback.c|349| <<purge_persistent_gnt>> num_clean = (max_pgrants / 100) * LRU_PERCENT_CLEAN;
+ *   - drivers/block/xen-blkback/blkback.c|350| <<purge_persistent_gnt>> num_clean = ring->persistent_gnt_c - max_pgrants + num_clean;
+ *   - drivers/block/xen-blkback/blkback.c|561| <<print_stats>> ring->persistent_gnt_c, max_pgrants);
+ *   - drivers/block/xen-blkback/blkback.c|881| <<xen_blkbk_map>> ring->persistent_gnt_c < max_pgrants) {
+ *   - drivers/block/xen-blkback/blkback.c|908| <<xen_blkbk_map>> max_pgrants);
+ */
 static int max_pgrants = 1056;
 module_param_named(max_persistent_grants, max_pgrants, int, 0644);
 MODULE_PARM_DESC(max_persistent_grants,
@@ -147,6 +167,11 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 static void make_response(struct xen_blkif_ring *ring, u64 id,
 			  unsigned short op, int st);
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|260| <<free_persistent_gnts>> foreach_grant_safe(persistent_gnt, n, root, node) {
+ *   - drivers/block/xen-blkback/blkback.c|370| <<purge_persistent_gnt>> foreach_grant_safe(persistent_gnt, n, root, node) {
+ */
 #define foreach_grant_safe(pos, n, rbtree, node) \
 	for ((pos) = container_of(rb_first((rbtree)), typeof(*(pos)), node), \
 	     (n) = (&(pos)->node != NULL) ? rb_next(&(pos)->node) : NULL; \
@@ -165,6 +190,10 @@ static void make_response(struct xen_blkif_ring *ring, u64 id,
  * bit operations to modify the flags of a persistent grant and to count
  * the number of used grants.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|949| <<xen_blkbk_map>> if (add_persistent_gnt(ring, persistent_gnt)) {
+ */
 static int add_persistent_gnt(struct xen_blkif_ring *ring,
 			       struct persistent_gnt *persistent_gnt)
 {
@@ -202,12 +231,27 @@ static int add_persistent_gnt(struct xen_blkif_ring *ring,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|787| <<xen_blkbk_map>> persistent_gnt = get_persistent_gnt(
+ */
 static struct persistent_gnt *get_persistent_gnt(struct xen_blkif_ring *ring,
 						 grant_ref_t gref)
 {
 	struct persistent_gnt *data;
 	struct rb_node *node = NULL;
 
+	/*
+	 * 在以下使用xen_blkif_ring->persistent_gnts:
+	 *   - drivers/block/xen-blkback/blkback.c|181| <<add_persistent_gnt>> new = &ring->persistent_gnts.rb_node;
+	 *   - drivers/block/xen-blkback/blkback.c|199| <<add_persistent_gnt>> rb_insert_color(&(persistent_gnt->node), &ring->persistent_gnts);
+	 *   - drivers/block/xen-blkback/blkback.c|215| <<get_persistent_gnt>> node = ring->persistent_gnts.rb_node;
+	 *   - drivers/block/xen-blkback/blkback.c|368| <<purge_persistent_gnt>> root = &ring->persistent_gnts;
+	 *   - drivers/block/xen-blkback/blkback.c|658| <<xen_blkbk_free_caches>> if (!RB_EMPTY_ROOT(&ring->persistent_gnts))
+	 *   - drivers/block/xen-blkback/blkback.c|659| <<xen_blkbk_free_caches>> free_persistent_gnts(ring, &ring->persistent_gnts,
+	 *   - drivers/block/xen-blkback/blkback.c|662| <<xen_blkbk_free_caches>> BUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));
+	 *   - drivers/block/xen-blkback/xenbus.c|354| <<xen_blkif_disconnect>> BUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));
+	 */
 	node = ring->persistent_gnts.rb_node;
 	while (node) {
 		data = container_of(node, struct persistent_gnt, node);
@@ -222,6 +266,13 @@ static struct persistent_gnt *get_persistent_gnt(struct xen_blkif_ring *ring,
 				return NULL;
 			}
 			data->active = true;
+			/*
+			 * 在以下使用xen_blkif_ring->persistent_gnt_in_use:
+			 *   - drivers/block/xen-blkback/blkback.c|230| <<add_persistent_gnt>> atomic_inc(&ring->persistent_gnt_in_use);
+			 *   - drivers/block/xen-blkback/blkback.c|269| <<get_persistent_gnt>> atomic_inc(&ring->persistent_gnt_in_use);
+			 *   - drivers/block/xen-blkback/blkback.c|287| <<put_persistent_gnt>> atomic_dec(&ring->persistent_gnt_in_use);
+			 *   - drivers/block/xen-blkback/xenbus.c|352| <<xen_blkif_disconnect>> BUG_ON(atomic_read(&ring->persistent_gnt_in_use) != 0);
+			 */
 			atomic_inc(&ring->persistent_gnt_in_use);
 			return data;
 		}
@@ -229,6 +280,10 @@ static struct persistent_gnt *get_persistent_gnt(struct xen_blkif_ring *ring,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|730| <<xen_blkbk_unmap_prepare>> put_persistent_gnt(ring, pages[i]->persistent_gnt);
+ */
 static void put_persistent_gnt(struct xen_blkif_ring *ring,
                                struct persistent_gnt *persistent_gnt)
 {
@@ -236,9 +291,20 @@ static void put_persistent_gnt(struct xen_blkif_ring *ring,
 		pr_alert_ratelimited("freeing a grant already unused\n");
 	persistent_gnt->last_used = jiffies;
 	persistent_gnt->active = false;
+	/*
+	 * 在以下使用xen_blkif_ring->persistent_gnt_in_use:
+	 *   - drivers/block/xen-blkback/blkback.c|230| <<add_persistent_gnt>> atomic_inc(&ring->persistent_gnt_in_use);
+	 *   - drivers/block/xen-blkback/blkback.c|269| <<get_persistent_gnt>> atomic_inc(&ring->persistent_gnt_in_use);
+	 *   - drivers/block/xen-blkback/blkback.c|287| <<put_persistent_gnt>> atomic_dec(&ring->persistent_gnt_in_use);
+	 *   - drivers/block/xen-blkback/xenbus.c|352| <<xen_blkif_disconnect>> BUG_ON(atomic_read(&ring->persistent_gnt_in_use) != 0);
+	 */
 	atomic_dec(&ring->persistent_gnt_in_use);
 }
 
+/*
+ * 在以下使用free_persistent_gnts():
+ *   - drivers/block/xen-blkback/blkback.c|709| <<xen_blkbk_free_caches>> free_persistent_gnts(ring, &ring->persistent_gnts,
+ */
 static void free_persistent_gnts(struct xen_blkif_ring *ring, struct rb_root *root,
                                  unsigned int num)
 {
@@ -282,6 +348,17 @@ static void free_persistent_gnts(struct xen_blkif_ring *ring, struct rb_root *ro
 	BUG_ON(num != 0);
 }
 
+/*
+ * 在以下使用xen_blkif_ring->persistent_purge_work:
+ *   - drivers/block/xen-blkback/blkback.c|295| <<xen_blkbk_unmap_purged_grants>> struct xen_blkif_ring *ring = container_of(work, typeof(*ring), persistent_purge_work);
+ *   - drivers/block/xen-blkback/blkback.c|339| <<purge_persistent_gnt>> if (work_busy(&ring->persistent_purge_work)) {
+ *   - drivers/block/xen-blkback/blkback.c|402| <<purge_persistent_gnt>> schedule_work(&ring->persistent_purge_work);
+ *   - drivers/block/xen-blkback/blkback.c|642| <<xen_blkif_schedule>> flush_work(&ring->persistent_purge_work);
+ *   - drivers/block/xen-blkback/xenbus.c|146| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+ *
+ * 在以下使用xen_blkbk_unmap_purged_grants():
+ *   - drivers/block/xen-blkback/xenbus.c|176| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+ */
 void xen_blkbk_unmap_purged_grants(struct work_struct *work)
 {
 	struct gnttab_unmap_grant_ref unmap[BLKIF_MAX_SEGMENTS_PER_REQUEST];
@@ -324,6 +401,10 @@ void xen_blkbk_unmap_purged_grants(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|672| <<xen_blkif_schedule>> purge_persistent_gnt(ring);
+ */
 static void purge_persistent_gnt(struct xen_blkif_ring *ring)
 {
 	struct persistent_gnt *persistent_gnt;
@@ -406,11 +487,31 @@ static void purge_persistent_gnt(struct xen_blkif_ring *ring)
 /*
  * Retrieve from the 'pending_reqs' a free pending_req structure to be used.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1122| <<__do_block_io_op>> pending_req = alloc_req(ring);
+ *
+ * 从xen_blkif_ring->pending_free取出一个pending_req
+ * 没看到内存分配
+ * Retrieve from the 'pending_reqs' a free pending_req structure to be used.
+ */
 static struct pending_req *alloc_req(struct xen_blkif_ring *ring)
 {
 	struct pending_req *req = NULL;
 	unsigned long flags;
 
+	/*
+	 * 在以下使用xen_blkif_ring->pending_free:
+	 *   - drivers/block/xen-blkback/blkback.c|415| <<alloc_req>> if (!list_empty(&ring->pending_free)) {
+	 *   - drivers/block/xen-blkback/blkback.c|416| <<alloc_req>> req = list_entry(ring->pending_free.next, struct pending_req,
+	 *   - drivers/block/xen-blkback/blkback.c|434| <<free_req>> was_empty = list_empty(&ring->pending_free);
+	 *   - drivers/block/xen-blkback/blkback.c|435| <<free_req>> list_add(&req->free_list, &ring->pending_free);
+	 *   - drivers/block/xen-blkback/blkback.c|580| <<xen_blkif_schedule>> !list_empty(&ring->pending_free) ||
+	 *   - drivers/block/xen-blkback/xenbus.c|144| <<xen_blkif_alloc_rings>> INIT_LIST_HEAD(&ring->pending_free);
+	 *   - drivers/block/xen-blkback/xenbus.c|309| <<xen_blkif_disconnect>> list_for_each_entry_safe(req, n, &ring->pending_free, free_list) {
+	 *   - drivers/block/xen-blkback/xenbus.c|1017| <<read_per_ring_refs>> list_add_tail(&req->free_list, &ring->pending_free);
+	 *   - drivers/block/xen-blkback/xenbus.c|1041| <<read_per_ring_refs>> list_for_each_entry_safe(req, n, &ring->pending_free, free_list) {
+	 */
 	spin_lock_irqsave(&ring->pending_free_lock, flags);
 	if (!list_empty(&ring->pending_free)) {
 		req = list_entry(ring->pending_free.next, struct pending_req,
@@ -425,6 +526,13 @@ static struct pending_req *alloc_req(struct xen_blkif_ring *ring)
  * Return the 'pending_req' structure back to the freepool. We also
  * wake up the thread if it was waiting for a free page.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|758| <<xen_blkbk_unmap_and_respond_callback>> free_req(ring, pending_req);
+ *   - drivers/block/xen-blkback/blkback.c|1124| <<dispatch_other_io>> free_req(ring, pending_req);
+ *   - drivers/block/xen-blkback/blkback.c|1263| <<__do_block_io_op>> free_req(ring, pending_req);
+ *   - drivers/block/xen-blkback/blkback.c|1499| <<dispatch_rw_block_io>> free_req(ring, pending_req);
+ */
 static void free_req(struct xen_blkif_ring *ring, struct pending_req *req)
 {
 	unsigned long flags;
@@ -441,6 +549,11 @@ static void free_req(struct xen_blkif_ring *ring, struct pending_req *req)
 /*
  * Routines for managing virtual block devices (vbds).
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1040| <<dispatch_discard_io>> err = xen_vbd_translate(&preq, blkif, REQ_OP_WRITE);
+ *   - drivers/block/xen-blkback/blkback.c|1354| <<dispatch_rw_block_io>> if (xen_vbd_translate(&preq, ring->blkif, operation) != 0) {
+ */
 static int xen_vbd_translate(struct phys_req *req, struct xen_blkif *blkif,
 			     enum req_op operation)
 {
@@ -467,6 +580,10 @@ static int xen_vbd_translate(struct phys_req *req, struct xen_blkif *blkif,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|634| <<xen_blkif_schedule>> xen_vbd_resize(blkif);
+ */
 static void xen_vbd_resize(struct xen_blkif *blkif)
 {
 	struct xen_vbd *vbd = &blkif->vbd;
@@ -515,12 +632,32 @@ static void xen_vbd_resize(struct xen_blkif *blkif)
 /*
  * Notification from the guest OS.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|587| <<xen_blkif_be_int>> blkif_notify_work(dev_id);
+ */
 static void blkif_notify_work(struct xen_blkif_ring *ring)
 {
+	/*
+	 * 在以下使用xen_blkif_ring->waiting_reqs:
+	 *   - drivers/block/xen-blkback/blkback.c|623| <<blkif_notify_work>> ring->waiting_reqs = 1;
+	 *   - drivers/block/xen-blkback/blkback.c|686| <<xen_blkif_schedule>> ring->waiting_reqs || kthread_should_stop(),
+	 *   - drivers/block/xen-blkback/blkback.c|698| <<xen_blkif_schedule>> do_eoi = ring->waiting_reqs;
+	 *   - drivers/block/xen-blkback/blkback.c|700| <<xen_blkif_schedule>> ring->waiting_reqs = 0;
+	 *   - drivers/block/xen-blkback/blkback.c|705| <<xen_blkif_schedule>> ring->waiting_reqs = 1;
+	 *   - drivers/block/xen-blkback/blkback.c|710| <<xen_blkif_schedule>> if (do_eoi && !ring->waiting_reqs) {
+	 */
 	ring->waiting_reqs = 1;
+	/*
+	 * xen_blkif_schedule()
+	 */
 	wake_up(&ring->wq);
 }
 
+/*
+ * 在以下使用xen_blkif_be_int():
+ *   - drivers/block/xen-blkback/xenbus.c|284| <<xen_blkif_map>> err = bind_interdomain_evtchn_to_irqhandler_lateeoi(blkif->be->dev, evtchn, xen_blkif_be_int, 0, "blkif-backend", ring);
+ */
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id)
 {
 	blkif_notify_work(dev_id);
@@ -531,6 +668,11 @@ irqreturn_t xen_blkif_be_int(int irq, void *dev_id)
  * SCHEDULER FUNCTIONS
  */
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|638| <<xen_blkif_schedule>> print_stats(ring);
+ *   - drivers/block/xen-blkback/blkback.c|645| <<xen_blkif_schedule>> print_stats(ring);
+ */
 static void print_stats(struct xen_blkif_ring *ring)
 {
 	pr_info("(%s): oo %3llu  |  rd %4llu  |  wr %4llu  |  f %4llu"
@@ -546,6 +688,10 @@ static void print_stats(struct xen_blkif_ring *ring)
 	ring->st_ds_req = 0;
 }
 
+/*
+ * 在以下使用xen_blkif_schedule():
+ *   - drivers/block/xen-blkback/xenbus.c|111| <<xen_update_blkif_status>> ring->xenblkd = kthread_run(xen_blkif_schedule, ring, "%s-%d", name, i);
+ */
 int xen_blkif_schedule(void *arg)
 {
 	struct xen_blkif_ring *ring = arg;
@@ -579,6 +725,15 @@ int xen_blkif_schedule(void *arg)
 		if (timeout == 0)
 			goto purge_gnt_list;
 
+		/*
+		 * 在以下使用xen_blkif_ring->waiting_reqs:
+		 *   - drivers/block/xen-blkback/blkback.c|623| <<blkif_notify_work>> ring->waiting_reqs = 1;
+		 *   - drivers/block/xen-blkback/blkback.c|686| <<xen_blkif_schedule>> ring->waiting_reqs || kthread_should_stop(),
+		 *   - drivers/block/xen-blkback/blkback.c|698| <<xen_blkif_schedule>> do_eoi = ring->waiting_reqs;
+		 *   - drivers/block/xen-blkback/blkback.c|700| <<xen_blkif_schedule>> ring->waiting_reqs = 0;
+		 *   - drivers/block/xen-blkback/blkback.c|705| <<xen_blkif_schedule>> ring->waiting_reqs = 1;
+		 *   - drivers/block/xen-blkback/blkback.c|710| <<xen_blkif_schedule>> if (do_eoi && !ring->waiting_reqs) {
+		 */
 		do_eoi = ring->waiting_reqs;
 
 		ring->waiting_reqs = 0;
@@ -628,6 +783,10 @@ int xen_blkif_schedule(void *arg)
 /*
  * Remove persistent grants and empty the pool of free pages
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/xenbus.c|336| <<xen_blkif_disconnect>> xen_blkbk_free_caches(ring);
+ */
 void xen_blkbk_free_caches(struct xen_blkif_ring *ring)
 {
 	/* Free all persistent grant pages */
@@ -642,6 +801,11 @@ void xen_blkbk_free_caches(struct xen_blkif_ring *ring)
 	gnttab_page_cache_shrink(&ring->free_pages, 0 /* All */);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|784| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+ *   - drivers/block/xen-blkback/blkback.c|817| <<xen_blkbk_unmap>> invcount = xen_blkbk_unmap_prepare(ring, pages, batch,
+ */
 static unsigned int xen_blkbk_unmap_prepare(
 	struct xen_blkif_ring *ring,
 	struct grant_page **pages,
@@ -668,6 +832,10 @@ static unsigned int xen_blkbk_unmap_prepare(
 	return invcount;
 }
 
+/*
+ * 在以下使用xen_blkbk_unmap_and_respond_callback():
+ *   - drivers/block/xen-blkback/blkback.c|788| <<xen_blkbk_unmap_and_respond>> work->done = xen_blkbk_unmap_and_respond_callback;
+ */
 static void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_queue_data *data)
 {
 	struct pending_req *pending_req = (struct pending_req *)(data->data);
@@ -679,6 +847,13 @@ static void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_
 	BUG_ON(result);
 
 	gnttab_page_cache_put(&ring->free_pages, data->pages, data->count);
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|756| <<xen_blkbk_unmap_and_respond_callback>> make_response(ring, pending_req->id,
+	 *   - drivers/block/xen-blkback/blkback.c|1115| <<dispatch_discard_io>> make_response(ring, req->u.discard.id, req->operation, status);
+	 *   - drivers/block/xen-blkback/blkback.c|1125| <<dispatch_other_io>> make_response(ring, req->u.other.id, req->operation,
+	 *   - drivers/block/xen-blkback/blkback.c|1498| <<dispatch_rw_block_io>> make_response(ring, req->u.rw.id, req_operation, BLKIF_RSP_ERROR);
+	 */
 	make_response(ring, pending_req->id,
 		      pending_req->operation, pending_req->status);
 	free_req(ring, pending_req);
@@ -700,6 +875,10 @@ static void xen_blkbk_unmap_and_respond_callback(int result, struct gntab_unmap_
 	xen_blkif_put(blkif);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1173| <<__end_block_io_op>> xen_blkbk_unmap_and_respond(pending_req);
+ */
 static void xen_blkbk_unmap_and_respond(struct pending_req *req)
 {
 	struct gntab_unmap_queue_data* work = &req->gnttab_unmap_data;
@@ -707,6 +886,16 @@ static void xen_blkbk_unmap_and_respond(struct pending_req *req)
 	struct grant_page **pages = req->segments;
 	unsigned int invcount;
 
+	/*
+	 * struct grant_page **pages = req->segments;
+	 *
+	 * 在以下使用pending_req->nr_segs:
+	 *   - drivers/block/xen-blkback/blkback.c|868| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+	 *   - drivers/block/xen-blkback/blkback.c|1106| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments, pending_req->nr_segs, (pending_req->operation != BLKIF_OP_READ));
+	 *   - drivers/block/xen-blkback/blkback.c|1126| <<xen_blkbk_parse_indirect>> nseg = pending_req->nr_segs;
+	 *   - drivers/block/xen-blkback/blkback.c|1513| <<dispatch_rw_block_io>> pending_req->nr_segs = nseg;
+	 *   - drivers/block/xen-blkback/blkback.c|1640| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments, pending_req->nr_segs);
+	 */
 	invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
 					   req->unmap, req->unmap_pages);
 
@@ -728,6 +917,11 @@ static void xen_blkbk_unmap_and_respond(struct pending_req *req)
  * of hypercalls, but since this is only used in error paths there's
  * no real need.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1072| <<xen_blkbk_parse_indirect>> xen_blkbk_unmap(ring, pages, indirect_grefs);
+ *   - drivers/block/xen-blkback/blkback.c|1494| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments,
+ */
 static void xen_blkbk_unmap(struct xen_blkif_ring *ring,
                             struct grant_page *pages[],
                             int num)
@@ -753,6 +947,11 @@ static void xen_blkbk_unmap(struct xen_blkif_ring *ring,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|910| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments,
+ *   - drivers/block/xen-blkback/blkback.c|934| <<xen_blkbk_parse_indirect>> rc = xen_blkbk_map(ring, pages, indirect_grefs, true);
+ */
 static int xen_blkbk_map(struct xen_blkif_ring *ring,
 			 struct grant_page *pages[],
 			 int num, bool ro)
@@ -779,6 +978,14 @@ static int xen_blkbk_map(struct xen_blkif_ring *ring,
 	for (i = map_until; i < num; i++) {
 		uint32_t flags;
 
+		/*
+		 * struct grant_page {
+		 *     struct page             *page;
+		 *     struct persistent_gnt   *persistent_gnt;
+		 *     grant_handle_t          handle;
+		 *     grant_ref_t             gref;
+		 * };
+		 */
 		if (use_persistent_gnts) {
 			persistent_gnt = get_persistent_gnt(
 				ring,
@@ -899,10 +1106,33 @@ static int xen_blkbk_map(struct xen_blkif_ring *ring,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1330| <<dispatch_rw_block_io>> if (xen_blkbk_map_seg(pending_req))
+ */
 static int xen_blkbk_map_seg(struct pending_req *pending_req)
 {
 	int rc;
 
+	/*
+	 * struct pending_req {
+	 *     struct xen_blkif_ring   *ring;
+	 *     u64                     id;
+	 *     int                     nr_segs;
+	 *     atomic_t                pendcnt;
+	 *     unsigned short          operation;
+	 *     int                     status;
+	 *     struct list_head        free_list;
+	 *     struct grant_page       *segments[MAX_INDIRECT_SEGMENTS];
+	 *     // Indirect descriptors
+	 *     struct grant_page       *indirect_pages[MAX_INDIRECT_PAGES];
+	 *     struct seg_buf          seg[MAX_INDIRECT_SEGMENTS];
+	 *     struct bio              *biolist[MAX_INDIRECT_SEGMENTS];
+	 *     struct gnttab_unmap_grant_ref unmap[MAX_INDIRECT_SEGMENTS];
+	 *     struct page                   *unmap_pages[MAX_INDIRECT_SEGMENTS];
+	 *     struct gntab_unmap_queue_data gnttab_unmap_data;
+	 * };
+	 */
 	rc = xen_blkbk_map(pending_req->ring, pending_req->segments,
 			   pending_req->nr_segs,
 	                   (pending_req->operation != BLKIF_OP_READ));
@@ -910,34 +1140,79 @@ static int xen_blkbk_map_seg(struct pending_req *pending_req)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1400| <<dispatch_rw_block_io>> if (xen_blkbk_parse_indirect(req, pending_req, seg, &preq))
+ */
 static int xen_blkbk_parse_indirect(struct blkif_request *req,
 				    struct pending_req *pending_req,
 				    struct seg_buf seg[],
 				    struct phys_req *preq)
 {
+	/*
+	 * struct pending_req:
+	 * -> struct grant_page       *segments[MAX_INDIRECT_SEGMENTS];
+	 * -> struct grant_page       *indirect_pages[MAX_INDIRECT_PAGES];
+	 * -> struct seg_buf          seg[MAX_INDIRECT_SEGMENTS];
+	 */
 	struct grant_page **pages = pending_req->indirect_pages;
 	struct xen_blkif_ring *ring = pending_req->ring;
 	int indirect_grefs, rc, n, nseg, i;
 	struct blkif_request_segment *segments = NULL;
 
+	/*
+	 * 如果是普通的     --> nseg来自 req->u.rw.nr_segments
+	 * 如果是indirect的 --> nseg来自 req->u.indirect.nr_segments
+	 */
 	nseg = pending_req->nr_segs;
 	indirect_grefs = INDIRECT_PAGES(nseg);
 	BUG_ON(indirect_grefs > BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST);
 
+	/*
+	 * struct pending_req:
+	 * -> struct grant_page       *segments[MAX_INDIRECT_SEGMENTS];
+	 * -> struct grant_page       *indirect_pages[MAX_INDIRECT_PAGES];
+	 * -> struct seg_buf          seg[MAX_INDIRECT_SEGMENTS];
+	 *
+	 * struct grant_page **pages = pending_req->indirect_pages;
+	 */
 	for (i = 0; i < indirect_grefs; i++)
 		pages[i]->gref = req->u.indirect.indirect_grefs[i];
 
+	/*
+	 * dispatch_rw_block_io()
+	 * -> xen_blkbk_map_seg()
+	 *    -> xen_blkbk_map()
+	 *
+	 * dispatch_rw_block_io()
+	 * -> xen_blkbk_parse_indirect()
+	 *    -> xen_blkbk_map()
+	 *
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|910| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments,
+	 *   - drivers/block/xen-blkback/blkback.c|934| <<xen_blkbk_parse_indirect>> rc = xen_blkbk_map(ring, pages, indirect_grefs, true);
+	 */
 	rc = xen_blkbk_map(ring, pages, indirect_grefs, true);
 	if (rc)
 		goto unmap;
 
+	/*
+	 * 如果是普通的     --> nseg来自 req->u.rw.nr_segments
+	 * 如果是indirect的 --> nseg来自 req->u.indirect.nr_segments
+	 */
 	for (n = 0; n < nseg; n++) {
 		uint8_t first_sect, last_sect;
 
 		if ((n % SEGS_PER_INDIRECT_FRAME) == 0) {
+			/*
+			 * struct blkif_request_segment *segments = NULL;
+			 */
 			/* Map indirect segments */
 			if (segments)
 				kunmap_atomic(segments);
+			/*
+			 * struct grant_page **pages = pending_req->indirect_pages;
+			 */
 			segments = kmap_atomic(pages[n/SEGS_PER_INDIRECT_FRAME]->page);
 		}
 		i = n % SEGS_PER_INDIRECT_FRAME;
@@ -953,16 +1228,32 @@ static int xen_blkbk_parse_indirect(struct blkif_request *req,
 
 		seg[n].nsec = last_sect - first_sect + 1;
 		seg[n].offset = first_sect << 9;
+		/*
+		 * 在以下设置phys_req->nr_sects:
+		 *   - drivers/block/xen-blkback/blkback.c|1159| <<xen_blkbk_parse_indirect>> preq->nr_sects += seg[n].nsec;
+		 *   - drivers/block/xen-blkback/blkback.c|1185| <<dispatch_discard_io>> preq.nr_sects = req->u.discard.nr_sectors;
+		 *   - drivers/block/xen-blkback/blkback.c|1507| <<dispatch_rw_block_io>> preq.nr_sects = 0;
+		 *   - drivers/block/xen-blkback/blkback.c|1548| <<dispatch_rw_block_io>> preq.nr_sects += seg[i].nsec;
+		 */
 		preq->nr_sects += seg[n].nsec;
 	}
 
 unmap:
 	if (segments)
 		kunmap_atomic(segments);
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkback/blkback.c|1072| <<xen_blkbk_parse_indirect>> xen_blkbk_unmap(ring, pages, indirect_grefs);
+	 *   - drivers/block/xen-blkback/blkback.c|1494| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments,
+	 */
 	xen_blkbk_unmap(ring, pages, indirect_grefs);
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1264| <<__do_block_io_op>> if (dispatch_discard_io(ring, &req))
+ */
 static int dispatch_discard_io(struct xen_blkif_ring *ring,
 				struct blkif_request *req)
 {
@@ -1007,6 +1298,10 @@ static int dispatch_discard_io(struct xen_blkif_ring *ring,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1268| <<__do_block_io_op>> if (dispatch_other_io(ring, &req, pending_req))
+ */
 static int dispatch_other_io(struct xen_blkif_ring *ring,
 			     struct blkif_request *req,
 			     struct pending_req *pending_req)
@@ -1017,6 +1312,10 @@ static int dispatch_other_io(struct xen_blkif_ring *ring,
 	return -EIO;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1430| <<dispatch_rw_block_io>> xen_blk_drain_io(pending_req->ring);
+ */
 static void xen_blk_drain_io(struct xen_blkif_ring *ring)
 {
 	struct xen_blkif *blkif = ring->blkif;
@@ -1034,6 +1333,10 @@ static void xen_blk_drain_io(struct xen_blkif_ring *ring)
 	atomic_set(&blkif->drain, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1181| <<end_block_io_op>> __end_block_io_op(bio->bi_private, bio->bi_status);
+ */
 static void __end_block_io_op(struct pending_req *pending_req,
 		blk_status_t error)
 {
@@ -1059,6 +1362,11 @@ static void __end_block_io_op(struct pending_req *pending_req,
 	 * the grant references associated with 'request' and provide
 	 * the proper response on the ring.
 	 */
+	/*
+	 * 在以下使用pending_req->pendcnt:
+	 *   - drivers/block/xen-blkback/blkback.c|1256| <<__end_block_io_op>> if (atomic_dec_and_test(&pending_req->pendcnt))
+	 *   - drivers/block/xen-blkback/blkback.c|1566| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, nbio);
+	 */
 	if (atomic_dec_and_test(&pending_req->pendcnt))
 		xen_blkbk_unmap_and_respond(pending_req);
 }
@@ -1066,6 +1374,11 @@ static void __end_block_io_op(struct pending_req *pending_req,
 /*
  * bio callback.
  */
+/*
+ * 在以下使用end_block_io_op():
+ *   - drivers/block/xen-blkback/blkback.c|1459| <<dispatch_rw_block_io>> bio->bi_end_io = end_block_io_op;
+ *   - drivers/block/xen-blkback/blkback.c|1474| <<dispatch_rw_block_io>> bio->bi_end_io = end_block_io_op;
+ */
 static void end_block_io_op(struct bio *bio)
 {
 	__end_block_io_op(bio->bi_private, bio->bi_status);
@@ -1079,6 +1392,10 @@ static void end_block_io_op(struct bio *bio)
  * (which has the sectors we want, number of them, grant references, etc),
  * and transmute  it to the block API to hand it over to the proper block disk.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1173| <<do_block_io_op>> more_to_do = __do_block_io_op(ring, eoi_flags);
+ */
 static int
 __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 {
@@ -1111,6 +1428,11 @@ __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 			break;
 		}
 
+		/*
+		 * 从xen_blkif_ring->pending_free取出一个pending_req
+		 * 没看到内存分配
+		 * Retrieve from the 'pending_reqs' a free pending_req structure to be used.
+		 */
 		pending_req = alloc_req(ring);
 		if (NULL == pending_req) {
 			ring->st_oo_req++;
@@ -1118,6 +1440,9 @@ __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 			break;
 		}
 
+		/*
+		 * 拷贝struct blkif_request req;
+		 */
 		switch (ring->blkif->blk_protocol) {
 		case BLKIF_PROTOCOL_NATIVE:
 			memcpy(&req, RING_GET_REQUEST(&blk_rings->native, rc), sizeof(req));
@@ -1163,6 +1488,10 @@ __do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 	return more_to_do;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|587| <<xen_blkif_schedule>> ret = do_block_io_op(ring, &eoi_flags);
+ */
 static int
 do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
 {
@@ -1183,11 +1512,23 @@ do_block_io_op(struct xen_blkif_ring *ring, unsigned int *eoi_flags)
  * Transmutation of the 'struct blkif_request' to a proper 'struct bio'
  * and call the 'submit_bio' to pass it to the underlying storage.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1145| <<__do_block_io_op>> if (dispatch_rw_block_io(ring, &req, pending_req))
+ *
+ * 参数blkif_request是从ring buffer拷贝来的
+ */
 static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 				struct blkif_request *req,
 				struct pending_req *pending_req)
 {
 	struct phys_req preq;
+	/*
+	 * struct seg_buf {
+	 *     unsigned long offset;
+	 *     unsigned int nsec;
+	 * };
+	 */
 	struct seg_buf *seg = pending_req->seg;
 	unsigned int nseg;
 	struct bio *bio = NULL;
@@ -1218,6 +1559,9 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 	case BLKIF_OP_WRITE:
 		ring->st_wr_req++;
 		operation = REQ_OP_WRITE;
+		/*
+		 * __REQ_IDLE,             // anticipate more IO after this one
+		 */
 		operation_flags = REQ_SYNC | REQ_IDLE;
 		break;
 	case BLKIF_OP_WRITE_BARRIER:
@@ -1234,6 +1578,10 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 		break;
 	}
 
+	/*
+	 * 如果是普通的     --> 返回 req->u.rw.nr_segments
+	 * 如果是indirect的 --> 返回 req->u.indirect.nr_segments
+	 */
 	/* Check that the number of segments is sane. */
 	nseg = req->operation == BLKIF_OP_INDIRECT ?
 	       req->u.indirect.nr_segments : req->u.rw.nr_segments;
@@ -1248,19 +1596,64 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 		goto fail_response;
 	}
 
+	/*
+	 * 在stack上声明的struct phys_req preq;
+	 *
+	 * struct phys_req {
+	 *     unsigned short          dev;
+	 *     blkif_sector_t          nr_sects;
+	 *     struct block_device     *bdev;
+	 *     blkif_sector_t          sector_number;
+	 * };
+	 *
+	 * 在以下设置phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|1159| <<xen_blkbk_parse_indirect>> preq->nr_sects += seg[n].nsec;
+	 *   - drivers/block/xen-blkback/blkback.c|1185| <<dispatch_discard_io>> preq.nr_sects = req->u.discard.nr_sectors;
+	 *   - drivers/block/xen-blkback/blkback.c|1507| <<dispatch_rw_block_io>> preq.nr_sects = 0;
+	 *   - drivers/block/xen-blkback/blkback.c|1548| <<dispatch_rw_block_io>> preq.nr_sects += seg[i].nsec;
+	 */
 	preq.nr_sects      = 0;
 
 	pending_req->ring      = ring;
 	pending_req->id        = req->u.rw.id;
 	pending_req->operation = req_operation;
 	pending_req->status    = BLKIF_RSP_OKAY;
+	/*
+	 * 在以下使用pending_req->nr_segs:
+	 *   - drivers/block/xen-blkback/blkback.c|868| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+	 *   - drivers/block/xen-blkback/blkback.c|1106| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments, pending_req->nr_segs, (pending_req->operation != BLKIF_OP_READ));
+	 *   - drivers/block/xen-blkback/blkback.c|1126| <<xen_blkbk_parse_indirect>> nseg = pending_req->nr_segs;
+	 *   - drivers/block/xen-blkback/blkback.c|1513| <<dispatch_rw_block_io>> pending_req->nr_segs = nseg;
+	 *   - drivers/block/xen-blkback/blkback.c|1640| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments, pending_req->nr_segs);
+	 *
+	 * 如果是普通的     --> nseg来自 req->u.rw.nr_segments
+	 * 如果是indirect的 --> nseg来自 req->u.indirect.nr_segments
+	 */
 	pending_req->nr_segs   = nseg;
 
 	if (req->operation != BLKIF_OP_INDIRECT) {
 		preq.dev               = req->u.rw.handle;
 		preq.sector_number     = req->u.rw.sector_number;
 		for (i = 0; i < nseg; i++) {
+			/*
+			 * struct pending_req:
+			 * -> struct grant_page       *segments[MAX_INDIRECT_SEGMENTS];
+			 * -> struct grant_page       *indirect_pages[MAX_INDIRECT_PAGES];
+			 * -> struct seg_buf          seg[MAX_INDIRECT_SEGMENTS];
+			 *
+			 * struct grant_page **pages = pending_req->segments;
+			 *
+			 * struct grant_page {
+			 *     struct page             *page;
+			 *     struct persistent_gnt   *persistent_gnt;
+			 *     grant_handle_t          handle;
+			 *     grant_ref_t             gref;
+			 * };
+			 */
 			pages[i]->gref = req->u.rw.seg[i].gref;
+			/*
+			 * struct seg_buf *seg = pending_req->seg;
+			 */
 			seg[i].nsec = req->u.rw.seg[i].last_sect -
 				req->u.rw.seg[i].first_sect + 1;
 			seg[i].offset = (req->u.rw.seg[i].first_sect << 9);
@@ -1286,6 +1679,10 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 		goto fail_response;
 	}
 
+	/*
+	 * 如果是普通的     --> nseg来自 req->u.rw.nr_segments
+	 * 如果是indirect的 --> nseg来自 req->u.indirect.nr_segments
+	 */
 	/*
 	 * This check _MUST_ be done after xen_vbd_translate as the preq.bdev
 	 * is set there.
@@ -1322,6 +1719,14 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 	atomic_inc(&ring->inflight);
 
 	for (i = 0; i < nseg; i++) {
+		/*
+		 * struct pending_req:
+		 * -> struct grant_page       *segments[MAX_INDIRECT_SEGMENTS];
+		 * -> struct grant_page       *indirect_pages[MAX_INDIRECT_PAGES];
+		 * -> struct seg_buf          seg[MAX_INDIRECT_SEGMENTS];
+		 *
+		 * struct grant_page **pages = pending_req->segments;
+		 */
 		while ((bio == NULL) ||
 		       (bio_add_page(bio,
 				     pages[i]->page,
@@ -1350,6 +1755,11 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 		bio->bi_end_io  = end_block_io_op;
 	}
 
+	/*
+	 * 在以下使用pending_req->pendcnt:
+	 *   - drivers/block/xen-blkback/blkback.c|1256| <<__end_block_io_op>> if (atomic_dec_and_test(&pending_req->pendcnt))
+	 *   - drivers/block/xen-blkback/blkback.c|1566| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, nbio);
+	 */
 	atomic_set(&pending_req->pendcnt, nbio);
 	blk_start_plug(&plug);
 
@@ -1382,6 +1792,13 @@ static int dispatch_rw_block_io(struct xen_blkif_ring *ring,
 /*
  * Put a response on the ring on how the operation fared.
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|756| <<xen_blkbk_unmap_and_respond_callback>> make_response(ring, pending_req->id,
+ *   - drivers/block/xen-blkback/blkback.c|1115| <<dispatch_discard_io>> make_response(ring, req->u.discard.id, req->operation, status);
+ *   - drivers/block/xen-blkback/blkback.c|1125| <<dispatch_other_io>> make_response(ring, req->u.other.id, req->operation,
+ *   - drivers/block/xen-blkback/blkback.c|1498| <<dispatch_rw_block_io>> make_response(ring, req->u.rw.id, req_operation, BLKIF_RSP_ERROR);
+ */
 static void make_response(struct xen_blkif_ring *ring, u64 id,
 			  unsigned short op, int st)
 {
diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index a28473470..4a04de1be 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -262,17 +262,56 @@ struct xen_blkif_ring {
 	bool			active;
 	/* One thread per blkif ring. */
 	struct task_struct	*xenblkd;
+	/*
+	 * 在以下使用xen_blkif_ring->waiting_reqs:
+	 *   - drivers/block/xen-blkback/blkback.c|623| <<blkif_notify_work>> ring->waiting_reqs = 1;
+	 *   - drivers/block/xen-blkback/blkback.c|686| <<xen_blkif_schedule>> ring->waiting_reqs || kthread_should_stop(),
+	 *   - drivers/block/xen-blkback/blkback.c|698| <<xen_blkif_schedule>> do_eoi = ring->waiting_reqs;
+	 *   - drivers/block/xen-blkback/blkback.c|700| <<xen_blkif_schedule>> ring->waiting_reqs = 0;
+	 *   - drivers/block/xen-blkback/blkback.c|705| <<xen_blkif_schedule>> ring->waiting_reqs = 1;
+	 *   - drivers/block/xen-blkback/blkback.c|710| <<xen_blkif_schedule>> if (do_eoi && !ring->waiting_reqs) {
+	 */
 	unsigned int		waiting_reqs;
 
 	/* List of all 'pending_req' available */
+	/*
+	 * 在以下使用xen_blkif_ring->pending_free:
+	 *   - drivers/block/xen-blkback/blkback.c|415| <<alloc_req>> if (!list_empty(&ring->pending_free)) {
+	 *   - drivers/block/xen-blkback/blkback.c|416| <<alloc_req>> req = list_entry(ring->pending_free.next, struct pending_req,
+	 *   - drivers/block/xen-blkback/blkback.c|434| <<free_req>> was_empty = list_empty(&ring->pending_free);
+	 *   - drivers/block/xen-blkback/blkback.c|435| <<free_req>> list_add(&req->free_list, &ring->pending_free);
+	 *   - drivers/block/xen-blkback/blkback.c|580| <<xen_blkif_schedule>> !list_empty(&ring->pending_free) ||
+	 *   - drivers/block/xen-blkback/xenbus.c|144| <<xen_blkif_alloc_rings>> INIT_LIST_HEAD(&ring->pending_free);
+	 *   - drivers/block/xen-blkback/xenbus.c|309| <<xen_blkif_disconnect>> list_for_each_entry_safe(req, n, &ring->pending_free, free_list) {
+	 *   - drivers/block/xen-blkback/xenbus.c|1017| <<read_per_ring_refs>> list_add_tail(&req->free_list, &ring->pending_free);
+	 *   - drivers/block/xen-blkback/xenbus.c|1041| <<read_per_ring_refs>> list_for_each_entry_safe(req, n, &ring->pending_free, free_list) {
+	 */
 	struct list_head	pending_free;
 	/* And its spinlock. */
 	spinlock_t		pending_free_lock;
 	wait_queue_head_t	pending_free_wq;
 
 	/* Tree to store persistent grants. */
+	/*
+	 * 在以下使用xen_blkif_ring->persistent_gnts:
+	 *   - drivers/block/xen-blkback/blkback.c|181| <<add_persistent_gnt>> new = &ring->persistent_gnts.rb_node;
+	 *   - drivers/block/xen-blkback/blkback.c|199| <<add_persistent_gnt>> rb_insert_color(&(persistent_gnt->node), &ring->persistent_gnts);
+	 *   - drivers/block/xen-blkback/blkback.c|215| <<get_persistent_gnt>> node = ring->persistent_gnts.rb_node;
+	 *   - drivers/block/xen-blkback/blkback.c|368| <<purge_persistent_gnt>> root = &ring->persistent_gnts;
+	 *   - drivers/block/xen-blkback/blkback.c|658| <<xen_blkbk_free_caches>> if (!RB_EMPTY_ROOT(&ring->persistent_gnts))
+	 *   - drivers/block/xen-blkback/blkback.c|659| <<xen_blkbk_free_caches>> free_persistent_gnts(ring, &ring->persistent_gnts,
+	 *   - drivers/block/xen-blkback/blkback.c|662| <<xen_blkbk_free_caches>> BUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));
+	 *   - drivers/block/xen-blkback/xenbus.c|354| <<xen_blkif_disconnect>> BUG_ON(!RB_EMPTY_ROOT(&ring->persistent_gnts));
+	 */
 	struct rb_root		persistent_gnts;
 	unsigned int		persistent_gnt_c;
+	/*
+	 * 在以下使用xen_blkif_ring->persistent_gnt_in_use:
+	 *   - drivers/block/xen-blkback/blkback.c|230| <<add_persistent_gnt>> atomic_inc(&ring->persistent_gnt_in_use);
+	 *   - drivers/block/xen-blkback/blkback.c|269| <<get_persistent_gnt>> atomic_inc(&ring->persistent_gnt_in_use);
+	 *   - drivers/block/xen-blkback/blkback.c|287| <<put_persistent_gnt>> atomic_dec(&ring->persistent_gnt_in_use);
+	 *   - drivers/block/xen-blkback/xenbus.c|352| <<xen_blkif_disconnect>> BUG_ON(atomic_read(&ring->persistent_gnt_in_use) != 0);
+	 */
 	atomic_t		persistent_gnt_in_use;
 	unsigned long           next_lru;
 
@@ -288,6 +327,14 @@ struct xen_blkif_ring {
 
 	/* Used by the kworker that offload work from the persistent purge. */
 	struct list_head	persistent_purge_list;
+	/*
+	 * 在以下使用xen_blkif_ring->persistent_purge_work:
+	 *   - drivers/block/xen-blkback/blkback.c|295| <<xen_blkbk_unmap_purged_grants>> struct xen_blkif_ring *ring = container_of(work, typeof(*ring), persistent_purge_work);
+	 *   - drivers/block/xen-blkback/blkback.c|339| <<purge_persistent_gnt>> if (work_busy(&ring->persistent_purge_work)) {
+	 *   - drivers/block/xen-blkback/blkback.c|402| <<purge_persistent_gnt>> schedule_work(&ring->persistent_purge_work);
+	 *   - drivers/block/xen-blkback/blkback.c|642| <<xen_blkif_schedule>> flush_work(&ring->persistent_purge_work);
+	 *   - drivers/block/xen-blkback/xenbus.c|146| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+	 */
 	struct work_struct	persistent_purge_work;
 
 	/* Buffer of free pages to map grant refs. */
@@ -330,6 +377,16 @@ struct seg_buf {
 
 struct grant_page {
 	struct page 		*page;
+	/*
+	 * 在以下使用grant_page->persistent_gnt:
+	 *   - drivers/block/xen-blkback/blkback.c|819| <<xen_blkbk_unmap_prepare>> if (pages[i]->persistent_gnt != NULL) {
+	 *   - drivers/block/xen-blkback/blkback.c|820| <<xen_blkbk_unmap_prepare>> put_persistent_gnt(ring, pages[i]->persistent_gnt);
+	 *   - drivers/block/xen-blkback/blkback.c|1001| <<xen_blkbk_map>> pages[i]->persistent_gnt = persistent_gnt;
+	 *   - drivers/block/xen-blkback/blkback.c|1013| <<xen_blkbk_map>> pages[i]->persistent_gnt = NULL;
+	 *   - drivers/block/xen-blkback/blkback.c|1035| <<xen_blkbk_map>> if (!pages[seg_idx]->persistent_gnt) {
+	 *   - drivers/block/xen-blkback/blkback.c|1075| <<xen_blkbk_map>> pages[seg_idx]->persistent_gnt = persistent_gnt;
+	 *   - drivers/block/xen-blkback/blkback.c|1102| <<xen_blkbk_map>> pages[i]->persistent_gnt = NULL;
+	 */
 	struct persistent_gnt	*persistent_gnt;
 	grant_handle_t		handle;
 	grant_ref_t		gref;
@@ -344,15 +401,70 @@ struct grant_page {
 struct pending_req {
 	struct xen_blkif_ring   *ring;
 	u64			id;
+	/*
+	 * 在以下使用pending_req->nr_segs:
+	 *   - drivers/block/xen-blkback/blkback.c|868| <<xen_blkbk_unmap_and_respond>> invcount = xen_blkbk_unmap_prepare(ring, pages, req->nr_segs,
+	 *   - drivers/block/xen-blkback/blkback.c|1106| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments, pending_req->nr_segs, (pending_req->operation != BLKIF_OP_READ));
+	 *   - drivers/block/xen-blkback/blkback.c|1126| <<xen_blkbk_parse_indirect>> nseg = pending_req->nr_segs;
+	 *   - drivers/block/xen-blkback/blkback.c|1513| <<dispatch_rw_block_io>> pending_req->nr_segs = nseg;
+	 *   - drivers/block/xen-blkback/blkback.c|1640| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments, pending_req->nr_segs);
+	 */
 	int			nr_segs;
+	/*
+	 * 在以下使用pending_req->pendcnt:
+	 *   - drivers/block/xen-blkback/blkback.c|1256| <<__end_block_io_op>> if (atomic_dec_and_test(&pending_req->pendcnt))
+	 *   - drivers/block/xen-blkback/blkback.c|1566| <<dispatch_rw_block_io>> atomic_set(&pending_req->pendcnt, nbio);
+	 */
 	atomic_t		pendcnt;
 	unsigned short		operation;
 	int			status;
+	/*
+	 * 在以下使用pending_req->free_list:
+	 *   - drivers/block/xen-blkback/blkback.c|518| <<alloc_req>> req = list_entry(ring->pending_free.next, struct pending_req, free_list);
+	 *   - drivers/block/xen-blkback/blkback.c|519| <<alloc_req>> list_del(&req->free_list);
+	 *   - drivers/block/xen-blkback/blkback.c|543| <<free_req>> list_add(&req->free_list, &ring->pending_free);
+	 *   - drivers/block/xen-blkback/xenbus.c|339| <<xen_blkif_disconnect>> list_for_each_entry_safe(req, n, &ring->pending_free, free_list) {
+	 *   - drivers/block/xen-blkback/xenbus.c|340| <<xen_blkif_disconnect>> list_del(&req->free_list);
+	 *   - drivers/block/xen-blkback/xenbus.c|1051| <<read_per_ring_refs>> list_add_tail(&req->free_list, &ring->pending_free);
+	 *   - drivers/block/xen-blkback/xenbus.c|1075| <<read_per_ring_refs>> list_for_each_entry_safe(req, n, &ring->pending_free, free_list) {
+	 *   - drivers/block/xen-blkback/xenbus.c|1076| <<read_per_ring_refs>> list_del(&req->free_list);
+	 */
 	struct list_head	free_list;
+	/*
+	 * 在以下使用pending_req->segments:
+	 *   - drivers/block/xen-blkback/common.h|412| <<global>> struct grant_page *segments[MAX_INDIRECT_SEGMENTS];
+	 *   - drivers/block/xen-blkback/blkback.c|886| <<xen_blkbk_unmap_and_respond>> struct grant_page **pages = req->segments;
+	 *   - drivers/block/xen-blkback/blkback.c|1136| <<xen_blkbk_map_seg>> rc = xen_blkbk_map(pending_req->ring, pending_req->segments,
+	 *   - drivers/block/xen-blkback/blkback.c|1220| <<xen_blkbk_parse_indirect>> pending_req->segments[n]->gref = segments[i].gref;
+	 *   - drivers/block/xen-blkback/blkback.c|1541| <<dispatch_rw_block_io>> struct grant_page **pages = pending_req->segments;
+	 *   - drivers/block/xen-blkback/blkback.c|1780| <<dispatch_rw_block_io>> xen_blkbk_unmap(ring, pending_req->segments,
+	 *   - drivers/block/xen-blkback/xenbus.c|343| <<xen_blkif_disconnect>> kfree(req->segments[j]);
+	 *   - drivers/block/xen-blkback/xenbus.c|1053| <<read_per_ring_refs>> req->segments[j] = kzalloc(sizeof(*req->segments[0]), GFP_KERNEL);
+	 *   - drivers/block/xen-blkback/xenbus.c|1054| <<read_per_ring_refs>> if (!req->segments[j])
+	 *   - drivers/block/xen-blkback/xenbus.c|1078| <<read_per_ring_refs>> if (!req->segments[j])
+	 *   - drivers/block/xen-blkback/xenbus.c|1080| <<read_per_ring_refs>> kfree(req->segments[j]);
+	 */
 	struct grant_page	*segments[MAX_INDIRECT_SEGMENTS];
 	/* Indirect descriptors */
+	/*
+	 * 在以下使用pending_req->indirect_pages:
+	 *   - drivers/block/xen-blkback/common.h|414| <<global>> struct grant_page *indirect_pages[MAX_INDIRECT_PAGES];
+	 *   - drivers/block/xen-blkback/blkback.c|1158| <<xen_blkbk_parse_indirect>> struct grant_page **pages = pending_req->indirect_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|346| <<xen_blkif_disconnect>> kfree(req->indirect_pages[j]);
+	 *   - drivers/block/xen-blkback/xenbus.c|1058| <<read_per_ring_refs>> req->indirect_pages[j] = kzalloc(sizeof(*req->indirect_pages[0]),
+	 *   - drivers/block/xen-blkback/xenbus.c|1060| <<read_per_ring_refs>> if (!req->indirect_pages[j])
+	 *   - drivers/block/xen-blkback/xenbus.c|1083| <<read_per_ring_refs>> if (!req->indirect_pages[j])
+	 *   - drivers/block/xen-blkback/xenbus.c|1085| <<read_per_ring_refs>> kfree(req->indirect_pages[j]);
+	 */
 	struct grant_page	*indirect_pages[MAX_INDIRECT_PAGES];
 	struct seg_buf		seg[MAX_INDIRECT_SEGMENTS];
+	/*
+	 * 在以下使用pending_req->biolist[MAX_INDIRECT_SEGMENTS]:
+	 *   - drivers/block/xen-blkback/blkback.c|1440| <<dispatch_rw_block_io>> struct bio **biolist = pending_req->biolist;
+	 *   - drivers/block/xen-blkback/blkback.c|1602| <<dispatch_rw_block_io>> biolist[nbio++] = bio;
+	 *   - drivers/block/xen-blkback/blkback.c|1617| <<dispatch_rw_block_io>> biolist[nbio++] = bio;
+	 *   - drivers/block/xen-blkback/blkback.c|1626| <<dispatch_rw_block_io>> submit_bio(biolist[i]);
+	 */
 	struct bio		*biolist[MAX_INDIRECT_SEGMENTS];
 	struct gnttab_unmap_grant_ref unmap[MAX_INDIRECT_SEGMENTS];
 	struct page                   *unmap_pages[MAX_INDIRECT_SEGMENTS];
@@ -362,7 +474,18 @@ struct pending_req {
 
 #define vbd_sz(_v)	bdev_nr_sectors((_v)->bdev)
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1182| <<dispatch_discard_io>> xen_blkif_get(blkif);
+ *   - drivers/block/xen-blkback/blkback.c|1598| <<dispatch_rw_block_io>> xen_blkif_get(ring->blkif);
+ */
 #define xen_blkif_get(_b) (atomic_inc(&(_b)->refcnt))
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|854| <<xen_blkbk_unmap_and_respond_callback>> xen_blkif_put(blkif);
+ *   - drivers/block/xen-blkback/blkback.c|1213| <<dispatch_discard_io>> xen_blkif_put(blkif);
+ *   - drivers/block/xen-blkback/xenbus.c|582| <<xen_blkbk_remove>> xen_blkif_put(be->blkif);
+ */
 #define xen_blkif_put(_b)				\
 	do {						\
 		if (atomic_dec_and_test(&(_b)->refcnt))	\
@@ -371,6 +494,20 @@ struct pending_req {
 
 struct phys_req {
 	unsigned short		dev;
+	/*
+	 * 在以下设置phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|1159| <<xen_blkbk_parse_indirect>> preq->nr_sects += seg[n].nsec;
+	 *   - drivers/block/xen-blkback/blkback.c|1185| <<dispatch_discard_io>> preq.nr_sects = req->u.discard.nr_sectors;
+	 *   - drivers/block/xen-blkback/blkback.c|1507| <<dispatch_rw_block_io>> preq.nr_sects = 0;
+	 *   - drivers/block/xen-blkback/blkback.c|1548| <<dispatch_rw_block_io>> preq.nr_sects += seg[i].nsec;
+	 * 在以下使用phys_req->nr_sects:
+	 *   - drivers/block/xen-blkback/blkback.c|552| <<xen_vbd_translate>> if (likely(req->nr_sects)) {
+	 *   - drivers/block/xen-blkback/blkback.c|553| <<xen_vbd_translate>> blkif_sector_t end = req->sector_number + req->nr_sects;
+	 *   - drivers/block/xen-blkback/blkback.c|1191| <<dispatch_discard_io>> preq.sector_number + preq.nr_sects, blkif->vbd.pdevice);
+	 *   - drivers/block/xen-blkback/blkback.c|1561| <<dispatch_rw_block_io>> preq.sector_number + preq.nr_sects,
+	 *   - drivers/block/xen-blkback/blkback.c|1645| <<dispatch_rw_block_io>> ring->st_rd_sect += preq.nr_sects;
+	 *   - drivers/block/xen-blkback/blkback.c|1647| <<dispatch_rw_block_io>> ring->st_wr_sect += preq.nr_sects;
+	 */
 	blkif_sector_t		nr_sects;
 	struct block_device	*bdev;
 	blkif_sector_t		sector_number;
@@ -443,6 +580,10 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1193| <<__do_block_io_op>> blkif_get_x86_64_req(&req, RING_GET_REQUEST(&blk_rings->x86_64, rc));
+ */
 static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 					struct blkif_x86_64_request *src)
 {
diff --git a/drivers/block/xen-blkback/xenbus.c b/drivers/block/xen-blkback/xenbus.c
index c0227dfa4..f7fa1897e 100644
--- a/drivers/block/xen-blkback/xenbus.c
+++ b/drivers/block/xen-blkback/xenbus.c
@@ -27,6 +27,15 @@ struct backend_info {
 	char			*mode;
 };
 
+/*
+ * 在以下使用xen_blkif_cachep:
+ *   - drivers/block/xen-blkback/xenbus.c|171| <<xen_blkif_alloc>> blkif = kmem_cache_zalloc(xen_blkif_cachep, GFP_KERNEL);
+ *   - drivers/block/xen-blkback/xenbus.c|353| <<xen_blkif_free>> kmem_cache_free(xen_blkif_cachep, blkif);
+ *   - drivers/block/xen-blkback/xenbus.c|359| <<xen_blkif_interface_init>> xen_blkif_cachep = kmem_cache_create("blkif_cache",
+ *   - drivers/block/xen-blkback/xenbus.c|362| <<xen_blkif_interface_init>> if (!xen_blkif_cachep)
+ *   - drivers/block/xen-blkback/xenbus.c|370| <<xen_blkif_interface_fini>> kmem_cache_destroy(xen_blkif_cachep);
+ *   - drivers/block/xen-blkback/xenbus.c|371| <<xen_blkif_interface_fini>> xen_blkif_cachep = NULL;
+ */
 static struct kmem_cache *xen_blkif_cachep;
 static void connect(struct backend_info *);
 static int connect_ring(struct backend_info *);
@@ -44,6 +53,10 @@ struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be)
  * The last request could free the device from softirq context and
  * xen_blkif_free() can sleep.
  */
+/*
+ * 在以下使用xen_blkif_deferred_free():
+ *   - drivers/block/xen-blkback/xenbus.c|187| <<xen_blkif_alloc>> INIT_WORK(&blkif->free_work, xen_blkif_deferred_free);
+ */
 static void xen_blkif_deferred_free(struct work_struct *work)
 {
 	struct xen_blkif *blkif;
@@ -73,6 +86,11 @@ static int blkback_name(struct xen_blkif *blkif, char *buf)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/xenbus.c|782| <<backend_changed>> xen_update_blkif_status(be->blkif);
+ *   - drivers/block/xen-blkback/xenbus.c|834| <<frontend_changed>> xen_update_blkif_status(be->blkif);
+ */
 static void xen_update_blkif_status(struct xen_blkif *blkif)
 {
 	int err;
@@ -127,6 +145,10 @@ static void xen_update_blkif_status(struct xen_blkif *blkif)
 	return;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/xenbus.c|1110| <<connect_ring>> if (xen_blkif_alloc_rings(blkif))
+ */
 static int xen_blkif_alloc_rings(struct xen_blkif *blkif)
 {
 	unsigned int r;
@@ -143,6 +165,14 @@ static int xen_blkif_alloc_rings(struct xen_blkif *blkif)
 		init_waitqueue_head(&ring->wq);
 		INIT_LIST_HEAD(&ring->pending_free);
 		INIT_LIST_HEAD(&ring->persistent_purge_list);
+		/*
+		 * 在以下使用xen_blkif_ring->persistent_purge_work:
+		 *   - drivers/block/xen-blkback/blkback.c|295| <<xen_blkbk_unmap_purged_grants>> struct xen_blkif_ring *ring = container_of(work, typeof(*ring), persistent_purge_work);
+		 *   - drivers/block/xen-blkback/blkback.c|339| <<purge_persistent_gnt>> if (work_busy(&ring->persistent_purge_work)) {
+		 *   - drivers/block/xen-blkback/blkback.c|402| <<purge_persistent_gnt>> schedule_work(&ring->persistent_purge_work);
+		 *   - drivers/block/xen-blkback/blkback.c|642| <<xen_blkif_schedule>> flush_work(&ring->persistent_purge_work);
+		 *   - drivers/block/xen-blkback/xenbus.c|146| <<xen_blkif_alloc_rings>> INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
+		 */
 		INIT_WORK(&ring->persistent_purge_work, xen_blkbk_unmap_purged_grants);
 		gnttab_page_cache_init(&ring->free_pages);
 
@@ -342,6 +372,10 @@ static int xen_blkif_disconnect(struct xen_blkif *blkif)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/xenbus.c|52| <<xen_blkif_deferred_free>> xen_blkif_free(blkif);
+ */
 static void xen_blkif_free(struct xen_blkif *blkif)
 {
 	WARN_ON(xen_blkif_disconnect(blkif));
@@ -1169,6 +1203,10 @@ static struct xenbus_driver xen_blkbk_driver = {
 	.reclaim_memory = reclaim_memory,
 };
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/blkback.c|1518| <<xen_blkif_init>> rc = xen_blkif_xenbus_init();
+ */
 int xen_blkif_xenbus_init(void)
 {
 	return xenbus_register_backend(&xen_blkbk_driver);
diff --git a/drivers/iommu/amd/iommu.c b/drivers/iommu/amd/iommu.c
index 828672a46..7c166d4db 100644
--- a/drivers/iommu/amd/iommu.c
+++ b/drivers/iommu/amd/iommu.c
@@ -2997,6 +2997,12 @@ static int modify_irte_ga(struct amd_iommu *iommu, u16 devid, int index,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/amd/iommu.c|3072| <<irte_activate>> modify_irte(iommu, devid, index, irte);
+ *   - drivers/iommu/amd/iommu.c|3088| <<irte_deactivate>> modify_irte(iommu, devid, index, irte);
+ *   - drivers/iommu/amd/iommu.c|3106| <<irte_set_affinity>> modify_irte(iommu, devid, index, irte);
+ */
 static int modify_irte(struct amd_iommu *iommu,
 		       u16 devid, int index, union irte *irte)
 {
@@ -3645,6 +3651,10 @@ int amd_iommu_create_irq_domain(struct amd_iommu *iommu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|1053| <<avic_update_iommu_vcpu_affinity>> ret = amd_iommu_update_ga(cpu, r, ir->data);
+ */
 int amd_iommu_update_ga(int cpu, bool is_run, void *data)
 {
 	unsigned long flags;
diff --git a/drivers/iommu/intel/irq_remapping.c b/drivers/iommu/intel/irq_remapping.c
index 2e9683e97..c16750e88 100644
--- a/drivers/iommu/intel/irq_remapping.c
+++ b/drivers/iommu/intel/irq_remapping.c
@@ -155,6 +155,13 @@ static int qi_flush_iec(struct intel_iommu *iommu, int index, int mask)
 	return qi_submit_sync(iommu, &desc, 1, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1157| <<intel_ir_reconfigure_irte>> modify_irte(&ir_data->irq_2_iommu, irte);
+ *   - drivers/iommu/intel/irq_remapping.c|1212| <<intel_ir_set_vcpu_affinity>> modify_irte(&ir_data->irq_2_iommu, &ir_data->irte_entry);
+ *   - drivers/iommu/intel/irq_remapping.c|1235| <<intel_ir_set_vcpu_affinity>> modify_irte(&ir_data->irq_2_iommu, &irte_pi);
+ *   - drivers/iommu/intel/irq_remapping.c|1421| <<intel_irq_remapping_deactivate>> modify_irte(&data->irq_2_iommu, &entry);
+ */
 static int modify_irte(struct irq_2_iommu *irq_iommu,
 		       struct irte *irte_modified)
 {
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index 262658fd5..4972acdcd 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -86,6 +86,12 @@ static DEFINE_STATIC_KEY_TRUE(supports_deactivate_key);
  * - Figure 4-7 Secure read of the priority field for a Non-secure Group 1
  *   interrupt.
  */
+/*
+ * 在以下使用supports_pseudo_nmis:
+ *   - drivers/irqchip/irq-gic-v3.c|89| <<global>> static DEFINE_STATIC_KEY_FALSE(supports_pseudo_nmis);
+ *   - drivers/irqchip/irq-gic-v3.c|401| <<gic_supports_nmi>> static_branch_likely(&supports_pseudo_nmis);
+ *   - drivers/irqchip/irq-gic-v3.c|1811| <<gic_enable_nmi_support>> static_branch_enable(&supports_pseudo_nmis);
+ */
 static DEFINE_STATIC_KEY_FALSE(supports_pseudo_nmis);
 
 /*
@@ -395,6 +401,14 @@ static void gic_unmask_irq(struct irq_data *d)
 	gic_poke_irq(d, GICD_ISENABLER);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|494| <<gic_irq_nmi_setup>> if (!gic_supports_nmi())
+ *   - drivers/irqchip/irq-gic-v3.c|531| <<gic_irq_nmi_teardown>> if (WARN_ON(!gic_supports_nmi()))
+ *   - drivers/irqchip/irq-gic-v3.c|677| <<gic_rpr_is_nmi_prio>> if (!gic_supports_nmi())
+ *   - drivers/irqchip/irq-gic-v3.c|787| <<gic_handle_irq>> if (unlikely(gic_supports_nmi() && !interrupts_enabled(regs)))
+ *   - drivers/irqchip/irq-gic-v3.c|1075| <<gic_cpu_sys_reg_init>> } else if (gic_supports_nmi()) {
+ */
 static inline bool gic_supports_nmi(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_PSEUDO_NMI) &&
@@ -698,6 +712,11 @@ static void __gic_handle_irq(u32 irqnr, struct pt_regs *regs)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|736| <<__gic_handle_irq_from_irqson>> __gic_handle_nmi(irqnr, regs);
+ *   - drivers/irqchip/irq-gic-v3.c|782| <<__gic_handle_irq_from_irqsoff>> __gic_handle_nmi(irqnr, regs);
+ */
 static void __gic_handle_nmi(u32 irqnr, struct pt_regs *regs)
 {
 	if (gic_irqnr_is_special(irqnr))
@@ -755,6 +774,10 @@ static void __gic_handle_irq_from_irqson(struct pt_regs *regs)
  *
  * The entry code has performed NMI entry.
  */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|788| <<gic_handle_irq>> __gic_handle_irq_from_irqsoff(regs);
+ */
 static void __gic_handle_irq_from_irqsoff(struct pt_regs *regs)
 {
 	u64 pmr;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 9cce7dec7..6bc7b3777 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -214,6 +214,12 @@ struct virtnet_info {
 	u16 max_queue_pairs;
 
 	/* # of queue pairs currently used by the driver */
+	/*
+	 * 在以下设置virtnet_info->curr_queue_pairs:
+	 *   - drivers/net/virtio_net.c|2128| <<_virtnet_set_queues>> vi->curr_queue_pairs = queue_pairs;
+	 *   - drivers/net/virtio_net.c|3881| <<virtnet_probe>> vi->curr_queue_pairs = max_queue_pairs;
+	 *   - drivers/net/virtio_net.c|3883| <<virtnet_probe>> vi->curr_queue_pairs = num_online_cpus();
+	 */
 	u16 curr_queue_pairs;
 
 	/* # of XDP queue pairs currently used by the driver */
@@ -2102,6 +2108,12 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2136| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2628| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|3140| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -2128,6 +2140,11 @@ static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3916| <<virtnet_probe>> virtnet_set_queues(vi, vi->curr_queue_pairs);
+ *   - drivers/net/virtio_net.c|4028| <<virtnet_restore>> virtnet_set_queues(vi, vi->curr_queue_pairs);
+ */
 static int virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	int err;
@@ -2271,6 +2288,13 @@ static void virtnet_clean_affinity(struct virtnet_info *vi)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2334| <<virtnet_cpu_online>> virtnet_set_affinity(vi);
+ *   - drivers/net/virtio_net.c|2342| <<virtnet_cpu_dead>> virtnet_set_affinity(vi);
+ *   - drivers/net/virtio_net.c|2654| <<virtnet_set_channels>> virtnet_set_affinity(vi);
+ *   - drivers/net/virtio_net.c|3603| <<init_vqs>> virtnet_set_affinity(vi);
+ */
 static void virtnet_set_affinity(struct virtnet_info *vi)
 {
 	cpumask_var_t mask;
@@ -2601,6 +2625,10 @@ static void virtnet_get_drvinfo(struct net_device *dev,
 }
 
 /* TODO: Eliminate OOO packets during switching */
+/*
+ * 只在以下使用:
+ *   - struct ethtool_ops virtnet_ethtool_ops.set_channels = virtnet_set_channels()
+ */
 static int virtnet_set_channels(struct net_device *dev,
 				struct ethtool_channels *channels)
 {
@@ -2746,6 +2774,10 @@ static int virtnet_get_link_ksettings(struct net_device *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2819| <<virtnet_set_coalesce>> ret = virtnet_send_notf_coal_cmds(vi, ec);
+ */
 static int virtnet_send_notf_coal_cmds(struct virtnet_info *vi,
 				       struct ethtool_coalesce *ec)
 {
@@ -3961,6 +3993,29 @@ static void remove_vq_common(struct virtnet_info *vi)
 	virtnet_del_vqs(vi);
 }
 
+/*
+ * [0] virtnet_remove
+ * [0] virtio_dev_remove
+ * [0] device_release_driver_internal
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] unregister_virtio_device
+ * [0] virtio_pci_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] pci_stop_bus_device
+ * [0] pci_stop_and_remove_bus_device
+ * [0] disable_slot
+ * [0] acpiphp_disable_and_eject_slot
+ * [0] acpiphp_hotplug_notify
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void virtnet_remove(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 27a11cc08..659e32ffc 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -877,6 +877,13 @@ static int xennet_close(struct net_device *dev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|899| <<xennet_uninit>> xennet_destroy_queues(np);
+ *   - drivers/net/xen-netfront.c|2292| <<talk_to_netback>> xennet_destroy_queues(info);
+ *   - drivers/net/xen-netfront.c|2402| <<talk_to_netback>> xennet_destroy_queues(info);
+ *   - drivers/net/xen-netfront.c|2656| <<xennet_remove>> xennet_destroy_queues(info);
+ */
 static void xennet_destroy_queues(struct netfront_info *info)
 {
 	unsigned int i;
diff --git a/drivers/pci/hotplug/pciehp.h b/drivers/pci/hotplug/pciehp.h
index e0a614ace..e9ff9b118 100644
--- a/drivers/pci/hotplug/pciehp.h
+++ b/drivers/pci/hotplug/pciehp.h
@@ -97,6 +97,20 @@ struct controller {
 	unsigned int cmd_busy:1;
 	wait_queue_head_t queue;
 
+	/*
+	 * 在以下使用controller-pending_events:
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|143| <<remove_board>> atomic_and(~(PCI_EXP_SLTSTA_DLLSC | PCI_EXP_SLTSTA_PDC), &ctrl->pending_events);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|155| <<pciehp_request>> atomic_or(action, &ctrl->pending_events);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|458| <<pciehp_sysfs_enable_slot>> !atomic_read(&ctrl->pending_events) &&
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|492| <<pciehp_sysfs_disable_slot>> !atomic_read(&ctrl->pending_events) &&
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|314| <<pciehp_check_link_status>> atomic_and(~(PCI_EXP_SLTSTA_DLLSC | PCI_EXP_SLTSTA_PDC), &ctrl->pending_events);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|587| <<pciehp_ignore_dpc_link_change>> atomic_and(~PCI_EXP_SLTSTA_DLLSC, &ctrl->pending_events);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|636| <<pciehp_isr>> atomic_or(RERUN_ISR, &ctrl->pending_events);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|712| <<pciehp_isr>> atomic_or(events, &ctrl->pending_events);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|732| <<pciehp_ist>> if (atomic_fetch_and(~RERUN_ISR, &ctrl->pending_events) & RERUN_ISR) {
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|740| <<pciehp_ist>> events = atomic_xchg(&ctrl->pending_events, 0);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|798| <<pciehp_poll>> atomic_read(&ctrl->pending_events))
+	 */
 	atomic_t pending_events;		/* event handling */
 	unsigned int notification_enabled:1;
 	unsigned int power_fault_detected;
@@ -104,6 +118,17 @@ struct controller {
 
 	u8 state;				/* state machine */
 	struct mutex state_lock;
+	/*
+	 * 在以下使用controller->button_work:
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|163| <<pciehp_queue_pushbutton_work>> button_work.work);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|201| <<pciehp_handle_button_press>> schedule_delayed_work(&ctrl->button_work, 5 * HZ);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|211| <<pciehp_handle_button_press>> cancel_delayed_work(&ctrl->button_work);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|242| <<pciehp_handle_disable_request>> cancel_delayed_work(&ctrl->button_work);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|266| <<pciehp_handle_presence_or_link_change>> cancel_delayed_work(&ctrl->button_work);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|295| <<pciehp_handle_presence_or_link_change>> cancel_delayed_work(&ctrl->button_work);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|1036| <<pcie_init>> INIT_DELAYED_WORK(&ctrl->button_work, pciehp_queue_pushbutton_work);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|1094| <<pciehp_release_ctrl>> cancel_delayed_work_sync(&ctrl->button_work);
+	 */
 	struct delayed_work button_work;
 
 	struct hotplug_slot hotplug_slot;	/* hotplug core interface */
diff --git a/drivers/pci/hotplug/pciehp_core.c b/drivers/pci/hotplug/pciehp_core.c
index 4042d87d5..39d173451 100644
--- a/drivers/pci/hotplug/pciehp_core.c
+++ b/drivers/pci/hotplug/pciehp_core.c
@@ -162,6 +162,11 @@ static int get_adapter_status(struct hotplug_slot *hotplug_slot, u8 *value)
  * interrupt generation is disabled [when] interrupt generation is subsequently
  * enabled" is optional per PCIe r4.0, sec 6.7.3.4.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_core.c|230| <<pciehp_probe>> pciehp_check_presence(ctrl);
+ *   - drivers/pci/hotplug/pciehp_core.c|309| <<pciehp_resume>> pciehp_check_presence(ctrl);
+ */
 static void pciehp_check_presence(struct controller *ctrl)
 {
 	int occupied;
@@ -170,6 +175,16 @@ static void pciehp_check_presence(struct controller *ctrl)
 	mutex_lock(&ctrl->state_lock);
 
 	occupied = pciehp_card_present_or_link_active(ctrl);
+	/*
+	 * called by:
+	 *   - drivers/pci/hotplug/pciehp_core.c|177| <<pciehp_check_presence>> pciehp_request(ctrl, PCI_EXP_SLTSTA_PDC);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|182| <<pciehp_queue_pushbutton_work>> pciehp_request(ctrl, DISABLE_SLOT);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|185| <<pciehp_queue_pushbutton_work>> pciehp_request(ctrl, PCI_EXP_SLTSTA_PDC);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|460| <<pciehp_sysfs_enable_slot>> pciehp_request(ctrl, PCI_EXP_SLTSTA_PDC);
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|494| <<pciehp_sysfs_disable_slot>> pciehp_request(ctrl, DISABLE_SLOT);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|606| <<pciehp_ignore_dpc_link_change>> pciehp_request(ctrl, PCI_EXP_SLTSTA_DLLSC);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|916| <<pciehp_slot_reset>> pciehp_request(ctrl, PCI_EXP_SLTSTA_DLLSC);
+	 */
 	if ((occupied > 0 && (ctrl->state == OFF_STATE ||
 			  ctrl->state == BLINKINGON_STATE)) ||
 	    (!occupied && (ctrl->state == ON_STATE ||
diff --git a/drivers/pci/hotplug/pciehp_ctrl.c b/drivers/pci/hotplug/pciehp_ctrl.c
index 529c34808..793025cf8 100644
--- a/drivers/pci/hotplug/pciehp_ctrl.c
+++ b/drivers/pci/hotplug/pciehp_ctrl.c
@@ -56,9 +56,23 @@ static void set_slot_off(struct controller *ctrl)
  * Turns power on for the board.
  * Configures board.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/cpqphp_ctrl.c|1988| <<cpqhp_process_SI>> rc = board_added(func, ctrl);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|306| <<__pciehp_enable_slot>> return board_added(ctrl);
+ *   - drivers/pci/hotplug/shpchp_ctrl.c|229| <<board_added>> static int board_added(struct slot *p_slot)
+ *   - drivers/pci/hotplug/shpchp_ctrl.c|580| <<shpchp_enable_slot>> retval = board_added(p_slot);
+ *   - drivers/pci/hotplug/shpchp_ctrl.c|584| <<shpchp_enable_slot>> retval = board_added(p_slot);
+ */
 static int board_added(struct controller *ctrl)
 {
 	int retval = 0;
+	/*
+	 * struct controller *ctrl:
+	 * -> struct pcie_device *pcie;
+	 *    -> struct pci_dev *port;
+	 *       -> struct pci_bus  *subordinate;   // Bus this device bridges to
+	 */
 	struct pci_bus *parent = ctrl->pcie->port->subordinate;
 
 	if (POWER_CTRL(ctrl)) {
@@ -106,6 +120,10 @@ static int board_added(struct controller *ctrl)
  * @ctrl: PCIe hotplug controller where board is being removed
  * @safe_removal: whether the board is safely removed (versus surprise removed)
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|359| <<__pciehp_disable_slot>> remove_board(ctrl, safe_removal);
+ */
 static void remove_board(struct controller *ctrl, bool safe_removal)
 {
 	pciehp_unconfigure_device(ctrl, safe_removal);
@@ -132,6 +150,49 @@ static void remove_board(struct controller *ctrl, bool safe_removal)
 static int pciehp_enable_slot(struct controller *ctrl);
 static int pciehp_disable_slot(struct controller *ctrl, bool safe_removal);
 
+/*
+ * [0] pciehp_request
+ * [0] pciehp_check_presence
+ * [0] pciehp_probe
+ * [0] pcie_port_probe_service
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __device_attach_driver
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] device_initial_probe
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] device_register
+ * [0] pcie_port_device_register
+ * [0] pcie_portdrv_probe
+ * [0] local_pci_probe
+ * [0] pci_device_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] driver_attach
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] __pci_register_driver
+ * [0] pcie_portdrv_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_core.c|177| <<pciehp_check_presence>> pciehp_request(ctrl, PCI_EXP_SLTSTA_PDC);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|182| <<pciehp_queue_pushbutton_work>> pciehp_request(ctrl, DISABLE_SLOT);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|185| <<pciehp_queue_pushbutton_work>> pciehp_request(ctrl, PCI_EXP_SLTSTA_PDC);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|460| <<pciehp_sysfs_enable_slot>> pciehp_request(ctrl, PCI_EXP_SLTSTA_PDC);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|494| <<pciehp_sysfs_disable_slot>> pciehp_request(ctrl, DISABLE_SLOT);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|606| <<pciehp_ignore_dpc_link_change>> pciehp_request(ctrl, PCI_EXP_SLTSTA_DLLSC);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|916| <<pciehp_slot_reset>> pciehp_request(ctrl, PCI_EXP_SLTSTA_DLLSC);
+ */
 void pciehp_request(struct controller *ctrl, int action)
 {
 	atomic_or(action, &ctrl->pending_events);
@@ -139,6 +200,20 @@ void pciehp_request(struct controller *ctrl, int action)
 		irq_wake_thread(ctrl->pcie->irq, ctrl);
 }
 
+/*
+ * 在以下使用controller->button_work:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|163| <<pciehp_queue_pushbutton_work>> button_work.work);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|201| <<pciehp_handle_button_press>> schedule_delayed_work(&ctrl->button_work, 5 * HZ);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|211| <<pciehp_handle_button_press>> cancel_delayed_work(&ctrl->button_work);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|242| <<pciehp_handle_disable_request>> cancel_delayed_work(&ctrl->button_work);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|266| <<pciehp_handle_presence_or_link_change>> cancel_delayed_work(&ctrl->button_work);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|295| <<pciehp_handle_presence_or_link_change>> cancel_delayed_work(&ctrl->button_work);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|1036| <<pcie_init>> INIT_DELAYED_WORK(&ctrl->button_work, pciehp_queue_pushbutton_work);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|1094| <<pciehp_release_ctrl>> cancel_delayed_work_sync(&ctrl->button_work);
+ *
+ * 在以下使用pciehp_queue_pushbutton_work():
+ *   - drivers/pci/hotplug/pciehp_hpc.c|1036| <<pcie_init>> INIT_DELAYED_WORK(&ctrl->button_work, pciehp_queue_pushbutton_work);
+ */
 void pciehp_queue_pushbutton_work(struct work_struct *work)
 {
 	struct controller *ctrl = container_of(work, struct controller,
@@ -158,6 +233,10 @@ void pciehp_queue_pushbutton_work(struct work_struct *work)
 	mutex_unlock(&ctrl->state_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|728| <<pciehp_ist>> pciehp_handle_button_press(ctrl);
+ */
 void pciehp_handle_button_press(struct controller *ctrl)
 {
 	mutex_lock(&ctrl->state_lock);
@@ -176,6 +255,17 @@ void pciehp_handle_button_press(struct controller *ctrl)
 		/* blink power indicator and turn off attention */
 		pciehp_set_indicators(ctrl, PCI_EXP_SLTCTL_PWR_IND_BLINK,
 				      PCI_EXP_SLTCTL_ATTN_IND_OFF);
+		/*
+		 * 在以下使用controller->button_work:
+		 *   - drivers/pci/hotplug/pciehp_ctrl.c|163| <<pciehp_queue_pushbutton_work>> button_work.work);
+		 *   - drivers/pci/hotplug/pciehp_ctrl.c|201| <<pciehp_handle_button_press>> schedule_delayed_work(&ctrl->button_work, 5 * HZ);
+		 *   - drivers/pci/hotplug/pciehp_ctrl.c|211| <<pciehp_handle_button_press>> cancel_delayed_work(&ctrl->button_work);
+		 *   - drivers/pci/hotplug/pciehp_ctrl.c|242| <<pciehp_handle_disable_request>> cancel_delayed_work(&ctrl->button_work);
+		 *   - drivers/pci/hotplug/pciehp_ctrl.c|266| <<pciehp_handle_presence_or_link_change>> cancel_delayed_work(&ctrl->button_work);
+		 *   - drivers/pci/hotplug/pciehp_ctrl.c|295| <<pciehp_handle_presence_or_link_change>> cancel_delayed_work(&ctrl->button_work);
+		 *   - drivers/pci/hotplug/pciehp_hpc.c|1036| <<pcie_init>> INIT_DELAYED_WORK(&ctrl->button_work, pciehp_queue_pushbutton_work);
+		 *   - drivers/pci/hotplug/pciehp_hpc.c|1094| <<pciehp_release_ctrl>> cancel_delayed_work_sync(&ctrl->button_work);
+		 */
 		schedule_delayed_work(&ctrl->button_work, 5 * HZ);
 		break;
 	case BLINKINGOFF_STATE:
@@ -207,6 +297,10 @@ void pciehp_handle_button_press(struct controller *ctrl)
 	mutex_unlock(&ctrl->state_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|758| <<pciehp_ist>> pciehp_handle_disable_request(ctrl);
+ */
 void pciehp_handle_disable_request(struct controller *ctrl)
 {
 	mutex_lock(&ctrl->state_lock);
@@ -222,6 +316,10 @@ void pciehp_handle_disable_request(struct controller *ctrl)
 	ctrl->request_result = pciehp_disable_slot(ctrl, SAFE_REMOVAL);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|760| <<pciehp_ist>> pciehp_handle_presence_or_link_change(ctrl, events);
+ */
 void pciehp_handle_presence_or_link_change(struct controller *ctrl, u32 events)
 {
 	int present, link_active;
@@ -281,6 +379,10 @@ void pciehp_handle_presence_or_link_change(struct controller *ctrl, u32 events)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|373| <<pciehp_enable_slot>> ret = __pciehp_enable_slot(ctrl);
+ */
 static int __pciehp_enable_slot(struct controller *ctrl)
 {
 	u8 getstatus = 0;
@@ -306,6 +408,10 @@ static int __pciehp_enable_slot(struct controller *ctrl)
 	return board_added(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|331| <<pciehp_handle_presence_or_link_change>> ctrl->request_result = pciehp_enable_slot(ctrl);
+ */
 static int pciehp_enable_slot(struct controller *ctrl)
 {
 	int ret;
@@ -325,6 +431,10 @@ static int pciehp_enable_slot(struct controller *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|368| <<pciehp_disable_slot>> ret = __pciehp_disable_slot(ctrl, safe_removal);
+ */
 static int __pciehp_disable_slot(struct controller *ctrl, bool safe_removal)
 {
 	u8 getstatus = 0;
@@ -342,6 +452,25 @@ static int __pciehp_disable_slot(struct controller *ctrl, bool safe_removal)
 	return 0;
 }
 
+/*
+ * [0] CPU: 0 PID: 45 Comm: irq/51-pciehp Not tainted 6.0.0 #3
+ * [0] Hardware name: QEMU KVM Virtual Machine, BIOS 1.6.3 06/28/2022
+ * [0] dump_backtrace
+ * [0] show_stack
+ * [0] dump_stack_lvl
+ * [0] dump_stack
+ * [0] pciehp_disable_slot
+ * [0] pciehp_handle_disable_request
+ * [0] pciehp_ist
+ * [0] irq_thread_fn
+ * [0] irq_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|240| <<pciehp_handle_disable_request>> ctrl->request_result = pciehp_disable_slot(ctrl, SAFE_REMOVAL);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|265| <<pciehp_handle_presence_or_link_change>> pciehp_disable_slot(ctrl, SURPRISE_REMOVAL);
+ */
 static int pciehp_disable_slot(struct controller *ctrl, bool safe_removal)
 {
 	int ret;
diff --git a/drivers/pci/hotplug/pciehp_hpc.c b/drivers/pci/hotplug/pciehp_hpc.c
index 040ae076e..fe24af35b 100644
--- a/drivers/pci/hotplug/pciehp_hpc.c
+++ b/drivers/pci/hotplug/pciehp_hpc.c
@@ -108,6 +108,11 @@ static int pcie_poll_cmd(struct controller *ctrl, int timeout)
 	return 0;	/* timeout */
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|167| <<pcie_do_write_cmd>> pcie_wait_cmd(ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|200| <<pcie_do_write_cmd>> pcie_wait_cmd(ctrl);
+ */
 static void pcie_wait_cmd(struct controller *ctrl)
 {
 	unsigned int msecs = pciehp_poll_mode ? 2500 : 1000;
@@ -231,6 +236,13 @@ static void pcie_write_cmd_nowait(struct controller *ctrl, u16 cmd, u16 mask)
  * If the hotplug controller itself is not available anymore returns
  * %-ENODEV.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|322| <<pciehp_handle_presence_or_link_change>> link_active = pciehp_check_link_active(ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|490| <<pciehp_card_present_or_link_active>> return pciehp_check_link_active(ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|609| <<pciehp_ignore_dpc_link_change>> if (!pciehp_check_link_active(ctrl))
+ *   - drivers/pci/hotplug/pciehp_hpc.c|922| <<pciehp_slot_reset>> if (!pciehp_check_link_active(ctrl))
+ */
 int pciehp_check_link_active(struct controller *ctrl)
 {
 	struct pci_dev *pdev = ctrl_dev(ctrl);
@@ -247,6 +259,10 @@ int pciehp_check_link_active(struct controller *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|320| <<pciehp_check_link_status>> found = pci_bus_check_dev(ctrl->pcie->port->subordinate, PCI_DEVFN(0, 0));
+ */
 static bool pci_bus_check_dev(struct pci_bus *bus, int devfn)
 {
 	u32 l;
@@ -265,6 +281,9 @@ static bool pci_bus_check_dev(struct pci_bus *bus, int devfn)
 		delay -= step;
 	} while (delay > 0);
 
+	/*
+	 * [    1.734915] pci 0000:01:00.0 id reading try 50 times with interval 20 ms to get ffffffff
+	 */
 	if (count > 1)
 		pr_debug("pci %04x:%02x:%02x.%d id reading try %d times with interval %d ms to get %08x\n",
 			pci_domain_nr(bus), bus->number, PCI_SLOT(devfn),
@@ -287,6 +306,10 @@ static void pcie_wait_for_presence(struct pci_dev *pdev)
 	} while (timeout > 0);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|75| <<board_added>> retval = pciehp_check_link_status(ctrl);
+ */
 int pciehp_check_link_status(struct controller *ctrl)
 {
 	struct pci_dev *pdev = ctrl_dev(ctrl);
@@ -301,6 +324,9 @@ int pciehp_check_link_status(struct controller *ctrl)
 	if (ctrl->inband_presence_disabled)
 		pcie_wait_for_presence(pdev);
 
+	/*
+	 * 如果没有插入设备
+	 */
 	found = pci_bus_check_dev(ctrl->pcie->port->subordinate,
 					PCI_DEVFN(0, 0));
 
@@ -395,6 +421,13 @@ int pciehp_get_attention_status(struct hotplug_slot *hotplug_slot, u8 *status)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_core.c|122| <<get_power_status>> pciehp_get_power_status(ctrl, value);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|328| <<__pciehp_enable_slot>> pciehp_get_power_status(ctrl, &getstatus);
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|367| <<__pciehp_disable_slot>> pciehp_get_power_status(ctrl, &getstatus);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|1075| <<pcie_init>> pciehp_get_power_status(ctrl, &poweron);
+ */
 void pciehp_get_power_status(struct controller *ctrl, u8 *status)
 {
 	struct pci_dev *pdev = ctrl_dev(ctrl);
@@ -438,6 +471,11 @@ void pciehp_get_latch_status(struct controller *ctrl, u8 *status)
  * It the hotplug controller itself is not available anymore returns
  * %-ENODEV.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|354| <<pciehp_handle_presence_or_link_change>> present = pciehp_card_present(ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|493| <<pciehp_card_present_or_link_active>> ret = pciehp_card_present(ctrl);
+ */
 int pciehp_card_present(struct controller *ctrl)
 {
 	struct pci_dev *pdev = ctrl_dev(ctrl);
@@ -463,6 +501,12 @@ int pciehp_card_present(struct controller *ctrl)
  * Returns: %1 if the slot is occupied and %0 if it is not. If the hotplug
  *	    port is not present anymore returns %-ENODEV.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_core.c|145| <<get_adapter_status>> ret = pciehp_card_present_or_link_active(ctrl);
+ *   - drivers/pci/hotplug/pciehp_core.c|177| <<pciehp_check_presence>> occupied = pciehp_card_present_or_link_active(ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|1110| <<pcie_init>> if (!pciehp_card_present_or_link_active(ctrl) && poweron) {
+ */
 int pciehp_card_present_or_link_active(struct controller *ctrl)
 {
 	int ret;
@@ -471,6 +515,13 @@ int pciehp_card_present_or_link_active(struct controller *ctrl)
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - drivers/pci/hotplug/pciehp_ctrl.c|322| <<pciehp_handle_presence_or_link_change>> link_active = pciehp_check_link_active(ctrl);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|490| <<pciehp_card_present_or_link_active>> return pciehp_check_link_active(ctrl);
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|609| <<pciehp_ignore_dpc_link_change>> if (!pciehp_check_link_active(ctrl))
+	 *   - drivers/pci/hotplug/pciehp_hpc.c|922| <<pciehp_slot_reset>> if (!pciehp_check_link_active(ctrl))
+	 */
 	return pciehp_check_link_active(ctrl);
 }
 
@@ -565,6 +616,10 @@ void pciehp_power_off_slot(struct controller *ctrl)
 		 PCI_EXP_SLTCTL_PWR_OFF);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|772| <<pciehp_ist>> pciehp_ignore_dpc_link_change(ctrl, pdev, irq);
+ */
 static void pciehp_ignore_dpc_link_change(struct controller *ctrl,
 					  struct pci_dev *pdev, int irq)
 {
@@ -591,6 +646,12 @@ static void pciehp_ignore_dpc_link_change(struct controller *ctrl,
 	up_read(&ctrl->reset_lock);
 }
 
+/*
+ * 在以下使用pciehp_isr():
+ *   - drivers/pci/hotplug/pciehp_hpc.c|69| <<pciehp_request_irq>> retval = request_threaded_irq(irq, pciehp_isr, pciehp_ist, IRQF_SHARED, "pciehp", ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|715| <<pciehp_ist>> ret = pciehp_isr(irq, dev_id);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|779| <<pciehp_poll>> while (pciehp_isr(IRQ_NOTCONNECTED, ctrl) == IRQ_WAKE_THREAD ||
+ */
 static irqreturn_t pciehp_isr(int irq, void *dev_id)
 {
 	struct controller *ctrl = (struct controller *)dev_id;
@@ -696,6 +757,11 @@ static irqreturn_t pciehp_isr(int irq, void *dev_id)
 	return IRQ_WAKE_THREAD;
 }
 
+/*
+ * 在以下使用pciehp_ist():
+ *   - drivers/pci/hotplug/pciehp_hpc.c|69| <<pciehp_request_irq>> retval = request_threaded_irq(irq, pciehp_isr, pciehp_ist, IRQF_SHARED, "pciehp", ctrl);
+ *   - drivers/pci/hotplug/pciehp_hpc.c|781| <<pciehp_poll>> pciehp_ist(IRQ_NOTCONNECTED, ctrl);
+ */
 static irqreturn_t pciehp_ist(int irq, void *dev_id)
 {
 	struct controller *ctrl = (struct controller *)dev_id;
@@ -764,6 +830,10 @@ static irqreturn_t pciehp_ist(int irq, void *dev_id)
 	return ret;
 }
 
+/*
+ * 在以下使用pciehp_poll():
+ *   - drivers/pci/hotplug/pciehp_hpc.c|62| <<pciehp_request_irq>> ctrl->poll_thread = kthread_run(&pciehp_poll, ctrl, "pciehp_poll-%s", slot_name(ctrl));
+ */
 static int pciehp_poll(void *data)
 {
 	struct controller *ctrl = data;
@@ -785,6 +855,10 @@ static int pciehp_poll(void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|974| <<pcie_init_notification>> pcie_enable_notification(ctrl);
+ */
 static void pcie_enable_notification(struct controller *ctrl)
 {
 	u16 cmd, mask;
@@ -875,6 +949,9 @@ void pcie_disable_interrupt(struct controller *ctrl)
  * If the link failed to retrain successfully, synthesize the ignored event.
  * Surprise removal during reset is detected through Presence Detect Changed.
  */
+/*
+ * struct pcie_port_service_driver hpdriver_portdrv.slot_reset = pciehp_slot_reset()
+ */
 int pciehp_slot_reset(struct pcie_device *dev)
 {
 	struct controller *ctrl = get_service_data(dev);
@@ -933,6 +1010,10 @@ int pciehp_reset_slot(struct hotplug_slot *hotplug_slot, bool probe)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_core.c|222| <<pciehp_probe>> rc = pcie_init_notification(ctrl);
+ */
 int pcie_init_notification(struct controller *ctrl)
 {
 	if (pciehp_request_irq(ctrl))
@@ -979,6 +1060,10 @@ static inline int pcie_hotplug_depth(struct pci_dev *dev)
 	return depth;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_core.c|204| <<pciehp_probe>> ctrl = pcie_init(dev);
+ */
 struct controller *pcie_init(struct pcie_device *dev)
 {
 	struct controller *ctrl;
diff --git a/drivers/pci/hotplug/pciehp_pci.c b/drivers/pci/hotplug/pciehp_pci.c
index d17f3bf36..758f5221f 100644
--- a/drivers/pci/hotplug/pciehp_pci.c
+++ b/drivers/pci/hotplug/pciehp_pci.c
@@ -81,6 +81,10 @@ int pciehp_configure_device(struct controller *ctrl)
  * them from the system.  Safely removed devices are quiesced.  Surprise
  * removed devices are marked as such to prevent further accesses.
  */
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_ctrl.c|125| <<remove_board>> pciehp_unconfigure_device(ctrl, safe_removal);
+ */
 void pciehp_unconfigure_device(struct controller *ctrl, bool presence)
 {
 	struct pci_dev *dev, *temp;
diff --git a/drivers/pci/iov.c b/drivers/pci/iov.c
index 952217572..8c0f8b2a8 100644
--- a/drivers/pci/iov.c
+++ b/drivers/pci/iov.c
@@ -1201,6 +1201,13 @@ EXPORT_SYMBOL_GPL(pci_sriov_get_totalvfs);
  * before enabling SR-IOV.  Return value is negative on error, or number of
  * VFs allocated on success.
  */
+/*
+ * 在以下使用pci_sriov_configure_simple():
+ *   - drivers/misc/pci_endpoint_test.c|990| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/net/ethernet/amazon/ena/ena_netdev.c|4543| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/nvme/host/pci.c|3562| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ *   - drivers/pci/pci-pf-stub.c|38| <<global>> .sriov_configure = pci_sriov_configure_simple,
+ */
 int pci_sriov_configure_simple(struct pci_dev *dev, int nr_virtfn)
 {
 	int rc;
diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c
index 95bc329e7..0fabb0d35 100644
--- a/drivers/pci/pci.c
+++ b/drivers/pci/pci.c
@@ -1147,6 +1147,13 @@ void pci_resume_bus(struct pci_bus *bus)
 		pci_walk_bus(bus, pci_resume_one, NULL);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/pci.c|4762| <<pcie_flr>> return pci_dev_wait(dev, "FLR", PCIE_RESET_READY_POLL_MS);
+ *   - drivers/pci/pci.c|4829| <<pci_af_flr>> return pci_dev_wait(dev, "AF_FLR", PCIE_RESET_READY_POLL_MS);
+ *   - drivers/pci/pci.c|4874| <<pci_pm_reset>> return pci_dev_wait(dev, "PM D3hot->D0", PCIE_RESET_READY_POLL_MS);
+ *   - drivers/pci/pci.c|5103| <<pci_bridge_secondary_bus_reset>> return pci_dev_wait(dev, "bus reset", PCIE_RESET_READY_POLL_MS);
+ */
 static int pci_dev_wait(struct pci_dev *dev, char *reset_type, int timeout)
 {
 	int delay = 1;
@@ -4742,6 +4749,36 @@ EXPORT_SYMBOL(pci_wait_for_pending_transaction);
  * Initiate a function level reset unconditionally on @dev without
  * checking any flags and DEVCAP
  */
+/*
+ * 例子:
+ * [0] pci_flr_wait
+ * [0] pcie_flr
+ * [0] __pci_reset_function_locked
+ * [0] pci_try_reset_function
+ * [0] vfio_pci_ioctl
+ * [0] vfio_device_fops_unl_ioctt
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/crypto/qat/qat_common/adf_aer.c|68| <<adf_reset_flr>> pcie_flr(accel_to_pci_dev(accel_dev));
+ *   - drivers/infiniband/hw/hfi1/chip.c|14063| <<init_chip>> pcie_flr(dd->pcidev);
+ *   - drivers/infiniband/hw/hfi1/chip.c|14075| <<init_chip>> pcie_flr(dd->pcidev);
+ *   - drivers/net/ethernet/broadcom/bnxt/bnxt.c|13531| <<bnxt_init_one>> pcie_flr(pdev);
+ *   - drivers/net/ethernet/cavium/liquidio/lio_vf_main.c|432| <<octeon_pci_flr>> pcie_flr(oct->pci_dev);
+ *   - drivers/net/ethernet/cavium/liquidio/octeon_mailbox.c|263| <<octeon_mbox_process_cmd>> pcie_flr(oct->sriov_info.dpiring_to_vfpcidev_lut[mbox->q_no]);
+ *   - drivers/net/ethernet/freescale/enetc/enetc.c|2852| <<enetc_pci_probe>> pcie_flr(pdev);
+ *   - drivers/net/ethernet/freescale/enetc/enetc_pci_mdio.c|50| <<enetc_pci_mdio_probe>> pcie_flr(pdev);
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_main.c|7742| <<ixgbe_check_for_bad_vf>> pcie_flr(vfdev);
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_main.c|11427| <<ixgbe_io_error_detected>> pcie_flr(vfdev);
+ *   - drivers/pci/pci.c|4784| <<pcie_reset_flr>> return pcie_flr(dev);
+ *   - drivers/pci/quirks.c|3782| <<reset_intel_82599_sfp_virtfn>> pcie_flr(dev);
+ *   - drivers/pci/quirks.c|3885| <<reset_chelsio_generic_dev>> pcie_flr(dev);
+ *   - drivers/pci/quirks.c|3977| <<nvme_disable_and_flr>> pcie_flr(dev);
+ *   - drivers/pci/quirks.c|4033| <<reset_hinic_vf_dev>> pcie_flr(pdev);
+ */
 int pcie_flr(struct pci_dev *dev)
 {
 	if (!pci_wait_for_pending_transaction(dev))
diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index c5286b027..080a153e6 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -2362,6 +2362,11 @@ static bool pci_bus_wait_crs(struct pci_bus *bus, int devfn, u32 *l,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/pci/probe.c|2397| <<pci_bus_read_dev_vendor_id>> return pci_bus_generic_read_dev_vendor_id(bus, devfn, l, timeout);
+ *   - drivers/pci/quirks.c|5575| <<pci_idt_bus_quirk>> found = pci_bus_generic_read_dev_vendor_id(bus, devfn, l, timeout);
+ */
 bool pci_bus_generic_read_dev_vendor_id(struct pci_bus *bus, int devfn, u32 *l,
 					int timeout)
 {
diff --git a/drivers/pci/setup-res.c b/drivers/pci/setup-res.c
index 439ac5f59..67ba1f21c 100644
--- a/drivers/pci/setup-res.c
+++ b/drivers/pci/setup-res.c
@@ -311,6 +311,19 @@ static int _pci_assign_resource(struct pci_dev *dev, int resno,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/bcma/driver_pci_host.c|571| <<bcma_core_pci_fixup_addresses>> err = pci_assign_resource(dev, pos);
+ *   - drivers/bus/mhi/host/pci_generic.c|680| <<mhi_pci_claim>> err = pci_assign_resource(pdev, bar_num);
+ *   - drivers/char/agp/efficeon-agp.c|389| <<agp_efficeon_probe>> if (pci_assign_resource(pdev, 0)) {
+ *   - drivers/char/agp/intel-agp.c|778| <<agp_intel_probe>> if (pci_assign_resource(pdev, 0)) {
+ *   - drivers/mtd/maps/l440gx.c|112| <<init_l440gx>> if (pci_assign_resource(pm_dev, PIIXE_IOBASE_RESOURCE) != 0) {
+ *   - drivers/net/wireless/ath/ath11k/pci.c|510| <<ath11k_pci_claim>> ret = pci_assign_resource(pdev, ATH11K_PCI_BAR_NUM);
+ *   - drivers/parisc/dino.c|636| <<dino_fixup_bus>> WARN_ON(pci_assign_resource(bus->self, i));
+ *   - drivers/pci/rom.c|143| <<pci_map_rom>> if (res->parent == NULL && pci_assign_resource(pdev, PCI_ROM_RESOURCE))
+ *   - drivers/pci/setup-bus.c|248| <<reassign_resources_sorted>> if (pci_assign_resource(add_res->dev, idx))
+ *   - drivers/pci/setup-bus.c|286| <<assign_requested_resources_sorted>> pci_assign_resource(dev_res->dev, idx)) {
+ */
 int pci_assign_resource(struct pci_dev *dev, int resno)
 {
 	struct resource *res = dev->resource + resno;
diff --git a/drivers/perf/arm_dmc620_pmu.c b/drivers/perf/arm_dmc620_pmu.c
index 280a6ae3e..ce0dd1ca6 100644
--- a/drivers/perf/arm_dmc620_pmu.c
+++ b/drivers/perf/arm_dmc620_pmu.c
@@ -480,6 +480,9 @@ static void dmc620_pmu_put_irq(struct dmc620_pmu *dmc620_pmu)
 	kfree(irq);
 }
 
+/*
+ * called by perf_try_init_event()
+ */
 static int dmc620_pmu_event_init(struct perf_event *event)
 {
 	struct dmc620_pmu *dmc620_pmu = to_dmc620_pmu(event->pmu);
diff --git a/drivers/perf/arm_pmu.c b/drivers/perf/arm_pmu.c
index 59d3980b8..1f9cc2d30 100644
--- a/drivers/perf/arm_pmu.c
+++ b/drivers/perf/arm_pmu.c
@@ -168,6 +168,24 @@ armpmu_map_raw_event(u32 raw_event_mask, u64 config)
 	return (int)(config & raw_event_mask);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/perf_event.c|1073| <<global>> hw_event_id = armpmu_map_event(event, &armv8_pmuv3_perf_map,
+ *   - arch/arm64/kernel/perf_event.c|1105| <<global>> return armpmu_map_event(event, extra_event_map, extra_cache_map,
+ *   - arch/arm/kernel/perf_event_v6.c|488| <<armv6_map_event>> return armpmu_map_event(event, &armv6_perf_map,
+ *   - arch/arm/kernel/perf_event_v6.c|538| <<armv6mpcore_map_event>> return armpmu_map_event(event, &armv6mpcore_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1116| <<armv7_a8_map_event>> return armpmu_map_event(event, &armv7_a8_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1122| <<armv7_a9_map_event>> return armpmu_map_event(event, &armv7_a9_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1128| <<armv7_a5_map_event>> return armpmu_map_event(event, &armv7_a5_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1134| <<armv7_a15_map_event>> return armpmu_map_event(event, &armv7_a15_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1140| <<armv7_a7_map_event>> return armpmu_map_event(event, &armv7_a7_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1146| <<armv7_a12_map_event>> return armpmu_map_event(event, &armv7_a12_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1152| <<krait_map_event>> return armpmu_map_event(event, &krait_perf_map,
+ *   - arch/arm/kernel/perf_event_v7.c|1158| <<krait_map_event_no_branch>> return armpmu_map_event(event, &krait_perf_map_no_branch,
+ *   - arch/arm/kernel/perf_event_v7.c|1164| <<scorpion_map_event>> return armpmu_map_event(event, &scorpion_perf_map,
+ *   - arch/arm/kernel/perf_event_xscale.c|366| <<xscale_map_event>> return armpmu_map_event(event, &xscale_perf_map,
+ *   - drivers/perf/apple_m1_cpu_pmu.c|493| <<m1_pmu_map_event>> return armpmu_map_event(event, &m1_pmu_perf_map, NULL, M1_PMU_CFG_EVENT);
+ */
 int
 armpmu_map_event(struct perf_event *event,
 		 const unsigned (*event_map)[PERF_COUNT_HW_MAX],
@@ -933,6 +951,11 @@ void armpmu_free(struct arm_pmu *pmu)
 	kfree(pmu);
 }
 
+/*
+ * called by:
+ *   - drivers/perf/arm_pmu_acpi.c|330| <<arm_pmu_acpi_probe>> ret = armpmu_register(pmu);
+ *   - drivers/perf/arm_pmu_platform.c|232| <<arm_pmu_device_probe>> ret = armpmu_register(pmu);
+ */
 int armpmu_register(struct arm_pmu *pmu)
 {
 	int ret;
diff --git a/drivers/perf/arm_pmu_acpi.c b/drivers/perf/arm_pmu_acpi.c
index 96ffadd65..b5778ab01 100644
--- a/drivers/perf/arm_pmu_acpi.c
+++ b/drivers/perf/arm_pmu_acpi.c
@@ -286,6 +286,10 @@ static int arm_pmu_acpi_cpu_starting(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kernel/perf_event.c|1405| <<armv8_pmu_driver_init>> return arm_pmu_acpi_probe(armv8_pmuv3_pmu_init);
+ */
 int arm_pmu_acpi_probe(armpmu_init_fn init_fn)
 {
 	int pmu_idx = 0;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 578c4b6d0..d85a90741 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -599,6 +599,11 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|653| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|711| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+ */
 static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 {
 	DECLARE_COMPLETION_ONSTACK(comp);
@@ -708,6 +713,11 @@ static int virtscsi_abort(struct scsi_cmnd *sc)
 		.lun[3] = sc->device->lun & 0xff,
 		.tag = cpu_to_virtio64(vscsi->vdev, (unsigned long)sc),
 	};
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|653| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+	 *   - drivers/scsi/virtio_scsi.c|711| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+	 */
 	return virtscsi_tmf(vscsi, cmd);
 }
 
diff --git a/drivers/target/target_core_file.c b/drivers/target/target_core_file.c
index 28aa643be..2d9bdd82d 100644
--- a/drivers/target/target_core_file.c
+++ b/drivers/target/target_core_file.c
@@ -903,6 +903,10 @@ static void fd_free_prot(struct se_device *dev)
 	fd_dev->fd_prot_file = NULL;
 }
 
+/*
+ * 在以下使用fd_sbc_ops():
+ *   - drivers/target/target_core_file.c|916| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_sbc_ops);
+ */
 static struct sbc_ops fd_sbc_ops = {
 	.execute_rw		= fd_execute_rw,
 	.execute_sync_cache	= fd_execute_sync_cache,
@@ -910,9 +914,19 @@ static struct sbc_ops fd_sbc_ops = {
 	.execute_unmap		= fd_execute_unmap,
 };
 
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|1545| <<target_cmd_parse_cdb>> ret = dev->transport->parse_cdb(cmd);
+ */
 static sense_reason_t
 fd_parse_cdb(struct se_cmd *cmd)
 {
+	/*
+	 * called by:
+	 *   - drivers/target/target_core_file.c|916| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_sbc_ops);
+	 *   - drivers/target/target_core_iblock.c|914| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_sbc_ops);
+	 *   - drivers/target/target_core_rd.c|653| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_sbc_ops);
+	 */
 	return sbc_parse_cdb(cmd, &fd_sbc_ops);
 }
 
diff --git a/drivers/target/target_core_hba.c b/drivers/target/target_core_hba.c
index d508b343b..eddde116e 100644
--- a/drivers/target/target_core_hba.c
+++ b/drivers/target/target_core_hba.c
@@ -35,6 +35,14 @@ static DEFINE_SPINLOCK(hba_lock);
 static LIST_HEAD(hba_list);
 
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|944| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+ *   - drivers/target/target_core_iblock.c|939| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+ *   - drivers/target/target_core_pscsi.c|1057| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+ *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+ *   - drivers/target/target_core_user.c|3358| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+ */
 int transport_backend_register(const struct target_backend_ops *ops)
 {
 	struct target_backend *tb, *old;
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index 8351c974c..1bbb317f8 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -310,6 +310,12 @@ static unsigned long long iblock_emulate_read_cap_with_block_size(
 	return blocks_long;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|350| <<iblock_bio_done>> iblock_complete_cmd(cmd);
+ *   - drivers/target/target_core_iblock.c|773| <<iblock_execute_rw>> iblock_complete_cmd(cmd);
+ *   - drivers/target/target_core_iblock.c|831| <<iblock_execute_rw>> iblock_complete_cmd(cmd);
+ */
 static void iblock_complete_cmd(struct se_cmd *cmd)
 {
 	struct iblock_req *ibr = cmd->priv;
@@ -327,6 +333,10 @@ static void iblock_complete_cmd(struct se_cmd *cmd)
 	kfree(ibr);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|367| <<iblock_get_bio>> bio->bi_end_io = &iblock_bio_done;
+ */
 static void iblock_bio_done(struct bio *bio)
 {
 	struct se_cmd *cmd = bio->bi_private;
@@ -346,6 +356,13 @@ static void iblock_bio_done(struct bio *bio)
 	iblock_complete_cmd(cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|525| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+ *   - drivers/target/target_core_iblock.c|538| <<iblock_execute_write_same>> bio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);
+ *   - drivers/target/target_core_iblock.c|766| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);
+ *   - drivers/target/target_core_iblock.c|799| <<iblock_execute_rw>> bio = iblock_get_bio(cmd, block_lba, sg_num, opf);
+ */
 static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 				  blk_opf_t opf)
 {
@@ -370,6 +387,12 @@ static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 	return bio;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|562| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|806| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|830| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ */
 static void iblock_submit_bios(struct bio_list *list)
 {
 	struct blk_plug plug;
@@ -716,6 +739,9 @@ iblock_alloc_bip(struct se_cmd *cmd, struct bio *bio,
 	return 0;
 }
 
+/*
+ * struct sbc_ops iblock_sbc_ops.execute_rw = iblock_execute_rw()
+ */
 static sense_reason_t
 iblock_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 		  enum dma_data_direction data_direction)
@@ -885,9 +911,19 @@ static struct sbc_ops iblock_sbc_ops = {
 	.execute_unmap		= iblock_execute_unmap,
 };
 
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|1545| <<target_cmd_parse_cdb>> ret = dev->transport->parse_cdb(cmd);
+ */
 static sense_reason_t
 iblock_parse_cdb(struct se_cmd *cmd)
 {
+	/*
+	 * called by:
+	 *   - drivers/target/target_core_file.c|916| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_sbc_ops);
+	 *   - drivers/target/target_core_iblock.c|914| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_sbc_ops);
+	 *   - drivers/target/target_core_rd.c|653| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_sbc_ops);
+	 */
 	return sbc_parse_cdb(cmd, &iblock_sbc_ops);
 }
 
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index 1e3216de1..6d1b4d376 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -761,6 +761,12 @@ sbc_check_dpofua(struct se_device *dev, struct se_cmd *cmd, unsigned char *cdb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|916| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_sbc_ops);
+ *   - drivers/target/target_core_iblock.c|914| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_sbc_ops);
+ *   - drivers/target/target_core_rd.c|653| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_sbc_ops);
+ */
 sense_reason_t
 sbc_parse_cdb(struct se_cmd *cmd, struct sbc_ops *ops)
 {
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 7838dc20f..cd7d61997 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -796,6 +796,12 @@ void transport_copy_sense_to_cmd(struct se_cmd *cmd, unsigned char *sense)
 }
 EXPORT_SYMBOL(transport_copy_sense_to_cmd);
 
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|849| <<target_abort_work>> target_handle_abort(cmd);
+ *   - drivers/target/target_core_transport.c|3600| <<target_tmr_work>> target_handle_abort(cmd);
+ *   - drivers/target/target_core_transport.c|3622| <<transport_generic_handle_tmr>> target_handle_abort(cmd);
+ */
 static void target_handle_abort(struct se_cmd *cmd)
 {
 	bool tas = cmd->transport_state & CMD_T_TAS;
@@ -870,6 +876,11 @@ static bool target_cmd_interrupted(struct se_cmd *cmd)
 }
 
 /* May be called from interrupt context so must not sleep. */
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|917| <<target_complete_cmd>> target_complete_cmd_with_sense(cmd, scsi_status, scsi_status ?
+ *   - drivers/target/target_core_xcopy.c|781| <<target_xcopy_do_work>> target_complete_cmd_with_sense(ec_cmd, SAM_STAT_CHECK_CONDITION, sense_rc);
+ */
 void target_complete_cmd_with_sense(struct se_cmd *cmd, u8 scsi_status,
 				    sense_reason_t sense_reason)
 {
@@ -1519,12 +1530,21 @@ target_cmd_init_cdb(struct se_cmd *cmd, unsigned char *cdb, gfp_t gfp)
 }
 EXPORT_SYMBOL(target_cmd_init_cdb);
 
+/*
+ * called by:
+ *   - drivers/target/iscsi/iscsi_target.c|1224| <<iscsit_setup_scsi_cmd>> cmd->sense_reason = target_cmd_parse_cdb(&cmd->se_cmd);
+ *   - drivers/target/target_core_transport.c|1721| <<target_submit_prep>> rc = target_cmd_parse_cdb(se_cmd);
+ *   - drivers/target/target_core_xcopy.c|547| <<target_xcopy_setup_pt_cmd>> if (target_cmd_parse_cdb(cmd))
+ */
 sense_reason_t
 target_cmd_parse_cdb(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
 	sense_reason_t ret;
 
+	/*
+	 * 一个例子:iblock_parse_cdb()
+	 */
 	ret = dev->transport->parse_cdb(cmd);
 	if (ret == TCM_UNSUPPORTED_SCSI_OPCODE)
 		pr_debug_ratelimited("%s/%s: Unsupported SCSI Opcode 0x%02x, sending CHECK_CONDITION.\n",
@@ -1688,6 +1708,17 @@ EXPORT_SYMBOL_GPL(target_init_cmd);
  * If failure is returned, lio will the callers queue_status to complete
  * the cmd.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1537| <<srpt_handle_cmd>> if (target_submit_prep(cmd, srp_cmd->cdb, sg, sg_cnt, NULL, 0, NULL, 0,
+ *   - drivers/scsi/elx/efct/efct_lio.c|1403| <<efct_scsi_recv_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|495| <<tcm_qla2xxx_handle_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0, NULL, 0,
+ *   - drivers/target/loopback/tcm_loop.c|158| <<tcm_loop_target_queue_cmd>> if (target_submit_prep(se_cmd, sc->cmnd, scsi_sglist(sc),
+ *   - drivers/target/target_core_transport.c|1846| <<target_submit_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0, NULL, 0,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|557| <<ft_send_work>> if (target_submit_prep(&cmd->se_cmd, fcp->fc_cdb, NULL, 0, NULL, 0,
+ *   - drivers/vhost/scsi.c|952| <<vhost_scsi_target_queue_cmd>> if (target_submit_prep(se_cmd, cmd->tvc_cdb, sg_ptr,
+ *   - drivers/xen/xen-scsiback.c|444| <<scsiback_cmd_exec>> if (target_submit_prep(se_cmd, pending_req->cmnd, pending_req->sgl,
+ */
 int target_submit_prep(struct se_cmd *se_cmd, unsigned char *cdb,
 		       struct scatterlist *sgl, u32 sgl_count,
 		       struct scatterlist *sgl_bidi, u32 sgl_bidi_count,
@@ -1901,6 +1932,11 @@ void target_queued_submit_work(struct work_struct *work)
  * target_queue_submission - queue the cmd to run on the LIO workqueue
  * @se_cmd: command descriptor to submit
  */
+/*
+ * called by:
+ *   - drivers/target/loopback/tcm_loop.c|164| <<tcm_loop_target_queue_cmd>> target_queue_submission(se_cmd);
+ *   - drivers/vhost/scsi.c|957| <<vhost_scsi_target_queue_cmd>> target_queue_submission(se_cmd);
+ */
 void target_queue_submission(struct se_cmd *se_cmd)
 {
 	struct se_device *se_dev = se_cmd->se_dev;
@@ -1941,6 +1977,17 @@ static void target_complete_tmr_failure(struct work_struct *work)
  * Callable from all contexts.
  **/
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1599| <<srpt_handle_tsk_mgmt>> rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,
+ *   - drivers/scsi/elx/efct/efct_lio.c|1462| <<efct_scsi_recv_tmf>> rc = target_submit_tmr(&ocp->cmd, se_sess, NULL, lun, ocp, tmr_func,
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2799| <<ibmvscsis_parse_task>> rc = target_submit_tmr(&cmd->se_cmd, nexus->se_sess, NULL,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|627| <<tcm_qla2xxx_handle_tmr>> return target_submit_tmr(se_cmd, sess->se_sess, NULL, lun, mcmd,
+ *   - drivers/target/loopback/tcm_loop.c|223| <<tcm_loop_issue_tmr>> rc = target_submit_tmr(se_cmd, se_sess, tl_cmd->tl_sense_buf, lun,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|370| <<ft_send_tm>> rc = target_submit_tmr(&cmd->se_cmd, cmd->sess->se_sess,
+ *   - drivers/vhost/scsi.c|1297| <<vhost_scsi_handle_tmf>> if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
+ *   - drivers/xen/xen-scsiback.c|638| <<scsiback_device_action>> rc = target_submit_tmr(&pending_req->se_cmd, nexus->tvn_se_sess,
+ */
 int target_submit_tmr(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *sense, u64 unpacked_lun,
 		void *fabric_tmr_ptr, unsigned char tm_type,
@@ -2238,6 +2285,28 @@ static bool target_handle_task_attr(struct se_cmd *cmd)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1656| <<isert_rdma_read_done>> target_execute_cmd(se_cmd);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1340| <<srpt_rdma_read_done>> target_execute_cmd(&ioctx->cmd);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|2749| <<srpt_write_pending>> target_execute_cmd(&ioctx->cmd);
+ *   - drivers/scsi/elx/efct/efct_lio.c|632| <<efct_lio_datamove_done>> target_execute_cmd(&io->tgt_io.cmd);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|3764| <<ibmvscsis_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|549| <<tcm_qla2xxx_handle_data_work>> return target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target.c|1723| <<iscsit_check_dataout_payload>> target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_erl1.c|924| <<iscsit_execute_cmd>> target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_tmr.c|256| <<iscsit_task_reassign_complete_write>> target_execute_cmd(se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_tmr.c|256| <<iscsit_task_reassign_complete_write>> target_execute_cmd(se_cmd);
+ *   - drivers/target/loopback/tcm_loop.c|547| <<tcm_loop_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/target/sbp/sbp_target.c|1732| <<sbp_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/target/target_core_transport.c|2809| <<transport_generic_new_cmd>> target_execute_cmd(cmd);
+ *   - drivers/target/target_core_xcopy.c|570| <<target_xcopy_issue_pt_cmd>> target_execute_cmd(se_cmd);
+ *   - drivers/target/tcm_fc/tfc_io.c|189| <<ft_execute_work>> target_execute_cmd(&cmd->se_cmd);
+ *   - drivers/usb/gadget/function/f_tcm.c|283| <<bot_send_write_request>> target_execute_cmd(se_cmd);
+ *   - drivers/usb/gadget/function/f_tcm.c|714| <<uasp_send_write_request>> target_execute_cmd(se_cmd);
+ *   - drivers/vhost/scsi.c|416| <<vhost_scsi_write_pending>> target_execute_cmd(se_cmd);
+ *   - drivers/xen/xen-scsiback.c|1446| <<scsiback_write_pending>> target_execute_cmd(se_cmd);
+ */
 void target_execute_cmd(struct se_cmd *cmd)
 {
 	/*
@@ -2990,6 +3059,10 @@ static void target_free_cmd_mem(struct se_cmd *cmd)
 		kfree(cmd->t_task_cdb);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|3080| <<target_put_sess_cmd>> return kref_put(&se_cmd->cmd_kref, target_release_cmd_kref);
+ */
 static void target_release_cmd_kref(struct kref *kref)
 {
 	struct se_cmd *se_cmd = container_of(kref, struct se_cmd, cmd_kref);
@@ -3562,6 +3635,12 @@ static void target_tmr_work(struct work_struct *work)
 	target_handle_abort(cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/target/iscsi/iscsi_target.c|2173| <<iscsit_handle_task_mgt_cmd>> return transport_generic_handle_tmr(&cmd->se_cmd);
+ *   - drivers/target/iscsi/iscsi_target_erl1.c|967| <<iscsit_execute_cmd>> return transport_generic_handle_tmr(&cmd->se_cmd);
+ *   - drivers/target/target_core_transport.c|1995| <<target_submit_tmr>> transport_generic_handle_tmr(se_cmd);
+ */
 int transport_generic_handle_tmr(
 	struct se_cmd *cmd)
 {
diff --git a/drivers/vfio/pci/vfio_pci_core.c b/drivers/vfio/pci/vfio_pci_core.c
index c8d3b0450..b82c74b04 100644
--- a/drivers/vfio/pci/vfio_pci_core.c
+++ b/drivers/vfio/pci/vfio_pci_core.c
@@ -359,6 +359,12 @@ int vfio_pci_core_enable(struct vfio_pci_core_device *vdev)
 }
 EXPORT_SYMBOL_GPL(vfio_pci_core_enable);
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/hisilicon/hisi_acc_vfio_pci.c|1191| <<hisi_acc_vfio_pci_open_device>> vfio_pci_core_disable(vdev);
+ *   - drivers/vfio/pci/vfio_pci.c|118| <<vfio_pci_open_device>> vfio_pci_core_disable(vdev);
+ *   - drivers/vfio/pci/vfio_pci_core.c|488| <<vfio_pci_core_close_device>> vfio_pci_core_disable(vdev);
+ */
 void vfio_pci_core_disable(struct vfio_pci_core_device *vdev)
 {
 	struct pci_dev *pdev = vdev->pdev;
@@ -923,11 +929,17 @@ long vfio_pci_core_ioctl(struct vfio_device *core_vdev, unsigned int cmd,
 		int max, ret = 0;
 		size_t data_size = 0;
 
+		/*
+		 * Report the offset of a struct field within the struct
+		 */
 		minsz = offsetofend(struct vfio_irq_set, count);
 
 		if (copy_from_user(&hdr, (void __user *)arg, minsz))
 			return -EFAULT;
 
+		/*
+		 * 最大支持中断的数目???
+		 */
 		max = vfio_pci_get_irq_count(vdev, hdr.index);
 
 		ret = vfio_set_irqs_validate_and_prepare(&hdr, max,
@@ -1363,6 +1375,14 @@ void vfio_pci_zap_and_down_write_memory_lock(struct vfio_pci_core_device *vdev)
 	mutex_unlock(&vdev->vma_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_core.c|804| <<vfio_pci_core_ioctl>> cmd = vfio_pci_memory_lock_and_enable(vdev);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|266| <<vfio_msi_enable>> cmd = vfio_pci_memory_lock_and_enable(vdev);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|313| <<vfio_msi_set_vector_signal>> cmd = vfio_pci_memory_lock_and_enable(vdev);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|354| <<vfio_msi_set_vector_signal>> cmd = vfio_pci_memory_lock_and_enable(vdev);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|434| <<vfio_msi_disable>> cmd = vfio_pci_memory_lock_and_enable(vdev);
+ */
 u16 vfio_pci_memory_lock_and_enable(struct vfio_pci_core_device *vdev)
 {
 	u16 cmd;
diff --git a/drivers/vfio/pci/vfio_pci_intrs.c b/drivers/vfio/pci/vfio_pci_intrs.c
index 6069a11fb..b331e0b31 100644
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@ -22,6 +22,10 @@
 
 #include <linux/vfio_pci_core.h>
 
+/*
+ * msix的核心是VFIO_IRQ_SET_ACTION_TRIGGER
+ */
+
 /*
  * INTx
  */
@@ -236,6 +240,10 @@ static void vfio_intx_disable(struct vfio_pci_core_device *vdev)
 /*
  * MSI/MSI-X
  */
+/*
+ * 在以下使用vfio_msihandler():
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|362| <<vfio_msi_set_vector_signal>> ret = request_irq(irq, vfio_msihandler, 0, vdev->ctx[vector].name, trigger);
+ */
 static irqreturn_t vfio_msihandler(int irq, void *arg)
 {
 	struct eventfd_ctx *trigger = arg;
@@ -244,6 +252,10 @@ static irqreturn_t vfio_msihandler(int irq, void *arg)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|548| <<vfio_pci_set_msi_trigger>> ret = vfio_msi_enable(vdev, start + count, msix);
+ */
 static int vfio_msi_enable(struct vfio_pci_core_device *vdev, int nvec, bool msix)
 {
 	struct pci_dev *pdev = vdev->pdev;
@@ -270,6 +282,13 @@ static int vfio_msi_enable(struct vfio_pci_core_device *vdev, int nvec, bool msi
 	}
 	vfio_pci_memory_unlock_and_restore(vdev, cmd);
 
+	/*
+	 * 在以下设置vfio_pci_core_device->num_ctx:
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|154| <<vfio_intx_enable>> vdev->num_ctx = 1;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|232| <<vfio_intx_disable>> vdev->num_ctx = 0;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|277| <<vfio_msi_enable>> vdev->num_ctx = nvec;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|446| <<vfio_msi_disable>> vdev->num_ctx = 0;
+	 */
 	vdev->num_ctx = nvec;
 	vdev->irq_type = msix ? VFIO_PCI_MSIX_IRQ_INDEX :
 				VFIO_PCI_MSI_IRQ_INDEX;
@@ -285,6 +304,11 @@ static int vfio_msi_enable(struct vfio_pci_core_device *vdev, int nvec, bool msi
 	return 0;
 }
 
+/*
+ * called by:
+ *    - drivers/vfio/pci/vfio_pci_intrs.c|377| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+ *    - drivers/vfio/pci/vfio_pci_intrs.c|382| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, j, -1, msix);
+ */
 static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 				      int vector, int fd, bool msix)
 {
@@ -298,6 +322,18 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 
 	irq = pci_irq_vector(pdev, vector);
 
+	/*
+	 * struct vfio_pci_core_device *vdev:
+	 * -> struct vfio_device vdev;
+	 * -> struct pci_dev *pdev;
+	 * -> struct vfio_pci_irq_ctx *ctx;
+	 *    -> struct eventfd_ctx      *trigger;
+	 *    -> struct virqfd           *unmask;
+	 *    -> struct virqfd           *mask;
+	 *    -> char                    *name;
+	 *    -> bool                    masked;
+	 *    -> struct irq_bypass_producer      producer;
+	 */
 	if (vdev->ctx[vector].trigger) {
 		irq_bypass_unregister_producer(&vdev->ctx[vector].producer);
 
@@ -313,6 +349,16 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 	if (fd < 0)
 		return 0;
 
+	/*
+	 * struct vfio_pci_core_device *vdev:
+	 * -> struct vfio_pci_irq_ctx *ctx;
+	 *    -> struct eventfd_ctx      *trigger;
+	 *    -> struct virqfd           *unmask;
+	 *    -> struct virqfd           *mask;
+	 *    -> char                    *name;     
+	 *    -> bool                    masked;
+	 *    -> struct irq_bypass_producer      producer;
+	 */
 	vdev->ctx[vector].name = kasprintf(GFP_KERNEL, "vfio-msi%s[%d](%s)",
 					   msix ? "x" : "", vector,
 					   pci_name(pdev));
@@ -351,6 +397,14 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 
 	vdev->ctx[vector].producer.token = trigger;
 	vdev->ctx[vector].producer.irq = irq;
+	/*
+	 * 在以下调用irq_bypass_register_producer():
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|354| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&vdev->ctx[vector].producer);
+	 *   - drivers/vhost/vdpa.c|196| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer);
+	 *
+	 * 在以下调用irq_bypass_register_consumer():
+	 *   - virt/kvm/eventfd.c|426| <<kvm_irqfd_assign>> ret = irq_bypass_register_consumer(&irqfd->consumer);
+	 */
 	ret = irq_bypass_register_producer(&vdev->ctx[vector].producer);
 	if (unlikely(ret)) {
 		dev_info(&pdev->dev,
@@ -364,6 +418,12 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|399| <<vfio_msi_disable>> vfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|530| <<vfio_pci_set_msi_trigger>> return vfio_msi_set_block(vdev, start, count, fds, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|537| <<vfio_pci_set_msi_trigger>> ret = vfio_msi_set_block(vdev, start, count, fds, msix);
+ */
 static int vfio_msi_set_block(struct vfio_pci_core_device *vdev, unsigned start,
 			      unsigned count, int32_t *fds, bool msix)
 {
@@ -385,6 +445,11 @@ static int vfio_msi_set_block(struct vfio_pci_core_device *vdev, unsigned start,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|555| <<vfio_pci_set_msi_trigger>> vfio_msi_disable(vdev, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|587| <<vfio_pci_set_msi_trigger>> vfio_msi_disable(vdev, msix);
+ */
 static void vfio_msi_disable(struct vfio_pci_core_device *vdev, bool msix)
 {
 	struct pci_dev *pdev = vdev->pdev;
@@ -410,6 +475,13 @@ static void vfio_msi_disable(struct vfio_pci_core_device *vdev, bool msix)
 		pci_intx(pdev, 0);
 
 	vdev->irq_type = VFIO_PCI_NUM_IRQS;
+	/*
+	 * 在以下设置vfio_pci_core_device->num_ctx:
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|154| <<vfio_intx_enable>> vdev->num_ctx = 1;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|232| <<vfio_intx_disable>> vdev->num_ctx = 0;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|277| <<vfio_msi_enable>> vdev->num_ctx = nvec;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|446| <<vfio_msi_disable>> vdev->num_ctx = 0;
+	 */
 	vdev->num_ctx = 0;
 	kfree(vdev->ctx);
 }
@@ -507,6 +579,10 @@ static int vfio_pci_set_intx_trigger(struct vfio_pci_core_device *vdev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|668| <<vfio_pci_set_irqs_ioctl(VFIO_IRQ_SET_ACTION_TRIGGER)>> func = vfio_pci_set_msi_trigger;
+ */
 static int vfio_pci_set_msi_trigger(struct vfio_pci_core_device *vdev,
 				    unsigned index, unsigned start,
 				    unsigned count, uint32_t flags, void *data)
@@ -522,18 +598,44 @@ static int vfio_pci_set_msi_trigger(struct vfio_pci_core_device *vdev,
 	if (!(irq_is(vdev, index) || is_irq_none(vdev)))
 		return -EINVAL;
 
+	/*
+	 * 相关注释.
+	 * DATA_EVENTFD binds the specified ACTION to the provided __s32 eventfd.
+	 * A value of -1 can be used to either de-assign interrupts if already
+	 * assigned or skip un-assigned interrupts.  For example, to set an eventfd
+	 * to be trigger for interrupts [0,0] and [0,2]:
+	 * flags = (DATA_EVENTFD|ACTION_TRIGGER), index = 0, start = 0, count = 3,
+	 * data = {fd1, -1, fd2}
+	 * If index [0,1] is previously set, two count = 1 ioctls calls would be
+	 * required to set [0,0] and [0,2] without changing [0,1].
+	 */
 	if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {
 		int32_t *fds = data;
 		int ret;
 
+		/*
+		 * called by:
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|399| <<vfio_msi_disable>> vfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|530| <<vfio_pci_set_msi_trigger>> return vfio_msi_set_block(vdev, start, count, fds, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|537| <<vfio_pci_set_msi_trigger>> ret = vfio_msi_set_block(vdev, start, count, fds, msix);
+		 */
 		if (vdev->irq_type == index)
 			return vfio_msi_set_block(vdev, start, count,
 						  fds, msix);
 
+		/*
+		 * 只在这里调用
+		 */
 		ret = vfio_msi_enable(vdev, start + count, msix);
 		if (ret)
 			return ret;
 
+		/*
+		 * called by:
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|399| <<vfio_msi_disable>> vfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|530| <<vfio_pci_set_msi_trigger>> return vfio_msi_set_block(vdev, start, count, fds, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|537| <<vfio_pci_set_msi_trigger>> ret = vfio_msi_set_block(vdev, start, count, fds, msix);
+		 */
 		ret = vfio_msi_set_block(vdev, start, count, fds, msix);
 		if (ret)
 			vfio_msi_disable(vdev, msix);
@@ -558,6 +660,13 @@ static int vfio_pci_set_msi_trigger(struct vfio_pci_core_device *vdev,
 	return 0;
 }
 
+/*
+ * called by:
+ *  - drivers/vfio/pci/vfio_pci_intrs.c|671| <<vfio_pci_set_err_trigger>> return vfio_pci_set_ctx_trigger_single(&vdev->err_trigger, count, flags, data);
+ *  - drivers/vfio/pci/vfio_pci_intrs.c|682| <<vfio_pci_set_req_trigger>> return vfio_pci_set_ctx_trigger_single(&vdev->req_trigger, count, flags, data);
+ *
+ * 全是关于eventfd的初始化和trigger的
+ */
 static int vfio_pci_set_ctx_trigger_single(struct eventfd_ctx **ctx,
 					   unsigned int count, uint32_t flags,
 					   void *data)
@@ -613,6 +722,10 @@ static int vfio_pci_set_ctx_trigger_single(struct eventfd_ctx **ctx,
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|784| <<vfio_pci_set_irqs_ioctl(VFIO_PCI_ERR_IRQ_INDEX)>> func = vfio_pci_set_err_trigger;
+ */
 static int vfio_pci_set_err_trigger(struct vfio_pci_core_device *vdev,
 				    unsigned index, unsigned start,
 				    unsigned count, uint32_t flags, void *data)
@@ -624,6 +737,10 @@ static int vfio_pci_set_err_trigger(struct vfio_pci_core_device *vdev,
 					       count, flags, data);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|791| <<vfio_pci_set_irqs_ioctl(VFIO_PCI_REQ_IRQ_INDEX)>> func = vfio_pci_set_req_trigger;
+ */
 static int vfio_pci_set_req_trigger(struct vfio_pci_core_device *vdev,
 				    unsigned index, unsigned start,
 				    unsigned count, uint32_t flags, void *data)
@@ -635,6 +752,11 @@ static int vfio_pci_set_req_trigger(struct vfio_pci_core_device *vdev,
 					       count, flags, data);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_core.c|386| <<vfio_pci_core_disable>> vfio_pci_set_irqs_ioctl(vdev, VFIO_IRQ_SET_DATA_NONE |
+ *   - drivers/vfio/pci/vfio_pci_core.c|947| <<vfio_pci_core_ioctl(VFIO_DEVICE_SET_IRQS)>> ret = vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index,
+ */
 int vfio_pci_set_irqs_ioctl(struct vfio_pci_core_device *vdev, uint32_t flags,
 			    unsigned index, unsigned start, unsigned count,
 			    void *data)
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 68e4ecd1c..28f0f5a7d 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -958,6 +958,11 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1260| <<handle_tx_kick>> handle_tx(net);
+ *   - drivers/vhost/net.c|1276| <<handle_tx_net>> handle_tx(net);
+ */
 static void handle_tx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 7ebf106d5..4af604558 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -76,6 +76,19 @@ struct vhost_scsi_cmd {
 	/* Saved unpacked SCSI LUN for vhost_scsi_target_queue_cmd() */
 	u32 tvc_lun;
 	/* Pointer to the SGL formatted memory from virtio-scsi */
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|437| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|768| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|772| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|909| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|912| <<vhost_scsi_mapal>> pr_debug("%s data_sg %p data_sgl_count %u\n", __func__, cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|915| <<vhost_scsi_mapal>> ret = vhost_scsi_iov_to_sgl(cmd, write, data_iter, cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|952| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1659| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|1714| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|1717| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	struct scatterlist *tvc_sgl;
 	struct scatterlist *tvc_prot_sgl;
 	struct page **tvc_upages;
@@ -179,8 +192,37 @@ struct vhost_scsi_virtqueue {
 	 * Writers must also take dev mutex and flush under it.
 	 */
 	int inflight_idx;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|639| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|1482| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|1486| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|1494| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|1495| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|1505| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|1513| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1514| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|1520| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	struct vhost_scsi_cmd *scsi_cmds;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	struct sbitmap scsi_tags;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->max_cmds:
+	 *   - drivers/vhost/scsi.c|1546| <<vhost_scsi_destroy_vq_cmds>> for (i = 0; i < svq->max_cmds; i++) {
+	 *   - drivers/vhost/scsi.c|1563| <<vhost_scsi_setup_vq_cmds>> static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1576| <<vhost_scsi_setup_vq_cmds>> svq->max_cmds = max_cmds;
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_setup_vq_cmds>> for (i = 0; i < max_cmds; i++) {
+	 */
 	int max_cmds;
 };
 
@@ -191,16 +233,67 @@ struct vhost_scsi {
 
 	struct vhost_dev dev;
 	struct vhost_scsi_virtqueue *vqs;
+	/*
+	 * 在以下使用vhost_scsi->compl_bitmap:
+	 *   - drivers/vhost/scsi.c|549| <<vhost_scsi_complete_cmd_work>> bitmap_zero(vs->compl_bitmap, vs->dev.nvqs);
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_complete_cmd_work>> __set_bit(vq, vs->compl_bitmap);
+	 *   - drivers/vhost/scsi.c|582| <<vhost_scsi_complete_cmd_work>> while ((vq = find_next_bit(vs->compl_bitmap, vs->dev.nvqs, vq + 1))
+	 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_open>> vs->compl_bitmap = bitmap_alloc(nvqs, GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_open>> if (!vs->compl_bitmap)
+	 *   - drivers/vhost/scsi.c|1844| <<vhost_scsi_open>> bitmap_free(vs->compl_bitmap);
+	 *   - drivers/vhost/scsi.c|1865| <<vhost_scsi_release>> bitmap_free(vs->compl_bitmap);
+	 *
+	 * 和number of queues有关
+	 */
 	unsigned long *compl_bitmap;
+	/*
+	 * 在以下使用vhost_scsi->old_inflight:
+	 *   - drivers/vhost/scsi.c|270| <<vhost_scsi_init_inflight>> struct vhost_scsi_inflight *old_inflight[])
+	 *   - drivers/vhost/scsi.c|283| <<vhost_scsi_init_inflight>> if (old_inflight)
+	 *   - drivers/vhost/scsi.c|284| <<vhost_scsi_init_inflight>> old_inflight[i] = &vs->vqs[i].inflights[idx];
+	 *   - drivers/vhost/scsi.c|1483| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+	 *   - drivers/vhost/scsi.c|1491| <<vhost_scsi_flush>> kref_put(&vs->old_inflight[i]->kref, vhost_scsi_done_inflight);
+	 *   - drivers/vhost/scsi.c|1498| <<vhost_scsi_flush>> wait_for_completion(&vs->old_inflight[i]->comp);
+	 *   - drivers/vhost/scsi.c|1856| <<vhost_scsi_open>> vs->old_inflight = kmalloc_array(nvqs, sizeof(*vs->old_inflight),
+	 *   - drivers/vhost/scsi.c|1858| <<vhost_scsi_open>> if (!vs->old_inflight)
+	 *   - drivers/vhost/scsi.c|1895| <<vhost_scsi_open>> kfree(vs->old_inflight);
+	 *   - drivers/vhost/scsi.c|1917| <<vhost_scsi_release>> kfree(vs->old_inflight);
+	 *
+	 * 二维数组, 每个queue一个*old_inflight.
+	 */
 	struct vhost_scsi_inflight **old_inflight;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_completion_work:
+	 *   - drivers/vhost/scsi.c|464| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+	 *   - drivers/vhost/scsi.c|628| <<vhost_scsi_complete_cmd_work>> vs_completion_work);
+	 *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 */
 	struct vhost_work vs_completion_work; /* cmd completion work item */
+	/*
+	 * 在以下使用vhost_scsi->vs_completion_list:
+	 *   - drivers/vhost/scsi.c|416| <<vhost_scsi_release_cmd>> llist_add(&cmd->tvc_completion_list, &vs->vs_completion_list);
+	 *   - drivers/vhost/scsi.c|590| <<vhost_scsi_complete_cmd_work>> llnode = llist_del_all(&vs->vs_completion_list);
+	 */
 	struct llist_head vs_completion_list; /* cmd completion queue */
 
 	struct vhost_work vs_event_work; /* evt injection work item */
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_evt_work>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1461| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	struct llist_head vs_event_list; /* evt injection queue */
 
 	bool vs_events_missed; /* any missed events, protected by vq->mutex */
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|1807| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|1886| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	int vs_events_nr; /* num of pending events, protected by vq->mutex */
 };
 
@@ -271,6 +364,11 @@ static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|654| <<vhost_scsi_get_cmd>> cmd->inflight = vhost_scsi_get_inflight(vq);
+ *   - drivers/vhost/scsi.c|1238| <<vhost_scsi_handle_tmf>> tmf->inflight = vhost_scsi_get_inflight(vq);
+ */
 static struct vhost_scsi_inflight *
 vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 {
@@ -284,6 +382,11 @@ vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|437| <<vhost_scsi_release_cmd_res>> vhost_scsi_put_inflight(inflight);
+ *   - drivers/vhost/scsi.c|448| <<vhost_scsi_release_tmf_res>> vhost_scsi_put_inflight(inflight);
+ */
 static void vhost_scsi_put_inflight(struct vhost_scsi_inflight *inflight)
 {
 	kref_put(&inflight->kref, vhost_scsi_done_inflight);
@@ -328,6 +431,11 @@ static u32 vhost_scsi_tpg_get_inst_index(struct se_portal_group *se_tpg)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|682| <<vhost_scsi_complete_cmd_work>> vhost_scsi_release_cmd_res(se_cmd);
+ *   - drivers/vhost/scsi.c|1233| <<vhost_scsi_handle_vq>> vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
+ */
 static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *tv_cmd = container_of(se_cmd,
@@ -346,10 +454,23 @@ static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 			put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
 	}
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * 在以下使用vhost_scsi_release_tmf_res():
+ *   - drivers/vhost/scsi.c|1330| <<vhost_scsi_tmf_resp_work>> vhost_scsi_release_tmf_res(tmf);
+ *   - drivers/vhost/scsi.c|1376| <<vhost_scsi_handle_tmf>> vhost_scsi_release_tmf_res(tmf);
+ */
 static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 {
 	struct vhost_scsi_tpg *tpg = tmf->tpg;
@@ -361,6 +482,9 @@ static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.release_cmd = vhost_scsi_release_cmd()
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	if (se_cmd->se_cmd_flags & SCF_SCSI_TMR_CDB) {
@@ -374,6 +498,12 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 		struct vhost_scsi *vs = cmd->tvc_vhost;
 
 		llist_add(&cmd->tvc_completion_list, &vs->vs_completion_list);
+		/*
+		 * 在以下使用vhost_scsi->vs_completion_work:
+		 *   - drivers/vhost/scsi.c|464| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+		 *   - drivers/vhost/scsi.c|628| <<vhost_scsi_complete_cmd_work>> vs_completion_work);
+		 *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+		 */
 		vhost_work_queue(&vs->dev, &vs->vs_completion_work);
 	}
 }
@@ -535,6 +665,15 @@ static void vhost_scsi_evt_work(struct vhost_work *work)
  * This is scheduled in the vhost work queue so we are called with the owner
  * process mm and can access the vring.
  */
+/*
+ * 在以下使用vhost_scsi->vs_completion_work:
+ *   - drivers/vhost/scsi.c|464| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|628| <<vhost_scsi_complete_cmd_work>> vs_completion_work);
+ *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *
+ * 在以下使用vhost_scsi_complete_cmd_work():
+ *   - drivers/vhost/scsi.c|1967| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ */
 static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 {
 	struct vhost_scsi *vs = container_of(work, struct vhost_scsi,
@@ -568,9 +707,26 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		ret = copy_to_iter(&v_rsp, sizeof(v_rsp), &iov_iter);
 		if (likely(ret == sizeof(v_rsp))) {
 			struct vhost_scsi_virtqueue *q;
+			/*
+			 * called by:
+			 *   - drivers/vhost/scsi.c|571| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+			 *   - drivers/vhost/vhost.c|2538| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+			 *   - drivers/vhost/vsock.c|222| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+			 *   - drivers/vhost/vsock.c|554| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+			 */
 			vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
 			q = container_of(cmd->tvc_vq, struct vhost_scsi_virtqueue, vq);
 			vq = q - vs->vqs;
+			/*
+			 * 在以下使用vhost_scsi->compl_bitmap:
+			 *   - drivers/vhost/scsi.c|549| <<vhost_scsi_complete_cmd_work>> bitmap_zero(vs->compl_bitmap, vs->dev.nvqs);
+			 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_complete_cmd_work>> __set_bit(vq, vs->compl_bitmap);
+			 *   - drivers/vhost/scsi.c|582| <<vhost_scsi_complete_cmd_work>> while ((vq = find_next_bit(vs->compl_bitmap, vs->dev.nvqs, vq + 1))
+			 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_open>> vs->compl_bitmap = bitmap_alloc(nvqs, GFP_KERNEL);
+			 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_open>> if (!vs->compl_bitmap)
+			 *   - drivers/vhost/scsi.c|1844| <<vhost_scsi_open>> bitmap_free(vs->compl_bitmap);
+			 *   - drivers/vhost/scsi.c|1865| <<vhost_scsi_release>> bitmap_free(vs->compl_bitmap);
+			 */
 			__set_bit(vq, vs->compl_bitmap);
 		} else
 			pr_err("Faulted on virtio_scsi_cmd_resp\n");
@@ -584,6 +740,10 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_signal(&vs->dev, &vs->vqs[vq].vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1169| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -603,6 +763,14 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		return ERR_PTR(-EIO);
 	}
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	tag = sbitmap_get(&svq->scsi_tags);
 	if (tag < 0) {
 		pr_err("Unable to obtain tag for vhost_scsi_cmd\n");
@@ -636,12 +804,22 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
  *
  * Returns the number of scatterlist entries used or -errno on error.
  */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|850| <<vhost_scsi_iov_to_sgl>> ret = vhost_scsi_map_to_sgl(cmd, iter, sg, write);
+ */
 static int
 vhost_scsi_map_to_sgl(struct vhost_scsi_cmd *cmd,
 		      struct iov_iter *iter,
 		      struct scatterlist *sgl,
 		      bool write)
 {
+	/*
+	 * struct vhost_scsi_cmd *cmd:
+	 * -> struct scatterlist *tvc_sgl;
+	 * -> struct scatterlist *tvc_prot_sgl;
+	 * -> struct page **tvc_upages;
+	 */
 	struct page **pages = cmd->tvc_upages;
 	struct scatterlist *sg = sgl;
 	ssize_t bytes;
@@ -656,6 +834,20 @@ vhost_scsi_map_to_sgl(struct vhost_scsi_cmd *cmd,
 
 	while (bytes) {
 		unsigned n = min_t(unsigned, PAGE_SIZE - offset, bytes);
+		/*
+		 * struct scatterlist {
+		 *     unsigned long   page_link;
+		 *     unsigned int    offset;
+		 *     unsigned int    length;
+		 *     dma_addr_t      dma_address;
+		 * #ifdef CONFIG_NEED_SG_DMA_LENGTH
+		 *     unsigned int    dma_length;
+		 * #endif
+		 * #ifdef CONFIG_PCI_P2PDMA
+		 *     unsigned int    dma_flags;
+		 * #endif
+		 * };
+		 */
 		sg_set_page(sg++, pages[npages++], n, offset);
 		bytes -= n;
 		offset = 0;
@@ -683,6 +875,11 @@ vhost_scsi_calc_sgls(struct iov_iter *iter, size_t bytes, int max_sgls)
 	return sgl_count;
 }
 
+/*
+ * caled by:
+ *   - drivers/vhost/scsi.c|883| <<vhost_scsi_mapal>> ret = vhost_scsi_iov_to_sgl(cmd, write, prot_iter,
+ *   - drivers/vhost/scsi.c|901| <<vhost_scsi_mapal>> ret = vhost_scsi_iov_to_sgl(cmd, write, data_iter,
+ */
 static int
 vhost_scsi_iov_to_sgl(struct vhost_scsi_cmd *cmd, bool write,
 		      struct iov_iter *iter,
@@ -706,6 +903,10 @@ vhost_scsi_iov_to_sgl(struct vhost_scsi_cmd *cmd, bool write,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1272| <<vhost_scsi_handle_vq>> if (unlikely(vhost_scsi_mapal(cmd, prot_bytes,
+ */
 static int
 vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 		 size_t prot_bytes, struct iov_iter *prot_iter,
@@ -733,6 +934,9 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 			return ret;
 		}
 	}
+	/*
+	 * data_bytes用不到
+	 */
 	sgl_count = vhost_scsi_calc_sgls(data_iter, data_bytes,
 					 VHOST_SCSI_PREALLOC_SGLS);
 	if (sgl_count < 0)
@@ -743,6 +947,11 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 	pr_debug("%s data_sg %p data_sgl_count %u\n", __func__,
 		  cmd->tvc_sgl, cmd->tvc_sgl_count);
 
+	/*
+	 * caled by:
+	 *   - drivers/vhost/scsi.c|883| <<vhost_scsi_mapal>> ret = vhost_scsi_iov_to_sgl(cmd, write, prot_iter,
+	 *   - drivers/vhost/scsi.c|901| <<vhost_scsi_mapal>> ret = vhost_scsi_iov_to_sgl(cmd, write, data_iter,
+	 */
 	ret = vhost_scsi_iov_to_sgl(cmd, write, data_iter,
 				    cmd->tvc_sgl, cmd->tvc_sgl_count);
 	if (ret < 0) {
@@ -769,6 +978,10 @@ static int vhost_scsi_to_tcm_attr(int attr)
 	return TCM_SIMPLE_TAG;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1282| <<vhost_scsi_handle_vq>> vhost_scsi_target_queue_cmd(cmd);
+ */
 static void vhost_scsi_target_queue_cmd(struct vhost_scsi_cmd *cmd)
 {
 	struct se_cmd *se_cmd = &cmd->tvc_se_cmd;
@@ -827,6 +1040,20 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 {
 	int ret = -ENXIO;
 
+	/*
+	 * struct vhost_virtqueue *vq:
+	 * -> struct iovec iov[UIO_MAXIOV];
+	 *
+	 * struct vhost_scsi_ctx {
+	 *     int head;
+	 *     unsigned int out, in;
+	 *     size_t req_size, rsp_size;
+	 *     size_t out_size, in_size;
+	 *     u8 *target, *lunp;
+	 *     void *req;
+	 *     struct iov_iter out_iter;
+	 * };
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -851,6 +1078,9 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	 * Get the size of request and response buffers.
 	 * FIXME: Not correct for BIDI operation
 	 */
+	/*
+	 * 加起来的和
+	 */
 	vc->out_size = iov_length(vq->iov, vc->out);
 	vc->in_size = iov_length(&vq->iov[vc->out], vc->in);
 
@@ -889,12 +1119,30 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1015| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1362| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
 {
 	int ret = -EIO;
 
+	/*
+	 * struct vhost_scsi_ctx {
+	 *     int head;
+	 *     unsigned int out, in;
+	 *     size_t req_size, rsp_size;
+	 *     size_t out_size, in_size;
+	 *     u8 *target, *lunp;
+	 *     void *req;
+	 *     struct iov_iter out_iter;
+	 * };
+	 *
+	 * 只拷贝req_size
+	 */
 	if (unlikely(!copy_from_iter_full(vc->req, vc->req_size,
 					  &vc->out_iter))) {
 		vq_err(vq, "Faulted on copy_from_iter_full\n");
@@ -924,6 +1172,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1509| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -946,6 +1198,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	 * We can handle the vq only after the endpoint is setup by calling the
 	 * VHOST_SCSI_SET_ENDPOINT ioctl.
 	 */
+	/*
+	 * struct vhost_scsi_tpg **vs_tpg, *tpg
+	 */
 	vs_tpg = vhost_vq_get_backend(vq);
 	if (!vs_tpg)
 		goto out;
@@ -956,6 +1211,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	do {
+		/*
+		 * 一个request处理完了
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
@@ -971,6 +1229,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			vc.target = &v_req_pi.lun[1];
 		} else {
 			vc.req = &v_req;
+			/*
+			 * struct virtio_scsi_cmd_req v_req;
+			 */
 			vc.req_size = sizeof(v_req);
 			vc.lunp = &v_req.lun[0];
 			vc.target = &v_req.lun[1];
@@ -985,6 +1246,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		if (ret)
 			goto err;
 
+		/*
+		 * vhost_scsi_ctx->req拷贝了req_size
+		 */
 		ret = vhost_scsi_get_req(vq, &vc, &tpg);
 		if (ret)
 			goto err;
@@ -1155,6 +1419,10 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		pr_err("Faulted on virtio_scsi_ctrl_tmf_resp\n");
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf_resp_work():
+ *   - drivers/vhost/scsi.c|2212| <<vhost_scsi_port_link>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ */
 static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
@@ -1171,6 +1439,10 @@ static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 	vhost_scsi_release_tmf_res(tmf);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1517| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_handle_tmf(vs, tpg, vq, &v_req.tmf, &vc);
+ */
 static void
 vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 		      struct vhost_virtqueue *vq,
@@ -1208,6 +1480,11 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 	tmf->resp_iov = vq->iov[vc->out];
 	tmf->vq_desc = vc->head;
 	tmf->in_iovs = vc->in;
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|654| <<vhost_scsi_get_cmd>> cmd->inflight = vhost_scsi_get_inflight(vq);
+	 *   - drivers/vhost/scsi.c|1238| <<vhost_scsi_handle_tmf>> tmf->inflight = vhost_scsi_get_inflight(vq);
+	 */
 	tmf->inflight = vhost_scsi_get_inflight(vq);
 
 	if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
@@ -1247,6 +1524,10 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_ctrl_an_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1393| <<vhost_scsi_ctl_handle_kick>> vhost_scsi_ctl_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1468,6 +1749,10 @@ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 	svq->scsi_cmds = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1679| <<vhost_scsi_set_endpoint>> ret = vhost_scsi_setup_vq_cmds(vq, vq->num);
+ */
 static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1478,9 +1763,26 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 	if (svq->scsi_cmds)
 		return 0;
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|411| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|685| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1554| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
 			      NUMA_NO_NODE, false, true))
 		return -ENOMEM;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->max_cmds:
+	 *   - drivers/vhost/scsi.c|1546| <<vhost_scsi_destroy_vq_cmds>> for (i = 0; i < svq->max_cmds; i++) {
+	 *   - drivers/vhost/scsi.c|1563| <<vhost_scsi_setup_vq_cmds>> static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
+	 *   - drivers/vhost/scsi.c|1573| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
+	 *   - drivers/vhost/scsi.c|1576| <<vhost_scsi_setup_vq_cmds>> svq->max_cmds = max_cmds;
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_setup_vq_cmds>> for (i = 0; i < max_cmds; i++) {
+	 */
 	svq->max_cmds = max_cmds;
 
 	svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 40097826c..da09e3537 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -46,6 +46,11 @@ enum {
 	VHOST_MEMORY_F_LOG = 0x1,
 };
 
+/*
+ * VIRTIO_RING_F_EVENT_IDX:
+ *   后端用了used ring的最后一个元素,告诉前端驱动后端处理到哪个avail ring上的元素了,
+ *   同时前端使用avail ring的最后一个元素告诉后端,处理到那个used ring了
+ */
 #define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])
 #define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])
 
@@ -302,6 +307,11 @@ bool vhost_vq_is_setup(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_is_setup);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|501| <<vhost_dev_init>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|699| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -680,6 +690,15 @@ static void vhost_clear_msg(struct vhost_dev *dev)
 	spin_unlock(&dev->iotlb_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1402| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+ *   - drivers/vhost/test.c|160| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/vdpa.c|1165| <<vhost_vdpa_cleanup>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vhost.c|645| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+ *   - drivers/vhost/vsock.c|774| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev)
 {
 	int i;
@@ -1026,6 +1045,10 @@ static inline int vhost_get_avail_flags(struct vhost_virtqueue *vq,
 	return vhost_get_avail(vq, *flags, &vq->avail->flags);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2563| <<vhost_notify>> if (vhost_get_used_event(vq, &event)) {
+ */
 static inline int vhost_get_used_event(struct vhost_virtqueue *vq,
 				       __virtio16 *event)
 {
@@ -2011,6 +2034,14 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1537| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|1622| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|197| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/test.c|290| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/vsock.c|600| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -2286,6 +2317,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			       i, vq->num, head);
 			return -EINVAL;
 		}
+		/*
+		 * struct vring_desc desc;
+		 */
 		ret = vhost_get_desc(vq, &desc, i);
 		if (unlikely(ret)) {
 			vq_err(vq, "Failed to get descriptor: idx %d addr %p\n",
@@ -2358,6 +2392,13 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|571| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2538| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vsock.c|222| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|554| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2369,6 +2410,11 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2414| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|2420| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2390,12 +2436,39 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|968| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2048| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2393| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2406| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2407| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2511| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->last_used_idx;
 	new = (vq->last_used_idx += count);
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2028| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2466| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	/* If the driver never bothers to signal in a very long while,
 	 * used index might wrap around. If that happens, invalidate
 	 * signalled_used index we stored. TODO: make sure driver
 	 * signals at least once in 2^16 and remove this. */
+	/*
+	 * 下面这个也不可能成立
+	 * new = 2
+	 * vq->signalled_used = 65530
+	 * old = 65533
+	 *
+	 * new - old 永远都是count !!!
+	 */
 	if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
 		vq->signalled_used_valid = false;
 	return 0;
@@ -2403,6 +2476,12 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2368| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|2502| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ *   - drivers/vhost/vhost.h|194| <<vhost_add_used_and_signal_n>> int vhost_add_used_n(struct vhost_virtqueue *, struct vring_used_elem *heads,
+ */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		     unsigned count)
 {
@@ -2438,6 +2517,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2482| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2458,24 +2541,77 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 			vq_err(vq, "Failed to get flags");
 			return true;
 		}
+		/*
+		 * 如果设置了VRING_AVAIL_F_NO_INTERRUPT, vhost_notify()返回false
+		 */
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->vhost_signal:
+	 *   - drivers/vhost/vhost.c|315| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2399| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2463| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2465| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->signalled_used;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2028| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2466| <<vhost_notify>> vq->signalled_used_valid = true;
+	 *
+	 * 除了初始化,
+	 * If the driver never bothers to signal in a very long while,
+	 * used index might wrap around. If that happens, invalidate
+	 * signalled_used index we stored. TODO: make sure driver
+	 * signals at least once in 2^16 and remove this.
+	 */
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|968| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2048| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2393| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2406| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2407| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2511| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
 	if (unlikely(!v))
 		return true;
 
+	/*
+	 * VIRTIO_RING_F_EVENT_IDX:
+	 *   后端用了used ring的最后一个元素,告诉前端驱动后端处理到哪个avail ring上的元素了,
+	 *   同时前端使用avail ring的最后一个元素告诉后端,处理到那个used ring了
+	 */
 	if (vhost_get_used_event(vq, &event)) {
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
+	 *
+	 * new - event <= new - old
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|584| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2539| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2549| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|260| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|560| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
@@ -2485,6 +2621,15 @@ void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_signal);
 
 /* And here's the combo meal deal.  Supersize me! */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|952| <<handle_tx_zerocopy>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|511| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|819| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1153| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+ *   - drivers/vhost/scsi.c|1245| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|87| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
@@ -2495,6 +2640,11 @@ void vhost_add_used_and_signal(struct vhost_dev *dev,
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
 /* multi-buffer version of vhost_add_used_and_signal */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|377| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|456| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+ */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index d9109107a..3b2b883d9 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -92,15 +92,41 @@ struct vhost_virtqueue {
 	u16 avail_idx;
 
 	/* Last index we used. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|314| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|968| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),
+	 *   - drivers/vhost/vhost.c|2048| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|2393| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2406| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|2407| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|2438| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|2511| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
 	/* Used flags */
 	u16 used_flags;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->vhost_signal:
+	 *   - drivers/vhost/vhost.c|315| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|2399| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|2463| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|2465| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 signalled_used;
 
 	/* Last used index value we have signalled on */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|316| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2028| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2400| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|2464| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|2466| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index 828ced060..1771c84c2 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -246,9 +246,29 @@ static int virtio_dev_probe(struct device *_d)
 	/* We have a driver! */
 	virtio_add_status(dev, VIRTIO_CONFIG_S_DRIVER);
 
+	/*
+	 * legacy: vp_get_features()
+	 * modern: vp_get_features()
+	 *
+	 * 关于modern, 写入device_feature_select, 读取device_feature
+	 */
 	/* Figure out what features the device supports. */
 	device_features = dev->config->get_features(dev);
 
+	/*
+	 * virtio-scsi的feature的例子
+	 * 969 static unsigned int features[] = {
+	 * 970         VIRTIO_SCSI_F_HOTPLUG,
+	 * 971         VIRTIO_SCSI_F_CHANGE,
+	 * 972 #ifdef CONFIG_BLK_DEV_INTEGRITY
+	 * 973         VIRTIO_SCSI_F_T10_PI,
+	 * 974 #endif
+	 * 975 };
+	 * 976
+	 * 977 static struct virtio_driver virtio_scsi_driver = {
+	 * 978         .feature_table = features,
+	 * 979         .feature_table_size = ARRAY_SIZE(features),
+	 */
 	/* Figure out what features the driver supports. */
 	driver_features = 0;
 	for (i = 0; i < drv->feature_table_size; i++) {
@@ -269,16 +289,29 @@ static int virtio_dev_probe(struct device *_d)
 		driver_features_legacy = driver_features;
 	}
 
+	/*
+	 * device_features是上面用vp_get_features()读取
+	 * 关于modern, 写入device_feature_select, 读取device_feature
+	 */
 	if (device_features & (1ULL << VIRTIO_F_VERSION_1))
 		dev->features = driver_features & device_features;
 	else
 		dev->features = driver_features_legacy & device_features;
 
+	/*
+	 * #define VIRTIO_TRANSPORT_F_START        28
+	 * #define VIRTIO_TRANSPORT_F_END          41
+	 * #define VIRTIO_F_VERSION_1              32
+	 */
 	/* Transport features always preserved to pass to finalize_features. */
 	for (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++)
 		if (device_features & (1ULL << i))
 			__virtio_set_bit(dev, i);
 
+	/*
+	 * legacy: vp_finalize_features()
+	 * modern: vp_finalize_features()
+	 */
 	err = dev->config->finalize_features(dev);
 	if (err)
 		goto err;
@@ -415,6 +448,16 @@ static int virtio_device_of_init(struct virtio_device *dev)
  *
  * Returns: 0 on suceess, -error on failure
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1218| <<virtio_uml_probe>> rc = register_virtio_device(&vu_dev->vdev);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1097| <<mlxbf_tmfifo_create_vdev>> ret = register_virtio_device(&tm_vdev->vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|430| <<rproc_add_virtio_dev>> ret = register_virtio_device(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1345| <<virtio_ccw_online>> ret = register_virtio_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|667| <<virtio_mmio_probe>> rc = register_virtio_device(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|559| <<virtio_pci_probe>> rc = register_virtio_device(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|379| <<virtio_vdpa_probe>> ret = register_virtio_device(&vd_dev->vdev);
+ */
 int register_virtio_device(struct virtio_device *dev)
 {
 	int err;
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index 3f78a3a1e..ad9f21a05 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -447,6 +447,13 @@ static void virtballoon_changed(struct virtio_device *vdev)
 	spin_unlock_irqrestore(&vb->stop_update_lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_balloon.c|484| <<update_balloon_size_func>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|843| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1015| <<remove_common>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1078| <<virtballoon_restore>> update_balloon_size(vb);
+ */
 static void update_balloon_size(struct virtio_balloon *vb)
 {
 	u32 actual = vb->num_pages;
diff --git a/drivers/virtio/virtio_pci_common.c b/drivers/virtio/virtio_pci_common.c
index ad258a9d3..97299b608 100644
--- a/drivers/virtio/virtio_pci_common.c
+++ b/drivers/virtio/virtio_pci_common.c
@@ -16,6 +16,13 @@
 
 #include "virtio_pci_common.h"
 
+/*
+ * 在以下使用force_legacy:
+ *   - drivers/virtio/virtio_pci_common.c|19| <<global>> static bool force_legacy = false;
+ *   - drivers/virtio/virtio_pci_common.c|22| <<global>> module_param(force_legacy, bool, 0444);
+ *   - drivers/virtio/virtio_pci_common.c|23| <<global>> MODULE_PARM_DESC(force_legacy,
+ *   - drivers/virtio/virtio_pci_common.c|540| <<virtio_pci_probe>> if (force_legacy) {
+ */
 static bool force_legacy = false;
 
 #if IS_ENABLED(CONFIG_VIRTIO_PCI_LEGACY)
@@ -537,6 +544,13 @@ static int virtio_pci_probe(struct pci_dev *pci_dev,
 	if (rc)
 		goto err_enable_device;
 
+	/*
+	 * 在以下使用force_legacy:
+	 *   - drivers/virtio/virtio_pci_common.c|19| <<global>> static bool force_legacy = false;
+	 *   - divers/virtio/virtio_pci_common.c|22| <<global>> module_param(force_legacy, bool, 0444);
+	 *   - drivers/virtio/virtio_pci_common.c|23| <<global>> MODULE_PARM_DESC(force_legacy,
+	 *   - drivers/virtio/virtio_pci_common.c|540| <<virtio_pci_probe>> if (force_legacy) {
+	 */
 	if (force_legacy) {
 		rc = virtio_pci_legacy_probe(vp_dev);
 		/* Also try modern mode if we can't map BAR0 (no IO space). */
@@ -554,6 +568,13 @@ static int virtio_pci_probe(struct pci_dev *pci_dev,
 
 	pci_set_master(pci_dev);
 
+	/*
+	 * struct virtio_pci_device *vp_dev:
+	 * -> struct virtio_pci_legacy_device ldev;
+	 *    -> u8 __iomem *isr;
+	 *    -> void __iomem *ioaddr;
+	 * -> struct virtio_pci_modern_device mdev;
+	 */
 	vp_dev->is_legacy = vp_dev->ldev.ioaddr ? true : false;
 
 	rc = register_virtio_device(&vp_dev->vdev);
@@ -603,6 +624,25 @@ static void virtio_pci_remove(struct pci_dev *pci_dev)
 	put_device(dev);
 }
 
+/*
+ * commit cfecc2918d2b3c5e86ff1a6c95eabbbb17bb8fd3
+ * Author: Tiwei Bie <tiwei.bie@intel.com>
+ * Date:   Fri Jun 1 12:02:39 2018 +0800
+ *
+ * virtio_pci: support enabling VFs
+ *
+ * There is a new feature bit allocated in virtio spec to
+ * support SR-IOV (Single Root I/O Virtualization):
+ *
+ * https://github.com/oasis-tcs/virtio-spec/issues/11
+ *
+ * This patch enables the support for this feature bit in
+ * virtio driver.
+ *
+ * Signed-off-by: Tiwei Bie <tiwei.bie@intel.com>
+ * Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
+ */
+
 static int virtio_pci_sriov_configure(struct pci_dev *pci_dev, int num_vfs)
 {
 	struct virtio_pci_device *vp_dev = pci_get_drvdata(pci_dev);
diff --git a/drivers/virtio/virtio_pci_modern.c b/drivers/virtio/virtio_pci_modern.c
index c3b9f2761..6892dc5b2 100644
--- a/drivers/virtio/virtio_pci_modern.c
+++ b/drivers/virtio/virtio_pci_modern.c
@@ -26,6 +26,10 @@ static u64 vp_get_features(struct virtio_device *vdev)
 	return vp_modern_get_features(&vp_dev->mdev);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|52| <<vp_finalize_features>> vp_transport_features(vdev, features);
+ */
 static void vp_transport_features(struct virtio_device *vdev, u64 features)
 {
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
@@ -45,6 +49,15 @@ static int vp_finalize_features(struct virtio_device *vdev)
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
 	u64 features = vdev->features;
 
+	/*
+	 * 清空virtio_device->features中在VIRTIO_TRANSPORT_F_START->VIRTIO_TRANSPORT_F_END并且不在以下的bit
+	 * - VIRTIO_RING_F_INDIRECT_DESC
+	 * - VIRTIO_RING_F_EVENT_IDX
+	 * - VIRTIO_F_VERSION_1:
+	 * - VIRTIO_F_ACCESS_PLATFORM:
+	 * - VIRTIO_F_RING_PACKED:
+	 * - VIRTIO_F_ORDER_PLATFORM:
+	 */
 	/* Give virtio_ring a chance to accept features. */
 	vring_transport_features(vdev);
 
diff --git a/drivers/virtio/virtio_pci_modern_dev.c b/drivers/virtio/virtio_pci_modern_dev.c
index 869cb46be..486067f8b 100644
--- a/drivers/virtio/virtio_pci_modern_dev.c
+++ b/drivers/virtio/virtio_pci_modern_dev.c
@@ -111,6 +111,13 @@ vp_modern_map_capability(struct virtio_pci_modern_device *mdev, int off,
  *
  * Returns offset of the capability, or 0.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern_dev.c|240| <<vp_modern_probe>> common = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_COMMON_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|250| <<vp_modern_probe>> isr = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_ISR_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|253| <<vp_modern_probe>> notify = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_NOTIFY_CFG,
+ *   - drivers/virtio/virtio_pci_modern_dev.c|273| <<vp_modern_probe>> device = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_DEVICE_CFG,
+ */
 static inline int virtio_pci_find_capability(struct pci_dev *dev, u8 cfg_type,
 					     u32 ioresource_types, int *bars)
 {
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 4620e9d79..64156dff9 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -32,6 +32,16 @@
 	} while (0)
 #define END_USE(_vq) \
 	do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while(0)
+/*
+ * 在以下使用vring_virtqueue->last_add_time:
+ *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+ *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+ *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+ *
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|535| <<virtqueue_add_split>> LAST_ADD_TIME_UPDATE(vq);
+ *   - drivers/virtio/virtio_ring.c|1363| <<virtqueue_add_packed>> LAST_ADD_TIME_UPDATE(vq);
+ */
 #define LAST_ADD_TIME_UPDATE(_vq)				\
 	do {							\
 		ktime_t now = ktime_get();			\
@@ -43,6 +53,11 @@
 		(_vq)->last_add_time = now;			\
 		(_vq)->last_add_time_valid = true;		\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|707| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_CHECK(vq);
+ *   - drivers/virtio/virtio_ring.c|1515| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_CHECK(vq);
+ */
 #define LAST_ADD_TIME_CHECK(_vq)				\
 	do {							\
 		if ((_vq)->last_add_time_valid) {		\
@@ -50,6 +65,13 @@
 				      (_vq)->last_add_time)) > 100); \
 		}						\
 	} while (0)
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|765| <<virtqueue_kick_prepare_split>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|888| <<virtqueue_get_buf_ctx_split>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|1612| <<virtqueue_kick_prepare_packed>> LAST_ADD_TIME_INVALID(vq);
+ *   - drivers/virtio/virtio_ring.c|1766| <<virtqueue_get_buf_ctx_packed>> LAST_ADD_TIME_INVALID(vq);
+ */
 #define LAST_ADD_TIME_INVALID(_vq)				\
 	((_vq)->last_add_time_valid = false)
 #else
@@ -90,6 +112,22 @@ struct vring_virtqueue_split {
 	struct vring vring;
 
 	/* Last written value to avail->flags */
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1068| <<virtqueue_get_buf_ctx_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|1083| <<virtqueue_disable_cb_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|1084| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1091| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow); 
+	 *   - drivers/virtio/virtio_ring.c|1111| <<virtqueue_enable_cb_prepare_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1112| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1116| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1148| <<virtqueue_enable_cb_delayed_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1149| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1153| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1215| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1218| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow);
+	 */
 	u16 avail_flags_shadow;
 
 	/*
@@ -100,6 +138,10 @@ struct vring_virtqueue_split {
 
 	/* Per-descriptor state. */
 	struct vring_desc_state_split *desc_state;
+	/*
+	 * 在以下分配vring_virtqueue_split->desc_extra, num个:
+	 *   - drivers/virtio/virtio_ring.c|1096| <<vring_alloc_state_extra_split>> vring_split->desc_extra = extra;
+	 */
 	struct vring_desc_extra *desc_extra;
 
 	/* DMA address and size information */
@@ -111,9 +153,22 @@ struct vring_virtqueue_split {
 	 * vring.
 	 */
 	u32 vring_align;
+	/*
+	 * 在以下使用vring_virtqueue_split->may_reduce_num:
+	 *   - drivers/virtio/virtio_ring.c|1159| <<vring_alloc_queue_split>> vring_split->may_reduce_num = may_reduce_num;
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_resize_split>> vq->split.may_reduce_num);
+	 */
 	bool may_reduce_num;
 };
 
+/*
+ * struct vring_packed_desc_event {
+ *     // Descriptor Ring Change Event Offset/Wrap Counter.
+ *     __le16 off_wrap;
+ *     // Descriptor Ring Change Event Flags.
+ *     __le16 flags;
+ * };
+ */
 struct vring_virtqueue_packed {
 	/* Actual memory layout for this queue. */
 	struct {
@@ -124,18 +179,80 @@ struct vring_virtqueue_packed {
 	} vring;
 
 	/* Driver ring wrap counter. */
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_wrap_counter:
+	 *   - drivers/virtio/virtio_ring.c|1417| <<virtqueue_add_indirect_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1655| <<virtqueue_kick_prepare_packed>> if (wrap_counter != vq->packed.avail_wrap_counter)
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_vring_init_packed>> vring_packed->avail_wrap_counter = 1;
+	 *
+	 * 主要和vq->packed.vring.device.off_wrap比较,
+	 * 在add的时候当avail index wrap回0的时候会flip
+	 */
 	bool avail_wrap_counter;
 
 	/* Avail used flags. */
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_used_flags:
+	 *   - drivers/virtio/virtio_ring.c|1466| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags);
+	 *   - drivers/virtio/virtio_ring.c|1486| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags ^=
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> avail_used_flags = vq->packed.avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|1602| <<virtqueue_add_packed>> flags = cpu_to_le16(vq->packed.avail_used_flags |
+	 *   - drivers/virtio/virtio_ring.c|1625| <<virtqueue_add_packed>> vq->packed.avail_used_flags ^=
+	 *   - drivers/virtio/virtio_ring.c|1667| <<virtqueue_add_packed>> vq->packed.avail_used_flags = avail_used_flags;
+	 *   - drivers/virtio/virtio_ring.c|2143| <<virtqueue_vring_init_packed>> vring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+	 *
+	 * 表示在add的时候在desc[i].flags中应该用什么标记avail/used
+	 */
 	u16 avail_used_flags;
 
+	/*
+	 * 在以下使用vring_virtqueue_packed->next_avail_idx:
+	 *   - drivers/virtio/virtio_ring.c|1429| <<virtqueue_add_indirect_packed>> head = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|1501| <<virtqueue_add_indirect_packed>> vq->packed.next_avail_idx = n;
+	 *   - drivers/virtio/virtio_ring.c|1575| <<virtqueue_add_packed>> head = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|1669| <<virtqueue_add_packed>> vq->packed.next_avail_idx = i;
+	 *   - drivers/virtio/virtio_ring.c|1734| <<virtqueue_kick_prepare_packed>> old = vq->packed.next_avail_idx - vq->num_added;
+	 *   - drivers/virtio/virtio_ring.c|1735| <<virtqueue_kick_prepare_packed>> new = vq->packed.next_avail_idx;
+	 *   - drivers/virtio/virtio_ring.c|2189| <<virtqueue_vring_init_packed>> vring_packed->next_avail_idx = 0;
+	 */
 	/* Index of the next avail descriptor. */
+	/*
+	 * 用来索引vq->packed.vring.desc[i]
+	 */
 	u16 next_avail_idx;
 
 	/*
 	 * Last written value to driver->flags in
 	 * guest byte order.
 	 */
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_FLAG_DESC:
+	 *   - drivers/virtio/virtio_ring.c|1822| <<virtqueue_kick_prepare_packed>> if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+	 *   - drivers/virtio/virtio_ring.c|1997| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+	 *   - drivers/virtio/virtio_ring.c|2042| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2101| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *
+	 * 在以下使用vring_virtqueue_pack->event_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1916| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+	 *   - drivers/virtio/virtio_ring.c|1931| <<virtqueue_disable_cb_packed>> if (vq->packed.event_flags_shadow != VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|1932| <<virtqueue_disable_cb_packed>> vq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+	 *   - drivers/virtio/virtio_ring.c|1934| <<virtqueue_disable_cb_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1959| <<virtqueue_enable_cb_prepare_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|1960| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ?
+	 *   - drivers/virtio/virtio_ring.c|1964| <<virtqueue_enable_cb_prepare_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|2018| <<virtqueue_enable_cb_delayed_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+	 *   - drivers/virtio/virtio_ring.c|2019| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ?
+	 *   - drivers/virtio/virtio_ring.c|2023| <<virtqueue_enable_cb_delayed_packed>> cpu_to_le16(vq->packed.event_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|2191| <<virtqueue_vring_init_packed>> vring_packed->event_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|2196| <<virtqueue_vring_init_packed>> vring_packed->event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+	 *   - drivers/virtio/virtio_ring.c|2198| <<virtqueue_vring_init_packed>> cpu_to_le16(vring_packed->event_flags_shadow);
+	 *
+	 * 设置为VRING_PACKED_EVENT_FLAG_DESC或者VRING_PACKED_EVENT_FLAG_ENABLE/VRING_PACKED_EVENT_FLAG_DISABLE
+	 * 用来控制virtio是否想接受QEMU的中断
+	 * 比如说: 也可以判断是否使用event idx告诉backend前端的progress
+	 */
 	u16 event_flags_shadow;
 
 	/* Per-descriptor state. */
@@ -144,9 +261,35 @@ struct vring_virtqueue_packed {
 
 	/* DMA address and size information */
 	dma_addr_t ring_dma_addr;
+	/*
+	 * 在以下使用vring_virtqueue_packed->driver_event_dma_addr:
+	 *   - drivers/virtio/virtio_ring.c|1942| <<vring_free_packed>> vring_packed->driver_event_dma_addr);
+	 *   - drivers/virtio/virtio_ring.c|1984| <<vring_alloc_queue_packed>> vring_packed->driver_event_dma_addr = driver_event_dma_addr;
+	 *   - drivers/virtio/virtio_ring.c|2754| <<vring_free>> vq->packed.driver_event_dma_addr);
+	 *   - drivers/virtio/virtio_ring.c|2926| <<virtqueue_get_avail_addr>> return vq->packed.driver_event_dma_addr;
+	 *
+	 * 代表了avail
+	 */
 	dma_addr_t driver_event_dma_addr;
 	dma_addr_t device_event_dma_addr;
+	/*
+	 * 在以下使用vring_virtqueue_packed->ring_size_in_bytes:
+	 *   - drivers/virtio/virtio_ring.c|2174| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->ring_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2211| <<vring_alloc_queue_packed>> vring_packed->ring_size_in_bytes = ring_size_in_bytes;
+	 *   - drivers/virtio/virtio_ring.c|2301| <<virtqueue_reinit_packed>> memset(vq->packed.vring.desc, 0, vq->packed.ring_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|3100| <<vring_free>> vq->packed.ring_size_in_bytes,
+	 */
 	size_t ring_size_in_bytes;
+	/*
+	 * 在以下使用vring_virtqueue_packed->event_size_in_bytes:
+	 *   - drivers/virtio/virtio_ring.c|2179| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2184| <<vring_free_packed>> vring_free_queue(vdev, vring_packed->event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|2222| <<vring_alloc_queue_packed>> vring_packed->event_size_in_bytes = event_size_in_bytes;
+	 *   - drivers/virtio/virtio_ring.c|2297| <<virtqueue_reinit_packed>> memset(vq->packed.vring.device, 0, vq->packed.event_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|2298| <<virtqueue_reinit_packed>> memset(vq->packed.vring.driver, 0, vq->packed.event_size_in_bytes);
+	 *   - drivers/virtio/virtio_ring.c|3105| <<vring_free>> vq->packed.event_size_in_bytes,
+	 *   - drivers/virtio/virtio_ring.c|3110| <<vring_free>> vq->packed.event_size_in_bytes,
+	 */
 	size_t event_size_in_bytes;
 };
 
@@ -154,6 +297,11 @@ struct vring_virtqueue {
 	struct virtqueue vq;
 
 	/* Is this a packed ring? */
+	/*
+	 * 在以下设置vring_virtqueue->packed_ring:
+	 *   - drivers/virtio/virtio_ring.c|2008| <<bool>> vq->packed_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2488| <<bool>> vq->packed_ring = false;
+	 */
 	bool packed_ring;
 
 	/* Is DMA API used? */
@@ -169,6 +317,22 @@ struct vring_virtqueue {
 	bool indirect;
 
 	/* Host publishes avail event idx */
+	/*
+	 * 在以下设置vring_virtqueue->event:
+	 *   - drivers/virtio/virtio_ring.c|2377| <<vring_create_virtqueue_packed>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+	 *   - drivers/virtio/virtio_ring.c|2973| <<__vring_new_virtqueue>> vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+	 * 在以下使用vring_virtqueue->event:
+	 *   - drivers/virtio/virtio_ring.c|928| <<virtqueue_kick_prepare_split>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|1070| <<virtqueue_disable_cb_split>> if (vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1098| <<virtqueue_enable_cb_prepare_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1135| <<virtqueue_enable_cb_delayed_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|1201| <<virtqueue_vring_init_split>> if (!vq->event)
+	 *   - drivers/virtio/virtio_ring.c|2062| <<virtqueue_enable_cb_prepare_packed>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2109| <<virtqueue_enable_cb_delayed_packed>> if (vq->event) {
+	 *   - drivers/virtio/virtio_ring.c|2132| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *   - drivers/virtio/virtio_ring.c|2919| <<vring_interrupt>> if (vq->event)
+	 */
 	bool event;
 
 	/* Head of free buffer list. */
@@ -185,6 +349,16 @@ struct vring_virtqueue {
 	u16 last_used_idx;
 
 	/* Hint for event idx: already triggered no need to disable. */
+	/*
+	 * 在以下使用vring_virtqueue->event_triggered:
+	 *   - drivers/virtio/virtio_ring.c|500| <<virtqueue_init>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2465| <<virtqueue_disable_cb>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2491| <<virtqueue_enable_cb_prepare>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2492| <<virtqueue_enable_cb_prepare>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2557| <<virtqueue_enable_cb_delayed>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2558| <<virtqueue_enable_cb_delayed>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2616| <<vring_interrupt>> vq->event_triggered = true;
+	 */
 	bool event_triggered;
 
 	union {
@@ -199,14 +373,47 @@ struct vring_virtqueue {
 	bool (*notify)(struct virtqueue *vq);
 
 	/* DMA, allocation, and size information */
+	/*
+	 * 在以下使用vring_virtqueue->we_own_ring:
+	 *   - drivers/virtio/virtio_ring.c|1213| <<vring_create_virtqueue_split>> to_vvq(vq)->we_own_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2141| <<vring_create_virtqueue_packed>> vq->we_own_ring = true;
+	 *   - drivers/virtio/virtio_ring.c|2641| <<__vring_new_virtqueue>> vq->we_own_ring = false;
+	 *   - drivers/virtio/virtio_ring.c|2731| <<virtqueue_resize>> if (!vq->we_own_ring)
+	 *   - drivers/virtio/virtio_ring.c|2795| <<vring_free>> if (vq->we_own_ring) {
+	 *   - drivers/virtio/virtio_ring.c|2961| <<virtqueue_get_desc_addr>> BUG_ON(!vq->we_own_ring);
+	 *   - drivers/virtio/virtio_ring.c|2974| <<virtqueue_get_avail_addr>> BUG_ON(!vq->we_own_ring);
+	 *   - drivers/virtio/virtio_ring.c|2988| <<virtqueue_get_used_addr>> BUG_ON(!vq->we_own_ring);
+	 */
 	bool we_own_ring;
 
 #ifdef DEBUG
 	/* They're supposed to lock for us. */
+	/*
+	 * 在以下使用vring_virtqueue->in_use:
+	 *   - drivers/virtio/virtio_ring.c|28| <<START_USE>> if ((_vq)->in_use) \
+	 *   - drivers/virtio/virtio_ring.c|30| <<START_USE>> (_vq)->vq.name, (_vq)->in_use); \
+	 *   - drivers/virtio/virtio_ring.c|31| <<START_USE>> (_vq)->in_use = __LINE__; \
+	 *   - drivers/virtio/virtio_ring.c|34| <<END_USE>> do { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while (0)
+	 *   - drivers/virtio/virtio_ring.c|401| <<virtqueue_init>> vq->in_use = false;
+	 */
 	unsigned int in_use;
 
 	/* Figure out if their kicks are too delayed. */
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time_valid:
+	 *   - drivers/virtio/virtio_ring.c|40| <<LAST_ADD_TIME_UPDATE>> if ((_vq)->last_add_time_valid) \
+	 *   - drivers/virtio/virtio_ring.c|44| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time_valid = true; \
+	 *   - drivers/virtio/virtio_ring.c|48| <<LAST_ADD_TIME_CHECK>> if ((_vq)->last_add_time_valid) { \
+	 *   - drivers/virtio/virtio_ring.c|54| <<LAST_ADD_TIME_INVALID>> ((_vq)->last_add_time_valid = false)
+	 *   - drivers/virtio/virtio_ring.c|402| <<virtqueue_init>> vq->last_add_time_valid = false;
+	 */
 	bool last_add_time_valid;
+	/*
+	 * 在以下使用vring_virtqueue->last_add_time:
+	 *   - drivers/virtio/virtio_ring.c|42| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time)) > 100); \
+	 *   - drivers/virtio/virtio_ring.c|43| <<LAST_ADD_TIME_UPDATE>> (_vq)->last_add_time = now; \
+	 *   - drivers/virtio/virtio_ring.c|50| <<LAST_ADD_TIME_CHECK>> (_vq)->last_add_time)) > 100); \
+	 */
 	ktime_t last_add_time;
 #endif
 };
@@ -284,6 +491,10 @@ static bool vring_use_dma_api(struct virtio_device *vdev)
 	return false;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|997| <<virtblk_probe>> max_size = virtio_max_dma_size(vdev);
+ */
 size_t virtio_max_dma_size(struct virtio_device *vdev)
 {
 	size_t max_segment_size = SIZE_MAX;
@@ -295,6 +506,16 @@ size_t virtio_max_dma_size(struct virtio_device *vdev)
 }
 EXPORT_SYMBOL_GPL(virtio_max_dma_size);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1054| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1068| <<vring_alloc_queue_split>> queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ *   - drivers/virtio/virtio_ring.c|1868| <<vring_alloc_queue_packed>> ring = vring_alloc_queue(vdev, ring_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1880| <<vring_alloc_queue_packed>> driver = vring_alloc_queue(vdev, event_size_in_bytes,
+ *   - drivers/virtio/virtio_ring.c|1890| <<vring_alloc_queue_packed>> device = vring_alloc_queue(vdev, event_size_in_bytes,
+ *
+ * 分配size
+ */
 static void *vring_alloc_queue(struct virtio_device *vdev, size_t size,
 			      dma_addr_t *dma_handle, gfp_t flag)
 {
@@ -342,6 +563,18 @@ static void vring_free_queue(struct virtio_device *vdev, size_t size,
  * making all of the arch DMA ops work on the vring device itself
  * is a mess.  For now, we use the parent device for DMA ops.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|363| <<vring_map_one_sg>> return dma_map_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|375| <<vring_map_single>> return dma_map_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|385| <<vring_mapping_error>> return dma_mapping_error(vring_dma_dev(vq), addr);
+ *   - drivers/virtio/virtio_ring.c|421| <<vring_unmap_one_split_indirect>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|440| <<vring_unmap_one_split>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|446| <<vring_unmap_one_split>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1176| <<vring_unmap_extra_packed>> dma_unmap_single(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1181| <<vring_unmap_extra_packed>> dma_unmap_page(vring_dma_dev(vq),
+ *   - drivers/virtio/virtio_ring.c|1198| <<vring_unmap_desc_packed>> dma_unmap_page(vring_dma_dev(vq),
+ */
 static inline struct device *vring_dma_dev(const struct vring_virtqueue *vq)
 {
 	return vq->vq.vdev->dev.parent;
@@ -385,6 +618,15 @@ static int vring_mapping_error(const struct vring_virtqueue *vq,
 	return dma_mapping_error(vring_dma_dev(vq), addr);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1097| <<virtqueue_reinit_split>> virtqueue_init(vq, num);
+ *   - drivers/virtio/virtio_ring.c|1254| <<virtqueue_resize_split>> virtqueue_init(vq, vring_split.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2120| <<virtqueue_reinit_packed>> virtqueue_init(vq, vq->packed.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2176| <<vring_create_virtqueue_packed>> virtqueue_init(vq, num);
+ *   - drivers/virtio/virtio_ring.c|2210| <<virtqueue_resize_packed>> virtqueue_init(vq, vring_packed.vring.num);
+ *   - drivers/virtio/virtio_ring.c|2677| <<__vring_new_virtqueue>> virtqueue_init(vq, vring_split->vring.num);
+ */
 static void virtqueue_init(struct vring_virtqueue *vq, u32 num)
 {
 	vq->vq.num_free = num;
@@ -506,6 +748,10 @@ static inline unsigned int virtqueue_add_desc_split(struct virtqueue *vq,
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2240| <<virtqueue_add>> virtqueue_add_split(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_split(struct virtqueue *_vq,
 				      struct scatterlist *sgs[],
 				      unsigned int total_sg,
@@ -689,6 +935,10 @@ static inline int virtqueue_add_split(struct virtqueue *_vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2363| <<virtqueue_kick_prepare>> virtqueue_kick_prepare_split(_vq);
+ */
 static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -720,6 +970,11 @@ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
 	return needs_kick;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|928| <<virtqueue_get_buf_ctx_split>> detach_buf_split(vq, i, ctx);
+ *   - drivers/virtio/virtio_ring.c|1046| <<virtqueue_detach_unused_buf_split>> detach_buf_split(vq, i, NULL);
+ */
 static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
 			     void **ctx)
 {
@@ -776,6 +1031,10 @@ static inline bool more_used_split(const struct vring_virtqueue *vq)
 			vq->split.vring.used->idx);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2378| <<virtqueue_get_buf_ctx>> virtqueue_get_buf_ctx_split(_vq, len, ctx);
+ */
 static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
 					 unsigned int *len,
 					 void **ctx)
@@ -838,6 +1097,22 @@ static void virtqueue_disable_cb_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
+	/*
+	 * 在以下使用vring_virtqueue_split->avail_flags_shadow:
+	 *   - drivers/virtio/virtio_ring.c|1068| <<virtqueue_get_buf_ctx_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+	 *   - drivers/virtio/virtio_ring.c|1083| <<virtqueue_disable_cb_split>> if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+	 *   - drivers/virtio/virtio_ring.c|1084| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1091| <<virtqueue_disable_cb_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1111| <<virtqueue_enable_cb_prepare_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1112| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1116| <<virtqueue_enable_cb_prepare_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1148| <<virtqueue_enable_cb_delayed_split>> if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+	 *   - drivers/virtio/virtio_ring.c|1149| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1153| <<virtqueue_enable_cb_delayed_split>> vq->split.avail_flags_shadow);
+	 *   - drivers/virtio/virtio_ring.c|1210| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow = 0;
+	 *   - drivers/virtio/virtio_ring.c|1215| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+	 *   - drivers/virtio/virtio_ring.c|1218| <<virtqueue_vring_init_split>> vring_split->avail_flags_shadow);
+	 */
 	if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
 		vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
 		if (vq->event)
@@ -850,6 +1125,10 @@ static void virtqueue_disable_cb_split(struct virtqueue *_vq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2495| <<virtqueue_enable_cb_prepare>> virtqueue_enable_cb_prepare_split(_vq);
+ */
 static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -875,6 +1154,10 @@ static unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
 	return last_used_idx;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2359| <<virtqueue_poll>> virtqueue_poll_split(_vq, last_used_idx);
+ */
 static bool virtqueue_poll_split(struct virtqueue *_vq, unsigned int last_used_idx)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -919,6 +1202,10 @@ static bool virtqueue_enable_cb_delayed_split(struct virtqueue *_vq)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2578| <<virtqueue_detach_unused_buf>> virtqueue_detach_unused_buf_split(_vq);
+ */
 static void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1082,6 +1369,10 @@ static int vring_alloc_queue_split(struct vring_virtqueue_split *vring_split,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2704| <<vring_create_virtqueue>> return vring_create_virtqueue_split(index, num, vring_align,
+ */
 static struct virtqueue *vring_create_virtqueue_split(
 	unsigned int index,
 	unsigned int num,
@@ -1115,6 +1406,10 @@ static struct virtqueue *vring_create_virtqueue_split(
 	return vq;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2612| <<virtqueue_resize>> err = virtqueue_resize_split(_vq, num);
+ */
 static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 {
 	struct vring_virtqueue_split vring_split = {};
@@ -1152,8 +1447,27 @@ static int virtqueue_resize_split(struct virtqueue *_vq, u32 num)
 /*
  * Packed ring specific functions - *_packed().
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1854| <<more_used_packed>> used_wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|1884| <<virtqueue_get_buf_ctx_packed>> used_wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|2000| <<virtqueue_enable_cb_delayed_packed>> wrap_counter = packed_used_wrap_counter(last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|2033| <<virtqueue_enable_cb_delayed_packed>> wrap_counter = packed_used_wrap_counter(last_used_idx);
+ */
 static inline bool packed_used_wrap_counter(u16 last_used_idx)
 {
+	/*
+	 * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+	 *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+	 *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+	 *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+	 */
 	return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
@@ -1162,6 +1476,11 @@ static inline u16 packed_last_used(u16 last_used_idx)
 	return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_packed>> vring_unmap_extra_packed(vq, &vq->packed.desc_extra[curr]);
+ *   - drivers/virtio/virtio_ring.c|1555| <<detach_buf_packed>> vring_unmap_extra_packed(vq,
+ */
 static void vring_unmap_extra_packed(const struct vring_virtqueue *vq,
 				     struct vring_desc_extra *extra)
 {
@@ -1219,6 +1538,10 @@ static struct vring_packed_desc *alloc_indirect_packed(unsigned int total_sg,
 	return desc;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1675| <<virtqueue_add_packed>> err = virtqueue_add_indirect_packed(vq, sgs, total_sg, out_sgs,
+ */
 static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 					 struct scatterlist *sgs[],
 					 unsigned int total_sg,
@@ -1333,6 +1656,10 @@ static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2168| <<virtqueue_add>> return vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,
+ */
 static inline int virtqueue_add_packed(struct virtqueue *_vq,
 				       struct scatterlist *sgs[],
 				       unsigned int total_sg,
@@ -1380,7 +1707,20 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 
 	WARN_ON_ONCE(total_sg > vq->packed.vring.num && !vq->indirect);
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *        struct {
+	 *            unsigned int num;
+	 *            struct vring_packed_desc *desc;
+	 *            struct vring_packed_desc_event *driver;
+	 *            struct vring_packed_desc_event *device;
+	 *        } vring;
+	 */
 	desc = vq->packed.vring.desc;
+	/*
+	 * i用来索引desc
+	 */
 	i = head;
 	descs_used = total_sg;
 
@@ -1403,6 +1743,22 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 			if (vring_mapping_error(vq, addr))
 				goto unmap_release;
 
+			/*
+			 * 在以下使用vring_virtqueue_packed->avail_used_flags:
+			 *   - drivers/virtio/virtio_ring.c|1466| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|1476| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags);
+			 *   - drivers/virtio/virtio_ring.c|1486| <<virtqueue_add_indirect_packed>> vq->packed.avail_used_flags ^=
+			 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> avail_used_flags = vq->packed.avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|1602| <<virtqueue_add_packed>> flags = cpu_to_le16(vq->packed.avail_used_flags |
+			 *   - drivers/virtio/virtio_ring.c|1625| <<virtqueue_add_packed>> vq->packed.avail_used_flags ^=
+			 *   - drivers/virtio/virtio_ring.c|1667| <<virtqueue_add_packed>> vq->packed.avail_used_flags = avail_used_flags;
+			 *   - drivers/virtio/virtio_ring.c|2143| <<virtqueue_vring_init_packed>> vring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+			 *
+			 * flags分成三部分:
+			 * - vq->packed.avail_used_flags
+			 * - (++c == total_sg ? 0 : VRING_DESC_F_NEXT)
+			 * - (n < out_sgs ? 0 : VRING_DESC_F_WRITE)
+			 */
 			flags = cpu_to_le16(vq->packed.avail_used_flags |
 				    (++c == total_sg ? 0 : VRING_DESC_F_NEXT) |
 				    (n < out_sgs ? 0 : VRING_DESC_F_WRITE));
@@ -1433,6 +1789,15 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 		}
 	}
 
+	/*
+	 * 在以下使用vring_virtqueue_packed->avail_wrap_counter:
+	 *   - drivers/virtio/virtio_ring.c|1417| <<virtqueue_add_indirect_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1565| <<virtqueue_add_packed>> vq->packed.avail_wrap_counter ^= 1;
+	 *   - drivers/virtio/virtio_ring.c|1655| <<virtqueue_kick_prepare_packed>> if (wrap_counter != vq->packed.avail_wrap_counter)
+	 *   - drivers/virtio/virtio_ring.c|2073| <<virtqueue_vring_init_packed>> vring_packed->avail_wrap_counter = 1;
+	 *
+	 * 似乎只用在kick的判断上
+	 */
 	if (i < head)
 		vq->packed.avail_wrap_counter ^= 1;
 
@@ -1484,6 +1849,10 @@ static inline int virtqueue_add_packed(struct virtqueue *_vq,
 	return -EIO;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2503| <<virtqueue_kick_prepare>> return vq->packed_ring ? virtqueue_kick_prepare_packed(_vq) :
+ */
 static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1505,16 +1874,43 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 	 */
 	virtio_mb(vq->weak_barriers);
 
+	/*
+	 * 用来索引desc = vq->packed.vring.desc
+	 */
 	old = vq->packed.next_avail_idx - vq->num_added;
 	new = vq->packed.next_avail_idx;
 	vq->num_added = 0;
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *    -> struct {
+	 *           unsigned int num;
+	 *           struct vring_packed_desc *desc;
+	 *           struct vring_packed_desc_event *driver;
+	 *           struct vring_packed_desc_event *device;
+	 *       } vring;
+	 */
 	snapshot.u32 = *(u32 *)vq->packed.vring.device;
 	flags = le16_to_cpu(snapshot.flags);
 
 	LAST_ADD_TIME_CHECK(vq);
 	LAST_ADD_TIME_INVALID(vq);
 
+	/*
+	 * 有这样类似的代码
+	 * 2012                 vq->packed.event_flags_shadow = vq->event ?
+	 * 2013                                 VRING_PACKED_EVENT_FLAG_DESC :
+	 * 2014                                 VRING_PACKED_EVENT_FLAG_ENABLE;
+	 *
+	 * 在QEMU side, 设置handle_request (avai)是否想接收中断.
+	 *
+	 * disable: 设置VRING_PACKED_EVENT_FLAG_DISABLE
+	 * enable:  设置VRING_PACKED_EVENT_FLAG_DESC或者VRING_PACKED_EVENT_FLAG_ENABLE
+	 *
+	 * 如果设置VRING_PACKED_EVENT_FLAG_DESC, 直接跳过, 说明是ENABLE
+	 * 否则有可能是DISABLE
+	 */
 	if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
 		needs_kick = (flags != VRING_PACKED_EVENT_FLAG_DISABLE);
 		goto out;
@@ -1527,12 +1923,21 @@ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
 	if (wrap_counter != vq->packed.avail_wrap_counter)
 		event_idx -= vq->packed.vring.num;
 
+	/*
+	 * event_idx应该相当于现在otherend处理到的,比如vring_used_event(&ring)
+	 */
 	needs_kick = vring_need_event(event_idx, new, old);
 out:
 	END_USE(vq);
 	return needs_kick;
 }
 
+
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1745| <<virtqueue_get_buf_ctx_packed>> detach_buf_packed(vq, id, ctx);
+ *   - drivers/virtio/virtio_ring.c|1902| <<virtqueue_detach_unused_buf_packed>> detach_buf_packed(vq, i, NULL);
+ */
 static void detach_buf_packed(struct vring_virtqueue *vq,
 			      unsigned int id, void **ctx)
 {
@@ -1579,16 +1984,35 @@ static void detach_buf_packed(struct vring_virtqueue *vq,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1806| <<more_used_packed>> return is_used_desc_packed(vq, last_used, used_wrap_counter);
+ *   - drivers/virtio/virtio_ring.c|1931| <<virtqueue_poll_packed>> return is_used_desc_packed(vq, used_idx, wrap_counter);
+ *   - drivers/virtio/virtio_ring.c|1986| <<virtqueue_enable_cb_delayed_packed>> if (is_used_desc_packed(vq, used_idx, wrap_counter)) {
+ */
 static inline bool is_used_desc_packed(const struct vring_virtqueue *vq,
 				       u16 idx, bool used_wrap_counter)
 {
 	bool avail, used;
 	u16 flags;
 
+	/*
+	 * struct vring_virtqueue *vq:
+	 * -> struct vring_virtqueue_packed packed;
+	 *    -> struct {
+	 *           unsigned int num;
+	 *           struct vring_packed_desc *desc;
+	 *           struct vring_packed_desc_event *driver;
+	 *           struct vring_packed_desc_event *device;
+	 *       } vring;
+	 */
 	flags = le16_to_cpu(vq->packed.vring.desc[idx].flags);
 	avail = !!(flags & (1 << VRING_PACKED_DESC_F_AVAIL));
 	used = !!(flags & (1 << VRING_PACKED_DESC_F_USED));
 
+	/*
+	 * 如果只设置了avail, 没设置used, 就返回false
+	 */
 	return avail == used && used == used_wrap_counter;
 }
 
@@ -1673,6 +2097,10 @@ static void *virtqueue_get_buf_ctx_packed(struct virtqueue *_vq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|2755| <<virtqueue_disable_cb>> virtqueue_disable_cb_packed(_vq);
+ */
 static void virtqueue_disable_cb_packed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -1969,6 +2397,10 @@ static void virtqueue_reinit_packed(struct vring_virtqueue *vq)
 	virtqueue_vring_init_packed(&vq->packed, !!vq->vq.callback);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|3082| <<vring_create_virtqueue>> return vring_create_virtqueue_packed(index, num, vring_align,
+ */
 static struct virtqueue *vring_create_virtqueue_packed(
 	unsigned int index,
 	unsigned int num,
@@ -2203,6 +2635,27 @@ EXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);
  * This is sometimes useful because the virtqueue_kick_prepare() needs
  * to be serialized, but the actual virtqueue_notify() call does not.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|307| <<virtio_commit_rqs>> kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/block/virtio_blk.c|379| <<virtio_queue_rq>> if (bd->last && virtqueue_kick_prepare(vblk->vqs[qid].vq))
+ *   - drivers/block/virtio_blk.c|419| <<virtblk_add_req_batch>> kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|429| <<virtio_gpu_notify>> notify = virtqueue_kick_prepare(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|471| <<virtio_gpu_queue_cursor>> notify = virtqueue_kick_prepare(vq);
+ *   - drivers/net/virtio_net.c|682| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1461| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1682| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1885| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|984| <<rpmsg_probe>> notify = virtqueue_kick_prepare(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|468| <<virtscsi_kick_vq>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|495| <<virtscsi_add_cmd>> needs_kick = virtqueue_kick_prepare(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2546| <<virtqueue_kick>> if (virtqueue_kick_prepare(vq))
+ *   - fs/fuse/virtio_fs.c|457| <<send_forget_request>> notify = virtqueue_kick_prepare(vq);
+ *   - fs/fuse/virtio_fs.c|1203| <<virtio_fs_enqueue_req>> notify = virtqueue_kick_prepare(vq);
+ *   - sound/virtio/virtio_card.c|44| <<virtsnd_event_send>> if (virtqueue_kick_prepare(vqueue))
+ *   - sound/virtio/virtio_ctl_msg.c|154| <<virtsnd_ctl_msg_send>> notify = virtqueue_kick_prepare(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|245| <<virtsnd_pcm_msg_send>> notify = virtqueue_kick_prepare(vqueue);
+ */
 bool virtqueue_kick_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2220,6 +2673,27 @@ EXPORT_SYMBOL_GPL(virtqueue_kick_prepare);
  *
  * Returns false if host notify failed or queue is broken, otherwise true.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|311| <<virtio_commit_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/block/virtio_blk.c|384| <<virtio_queue_rq>> virtqueue_notify(vblk->vqs[qid].vq);
+ *   - drivers/block/virtio_blk.c|445| <<virtio_queue_rqs>> virtqueue_notify(vq->vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|433| <<virtio_gpu_notify>> virtqueue_notify(vgdev->ctrlq.vq);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|477| <<virtio_gpu_queue_cursor>> virtqueue_notify(vq);
+ *   - drivers/net/virtio_net.c|682| <<virtnet_xdp_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))
+ *   - drivers/net/virtio_net.c|1461| <<try_fill_recv>> if (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {
+ *   - drivers/net/virtio_net.c|1682| <<virtnet_poll>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/net/virtio_net.c|1885| <<start_xmit>> if (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|995| <<rpmsg_probe>> virtqueue_notify(vrp->rvq);
+ *   - drivers/scsi/virtio_scsi.c|472| <<virtscsi_kick_vq>> virtqueue_notify(vq->vq);
+ *   - drivers/scsi/virtio_scsi.c|500| <<virtscsi_add_cmd>> virtqueue_notify(vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2547| <<virtqueue_kick>> return virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|461| <<send_forget_request>> virtqueue_notify(vq);
+ *   - fs/fuse/virtio_fs.c|1208| <<virtio_fs_enqueue_req>> virtqueue_notify(vq);
+ *   - sound/virtio/virtio_card.c|45| <<virtsnd_event_send>> virtqueue_notify(vqueue);
+ *   - sound/virtio/virtio_ctl_msg.c|174| <<virtsnd_ctl_msg_send>> virtqueue_notify(queue->vqueue);
+ *   - sound/virtio/virtio_pcm_msg.c|248| <<virtsnd_pcm_msg_send>> virtqueue_notify(vqueue);
+ */
 bool virtqueue_notify(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2273,6 +2747,12 @@ EXPORT_SYMBOL_GPL(virtqueue_kick);
  * Returns NULL if there are no used buffers, or the "data" token
  * handed to virtqueue_add_*().
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1128| <<receive_mergeable>> buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx);
+ *   - drivers/net/virtio_net.c|1552| <<virtnet_receive>> (buf = virtqueue_get_buf_ctx(rq->vq, &len, &ctx))) {
+ *   - drivers/virtio/virtio_ring.c|2384| <<virtqueue_get_buf>> return virtqueue_get_buf_ctx(_vq, len, NULL);
+ */
 void *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,
 			    void **ctx)
 {
@@ -2326,6 +2806,14 @@ EXPORT_SYMBOL_GPL(virtqueue_disable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/firmware/arm_scmi/virtio.c|520| <<virtio_send_message>> msg->poll_idx = virtqueue_enable_cb_prepare(vioch->vqueue);
+ *   - drivers/firmware/arm_scmi/virtio.c|771| <<virtio_poll_done>> msg->poll_idx = virtqueue_enable_cb_prepare(vioch->vqueue);
+ *   - drivers/net/virtio_net.c|397| <<virtqueue_napi_complete>> opaque = virtqueue_enable_cb_prepare(vq);
+ *   - drivers/net/virtio_net.c|1747| <<virtnet_poll_tx>> opaque = virtqueue_enable_cb_prepare(sq->vq);
+ *   - drivers/virtio/virtio_ring.c|2584| <<virtqueue_enable_cb>> unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);
+ */
 unsigned int virtqueue_enable_cb_prepare(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2371,6 +2859,29 @@ EXPORT_SYMBOL_GPL(virtqueue_poll);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|292| <<virtblk_done>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|47| <<virtcrypto_ctrlq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|94| <<virtcrypto_dataq_callback>> } while (!virtqueue_enable_cb(vq));
+ *   - drivers/firmware/arm_scmi/virtio.c|293| <<scmi_vio_complete_cb>> if (virtqueue_enable_cb(vqueue)) {
+ *   - drivers/firmware/arm_scmi/virtio.c|769| <<virtio_poll_done>> pending = !virtqueue_enable_cb(vioch->vqueue);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|212| <<virtio_gpu_dequeue_ctrl_func>> } while (!virtqueue_enable_cb(vgdev->ctrlq.vq));
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|260| <<virtio_gpu_dequeue_cursor_func>> } while (!virtqueue_enable_cb(vgdev->cursorq.vq));
+ *   - drivers/net/caif/caif_virtio.c|565| <<cfv_netdev_tx>> virtqueue_enable_cb(cfv->vq_tx);
+ *   - drivers/rpmsg/virtio_rpmsg_bus.c|486| <<rpmsg_upref_sleepers>> virtqueue_enable_cb(vrp->svq);
+ *   - drivers/scsi/virtio_scsi.c|187| <<virtscsi_vq_done>> } while (!virtqueue_enable_cb(vq));
+ *   - fs/fuse/virtio_fs.c|348| <<virtio_fs_hiprio_done_work>> } while (!virtqueue_enable_cb(vq) && likely(!virtqueue_is_broken(vq)));
+ *   - fs/fuse/virtio_fs.c|630| <<virtio_fs_requests_done_work>> } while (!virtqueue_enable_cb(vq) && likely(!virtqueue_is_broken(vq)));
+ *   - net/vmw_vsock/virtio_transport.c|310| <<virtio_transport_tx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|418| <<virtio_transport_event_work>> } while (!virtqueue_enable_cb(vq));
+ *   - net/vmw_vsock/virtio_transport.c|561| <<virtio_transport_rx_work>> } while (!virtqueue_enable_cb(vq));
+ *   - sound/virtio/virtio_card.c|96| <<virtsnd_event_notify_cb>> } while (!virtqueue_enable_cb(vqueue));
+ *   - sound/virtio/virtio_card.c|166| <<virtsnd_enable_event_vq>> if (!virtqueue_enable_cb(queue->vqueue))
+ *   - sound/virtio/virtio_ctl_msg.c|308| <<virtsnd_ctl_notify_cb>> } while (!virtqueue_enable_cb(vqueue));
+ *   - sound/virtio/virtio_pcm_msg.c|350| <<virtsnd_pcm_notify_cb>> } while (!virtqueue_enable_cb(queue->vqueue));
+ *   - tools/virtio/virtio_test.c|266| <<run_test>> if (virtqueue_enable_cb(vq->vq))
+ */
 bool virtqueue_enable_cb(struct virtqueue *_vq)
 {
 	unsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);
@@ -2392,6 +2903,15 @@ EXPORT_SYMBOL_GPL(virtqueue_enable_cb);
  * Caller must ensure we don't call this with other virtqueue
  * operations at the same time (except where noted).
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1651| <<virtnet_poll_cleantx>> } while (unlikely(!virtqueue_enable_cb_delayed(sq->vq)));
+ *   - drivers/net/virtio_net.c|1835| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq)));
+ *   - drivers/net/virtio_net.c|1874| <<start_xmit>> unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {
+ *   - tools/virtio/virtio_test.c|263| <<run_test>> if (virtqueue_enable_cb_delayed(vq->vq))
+ *   - tools/virtio/vringh_test.c|388| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ *   - tools/virtio/vringh_test.c|422| <<bool>> if (!virtqueue_enable_cb_delayed(vq))
+ */
 bool virtqueue_enable_cb_delayed(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2453,6 +2973,16 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 #endif
 	}
 
+	/*
+	 * 在以下使用vring_virtqueue->event_triggered:
+	 *   - drivers/virtio/virtio_ring.c|500| <<virtqueue_init>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2465| <<virtqueue_disable_cb>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2491| <<virtqueue_enable_cb_prepare>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2492| <<virtqueue_enable_cb_prepare>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2557| <<virtqueue_enable_cb_delayed>> if (vq->event_triggered)
+	 *   - drivers/virtio/virtio_ring.c|2558| <<virtqueue_enable_cb_delayed>> vq->event_triggered = false;
+	 *   - drivers/virtio/virtio_ring.c|2616| <<vring_interrupt>> vq->event_triggered = true;
+	 */
 	/* Just a hint for performance: so it's ok that this can be racy! */
 	if (vq->event)
 		vq->event_triggered = true;
@@ -2466,6 +2996,11 @@ irqreturn_t vring_interrupt(int irq, void *_vq)
 EXPORT_SYMBOL_GPL(vring_interrupt);
 
 /* Only available for split ring */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1267| <<vring_create_virtqueue_split>> vq = __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,
+ *   - drivers/virtio/virtio_ring.c|2852| <<vring_new_virtqueue>> return __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,
+ */
 static struct virtqueue *__vring_new_virtqueue(unsigned int index,
 					       struct vring_virtqueue_split *vring_split,
 					       struct virtio_device *vdev,
@@ -2525,6 +3060,15 @@ static struct virtqueue *__vring_new_virtqueue(unsigned int index,
 	return &vq->vq;
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|954| <<vu_setup_vq>> vq = vring_create_virtqueue(index, num, PAGE_SIZE, vdev, true, true,
+ *   - drivers/s390/virtio/virtio_ccw.c|525| <<virtio_ccw_setup_vq>> vq = vring_create_virtqueue(i, info->num, KVM_VIRTIO_CCW_RING_ALIGN,
+ *   - drivers/virtio/virtio_mmio.c|399| <<vm_setup_vq>> vq = vring_create_virtqueue(index, num, VIRTIO_MMIO_VRING_ALIGN, vdev,
+ *   - drivers/virtio/virtio_pci_legacy.c|131| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_pci_modern.c|334| <<setup_vq>> vq = vring_create_virtqueue(index, num,
+ *   - drivers/virtio/virtio_vdpa.c|178| <<virtio_vdpa_setup_vq>> vq = vring_create_virtqueue(index, max_num, align, vdev,
+ */
 struct virtqueue *vring_create_virtqueue(
 	unsigned int index,
 	unsigned int num,
@@ -2573,6 +3117,11 @@ EXPORT_SYMBOL_GPL(vring_create_virtqueue);
  * -EPERM: Operation not permitted
  *
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1906| <<virtnet_rx_resize>> err = virtqueue_resize(rq->vq, ring_num, virtnet_rq_free_unused_buf);
+ *   - drivers/net/virtio_net.c|1945| <<virtnet_tx_resize>> err = virtqueue_resize(sq->vq, ring_num, virtnet_sq_free_unused_buf);
+ */
 int virtqueue_resize(struct virtqueue *_vq, u32 num,
 		     void (*recycle)(struct virtqueue *vq, void *buf))
 {
@@ -2619,6 +3168,16 @@ int virtqueue_resize(struct virtqueue *_vq, u32 num,
 EXPORT_SYMBOL_GPL(virtqueue_resize);
 
 /* Only available for split ring */
+/*
+ * called by:
+ *   - tools/virtio/linux/virtio.h|60| <<global>> struct virtqueue *vring_new_virtqueue(unsigned int index,
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|952| <<mlxbf_tmfifo_virtio_find_vqs>> vq = vring_new_virtqueue(i, vring->num, vring->align, vdev,
+ *   - drivers/remoteproc/remoteproc_virtio.c|120| <<rp_find_vq>> vq = vring_new_virtqueue(id, num, rvring->align, vdev, false, ctx,
+ *   - tools/virtio/virtio_test.c|105| <<vq_reset>> info->vq = vring_new_virtqueue(info->idx, num, 4096, vdev, true, false,
+ *   - tools/virtio/vringh_test.c|318| <<parallel_test>> vq = vring_new_virtqueue(0, RINGSIZE, ALIGN, &gvdev.vdev, true,
+ *   - tools/virtio/vringh_test.c|486| <<main>> vq = vring_new_virtqueue(0, RINGSIZE, ALIGN, &vdev, true, false,
+ *   - tools/virtio/vringh_test.c|669| <<main>> vq = vring_new_virtqueue(0, RINGSIZE, ALIGN, &vdev, true,
+ */
 struct virtqueue *vring_new_virtqueue(unsigned int index,
 				      unsigned int num,
 				      unsigned int vring_align,
@@ -2641,6 +3200,12 @@ struct virtqueue *vring_new_virtqueue(unsigned int index,
 }
 EXPORT_SYMBOL_GPL(vring_new_virtqueue);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1300| <<virtqueue_resize_split>> vring_free(&vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2256| <<virtqueue_resize_packed>> vring_free(&vq->vq);
+ *   - drivers/virtio/virtio_ring.c|2901| <<vring_del_virtqueue>> vring_free(_vq);
+ */
 static void vring_free(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2677,6 +3242,27 @@ static void vring_free(struct virtqueue *_vq)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|870| <<vu_del_vq>> vring_del_virtqueue(vq);
+ *   - arch/um/drivers/virtio_uml.c|1005| <<vu_setup_vq>> vring_del_virtqueue(vq);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|222| <<mlxbf_tmfifo_free_vrings>> vring_del_virtqueue(vring->vq);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|920| <<mlxbf_tmfifo_virtio_del_vqs>> vring_del_virtqueue(vq);
+ *   - drivers/remoteproc/remoteproc_virtio.c|148| <<__rproc_virtio_del_vqs>> vring_del_virtqueue(vq);
+ *   - drivers/s390/virtio/virtio_ccw.c|468| <<virtio_ccw_del_vq>> vring_del_virtqueue(vq);
+ *   - drivers/s390/virtio/virtio_ccw.c|578| <<virtio_ccw_setup_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_mmio.c|338| <<vm_del_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_mmio.c|459| <<vm_setup_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_pci_legacy.c|167| <<setup_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_pci_legacy.c|186| <<del_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_pci_modern.c|356| <<setup_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_pci_modern.c|395| <<del_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_vdpa.c|230| <<virtio_vdpa_setup_vq>> vring_del_virtqueue(vq);
+ *   - drivers/virtio/virtio_vdpa.c|255| <<virtio_vdpa_del_vq>> vring_del_virtqueue(vq);
+ *   - tools/virtio/virtio_test.c|101| <<vq_reset>> vring_del_virtqueue(info->vq);
+ *   - tools/virtio/vringh_test.c|433| <<parallel_test>> vring_del_virtqueue(vq);
+ *   - tools/virtio/vringh_test.c|749| <<main>> vring_del_virtqueue(vq);
+ */
 void vring_del_virtqueue(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2692,6 +3278,24 @@ void vring_del_virtqueue(struct virtqueue *_vq)
 EXPORT_SYMBOL_GPL(vring_del_virtqueue);
 
 /* Manipulates transport-specific feature bits. */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1077| <<vu_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|248| <<rproc_virtio_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|821| <<virtio_ccw_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_mmio.c|128| <<vm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_legacy.c|36| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_modern.c|49| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_vdpa.c|320| <<virtio_vdpa_finalize_features>> vring_transport_features(vdev);
+ *
+ * 清空virtio_device->features中在VIRTIO_TRANSPORT_F_START->VIRTIO_TRANSPORT_F_END并且不在以下的bit
+ * - VIRTIO_RING_F_INDIRECT_DESC
+ * - VIRTIO_RING_F_EVENT_IDX
+ * - VIRTIO_F_VERSION_1:
+ * - VIRTIO_F_ACCESS_PLATFORM:
+ * - VIRTIO_F_RING_PACKED:
+ * - VIRTIO_F_ORDER_PLATFORM:
+ */
 void vring_transport_features(struct virtio_device *vdev)
 {
 	unsigned int i;
@@ -2749,6 +3353,10 @@ EXPORT_SYMBOL_GPL(__virtqueue_break);
 /*
  * This function should only be called by the core, not directly by the driver.
  */
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|277| <<vp_modern_enable_vq_after_reset>> __virtqueue_unbreak(vq);
+ */
 void __virtqueue_unbreak(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
@@ -2770,6 +3378,17 @@ EXPORT_SYMBOL_GPL(virtqueue_is_broken);
  * This should prevent the device from being used, allowing drivers to
  * recover.  You may need to grab appropriate locks to flush.
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|171| <<vhost_user_check_reset>> virtio_break_device(&vu_dev->vdev);
+ *   - drivers/char/virtio_console.c|1961| <<virtcons_remove>> virtio_break_device(vdev);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|269| <<virtcrypto_update_status>> virtio_break_device(vcrypto->vdev);
+ *   - drivers/firmware/arm_scmi/virtio.c|164| <<scmi_vio_channel_cleanup_sync>> virtio_break_device(vioch->vqueue->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1233| <<virtio_ccw_remove>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1250| <<virtio_ccw_offline>> virtio_break_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio.c|229| <<virtio_reset_device>> virtio_break_device(dev);
+ *   - drivers/virtio/virtio_pci_common.c|591| <<virtio_pci_remove>> virtio_break_device(&vp_dev->vdev);
+ */
 void virtio_break_device(struct virtio_device *dev)
 {
 	struct virtqueue *_vq;
@@ -2813,6 +3432,9 @@ dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * num个sizeof(struct vring_packed_desc)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.ring_dma_addr;
 
@@ -2820,12 +3442,23 @@ dma_addr_t virtqueue_get_desc_addr(struct virtqueue *_vq)
 }
 EXPORT_SYMBOL_GPL(virtqueue_get_desc_addr);
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|989| <<vu_setup_vq>> virtqueue_get_avail_addr(vq),
+ *   - drivers/s390/virtio/virtio_ccw.c|553| <<virtio_ccw_setup_vq>> info->info_block->s.avail = (__u64)virtqueue_get_avail_addr(vq);
+ *   - drivers/virtio/virtio_mmio.c|436| <<vm_setup_vq>> addr = virtqueue_get_avail_addr(vq);
+ *   - drivers/virtio/virtio_pci_modern.c|193| <<vp_active_vq>> virtqueue_get_avail_addr(vq),
+ *   - drivers/virtio/virtio_vdpa.c|195| <<virtio_vdpa_setup_vq>> driver_addr = virtqueue_get_avail_addr(vq);
+ */
 dma_addr_t virtqueue_get_avail_addr(struct virtqueue *_vq)
 {
 	struct vring_virtqueue *vq = to_vvq(_vq);
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * 分配的时候只有sizeof(struct vring_packed_desc_event)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.driver_event_dma_addr;
 
@@ -2840,6 +3473,9 @@ dma_addr_t virtqueue_get_used_addr(struct virtqueue *_vq)
 
 	BUG_ON(!vq->we_own_ring);
 
+	/*
+	 * 分配的时候只有sizeof(struct vring_packed_desc_event)
+	 */
 	if (vq->packed_ring)
 		return vq->packed.device_event_dma_addr;
 
diff --git a/drivers/xen/events/events_base.c b/drivers/xen/events/events_base.c
index c443f04aa..68f2272d8 100644
--- a/drivers/xen/events/events_base.c
+++ b/drivers/xen/events/events_base.c
@@ -2207,6 +2207,10 @@ void xen_setup_callback_vector(void)
  * Setup per-vCPU vector-type callbacks. If this setup is unavailable,
  * fallback to the global vector-type callback.
  */
+/*
+ * called by:
+ *   - drivers/xen/events/events_base.c|2317| <<xen_init_IRQ>> xen_init_setup_upcall_vector();
+ */
 static __init void xen_init_setup_upcall_vector(void)
 {
 	if (!xen_have_vector_callback)
@@ -2221,6 +2225,12 @@ static __init void xen_init_setup_upcall_vector(void)
 		xen_have_vector_callback = false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/enlighten_hvm.c|181| <<xen_cpu_up_prepare_hvm>> rc = xen_set_upcall_vector(cpu);
+ *   - arch/x86/xen/suspend_hvm.c|22| <<xen_hvm_post_suspend>> BUG_ON(xen_set_upcall_vector(cpu));
+ *   - drivers/xen/events/events_base.c|2216| <<xen_init_setup_upcall_vector>> !xen_set_upcall_vector(0))
+ */
 int xen_set_upcall_vector(unsigned int cpu)
 {
 	int rc;
diff --git a/fs/proc/interrupts.c b/fs/proc/interrupts.c
index cb0edc7cb..b346dad1e 100644
--- a/fs/proc/interrupts.c
+++ b/fs/proc/interrupts.c
@@ -27,6 +27,10 @@ static void int_seq_stop(struct seq_file *f, void *v)
 	/* Nothing to do */
 }
 
+/*
+ * 在以下使用int_seq_ops:
+ *   - fs/proc/interrupts.c|39| <<proc_interrupts_init>> proc_create_seq("interrupts", 0, NULL, &int_seq_ops);
+ */
 static const struct seq_operations int_seq_ops = {
 	.start = int_seq_start,
 	.next  = int_seq_next,
diff --git a/fs/select.c b/fs/select.c
index 0ee55af1a..6408af1c4 100644
--- a/fs/select.c
+++ b/fs/select.c
@@ -136,6 +136,11 @@ static void free_poll_entry(struct poll_table_entry *entry)
 	fput(entry->filp);
 }
 
+/*
+ * called by:
+ *   - fs/select.c|612| <<do_select>> poll_freewait(&table);
+ *   - fs/select.c|1016| <<do_sys_poll>> poll_freewait(&table);
+ */
 void poll_freewait(struct poll_wqueues *pwq)
 {
 	struct poll_table_page * p = pwq->table;
diff --git a/include/kvm/arm_pmu.h b/include/kvm/arm_pmu.h
index c0b868ce6..7781ecc7c 100644
--- a/include/kvm/arm_pmu.h
+++ b/include/kvm/arm_pmu.h
@@ -26,12 +26,26 @@ struct kvm_pmu_events {
 };
 
 struct kvm_pmu {
+	/*
+	 * 在以下使用kvm_pmu->overflow_work:
+	 *   - arch/arm64/kvm/pmu-emul.c|440| <<kvm_pmu_vcpu_destroy>> irq_work_sync(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|626| <<kvm_pmu_perf_overflow_notify_vcpu>> pmu = container_of(work, struct kvm_pmu, overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|668| <<kvm_pmu_perf_overflow>> irq_work_queue(&vcpu->arch.pmu.overflow_work);
+	 *   - arch/arm64/kvm/pmu-emul.c|1198| <<kvm_arm_pmu_v3_init>> init_irq_work(&vcpu->arch.pmu.overflow_work, kvm_pmu_perf_overflow_notify_vcpu);
+	 */
 	struct irq_work overflow_work;
 	struct kvm_pmu_events events;
 	struct kvm_pmc pmc[ARMV8_PMU_MAX_COUNTERS];
 	DECLARE_BITMAP(chained, ARMV8_PMU_MAX_COUNTER_PAIRS);
 	int irq_num;
 	bool created;
+	/*
+	 * 在以下使用kvm_pmu->irq_level:
+	 *   - arch/arm64/kvm/pmu-emul.c|586| <<kvm_pmu_update_state>> if (pmu->irq_level == overflow)
+	 *   - arch/arm64/kvm/pmu-emul.c|589| <<kvm_pmu_update_state>> pmu->irq_level = overflow;
+	 *   - arch/arm64/kvm/pmu-emul.c|619| <<kvm_pmu_should_notify_user>> return pmu->irq_level != run_level;
+	 *   - arch/arm64/kvm/pmu-emul.c|635| <<kvm_pmu_update_run>> if (vcpu->arch.pmu.irq_level)
+	 */
 	bool irq_level;
 };
 
@@ -42,8 +56,29 @@ struct arm_pmu_entry {
 
 DECLARE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|289| <<kvm_vm_ioctl_check_extension>> r = kvm_arm_support_pmu_v3();
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|83| <<__activate_traps_common>> if (kvm_arm_support_pmu_v3()) {
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|97| <<__deactivate_traps_common>> if (kvm_arm_support_pmu_v3())
+ *   - arch/arm64/kvm/pmu.c|58| <<kvm_set_pmu_events>> if (!kvm_arm_support_pmu_v3() || !pmu || !kvm_pmu_switch_needed(attr))
+ *   - arch/arm64/kvm/pmu.c|74| <<kvm_clr_pmu_events>> if (!kvm_arm_support_pmu_v3() || !pmu)
+ *   - arch/arm64/kvm/pmu.c|209| <<kvm_vcpu_pmu_restore_guest>> if (!kvm_arm_support_pmu_v3() || !has_vhe())
+ *   - arch/arm64/kvm/pmu.c|234| <<kvm_vcpu_pmu_restore_host>> if (!kvm_arm_support_pmu_v3() || !has_vhe())
+ *   - arch/arm64/kvm/reset.c|301| <<kvm_reset_vcpu>> if (kvm_vcpu_has_pmu(vcpu) && !kvm_arm_support_pmu_v3()) {
+ *   - arch/arm64/kvm/sys_regs.c|610| <<reset_pmu_reg>> if (!kvm_arm_support_pmu_v3())
+ *   - arch/arm64/kvm/sys_regs.c|645| <<reset_pmcr>> if (!kvm_arm_support_pmu_v3())
+ */
 static __always_inline bool kvm_arm_support_pmu_v3(void)
 {
+	/*
+	 * 在以下使用kvm_arm_pmu_available:
+	 *   - arch/arm64/kernel/image-vars.h|116| <<global>> KVM_NVHE_ALIAS(kvm_arm_pmu_available);
+	 *   - arch/arm64/kvm/pmu-emul.c|18| <<global>> DEFINE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+	 *   - include/kvm/arm_pmu.h|43| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_arm_pmu_available);
+	 *   - arch/arm64/kvm/pmu-emul.c|848| <<kvm_host_pmu_init>> static_branch_enable(&kvm_arm_pmu_available);
+	 *   - include/kvm/arm_pmu.h|47| <<kvm_arm_support_pmu_v3>> return static_branch_likely(&kvm_arm_pmu_available);
+	 */
 	return static_branch_likely(&kvm_arm_pmu_available);
 }
 
@@ -85,6 +120,10 @@ void kvm_vcpu_pmu_restore_host(struct kvm_vcpu *vcpu);
  * Must be called before every vcpu run after disabling interrupts, to ensure
  * that an interrupt cannot fire and update the structure.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|912| <<kvm_arch_vcpu_ioctl_run>> kvm_pmu_update_vcpu_events(vcpu);
+ */
 #define kvm_pmu_update_vcpu_events(vcpu)				\
 	do {								\
 		if (!has_vhe() && kvm_vcpu_has_pmu(vcpu))		\
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 4df9e73a8..3ecf8a1ab 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -338,6 +338,23 @@ struct vgic_cpu {
 	 * were one of the two and need to be migrated off this list to another
 	 * VCPU.
 	 */
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|195| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|372| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|158| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|303| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|408| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|636| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|708| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|775| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|807| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|833| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|873| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|912| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|918| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|982| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	struct list_head ap_list_head;
 
 	/*
diff --git a/include/kvm/iodev.h b/include/kvm/iodev.h
index d75fc4365..871100746 100644
--- a/include/kvm/iodev.h
+++ b/include/kvm/iodev.h
@@ -47,6 +47,12 @@ static inline int kvm_iodevice_read(struct kvm_vcpu *vcpu,
 				: -EOPNOTSUPP;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7637| <<vcpu_mmio_write>> !kvm_iodevice_write(vcpu, &vcpu->arch.apic->dev, addr, n, v))
+ *   - virt/kvm/kvm_main.c|5203| <<__kvm_io_bus_write>> if (!kvm_iodevice_write(vcpu, bus->range[idx].dev, range->addr,
+ *   - virt/kvm/kvm_main.c|5281| <<kvm_io_bus_write_cookie>> if (!kvm_iodevice_write(vcpu, bus->range[cookie].dev, addr, len,
+ */
 static inline int kvm_iodevice_write(struct kvm_vcpu *vcpu,
 				     struct kvm_io_device *dev, gpa_t addr,
 				     int l, const void *v)
diff --git a/include/linux/clocksource.h b/include/linux/clocksource.h
index 1d42d4b17..14f0a563a 100644
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@ -138,6 +138,17 @@ struct clocksource {
 #define CLOCK_SOURCE_WATCHDOG			0x10
 #define CLOCK_SOURCE_VALID_FOR_HRES		0x20
 #define CLOCK_SOURCE_UNSTABLE			0x40
+/*
+ * 在以下使用CLOCK_SOURCE_SUSPEND_NONSTOP:
+ *   - drivers/clocksource/clksrc-dbx500-prcmu.c|49| <<global>> .flags = CLOCK_SOURCE_IS_CONTINUOUS | CLOCK_SOURCE_SUSPEND_NONSTOP,
+ *   - drivers/clocksource/timer-pistachio.c|145| <<global>> CLOCK_SOURCE_SUSPEND_NONSTOP,
+ *   - drivers/clocksource/timer-sprd.c|190| <<global>> .flags = CLOCK_SOURCE_IS_CONTINUOUS | CLOCK_SOURCE_SUSPEND_NONSTOP,
+ *   - drivers/clocksource/timer-tegra.c|209| <<global>> .flags = CLOCK_SOURCE_IS_CONTINUOUS | CLOCK_SOURCE_SUSPEND_NONSTOP,
+ *   - arch/x86/kernel/tsc.c|1493| <<init_tsc_clocksource>> clocksource_tsc.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
+ *   - drivers/clocksource/arm_arch_timer.c|1102| <<arch_counter_register>> clocksource_counter.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
+ *   - drivers/clocksource/timer-ti-32k.c|134| <<ti_32k_timer_init>> ti_32k_timer.cs.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;
+ *   - kernel/time/clocksource.c|783| <<__clocksource_suspend_select>> if (!(cs->flags & CLOCK_SOURCE_SUSPEND_NONSTOP))
+ */
 #define CLOCK_SOURCE_SUSPEND_NONSTOP		0x80
 #define CLOCK_SOURCE_RESELECT			0x100
 #define CLOCK_SOURCE_VERIFY_PERCPU		0x200
@@ -201,6 +212,15 @@ static inline u32 clocksource_hz2mult(u32 hz, u32 shift_constant)
  *
  * XXX - This could use some mult_lxl_ll() asm optimization
  */
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|258| <<cs_watchdog_read>> wd_delay = clocksource_cyc2ns(wd_delta, watchdog->mult,
+ *   - kernel/time/clocksource.c|278| <<cs_watchdog_read>> wd_seq_delay = clocksource_cyc2ns(wd_delta, watchdog->mult, watchdog->shift);
+ *   - kernel/time/clocksource.c|390| <<clocksource_verify_percpu>> cs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);
+ *   - kernel/time/clocksource.c|457| <<clocksource_watchdog>> wd_nsec = clocksource_cyc2ns(delta, watchdog->mult,
+ *   - kernel/time/clocksource.c|461| <<clocksource_watchdog>> cs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);
+ *   - kernel/time/clocksource.c|940| <<clocks_calc_max_nsecs>> max_nsecs = clocksource_cyc2ns(max_cycles, mult - maxadj, shift);
+ */
 static inline s64 clocksource_cyc2ns(u64 cycles, u32 mult, u32 shift)
 {
 	return ((u64) cycles * mult) >> shift;
@@ -236,16 +256,35 @@ __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq);
  * Don't call this unless you are a default clocksource
  * (AKA: jiffies) and absolutely have to.
  */
+/*
+ * called by:
+ *   - arch/s390/kernel/time.c|277| <<time_init>> if (__clocksource_register(&clocksource_tod) != 0)
+ *   - kernel/time/clocksource-wdtest.c|117| <<wdtest_func>> __clocksource_register(&clocksource_wdtest_jiffies);
+ *   - kernel/time/jiffies.c|66| <<init_jiffies_clocksource>> return __clocksource_register(&clocksource_jiffies);
+ *   - kernel/time/jiffies.c|102| <<register_refined_jiffies>> __clocksource_register(&refined_jiffies);
+ */
 static inline int __clocksource_register(struct clocksource *cs)
 {
 	return __clocksource_register_scale(cs, 1, 0);
 }
 
+/*
+ * 特别多调用
+ */
 static inline int clocksource_register_hz(struct clocksource *cs, u32 hz)
 {
 	return __clocksource_register_scale(cs, 1, hz);
 }
 
+/*
+ * called by:
+ *   -  arch/hexagon/kernel/time.c|180| <<time_init_deferred>> clocksource_register_khz(&hexagon_clocksource, pcycle_freq_mhz * 1000);
+ *   - arch/x86/kernel/tsc.c|1517| <<tsc_refine_calibration_work>> clocksource_register_khz(&clocksource_tsc, tsc_khz);
+ *   - arch/x86/kernel/tsc.c|1541| <<init_tsc_clocksource>> clocksource_register_khz(&clocksource_tsc, tsc_khz);
+ *   - arch/x86/kernel/tsc.c|1670| <<tsc_init>> clocksource_register_khz(&clocksource_tsc_early, tsc_khz);
+ *   - kernel/time/clocksource-wdtest.c|99| <<wdtest_ktime_clocksource_reset>> clocksource_register_khz(&clocksource_wdtest_ktime, 1000 * 1000);
+ *   - kernel/time/clocksource-wdtest.c|132| <<wdtest_func>> clocksource_register_khz(&clocksource_wdtest_ktime, 1000 * 1000);
+ */
 static inline int clocksource_register_khz(struct clocksource *cs, u32 khz)
 {
 	return __clocksource_register_scale(cs, 1000, khz);
diff --git a/include/linux/irqbypass.h b/include/linux/irqbypass.h
index 9bdb2a781..4c73bb2b0 100644
--- a/include/linux/irqbypass.h
+++ b/include/linux/irqbypass.h
@@ -46,8 +46,20 @@ struct irq_bypass_producer {
 	struct list_head node;
 	void *token;
 	int irq;
+	/*
+	 * 在以下使用irq_bypass_producer->add_consumer:
+	 *   - virt/lib/irqbypass.c|62| <<__connect>> if (prod->add_consumer)
+	 *   - virt/lib/irqbypass.c|63| <<__connect>> ret = prod->add_consumer(prod, cons);
+	 */
 	int (*add_consumer)(struct irq_bypass_producer *,
 			    struct irq_bypass_consumer *);
+	/*
+	 * 在以下使用irq_bypass_producer->del_consumer:
+	 *   - virt/lib/irqbypass.c|67| <<__connect>> if (ret && prod->del_consumer)
+	 *   - virt/lib/irqbypass.c|68| <<__connect>> prod->del_consumer(prod, cons);
+	 *   - virt/lib/irqbypass.c|90| <<__disconnect>> if (prod->del_consumer)
+	 *   - virt/lib/irqbypass.c|91| <<__disconnect>> prod->del_consumer(prod, cons);
+	 */
 	void (*del_consumer)(struct irq_bypass_producer *,
 			     struct irq_bypass_consumer *);
 	void (*stop)(struct irq_bypass_producer *);
@@ -71,8 +83,20 @@ struct irq_bypass_producer {
 struct irq_bypass_consumer {
 	struct list_head node;
 	void *token;
+	/*
+	 * 在以下使用irq_bypass_consumer->add_producer:
+	 *   - virt/kvm/eventfd.c|432| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+	 *   - virt/lib/irqbypass.c|66| <<__connect>> ret = cons->add_producer(cons, prod);
+	 *   - virt/lib/irqbypass.c|217| <<irq_bypass_register_consumer>> !consumer->add_producer || !consumer->del_producer)
+	 */
 	int (*add_producer)(struct irq_bypass_consumer *,
 			    struct irq_bypass_producer *);
+	/*
+	 * 在以下使用irq_bypass_consumer->del_producer:
+	 *   - virt/kvm/eventfd.c|433| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+	 *   - virt/lib/irqbypass.c|88| <<__disconnect>> cons->del_producer(cons, prod);
+	 *   - virt/lib/irqbypass.c|217| <<irq_bypass_register_consumer>> !consumer->add_producer || !consumer->del_producer)
+	 */
 	void (*del_producer)(struct irq_bypass_consumer *,
 			     struct irq_bypass_producer *);
 	void (*stop)(struct irq_bypass_consumer *);
diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
index 1cd4e3689..df2bf362f 100644
--- a/include/linux/irqdesc.h
+++ b/include/linux/irqdesc.h
@@ -55,6 +55,31 @@ struct pt_regs;
 struct irq_desc {
 	struct irq_common_data	irq_common_data;
 	struct irq_data		irq_data;
+	/*
+	 * 使用irq_desc->kstat_irqs:
+	 *   - arch/mips/dec/setup.c|759| <<arch_init_irq>> fpu_kstat_irq = this_cpu_ptr(desc_fpu->kstat_irqs);
+	 *   - arch/parisc/kernel/smp.c|349| <<smp_boot_one_cpu>> if (desc && desc->kstat_irqs)
+	 *   - arch/parisc/kernel/smp.c|350| <<smp_boot_one_cpu>> *per_cpu_ptr(desc->kstat_irqs, cpuid) = 0;
+	 *   - arch/powerpc/kvm/book3s_hv_rm_xics.c|840| <<kvmppc_rm_handle_irq_desc>> this_cpu_inc_rm(desc->kstat_irqs);
+	 *   - include/linux/irqdesc.h|119| <<irq_desc_kstat_cpu>> return desc->kstat_irqs ? *per_cpu_ptr(desc->kstat_irqs, cpu) : 0;
+	 *   - kernel/irq/internals.h|254| <<__kstat_incr_irqs_this_cpu>> __this_cpu_inc(*desc->kstat_irqs);
+	 *   - kernel/irq/irqdesc.c|126| <<desc_set_defaults>> *per_cpu_ptr(desc->kstat_irqs, cpu) = 0;
+	 *   - kernel/irq/irqdesc.c|399| <<alloc_desc>> desc->kstat_irqs = alloc_percpu(unsigned int );
+	 *   - kernel/irq/irqdesc.c|400| <<alloc_desc>> if (!desc->kstat_irqs)
+	 *   - kernel/irq/irqdesc.c|419| <<alloc_desc>> free_percpu(desc->kstat_irqs);
+	 *   - kernel/irq/irqdesc.c|430| <<irq_kobj_release>> free_percpu(desc->kstat_irqs);
+	 *   - kernel/irq/irqdesc.c|574| <<early_irq_init>> desc[i].kstat_irqs = alloc_percpu(unsigned int );
+	 *   - kernel/irq/irqdesc.c|909| <<kstat_irqs_cpu>> return desc && desc->kstat_irqs ?
+	 *   - kernel/irq/irqdesc.c|910| <<kstat_irqs_cpu>> *per_cpu_ptr(desc->kstat_irqs, cpu) : 0;
+	 *   - kernel/irq/irqdesc.c|918| <<kstat_irqs>> static unsigned int kstat_irqs(unsigned int irq)
+	 *   - kernel/irq/irqdesc.c|924| <<kstat_irqs>> if (!desc || !desc->kstat_irqs)
+	 *   - kernel/irq/irqdesc.c|932| <<kstat_irqs>> sum += data_race(*per_cpu_ptr(desc->kstat_irqs, cpu));
+	 *   - kernel/irq/irqdesc.c|951| <<kstat_irqs_usr>> sum = kstat_irqs(irq);
+	 *   - kernel/irq/proc.c|491| <<show_interrupts>> if (desc->kstat_irqs) {
+	 *   - kernel/irq/proc.c|493| <<show_interrupts>> any_count |= data_race(*per_cpu_ptr(desc->kstat_irqs, j));
+	 *   - kernel/irq/proc.c|501| <<show_interrupts>> seq_printf(p, "%10u ", desc->kstat_irqs ?
+	 *   - kernel/irq/proc.c|502| <<show_interrupts>> *per_cpu_ptr(desc->kstat_irqs, j) : 0);
+	 */
 	unsigned int __percpu	*kstat_irqs;
 	irq_flow_handler_t	handle_irq;
 	struct irqaction	*action;	/* IRQ action list */
@@ -113,6 +138,14 @@ static inline void irq_unlock_sparse(void) { }
 extern struct irq_desc irq_desc[NR_IRQS];
 #endif
 
+/*
+ * called by:
+ *   - arch/arm/kernel/smp.c|560| <<show_ipi_list>> seq_printf(p, "%10u ", irq_desc_kstat_cpu(ipi_desc[i], cpu));
+ *   - arch/arm64/kernel/smp.c|787| <<arch_show_interrupts>> seq_printf(p, "%10u ", irq_desc_kstat_cpu(ipi_desc[i], cpu));
+ *   - arch/parisc/kernel/irq.c|204| <<show_interrupts>> seq_printf(p, "%10u ", irq_desc_kstat_cpu(desc, j));
+ *   - arch/s390/kernel/irq.c|216| <<show_msi_interrupt>> seq_printf(p, "%10u ", irq_desc_kstat_cpu(desc, cpu));
+ *   - kernel/irq/irqdesc.c|155| <<per_cpu_count_show>> unsigned int c = irq_desc_kstat_cpu(desc, cpu);
+ */
 static inline unsigned int irq_desc_kstat_cpu(struct irq_desc *desc,
 					      unsigned int cpu)
 {
diff --git a/include/linux/irqnr.h b/include/linux/irqnr.h
index 3496baa0b..699acb614 100644
--- a/include/linux/irqnr.h
+++ b/include/linux/irqnr.h
@@ -9,6 +9,24 @@ extern int nr_irqs;
 extern struct irq_desc *irq_to_desc(unsigned int irq);
 unsigned int irq_get_next_irq(unsigned int offset);
 
+/*
+ * called by:
+ *   - arch/arm/kernel/machine_kexec.c|121| <<machine_kexec_mask_interrupts>> for_each_irq_desc(i, desc) {
+ *   - arch/arm64/kernel/machine_kexec.c|228| <<machine_kexec_mask_interrupts>> for_each_irq_desc(i, desc) {
+ *   - arch/powerpc/kexec/core.c|29| <<machine_kexec_mask_interrupts>> for_each_irq_desc(i, desc) {
+ *   - arch/powerpc/sysdev/xics/xics-common.c|198| <<xics_migrate_irqs_away>> for_each_irq_desc(virq, desc) {
+ *   - arch/powerpc/sysdev/xive/common.c|334| <<xmon_xive_get_irq_all>> for_each_irq_desc(i, desc) {
+ *   - arch/powerpc/sysdev/xive/common.c|1777| <<xive_irq_debug_show>> for_each_irq_desc(i, desc) {
+ *   - kernel/irq/autoprobe.c|86| <<probe_irq_on>> for_each_irq_desc(i, desc) {
+ *   - kernel/irq/autoprobe.c|123| <<probe_irq_mask>> for_each_irq_desc(i, desc) {
+ *   - kernel/irq/autoprobe.c|162| <<probe_irq_off>> for_each_irq_desc(i, desc) {
+ *   - kernel/irq/irqdesc.c|325| <<irq_sysfs_init>> for_each_irq_desc(irq, desc)
+ *   - kernel/irq/pm.c|136| <<suspend_device_irqs>> for_each_irq_desc(irq, desc) {
+ *   - kernel/irq/pm.c|188| <<resume_irqs>> for_each_irq_desc(irq, desc) {
+ *   - kernel/irq/proc.c|445| <<init_irq_proc>> for_each_irq_desc(irq, desc)
+ *   - kernel/irq/spurious.c|128| <<misrouted_irq>> for_each_irq_desc(i, desc) {
+ *   - kernel/irq/spurious.c|153| <<poll_spurious_irqs>> for_each_irq_desc(i, desc) {
+ */
 # define for_each_irq_desc(irq, desc)					\
 	for (irq = 0, desc = irq_to_desc(irq); irq < nr_irqs;		\
 	     irq++, desc = irq_to_desc(irq))				\
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index f4519d368..de66658b5 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -449,6 +449,15 @@ static __always_inline void guest_enter_irqoff(void)
  *
  * Note: this is analogous to exit_to_user_mode().
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|842| <<kvm_arm_vcpu_enter_exit>> guest_state_enter_irqoff();
+ *   - arch/mips/kvm/mips.c|428| <<kvm_mips_vcpu_enter_exit>> guest_state_enter_irqoff();
+ *   - arch/mips/kvm/mips.c|1397| <<kvm_mips_handle_exit>> guest_state_enter_irqoff();
+ *   - arch/riscv/kvm/vcpu.c|909| <<kvm_riscv_vcpu_enter_exit>> guest_state_enter_irqoff();
+ *   - arch/x86/kvm/svm/svm.c|3941| <<svm_vcpu_enter_exit>> guest_state_enter_irqoff();
+ *   - arch/x86/kvm/vmx/vmx.c|7144| <<vmx_vcpu_enter_exit>> guest_state_enter_irqoff();
+ */
 static __always_inline void guest_state_enter_irqoff(void)
 {
 	instrumentation_begin();
@@ -531,6 +540,13 @@ static __always_inline void guest_state_exit_irqoff(void)
 	instrumentation_end();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|74| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - arch/riscv/kvm/vcpu.c|227| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - arch/x86/kvm/x86.c|13784| <<kvm_arch_vcpu_should_kick>> return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+ *   - virt/kvm/kvm_main.c|244| <<kvm_request_needs_ipi>> int mode = kvm_vcpu_exiting_guest_mode(vcpu);
+ */
 static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -539,6 +555,11 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 	 * memory barrier following the write of vcpu->mode in VCPU RUN.
 	 */
 	smp_mb__before_atomic();
+	/*
+	 * 将old和ptr指向的内容比较.
+	 * 如果相等,则将new写入到ptr中,返回old.
+	 * 如果不相等,则返回ptr指向的内容.
+	 */
 	return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
 }
 
@@ -729,6 +750,11 @@ struct kvm {
 	 */
 	atomic_t online_vcpus;
 	int max_vcpus;
+	/*
+	 * 在以下修改kvm->created_vcpus:
+	 *   - virt/kvm/kvm_main.c|3921| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus++;
+	 *   - virt/kvm/kvm_main.c|3995| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus--;
+	 */
 	int created_vcpus;
 	int last_boosted_vcpu;
 	struct list_head vm_list;
@@ -741,10 +767,28 @@ struct kvm {
 		struct list_head  resampler_list;
 		struct mutex      resampler_lock;
 	} irqfds;
+	/*
+	 * 在以下使用kvm->ioeventfds:
+	 *   - virt/kvm/eventfd.c|532| <<kvm_eventfd_init>> INIT_LIST_HEAD(&kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|783| <<ioeventfd_check_collision>> list_for_each_entry(_p, &kvm->ioeventfds, list)
+	 *   - virt/kvm/eventfd.c|851| <<kvm_assign_ioeventfd_idx>> list_add_tail(&p->list, &kvm->ioeventfds);
+	 *   - virt/kvm/eventfd.c|885| <<kvm_deassign_ioeventfd_idx>> list_for_each_entry_safe(p, tmp, &kvm->ioeventfds, list) {
+	 */
 	struct list_head ioeventfds;
 #endif
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - include/linux/kvm_host.h|876| <<kvm_get_bus>> !refcount_read(&kvm->users_count));
+	 *   - include/linux/kvm_host.h|955| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1176| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1254| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1337| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1347| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1353| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1367| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
@@ -845,6 +889,32 @@ static inline void kvm_vm_bugged(struct kvm *kvm)
 	unlikely(__ret);					\
 })
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|173| <<kvmppc_mmu_walk_radix_tree>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|251| <<kvmppc_mmu_radix_translate_table>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|309| <<kvmhv_enter_nested_guest>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|413| <<kvmhv_enter_nested_guest>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|597| <<kvmhv_copy_tofrom_guest_nested>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/book3s_hv_nested.c|604| <<kvmhv_copy_tofrom_guest_nested>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/book3s_rtas.c|232| <<kvmppc_rtas_hcall>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|430| <<kvmppc_ld>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|858| <<kvm_riscv_check_vcpu_requests>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|924| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|1003| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|1057| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/riscv/kvm/vcpu_insn.c|193| <<kvm_riscv_vcpu_wfi>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1343| <<kvm_s390_handle_wait>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4631| <<__vcpu_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4675| <<__vcpu_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/s390/kvm/vsie.c|1144| <<do_vsie_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/x86/kvm/x86.c|11533| <<vcpu_enter_guest>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/x86/kvm/x86.c|11675| <<vcpu_enter_guest>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/x86/kvm/x86.c|11733| <<vcpu_block>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/x86/kvm/x86.c|11824| <<vcpu_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/x86/kvm/x86.c|11951| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ *   - arch/x86/kvm/x86.c|11965| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_srcu_read_lock(vcpu);
+ */
 static inline void kvm_vcpu_srcu_read_lock(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_PROVE_RCU
diff --git a/include/linux/pci.h b/include/linux/pci.h
index 060af91ba..29eacf198 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -458,6 +458,11 @@ struct pci_dev {
 	unsigned int	irq_managed:1;
 	unsigned int	non_compliant_bars:1;	/* Broken BARs; ignore them */
 	unsigned int	is_probed:1;		/* Device probing in progress */
+	/*
+	 * 在以下使用pci_dev->link_active_reporting:
+	 *   - drivers/pci/pci.c|4933| <<pcie_wait_for_link_delay>> if (!pdev->link_active_reporting) {
+	 *   - drivers/pci/probe.c|822| <<pci_set_bus_speed>> bridge->link_active_reporting = !!(linkcap & PCI_EXP_LNKCAP_DLLLARC);
+	 */
 	unsigned int	link_active_reporting:1;/* Device capable of reporting link active */
 	unsigned int	no_vf_scan:1;		/* Don't scan for VFs after IOV enablement */
 	unsigned int	no_command_memory:1;	/* No PCI_COMMAND_MEMORY */
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index ee8b9ecdc..574e2456e 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -151,6 +151,18 @@ struct hw_perf_event {
 			u64		last_tag;
 			unsigned long	config_base;
 			unsigned long	event_base;
+			/*
+			 * 在以下使用hw_perf_event->event_base_rdpmc:
+			 *   - arch/x86/events/amd/uncore.c|94| <<amd_uncore_read>> rdpmcl(hwc->event_base_rdpmc, new);
+			 *   - arch/x86/events/amd/uncore.c|158| <<amd_uncore_add>> hwc->event_base_rdpmc = uncore->rdpmc_base + hwc->idx;
+			 *   - arch/x86/events/amd/uncore.c|169| <<amd_uncore_add>> hwc->event_base_rdpmc += NUM_COUNTERS_L3;
+			 *   - arch/x86/events/core.c|131| <<x86_perf_event_update>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+			 *   - arch/x86/events/core.c|1242| <<x86_assign_hw_event>> hwc->event_base_rdpmc = (idx - INTEL_PMC_IDX_FIXED) |
+			 *   - arch/x86/events/core.c|1249| <<x86_assign_hw_event>> hwc->event_base_rdpmc = x86_pmu_rdpmc_index(hwc->idx);
+			 *   - arch/x86/events/core.c|1272| <<x86_perf_rdpmc_index>> return event->hw.event_base_rdpmc;
+			 *   - arch/x86/events/core.c|2576| <<x86_pmu_event_idx>> return hwc->event_base_rdpmc + 1;
+			 *   - arch/x86/events/intel/ds.c|1880| <<intel_pmu_save_and_restart_reload>> rdpmcl(hwc->event_base_rdpmc, new_raw_count);
+			 */
 			int		event_base_rdpmc;
 			int		idx;
 			int		last_cpu;
@@ -304,6 +316,31 @@ struct pmu {
 	int				capabilities;
 
 	int __percpu			*pmu_disable_count;
+	/*
+	 * 在以下使用percpu的pmu->pmu_cpu_context:
+	 *   - arch/arm64/kernel/perf_event.c|819| <<armv8pmu_start>> this_cpu_ptr(cpu_pmu->pmu.pmu_cpu_context)->task_ctx;
+	 *   - arch/x86/events/core.c|2106| <<x86_pmu_update_cpu_context>> if (!pmu->pmu_cpu_context)
+	 *   - arch/x86/events/core.c|2109| <<x86_pmu_update_cpu_context>> cpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);
+	 *   - kernel/events/core.c|160| <<__get_cpu_context>> return this_cpu_ptr(ctx->pmu->pmu_cpu_context);
+	 *   - kernel/events/core.c|913| <<perf_cgroup_ensure_storage>> cpuctx = per_cpu_ptr(event->pmu->pmu_cpu_context, cpu);
+	 *   - kernel/events/core.c|2761| <<perf_pmu_resched>> struct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
+	 *   - kernel/events/core.c|3551| <<perf_sched_cb_dec>> struct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
+	 *   - kernel/events/core.c|3570| <<perf_sched_cb_inc>> struct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
+	 *   - kernel/events/core.c|4775| <<find_get_context>> cpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);
+	 *   - kernel/events/core.c|7980| <<__perf_pmu_output_stop>> struct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
+	 *   - kernel/events/core.c|11242| <<find_pmu_context>> return pmu->pmu_cpu_context;
+	 *   - kernel/events/core.c|11258| <<free_pmu_context>> free_percpu(pmu->pmu_cpu_context);
+	 *   - kernel/events/core.c|11323| <<perf_event_mux_interval_ms_store>> cpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);
+	 *   - kernel/events/core.c|11453| <<perf_pmu_register>> pmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);
+	 *   - kernel/events/core.c|11454| <<perf_pmu_register>> if (pmu->pmu_cpu_context)
+	 *   - kernel/events/core.c|11458| <<perf_pmu_register>> pmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);
+	 *   - kernel/events/core.c|11459| <<perf_pmu_register>> if (!pmu->pmu_cpu_context)
+	 *   - kernel/events/core.c|11465| <<perf_pmu_register>> cpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);
+	 *   - kernel/events/core.c|13049| <<perf_pmu_migrate_context>> src_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;
+	 *   - kernel/events/core.c|13050| <<perf_pmu_migrate_context>> dst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;
+	 *   - kernel/events/core.c|13796| <<perf_event_exit_cpu_context>> cpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);
+	 *   - kernel/events/core.c|13824| <<perf_event_init_cpu>> cpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);
+	 */
 	struct perf_cpu_context __percpu *pmu_cpu_context;
 	atomic_t			exclusive_cnt; /* < 0: cpu; > 0: tsk */
 	int				task_ctx_nr;
@@ -1224,9 +1261,21 @@ static inline void perf_event_task_migrate(struct task_struct *task)
 		task->sched_migrated = 1;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|5057| <<finish_task_switch>> perf_event_task_sched_in(prev, current);
+ */
 static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
 {
+	/*
+	 * 在以下使用perf_sched_events:
+	 *   - kernel/events/core.c|391| <<global>> DEFINE_STATIC_KEY_FALSE(perf_sched_events);
+	 *   - include/linux/perf_event.h|1271| <<perf_event_task_sched_in>> if (static_branch_unlikely(&perf_sched_events))
+	 *   - include/linux/perf_event.h|1294| <<perf_event_task_sched_out>> if (static_branch_unlikely(&perf_sched_events))
+	 *   - kernel/events/core.c|4990| <<perf_sched_delayed>> static_branch_disable(&perf_sched_events);
+	 *   - kernel/events/core.c|11838| <<account_event>> static_branch_enable(&perf_sched_events);
+	 */
 	if (static_branch_unlikely(&perf_sched_events))
 		__perf_event_task_sched_in(prev, task);
 
@@ -1237,6 +1286,10 @@ static inline void perf_event_task_sched_in(struct task_struct *prev,
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|4992| <<prepare_task_switch>> perf_event_task_sched_out(prev, next);
+ */
 static inline void perf_event_task_sched_out(struct task_struct *prev,
 					     struct task_struct *next)
 {
@@ -1250,6 +1303,14 @@ static inline void perf_event_task_sched_out(struct task_struct *prev,
 		__perf_sw_event_sched(PERF_COUNT_SW_CGROUP_SWITCHES, 1, 0);
 #endif
 
+	/*
+	 * 在以下使用perf_sched_events:
+	 *   - kernel/events/core.c|391| <<global>> DEFINE_STATIC_KEY_FALSE(perf_sched_events);
+	 *   - include/linux/perf_event.h|1271| <<perf_event_task_sched_in>> if (static_branch_unlikely(&perf_sched_events))
+	 *   - include/linux/perf_event.h|1294| <<perf_event_task_sched_out>> if (static_branch_unlikely(&perf_sched_events))
+	 *   - kernel/events/core.c|4990| <<perf_sched_delayed>> static_branch_disable(&perf_sched_events);
+	 *   - kernel/events/core.c|11838| <<account_event>> static_branch_enable(&perf_sched_events);
+	 */
 	if (static_branch_unlikely(&perf_sched_events))
 		__perf_event_task_sched_out(prev, next);
 }
diff --git a/include/linux/sched.h b/include/linux/sched.h
index e7b2f8a5c..6f0f6b3e2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1230,6 +1230,15 @@ struct task_struct {
 #ifdef CONFIG_PERF_EVENTS
 	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];
 	struct mutex			perf_event_mutex;
+	/*
+	 * 在以下使用task_struct->perf_event_list:
+	 *   - init/init_task.c|143| <<global>> init_task.perf_event_list = LIST_HEAD_INIT(init_task.perf_event_list),
+	 *   - kernel/events/core.c|5868| <<perf_event_task_enable>> list_for_each_entry(event, &current->perf_event_list, owner_entry) {
+	 *   - kernel/events/core.c|5884| <<perf_event_task_disable>> list_for_each_entry(event, &current->perf_event_list, owner_entry) {
+	 *   - kernel/events/core.c|12813| <<SYSCALL_DEFINE5(perf_event_open)>> list_add_tail(&event->owner_entry, &current->perf_event_list);
+	 *   - kernel/events/core.c|13200| <<perf_event_exit_task>> list_for_each_entry_safe(event, tmp, &child->perf_event_list,
+	 *   - kernel/events/core.c|13657| <<perf_event_init_task>> INIT_LIST_HEAD(&child->perf_event_list);
+	 */
 	struct list_head		perf_event_list;
 #endif
 #ifdef CONFIG_DEBUG_PREEMPT
diff --git a/include/linux/time_namespace.h b/include/linux/time_namespace.h
index 3146f1c05..bfe76abba 100644
--- a/include/linux/time_namespace.h
+++ b/include/linux/time_namespace.h
@@ -62,6 +62,13 @@ struct proc_timens_offset {
 int proc_timens_set_offset(struct file *file, struct task_struct *p,
 			   struct proc_timens_offset *offsets, int n);
 
+/*
+ * called by:
+ *   - kernel/time/posix-stubs.c|86| <<do_clock_gettime>> timens_add_monotonic(tp);
+ *   - kernel/time/posix-timers.c|200| <<posix_get_monotonic_timespec>> timens_add_monotonic(tp);
+ *   - kernel/time/posix-timers.c|215| <<posix_get_monotonic_raw>> timens_add_monotonic(tp);
+ *   - kernel/time/posix-timers.c|230| <<posix_get_monotonic_coarse>> timens_add_monotonic(tp);
+ */
 static inline void timens_add_monotonic(struct timespec64 *ts)
 {
 	struct timens_offsets *ns_offsets = &current->nsproxy->time_ns->offsets;
diff --git a/include/linux/timekeeper_internal.h b/include/linux/timekeeper_internal.h
index 84ff2844d..f634adb70 100644
--- a/include/linux/timekeeper_internal.h
+++ b/include/linux/timekeeper_internal.h
@@ -34,10 +34,47 @@
 struct tk_read_base {
 	struct clocksource	*clock;
 	u64			mask;
+	/*
+	 * 在以下修改tk_read_base->cycle_last:
+	 *   - kernel/time/timekeeping.c|327| <<tk_setup_internals>> tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
+	 *   - kernel/time/timekeeping.c|331| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+	 *   - kernel/time/timekeeping.c|814| <<timekeeping_forward_now>> tk->tkr_mono.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1864| <<timekeeping_resume>> tk->tkr_mono.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1865| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now
+	 *   - kernel/time/timekeeping.c|2170| <<logarithmic_accumulation>> tk->tkr_mono.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2171| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2217| <<timekeeping_advance>> tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
+	 */
 	u64			cycle_last;
 	u32			mult;
 	u32			shift;
+	/*
+	 * 在以下设置tk_read_base->xtime_nsec:
+	 *   - kernel/time/timekeeping.c|125| <<tk_normalize_xtime>> tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
+	 *   - kernel/time/timekeeping.c|129| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+	 *   - kernel/time/timekeeping.c|146| <<tk_set_xtime>> tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;
+	 *   - kernel/time/timekeeping.c|152| <<tk_xtime_add>> tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;
+	 *   - kernel/time/timekeeping.c|383| <<tk_setup_internals>> tk->tkr_mono.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|384| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|386| <<tk_setup_internals>> tk->tkr_mono.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|387| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|901| <<timekeeping_forward_now>> tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;
+	 *   - kernel/time/timekeeping.c|902| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+	 *   - kernel/time/timekeeping.c|2139| <<timekeeping_apply_adjustment>> tk->tkr_mono.xtime_nsec -= offset;
+	 *   - kernel/time/timekeeping.c|2193| <<timekeeping_adjust>> tk->tkr_mono.xtime_nsec += (u64)NSEC_PER_SEC <<
+	 *   - kernel/time/timekeeping.c|2215| <<accumulate_nsecs_to_secs>> tk->tkr_mono.xtime_nsec -= nsecps;
+	 *   - kernel/time/timekeeping.c|2275| <<logarithmic_accumulation>> tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
+	 *   - kernel/time/timekeeping.c|2279| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 *   - kernel/time/timekeeping.c|2282| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+	 *   - kernel/time/vsyscall.c|99| <<update_vsyscall>> nsec = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;
+	 */
 	u64			xtime_nsec;
+	/*
+	 * 在以下修改tk_read_base->base:
+	 *   - kernel/time/timekeeping.c|754| <<tk_update_ktime_data>> tk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);
+	 *   - kernel/time/timekeeping.c|767| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+	 */
 	ktime_t			base;
 	u64			base_real;
 };
@@ -91,24 +128,127 @@ struct tk_read_base {
  */
 struct timekeeper {
 	struct tk_read_base	tkr_mono;
+	/*
+	 * 在以下修改timekeeper->tkr_raw的内容:
+	 *   - kernel/time/timekeeping.c|255| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+	 *   - kernel/time/timekeeping.c|536| <<tk_setup_internals>> tk->tkr_raw.clock = clock;
+	 *   - kernel/time/timekeeping.c|537| <<tk_setup_internals>> tk->tkr_raw.mask = clock->mask;
+	 *   - kernel/time/timekeeping.c|538| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+	 *   - kernel/time/timekeeping.c|567| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|570| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|575| <<tk_setup_internals>> tk->tkr_raw.shift = clock->shift;
+	 *   - kernel/time/timekeeping.c|587| <<tk_setup_internals>> tk->tkr_raw.mult = clock->mult;
+	 *   - kernel/time/timekeeping.c|1062| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+	 *   - kernel/time/timekeeping.c|1138| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1141| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+	 *   - kernel/time/timekeeping.c|2276| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|2599| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2610| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 *   - kernel/time/timekeeping.c|2613| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+	 */
 	struct tk_read_base	tkr_raw;
+	/*
+	 * 在以下使用timekeeper->xtime_sec:
+	 *   - kernel/time/timekeeping.c|121| <<tk_normalize_xtime>> tk->xtime_sec++;
+	 *   - kernel/time/timekeeping.c|140| <<tk_set_xtime>> tk->xtime_sec = ts->tv_sec;
+	 *   - kernel/time/timekeeping.c|146| <<tk_xtime_add>> tk->xtime_sec += ts->tv_sec;
+	 *   - kernel/time/timekeeping.c|2072| <<timekeeping_adjust>> tk->xtime_sec--;
+	 *   - kernel/time/timekeeping.c|2093| <<accumulate_nsecs_to_secs>> tk->xtime_sec++;
+	 *   - kernel/time/timekeeping.c|2109| <<accumulate_nsecs_to_secs>> tk->xtime_sec += leap;
+	 */
 	u64			xtime_sec;
+	/*
+	 * 在以下使用timekeeper->ktime_sec:
+	 *   - kernel/time/timekeeping.c|1188| <<tk_update_ktime_data>> tk->ktime_sec = seconds;
+	 *   - kernel/time/timekeeping.c|1576| <<ktime_get_seconds>> return tk->ktime_sec;
+	 */
 	unsigned long		ktime_sec;
+	/*
+	 * x86和arm在以下使用timekeeper->wall_to_monotonic:
+	 *   - arch/arm/xen/enlighten.c|111| <<xen_pvclock_gtod_notify>> system_time = timespec64_add(now, tk->wall_to_monotonic);
+	 *   - kernel/time/timekeeping.c|413| <<tk_set_wall_to_mono>> set_normalized_timespec64(&tmp, -tk->wall_to_monotonic.tv_sec,
+	 *   - kernel/time/timekeeping.c|414| <<tk_set_wall_to_mono>> -tk->wall_to_monotonic.tv_nsec);
+	 *   - kernel/time/timekeeping.c|416| <<tk_set_wall_to_mono>> tk->wall_to_monotonic = wtm;
+	 *   - kernel/time/timekeeping.c|1176| <<tk_update_ktime_data>> seconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
+	 *   - kernel/time/timekeeping.c|1177| <<tk_update_ktime_data>> nsec = (u32) tk->wall_to_monotonic.tv_nsec;
+	 *   - kernel/time/timekeeping.c|1552| <<ktime_get_ts64>> tomono = tk->wall_to_monotonic;
+	 *   - kernel/time/timekeeping.c|1920| <<do_settimeofday64>> if (timespec64_compare(&tk->wall_to_monotonic, &ts_delta) > 0) {
+	 *   - kernel/time/timekeeping.c|1925| <<do_settimeofday64>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts_delta));
+	 *   - kernel/time/timekeeping.c|1974| <<timekeeping_inject_offset>> if (timespec64_compare(&tk->wall_to_monotonic, ts) > 0 ||
+	 *   - kernel/time/timekeeping.c|1981| <<timekeeping_inject_offset>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *ts));
+	 *   - kernel/time/timekeeping.c|2351| <<__timekeeping_inject_sleeptime>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));
+	 *   - kernel/time/timekeeping.c|2763| <<accumulate_nsecs_to_secs>> timespec64_sub(tk->wall_to_monotonic, ts));
+	 *   - kernel/time/timekeeping.c|3119| <<ktime_get_coarse_ts64>> mono = tk->wall_to_monotonic;
+	 *   - kernel/time/vsyscall.c|35| <<update_vdso_data>> vdso_ts->sec = tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
+	 *   - kernel/time/vsyscall.c|38| <<update_vdso_data>> nsec += ((u64)tk->wall_to_monotonic.tv_nsec << tk->tkr_mono.shift);
+	 *   - kernel/time/vsyscall.c|98| <<update_vsyscall>> vdso_ts->sec = tk->xtime_sec + tk->wall_to_monotonic.tv_sec;
+	 *   - kernel/time/vsyscall.c|100| <<update_vsyscall>> nsec = nsec + tk->wall_to_monotonic.tv_nsec;
+	 */
 	struct timespec64	wall_to_monotonic;
 	ktime_t			offs_real;
+	/*
+	 * 在以下使用timekeeper->offs_boot:
+	 *   - kernel/time/timekeeping.c|1124| <<global>> [TK_OFFS_BOOT] = &tk_core.timekeeper.offs_boot,
+	 *   - arch/x86/kvm/x86.c|2377| <<update_pvclock_gtod>> vdata->offs_boot = tk->offs_boot;
+	 *   - kernel/time/timekeeping.c|295| <<tk_update_sleep_time>> tk->offs_boot = ktime_add(tk->offs_boot, delta);
+	 *   - kernel/time/timekeeping.c|300| <<tk_update_sleep_time>> tk->monotonic_to_boot = ktime_to_timespec64(tk->offs_boot);
+	 *   - kernel/time/timekeeping.c|737| <<ktime_get_boot_fast_ns>> return (ktime_get_mono_fast_ns() + ktime_to_ns(data_race(tk->offs_boot)));
+	 *   - kernel/time/timekeeping.c|839| <<ktime_get_fast_timestamps>> snapshot->boot = snapshot->mono + ktime_to_ns(data_race(tk->offs_boot));
+	 *   - kernel/time/timekeeping.c|2580| <<getboottime64>> ktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);
+	 *   - kernel/time/timekeeping.c|2657| <<ktime_get_update_offsets_now>> *offs_boot = tk->offs_boot;
+	 */
 	ktime_t			offs_boot;
 	ktime_t			offs_tai;
 	s32			tai_offset;
 	unsigned int		clock_was_set_seq;
 	u8			cs_was_changed_seq;
 	ktime_t			next_leap_ktime;
+	/*
+	 * 在以下使用timekeeper->raw_sec:
+	 *   - kernel/time/timekeeping.c|225| <<tk_normalize_xtime>> tk->raw_sec++;
+	 *   - kernel/time/timekeeping.c|1031| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+	 *   - kernel/time/timekeeping.c|1934| <<ktime_get_raw_ts64>> ts->tv_sec = tk->raw_sec;
+	 *   - kernel/time/timekeeping.c|2089| <<timekeeping_init>> tk->raw_sec = 0;
+	 *   - kernel/time/timekeeping.c|2583| <<logarithmic_accumulation>> tk->raw_sec++;
+	 *   - kernel/time/vsyscall.c|63| <<update_vdso_data>> vdso_ts->sec = tk->raw_sec;
+	 */
 	u64			raw_sec;
+	/*
+	 * 在以下使用timekeeper->monotonic_to_boot:
+	 *   - kernel/time/timekeeping.c|518| <<tk_update_sleep_time>> tk->monotonic_to_boot = ktime_to_timespec64(tk->offs_boot);
+	 *   - kernel/time/vsyscall.c|48| <<update_vdso_data>> sec += tk->monotonic_to_boot.tv_sec;
+	 *   - kernel/time/vsyscall.c|49| <<update_vdso_data>> nsec += (u64)tk->monotonic_to_boot.tv_nsec << tk->tkr_mono.shift;
+	 */
 	struct timespec64	monotonic_to_boot;
 
 	/* The following members are for timekeeping internal use */
+	/*
+	 * 在以下使用timekeeper->cycle_interval:
+	 *   - kernel/time/timekeeping.c|555| <<tk_setup_internals>> tk->cycle_interval = interval;
+	 *   - kernel/time/timekeeping.c|2381| <<timekeeping_apply_adjustment>> s64 interval = tk->cycle_interval;
+	 *   - kernel/time/timekeeping.c|2468| <<timekeeping_adjust>> tk->xtime_remainder, tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2577| <<logarithmic_accumulation>> u64 interval = tk->cycle_interval << shift;
+	 *   - kernel/time/timekeeping.c|2665| <<timekeeping_advance>> if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
+	 *   - kernel/time/timekeeping.c|2682| <<timekeeping_advance>> shift = ilog2(offset) - ilog2(tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2687| <<timekeeping_advance>> while (offset >= tk->cycle_interval) {
+	 *   - kernel/time/timekeeping.c|2694| <<timekeeping_advance>> if (offset < tk->cycle_interval<<shift)
+	 */
 	u64			cycle_interval;
+	/*
+	 * 在以下使用timekeeper->xtime_interval:
+	 *   - kernel/time/timekeeping.c|558| <<tk_setup_internals>> tk->xtime_interval = interval * clock->mult;
+	 *   - kernel/time/timekeeping.c|559| <<tk_setup_internals>> tk->xtime_remainder = ntpinterval - tk->xtime_interval;
+	 *   - kernel/time/timekeeping.c|2459| <<timekeeping_apply_adjustment>> tk->xtime_interval += interval;
+	 *   - kernel/time/timekeeping.c|2667| <<logarithmic_accumulation>> tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
+	 *   - kernel/time/timekeeping.c|2694| <<logarithmic_accumulation>> tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<
+	 */
 	u64			xtime_interval;
 	s64			xtime_remainder;
+	/*
+	 * 在以下使用timekeeper->raw_interval:
+	 *   - kernel/time/timekeeping.c|499| <<tk_setup_internals>> tk->raw_interval = interval * clock->mult;
+	 *   - kernel/time/timekeeping.c|2432| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 */
 	u64			raw_interval;
 	/* The ntp_tick_length() value currently being used.
 	 * This cached copy ensures we consistently apply the tick
@@ -116,11 +256,35 @@ struct timekeeper {
 	 * mid-tick, and we don't want to apply that new value to
 	 * the tick in progress.
 	 */
+	/*
+	 * 在以下使用timekeeper->ntp_tick:
+	 *   - kernel/time/timekeeping.c|698| <<tk_setup_internals>> tk->ntp_tick = ntpinterval << tk->ntp_error_shift;
+	 *   - kernel/time/timekeeping.c|2675| <<timekeeping_adjust>> if (likely(tk->ntp_tick == ntp_tick_length())) {
+	 *   - kernel/time/timekeeping.c|2678| <<timekeeping_adjust>> tk->ntp_tick = ntp_tick_length();
+	 *   - kernel/time/timekeeping.c|2679| <<timekeeping_adjust>> mult = div64_u64((tk->ntp_tick >> tk->ntp_error_shift) -
+	 *   - kernel/time/timekeeping.c|2921| <<logarithmic_accumulation>> tk->ntp_error += tk->ntp_tick << shift;
+	 */
 	u64			ntp_tick;
 	/* Difference between accumulated time and NTP time in ntp
 	 * shifted nano seconds. */
+	/*
+	 * 在以下使用timekeeper->ntp_error:
+	 *   - kernel/time/timekeeping.c|792| <<tk_setup_internals>> tk->ntp_error = 0;
+	 *   - kernel/time/timekeeping.c|1394| <<timekeeping_update>> tk->ntp_error = 0;
+	 *   - kernel/time/timekeeping.c|2699| <<timekeeping_resume>> tk->ntp_error = 0;
+	 *   - kernel/time/timekeeping.c|2920| <<timekeeping_adjust>> tk->ntp_err_mult = tk->ntp_error > 0 ? 1 : 0;
+	 *   - kernel/time/timekeeping.c|3187| <<logarithmic_accumulation>> tk->ntp_error += tk->ntp_tick << shift;
+	 *   - kernel/time/timekeeping.c|3188| <<logarithmic_accumulation>> tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<
+	 */
 	s64			ntp_error;
 	u32			ntp_error_shift;
+	/*
+	 * 在以下使用timekeeper->ntp_err_mult:
+	 *   - kernel/time/timekeeping.c|803| <<tk_setup_internals>> tk->ntp_err_mult = 0;
+	 *   - kernel/time/timekeeping.c|2907| <<timekeeping_adjust>> mult = tk->tkr_mono.mult - tk->ntp_err_mult;
+	 *   - kernel/time/timekeeping.c|2920| <<timekeeping_adjust>> tk->ntp_err_mult = tk->ntp_error > 0 ? 1 : 0;
+	 *   - kernel/time/timekeeping.c|2921| <<timekeeping_adjust>> mult += tk->ntp_err_mult;
+	 */
 	u32			ntp_err_mult;
 	/* Flag used to avoid updating NTP twice with same second */
 	u32			skip_second_overflow;
diff --git a/include/linux/timekeeping.h b/include/linux/timekeeping.h
index fe1e467ba..56e3e307e 100644
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@ -169,6 +169,15 @@ static inline u64 ktime_get_clocktai_ns(void)
 	return ktime_to_ns(ktime_get_clocktai());
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/amd/amdkfd/kfd_chardev.c|677| <<kfd_ioctl_get_clock_counters>> args->cpu_clock_counter = ktime_get_raw_ns();
+ *   - drivers/gpu/drm/amd/display/dc/dm_services.h|261| <<dm_get_timestamp>> return ktime_get_raw_ns();
+ *   - drivers/gpu/drm/i915/gt/intel_rps.c|389| <<__gen5_ips_update>> now = ktime_get_raw_ns();
+ *   - drivers/gpu/drm/i915/gt/intel_rps.c|600| <<gen5_rps_enable>> rps->ips.last_time2 = ktime_get_raw_ns();
+ *   - drivers/iio/industrialio-core.c|321| <<iio_get_time_ns>> return ktime_get_raw_ns();
+ *   - drivers/misc/habanalabs/common/habanalabs_ioctl.c|313| <<time_sync_info>> time_sync.host_time = ktime_get_raw_ns();
+ */
 static inline u64 ktime_get_raw_ns(void)
 {
 	return ktime_to_ns(ktime_get_raw());
diff --git a/include/linux/timex.h b/include/linux/timex.h
index 3871b06bd..184d47f24 100644
--- a/include/linux/timex.h
+++ b/include/linux/timex.h
@@ -156,7 +156,21 @@ extern unsigned long tick_nsec;		/* SHIFTED_HZ period (nsec) */
 
 #define NTP_SCALE_SHIFT		32
 
+/*
+ * 在以下使用NTP_INTERVAL_FREQ:
+ *   - include/linux/timex.h|160| <<NTP_INTERVAL_LENGTH>> #define NTP_INTERVAL_LENGTH (NSEC_PER_SEC/NTP_INTERVAL_FREQ)
+ *   - kernel/time/ntp.c|45| <<MAX_TICKADJ_SCALED>> (((MAX_TICKADJ * NSEC_PER_USEC) << NTP_SCALE_SHIFT) / NTP_INTERVAL_FREQ)
+ *   - kernel/time/ntp.c|271| <<ntp_update_frequency>> new_base = div_u64(second_length, NTP_INTERVAL_FREQ);
+ *   - kernel/time/ntp.c|345| <<ntp_update_offset>> time_offset = div_s64(offset64 << NTP_SCALE_SHIFT, NTP_INTERVAL_FREQ);
+ *   - kernel/time/ntp.c|489| <<second_overflow>> tick_length += (s64)(time_adjust * NSEC_PER_USEC / NTP_INTERVAL_FREQ)
+ *   - kernel/time/ntp.c|797| <<__do_adjtimex>> txc->offset = shift_right(time_offset * NTP_INTERVAL_FREQ,
+ *   - kernel/time/ntp.c|1010| <<hardpps_update_phase>> NTP_INTERVAL_FREQ);
+ */
 #define NTP_INTERVAL_FREQ  (HZ)
+/*
+ * 只在以下使用NTP_INTERVAL_LENGTH:
+ *   - kernel/time/timekeeping.c|578| <<tk_setup_internals>> tmp = NTP_INTERVAL_LENGTH;
+ */
 #define NTP_INTERVAL_LENGTH (NSEC_PER_SEC/NTP_INTERVAL_FREQ)
 
 extern int do_adjtimex(struct __kernel_timex *);
diff --git a/include/linux/vfio_pci_core.h b/include/linux/vfio_pci_core.h
index 5579ece43..1e881e742 100644
--- a/include/linux/vfio_pci_core.h
+++ b/include/linux/vfio_pci_core.h
@@ -106,6 +106,19 @@ struct vfio_pci_core_device {
 	spinlock_t		irqlock;
 	struct mutex		igate;
 	struct vfio_pci_irq_ctx	*ctx;
+	/*
+	 * 在以下设置vfio_pci_core_device->num_ctx:
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|154| <<vfio_intx_enable>> vdev->num_ctx = 1;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|232| <<vfio_intx_disable>> vdev->num_ctx = 0;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|277| <<vfio_msi_enable>> vdev->num_ctx = nvec;
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|446| <<vfio_msi_disable>> vdev->num_ctx = 0;
+	 * 在以下使用vfio_pci_core_device->num_ctx:
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|305| <<vfio_msi_set_vector_signal>> if (vector < 0 || vector >= vdev->num_ctx)
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|405| <<vfio_msi_set_block>> if (start >= vdev->num_ctx || start + count > vdev->num_ctx)
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|427| <<vfio_msi_disable>> for (i = 0; i < vdev->num_ctx; i++) {
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|432| <<vfio_msi_disable>> vfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|592| <<vfio_pci_set_msi_trigger>> if (!irq_is(vdev, index) || start + count > vdev->num_ctx)
+	 */
 	int			num_ctx;
 	int			irq_type;
 	int			num_regions;
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index eed0315a7..f6e0f1805 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -2109,6 +2109,11 @@ struct kvm_dirty_gfn {
 #define KVM_BUS_LOCK_DETECTION_OFF             (1 << 0)
 #define KVM_BUS_LOCK_DETECTION_EXIT            (1 << 1)
 
+/*
+ * 在以下使用KVM_PMU_CAP_DISABLE:
+ *   - arch/x86/kvm/x86.c|115| <<KVM_CAP_PMU_VALID_MASK>> #define KVM_CAP_PMU_VALID_MASK KVM_PMU_CAP_DISABLE
+ *   - arch/x86/kvm/x86.c|7130| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+ */
 #define KVM_PMU_CAP_DISABLE                    (1 << 0)
 
 /**
diff --git a/include/uapi/linux/perf_event.h b/include/uapi/linux/perf_event.h
index 03b370062..e1a5ba8a3 100644
--- a/include/uapi/linux/perf_event.h
+++ b/include/uapi/linux/perf_event.h
@@ -673,6 +673,13 @@ struct perf_event_mmap_page {
 	 * data_{offset,size} indicate the location and size of the perf record
 	 * buffer within the mmapped area.
 	 */
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 	__u64   data_head;		/* head in the data section */
 	__u64	data_tail;		/* user-space written tail */
 	__u64	data_offset;		/* where the buffer starts */
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index f8c20d3de..7bd9cd285 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -64,6 +64,13 @@
 #define VRING_PACKED_EVENT_FLAG_ENABLE	0x0
 /* Disable events in packed ring. */
 #define VRING_PACKED_EVENT_FLAG_DISABLE	0x1
+/*
+ * 在以下使用VRING_PACKED_EVENT_FLAG_DESC:
+ *   - drivers/virtio/virtio_ring.c|1822| <<virtqueue_kick_prepare_packed>> if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+ *   - drivers/virtio/virtio_ring.c|1997| <<virtqueue_get_buf_ctx_packed>> if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+ *   - drivers/virtio/virtio_ring.c|2042| <<virtqueue_enable_cb_prepare_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+ *   - drivers/virtio/virtio_ring.c|2101| <<virtqueue_enable_cb_delayed_packed>> vq->packed.event_flags_shadow = vq->event ? VRING_PACKED_EVENT_FLAG_DESC : VRING_PACKED_EVENT_FLAG_ENABLE;
+ */
 /*
  * Enable events for a specific descriptor in packed ring.
  * (as specified by Descriptor Ring Change Event Offset/Wrap Counter).
@@ -75,6 +82,18 @@
  * Wrap counter bit shift in event suppression structure
  * of packed ring.
  */
+/*
+ * 在以下使用VRING_PACKED_EVENT_F_WRAP_CTR:
+ *   - drivers/virtio/virtio_ring.c|393| <<virtqueue_init>> vq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1157| <<packed_used_wrap_counter>> return !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1162| <<packed_last_used>> return last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1525| <<virtqueue_kick_prepare_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1526| <<virtqueue_kick_prepare_packed>> event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1657| <<virtqueue_get_buf_ctx_packed>> last_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ *   - drivers/virtio/virtio_ring.c|1726| <<virtqueue_poll_packed>> wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ *   - drivers/virtio/virtio_ring.c|1727| <<virtqueue_poll_packed>> used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ *   - drivers/virtio/virtio_ring.c|1758| <<virtqueue_enable_cb_delayed_packed>> (wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ */
 #define VRING_PACKED_EVENT_F_WRAP_CTR	15
 
 /* We support indirect buffer descriptors */
@@ -219,6 +238,15 @@ static inline unsigned vring_size(unsigned int num, unsigned long align)
 /* Assuming a given event_idx value from the other side, if
  * we have just incremented index from old to new_idx,
  * should we trigger an event? */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2475| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+ *   - drivers/vhost/vringh.c|525| <<__vringh_need_notify>> notify = vring_need_event(used_event,
+ *   - drivers/virtio/virtio_ring.c|884| <<virtqueue_kick_prepare_split>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,
+ *   - drivers/virtio/virtio_ring.c|1798| <<virtqueue_kick_prepare_packed>> needs_kick = vring_need_event(event_idx, new, old);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ */
 static inline int vring_need_event(__u16 event_idx, __u16 new_idx, __u16 old)
 {
 	/* Note: Xen has similar logic for notification hold-off
diff --git a/kernel/cpu.c b/kernel/cpu.c
index bbad5e375..d27adb40a 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -164,6 +164,15 @@ static bool cpuhp_step_empty(bool bringup, struct cpuhp_step *step)
  *
  * Return: %0 on success or a negative errno code
  */
+/*
+ * called by:
+ *   - kernel/cpu.c|716| <<cpuhp_invoke_callback_range>> err = cpuhp_invoke_callback(cpu, state, bringup, NULL, NULL);
+ *   - kernel/cpu.c|822| <<cpuhp_thread_fun>> st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);
+ *   - kernel/cpu.c|830| <<cpuhp_thread_fun>> st->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);
+ *   - kernel/cpu.c|873| <<cpuhp_invoke_ap_callback>> return cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ *   - kernel/cpu.c|1995| <<cpuhp_issue_call>> ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ *   - kernel/cpu.c|1997| <<cpuhp_issue_call>> ret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);
+ */
 static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,
 				 bool bringup, struct hlist_node *node,
 				 struct hlist_node **lastp)
@@ -413,8 +422,39 @@ static void lockdep_release_cpus_lock(void)
 void __weak arch_smt_update(void) { }
 
 #ifdef CONFIG_HOTPLUG_SMT
+/*
+ * 在以下使用cpu_smt_control:
+ *   - kernel/cpu.c|416| <<global>> enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
+ *   - arch/x86/kernel/cpu/bugs.c|1128| <<spectre_v2_user_select_mitigation>> if (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||
+ *   - arch/x86/kernel/cpu/bugs.c|1129| <<spectre_v2_user_select_mitigation>> cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
+ *   - arch/x86/power/hibernate.c|205| <<arch_resume_nosmt>> if (cpu_smt_control == CPU_SMT_DISABLED ||
+ *   - arch/x86/power/hibernate.c|206| <<arch_resume_nosmt>> cpu_smt_control == CPU_SMT_FORCE_DISABLED) {
+ *   - arch/x86/power/hibernate.c|207| <<arch_resume_nosmt>> enum cpuhp_smt_control old = cpu_smt_control;
+ *   - kernel/cpu.c|425| <<cpu_smt_disable>> cpu_smt_control = CPU_SMT_FORCE_DISABLED;
+ *   - kernel/cpu.c|428| <<cpu_smt_disable>> cpu_smt_control = CPU_SMT_DISABLED;
+ *   - kernel/cpu.c|439| <<cpu_smt_check_topology>> cpu_smt_control = CPU_SMT_NOT_SUPPORTED;
+ *   - kernel/cpu.c|451| <<cpu_smt_allowed>> if (cpu_smt_control == CPU_SMT_ENABLED)
+ *   - kernel/cpu.c|469| <<cpu_smt_possible>> return cpu_smt_control != CPU_SMT_FORCE_DISABLED &&
+ *   - kernel/cpu.c|470| <<cpu_smt_possible>> cpu_smt_control != CPU_SMT_NOT_SUPPORTED;
+ *   - kernel/cpu.c|2272| <<cpuhp_smt_disable>> cpu_smt_control = ctrlval;
+ *   - kernel/cpu.c|2282| <<cpuhp_smt_enable>> cpu_smt_control = CPU_SMT_ENABLED;
+ *   - kernel/cpu.c|2478| <<__store_smt_control>> if (cpu_smt_control == CPU_SMT_FORCE_DISABLED)
+ *   - kernel/cpu.c|2481| <<__store_smt_control>> if (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)
+ *   - kernel/cpu.c|2488| <<__store_smt_control>> if (ctrlval != cpu_smt_control) {
+ *   - kernel/cpu.c|2524| <<control_show>> const char *state = smt_states[cpu_smt_control];
+ */
 enum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|297| <<mds_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|400| <<taa_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|493| <<mmio_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|937| <<retbleed_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|2150| <<l1tf_select_mitigation>> cpu_smt_disable(false);
+ *   - arch/x86/kernel/cpu/bugs.c|2153| <<l1tf_select_mitigation>> cpu_smt_disable(true);
+ *   - kernel/cpu.c|444| <<smt_cmdline_disable>> cpu_smt_disable(str && !strcmp(str, "force"));
+ */
 void __init cpu_smt_disable(bool force)
 {
 	if (!cpu_smt_possible())
@@ -446,6 +486,11 @@ static int __init smt_cmdline_disable(char *str)
 }
 early_param("nosmt", smt_cmdline_disable);
 
+/*
+ * called by:
+ *   - kernel/cpu.c|580| <<bringup_wait_for_ap>> if (!cpu_smt_allowed(cpu))
+ *   - kernel/cpu.c|1429| <<cpu_up>> if (!cpu_smt_allowed(cpu)) {
+ */
 static inline bool cpu_smt_allowed(unsigned int cpu)
 {
 	if (cpu_smt_control == CPU_SMT_ENABLED)
@@ -464,6 +509,11 @@ static inline bool cpu_smt_allowed(unsigned int cpu)
 }
 
 /* Returns true if SMT is not supported of forcefully (irreversibly) disabled */
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|2533| <<kvm_get_hv_cpuid>> if (!cpu_smt_possible())
+ *   - kernel/cpu.c|420| <<cpu_smt_disable>> if (!cpu_smt_possible())
+ */
 bool cpu_smt_possible(void)
 {
 	return cpu_smt_control != CPU_SMT_FORCE_DISABLED &&
@@ -526,6 +576,14 @@ cpuhp_reset_state(int cpu, struct cpuhp_cpu_state *st,
 }
 
 /* Regular hotplug invocation of the AP hotplug thread */
+/*
+ * called by:
+ *   - kernel/cpu.c|602| <<cpuhp_kick_ap>> __cpuhp_kick_ap(st);
+ *   - kernel/cpu.c|605| <<cpuhp_kick_ap>> __cpuhp_kick_ap(st);
+ *   - kernel/cpu.c|892| <<cpuhp_invoke_ap_callback>> __cpuhp_kick_ap(st);
+ *   - kernel/cpu.c|901| <<cpuhp_invoke_ap_callback>> __cpuhp_kick_ap(st);
+ *   - kernel/cpu.c|1239| <<_cpu_down>> __cpuhp_kick_ap(st);
+ */
 static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)
 {
 	if (!st->single && st->state == st->target)
@@ -542,6 +600,11 @@ static void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)
 	wait_for_ap_thread(st, st->bringup);
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|636| <<bringup_wait_for_ap>> return cpuhp_kick_ap(cpu, st, st->target);
+ *   - kernel/cpu.c|925| <<cpuhp_kick_ap_work>> ret = cpuhp_kick_ap(cpu, st, st->target);
+ */
 static int cpuhp_kick_ap(int cpu, struct cpuhp_cpu_state *st,
 			 enum cpuhp_state target)
 {
@@ -694,6 +757,10 @@ static inline bool can_rollback_cpu(struct cpuhp_cpu_state *st)
 	return st->state <= CPUHP_BRINGUP_CPU;
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|1439| <<_cpu_up>> ret = cpuhp_up_callbacks(cpu, st, target);
+ */
 static int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 			      enum cpuhp_state target)
 {
@@ -804,6 +871,10 @@ static void cpuhp_thread_fun(unsigned int cpu)
 }
 
 /* Invoke a single callback on a remote cpu */
+/*
+ * called by:
+ *   - kernel/cpu.c|2032| <<cpuhp_issue_call>> ret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);
+ */
 static int
 cpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state, bool bringup,
 			 struct hlist_node *node)
@@ -1123,6 +1194,17 @@ static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
 }
 
 /* Requires cpu_add_remove_lock to be held */
+/*
+ * [0] _cpu_down
+ * [0] cpu_device_down
+ * [0] device_offline
+ * [0] online_store
+ * [0] kernfs_fop_write_iter
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
 			   enum cpuhp_state target)
 {
@@ -1384,6 +1466,9 @@ static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)
 	 * responsible for bringing it up to the target state.
 	 */
 	target = min((int)target, CPUHP_BRINGUP_CPU);
+	/*
+	 * 执行到这里!!!
+	 */
 	ret = cpuhp_up_callbacks(cpu, st, target);
 out:
 	cpus_write_unlock();
@@ -1392,6 +1477,20 @@ static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)
 	return ret;
 }
 
+/*
+ * [0] cpuhp_kick_ap
+ * [0] cpuhp_invoke_callback
+ * [0] _cpu_up
+ * [0] cpu_up
+ * [0] cpu_subsys_online
+ * [0] device_online
+ * [0] online_store
+ * [0] kernfs_fop_write_iter
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int cpu_up(unsigned int cpu, enum cpuhp_state target)
 {
 	int err = 0;
diff --git a/kernel/events/core.c b/kernel/events/core.c
index ff4bffc50..2124fe63e 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -257,6 +257,13 @@ static int event_function(void *info)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2399| <<perf_remove_from_context>> event_function_call(event, __perf_remove_from_context, (void *)flags);
+ *   - kernel/events/core.c|2452| <<_perf_event_disable>> event_function_call(event, __perf_event_disable, NULL);
+ *   - kernel/events/core.c|3002| <<_perf_event_enable>> event_function_call(event, __perf_event_enable, NULL);
+ *   - kernel/events/core.c|5618| <<_perf_event_period>> event_function_call(event, __perf_event_period, &value);
+ */
 static void event_function_call(struct perf_event *event, event_f func, void *data)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -381,12 +388,28 @@ enum event_type_t {
  */
 
 static void perf_sched_delayed(struct work_struct *work);
+/*
+ * 在以下使用perf_sched_events:
+ *   - kernel/events/core.c|391| <<global>> DEFINE_STATIC_KEY_FALSE(perf_sched_events);
+ *   - include/linux/perf_event.h|1271| <<perf_event_task_sched_in>> if (static_branch_unlikely(&perf_sched_events))
+ *   - include/linux/perf_event.h|1294| <<perf_event_task_sched_out>> if (static_branch_unlikely(&perf_sched_events))
+ *   - kernel/events/core.c|4990| <<perf_sched_delayed>> static_branch_disable(&perf_sched_events);
+ *   - kernel/events/core.c|11838| <<account_event>> static_branch_enable(&perf_sched_events);
+ */
 DEFINE_STATIC_KEY_FALSE(perf_sched_events);
 static DECLARE_DELAYED_WORK(perf_sched_work, perf_sched_delayed);
 static DEFINE_MUTEX(perf_sched_mutex);
 static atomic_t perf_sched_count;
 
 static DEFINE_PER_CPU(atomic_t, perf_cgroup_events);
+/*
+ * 在以下使用perf_sched_cb_usages:
+ *   - kernel/events/core.c|397| <<global>> static DEFINE_PER_CPU(int , perf_sched_cb_usages);
+ *   - kernel/events/core.c|3537| <<perf_sched_cb_dec>> this_cpu_dec(perf_sched_cb_usages);
+ *   - kernel/events/core.c|3551| <<perf_sched_cb_inc>> this_cpu_inc(perf_sched_cb_usages);
+ *   - kernel/events/core.c|3625| <<__perf_event_task_sched_out>> if (__this_cpu_read(perf_sched_cb_usages))
+ *   - kernel/events/core.c|3973| <<__perf_event_task_sched_in>> if (__this_cpu_read(perf_sched_cb_usages))
+ */
 static DEFINE_PER_CPU(int, perf_sched_cb_usages);
 static DEFINE_PER_CPU(struct pmu_event_list, pmu_sb_events);
 
@@ -395,6 +418,14 @@ static atomic_t nr_comm_events __read_mostly;
 static atomic_t nr_namespaces_events __read_mostly;
 static atomic_t nr_task_events __read_mostly;
 static atomic_t nr_freq_events __read_mostly;
+/*
+ * 在以下使用nr_switch_events:
+ *   - kernel/events/core.c|405| <<global>> static atomic_t nr_switch_events __read_mostly;
+ *   - kernel/events/core.c|3628| <<__perf_event_task_sched_out>> if (atomic_read(&nr_switch_events))
+ *   - kernel/events/core.c|3970| <<__perf_event_task_sched_in>> if (atomic_read(&nr_switch_events))
+ *   - kernel/events/core.c|4904| <<unaccount_event>> atomic_dec(&nr_switch_events);
+ *   - kernel/events/core.c|11737| <<account_event>> atomic_inc(&nr_switch_events);
+ */
 static atomic_t nr_switch_events __read_mostly;
 static atomic_t nr_ksymbol_events __read_mostly;
 static atomic_t nr_bpf_events __read_mostly;
@@ -427,11 +458,33 @@ int sysctl_perf_event_mlock __read_mostly = 512 + (PAGE_SIZE / 1024); /* 'free'
 #define DEFAULT_SAMPLE_PERIOD_NS	(NSEC_PER_SEC / DEFAULT_MAX_SAMPLE_RATE)
 #define DEFAULT_CPU_TIME_MAX_PERCENT	25
 
+/*
+ * 在以下使用sysctl_perf_event_sample_rate:
+ *   - kernel/sysctl.c|2021| <<global>> .data = &sysctl_perf_event_sample_rate,
+ *   - kernel/sysctl.c|2022| <<global>> .maxlen = sizeof(sysctl_perf_event_sample_rate),
+ *   - arch/s390/kernel/perf_cpum_sf.c|710| <<getrate>> sysctl_perf_event_sample_rate) {
+ *   - kernel/events/core.c|498| <<perf_proc_update_handler>> max_samples_per_tick = DIV_ROUND_UP(sysctl_perf_event_sample_rate, HZ);
+ *   - kernel/events/core.c|499| <<perf_proc_update_handler>> perf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;
+ *   - kernel/events/core.c|545| <<perf_duration_warn>> sysctl_perf_event_sample_rate);
+ *   - kernel/events/core.c|591| <<perf_sample_event_took>> sysctl_perf_event_sample_rate = max * HZ;
+ *   - kernel/events/core.c|592| <<perf_sample_event_took>> perf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;
+ *   - kernel/events/core.c|598| <<perf_sample_event_took>> sysctl_perf_event_sample_rate)
+ *   - kernel/events/core.c|5724| <<_perf_event_period>> if (event->attr.freq && value > sysctl_perf_event_sample_rate)
+ *   - kernel/events/core.c|12530| <<SYSCALL_DEFINE5>> if (attr.sample_freq > sysctl_perf_event_sample_rate)
+ */
 int sysctl_perf_event_sample_rate __read_mostly	= DEFAULT_MAX_SAMPLE_RATE;
 
 static int max_samples_per_tick __read_mostly	= DIV_ROUND_UP(DEFAULT_MAX_SAMPLE_RATE, HZ);
 static int perf_sample_period_ns __read_mostly	= DEFAULT_SAMPLE_PERIOD_NS;
 
+/*
+ * 在以下使用perf_sample_allowed_ns:
+ *   - kernel/events/core.c|480| <<global>> static int perf_sample_allowed_ns __read_mostly = DEFAULT_SAMPLE_PERIOD_NS * DEFAULT_CPU_TIME_MAX_PERCENT / 100;
+ *   - kernel/events/core.c|492| <<update_perf_cpu_limits>> WRITE_ONCE(perf_sample_allowed_ns, tmp);
+ *   - kernel/events/core.c|533| <<perf_cpu_time_max_percent_handler>> WRITE_ONCE(perf_sample_allowed_ns, 0);
+ *   - kernel/events/core.c|580| <<perf_sample_event_took>> u64 max_len = READ_ONCE(perf_sample_allowed_ns);
+ *   - kernel/events/core.c|622| <<perf_sample_event_took>> WRITE_ONCE(perf_sample_allowed_ns, avg_len);
+ */
 static int perf_sample_allowed_ns __read_mostly =
 	DEFAULT_SAMPLE_PERIOD_NS * DEFAULT_CPU_TIME_MAX_PERCENT / 100;
 
@@ -473,6 +526,10 @@ int perf_proc_update_handler(struct ctl_table *table, int write,
 
 int sysctl_perf_cpu_time_max_percent __read_mostly = DEFAULT_CPU_TIME_MAX_PERCENT;
 
+/*
+ * 在以下使用perf_cpu_time_max_percent_handler():
+ *   - kernel/sysctl.c|2032| <<global>> .proc_handler = perf_cpu_time_max_percent_handler,
+ */
 int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
 		void *buffer, size_t *lenp, loff_t *ppos)
 {
@@ -500,6 +557,12 @@ int perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,
  * we detect that events are taking too long.
  */
 #define NR_ACCUMULATED_SAMPLES 128
+/*
+ * 在以下使用percpu的running_sample_length:
+ *   - kernel/events/core.c|534| <<global>> static DEFINE_PER_CPU(u64, running_sample_length);
+ *   - kernel/events/core.c|561| <<perf_sample_event_took>> running_len = __this_cpu_read(running_sample_length);
+ *   - kernel/events/core.c|564| <<perf_sample_event_took>> __this_cpu_write(running_sample_length, running_len);
+ */
 static DEFINE_PER_CPU(u64, running_sample_length);
 
 static u64 __report_avg;
@@ -516,8 +579,24 @@ static void perf_duration_warn(struct irq_work *w)
 
 static DEFINE_IRQ_WORK(perf_duration_work, perf_duration_warn);
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/core-book3s.c|2472| <<perf_event_interrupt>> perf_sample_event_took(sched_clock() - start_clock);
+ *   - arch/sparc/kernel/perf_event.c|1677| <<perf_event_nmi_handler>> perf_sample_event_took(finish_clock - start_clock);
+ *   - arch/x86/events/amd/ibs.c|844| <<perf_ibs_nmi_handler>> perf_sample_event_took(sched_clock() - stamp);
+ *   - arch/x86/events/core.c|1793| <<perf_event_nmi_handler>> perf_sample_event_took(finish_clock - start_clock);
+ *   - drivers/perf/arm_pmu.c|455| <<armpmu_dispatch_irq>> perf_sample_event_took(finish_clock - start_clock);
+ */
 void perf_sample_event_took(u64 sample_len_ns)
 {
+	/*
+	 * 在以下使用perf_sample_allowed_ns:
+	 *   - kernel/events/core.c|480| <<global>> static int perf_sample_allowed_ns __read_mostly = DEFAULT_SAMPLE_PERIOD_NS * DEFAULT_CPU_TIME_MAX_PERCENT / 100;
+	 *   - kernel/events/core.c|492| <<update_perf_cpu_limits>> WRITE_ONCE(perf_sample_allowed_ns, tmp);
+	 *   - kernel/events/core.c|533| <<perf_cpu_time_max_percent_handler>> WRITE_ONCE(perf_sample_allowed_ns, 0);
+	 *   - kernel/events/core.c|580| <<perf_sample_event_took>> u64 max_len = READ_ONCE(perf_sample_allowed_ns);
+	 *   - kernel/events/core.c|622| <<perf_sample_event_took>> WRITE_ONCE(perf_sample_allowed_ns, avg_len);
+	 */
 	u64 max_len = READ_ONCE(perf_sample_allowed_ns);
 	u64 running_len;
 	u64 avg_len;
@@ -526,6 +605,12 @@ void perf_sample_event_took(u64 sample_len_ns)
 	if (max_len == 0)
 		return;
 
+	/*
+	 * 在以下使用percpu的running_sample_length:
+	 *   - kernel/events/core.c|534| <<global>> static DEFINE_PER_CPU(u64, running_sample_length);
+	 *   - kernel/events/core.c|561| <<perf_sample_event_took>> running_len = __this_cpu_read(running_sample_length);
+	 *   - kernel/events/core.c|564| <<perf_sample_event_took>> __this_cpu_write(running_sample_length, running_len);
+	 */
 	/* Decay the counter by 1 average sample. */
 	running_len = __this_cpu_read(running_sample_length);
 	running_len -= running_len/NR_ACCUMULATED_SAMPLES;
@@ -549,6 +634,10 @@ void perf_sample_event_took(u64 sample_len_ns)
 	 */
 	avg_len += avg_len / 4;
 	max = (TICK_NSEC / 100) * sysctl_perf_cpu_time_max_percent;
+	/*
+	 * 越慢, max这个数越小
+	 * 对于除法, avg_len越大, max越小
+	 */
 	if (avg_len < max)
 		max /= (u32)avg_len;
 	else
@@ -557,6 +646,22 @@ void perf_sample_event_took(u64 sample_len_ns)
 	WRITE_ONCE(perf_sample_allowed_ns, avg_len);
 	WRITE_ONCE(max_samples_per_tick, max);
 
+	/*
+	 * 在以下使用sysctl_perf_event_sample_rate:
+	 *   - kernel/sysctl.c|2021| <<global>> .data = &sysctl_perf_event_sample_rate,
+	 *   - kernel/sysctl.c|2022| <<global>> .maxlen = sizeof(sysctl_perf_event_sample_rate),
+	 *   - arch/s390/kernel/perf_cpum_sf.c|710| <<getrate>> sysctl_perf_event_sample_rate) {
+	 *   - kernel/events/core.c|498| <<perf_proc_update_handler>> max_samples_per_tick = DIV_ROUND_UP(sysctl_perf_event_sample_rate, HZ);
+	 *   - kernel/events/core.c|499| <<perf_proc_update_handler>> perf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;
+	 *   - kernel/events/core.c|545| <<perf_duration_warn>> sysctl_perf_event_sample_rate);
+	 *   - kernel/events/core.c|591| <<perf_sample_event_took>> sysctl_perf_event_sample_rate = max * HZ;
+	 *   - kernel/events/core.c|592| <<perf_sample_event_took>> perf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;
+	 *   - kernel/events/core.c|598| <<perf_sample_event_took>> sysctl_perf_event_sample_rate)
+	 *   - kernel/events/core.c|5724| <<_perf_event_period>> if (event->attr.freq && value > sysctl_perf_event_sample_rate)
+	 *   - kernel/events/core.c|12530| <<SYSCALL_DEFINE5>> if (attr.sample_freq > sysctl_perf_event_sample_rate)
+	 *
+	 * 越慢, 这个数越小
+	 */
 	sysctl_perf_event_sample_rate = max * HZ;
 	perf_sample_period_ns = NSEC_PER_SEC / sysctl_perf_event_sample_rate;
 
@@ -1066,6 +1171,10 @@ static void perf_cgroup_switch(struct task_struct *task)
 /*
  * function must be called with interrupts disabled
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|1216| <<__perf_mux_hrtimer_init>> timer->function = perf_mux_hrtimer_handler;
+ */
 static enum hrtimer_restart perf_mux_hrtimer_handler(struct hrtimer *hr)
 {
 	struct perf_cpu_context *cpuctx;
@@ -1762,6 +1871,10 @@ perf_event_groups_next(struct perf_event *event)
  * Add an event from the lists for its context.
  * Must be called with ctx->mutex and ctx->lock held.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2643| <<add_event_to_ctx>> list_add_event(event, ctx);
+ */
 static void
 list_add_event(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -2186,6 +2299,10 @@ static void perf_group_detach(struct perf_event *event)
 
 static void sync_child_event(struct perf_event *child_event);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2339| <<__perf_remove_from_context>> perf_child_detach(event);
+ */
 static void perf_child_detach(struct perf_event *event)
 {
 	struct perf_event *parent_event = event->parent;
@@ -2319,6 +2436,12 @@ group_sched_out(struct perf_event *group_event,
  * We disable the event on the hardware level first. After that we
  * remove it from the context list.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2382| <<perf_remove_from_context>> __perf_remove_from_context(event, __get_cpu_context(ctx),
+ *   - kernel/events/core.c|2389| <<perf_remove_from_context>> event_function_call(event, __perf_remove_from_context, (void *)flags);
+ *   - kernel/events/core.c|13392| <<__perf_event_exit_context>> __perf_remove_from_context(event, cpuctx, ctx, (void *)DETACH_GROUP);
+ */
 static void
 __perf_remove_from_context(struct perf_event *event,
 			   struct perf_cpu_context *cpuctx,
@@ -2451,6 +2574,15 @@ void perf_event_disable_local(struct perf_event *event)
  * Strictly speaking kernel users cannot create groups and therefore this
  * interface does not need the perf_event_ctx_lock() magic.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|430| <<kvm_pmu_release_perf_event>> perf_event_disable(pmc->perf_event);
+ *   - arch/arm64/kvm/pmu-emul.c|657| <<kvm_pmu_disable_counter_mask>> perf_event_disable(pmc->perf_event);
+ *   - arch/arm64/kvm/pmu-emul.c|1365| <<kvm_pmu_probe_armpmu>> perf_event_disable(event);
+ *   - kernel/events/hw_breakpoint.c|527| <<modify_user_hw_breakpoint>> perf_event_disable(bp);
+ *   - kernel/watchdog_hld.c|210| <<hardlockup_detector_perf_disable>> perf_event_disable(event);
+ *   - kernel/watchdog_hld.c|256| <<hardlockup_detector_perf_stop>> perf_event_disable(event);
+ */
 void perf_event_disable(struct perf_event *event)
 {
 	struct perf_event_context *ctx;
@@ -2612,6 +2744,14 @@ static int group_can_go_on(struct perf_event *event,
 	return can_add_hw;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2768| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2771| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2825| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2895| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|13060| <<inherit_event>> add_event_to_ctx(child_event, child_ctx);
+ */
 static void add_event_to_ctx(struct perf_event *event,
 			       struct perf_event_context *ctx)
 {
@@ -2950,6 +3090,14 @@ static void __perf_event_enable(struct perf_event *event,
  * perf_event_for_each_child or perf_event_for_each as described
  * for perf_event_disable.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|3013| <<perf_event_enable>> _perf_event_enable(event);
+ *   - kernel/events/core.c|3133| <<_perf_event_refresh>> _perf_event_enable(event);
+ *   - kernel/events/core.c|3164| <<perf_event_modify_breakpoint>> _perf_event_enable(bp);
+ *   - kernel/events/core.c|5670| <<_perf_ioctl(PERF_EVENT_IOC_ENABLE)>> func = _perf_event_enable;
+ *   - kernel/events/core.c|5826| <<perf_event_task_enable>> perf_event_for_each_child(event, _perf_event_enable);
+ */
 static void _perf_event_enable(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -2987,6 +3135,14 @@ static void _perf_event_enable(struct perf_event *event)
 /*
  * See perf_event_disable();
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|617| <<kvm_pmu_enable_counter_mask>> perf_event_enable(pmc->perf_event);
+ *   - arch/x86/kvm/pmu.c|393| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+ *   - kernel/events/hw_breakpoint.c|532| <<modify_user_hw_breakpoint>> perf_event_enable(bp);
+ *   - kernel/watchdog_hld.c|199| <<hardlockup_detector_perf_enable>> perf_event_enable(this_cpu_read(watchdog_ev));
+ *   - kernel/watchdog_hld.c|278| <<hardlockup_detector_perf_restart>> perf_event_enable(event);
+ */
 void perf_event_enable(struct perf_event *event)
 {
 	struct perf_event_context *ctx;
@@ -3487,6 +3643,14 @@ void perf_sched_cb_dec(struct pmu *pmu)
 }
 
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/core-book3s.c|427| <<power_pmu_bhrb_enable>> perf_sched_cb_inc(event->ctx->pmu);
+ *   - arch/s390/kernel/perf_pai_crypto.c|277| <<paicrypt_add>> perf_sched_cb_inc(event->pmu);
+ *   - arch/x86/events/intel/ds.c|1180| <<pebs_update_state>> perf_sched_cb_inc(pmu);
+ *   - arch/x86/events/intel/lbr.c|674| <<intel_pmu_lbr_add>> perf_sched_cb_inc(event->ctx->pmu);
+ *   - arch/x86/events/perf_event.h|1278| <<amd_pmu_brs_add>> perf_sched_cb_inc(event->ctx->pmu);
+ */
 void perf_sched_cb_inc(struct pmu *pmu)
 {
 	struct perf_cpu_context *cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
@@ -3494,6 +3658,14 @@ void perf_sched_cb_inc(struct pmu *pmu)
 	if (!cpuctx->sched_cb_usage++)
 		list_add(&cpuctx->sched_cb_entry, this_cpu_ptr(&sched_cb_list));
 
+	/*
+	 * 在以下使用perf_sched_cb_usages:
+	 *   - kernel/events/core.c|397| <<global>> static DEFINE_PER_CPU(int , perf_sched_cb_usages);
+	 *   - kernel/events/core.c|3537| <<perf_sched_cb_dec>> this_cpu_dec(perf_sched_cb_usages);
+	 *   - kernel/events/core.c|3551| <<perf_sched_cb_inc>> this_cpu_inc(perf_sched_cb_usages);
+	 *   - kernel/events/core.c|3625| <<__perf_event_task_sched_out>> if (__this_cpu_read(perf_sched_cb_usages))
+	 *   - kernel/events/core.c|3973| <<__perf_event_task_sched_in>> if (__this_cpu_read(perf_sched_cb_usages))
+	 */
 	this_cpu_inc(perf_sched_cb_usages);
 }
 
@@ -3505,6 +3677,11 @@ void perf_sched_cb_inc(struct pmu *pmu)
  * PEBS requires this to provide PID/TID information. This requires we flush
  * all queued PEBS records before we context switch to a new task.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|3599| <<perf_pmu_sched_task>> __perf_pmu_sched_task(cpuctx, sched_in);
+ *   - kernel/events/core.c|3907| <<perf_event_context_sched_in>> __perf_pmu_sched_task(cpuctx, true);
+ */
 static void __perf_pmu_sched_task(struct perf_cpu_context *cpuctx, bool sched_in)
 {
 	struct pmu *pmu;
@@ -3523,6 +3700,11 @@ static void __perf_pmu_sched_task(struct perf_cpu_context *cpuctx, bool sched_in
 	perf_ctx_unlock(cpuctx, cpuctx->task_ctx);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3585| <<__perf_event_task_sched_out>> perf_pmu_sched_task(task, next, false);
+ *   - kernel/events/core.c|3925| <<__perf_event_task_sched_in>> perf_pmu_sched_task(prev, task, true);
+ */
 static void perf_pmu_sched_task(struct task_struct *prev,
 				struct task_struct *next,
 				bool sched_in)
@@ -3744,6 +3926,10 @@ static int merge_sched_in(struct perf_event *event, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3909| <<ctx_sched_in>> ctx_pinned_sched_in(ctx, cpuctx);
+ */
 static void
 ctx_pinned_sched_in(struct perf_event_context *ctx,
 		    struct perf_cpu_context *cpuctx)
@@ -3825,6 +4011,10 @@ static void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,
 	ctx_sched_in(ctx, cpuctx, event_type);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3963| <<__perf_event_task_sched_in>> perf_event_context_sched_in(ctx, task);
+ */
 static void perf_event_context_sched_in(struct perf_event_context *ctx,
 					struct task_struct *task)
 {
@@ -3886,6 +4076,10 @@ static void perf_event_context_sched_in(struct perf_event_context *ctx,
  * accessing the event control register. If a NMI hits, then it will
  * keep the event running.
  */
+/*
+ * called by:
+ *   - include/linux/perf_event.h|1243| <<perf_event_task_sched_in>> __perf_event_task_sched_in(prev, task);
+ */
 void __perf_event_task_sched_in(struct task_struct *prev,
 				struct task_struct *task)
 {
@@ -3900,9 +4094,31 @@ void __perf_event_task_sched_in(struct task_struct *prev,
 		perf_event_context_sched_in(ctx, task);
 	}
 
+	/*
+	 * 在以下使用nr_switch_events:
+	 *   - kernel/events/core.c|405| <<global>> static atomic_t nr_switch_events __read_mostly;
+	 *   - kernel/events/core.c|3628| <<__perf_event_task_sched_out>> if (atomic_read(&nr_switch_events))
+	 *   - kernel/events/core.c|3970| <<__perf_event_task_sched_in>> if (atomic_read(&nr_switch_events))
+	 *   - kernel/events/core.c|4904| <<unaccount_event>> atomic_dec(&nr_switch_events);
+	 *   - kernel/events/core.c|11737| <<account_event>> atomic_inc(&nr_switch_events);
+	 *
+	 * called by:
+	 *   - kernel/events/core.c|3588| <<__perf_event_task_sched_out>> perf_event_switch(task, next, false);
+	 *   - kernel/events/core.c|3922| <<__perf_event_task_sched_in>> perf_event_switch(task, prev, true);
+	 *
+	 * 这里似乎是向ring buffer记录events
+	 */
 	if (atomic_read(&nr_switch_events))
 		perf_event_switch(task, prev, true);
 
+	/*
+	 * 在以下使用perf_sched_cb_usages:
+	 *   - kernel/events/core.c|397| <<global>> static DEFINE_PER_CPU(int , perf_sched_cb_usages);
+	 *   - kernel/events/core.c|3537| <<perf_sched_cb_dec>> this_cpu_dec(perf_sched_cb_usages);
+	 *   - kernel/events/core.c|3551| <<perf_sched_cb_inc>> this_cpu_inc(perf_sched_cb_usages);
+	 *   - kernel/events/core.c|3625| <<__perf_event_task_sched_out>> if (__this_cpu_read(perf_sched_cb_usages))
+	 *   - kernel/events/core.c|3973| <<__perf_event_task_sched_in>> if (__this_cpu_read(perf_sched_cb_usages))
+	 */
 	if (__this_cpu_read(perf_sched_cb_usages))
 		perf_pmu_sched_task(prev, task, true);
 }
@@ -4125,6 +4341,10 @@ ctx_event_to_rotate(struct perf_event_context *ctx)
 	return event;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1182| <<perf_mux_hrtimer_handler>> rotations = perf_rotate_context(cpuctx);
+ */
 static bool perf_rotate_context(struct perf_cpu_context *cpuctx)
 {
 	struct perf_event *cpu_event = NULL, *task_event = NULL;
@@ -4381,8 +4601,25 @@ static void __perf_event_read(void *info)
 	raw_spin_unlock(&ctx->lock);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5207| <<__perf_event_read_value>> total += perf_event_count(event);
+ *   - kernel/events/core.c|5216| <<__perf_event_read_value>> total += perf_event_count(child);
+ *   - kernel/events/core.c|5271| <<__perf_read_group_add>> values[n++] += perf_event_count(leader);
+ *   - kernel/events/core.c|5278| <<__perf_read_group_add>> values[n++] += perf_event_count(sub);
+ *   - kernel/events/core.c|5861| <<perf_event_update_userpage>> userpg->offset = perf_event_count(event);
+ *   - kernel/events/core.c|6881| <<perf_output_read_one>> values[n++] = perf_event_count(event);
+ *   - kernel/events/core.c|6926| <<perf_output_read_group>> values[n++] = perf_event_count(leader);
+ *   - kernel/events/core.c|6941| <<perf_output_read_group>> values[n++] = perf_event_count(sub);
+ *   - kernel/events/core.c|12706| <<sync_child_event>> child_val = perf_event_count(child_event);
+ */
 static inline u64 perf_event_count(struct perf_event *event)
 {
+	/*
+	 * struct perf_event *event:
+	 * -> local64_t count;
+	 * -> atomic64_t child_count;
+	 */
 	return local64_read(&event->count) + atomic64_read(&event->child_count);
 }
 
@@ -4471,6 +4708,14 @@ int perf_event_read_local(struct perf_event *event, u64 *value,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5206| <<__perf_event_read_value>> (void )perf_event_read(event, false);
+ *   - kernel/events/core.c|5215| <<__perf_event_read_value>> (void )perf_event_read(child, false);
+ *   - kernel/events/core.c|5247| <<__perf_read_group_add>> ret = perf_event_read(leader, true);
+ *   - kernel/events/core.c|5443| <<_perf_event_reset>> (void )perf_event_read(event, false);
+ *   - security/security.c|2644| <<security_perf_event_read>> return call_int_hook(perf_event_read, 0, event);
+ */
 static int perf_event_read(struct perf_event *event, bool group)
 {
 	enum perf_event_state state = READ_ONCE(event->state);
@@ -5064,6 +5309,19 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|431| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/arm64/kvm/pmu-emul.c|1366| <<kvm_pmu_probe_armpmu>> perf_event_release_kernel(event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1037| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1039| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.h|126| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|322| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ *   - kernel/events/core.c|5421| <<perf_release>> perf_event_release_kernel(file->private_data);
+ *   - kernel/events/hw_breakpoint.c|546| <<unregister_hw_breakpoint>> perf_event_release_kernel(bp);
+ *   - kernel/watchdog_hld.c|235| <<hardlockup_detector_perf_cleanup>> perf_event_release_kernel(event);
+ *   - kernel/watchdog_hld.c|292| <<hardlockup_detector_perf_init>> perf_event_release_kernel(this_cpu_read(watchdog_ev));
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5185,6 +5443,11 @@ static int perf_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5231| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+ *   - kernel/events/core.c|5342| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+ */
 static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event *child;
@@ -5324,6 +5587,10 @@ static int perf_read_group(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5395| <<__perf_read>> ret = perf_read_one(event, read_format, buf);
+ */
 static int perf_read_one(struct perf_event *event,
 				 u64 read_format, char __user *buf)
 {
@@ -5490,6 +5757,10 @@ static void perf_event_for_each(struct perf_event *event,
 		perf_event_for_each_child(sibling, func);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5604| <<_perf_event_period>> event_function_call(event, __perf_event_period, &value);
+ */
 static void __perf_event_period(struct perf_event *event,
 				struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
@@ -5532,6 +5803,11 @@ static int perf_event_check_period(struct perf_event *event, u64 value)
 	return event->pmu->check_period(event, value);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5615| <<perf_event_period>> ret = _perf_event_period(event, value);
+ *   - kernel/events/core.c|5670| <<_perf_ioctl(PERF_EVENT_IOC_PERIOD)>> return _perf_event_period(event, value);
+ */
 static int _perf_event_period(struct perf_event *event, u64 value)
 {
 	if (!is_sampling_event(event))
@@ -5554,6 +5830,11 @@ static int _perf_event_period(struct perf_event *event, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|294| <<pmc_resume_counter>> if (perf_event_period(pmc->perf_event,
+ *   - arch/x86/kvm/pmu.h|208| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+ */
 int perf_event_period(struct perf_event *event, u64 value)
 {
 	struct perf_event_context *ctx;
@@ -6152,6 +6433,21 @@ static const struct vm_operations_struct perf_mmap_vmops = {
 	.page_mkwrite	= perf_mmap_fault,
 };
 
+/*
+ * struct perf_event *event = file->private_data;
+ * -> struct perf_buffer *rb;
+ *    -> atomic_t mmap_count;
+ *    -> unsigned long mmap_locked;
+ *    -> struct user_struct *mmap_user;
+ *    -> struct perf_event_mmap_page *user_page;
+ *       -> __u64   data_head;              // head in the data section
+ *       -> __u64   data_tail;              // user-space written tail
+ *       -> __u64   data_offset;            // where the buffer starts
+ *       -> __u64   data_size;              // data buffer size
+ *    -> void *data_pages[];
+ *
+ * 猜测在perf_event_overflow()更新数据
+ */
 static int perf_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	struct perf_event *event = file->private_data;
@@ -6260,6 +6556,19 @@ static int perf_mmap(struct file *file, struct vm_area_struct *vma)
 			goto unlock;
 		}
 
+		/*
+		 * struct perf_event *event = file->private_data;
+		 * -> struct perf_buffer *rb;
+		 *    -> atomic_t mmap_count;
+		 *    -> unsigned long mmap_locked;
+		 *    -> struct user_struct *mmap_user;
+		 *    -> struct perf_event_mmap_page *user_page;
+		 *       -> __u64   data_head;              // head in the data section
+		 *       -> __u64   data_tail;              // user-space written tail
+		 *       -> __u64   data_offset;            // where the buffer starts
+		 *       -> __u64   data_size;              // data buffer size
+		 *    -> void *data_pages[];
+		 */
 		if (!atomic_inc_not_zero(&event->rb->mmap_count)) {
 			/*
 			 * Raced against perf_mmap_close(); remove the
@@ -6981,6 +7290,13 @@ static inline bool perf_sample_save_hw_index(struct perf_event *event)
 	return event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_HW_INDEX;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1343| <<dump_trace_imc_data>> perf_output_sample(&handle, &header, &data, event);
+ *   - arch/s390/kernel/perf_cpum_sf.c|682| <<cpumsf_output_event_pid>> perf_output_sample(&handle, &header, data, event);
+ *   - arch/x86/events/intel/ds.c|777| <<intel_pmu_drain_bts_buffer>> perf_output_sample(&handle, &header, &data, event);
+ *   - kernel/events/core.c|7558| <<__perf_event_output>> perf_output_sample(&handle, &header, data, event);
+ */
 void perf_output_sample(struct perf_output_handle *handle,
 			struct perf_event_header *header,
 			struct perf_sample_data *data,
@@ -7306,6 +7622,12 @@ perf_callchain(struct perf_event *event, struct pt_regs *regs)
 	return callchain ?: &__empty_callchain;
 }
 
+/*
+ * called by:
+ *   - arch/s390/kernel/perf_cpum_sf.c|674| <<cpumsf_output_event_pid>> perf_prepare_sample(&header, data, event, regs);
+ *   - arch/x86/events/intel/ds.c|762| <<intel_pmu_drain_bts_buffer>> perf_prepare_sample(&header, &data, event, &regs);
+ *   - kernel/events/core.c|7565| <<__perf_event_output>> perf_prepare_sample(&header, data, event, regs);
+ */
 void perf_prepare_sample(struct perf_event_header *header,
 			 struct perf_sample_data *data,
 			 struct perf_event *event,
@@ -7481,6 +7803,12 @@ void perf_prepare_sample(struct perf_event_header *header,
 	WARN_ON_ONCE(header->size & 7);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7572| <<perf_event_output_forward>> __perf_event_output(event, data, regs, perf_output_begin_forward);
+ *   - kernel/events/core.c|7580| <<perf_event_output_backward>> __perf_event_output(event, data, regs, perf_output_begin_backward);
+ *   - kernel/events/core.c|7588| <<perf_event_output>> return __perf_event_output(event, data, regs, perf_output_begin);
+ */
 static __always_inline int
 __perf_event_output(struct perf_event *event,
 		    struct perf_sample_data *data,
@@ -7512,6 +7840,11 @@ __perf_event_output(struct perf_event *event,
 	return err;
 }
 
+/*
+ * called by:
+ *   - include/linux/perf_event.h|1122| <<is_default_overflow_handler>> if (likely(event->overflow_handler == perf_event_output_forward))
+ *   - kernel/events/core.c|11730| <<perf_event_alloc>> event->overflow_handler = perf_event_output_forward;
+ */
 void
 perf_event_output_forward(struct perf_event *event,
 			 struct perf_sample_data *data,
@@ -7547,6 +7880,10 @@ struct perf_read_event {
 	u32				tid;
 };
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12737| <<sync_child_event>> perf_event_read_event(child_event, task);
+ */
 static void
 perf_event_read_event(struct perf_event *event,
 			struct task_struct *task)
@@ -7578,6 +7915,15 @@ perf_event_read_event(struct perf_event *event,
 
 typedef void (perf_iterate_f)(struct perf_event *event, void *data);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7874| <<perf_iterate_sb>> perf_iterate_ctx(task_ctx, output, data, false);
+ *   - kernel/events/core.c|7883| <<perf_iterate_sb>> perf_iterate_ctx(ctx, output, data, false);
+ *   - kernel/events/core.c|7935| <<perf_event_exec>> perf_iterate_ctx(ctx, perf_event_addr_filters_exec,
+ *   - kernel/events/core.c|7986| <<__perf_pmu_output_stop>> perf_iterate_ctx(&cpuctx->ctx, __perf_event_output_stop, &ro, false);
+ *   - kernel/events/core.c|7988| <<__perf_pmu_output_stop>> perf_iterate_ctx(cpuctx->task_ctx, __perf_event_output_stop,
+ *   - kernel/events/core.c|8814| <<perf_addr_filters_adjust>> perf_iterate_ctx(ctx, __perf_addr_filters_adjust, vma, true);
+ */
 static void
 perf_iterate_ctx(struct perf_event_context *ctx,
 		   perf_iterate_f output,
@@ -7820,6 +8166,10 @@ static int perf_event_task_match(struct perf_event *event)
 	       event->attr.task;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7936| <<perf_event_task>> perf_iterate_sb(perf_event_task_output,
+ */
 static void perf_event_task_output(struct perf_event *event,
 				   void *data)
 {
@@ -7863,6 +8213,12 @@ static void perf_event_task_output(struct perf_event *event,
 	task_event->event_id.header.size = size;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|7943| <<perf_event_fork>> perf_event_task(task, NULL, 1);
+ *   - kernel/events/core.c|12853| <<perf_event_exit_task_context>> perf_event_task(child, child_ctx, 0);
+ *   - kernel/events/core.c|12897| <<perf_event_exit_task>> perf_event_task(child, NULL, 0);
+ */
 static void perf_event_task(struct task_struct *task,
 			      struct perf_event_context *task_ctx,
 			      int new)
@@ -7924,6 +8280,10 @@ static int perf_event_comm_match(struct perf_event *event)
 	return event->attr.comm;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8016| <<perf_event_comm_event>> perf_iterate_sb(perf_event_comm_output,
+ */
 static void perf_event_comm_output(struct perf_event *event,
 				   void *data)
 {
@@ -7957,6 +8317,10 @@ static void perf_event_comm_output(struct perf_event *event,
 	comm_event->event_id.header.size = size;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8043| <<perf_event_comm>> perf_event_comm_event(&comm_event);
+ */
 static void perf_event_comm_event(struct perf_comm_event *comm_event)
 {
 	char comm[TASK_COMM_LEN];
@@ -7976,6 +8340,10 @@ static void perf_event_comm_event(struct perf_comm_event *comm_event)
 		       NULL);
 }
 
+/*
+ * called by:
+ *   - fs/exec.c|1234| <<__set_task_comm>> perf_event_comm(tsk, exec);
+ */
 void perf_event_comm(struct task_struct *task, bool exec)
 {
 	struct perf_comm_event comm_event;
@@ -8268,6 +8636,10 @@ static int perf_event_mmap_match(struct perf_event *event,
 	       (executable && (event->attr.mmap || event->attr.mmap2));
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8492| <<perf_event_mmap_event>> perf_iterate_sb(perf_event_mmap_output,
+ */
 static void perf_event_mmap_output(struct perf_event *event,
 				   void *data)
 {
@@ -8335,6 +8707,10 @@ static void perf_event_mmap_output(struct perf_event *event,
 	mmap_event->event_id.header.type = type;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8633| <<perf_event_mmap>> perf_event_mmap_event(&mmap_event);
+ */
 static void perf_event_mmap_event(struct perf_mmap_event *mmap_event)
 {
 	struct vm_area_struct *vma = mmap_event->vma;
@@ -8556,6 +8932,15 @@ static void perf_addr_filters_adjust(struct vm_area_struct *vma)
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - mm/mmap.c|1819| <<mmap_region>> perf_event_mmap(vma);
+ *   - mm/mmap.c|2427| <<expand_upwards>> perf_event_mmap(vma);
+ *   - mm/mmap.c|2505| <<expand_downwards>> perf_event_mmap(vma);
+ *   - mm/mmap.c|3032| <<do_brk_flags>> perf_event_mmap(vma);
+ *   - mm/mmap.c|3401| <<__install_special_mapping>> perf_event_mmap(vma);
+ *   - mm/mprotect.c|651| <<mprotect_fixup>> perf_event_mmap(vma);
+ */
 void perf_event_mmap(struct vm_area_struct *vma)
 {
 	struct perf_mmap_event mmap_event;
@@ -8678,6 +9063,10 @@ static int perf_event_switch_match(struct perf_event *event)
 	return event->attr.context_switch;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8841| <<perf_event_switch>> perf_iterate_sb(perf_event_switch_output, &switch_event, NULL);
+ */
 static void perf_event_switch_output(struct perf_event *event, void *data)
 {
 	struct perf_switch_event *se = data;
@@ -8707,6 +9096,17 @@ static void perf_event_switch_output(struct perf_event *event, void *data)
 	if (ret)
 		return;
 
+	/*
+	 * struct perf_switch_event *se = data;
+	 *     struct task_struct      *task;
+	 *     struct task_struct      *next_prev;
+	 *
+	 *     struct {
+	 *         struct perf_event_header        header;
+	 *         u32                             next_prev_pid;
+	 *         u32                             next_prev_tid;
+	 *     } event_id;
+	 */
 	if (event->ctx->task)
 		perf_output_put(&handle, se->event_id.header);
 	else
@@ -8717,6 +9117,11 @@ static void perf_event_switch_output(struct perf_event *event, void *data)
 	perf_output_end(&handle);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3588| <<__perf_event_task_sched_out>> perf_event_switch(task, next, false);
+ *   - kernel/events/core.c|3922| <<__perf_event_task_sched_in>> perf_event_switch(task, prev, true);
+ */
 static void perf_event_switch(struct task_struct *task,
 			      struct task_struct *next_prev, bool sched_in)
 {
@@ -9187,6 +9592,12 @@ int perf_event_account_interrupt(struct perf_event *event)
  * Generic event overflow handling, sampling.
  */
 
+/*
+ * called by:
+ *   - kernel/events/core.c|9399| <<perf_event_overflow>> return __perf_event_overflow(event, 1, data, regs);
+ *   - kernel/events/core.c|9461| <<perf_swevent_overflow>> if (__perf_event_overflow(event, throttle,
+ *   - kernel/events/core.c|10783| <<perf_swevent_hrtimer>> if (__perf_event_overflow(event, 1, &data, regs))
+ */
 static int __perf_event_overflow(struct perf_event *event,
 				   int throttle, struct perf_sample_data *data,
 				   struct pt_regs *regs)
@@ -9227,6 +9638,36 @@ static int __perf_event_overflow(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/perf_event.c|860| <<alpha_perf_event_irq_handler>> if (perf_event_overflow(event, &data, regs)) {
+ *   - arch/arc/kernel/perf_event.c|603| <<arc_pmu_intr>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_v6.c|347| <<armv6pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_v7.c|994| <<armv7pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_xscale.c|189| <<xscale1pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm/kernel/perf_event_xscale.c|535| <<xscale2pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/arm64/kernel/perf_event.c|882| <<armv8pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/csky/kernel/perf_event.c|1142| <<csky_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/mips/kernel/perf_event_mipsxx.c|794| <<handle_associated_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/powerpc/perf/core-book3s.c|2310| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/powerpc/perf/core-fsl-emb.c|637| <<record_and_restart>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/s390/kernel/perf_cpum_cf.c|669| <<cfdiag_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/s390/kernel/perf_cpum_sf.c|1138| <<perf_push_sample>> if (perf_event_overflow(event, &data, &regs)) {
+ *   - arch/s390/kernel/perf_pai_crypto.c|371| <<paicrypt_push_sample>> overflow = perf_event_overflow(event, &data, &regs);
+ *   - arch/sparc/kernel/perf_event.c|1671| <<perf_event_nmi_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/core.c|961| <<amd_pmu_v2_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/amd/ibs.c|804| <<perf_ibs_handle_irq>> throttle = perf_event_overflow(event, &data, &regs);
+ *   - arch/x86/events/core.c|1754| <<x86_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/core.c|2898| <<x86_pmu_handle_guest_pebs>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/core.c|3010| <<handle_pmi_common>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/ds.c|1976| <<__intel_pmu_pebs_event>> if (perf_event_overflow(event, data, regs))
+ *   - arch/x86/events/intel/knc.c|255| <<knc_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/intel/p4.c|1051| <<p4_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/x86/events/zhaoxin/core.c|400| <<zhaoxin_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - arch/xtensa/kernel/perf_event.c|382| <<xtensa_pmu_irq_handler>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/apple_m1_cpu_pmu.c|415| <<m1_pmu_handle_irq>> if (perf_event_overflow(event, &data, regs))
+ *   - drivers/perf/riscv_pmu_sbi.c|635| <<pmu_sbi_ovf_handler>> perf_event_overflow(event, &data, regs);
+ */
 int perf_event_overflow(struct perf_event *event,
 			  struct perf_sample_data *data,
 			  struct pt_regs *regs)
@@ -11242,6 +11683,33 @@ static inline bool has_extended_regs(struct perf_event *event)
 	       (event->attr.sample_regs_intr & PERF_REG_EXTENDED_MASK);
 }
 
+/*
+ * 5.4的例子
+ * [0] perf_try_init_event
+ * [0] perf_event_alloc
+ * [0] perf_event_alloc
+ * [0] perf_event_create_kernel_counter
+ * [0] kvm_pmu_create_perf_event
+ * [0] kvm_pmu_enable_counter_mask
+ * [0] kvm_pmu_handle_pmcr
+ * [0] access_pmcr
+ * [0] perform_access
+ * [0] kvm_handle_sys_reg
+ * [0] handle_exit
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __arm64_sys_ioctl
+ * [0] el0_svc_common
+ * [0] el0_svc_handler
+ * [0] el0_svc
+ *
+ * called by:
+ *   - kernel/events/core.c|11522| <<perf_init_event>> ret = perf_try_init_event(pmu, event);
+ *   - kernel/events/core.c|11551| <<perf_init_event>> ret = perf_try_init_event(pmu, event);
+ *   - kernel/events/core.c|11564| <<perf_init_event>> ret = perf_try_init_event(pmu, event);
+ */
 static int perf_try_init_event(struct pmu *pmu, struct perf_event *event)
 {
 	struct perf_event_context *ctx = NULL;
@@ -11291,6 +11759,10 @@ static int perf_try_init_event(struct pmu *pmu, struct perf_event *event)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|11851| <<perf_event_alloc>> pmu = perf_init_event(event);
+ */
 static struct pmu *perf_init_event(struct perf_event *event)
 {
 	bool extended_type = false;
@@ -11413,6 +11885,10 @@ static void account_freq_event(void)
 }
 
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12079| <<perf_event_alloc>> account_event(event);
+ */
 static void account_event(struct perf_event *event)
 {
 	bool inc = false;
@@ -11432,6 +11908,14 @@ static void account_event(struct perf_event *event)
 		atomic_inc(&nr_namespaces_events);
 	if (event->attr.cgroup)
 		atomic_inc(&nr_cgroup_events);
+	/*
+	 * 在以下使用nr_switch_events:
+	 *   - kernel/events/core.c|405| <<global>> static atomic_t nr_switch_events __read_mostly;
+	 *   - kernel/events/core.c|3628| <<__perf_event_task_sched_out>> if (atomic_read(&nr_switch_events))
+	 *   - kernel/events/core.c|3970| <<__perf_event_task_sched_in>> if (atomic_read(&nr_switch_events))
+	 *   - kernel/events/core.c|4904| <<unaccount_event>> atomic_dec(&nr_switch_events);
+	 *   - kernel/events/core.c|11737| <<account_event>> atomic_inc(&nr_switch_events);
+	 */
 	if (event->attr.task)
 		atomic_inc(&nr_task_events);
 	if (event->attr.freq)
@@ -11462,6 +11946,14 @@ static void account_event(struct perf_event *event)
 
 		mutex_lock(&perf_sched_mutex);
 		if (!atomic_read(&perf_sched_count)) {
+			/*
+			 * 在以下使用perf_sched_events:
+			 *   - kernel/events/core.c|391| <<global>> DEFINE_STATIC_KEY_FALSE(perf_sched_events);
+			 *   - include/linux/perf_event.h|1271| <<perf_event_task_sched_in>> if (static_branch_unlikely(&perf_sched_events))
+			 *   - include/linux/perf_event.h|1294| <<perf_event_task_sched_out>> if (static_branch_unlikely(&perf_sched_events))
+			 *   - kernel/events/core.c|4990| <<perf_sched_delayed>> static_branch_disable(&perf_sched_events);
+			 *   - kernel/events/core.c|11838| <<account_event>> static_branch_enable(&perf_sched_events);
+			 */
 			static_branch_enable(&perf_sched_events);
 			/*
 			 * Guarantee that all CPUs observe they key change and
@@ -11487,6 +11979,13 @@ static void account_event(struct perf_event *event)
 /*
  * Allocate and initialize an event structure
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|12389| <<SYSCALL_DEFINE5(perf_event_open)>> event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
+ *   - kernel/events/core.c|12781| <<perf_event_create_kernel_counter>> event = perf_event_alloc(attr, cpu, task, NULL, NULL,
+ *   - kernel/events/core.c|13237| <<inherit_event>> child_event = perf_event_alloc(&parent_event->attr,
+ *   - security/security.c|2634| <<security_perf_event_alloc>> return call_int_hook(perf_event_alloc, 0, event);
+ */
 static struct perf_event *
 perf_event_alloc(struct perf_event_attr *attr, int cpu,
 		 struct task_struct *task,
@@ -12046,6 +12545,31 @@ perf_check_permission(struct perf_event_attr *attr, struct task_struct *task)
 	return is_capable || ptrace_may_access(task, ptrace_mode);
 }
 
+/*
+ * pid == 0: event绑定到当前进程;
+ * pid > 0: event绑定到指定进程;
+ * pid == -1: event绑定到当前cpu的所有进程.
+ * cpu >= 0: event绑定到指定cpu;
+ * cpu == -1: event绑定到所有cpu;
+ *
+ * 在同时指定的情况下task维度优先于cpu维度,所以pid、cpu组合起来有以下几种情况:
+ *
+ * 组合1:pid >= 0, cpu >= 0. perf_event绑定到task维度的context.
+ * task在得到cpu调度运行的时候,
+ * context上挂载的本task相关的perf_event也开始运行.
+ * 但是如果event指定的cpu不等于当前运行的cpu, event不会得到执行,
+ * 这样就符合了这个组合的含义;
+ *
+ * 组合2:pid >= 0, cpu == -1. perf_event绑定到task维度的context.
+ * 只要task得到调度, 该perf_event就会得到执行;
+ *
+ * 组合3:pid == -1, cpu >= 0. perf_event绑定到cpu维度的context. 只要该cpu运行,
+ * 该perf_event就会得到执行. 目前只有在cpu online的情况下才能绑定perf_event,
+ * cpu hotplug支持可能会有问题;
+ *
+ * 组合4:pid == -1, cpu == -1. 这种组合目前是非法的, 相当于整系统所有cpu,
+ * 所有进程.
+ */
 /**
  * sys_perf_event_open - open a performance event, associate it to a task/cpu
  *
@@ -12163,6 +12687,13 @@ SYSCALL_DEFINE5(perf_event_open,
 	if (flags & PERF_FLAG_PID_CGROUP)
 		cgroup_fd = pid;
 
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|12389| <<SYSCALL_DEFINE5(perf_event_open)>> event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
+	 *   - kernel/events/core.c|12781| <<perf_event_create_kernel_counter>> event = perf_event_alloc(attr, cpu, task, NULL, NULL,
+	 *   - kernel/events/core.c|13237| <<inherit_event>> child_event = perf_event_alloc(&parent_event->attr,
+	 *   - security/security.c|2634| <<security_perf_event_alloc>> return call_int_hook(perf_event_alloc, 0, event);
+	 */
 	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
 				 NULL, NULL, cgroup_fd);
 	if (IS_ERR(event)) {
@@ -12525,6 +13056,37 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * 调度进去时:
+ *
+ * prepare_task_switch()
+ * -> perf_event_task_sched_out()
+ *    -> __perf_event_task_sched_out()
+ *       -> perf_pmu_sched_task()
+ *          -> pmu->sched_task(cpuctx->task_ctx, sched_in)
+ *
+ * 调度出去时:
+ *
+ * finish_task_switch()
+ * -> perf_event_task_sched_in()
+ *    -> perf_event_context_sched_in()
+ *       -> perf_event_sched_in()
+ *
+ * 似乎都是在下面触发的:
+ *   - kernel/events/core.c|9407| <<__perf_event_overflow>> READ_ONCE(event->overflow_handler)(event, data, regs);
+ *
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|684| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/arm64/kvm/pmu-emul.c|694| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/arm64/kvm/pmu-emul.c|819| <<kvm_pmu_probe_armpmu>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|949| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+ *   - arch/x86/kvm/pmu.c|196| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|289| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+ *   - kernel/events/hw_breakpoint.c|463| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+ *   - kernel/events/hw_breakpoint.c|573| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+ *   - kernel/watchdog_hld.c|176| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
@@ -12670,6 +13232,10 @@ void perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)
 }
 EXPORT_SYMBOL_GPL(perf_pmu_migrate_context);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2203| <<perf_child_detach>> sync_child_event(event);
+ */
 static void sync_child_event(struct perf_event *child_event)
 {
 	struct perf_event *parent_event = child_event->parent;
@@ -12744,6 +13310,10 @@ perf_event_exit_event(struct perf_event *event, struct perf_event_context *ctx)
 	perf_event_wakeup(event);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12889| <<perf_event_exit_task>> perf_event_exit_task_context(child, ctxn);
+ */
 static void perf_event_exit_task_context(struct task_struct *child, int ctxn)
 {
 	struct perf_event_context *child_ctx, *clone_ctx = NULL;
@@ -12811,6 +13381,11 @@ static void perf_event_exit_task_context(struct task_struct *child, int ctxn)
  * Can be called with exec_update_lock held when called from
  * setup_new_exec().
  */
+/*
+ * called by:
+ *   - fs/exec.c|1385| <<begin_new_exec>> perf_event_exit_task(me);
+ *   - kernel/exit.c|804| <<do_exit>> perf_event_exit_task(tsk);
+ */
 void perf_event_exit_task(struct task_struct *child)
 {
 	struct perf_event *event, *tmp;
diff --git a/kernel/events/internal.h b/kernel/events/internal.h
index 5150d5f84..173e75585 100644
--- a/kernel/events/internal.h
+++ b/kernel/events/internal.h
@@ -54,7 +54,38 @@ struct perf_buffer {
 	void				**aux_pages;
 	void				*aux_priv;
 
+	/*
+	 * 在以下使用perf_buffer->user_page:
+	 *   - kernel/events/core.c|5852| <<perf_event_init_userpage>> userpg = rb->user_page;
+	 *   - kernel/events/core.c|5896| <<perf_event_update_userpage>> userpg = rb->user_page;
+	 *   - kernel/events/core.c|6258| <<perf_mmap>> aux_offset = READ_ONCE(rb->user_page->aux_offset);
+	 *   - kernel/events/core.c|6259| <<perf_mmap>> aux_size = READ_ONCE(rb->user_page->aux_size);
+	 *   - kernel/events/ring_buffer.c|122| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - kernel/events/ring_buffer.c|213| <<__perf_output_begin>> tail = READ_ONCE(rb->user_page->data_tail);
+	 *   - kernel/events/ring_buffer.c|492| <<perf_aux_output_begin>> aux_tail = READ_ONCE(rb->user_page->aux_tail);
+	 *   - kernel/events/ring_buffer.c|582| <<perf_aux_output_end>> WRITE_ONCE(rb->user_page->aux_head, rb->aux_head);
+	 *   - kernel/events/ring_buffer.c|614| <<perf_aux_output_skip>> WRITE_ONCE(rb->user_page->aux_head, rb->aux_head);
+	 *   - kernel/events/ring_buffer.c|849| <<__perf_mmap_to_page>> return virt_to_page(rb->user_page);
+	 *   - kernel/events/ring_buffer.c|892| <<rb_alloc>> rb->user_page = perf_mmap_alloc_page(cpu);
+	 *   - kernel/events/ring_buffer.c|893| <<rb_alloc>> if (!rb->user_page)
+	 *   - kernel/events/ring_buffer.c|912| <<rb_alloc>> perf_mmap_free_page(rb->user_page);
+	 *   - kernel/events/ring_buffer.c|925| <<rb_free>> perf_mmap_free_page(rb->user_page);
+	 *   - kernel/events/ring_buffer.c|939| <<__perf_mmap_to_page>> return vmalloc_to_page((void *)rb->user_page + pgoff * PAGE_SIZE);
+	 *   - kernel/events/ring_buffer.c|958| <<rb_free_work>> base = rb->user_page;
+	 *   - kernel/events/ring_buffer.c|993| <<rb_alloc>> rb->user_page = all_buf;
+	 */
 	struct perf_event_mmap_page	*user_page;
+	/*
+	 * 在以下使用perf_buffer->data_pages[]:
+	 *   - kernel/events/internal.h|153| <<__DEFINE_OUTPUT_COPY_BODY(advance_buf)>> handle->addr = rb->data_pages[handle->page]; \
+	 *   - kernel/events/ring_buffer.c|257| <<__perf_output_begin>> handle->addr = rb->data_pages[handle->page] + offset;
+	 *   - kernel/events/ring_buffer.c|851| <<__perf_mmap_to_page>> return virt_to_page(rb->data_pages[pgoff - 1]);
+	 *   - kernel/events/ring_buffer.c|897| <<rb_alloc>> rb->data_pages[i] = perf_mmap_alloc_page(cpu);
+	 *   - kernel/events/ring_buffer.c|898| <<rb_alloc>> if (!rb->data_pages[i])
+	 *   - kernel/events/ring_buffer.c|910| <<rb_alloc>> perf_mmap_free_page(rb->data_pages[i]);
+	 *   - kernel/events/ring_buffer.c|927| <<rb_free>> perf_mmap_free_page(rb->data_pages[i]);
+	 *   - kernel/events/ring_buffer.c|994| <<rb_alloc>> rb->data_pages[0] = all_buf + PAGE_SIZE;
+	 */
 	void				*data_pages[];
 };
 
diff --git a/kernel/events/ring_buffer.c b/kernel/events/ring_buffer.c
index 726132039..155b85f9f 100644
--- a/kernel/events/ring_buffer.c
+++ b/kernel/events/ring_buffer.c
@@ -47,6 +47,11 @@ static void perf_output_get_handle(struct perf_output_handle *handle)
 	handle->wakeup = local_read(&rb->wakeup);
 }
 
+/*
+ * called by:
+ *   - kernel/events/ring_buffer.c|260| <<__perf_output_begin>> perf_output_put_handle(handle);
+ *   - kernel/events/ring_buffer.c|304| <<perf_output_end>> perf_output_put_handle(handle);
+ */
 static void perf_output_put_handle(struct perf_output_handle *handle)
 {
 	struct perf_buffer *rb = handle->rb;
@@ -106,6 +111,13 @@ static void perf_output_put_handle(struct perf_output_handle *handle)
 	 *
 	 * See perf_output_begin().
 	 */
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 	smp_wmb(); /* B, matches C */
 	WRITE_ONCE(rb->user_page->data_head, head);
 
@@ -145,6 +157,12 @@ ring_buffer_has_space(unsigned long head, unsigned long tail,
 		return CIRC_SPACE(tail, head, data_size) >= size;
 }
 
+/*
+ * called by:
+ *   - kernel/events/ring_buffer.c|271| <<perf_output_begin_forward>> return __perf_output_begin(handle, data, event, size, false);
+ *   - kernel/events/ring_buffer.c|278| <<perf_output_begin_backward>> return __perf_output_begin(handle, data, event, size, true);
+ *   - kernel/events/ring_buffer.c|286| <<perf_output_begin>> return __perf_output_begin(handle, data, event, size,
+ */
 static __always_inline int
 __perf_output_begin(struct perf_output_handle *handle,
 		    struct perf_sample_data *data,
@@ -278,6 +296,28 @@ int perf_output_begin_backward(struct perf_output_handle *handle,
 	return __perf_output_begin(handle, data, event, size, true);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1340| <<dump_trace_imc_data>> if (perf_output_begin(&handle, &data, event, header.size))
+ *   - arch/s390/kernel/perf_cpum_sf.c|675| <<cpumsf_output_event_pid>> if (perf_output_begin(&handle, data, event, header.size))
+ *   - arch/x86/events/intel/ds.c|764| <<intel_pmu_drain_bts_buffer>> if (perf_output_begin(&handle, &data, event,
+ *   - kernel/events/core.c|7578| <<perf_event_output>> return __perf_event_output(event, data, regs, perf_output_begin);
+ *   - kernel/events/core.c|7610| <<perf_event_read_event>> ret = perf_output_begin(&handle, &sample, event, read_event.header.size);
+ *   - kernel/events/core.c|7879| <<perf_event_task_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|7982| <<perf_event_comm_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8082| <<perf_event_namespaces_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8209| <<perf_event_cgroup_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8338| <<perf_event_mmap_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8659| <<perf_event_aux_event>> ret = perf_output_begin(&handle, &sample, event, rec.header.size);
+ *   - kernel/events/core.c|8693| <<perf_log_lost_samples>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8748| <<perf_event_switch_output>> ret = perf_output_begin(&handle, &sample, event, se->event_id.header.size);
+ *   - kernel/events/core.c|8822| <<perf_log_throttle>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8865| <<perf_event_ksymbol_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|8955| <<perf_event_bpf_output>> ret = perf_output_begin(&handle, data, event,
+ *   - kernel/events/core.c|9064| <<perf_event_text_poke_output>> ret = perf_output_begin(&handle, &sample, event,
+ *   - kernel/events/core.c|9146| <<perf_log_itrace_start>> ret = perf_output_begin(&handle, &sample, event, rec.header.size);
+ *   - kernel/events/core.c|9176| <<perf_report_aux_output_id>> ret = perf_output_begin(&handle, &sample, event, rec.header.size);
+ */
 int perf_output_begin(struct perf_output_handle *handle,
 		      struct perf_sample_data *data,
 		      struct perf_event *event, unsigned int size)
@@ -299,12 +339,39 @@ unsigned int perf_output_skip(struct perf_output_handle *handle,
 	return __output_skip(handle, NULL, len);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/perf/imc-pmu.c|1344| <<dump_trace_imc_data>> perf_output_end(&handle);
+ *   - arch/s390/kernel/perf_cpum_sf.c|683| <<cpumsf_output_event_pid>> perf_output_end(&handle);
+ *   - arch/x86/events/intel/ds.c|780| <<intel_pmu_drain_bts_buffer>> perf_output_end(&handle);
+ *   - kernel/events/core.c|7579| <<__perf_event_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|7656| <<perf_event_read_event>> perf_output_end(&handle);
+ *   - kernel/events/core.c|7945| <<perf_event_task_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8049| <<perf_event_comm_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8156| <<perf_event_namespaces_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8279| <<perf_event_cgroup_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8438| <<perf_event_mmap_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8744| <<perf_event_aux_event>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8777| <<perf_log_lost_samples>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8851| <<perf_event_switch_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8926| <<perf_log_throttle>> perf_output_end(&handle);
+ *   - kernel/events/core.c|8971| <<perf_event_ksymbol_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9060| <<perf_event_bpf_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9178| <<perf_event_text_poke_output>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9251| <<perf_log_itrace_start>> perf_output_end(&handle);
+ *   - kernel/events/core.c|9281| <<perf_report_aux_output_id>> perf_output_end(&handle);
+ */
 void perf_output_end(struct perf_output_handle *handle)
 {
 	perf_output_put_handle(handle);
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - kernel/events/ring_buffer.c|877| <<rb_alloc>> ring_buffer_init(rb, watermark, flags);
+ *   - kernel/events/ring_buffer.c|973| <<rb_alloc>> ring_buffer_init(rb, watermark, flags);
+ */
 static void
 ring_buffer_init(struct perf_buffer *rb, long watermark, int flags)
 {
@@ -943,6 +1010,10 @@ struct perf_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)
 
 #endif
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5945| <<perf_mmap_fault>> vmf->page = perf_mmap_to_page(rb, vmf->pgoff);
+ */
 struct page *
 perf_mmap_to_page(struct perf_buffer *rb, unsigned long pgoff)
 {
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 40fe7806c..fdf509d2c 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -647,6 +647,24 @@ int irq_setup_affinity(struct irq_desc *desc)
  *	outside, such as KVM. One example code path is as below:
  *	KVM -> IOMMU -> irq_set_vcpu_affinity().
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1152| <<kvm_timer_hyp_init>> err = irq_set_vcpu_affinity(host_vtimer_irq,
+ *   - arch/arm64/kvm/arch_timer.c|1176| <<kvm_timer_hyp_init>> err = irq_set_vcpu_affinity(host_ptimer_irq,
+ *   - arch/powerpc/kvm/book3s_xive.c|1588| <<kvmppc_xive_set_mapped>> rc = irq_set_vcpu_affinity(host_irq, state);
+ *   - arch/powerpc/kvm/book3s_xive.c|1687| <<kvmppc_xive_clr_mapped>> rc = irq_set_vcpu_affinity(host_irq, NULL);
+ *   - arch/x86/kvm/svm/avic.c|952| <<avic_pi_update_irte>> ret = irq_set_vcpu_affinity(host_irq, &pi);
+ *   - arch/x86/kvm/svm/avic.c|974| <<avic_pi_update_irte>> ret = irq_set_vcpu_affinity(host_irq, &pi);
+ *   - arch/x86/kvm/vmx/posted_intr.c|339| <<vmx_pi_update_irte>> ret = irq_set_vcpu_affinity(host_irq, NULL);
+ *   - arch/x86/kvm/vmx/posted_intr.c|357| <<vmx_pi_update_irte>> ret = irq_set_vcpu_affinity(host_irq, &vcpu_info);
+ *   - arch/x86/kvm/vmx/posted_intr.c|359| <<vmx_pi_update_irte>> ret = irq_set_vcpu_affinity(host_irq, NULL);
+ *   - drivers/irqchip/irq-gic-v4.c|233| <<its_send_vpe_cmd>> return irq_set_vcpu_affinity(vpe->irq, info);
+ *   - drivers/irqchip/irq-gic-v4.c|328| <<its_map_vlpi>> ret = irq_set_vcpu_affinity(irq, &info);
+ *   - drivers/irqchip/irq-gic-v4.c|344| <<its_get_vlpi>> return irq_set_vcpu_affinity(irq, &info);
+ *   - drivers/irqchip/irq-gic-v4.c|350| <<its_unmap_vlpi>> return irq_set_vcpu_affinity(irq, NULL);
+ *   - drivers/irqchip/irq-gic-v4.c|362| <<its_prop_update_vlpi>> return irq_set_vcpu_affinity(irq, &info);
+ *   - drivers/irqchip/irq-gic-v4.c|375| <<its_prop_update_vsgi>> return irq_set_vcpu_affinity(irq, &info);
+ */
 int irq_set_vcpu_affinity(unsigned int irq, void *vcpu_info)
 {
 	unsigned long flags;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index ee28253c9..0c6d6d492 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4714,6 +4714,10 @@ EXPORT_SYMBOL_GPL(preempt_notifier_dec);
  * preempt_notifier_register - tell me when current is being preempted & rescheduled
  * @notifier: notifier struct to register
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|225| <<vcpu_load>> preempt_notifier_register(&vcpu->preempt_notifier);
+ */
 void preempt_notifier_register(struct preempt_notifier *notifier)
 {
 	if (!static_branch_unlikely(&preempt_notifier_key))
@@ -4983,6 +4987,12 @@ static inline void kmap_local_sched_in(void)
  * prepare_task_switch sets up locking and calls architecture specific
  * hooks.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|5139| <<context_switch>> prepare_task_switch(rq, prev, next);
+ *
+ * 完成进程切换的准备工作
+ */
 static inline void
 prepare_task_switch(struct rq *rq, struct task_struct *prev,
 		    struct task_struct *next)
@@ -5016,6 +5026,13 @@ prepare_task_switch(struct rq *rq, struct task_struct *prev,
  * past. prev == current is still correct but we need to recalculate this_rq
  * because prev may have moved to another CPU.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|5118| <<schedule_tail>> finish_task_switch(prev);
+ *   - kernel/sched/core.c|5185| <<context_switch>> return finish_task_switch(prev);
+ *
+ * 进程切换之后的处理工作
+ */
 static struct rq *finish_task_switch(struct task_struct *prev)
 	__releases(rq->lock)
 {
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index 95fc77853..190987f3f 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -136,6 +136,15 @@ void account_user_time(struct task_struct *p, u64 cputime)
  * @p: the process that the CPU time gets accounted to
  * @cputime: the CPU time spent in virtual machine since the last update
  */
+/*
+ * called by:
+ *   - arch/ia64/kernel/time.c|77| <<vtime_flush>> account_guest_time(tsk, cycle_to_nsec(ti->gtime));
+ *   - arch/powerpc/kernel/time.c|424| <<vtime_flush>> account_guest_time(tsk, cputime_to_nsecs(acct->gtime));
+ *   - arch/s390/kernel/vtime.c|171| <<do_account_vtime>> account_guest_time(tsk, cputime_to_nsecs(guest));
+ *   - kernel/sched/cputime.c|189| <<account_system_time>> account_guest_time(p, cputime);
+ *   - kernel/sched/cputime.c|402| <<irqtime_account_process_tick>> account_guest_time(p, cputime);
+ *   - kernel/sched/cputime.c|693| <<vtime_account_guest>> account_guest_time(tsk, vtime->gtime);
+ */
 void account_guest_time(struct task_struct *p, u64 cputime)
 {
 	u64 *cpustat = kcpustat_this_cpu->cpustat;
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 914096c5b..1231ed732 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1326,6 +1326,11 @@ static bool numa_is_active_node(int nid, struct numa_group *ng)
 }
 
 /* Handle placement on systems where not all nodes are directly connected. */
+/*
+ * called by:
+ *   - kernel/sched/fair.c|1414| <<task_weight>> faults += score_nearby_nodes(p, nid, dist, true);
+ *   - kernel/sched/fair.c|1434| <<group_weight>> faults += score_nearby_nodes(p, nid, dist, false);
+ */
 static unsigned long score_nearby_nodes(struct task_struct *p, int nid,
 					int lim_dist, bool task)
 {
diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index b9f54544e..2d8c72ec2 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -103,6 +103,10 @@ enum {
  *
  * Returns 1 when the thread should exit, 0 otherwise.
  */
+/*
+ * 在以下使用smpboot_thread_fn()
+ *   - kernel/smpboot.c|184| <<__smpboot_create_thread>> tsk = kthread_create_on_cpu(smpboot_thread_fn, td, cpu,
+ */
 static int smpboot_thread_fn(void *data)
 {
 	struct smpboot_thread_data *td = data;
@@ -166,6 +170,11 @@ static int smpboot_thread_fn(void *data)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/smpboot.c|220| <<smpboot_create_threads>> ret = __smpboot_create_thread(cur, cpu);
+ *   - kernel/smpboot.c|297| <<smpboot_register_percpu_thread>> ret = __smpboot_create_thread(plug_thread, cpu);
+ */
 static int
 __smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
 {
@@ -286,6 +295,15 @@ static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
  *
  * Creates and starts the threads on all online cpus.
  */
+/*
+ * called by:
+ *   - drivers/powercap/idle_inject.c|368| <<idle_inject_init>> return smpboot_register_percpu_thread(&idle_inject_threads);
+ *   - kernel/cpu.c|971| <<cpuhp_threads_init>> BUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));
+ *   - kernel/irq_work.c|314| <<irq_work_init_threads>> BUG_ON(smpboot_register_percpu_thread(&irqwork_threads));
+ *   - kernel/rcu/tree.c|2629| <<rcu_spawn_core_kthreads>> WARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),
+ *   - kernel/softirq.c|983| <<spawn_ksoftirqd>> BUG_ON(smpboot_register_percpu_thread(&softirq_threads));
+ *   - kernel/stop_machine.c|579| <<cpu_stop_init>> BUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));
+ */
 int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 {
 	unsigned int cpu;
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index cedb17ba1..9b2bbbcb6 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -28,6 +28,11 @@
  * be shared by works on different cpus.
  */
 struct cpu_stop_done {
+	/*
+	 * 在以下使用cpu_stop_done->nr_todo:
+	 *   - kernel/stop_machine.c|73| <<cpu_stop_init_done>> atomic_set(&done->nr_todo, nr_todo);
+	 *   - kernel/stop_machine.c|80| <<cpu_stop_signal_done>> if (atomic_dec_and_test(&done->nr_todo))
+	 */
 	atomic_t		nr_todo;	/* nr left to execute */
 	int			ret;		/* collected return value */
 	struct completion	completion;	/* fired if nr_todo reaches 0 */
@@ -46,9 +51,35 @@ struct cpu_stopper {
 	cpu_stop_fn_t		fn;
 };
 
+/*
+ * 在以下使用percpu的cpu_stopper:
+ *   - kernel/stop_machine.c|49| <<global>> static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
+ *   - kernel/stop_machine.c|58| <<print_stop_info>> struct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));
+ *   - kernel/stop_machine.c|95| <<cpu_stop_queue_work>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|264| <<cpu_stop_queue_two_works>> struct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);
+ *   - kernel/stop_machine.c|265| <<cpu_stop_queue_two_works>> struct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);
+ *   - kernel/stop_machine.c|408| <<queue_stop_cpus_work>> work = &per_cpu(cpu_stopper.stop_work, cpu);
+ *   - kernel/stop_machine.c|476| <<cpu_stop_should_run>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|488| <<cpu_stopper_thread>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|528| <<stop_machine_park>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|540| <<cpu_stop_create>> sched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));
+ *   - kernel/stop_machine.c|545| <<cpu_stop_park>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|552| <<stop_machine_unpark>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|573| <<cpu_stop_init>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ */
 static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
+/*
+ * 在以下使用stop_machine_initialized:
+ *   - kernel/stop_machine.c|581| <<cpu_stop_init>> stop_machine_initialized = true;
+ *   - kernel/stop_machine.c|598| <<stop_machine_cpuslocked>> if (!stop_machine_initialized) {
+ */
 static bool stop_machine_initialized = false;
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|8886| <<sched_show_task>> print_stop_info(KERN_INFO, p);
+ *   - lib/dump_stack.c|70| <<dump_stack_print_info>> print_stop_info(log_lvl, current);
+ */
 void print_stop_info(const char *log_lvl, struct task_struct *task)
 {
 	/*
@@ -63,32 +94,104 @@ void print_stop_info(const char *log_lvl, struct task_struct *task)
 	printk("%sStopper: %pS <- %pS\n", log_lvl, stopper->fn, (void *)stopper->caller);
 }
 
+/*
+ * 在以下使用stop_cpus_mutex:
+ *   - kernel/stop_machine.c|98| <<global>> static DEFINE_MUTEX(stop_cpus_mutex);
+ *   - kernel/stop_machine.c|650| <<stop_cpus>> mutex_lock(&stop_cpus_mutex);
+ *   - kernel/stop_machine.c|652| <<stop_cpus>> mutex_unlock(&stop_cpus_mutex);
+ *   - kernel/stop_machine.c|962| <<stop_machine_from_inactive_cpu>> while (!mutex_trylock(&stop_cpus_mutex))
+ *   - kernel/stop_machine.c|976| <<stop_machine_from_inactive_cpu>> mutex_unlock(&stop_cpus_mutex);
+ */
 /* static data for stop_cpus */
 static DEFINE_MUTEX(stop_cpus_mutex);
+/*
+ * 在以下使用stop_cpus_in_progress:
+ *   - kernel/stop_machine.c|296| <<cpu_stop_queue_two_works>> if (unlikely(stop_cpus_in_progress)) {
+ *   - kernel/stop_machine.c|312| <<cpu_stop_queue_two_works>> while (stop_cpus_in_progress)
+ *   - kernel/stop_machine.c|405| <<queue_stop_cpus_work>> stop_cpus_in_progress = true;
+ *   - kernel/stop_machine.c|417| <<queue_stop_cpus_work>> stop_cpus_in_progress = false;
+ */
 static bool stop_cpus_in_progress;
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|144| <<stop_one_cpu>> cpu_stop_init_done(&done, 1);
+ *   - kernel/stop_machine.c|354| <<stop_two_cpus>> cpu_stop_init_done(&done, 2);
+ *   - kernel/stop_machine.c|428| <<__stop_cpus>> cpu_stop_init_done(&done, cpumask_weight(cpumask));
+ *   - kernel/stop_machine.c|695| <<stop_machine_from_inactive_cpu>> cpu_stop_init_done(&done, num_active_cpus());
+ */
 static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)
 {
 	memset(done, 0, sizeof(*done));
+	/*
+	 * 在以下使用cpu_stop_done->nr_todo:
+	 *   - kernel/stop_machine.c|73| <<cpu_stop_init_done>> atomic_set(&done->nr_todo, nr_todo);
+	 *   - kernel/stop_machine.c|80| <<cpu_stop_signal_done>> if (atomic_dec_and_test(&done->nr_todo))
+	 */
 	atomic_set(&done->nr_todo, nr_todo);
 	init_completion(&done->completion);
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|106| <<cpu_stop_queue_work>> cpu_stop_signal_done(work->done);
+ *   - kernel/stop_machine.c|515| <<cpu_stopper_thread>> cpu_stop_signal_done(done);
+ */
 /* signal completion unless @done is NULL */
 static void cpu_stop_signal_done(struct cpu_stop_done *done)
 {
+	/*
+	 * 在以下使用cpu_stop_done->nr_todo:
+	 *   - kernel/stop_machine.c|73| <<cpu_stop_init_done>> atomic_set(&done->nr_todo, nr_todo);
+	 *   - kernel/stop_machine.c|80| <<cpu_stop_signal_done>> if (atomic_dec_and_test(&done->nr_todo))
+	 */
 	if (atomic_dec_and_test(&done->nr_todo))
 		complete(&done->completion);
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|164| <<cpu_stop_queue_work>> __cpu_stop_queue_work(stopper, work, &wakeq);
+ *   - kernel/stop_machine.c|362| <<cpu_stop_queue_two_works>> __cpu_stop_queue_work(stopper1, work1, &wakeq);
+ *   - kernel/stop_machine.c|363| <<cpu_stop_queue_two_works>> __cpu_stop_queue_work(stopper2, work2, &wakeq);
+ */
 static void __cpu_stop_queue_work(struct cpu_stopper *stopper,
 					struct cpu_stop_work *work,
 					struct wake_q_head *wakeq)
 {
+	/*
+	 * struct cpu_stopper {
+	 *     struct task_struct      *thread;
+	 *
+	 *     raw_spinlock_t          lock;
+	 *     bool                    enabled;        // is this stopper enabled?
+	 *     struct list_head        works;          // list of pending works
+	 *
+	 *     struct cpu_stop_work    stop_work;      // for stop_cpus
+	 *     unsigned long           caller;
+	 *     cpu_stop_fn_t           fn;
+	 * };
+	 *
+	 * 把前面加到后面
+	 */
 	list_add_tail(&work->list, &stopper->works);
 	wake_q_add(wakeq, stopper->thread);
 }
 
+/*
+ * struct cpu_stop_work {
+ *     struct list_head        list;           // cpu_stopper->works
+ *     cpu_stop_fn_t           fn;
+ *     unsigned long           caller;
+ *     void                    *arg;
+ *     struct cpu_stop_done    *done;
+ * };
+ *
+ * called by:
+ *   - kernel/stop_machine.c|205| <<stop_one_cpu>> if (!cpu_stop_queue_work(cpu, &work))
+ *   - kernel/stop_machine.c|448| <<stop_one_cpu_nowait>> return cpu_stop_queue_work(cpu, work_buf);
+ *   - kernel/stop_machine.c|473| <<queue_stop_cpus_work>> if (cpu_stop_queue_work(cpu, work))
+ */
 /* queue @work to @stopper.  if offline, @work is completed immediately */
 static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
 {
@@ -100,6 +203,12 @@ static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
 	preempt_disable();
 	raw_spin_lock_irqsave(&stopper->lock, flags);
 	enabled = stopper->enabled;
+	/*
+	 * called by:
+	 *   - kernel/stop_machine.c|164| <<cpu_stop_queue_work>> __cpu_stop_queue_work(stopper, work, &wakeq);
+	 *   - kernel/stop_machine.c|362| <<cpu_stop_queue_two_works>> __cpu_stop_queue_work(stopper1, work1, &wakeq);
+	 *   - kernel/stop_machine.c|363| <<cpu_stop_queue_two_works>> __cpu_stop_queue_work(stopper2, work2, &wakeq);
+	 */
 	if (enabled)
 		__cpu_stop_queue_work(stopper, work, &wakeq);
 	else if (work->done)
@@ -136,12 +245,23 @@ static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
  * -ENOENT if @fn(@arg) was not executed because @cpu was offline;
  * otherwise, the return value of @fn.
  */
+/*
+ * caled by:
+ *   - kernel/sched/core.c|5324| <<sched_exec>> stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
+ *   - kernel/sched/core.c|9085| <<migrate_task_to>> return stop_one_cpu(curr_cpu, migration_cpu_stop, &arg);
+ */
 int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)
 {
 	struct cpu_stop_done done;
 	struct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };
 
 	cpu_stop_init_done(&done, 1);
+	/*
+	 * called by:
+	 *   - kernel/stop_machine.c|205| <<stop_one_cpu>> if (!cpu_stop_queue_work(cpu, &work))
+	 *   - kernel/stop_machine.c|448| <<stop_one_cpu_nowait>> return cpu_stop_queue_work(cpu, work_buf);
+	 *   - kernel/stop_machine.c|473| <<queue_stop_cpus_work>> if (cpu_stop_queue_work(cpu, work))
+	 */
 	if (!cpu_stop_queue_work(cpu, &work))
 		return -ENOENT;
 	/*
@@ -178,6 +298,14 @@ struct multi_stop_data {
 	atomic_t		thread_ack;
 };
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|254| <<ack_state>> set_state(msdata, msdata->state + 1);
+ *   - kernel/stop_machine.c|415| <<stop_two_cpus>> set_state(&msdata, MULTI_STOP_PREPARE);
+ *   - kernel/stop_machine.c|718| <<stop_machine_cpuslocked>> set_state(&msdata, MULTI_STOP_PREPARE);
+ *   - kernel/stop_machine.c|776| <<stop_core_cpuslocked>> set_state(&msdata, MULTI_STOP_PREPARE);
+ *   - kernel/stop_machine.c|825| <<stop_machine_from_inactive_cpu>> set_state(&msdata, MULTI_STOP_PREPARE);
+ */
 static void set_state(struct multi_stop_data *msdata,
 		      enum multi_stop_state newstate)
 {
@@ -187,6 +315,10 @@ static void set_state(struct multi_stop_data *msdata,
 	WRITE_ONCE(msdata->state, newstate);
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|305| <<multi_cpu_stop>> ack_state(msdata);
+ */
 /* Last one to ack a state moves to the next state. */
 static void ack_state(struct multi_stop_data *msdata)
 {
@@ -194,12 +326,24 @@ static void ack_state(struct multi_stop_data *msdata)
 		set_state(msdata, msdata->state + 1);
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|289| <<multi_cpu_stop>> stop_machine_yield(cpumask);
+ */
 notrace void __weak stop_machine_yield(const struct cpumask *cpumask)
 {
 	cpu_relax();
 }
 
 /* This is the cpu_stop function which stops the CPU. */
+/*
+ * 在以下使用multi_cpu_stop():
+ *   - kernel/stop_machine.c|408| <<stop_two_cpus>> .fn = multi_cpu_stop,
+ *   - kernel/stop_machine.c|719| <<stop_machine_cpuslocked>> return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
+ *   - kernel/stop_machine.c|777| <<stop_core_cpuslocked>> return stop_cpus(smt_mask, multi_cpu_stop, &msdata);
+ *   - kernel/stop_machine.c|827| <<stop_machine_from_inactive_cpu>> queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
+ *   - kernel/stop_machine.c|829| <<stop_machine_from_inactive_cpu>> ret = multi_cpu_stop(&msdata);
+ */
 static int multi_cpu_stop(void *data)
 {
 	struct multi_stop_data *msdata = data;
@@ -258,6 +402,10 @@ static int multi_cpu_stop(void *data)
 	return err;
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|419| <<stop_two_cpus>> if (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))
+ */
 static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,
 				    int cpu2, struct cpu_stop_work *work2)
 {
@@ -331,6 +479,10 @@ static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,
  *
  * returns when both are completed.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|3247| <<migrate_swap>> ret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);
+ */
 int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)
 {
 	struct cpu_stop_done done;
@@ -381,13 +533,36 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
  * true if cpu_stop_work was queued successfully and @fn will be called,
  * false otherwise.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|2470| <<migration_cpu_stop>> stop_one_cpu_nowait(task_cpu(p), migration_cpu_stop,
+ *   - kernel/sched/core.c|2726| <<affine_move_task>> stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+ *   - kernel/sched/core.c|2797| <<affine_move_task>> stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
+ *   - kernel/sched/core.c|9221| <<balance_push>> stop_one_cpu_nowait(rq->cpu, __balance_push_cpu_stop, push_task,
+ *   - kernel/sched/deadline.c|2462| <<pull_dl_task>> stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+ *   - kernel/sched/fair.c|10302| <<load_balance>> stop_one_cpu_nowait(cpu_of(busiest),
+ *   - kernel/sched/rt.c|2110| <<push_rt_task>> stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
+ *   - kernel/sched/rt.c|2449| <<pull_rt_task>> stop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,
+ *   - kernel/watchdog.c|449| <<watchdog_timer_fn>> stop_one_cpu_nowait(smp_processor_id(),
+ */
 bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,
 			struct cpu_stop_work *work_buf)
 {
 	*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };
+	/*
+	 * called by:
+	 *   - kernel/stop_machine.c|205| <<stop_one_cpu>> if (!cpu_stop_queue_work(cpu, &work))
+	 *   - kernel/stop_machine.c|448| <<stop_one_cpu_nowait>> return cpu_stop_queue_work(cpu, work_buf);
+	 *   - kernel/stop_machine.c|473| <<queue_stop_cpus_work>> if (cpu_stop_queue_work(cpu, work))
+	 */
 	return cpu_stop_queue_work(cpu, work_buf);
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|489| <<__stop_cpus>> if (!queue_stop_cpus_work(cpumask, fn, arg, &done))
+ *   - kernel/stop_machine.c|827| <<stop_machine_from_inactive_cpu>> queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata, &done);
+ */
 static bool queue_stop_cpus_work(const struct cpumask *cpumask,
 				 cpu_stop_fn_t fn, void *arg,
 				 struct cpu_stop_done *done)
@@ -410,6 +585,12 @@ static bool queue_stop_cpus_work(const struct cpumask *cpumask,
 		work->arg = arg;
 		work->done = done;
 		work->caller = _RET_IP_;
+		/*
+		 * called by:
+		 *   - kernel/stop_machine.c|205| <<stop_one_cpu>> if (!cpu_stop_queue_work(cpu, &work))
+		 *   - kernel/stop_machine.c|448| <<stop_one_cpu_nowait>> return cpu_stop_queue_work(cpu, work_buf);
+		 *   - kernel/stop_machine.c|473| <<queue_stop_cpus_work>> if (cpu_stop_queue_work(cpu, work))
+		 */
 		if (cpu_stop_queue_work(cpu, work))
 			queued = true;
 	}
@@ -420,6 +601,10 @@ static bool queue_stop_cpus_work(const struct cpumask *cpumask,
 	return queued;
 }
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|529| <<stop_cpus>> ret = __stop_cpus(cpumask, fn, arg);
+ */
 static int __stop_cpus(const struct cpumask *cpumask,
 		       cpu_stop_fn_t fn, void *arg)
 {
@@ -460,6 +645,11 @@ static int __stop_cpus(const struct cpumask *cpumask,
  * @cpumask were offline; otherwise, 0 if all executions of @fn
  * returned 0, any non zero return value if any returned non zero.
  */
+/*
+ * called by:
+ *   - kernel/stop_machine.c|719| <<stop_machine_cpuslocked>> return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
+ *   - kernel/stop_machine.c|777| <<stop_core_cpuslocked>> return stop_cpus(smt_mask, multi_cpu_stop, &msdata);
+ */
 static int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)
 {
 	int ret;
@@ -471,6 +661,9 @@ static int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)
 	return ret;
 }
 
+/*
+ * struct smp_hotplug_thread cpu_stop_threads.thread_should_run = cpu_stop_should_run()
+ */
 static int cpu_stop_should_run(unsigned int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
@@ -483,6 +676,9 @@ static int cpu_stop_should_run(unsigned int cpu)
 	return run;
 }
 
+/*
+ * struct smp_hotplug_thread cpu_stop_threads.thread_fn = cpu_stopper_thread()
+ */
 static void cpu_stopper_thread(unsigned int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
@@ -512,6 +708,11 @@ static void cpu_stopper_thread(unsigned int cpu)
 		if (done) {
 			if (ret)
 				done->ret = ret;
+			/*
+			 * called by:
+			 *   - kernel/stop_machine.c|106| <<cpu_stop_queue_work>> cpu_stop_signal_done(work->done);
+			 *   - kernel/stop_machine.c|515| <<cpu_stopper_thread>> cpu_stop_signal_done(done);
+			 */
 			cpu_stop_signal_done(done);
 		}
 		preempt_count_dec();
@@ -523,6 +724,10 @@ static void cpu_stopper_thread(unsigned int cpu)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|1099| <<take_cpu_down>> stop_machine_park(cpu);
+ */
 void stop_machine_park(int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
@@ -535,11 +740,17 @@ void stop_machine_park(int cpu)
 	kthread_park(stopper->thread);
 }
 
+/*
+ * struct smp_hotplug_thread cpu_stop_threads.create = cpu_stop_create()
+ */
 static void cpu_stop_create(unsigned int cpu)
 {
 	sched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));
 }
 
+/*
+ * struct smp_hotplug_thread cpu_stop_threads.park = cpu_stop_park()
+ */
 static void cpu_stop_park(unsigned int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
@@ -547,6 +758,11 @@ static void cpu_stop_park(unsigned int cpu)
 	WARN_ON(!list_empty(&stopper->works));
 }
 
+/*
+ * calleed by:
+ *   - kernel/cpu.c|1410| <<cpuhp_online_idle>> stop_machine_unpark(smp_processor_id());
+ *   - kernel/stop_machine.c|644| <<cpu_stop_init>> stop_machine_unpark(raw_smp_processor_id());
+ */
 void stop_machine_unpark(int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
@@ -555,6 +771,10 @@ void stop_machine_unpark(int cpu)
 	kthread_unpark(stopper->thread);
 }
 
+/*
+ * 在以下使用cpu_stop_threads:
+ *   - kernel/stop_machine.c|579| <<cpu_stop_init>> BUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));
+ */
 static struct smp_hotplug_thread cpu_stop_threads = {
 	.store			= &cpu_stopper.thread,
 	.thread_should_run	= cpu_stop_should_run,
@@ -583,6 +803,27 @@ static int __init cpu_stop_init(void)
 }
 early_initcall(cpu_stop_init);
 
+/*
+ * called by:
+ *   - arch/arm/kernel/patch.c|127| <<patch_text>> stop_machine_cpuslocked(patch_text_stop_machine, &patch, NULL);
+ *   - arch/arm/probes/kprobes/core.c|174| <<kprobes_remove_breakpoint>> stop_machine_cpuslocked(__kprobes_remove_breakpoint, &p,
+ *   - arch/arm64/kernel/patching.c|148| <<aarch64_insn_patch_text>> return stop_machine_cpuslocked(aarch64_insn_patch_text_cb, &patch,
+ *   - arch/csky/kernel/probes/kprobes.c|51| <<patch_text>> return stop_machine_cpuslocked(patch_text_cb, &param, cpu_online_mask);
+ *   - arch/parisc/kernel/patch.c|117| <<patch_text>> stop_machine_cpuslocked(patch_text_stop_machine, &patch, NULL);
+ *   - arch/parisc/kernel/patch.c|129| <<patch_text_multiple>> stop_machine_cpuslocked(patch_text_stop_machine, &patch, NULL);
+ *   - arch/powerpc/mm/book3s64/hash_pgtable.c|528| <<hash__change_memory_range>> stop_machine_cpuslocked(change_memory_range_fn, &chmem_parms,
+ *   - arch/powerpc/platforms/powernv/subcore.c|366| <<set_subcores_per_core>> stop_machine_cpuslocked(cpu_update_split_mode, &new_mode,
+ *   - arch/powerpc/platforms/pseries/lpar.c|1689| <<pseries_lpar_resize_hpt>> rc = stop_machine_cpuslocked(pseries_lpar_resize_hpt_commit,
+ *   - arch/riscv/kernel/patch.c|130| <<patch_text>> return stop_machine_cpuslocked(patch_text_cb, 
+ *   - arch/s390/kernel/kprobes.c|206| <<arch_arm_kprobe>> stop_machine_cpuslocked(swap_instruction, &args, NULL);
+ *   - arch/s390/kernel/kprobes.c|214| <<arch_disarm_kprobe>> stop_machine_cpuslocked(swap_instruction, &args, NULL);
+ *   - arch/s390/kernel/time.c|694| <<stp_work_fn>> stop_machine_cpuslocked(stp_sync_clock, &stp_sync, cpu_online_mask);
+ *   - arch/x86/kernel/cpu/microcode/core.c|502| <<microcode_reload_late>> ret = stop_machine_cpuslocked(__reload_late, NULL, cpu_online_mask);
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|248| <<set_mtrr_cpuslocked>> stop_machine_cpuslocked(mtrr_rendezvous_handler, &data, cpu_online_mask);
+ *   - arch/xtensa/kernel/jump_label.c|63| <<patch_text>> stop_machine_cpuslocked(patch_text_stop_machine,
+ *   - include/linux/stop_machine.h|161| <<stop_machine>> return stop_machine_cpuslocked(fn, data, cpus);
+ *   - kernel/stop_machine.c|628| <<stop_machine>> ret = stop_machine_cpuslocked(fn, data, cpus);
+ */
 int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 			    const struct cpumask *cpus)
 {
@@ -616,9 +857,40 @@ int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 
 	/* Set the initial state and stop all online cpus. */
 	set_state(&msdata, MULTI_STOP_PREPARE);
+	/*
+	 * 在以下使用multi_cpu_stop():
+	 *   - kernel/stop_machine.c|408| <<stop_two_cpus>> .fn = multi_cpu_stop,
+	 *   - kernel/stop_machine.c|719| <<stop_machine_cpuslocked>> return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
+	 *   - kernel/stop_machine.c|777| <<stop_core_cpuslocked>> return stop_cpus(smt_mask, multi_cpu_stop, &msdata);
+	 *   - kernel/stop_machine.c|827| <<stop_machine_from_inactive_cpu>> queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
+	 *   - kernel/stop_machine.c|829| <<stop_machine_from_inactive_cpu>> ret = multi_cpu_stop(&msdata);
+	 */
 	return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
 }
 
+/*
+ * called by:
+ *   - arch/arm/kernel/ftrace.c|58| <<arch_ftrace_update_code>> stop_machine(__ftrace_modify_code, &command, NULL);
+ *   - arch/arm/mm/init.c|449| <<fix_kernmem_perms>> stop_machine(__fix_kernmem_perms, NULL, NULL);
+ *   - arch/arm/mm/init.c|460| <<mark_rodata_ro>> stop_machine(__mark_rodata_ro, NULL, NULL);
+ *   - arch/arm64/kernel/alternative.c|229| <<apply_alternatives_all>> stop_machine(__apply_alternatives_multi_stop, NULL, cpu_online_mask);
+ *   - arch/arm64/kernel/cpufeature.c|2955| <<enable_cpu_capabilities>> stop_machine(cpu_enable_non_boot_scope_capabilities,
+ *   - arch/csky/kernel/ftrace.c|228| <<arch_ftrace_update_code>> stop_machine(__ftrace_modify_code, &param, cpu_online_mask);
+ *   - arch/powerpc/lib/feature-fixups.c|262| <<do_stf_barrier_fixups>> stop_machine(__do_stf_barrier_fixups, &types, NULL);
+ *   - arch/powerpc/lib/feature-fixups.c|435| <<do_entry_flush_fixups>> stop_machine(__do_entry_flush_fixups, &types, NULL);
+ *   - arch/powerpc/lib/feature-fixups.c|500| <<do_rfi_flush_fixups>> stop_machine(__do_rfi_flush_fixups, &types, NULL);
+ *   - arch/powerpc/platforms/pseries/mobility.c|691| <<pseries_suspend>> ret = stop_machine(do_join, &info, cpu_online_mask);
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|236| <<set_mtrr>> stop_machine(mtrr_rendezvous_handler, &data, cpu_online_mask);
+ *   - drivers/char/hw_random/intel-rng.c|372| <<intel_rng_mod_init>> err = stop_machine(intel_rng_hw_init, intel_rng_hw, NULL);
+ *   - drivers/edac/thunderx_edac.c|431| <<thunderx_lmc_inject_ecc_write>> stop_machine(inject_ecc_fn, lmc, NULL);
+ *   - drivers/gpu/drm/i915/gt/intel_ggtt.c|420| <<bxt_vtd_ggtt_insert_page__BKL>> stop_machine(bxt_vtd_ggtt_insert_page__cb, &arg, NULL);
+ *   - drivers/gpu/drm/i915/gt/intel_ggtt.c|447| <<bxt_vtd_ggtt_insert_entries__BKL>> stop_machine(bxt_vtd_ggtt_insert_entries__cb, &arg, NULL);
+ *   - drivers/xen/manage.c|136| <<do_suspend>> err = stop_machine(xen_suspend, &si, cpumask_of(0));
+ *   - include/linux/stop_machine.h|159| <<stop_machine>> stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)
+ *   - include/linux/stop_machine.h|168| <<stop_machine_from_inactive_cpu>> return stop_machine(fn, data, cpus);
+ *   - kernel/time/timekeeping.c|2329| <<timekeeping_notify>> stop_machine(change_clocksource, clock, NULL);
+ *   - kernel/trace/ftrace.c|2822| <<ftrace_run_stop_machine>> stop_machine(__ftrace_modify_code, &command, NULL);
+ */
 int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)
 {
 	int ret;
@@ -632,6 +904,10 @@ int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)
 EXPORT_SYMBOL_GPL(stop_machine);
 
 #ifdef CONFIG_SCHED_SMT
+/*
+ * called by:
+ *   - drivers/platform/x86/intel/ifs/runtest.c|193| <<ifs_test_core>> stop_core_cpuslocked(cpu, doscan, msrvals);
+ */
 int stop_core_cpuslocked(unsigned int cpu, cpu_stop_fn_t fn, void *data)
 {
 	const struct cpumask *smt_mask = cpu_smt_mask(cpu);
@@ -674,9 +950,34 @@ EXPORT_SYMBOL_GPL(stop_core_cpuslocked);
  * 0 if all executions of @fn returned 0, any non zero return value if any
  * returned non zero.
  */
+/*
+ * start_secondary()
+ * -> x86_cpuinit.early_percpu_clock_init = kvm_setup_secondary_clock()
+ *    -> pr_debug("kvm-clock: cpu %d, msr %llx, %s", smp_processor_id(), pa, txt);
+ * -> smp_callin()
+ *    -> smp_store_cpu_info()
+ *       -> identify_secondary_cpu()
+ *          -> mtrr_ap_init()
+ *             -> set_mtrr_from_inactive_cpu()
+ *
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|260| <<set_mtrr_from_inactive_cpu>> stop_machine_from_inactive_cpu(mtrr_rendezvous_handler, &data, cpu_callout_mask);
+ */
 int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,
 				  const struct cpumask *cpus)
 {
+	/*
+	 * struct multi_stop_data {
+	 *     cpu_stop_fn_t           fn;
+	 *     void                    *data;
+	 *     // Like num_online_cpus(), but hotplug cpu uses us, so we need this.
+	 *     unsigned int            num_threads;
+	 *     const struct cpumask    *active_cpus;
+	 *
+	 *     enum multi_stop_state   state;
+	 *     atomic_t                thread_ack;
+	 * };
+	 */
 	struct multi_stop_data msdata = { .fn = fn, .data = data,
 					    .active_cpus = cpus };
 	struct cpu_stop_done done;
@@ -686,6 +987,14 @@ int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,
 	BUG_ON(cpu_active(raw_smp_processor_id()));
 	msdata.num_threads = num_active_cpus() + 1;	/* +1 for local */
 
+	/*
+	 * 在以下使用stop_cpus_mutex:
+	 *   - kernel/stop_machine.c|98| <<global>> static DEFINE_MUTEX(stop_cpus_mutex);
+	 *   - kernel/stop_machine.c|650| <<stop_cpus>> mutex_lock(&stop_cpus_mutex);
+	 *   - kernel/stop_machine.c|652| <<stop_cpus>> mutex_unlock(&stop_cpus_mutex);
+	 *   - kernel/stop_machine.c|962| <<stop_machine_from_inactive_cpu>> while (!mutex_trylock(&stop_cpus_mutex))
+	 *   - kernel/stop_machine.c|976| <<stop_machine_from_inactive_cpu>> mutex_unlock(&stop_cpus_mutex);
+	 */
 	/* No proper task established and can't sleep - busy wait for lock. */
 	while (!mutex_trylock(&stop_cpus_mutex))
 		cpu_relax();
@@ -693,6 +1002,14 @@ int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,
 	/* Schedule work on other CPUs and execute directly for local CPU */
 	set_state(&msdata, MULTI_STOP_PREPARE);
 	cpu_stop_init_done(&done, num_active_cpus());
+	/*
+	 * 在以下使用multi_cpu_stop():
+	 *   - kernel/stop_machine.c|408| <<stop_two_cpus>> .fn = multi_cpu_stop,
+	 *   - kernel/stop_machine.c|719| <<stop_machine_cpuslocked>> return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
+	 *   - kernel/stop_machine.c|777| <<stop_core_cpuslocked>> return stop_cpus(smt_mask, multi_cpu_stop, &msdata);
+	 *   - kernel/stop_machine.c|827| <<stop_machine_from_inactive_cpu>> queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
+	 *   - kernel/stop_machine.c|829| <<stop_machine_from_inactive_cpu>> ret = multi_cpu_stop(&msdata);
+	 */
 	queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
 			     &done);
 	ret = multi_cpu_stop(&msdata);
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index cee5da1e5..70333b63a 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -20,6 +20,14 @@
 #include "tick-internal.h"
 #include "timekeeping_internal.h"
 
+/*
+ * https://zhuanlan.zhihu.com/p/285956997
+ *
+ * http://www.wowotech.net/timer_subsystem/clocksource.html
+ *
+ * https://blog.csdn.net/ganggexiongqi/article/details/7491468
+ */
+
 /**
  * clocks_calc_mult_shift - calculate mult/shift factors for scaled math of clocks
  * @mult:	pointer to mult variable
@@ -43,6 +51,46 @@
  * reduce the conversion accuracy by choosing smaller mult and shift
  * factors.
  */
+/*
+ * called by:
+ *   - arch/arm/lib/delay.c|70| <<register_current_timer_delay>> clocks_calc_mult_shift(&new_mult, &new_shift, timer->freq,
+ *   - arch/arm/mach-omap1/timer32k.c|235| <<omap_init_clocksource_32k>> clocks_calc_mult_shift(&persistent_mult, &persistent_shift,
+ *   - arch/x86/kernel/cpu/vmware.c|161| <<vmware_cyc2ns_setup>> clocks_calc_mult_shift(&d->cyc2ns_mul, &d->cyc2ns_shift,
+ *   - arch/x86/kernel/tsc.c|150| <<__set_cyc2ns_scale>> clocks_calc_mult_shift(&data.cyc2ns_mul, &data.cyc2ns_shift, khz,
+ *   - drivers/net/ethernet/ti/cpts.c|642| <<cpts_calc_mult_shift>> clocks_calc_mult_shift(&cpts->cc.mult, &cpts->cc.shift,
+ *   - include/linux/clockchips.h|195| <<clockevents_calc_mult_shift>> return clocks_calc_mult_shift(&ce->mult, &ce->shift, NSEC_PER_SEC, freq, maxsec);
+ *   - kernel/time/clocksource.c|1126| <<__clocksource_update_freq_scale>> clocks_calc_mult_shift(&cs->mult, &cs->shift, freq,
+ *   - kernel/time/sched_clock.c|169| <<u64>> clocks_calc_mult_shift(&new_mult, &new_shift, rate, NSEC_PER_SEC, 3600);
+ *   - sound/hda/hdac_stream.c|556| <<azx_timecounter_init>> clocks_calc_mult_shift(&cc->mult, &cc->shift, 24000000,
+ *
+ * ns = cycles / (freq / ns_per_sec)
+ *              ns = cycles * (ns_per_sec / freq)
+ *              ns = cycles * (10^9 / (cpu_khz * 10^3))
+ *              ns = cycles * (10^6 / cpu_khz)
+ *
+ *      Then we use scaling math (suggested by george@mvista.com) to get:
+ *              ns = cycles * (10^6 * SC / cpu_khz) / SC
+ *              ns = cycles * cyc2ns_scale / SC
+ *
+ * shift = SC
+ * mult  = (10^6 * SC / cpu_khz)
+ *
+ * 对于kvm-clock:
+ * from=1000000000, to=1000000000, maxsec=600, mult=8388608, shift=23
+ *
+ * 对于tsc:
+ * from=3392422, to=1000000, maxsec=600000, mult=4945498, shift=24
+ *
+ * sftacc保存了左移多少位才会造成最大cycle数(对应最大的时间范围值)的溢出,
+ * 对于32 bit以下的counter,统一设定为32个bit.而对于大于32 bit的counter,
+ * sftacc需要根据tmp值(这时候tmp保存了最大cycle数的高32 bit值)进行计算.
+ *
+ * clocksource: name=kvm-clock, from=1000000000, to=1000000000, maxsec=600
+ * clocksource: name=hpet,      from=100000000,  to=1000000000, maxsec=42
+ * clocksource: name=tsc-early, from=3392422,    to=1000000,    maxsec=600000
+ * clocksource: name=acpi_pm,   from=3579545,    to=1000000000, maxsec=4
+ * clocksource: name=tsc,       from=3392422,    to=1000000,    maxsec=600000
+ */
 void
 clocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)
 {
@@ -53,7 +101,19 @@ clocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)
 	 * Calculate the shift factor which is limiting the conversion
 	 * range:
 	 */
+	/*
+	 * freq是from, 一秒跑from次
+	 * maxsec秒就是maxsec*from次
+	 *
+	 * 这里tmp是ns
+	 */
 	tmp = ((u64)maxsec * from) >> 32;
+	/*
+	 * 来得到cycles最多可以左移多少位,能够保证不出现64位溢出
+	 * sftacc应该是高32-bit那些不能用的bit
+	 *
+	 * 也就是说, 最后的mult不能大于sftacc
+	 */
 	while (tmp) {
 		tmp >>=1;
 		sftacc--;
@@ -66,12 +126,37 @@ clocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)
 	for (sft = 32; sft > 0; sft--) {
 		tmp = (u64) to << sft;
 		tmp += from / 2;
+		/*
+		 * do_div() is NOT a C function. It wants to return
+		 * two values (the quotient and the remainder), but
+		 * since that doesn't work very well in C, what it
+		 * does is:
+		 *
+		 * - modifies the 64-bit dividend _in_place_
+		 * - returns the 32-bit remainder
+		 *
+		 * This ends up being the most efficient "calling
+		 * convention" on x86.
+		 */
 		do_div(tmp, from);
 		if ((tmp >> sftacc) == 0)
 			break;
 	}
+	/*
+	 * 得到的结果是tmp和sft
+	 */
 	*mult = tmp;
 	*shift = sft;
+
+	/*
+	 * 对于kvm-clock:
+	 * from=1000000000, to=1000000000, maxsec=600, mult=8388608, shift=23
+	 *
+	 * 对于tsc:
+	 * from=3392422, to=1000000, maxsec=600000, mult=4945498, shift=24
+	 *
+	 * clocksource_cyc2ns(): ((u64) cycles * mult) >> shift
+	 */
 }
 EXPORT_SYMBOL_GPL(clocks_calc_mult_shift);
 
@@ -119,6 +204,17 @@ static u64 suspend_start;
 static void clocksource_watchdog_work(struct work_struct *work);
 static void clocksource_select(void);
 
+/*
+ * 在以下使用watchdog_list:
+ *   - kernel/time/clocksource.c|122| <<global>> static LIST_HEAD(watchdog_list);
+ *   - kernel/time/clocksource.c|202| <<clocksource_mark_unstable>> list_add(&cs->wd_list, &watchdog_list);
+ *   - kernel/time/clocksource.c|402| <<clocksource_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list) {
+ *   - kernel/time/clocksource.c|524| <<clocksource_start_watchdog>> if (watchdog_running || !watchdog || list_empty(&watchdog_list))
+ *   - kernel/time/clocksource.c|534| <<clocksource_stop_watchdog>> if (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))
+ *   - kernel/time/clocksource.c|544| <<clocksource_reset_watchdog>> list_for_each_entry(cs, &watchdog_list, wd_list)
+ *   - kernel/time/clocksource.c|559| <<clocksource_enqueue_watchdog>> list_add(&cs->wd_list, &watchdog_list);
+ *   - kernel/time/clocksource.c|630| <<__clocksource_watchdog_kthread>> list_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {
+ */
 static LIST_HEAD(watchdog_list);
 static struct clocksource *watchdog;
 static struct timer_list watchdog_timer;
@@ -232,6 +328,15 @@ static enum wd_read_status cs_watchdog_read(struct clocksource *cs, u64 *csnow,
 		local_irq_enable();
 
 		wd_delta = clocksource_delta(wd_end, *wdnow, watchdog->mask);
+		/*
+		 * called by:
+		 *   - kernel/time/clocksource.c|258| <<cs_watchdog_read>> wd_delay = clocksource_cyc2ns(wd_delta, watchdog->mult,
+		 *   - kernel/time/clocksource.c|278| <<cs_watchdog_read>> wd_seq_delay = clocksource_cyc2ns(wd_delta, watchdog->mult, watchdog->shift);
+		 *   - kernel/time/clocksource.c|390| <<clocksource_verify_percpu>> cs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);
+		 *   - kernel/time/clocksource.c|457| <<clocksource_watchdog>> wd_nsec = clocksource_cyc2ns(delta, watchdog->mult,
+		 *   - kernel/time/clocksource.c|461| <<clocksource_watchdog>> cs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);
+		 *   - kernel/time/clocksource.c|940| <<clocks_calc_max_nsecs>> max_nsecs = clocksource_cyc2ns(max_cycles, mult - maxadj, shift);
+		 */
 		wd_delay = clocksource_cyc2ns(wd_delta, watchdog->mult,
 					      watchdog->shift);
 		if (wd_delay <= WATCHDOG_MAX_SKEW) {
@@ -384,6 +489,10 @@ void clocksource_verify_percpu(struct clocksource *cs)
 }
 EXPORT_SYMBOL_GPL(clocksource_verify_percpu);
 
+/*
+ * 在以下使用clocksource_watchdog:
+ *   - kernel/time/clocksource.c|526| <<clocksource_start_watchdog>> timer_setup(&watchdog_timer, clocksource_watchdog, 0);
+ */
 static void clocksource_watchdog(struct timer_list *unused)
 {
 	u64 csnow, wdnow, cslast, wdlast, delta;
@@ -550,6 +659,10 @@ static void clocksource_resume_watchdog(void)
 	atomic_inc(&watchdog_reset_pending);
 }
 
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|1179| <<__clocksource_register_scale>> clocksource_enqueue_watchdog(cs);
+ */
 static void clocksource_enqueue_watchdog(struct clocksource *cs)
 {
 	INIT_LIST_HEAD(&cs->wd_list);
@@ -565,6 +678,12 @@ static void clocksource_enqueue_watchdog(struct clocksource *cs)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|1202| <<__clocksource_register_scale>> clocksource_select_watchdog(false);
+ *   - kernel/time/clocksource.c|1231| <<clocksource_change_rating>> clocksource_select_watchdog(false);
+ *   - kernel/time/clocksource.c|1246| <<clocksource_unbind>> clocksource_select_watchdog(true);
+ */
 static void clocksource_select_watchdog(bool fallback)
 {
 	struct clocksource *cs, *old_wd;
@@ -882,6 +1001,11 @@ static u32 clocksource_max_adjustment(struct clocksource *cs)
  * delayed timers or bad hardware, which might result in time intervals that
  * are larger than what the math used can handle without overflows.
  */
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|972| <<clocksource_update_max_deferment>> cs->max_idle_ns = clocks_calc_max_nsecs(cs->mult, cs->shift,
+ *   - kernel/time/sched_clock.c|175| <<sched_clock_register>> wrap = clocks_calc_max_nsecs(new_mult, new_shift, 0, new_mask, NULL);
+ */
 u64 clocks_calc_max_nsecs(u32 mult, u32 shift, u32 maxadj, u64 mask, u64 *max_cyc)
 {
 	u64 max_nsecs, max_cycles;
@@ -917,6 +1041,10 @@ u64 clocks_calc_max_nsecs(u32 mult, u32 shift, u32 maxadj, u64 mask, u64 *max_cy
  * @cs:         Pointer to clocksource to be updated
  *
  */
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|1208| <<__clocksource_update_freq_scale>> clocksource_update_max_deferment(cs);
+ */
 static inline void clocksource_update_max_deferment(struct clocksource *cs)
 {
 	cs->max_idle_ns = clocks_calc_max_nsecs(cs->mult, cs->shift,
@@ -1066,6 +1194,22 @@ static void clocksource_enqueue(struct clocksource *cs)
  * __clocksource_update_freq_hz() or __clocksource_update_freq_khz() helper
  * functions.
  */
+/*
+ * called by:
+ *   - include/linux/clocksource.h|256| <<__clocksource_update_freq_hz>> __clocksource_update_freq_scale(cs, 1, hz);
+ *   - include/linux/clocksource.h|261| <<__clocksource_update_freq_khz>> __clocksource_update_freq_scale(cs, 1000, khz);
+ *   - kernel/time/clocksource.c|1191| <<__clocksource_register_scale>> __clocksource_update_freq_scale(cs, scale, freq);
+ *
+ * clocksource: name=kvm-clock,       scale=1,    freq=1000000000
+ * clocksource: name=refined-jiffies, scale=1,    freq=0
+ * clocksource: name=hpet,            scale=1,    freq=100000000
+ * clocksource: name=tsc-early,       scale=1000, freq=3392422
+ * clocksource: name=jiffies,         scale=1,    freq=0
+ * clocksource: name=acpi_pm,         scale=1,    freq=3579545
+ * clocksource: name=tsc,             scale=1000, freq=3392422
+ *
+ * kvm-clock: scale=1, freq=NSEC_PER_SEC
+ */
 void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq)
 {
 	u64 sec;
@@ -1085,6 +1229,10 @@ void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq
 		 * ~ 0.06ppm granularity for NTP.
 		 */
 		sec = cs->mask;
+		/*
+		 * - modifies the 64-bit dividend _in_place_
+		 * - returns the 32-bit remainder
+		 */
 		do_div(sec, freq);
 		do_div(sec, scale);
 		if (!sec)
@@ -1092,6 +1240,34 @@ void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq
 		else if (sec > 600 && cs->mask > UINT_MAX)
 			sec = 600;
 
+		/*
+		 * called by:
+		 *   - arch/arm/lib/delay.c|70| <<register_current_timer_delay>> clocks_calc_mult_shift(&new_mult, &new_shift, timer->freq,
+		 *   - arch/arm/mach-omap1/timer32k.c|235| <<omap_init_clocksource_32k>> clocks_calc_mult_shift(&persistent_mult, &persistent_shift,
+		 *   - arch/x86/kernel/cpu/vmware.c|161| <<vmware_cyc2ns_setup>> clocks_calc_mult_shift(&d->cyc2ns_mul, &d->cyc2ns_shift,
+		 *   - arch/x86/kernel/tsc.c|150| <<__set_cyc2ns_scale>> clocks_calc_mult_shift(&data.cyc2ns_mul, &data.cyc2ns_shift, khz,
+		 *   - drivers/net/ethernet/ti/cpts.c|642| <<cpts_calc_mult_shift>> clocks_calc_mult_shift(&cpts->cc.mult, &cpts->cc.shift,
+		 *   - include/linux/clockchips.h|195| <<clockevents_calc_mult_shift>> return clocks_calc_mult_shift(&ce->mult, &ce->shift, NSEC_PER_SEC, freq, maxsec);
+		 *   - kernel/time/clocksource.c|1126| <<__clocksource_update_freq_scale>> clocks_calc_mult_shift(&cs->mult, &cs->shift, freq,
+		 *   - kernel/time/sched_clock.c|169| <<u64>> clocks_calc_mult_shift(&new_mult, &new_shift, rate, NSEC_PER_SEC, 3600);
+		 *   - sound/hda/hdac_stream.c|556| <<azx_timecounter_init>> clocks_calc_mult_shift(&cc->mult, &cc->shift, 24000000,
+		 *
+		 * clocksource: name=kvm-clock,       scale=1,    freq=1000000000
+		 * clocksource: name=refined-jiffies, scale=1,    freq=0
+		 * clocksource: name=hpet,            scale=1,    freq=100000000
+		 * clocksource: name=tsc-early,       scale=1000, freq=3392422
+		 * clocksource: name=jiffies,         scale=1,    freq=0
+		 * clocksource: name=acpi_pm,         scale=1,    freq=3579545
+		 * clocksource: name=tsc,             scale=1000, freq=3392422
+		 *
+		 * clocksource: name=kvm-clock, from=1000000000, to=1000000000, maxsec=600
+		 * clocksource: name=hpet,      from=100000000,  to=1000000000, maxsec=42
+		 * clocksource: name=tsc-early, from=3392422,    to=1000000,    maxsec=600000
+		 * clocksource: name=acpi_pm,   from=3579545,    to=1000000000, maxsec=4
+		 * clocksource: name=tsc,       from=3392422,    to=1000000,    maxsec=600000
+		 *
+		 * kvm-clock: scale=1, freq=NSEC_PER_SEC
+		 */
 		clocks_calc_mult_shift(&cs->mult, &cs->shift, freq,
 				       NSEC_PER_SEC / scale, sec * scale);
 	}
@@ -1153,6 +1329,28 @@ EXPORT_SYMBOL_GPL(__clocksource_update_freq_scale);
  * This *SHOULD NOT* be called directly! Please use the
  * clocksource_register_hz() or clocksource_register_khz helper functions.
  */
+/*
+ * 一个例子:
+ * [0] __clocksource_register_scale
+ * [0] init_tsc_clocksource
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * clocksource: name=kvm-clock,       scale=1,    freq=1000000000
+ * clocksource: name=refined-jiffies, scale=1,    freq=0
+ * clocksource: name=hpet,            scale=1,    freq=100000000
+ * clocksource: name=tsc-early,       scale=1000, freq=3392422
+ * clocksource: name=jiffies,         scale=1,    freq=0
+ * clocksource: name=acpi_pm,         scale=1,    freq=3579545
+ * clocksource: name=tsc,             scale=1000, freq=3392422
+ *
+ * called by:
+ *   - include/linux/clocksource.h|241| <<__clocksource_register>> return __clocksource_register_scale(cs, 1, 0);
+ *   - include/linux/clocksource.h|246| <<clocksource_register_hz>> return __clocksource_register_scale(cs, 1, hz);
+ *   - include/linux/clocksource.h|251| <<clocksource_register_khz>> return __clocksource_register_scale(cs, 1000, khz);
+ */
 int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
 {
 	unsigned long flags;
@@ -1168,7 +1366,23 @@ int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
 		cs->vdso_clock_mode = VDSO_CLOCKMODE_NONE;
 	}
 
+	/*
+	 * called by:
+	 *   - include/linux/clocksource.h|256| <<__clocksource_update_freq_hz>> __clocksource_update_freq_scale(cs, 1, hz);
+	 *   - include/linux/clocksource.h|261| <<__clocksource_update_freq_khz>> __clocksource_update_freq_scale(cs, 1000, khz);
+	 *   - kernel/time/clocksource.c|1191| <<__clocksource_register_scale>> __clocksource_update_freq_scale(cs, scale, freq);
+	 */
 	/* Initialize mult/shift and max_idle_ns */
+	/* clocksource: name=kvm-clock,       scale=1,    freq=1000000000
+	 * clocksource: name=refined-jiffies, scale=1,    freq=0
+	 * clocksource: name=hpet,            scale=1,    freq=100000000
+	 * clocksource: name=tsc-early,       scale=1000, freq=3392422
+	 * clocksource: name=jiffies,         scale=1,    freq=0
+	 * clocksource: name=acpi_pm,         scale=1,    freq=3579545
+	 * clocksource: name=tsc,             scale=1000, freq=3392422
+	 *
+	 * kvm-clock: scale=1, freq=NSEC_PER_SEC
+	 */
 	__clocksource_update_freq_scale(cs, scale, freq);
 
 	/* Add clocksource to the clocksource list */
@@ -1180,6 +1394,12 @@ int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
 	clocksource_watchdog_unlock(&flags);
 
 	clocksource_select();
+	/*
+	 * called by:
+	 *   - kernel/time/clocksource.c|1202| <<__clocksource_register_scale>> clocksource_select_watchdog(false);
+	 *   - kernel/time/clocksource.c|1231| <<clocksource_change_rating>> clocksource_select_watchdog(false);
+	 *   - kernel/time/clocksource.c|1246| <<clocksource_unbind>> clocksource_select_watchdog(true);
+	 */
 	clocksource_select_watchdog(false);
 	__clocksource_suspend_select(cs);
 	mutex_unlock(&clocksource_mutex);
diff --git a/kernel/time/ntp.c b/kernel/time/ntp.c
index 406dccb79..7f6aa420b 100644
--- a/kernel/time/ntp.c
+++ b/kernel/time/ntp.c
@@ -30,12 +30,31 @@
  */
 
 
+/*
+ * 在以下使用tick_usec:
+ *   - kernel/time/ntp.c|275| <<ntp_update_frequency>> second_length = (u64)(tick_usec * NSEC_PER_USEC * USER_HZ)
+ *   - kernel/time/ntp.c|768| <<process_adjtimex_modes>> tick_usec = txc->tick;
+ *   - kernel/time/ntp.c|803| <<__do_adjtimex>> audit_ntp_set_old(ad, AUDIT_NTP_TICK, tick_usec);
+ *   - kernel/time/ntp.c|811| <<__do_adjtimex>> audit_ntp_set_new(ad, AUDIT_NTP_TICK, tick_usec);
+ *   - kernel/time/ntp.c|833| <<__do_adjtimex>> txc->tick = tick_usec;
+ */
 /* USER_HZ period (usecs): */
 unsigned long			tick_usec = USER_TICK_USEC;
 
 /* SHIFTED_HZ period (nsecs): */
 unsigned long			tick_nsec;
 
+/*
+ * 在以下使用tick_length:
+ *   - kernel/time/ntp.c|277| <<ntp_update_frequency>> tick_length += new_base - tick_length_base;
+ *   - kernel/time/ntp.c|360| <<ntp_clear>> tick_length = tick_length_base;
+ *   - kernel/time/ntp.c|371| <<ntp_tick_length>> return tick_length;
+ *   - kernel/time/ntp.c|465| <<second_overflow>> tick_length = tick_length_base;
+ *   - kernel/time/ntp.c|469| <<second_overflow>> tick_length += delta;
+ *   - kernel/time/ntp.c|479| <<second_overflow>> tick_length += MAX_TICKADJ_SCALED;
+ *   - kernel/time/ntp.c|485| <<second_overflow>> tick_length -= MAX_TICKADJ_SCALED;
+ *   - kernel/time/ntp.c|489| <<second_overflow>> tick_length += (s64)(time_adjust * NSEC_PER_USEC / NTP_INTERVAL_FREQ)
+ */
 static u64			tick_length;
 static u64			tick_length_base;
 
@@ -256,6 +275,13 @@ static inline int ntp_synced(void)
  * Update (tick_length, tick_length_base, tick_nsec), based
  * on (tick_usec, ntp_tick_adj, time_freq):
  */
+/*
+ * called by:
+ *   - kernel/time/ntp.c|377| <<ntp_clear>> ntp_update_frequency();
+ *   - kernel/time/ntp.c|779| <<process_adjtimex_modes>> ntp_update_frequency();
+ *   - kernel/time/ntp.c|802| <<__do_adjtimex>> ntp_update_frequency();
+ *   - kernel/time/ntp.c|1010| <<hardpps_update_freq>> ntp_update_frequency();
+ */
 static void ntp_update_frequency(void)
 {
 	u64 second_length;
@@ -366,8 +392,25 @@ void ntp_clear(void)
 }
 
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2569| <<timekeeping_adjust>> if (likely(tk->ntp_tick == ntp_tick_length())) {
+ *   - kernel/time/timekeeping.c|2572| <<timekeeping_adjust>> tk->ntp_tick = ntp_tick_length();
+ *   - kernel/time/timekeeping.c|2869| <<timekeeping_advance>> maxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;
+ */
 u64 ntp_tick_length(void)
 {
+	/*
+	 * 在以下使用tick_length:
+	 *   - kernel/time/ntp.c|277| <<ntp_update_frequency>> tick_length += new_base - tick_length_base;
+	 *   - kernel/time/ntp.c|360| <<ntp_clear>> tick_length = tick_length_base;
+	 *   - kernel/time/ntp.c|371| <<ntp_tick_length>> return tick_length;
+	 *   - kernel/time/ntp.c|465| <<second_overflow>> tick_length = tick_length_base;
+	 *   - kernel/time/ntp.c|469| <<second_overflow>> tick_length += delta;
+	 *   - kernel/time/ntp.c|479| <<second_overflow>> tick_length += MAX_TICKADJ_SCALED;
+	 *   - kernel/time/ntp.c|485| <<second_overflow>> tick_length -= MAX_TICKADJ_SCALED;
+	 *   - kernel/time/ntp.c|489| <<second_overflow>> tick_length += (s64)(time_adjust * NSEC_PER_USEC / NTP_INTERVAL_FREQ)
+	 */
 	return tick_length;
 }
 
@@ -397,6 +440,10 @@ ktime_t ntp_get_next_leap(void)
  *
  * Also handles leap second processing, and returns leap offset
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2861| <<accumulate_nsecs_to_secs>> leap = second_overflow(tk->xtime_sec);
+ */
 int second_overflow(time64_t secs)
 {
 	s64 delta;
@@ -759,6 +806,10 @@ static inline void process_adjtimex_modes(const struct __kernel_timex *txc,
  * adjtimex mainly allows reading (and writing, if superuser) of
  * kernel time-keeping variables. used by xntpd.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|3337| <<do_adjtimex>> ret = __do_adjtimex(txc, &ts, &tai, &ad);
+ */
 int __do_adjtimex(struct __kernel_timex *txc, const struct timespec64 *ts,
 		  s32 *time_tai, struct audit_ntp_data *ad)
 {
diff --git a/kernel/time/posix-stubs.c b/kernel/time/posix-stubs.c
index 90ea5f373..1485e30a7 100644
--- a/kernel/time/posix-stubs.c
+++ b/kernel/time/posix-stubs.c
@@ -70,6 +70,11 @@ SYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock,
 	return do_sys_settimeofday64(&new_tp, NULL);
 }
 
+/*
+ * called by:
+ *   - kernel/time/posix-stubs.c|100| <<SYSCALL_DEFINE2(clock_gettime)>> ret = do_clock_gettime(which_clock, &kernel_tp);
+ *   - kernel/time/posix-stubs.c|192| <<SYSCALL_DEFINE2(clock_gettime32)>> ret = do_clock_gettime(which_clock, &kernel_tp);
+ */
 static int do_clock_gettime(clockid_t which_clock, struct timespec64 *tp)
 {
 	switch (which_clock) {
diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 46789356f..07e304da0 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -47,6 +47,15 @@ ktime_t tick_next_period;
  *    at it will take over and keep the time keeping alive.  The handover
  *    procedure also covers cpu hotplug.
  */
+/*
+ * 在以下设置tick_do_timer_cpu:
+ *   - kernel/time/tick-common.c|201| <<giveup_do_timer>> tick_do_timer_cpu = cpu;
+ *   - kernel/time/tick-common.c|233| <<tick_setup_device>> tick_do_timer_cpu = cpu;
+ *   - kernel/time/tick-common.c|423| <<tick_handover_do_timer>> tick_do_timer_cpu = cpumask_first(cpu_online_mask);
+ *   - kernel/time/tick-sched.c|193| <<tick_sched_do_timer>> tick_do_timer_cpu = cpu;
+ *   - kernel/time/tick-sched.c|887| <<tick_nohz_stop_tick>> tick_do_timer_cpu = TICK_DO_TIMER_NONE;
+ *   - kernel/time/tick-sched.c|1053| <<can_stop_idle_tick>> tick_do_timer_cpu = TICK_DO_TIMER_NONE;
+ */
 int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
 #ifdef CONFIG_NO_HZ_FULL
 /*
@@ -82,8 +91,22 @@ int tick_is_oneshot_available(void)
 /*
  * Periodic tick
  */
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|112| <<tick_handle_periodic>> tick_periodic(cpu);
+ *   - kernel/time/tick-common.c|145| <<tick_handle_periodic>> tick_periodic(cpu);
+ */
 static void tick_periodic(int cpu)
 {
+	/*
+	 * 在以下设置tick_do_timer_cpu:
+	 *   - kernel/time/tick-common.c|201| <<giveup_do_timer>> tick_do_timer_cpu = cpu;
+	 *   - kernel/time/tick-common.c|233| <<tick_setup_device>> tick_do_timer_cpu = cpu;
+	 *   - kernel/time/tick-common.c|423| <<tick_handover_do_timer>> tick_do_timer_cpu = cpumask_first(cpu_online_mask);
+	 *   - kernel/time/tick-sched.c|193| <<tick_sched_do_timer>> tick_do_timer_cpu = cpu;
+	 *   - kernel/time/tick-sched.c|887| <<tick_nohz_stop_tick>> tick_do_timer_cpu = TICK_DO_TIMER_NONE;
+	 *   - kernel/time/tick-sched.c|1053| <<can_stop_idle_tick>> tick_do_timer_cpu = TICK_DO_TIMER_NONE;
+	 */
 	if (tick_do_timer_cpu == cpu) {
 		raw_spin_lock(&jiffies_lock);
 		write_seqcount_begin(&jiffies_seq);
@@ -104,6 +127,13 @@ static void tick_periodic(int cpu)
 /*
  * Event handler for periodic ticks
  */
+/*
+ * 在以下使用tick_handle_periodic():
+ *   - kernel/time/tick-broadcast.c|261| <<tick_device_uses_broadcast>> dev->event_handler = tick_handle_periodic;
+ *   - kernel/time/tick-broadcast.c|517| <<tick_set_periodic_handler>> dev->event_handler = tick_handle_periodic;
+ *   - kernel/time/tick-common.c|125| <<tick_handle_periodic>> if (dev->event_handler != tick_handle_periodic)
+ *   - kernel/time/tick-internal.h|89| <<tick_set_periodic_handler>> dev->event_handler = tick_handle_periodic;
+ */
 void tick_handle_periodic(struct clock_event_device *dev)
 {
 	int cpu = smp_processor_id();
@@ -521,6 +551,10 @@ static unsigned int tick_freeze_depth;
  * Call with interrupts disabled.  Must be balanced with %tick_unfreeze().
  * Interrupts must not be enabled before the subsequent %tick_unfreeze().
  */
+/*
+ * called by:
+ *   - drivers/cpuidle/cpuidle.c|147| <<enter_s2idle_proper>> tick_freeze();
+ */
 void tick_freeze(void)
 {
 	raw_spin_lock(&tick_freeze_lock);
diff --git a/kernel/time/tick-legacy.c b/kernel/time/tick-legacy.c
index af225b32f..5c0c4ed1a 100644
--- a/kernel/time/tick-legacy.c
+++ b/kernel/time/tick-legacy.c
@@ -22,6 +22,26 @@
  *
  * Must be called with interrupts disabled.
  */
+/*
+ * called by:
+ *   - arch/arm/mach-rpc/time.c|84| <<ioc_timer_interrupt>> legacy_timer_tick(1);
+ *   - arch/ia64/kernel/time.c|190| <<timer_interrupt>> legacy_timer_tick(smp_processor_id() == time_keeper_id);
+ *   - arch/m68k/68000/timers.c|64| <<hw_tick>> legacy_timer_tick(1);
+ *   - arch/m68k/amiga/config.c|473| <<ciab_timer_handler>> legacy_timer_tick(1);
+ *   - arch/m68k/apollo/config.c|173| <<dn_timer_int>> legacy_timer_tick(1);
+ *   - arch/m68k/atari/time.c|51| <<mfp_timer_c_handler>> legacy_timer_tick(1);
+ *   - arch/m68k/bvme6000/config.c|176| <<bvme6000_timer_int>> legacy_timer_tick(1);
+ *   - arch/m68k/coldfire/sltimers.c|91| <<mcfslt_tick>> legacy_timer_tick(1);
+ *   - arch/m68k/coldfire/timers.c|78| <<mcftmr_tick>> legacy_timer_tick(1);
+ *   - arch/m68k/hp300/time.c|66| <<hp300_tick>> legacy_timer_tick(1);
+ *   - arch/m68k/mac/via.c|588| <<via_timer_handler>> legacy_timer_tick(1);
+ *   - arch/m68k/mvme147/config.c|122| <<mvme147_timer_int>> legacy_timer_tick(1);
+ *   - arch/m68k/mvme16x/config.c|381| <<mvme16x_timer_int>> legacy_timer_tick(1);
+ *   - arch/m68k/q40/q40ints.c|145| <<q40_timer_int>> legacy_timer_tick(1);
+ *   - arch/m68k/sun3/sun3ints.c|75| <<sun3_int5>> legacy_timer_tick(1);
+ *   - arch/m68k/sun3x/time.c|86| <<sun3x_timer_tick>> legacy_timer_tick(1);
+ *   - arch/parisc/kernel/time.c|91| <<timer_interrupt>> legacy_timer_tick(ticks_elapsed);
+ */
 void legacy_timer_tick(unsigned long ticks)
 {
 	if (ticks) {
diff --git a/kernel/time/tick-oneshot.c b/kernel/time/tick-oneshot.c
index 475ecceda..a66803de7 100644
--- a/kernel/time/tick-oneshot.c
+++ b/kernel/time/tick-oneshot.c
@@ -20,6 +20,19 @@
 /**
  * tick_program_event
  */
+/*
+ * called by:
+ *   - kernel/time/hrtimer.c|681| <<__hrtimer_reprogram>> tick_program_event(expires_next, 1);
+ *   - kernel/time/hrtimer.c|1824| <<hrtimer_interrupt>> if (!tick_program_event(expires_next, 0)) {
+ *   - kernel/time/hrtimer.c|1868| <<hrtimer_interrupt>> tick_program_event(expires_next, 1);
+ *   - kernel/time/tick-broadcast.c|921| <<___tick_broadcast_oneshot_control>> tick_program_event(dev->next_event, 1);
+ *   - kernel/time/tick-sched.c|781| <<tick_nohz_restart>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ *   - kernel/time/tick-sched.c|942| <<tick_nohz_stop_tick>> tick_program_event(KTIME_MAX, 1);
+ *   - kernel/time/tick-sched.c|951| <<tick_nohz_stop_tick>> tick_program_event(tick, 1);
+ *   - kernel/time/tick-sched.c|1385| <<tick_nohz_handler>> tick_program_event(KTIME_MAX, 1);
+ *   - kernel/time/tick-sched.c|1390| <<tick_nohz_handler>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ *   - kernel/time/tick-sched.c|1427| <<tick_nohz_switch_to_nohz>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ */
 int tick_program_event(ktime_t expires, int force)
 {
 	struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index b0e3c9205..f14f63e95 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -54,6 +54,13 @@ static ktime_t last_jiffies_update;
 /*
  * Must be called with interrupts disabled !
  */
+/*
+ * called by:
+ *   - kernel/time/tick-sched.c|199| <<tick_sched_do_timer>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|210| <<tick_sched_do_timer>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|634| <<tick_nohz_update_jiffies>> tick_do_update_jiffies64(now);
+ *   - kernel/time/tick-sched.c|962| <<tick_nohz_restart_sched_tick>> tick_do_update_jiffies64(now);
+ */
 static void tick_do_update_jiffies64(ktime_t now)
 {
 	unsigned long ticks = 1;
@@ -624,6 +631,10 @@ bool tick_nohz_tick_stopped_cpu(int cpu)
  * value. We do this unconditionally on any CPU, as we don't know whether the
  * CPU, which has the update task assigned is in a long sleep.
  */
+/*
+ * called by:
+ *   - kernel/time/tick-sched.c|1438| <<tick_nohz_irq_enter>> tick_nohz_update_jiffies(now);
+ */
 static void tick_nohz_update_jiffies(ktime_t now)
 {
 	unsigned long flags;
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index f72b9f1de..c3c589087 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -29,10 +29,142 @@
 #include "ntp_internal.h"
 #include "timekeeping_internal.h"
 
+/*
+ * 一个计算的例子.
+ *
+ * struct timekeeper *tk_local = &tk_core.timekeeper;
+ * static u64 old_time = 0;
+ * u64 nsec, time;
+ * u64 tsc;
+ *
+ * write_seqcount_begin(&tk_core.seq);
+ *
+ * tsc = (u64)rdtsc_ordered();
+ * nsec = timekeeping_cycles_to_ns(&tk_local->tkr_raw, tsc);
+ * time = ktime_add_ns(tk_local->tkr_raw.base, nsec);
+ *
+ * write_seqcount_end(&tk_core.seq);
+ *
+ *
+ * 1. 获得tsc.
+ * 2. 计算tsc和cycle_last
+ * 3. nsec = delta * tkr->mult + tkr->xtime_nsec;
+ * 4. nsec >>= tkr->shift;
+ * 5. 把nsec加到tk_local->tkr_raw.base返回
+ */
+
+/*
+ * 在以下修改tk_read_base->cycle_last (非mono!!!):
+ *   - kernel/time/timekeeping.c|331| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+ *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+ *   - kernel/time/timekeeping.c|1865| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now
+ *   - kernel/time/timekeeping.c|2171| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+ *
+ * 在以下设置tk_read_base->xtime_nsec (非mono!!!):
+ *   - kernel/time/timekeeping.c|129| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+ *   - kernel/time/timekeeping.c|384| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+ *   - kernel/time/timekeeping.c|387| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+ *   - kernel/time/timekeeping.c|902| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+ *   - kernel/time/timekeeping.c|2279| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+ *   - kernel/time/timekeeping.c|2282| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+ *
+ * 在以下修改tk->tkr_raw.base:
+ *   - kernel/time/timekeeping.c|1150| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+ *
+ * 在以下修改timekeeper->raw_sec:
+ *   - kernel/time/timekeeping.c|225| <<tk_normalize_xtime>> tk->raw_sec++;
+ *   - kernel/time/timekeeping.c|2089| <<timekeeping_init>> tk->raw_sec = 0;
+ *   - kernel/time/timekeeping.c|2583| <<logarithmic_accumulation>> tk->raw_sec++;
+ *
+ *
+ * 1. tk_normalize_xtime()在简单的测试中只在change clocksource的时候调用
+ *
+ * 2. timekeeping_forward_now()在简单的测试中只在change clocksource的时候调用
+ */
+
+/*
+ * 在以下设置tk->tkr_raw.mult:
+ *   - kernel/time/timekeeping.c|711| <<tk_setup_internals>> tk->tkr_raw.mult = clock->mult;
+ * 在以下使用tk->tkr_raw.mult:
+ *   - arch/x86/kvm/x86.c|2402| <<update_pvclock_gtod>> vdata->raw_clock.mult = tk->tkr_raw.mult;
+ *   - kernel/time/timekeeping.c|1303| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+ *   - kernel/time/timekeeping.c|1745| <<adjust_historical_crosststamp>> (corr_raw, tk->tkr_mono.mult, tk->tkr_raw.mult);
+ *   - kernel/time/vsyscall.c|30| <<update_vdso_data>> vdata[CS_RAW].mult = tk->tkr_raw.mult;
+ *
+ * 在以下设置tk->tkr_raw.shift:
+ *   - kernel/time/timekeeping.c|699| <<tk_setup_internals>> tk->tkr_raw.shift = clock->shift;
+ * 在以下使用tk->tkr_raw.shift:
+ *   - arch/x86/kvm/x86.c|2403| <<update_pvclock_gtod>> vdata->raw_clock.shift = tk->tkr_raw.shift;
+ *   - kernel/time/timekeeping.c|349| <<tk_normalize_xtime>> while (tk->tkr_raw.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_raw.shift)) {
+ *   - kernel/time/timekeeping.c|350| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+ *   - kernel/time/timekeeping.c|2910| <<logarithmic_accumulation>> snsec_per_sec = (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+ *   - kernel/time/vsyscall.c|31| <<update_vdso_data>> vdata[CS_RAW].shift = tk->tkr_raw.shift;
+ *
+ * 在以下设置tk->tkr_mono.mult:
+ *   - kernel/time/timekeeping.c|710| <<tk_setup_internals>> tk->tkr_mono.mult = clock->mult;
+ *   - kernel/time/timekeeping.c|2659| <<timekeeping_apply_adjustment>> tk->tkr_mono.mult += mult_adj;
+ * 在以下使用tk->tkr_mono.mult:
+ *   - arch/ia64/kernel/time.c|438| <<update_vsyscall>> fsyscall_gtod_data.clk_mult = tk->tkr_mono.mult;
+ *   - arch/sparc/kernel/vdso.c|35| <<update_vsyscall>> vdata->clock.mult = tk->tkr_mono.mult;
+ *   - arch/x86/kvm/x86.c|2394| <<update_pvclock_gtod>> vdata->clock.mult = tk->tkr_mono.mult;
+ *   - kernel/time/timekeeping.c|1302| <<timekeeping_forward_now>> tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;
+ *   - kernel/time/timekeeping.c|1368| <<ktime_get_resolution_ns>> nsecs = tk->tkr_mono.mult >> tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|1745| <<adjust_historical_crosststamp>> (corr_raw, tk->tkr_mono.mult, tk->tkr_raw.mult);
+ *   - kernel/time/timekeeping.c|2653| <<timekeeping_apply_adjustment>> if ((mult_adj > 0) && (tk->tkr_mono.mult + mult_adj < mult_adj)) {
+ *   - kernel/time/timekeeping.c|2681| <<timekeeping_adjust>> mult = tk->tkr_mono.mult - tk->ntp_err_mult;
+ *   - kernel/time/timekeeping.c|2697| <<timekeeping_adjust>> timekeeping_apply_adjustment(tk, offset, mult - tk->tkr_mono.mult);
+ *   - kernel/time/timekeeping.c|2700| <<timekeeping_adjust>> (abs(tk->tkr_mono.mult - tk->tkr_mono.clock->mult)
+ *   - kernel/time/timekeeping.c|2704| <<timekeeping_adjust>> tk->tkr_mono.clock->name, (long )tk->tkr_mono.mult,
+ *   - kernel/time/vsyscall.c|26| <<update_vdso_data>> vdata[CS_HRES_COARSE].mult = tk->tkr_mono.mult;
+ *
+ * 在以下设置tk->tkr_mono.shift:
+ *   - kernel/time/timekeeping.c|698| <<tk_setup_internals>> tk->tkr_mono.shift = clock->shift;
+ * 在以下使用tk->tkr_mono.shift:
+ *   - arch/arm/xen/enlighten.c|110| <<xen_pvclock_gtod_notify>> now.tv_nsec = (long )(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ *   - arch/ia64/kernel/time.c|439| <<update_vsyscall>> fsyscall_gtod_data.clk_shift = tk->tkr_mono.shift;
+ *   - arch/ia64/kernel/time.c|450| <<update_vsyscall>> << tk->tkr_mono.shift);
+ *   - arch/ia64/kernel/time.c|454| <<update_vsyscall>> (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) {
+ *   - arch/ia64/kernel/time.c|456| <<update_vsyscall>> ((u64)NSEC_PER_SEC) << tk->tkr_mono.shift;
+ *   - arch/sparc/kernel/vdso.c|36| <<update_vsyscall>> vdata->clock.shift = tk->tkr_mono.shift;
+ *   - arch/sparc/kernel/vdso.c|45| <<update_vsyscall>> tk->tkr_mono.shift);
+ *   - arch/sparc/kernel/vdso.c|48| <<update_vsyscall>> (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) {
+ *   - arch/sparc/kernel/vdso.c|50| <<update_vsyscall>> ((u64)NSEC_PER_SEC) << tk->tkr_mono.shift;
+ *   - arch/sparc/kernel/vdso.c|56| <<update_vsyscall>> (long )(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ *   - arch/x86/kvm/x86.c|2395| <<update_pvclock_gtod>> vdata->clock.shift = tk->tkr_mono.shift;
+ *   - arch/x86/xen/time.c|109| <<xen_pvclock_gtod_notify>> now.tv_nsec = (long )(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ *   - kernel/time/timekeeping.c|345| <<tk_normalize_xtime>> while (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {
+ *   - kernel/time/timekeeping.c|346| <<tk_normalize_xtime>> tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|370| <<tk_xtime>> ts.tv_nsec = (long )(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ *   - kernel/time/timekeeping.c|377| <<tk_set_xtime>> tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|388| <<tk_xtime_add>> tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|1190| <<tk_update_ktime_data>> nsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
+ *   - kernel/time/timekeeping.c|1368| <<ktime_get_resolution_ns>> nsecs = tk->tkr_mono.mult >> tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|1414| <<ktime_get_coarse_with_offset>> nsecs = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|2720| <<timekeeping_adjust>> tk->tkr_mono.xtime_nsec += (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
+ *   - kernel/time/timekeeping.c|2740| <<accumulate_nsecs_to_secs>> u64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
+ *   - kernel/time/vsyscall.c|27| <<update_vdso_data>> vdata[CS_HRES_COARSE].shift = tk->tkr_mono.shift;
+ *   - kernel/time/vsyscall.c|38| <<update_vdso_data>> nsec += ((u64)tk->wall_to_monotonic.tv_nsec << tk->tkr_mono.shift);
+ *   - kernel/time/vsyscall.c|39| <<update_vdso_data>> while (nsec >= (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) {
+ *   - kernel/time/vsyscall.c|40| <<update_vdso_data>> nsec -= (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift);
+ *   - kernel/time/vsyscall.c|49| <<update_vdso_data>> nsec += (u64)tk->monotonic_to_boot.tv_nsec << tk->tkr_mono.shift;
+ *   - kernel/time/vsyscall.c|55| <<update_vdso_data>> while (nsec >= (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift)) {
+ *   - kernel/time/vsyscall.c|56| <<update_vdso_data>> nsec -= (((u64)NSEC_PER_SEC) << tk->tkr_mono.shift);
+ *   - kernel/time/vsyscall.c|94| <<update_vsyscall>> vdso_ts->nsec = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;
+ *   - kernel/time/vsyscall.c|99| <<update_vsyscall>> nsec = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;
+ */
+
 #define TK_CLEAR_NTP		(1 << 0)
 #define TK_MIRROR		(1 << 1)
 #define TK_CLOCK_WAS_SET	(1 << 2)
 
+/*
+ * 在以下使用TK_ADV_TICK:
+ *   - kernel/time/timekeeping.c|2322| <<timekeeping_advance>> if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
+ *   - kernel/time/timekeeping.c|2390| <<update_wall_time>> if (timekeeping_advance(TK_ADV_TICK))
+ *
+ * 在以下使用TK_ADV_FREQ:
+ *   - kernel/time/timekeeping.c|2635| <<do_adjtimex>> clock_set |= timekeeping_advance(TK_ADV_FREQ);
+ */
 enum timekeeping_adv_mode {
 	/* Update timekeeper when a tick has passed */
 	TK_ADV_TICK,
@@ -41,12 +173,96 @@ enum timekeeping_adv_mode {
 	TK_ADV_FREQ
 };
 
+/*
+ * 在以下使用DEFINE_RAW_SPINLOCK(timekeeper_lock):
+ *   - kernel/time/timekeeping.c|52| <<global>> DEFINE_RAW_SPINLOCK(timekeeper_lock);
+ *   - kernel/time/timekeeping.c|115| <<global>> .seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),
+ *   - kernel/time/timekeeping.c|950| <<pvclock_gtod_register_notifier>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|953| <<pvclock_gtod_register_notifier>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|969| <<pvclock_gtod_unregister_notifier>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|971| <<pvclock_gtod_unregister_notifier>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|1704| <<do_settimeofday64>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|1724| <<do_settimeofday64>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|1754| <<timekeeping_inject_offset>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|1774| <<timekeeping_inject_offset>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|1854| <<change_clocksource>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|1872| <<change_clocksource>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2074| <<timekeeping_init>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2096| <<timekeeping_init>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2177| <<timekeeping_inject_sleeptime64>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2189| <<timekeeping_inject_sleeptime64>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2213| <<timekeeping_resume>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2251| <<timekeeping_resume>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2282| <<timekeeping_suspend>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2321| <<timekeeping_suspend>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2612| <<timekeeping_advance>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2692| <<timekeeping_advance>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2956| <<do_adjtimex>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2970| <<do_adjtimex>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|2994| <<hardpps>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/timekeeping.c|3000| <<hardpps>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ *   - kernel/time/vsyscall.c|150| <<vdso_update_begin>> raw_spin_lock_irqsave(&timekeeper_lock, flags);
+ *   - kernel/time/vsyscall.c|169| <<vdso_update_end>> raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
+ */
 DEFINE_RAW_SPINLOCK(timekeeper_lock);
 
 /*
  * The most important data for readout fits into a single 64 byte
  * cache line.
  */
+/*
+ * 4.14的例子
+ * crash> tk_core
+ * tk_core = $41 = {
+ *   seq = {
+ *     sequence = 284378
+ *   },
+ *   timekeeper = {
+ *     tkr_mono = {
+ *       clock = 0xffffffffab42f460,
+ *       mask = 18446744073709551615,
+ *       cycle_last = 46833745739432346,
+ *       mult = 4945959,
+ *       shift = 24,
+ *       xtime_nsec = 1338771463721048,
+ *       base = 1075342708208
+ *     },
+ *     tkr_raw = {
+ *       clock = 0xffffffffab42f460,
+ *       mask = 18446744073709551615,
+ *       cycle_last = 46833745739432346,
+ *       mult = 4945498,
+ *       shift = 24,
+ *       xtime_nsec = 5525025441152622,
+ *       base = 1075000000000
+ *     },
+ *     xtime_sec = 1679499689,
+ *     ktime_sec = 1075,
+ *     wall_to_monotonic = {
+ *       tv_sec = -1679498614,
+ *       tv_nsec = 342708208
+ *     },
+ *     offs_real = 1679498613657291792,
+ *     offs_boot = 0,
+ *     offs_tai = 1679498613657291792,
+ *     tai_offset = 0,
+ *     clock_was_set_seq = 5,
+ *     cs_was_changed_seq = 3 '\003',
+ *     next_leap_ktime = 9223372036854775807,
+ *     raw_sec = 1075,
+ *     cycle_interval = 3392422,
+ *     xtime_interval = 16778780122698,
+ *     xtime_remainder = -216156,
+ *     raw_interval = 16777216216156,
+ *     ntp_tick = 4295367662436352,
+ *     ntp_error = -11228740608,
+ *     ntp_error_shift = 8,
+ *     ntp_err_mult = 0
+ *   }
+ * }
+ *
+ * tk_core只在这个文件用
+ */
 static struct {
 	seqcount_raw_spinlock_t	seq;
 	struct timekeeper	timekeeper;
@@ -54,8 +270,34 @@ static struct {
 	.seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),
 };
 
+/*
+ * 在以下使用shadow_timekeeper:
+ *   - kernel/time/timekeeping.c|872| <<timekeeping_update>> memcpy(&shadow_timekeeper, &tk_core.timekeeper,
+ *   - kernel/time/timekeeping.c|2306| <<timekeeping_advance>> struct timekeeper *tk = &shadow_timekeeper;
+ */
 static struct timekeeper shadow_timekeeper;
 
+/*
+ * 在以下使用timekeeping_suspended:
+ *   - drivers/bus/ti-sysc.c|247| <<sysc_poll_reset_sysstatus>> if (likely(!timekeeping_suspended)) {
+ *   - drivers/bus/ti-sysc.c|272| <<sysc_poll_reset_sysconfig>> if (likely(!timekeeping_suspended)) {
+ *   - drivers/clk/samsung/clk-pll.c|96| <<samsung_pll_lock_wait>> if (pll_early_timeout || timekeeping_suspended) {
+ *   - drivers/clk/ti/clkctrl.c|101| <<_omap4_is_timeout>> if (unlikely(_early_timeout || timekeeping_suspended)) {
+ *   - kernel/sched/clock.c|440| <<sched_clock_idle_wakeup_event>> if (unlikely(timekeeping_suspended))
+ *   - kernel/time/timekeeping.c|178| <<dummy_clock_read>> if (timekeeping_suspended)
+ *   - kernel/time/timekeeping.c|1174| <<ktime_get_real_ts64>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1199| <<ktime_get>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1218| <<ktime_get_resolution_ns>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1242| <<ktime_get_with_offset>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1263| <<ktime_get_coarse_with_offset>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1393| <<ktime_get_ts64>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1434| <<ktime_get_seconds>> WARN_ON(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|1494| <<ktime_get_snapshot>> WARN_ON_ONCE(timekeeping_suspended);
+ *   - kernel/time/timekeeping.c|2321| <<timekeeping_resume>> timekeeping_suspended = 0;
+ *   - kernel/time/timekeeping.c|2358| <<timekeeping_suspend>> timekeeping_suspended = 1;
+ *   - kernel/time/timekeeping.c|2774| <<timekeeping_advance>> if (unlikely(timekeeping_suspended))
+ *   - kernel/time/timekeeping.c|3071| <<random_get_entropy_fallback>> if (unlikely(timekeeping_suspended || !clock))
+ */
 /* flag for if timekeeping is suspended */
 int __read_mostly timekeeping_suspended;
 
@@ -73,6 +315,11 @@ struct tk_fast {
 	struct tk_read_base	base[2];
 };
 
+/*
+ * 在以下使用cycles_at_suspend:
+ *   - kernel/time/timekeeping.c|179| <<dummy_clock_read>> return cycles_at_suspend;
+ *   - kernel/time/timekeeping.c|930| <<halt_fast_timekeeper>> cycles_at_suspend = tk_clock_read(tkr);
+ */
 /* Suspend-time cycles value for halted fast timekeeper. */
 static u64 cycles_at_suspend;
 
@@ -102,20 +349,79 @@ static struct clocksource dummy_clock = {
 		.shift		= 0,				\
 	}
 
+/*
+ * 在以下使用tk_fast_mono:
+ *   - kernel/time/timekeeping.c|353| <<global>> .seq = SEQCNT_LATCH_ZERO(tk_fast_mono.seq),
+ *   - kernel/time/timekeeping.c|951| <<ktime_get_mono_fast_ns>> return __ktime_get_fast_ns(&tk_fast_mono);
+ *   - kernel/time/timekeeping.c|1048| <<ktime_get_real_fast_ns>> return __ktime_get_real_fast(&tk_fast_mono, NULL);
+ *   - kernel/time/timekeeping.c|1102| <<ktime_get_fast_timestamps>> snapshot->real = __ktime_get_real_fast(&tk_fast_mono, &snapshot->mono);
+ *   - kernel/time/timekeeping.c|1125| <<halt_fast_timekeeper>> update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);
+ *   - kernel/time/timekeeping.c|1347| <<timekeeping_update>> update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
+ */
 static struct tk_fast tk_fast_mono ____cacheline_aligned = {
 	.seq     = SEQCNT_LATCH_ZERO(tk_fast_mono.seq),
 	.base[0] = FAST_TK_INIT,
 	.base[1] = FAST_TK_INIT,
 };
 
+/*
+ * 在以下使用tk_fast_raw:
+ *   - kernel/time/timekeeping.c|178| <<global>> .seq = SEQCNT_LATCH_ZERO(tk_fast_raw.seq),
+ *   - kernel/time/timekeeping.c|672| <<ktime_get_raw_fast_ns>> return __ktime_get_fast_ns(&tk_fast_raw);
+ *   - kernel/time/timekeeping.c|833| <<halt_fast_timekeeper>> update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
+ *   - kernel/time/timekeeping.c|977| <<timekeeping_update>> update_fast_timekeeper(&tk->tkr_raw, &tk_fast_raw);
+ */
 static struct tk_fast tk_fast_raw  ____cacheline_aligned = {
 	.seq     = SEQCNT_LATCH_ZERO(tk_fast_raw.seq),
 	.base[0] = FAST_TK_INIT,
 	.base[1] = FAST_TK_INIT,
 };
 
+/*
+ * [0] tk_normalize_xtime
+ * [0] change_clocksource
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - kernel/time/timekeeping.c|148| <<tk_xtime_add>> tk_normalize_xtime(tk);
+ *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk_normalize_xtime(tk);
+ *
+ * 关于timekeeping_forward_now() 简单的测试只在change clocksource的时候调用
+ *
+ * 两个部分:
+ * 1. 把tk->tkr_mono.xtime_nsec多出的部分给到tk->xtime_sec
+ * 2. 把tk->tkr_raw.xtime_nsec多出的部分给到tk->raw_sec
+ *
+ * tk_normalize_xtime()在简单的测试中只在change clocksource的时候调用
+ */
 static inline void tk_normalize_xtime(struct timekeeper *tk)
 {
+	/*
+	 * 在以下设置tk_read_base->xtime_nsec:
+	 *   - kernel/time/timekeeping.c|125| <<tk_normalize_xtime>> tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
+	 *   - kernel/time/timekeeping.c|129| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+	 *   - kernel/time/timekeeping.c|146| <<tk_set_xtime>> tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;
+	 *   - kernel/time/timekeeping.c|152| <<tk_xtime_add>> tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;
+	 *   - kernel/time/timekeeping.c|383| <<tk_setup_internals>> tk->tkr_mono.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|384| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|386| <<tk_setup_internals>> tk->tkr_mono.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|387| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|901| <<timekeeping_forward_now>> tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;
+	 *   - kernel/time/timekeeping.c|902| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+	 *   - kernel/time/timekeeping.c|2139| <<timekeeping_apply_adjustment>> tk->tkr_mono.xtime_nsec -= offset;
+	 *   - kernel/time/timekeeping.c|2193| <<timekeeping_adjust>> tk->tkr_mono.xtime_nsec += (u64)NSEC_PER_SEC <<
+	 *   - kernel/time/timekeeping.c|2215| <<accumulate_nsecs_to_secs>> tk->tkr_mono.xtime_nsec -= nsecps;
+	 *   - kernel/time/timekeeping.c|2275| <<logarithmic_accumulation>> tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
+	 *   - kernel/time/timekeeping.c|2279| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 *   - kernel/time/timekeeping.c|2282| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+	 *   - kernel/time/vsyscall.c|99| <<update_vsyscall>> nsec = tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift;
+	 *
+	 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+	 */
 	while (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {
 		tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
 		tk->xtime_sec++;
@@ -126,6 +432,16 @@ static inline void tk_normalize_xtime(struct timekeeper *tk)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1453| <<do_settimeofday64>> xt = tk_xtime(tk);
+ *   - kernel/time/timekeeping.c|1504| <<timekeeping_inject_offset>> tmp = timespec64_add(tk_xtime(tk), *ts);
+ *   - kernel/time/timekeeping.c|2025| <<timekeeping_suspend>> delta = timespec64_sub(tk_xtime(tk), timekeeping_suspend_time);
+ *   - kernel/time/timekeeping.c|2422| <<ktime_get_coarse_real_ts64>> *ts = tk_xtime(tk);
+ *   - kernel/time/timekeeping.c|2436| <<ktime_get_coarse_ts64>> now = tk_xtime(tk);
+ *
+ * 返回的是基于mono的timespec64
+ */
 static inline struct timespec64 tk_xtime(const struct timekeeper *tk)
 {
 	struct timespec64 ts;
@@ -141,13 +457,31 @@ static void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)
 	tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1511| <<timekeeping_inject_offset>> tk_xtime_add(tk, ts);
+ *   - kernel/time/timekeeping.c|1841| <<__timekeeping_inject_sleeptime>> tk_xtime_add(tk, delta);
+ */
 static void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)
 {
 	tk->xtime_sec += ts->tv_sec;
 	tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;
+	/*
+	 * 两个部分:
+	 * 1. 把tk->tkr_mono.xtime_nsec多出的部分给到tk->xtime_sec
+	 * 2. 把tk->tkr_raw.xtime_nsec多出的部分给到tk->raw_sec
+	 */
 	tk_normalize_xtime(tk);
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1377| <<do_settimeofday64>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts_delta));
+ *   - kernel/time/timekeeping.c|1428| <<timekeeping_inject_offset>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *ts));
+ *   - kernel/time/timekeeping.c|1711| <<timekeeping_init>> tk_set_wall_to_mono(tk, wall_to_mono);
+ *   - kernel/time/timekeeping.c|1740| <<__timekeeping_inject_sleeptime>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));
+ *   - kernel/time/timekeeping.c|2134| <<accumulate_nsecs_to_secs>> tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts));
+ */
 static void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)
 {
 	struct timespec64 tmp;
@@ -161,10 +495,19 @@ static void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)
 	WARN_ON_ONCE(tk->offs_real != timespec64_to_ktime(tmp));
 	tk->wall_to_monotonic = wtm;
 	set_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);
+	/*
+	 * offs_real:          Offset clock monotonic -> clock realtime
+	 * @offs_boot:          Offset clock monotonic -> clock boottime
+	 * @offs_tai:           Offset clock monotonic -> clock tai
+	 */
 	tk->offs_real = timespec64_to_ktime(tmp);
 	tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1843| <<__timekeeping_inject_sleeptime>> tk_update_sleep_time(tk, timespec64_to_ktime(*delta));
+ */
 static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
 {
 	tk->offs_boot = ktime_add(tk->offs_boot, delta);
@@ -188,6 +531,19 @@ static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
  * a read of the fast-timekeeper tkrs (which is protected by its own locking
  * and update logic).
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|268| <<timekeeping_get_delta>> now = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|305| <<timekeeping_get_delta>> cycle_now = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|335| <<tk_setup_internals>> tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|458| <<fast_tk_get_delta_ns>> u64 delta, cycles = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|679| <<halt_fast_timekeeper>> cycles_at_suspend = tk_clock_read(tkr);
+ *   - kernel/time/timekeeping.c|845| <<timekeeping_forward_now>> cycle_now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1144| <<ktime_get_snapshot>> now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1322| <<ktime_get_snapshot>> now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1881| <<timekeeping_resume>> cycle_now = tk_clock_read(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|2249| <<timekeeping_advance>> offset = clocksource_delta(tk_clock_read(&tk->tkr_mono),
+ */
 static inline u64 tk_clock_read(const struct tk_read_base *tkr)
 {
 	struct clocksource *clock = READ_ONCE(tkr->clock);
@@ -237,6 +593,12 @@ static void timekeeping_check_update(struct timekeeper *tk, u64 offset)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|596| <<timekeeping_get_ns>> delta = timekeeping_get_delta(tkr);
+ *
+ * 带检测的版本
+ */
 static inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -278,9 +640,29 @@ static inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)
 	return delta;
 }
 #else
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2543| <<timekeeping_advance>> timekeeping_check_update(tk, offset);
+ */
 static inline void timekeeping_check_update(struct timekeeper *tk, u64 offset)
 {
 }
+/*
+ * struct tk_read_base - base structure for timekeeping readout
+ * @clock:      Current clocksource used for timekeeping.
+ * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+ * @cycle_last: @clock cycle value at last update
+ * @mult:       (NTP adjusted) multiplier for scaled math conversion
+ * @shift:      Shift value for scaled math conversion
+ * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+ * @base:       ktime_t (nanoseconds) base time for readout
+ * @base_real:  Nanoseconds base value for clock REALTIME readout
+ *
+ * calle by:
+ *   - kernel/time/timekeeping.c|410| <<timekeeping_get_ns>> delta = timekeeping_get_delta(tkr);
+ *
+ * 不会更新参数中结构的信息
+ */
 static inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)
 {
 	u64 cycle_now, delta;
@@ -289,6 +671,9 @@ static inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)
 	cycle_now = tk_clock_read(tkr);
 
 	/* calculate the delta since the last update_wall_time */
+	/*
+	 * (cycle_now - tkr->cycle_last) & tkr->mask
+	 */
 	delta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);
 
 	return delta;
@@ -306,6 +691,11 @@ static inline u64 timekeeping_get_delta(const struct tk_read_base *tkr)
  *
  * Unless you're the timekeeping code, you should not be using this!
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1588| <<change_clocksource>> tk_setup_internals(tk, new);
+ *   - kernel/time/timekeeping.c|1795| <<timekeeping_init>> tk_setup_internals(tk, clock);
+ */
 static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)
 {
 	u64 interval;
@@ -318,10 +708,27 @@ static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)
 	tk->tkr_mono.mask = clock->mask;
 	tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
 
+	/*
+	 * struct tk_read_base {
+	 *     struct clocksource      *clock;
+	 *     u64                     mask;
+	 *     u64                     cycle_last;
+	 *     u32                     mult;
+	 *     u32                     shift;
+	 *     u64                     xtime_nsec;
+	 *     ktime_t                 base;
+	 *     u64                     base_real;
+	 * };
+	 */
 	tk->tkr_raw.clock = clock;
 	tk->tkr_raw.mask = clock->mask;
 	tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
 
+	/*
+	 * 应该是(cycle * mult) >> shift
+	 *
+	 * NTP_INTERVAL_LENGTH应该是每tick一下的ns数量
+	 */
 	/* Do the ns -> cycle conversion first, using original mult */
 	tmp = NTP_INTERVAL_LENGTH;
 	tmp <<= clock->shift;
@@ -331,12 +738,36 @@ static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)
 	if (tmp == 0)
 		tmp = 1;
 
+	/*
+	 * internal是cycles, 从ns计算来的
+	 */
 	interval = (u64) tmp;
+	/*
+	 * 在以下使用timekeeper->cycle_interval:
+	 *   - kernel/time/timekeeping.c|555| <<tk_setup_internals>> tk->cycle_interval = interval;
+	 *   - kernel/time/timekeeping.c|2381| <<timekeeping_apply_adjustment>> s64 interval = tk->cycle_interval;
+	 *   - kernel/time/timekeeping.c|2468| <<timekeeping_adjust>> tk->xtime_remainder, tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2577| <<logarithmic_accumulation>> u64 interval = tk->cycle_interval << shift;
+	 *   - kernel/time/timekeeping.c|2665| <<timekeeping_advance>> if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
+	 *   - kernel/time/timekeeping.c|2682| <<timekeeping_advance>> shift = ilog2(offset) - ilog2(tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2687| <<timekeeping_advance>> while (offset >= tk->cycle_interval) {
+	 *   - kernel/time/timekeeping.c|2694| <<timekeeping_advance>> if (offset < tk->cycle_interval<<shift)
+	 */
 	tk->cycle_interval = interval;
 
+	/*
+	 * @cycle_interval:     Number of clock cycles in one NTP interval
+	 * @xtime_interval:     Number of clock shifted nano seconds in one NTP interval.
+	 * @raw_interval:       Shifted raw nano seconds accumulated per NTP interval.
+	 */
 	/* Go back from cycles -> shifted ns */
 	tk->xtime_interval = interval * clock->mult;
 	tk->xtime_remainder = ntpinterval - tk->xtime_interval;
+	/*
+	 * 在以下使用timekeeper->raw_interval:
+	 *   - kernel/time/timekeeping.c|499| <<tk_setup_internals>> tk->raw_interval = interval * clock->mult;
+	 *   - kernel/time/timekeeping.c|2432| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 */
 	tk->raw_interval = interval * clock->mult;
 
 	 /* if changing clocks, convert xtime_nsec shift units */
@@ -354,6 +785,12 @@ static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)
 	tk->tkr_mono.shift = clock->shift;
 	tk->tkr_raw.shift = clock->shift;
 
+	/*
+	 * @ntp_error:          Difference between accumulated time and NTP time in ntp
+	 *                      shifted nano seconds.
+	 * @ntp_error_shift:    Shift conversion between clock shifted nano seconds and
+	 *                      ntp shifted nano seconds.
+	 */
 	tk->ntp_error = 0;
 	tk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;
 	tk->ntp_tick = ntpinterval << tk->ntp_error_shift;
@@ -371,24 +808,65 @@ static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)
 
 /* Timekeeper helper functions. */
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|557| <<timekeeping_get_ns>> return timekeeping_delta_to_ns(tkr, delta);
+ *   - kernel/time/timekeeping.c|566| <<timekeeping_cycles_to_ns>> return timekeeping_delta_to_ns(tkr, delta);
+ *   - kernel/time/timekeeping.c|607| <<fast_tk_get_delta_ns>> return timekeeping_delta_to_ns(tkr, delta);
+ */
 static inline u64 timekeeping_delta_to_ns(const struct tk_read_base *tkr, u64 delta)
 {
 	u64 nsec;
 
+	/*
+	 * 在以下设置tk_read_base->xtime_nsec (非mono!!!):
+	 *   - kernel/time/timekeeping.c|129| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+	 *   - kernel/time/timekeeping.c|384| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|387| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|902| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+	 *   - kernel/time/timekeeping.c|2279| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 *   - kernel/time/timekeeping.c|2282| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+	 *
+	 * Shifted (fractional) nano seconds offset for readout
+	 */
 	nsec = delta * tkr->mult + tkr->xtime_nsec;
 	nsec >>= tkr->shift;
 
 	return nsec;
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|877| <<ktime_get_real_ts64>> nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|898| <<ktime_get>> nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|941| <<ktime_get_with_offset>> nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1003| <<ktime_get_raw>> nsecs = timekeeping_get_ns(&tk->tkr_raw);
+ *   - kernel/time/timekeeping.c|1053| <<ktime_get_ts64>> nsec = timekeeping_get_ns(&tk->tkr_mono);
+ *   - kernel/time/timekeeping.c|1601| <<ktime_get_raw_ts64>> nsecs = timekeeping_get_ns(&tk->tkr_raw);
+ *   - kernel/time/timekeeping.c|2410| <<ktime_get_update_offsets_now>> nsecs = timekeeping_get_ns(&tk->tkr_mono);
+ */
+/*
+ * 不会更新参数中结构的信息
+ * 调用的可以是mono, 也可以是raw
+ */
 static inline u64 timekeeping_get_ns(const struct tk_read_base *tkr)
 {
 	u64 delta;
 
+	/*
+	 * 不会更新参数中结构的信息
+	 */
 	delta = timekeeping_get_delta(tkr);
 	return timekeeping_delta_to_ns(tkr, delta);
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1317| <<ktime_get_snapshot>> nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, now);
+ *   - kernel/time/timekeeping.c|1318| <<ktime_get_snapshot>> nsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw, now);
+ *   - kernel/time/timekeeping.c|1503| <<ktime_get_snapshot>> nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono,
+ *   - kernel/time/timekeeping.c|1505| <<ktime_get_snapshot>> nsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw,
+ */
 static inline u64 timekeeping_cycles_to_ns(const struct tk_read_base *tkr, u64 cycles)
 {
 	u64 delta;
@@ -413,6 +891,13 @@ static inline u64 timekeeping_cycles_to_ns(const struct tk_read_base *tkr, u64 c
  * slightly wrong timestamp (a few nanoseconds). See
  * @ktime_get_mono_fast_ns.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|828| <<halt_fast_timekeeper>> update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);
+ *   - kernel/time/timekeeping.c|833| <<halt_fast_timekeeper>> update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
+ *   - kernel/time/timekeeping.c|976| <<timekeeping_update>> update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
+ *   - kernel/time/timekeeping.c|977| <<timekeeping_update>> update_fast_timekeeper(&tk->tkr_raw, &tk_fast_raw);
+ */
 static void update_fast_timekeeper(const struct tk_read_base *tkr,
 				   struct tk_fast *tkf)
 {
@@ -431,6 +916,11 @@ static void update_fast_timekeeper(const struct tk_read_base *tkr,
 	memcpy(base + 1, base, sizeof(*base));
 }
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|668| <<__ktime_get_fast_ns>> now += fast_tk_get_delta_ns(tkr);
+ *   - kernel/time/timekeeping.c|790| <<__ktime_get_real_fast>> delta = fast_tk_get_delta_ns(tkr);
+ */
 static __always_inline u64 fast_tk_get_delta_ns(struct tk_read_base *tkr)
 {
 	u64 delta, cycles = tk_clock_read(tkr);
@@ -487,6 +977,25 @@ static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)
  * of the following timestamps. Callers need to be aware of that and
  * deal with it.
  */
+/*
+ * 特别多的调用:
+ *   - kernel/trace/trace.c
+ *   - drivers/base/power/domain.c
+ *   - drivers/base/power/runtime.c
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_device.c
+ *   - drivers/gpu/drm/i915/i915_perf.c
+ *   - drivers/ufs/host/ufs-mediatek.c
+ *   - include/linux/pm_runtime.h
+ *   - kernel/bpf/helpers.c
+ *   - kernel/debug/kdb/kdb_main.c
+ *   - kernel/events/core.c
+ *   - kernel/rcu/rcuscale.c
+ *   - kernel/rcu/refscale.c
+ *   - kernel/rcu/refscale.c
+ *   - kernel/rcu/srcutree.c
+ *   - kernel/time/timekeeping.c
+ *   - kernel/watchdog_hld.c
+ */
 u64 notrace ktime_get_mono_fast_ns(void)
 {
 	return __ktime_get_fast_ns(&tk_fast_mono);
@@ -499,6 +1008,12 @@ EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);
  * Contrary to ktime_get_mono_fast_ns() this is always correct because the
  * conversion factor is not affected by NTP/PTP correction.
  */
+/*
+ * called by:
+ *   - kernel/trace/trace.c|1545| <<global>> { ktime_get_raw_fast_ns, "mono_raw", 1 },
+ *   - drivers/gpu/drm/i915/gt/intel_context.h|371| <<intel_context_clock>> return ktime_get_raw_fast_ns();
+ *   - kernel/events/core.c|12178| <<perf_event_set_clock>> event->clock = &ktime_get_raw_fast_ns;
+ */
 u64 notrace ktime_get_raw_fast_ns(void)
 {
 	return __ktime_get_fast_ns(&tk_fast_raw);
@@ -546,6 +1061,12 @@ EXPORT_SYMBOL_GPL(ktime_get_boot_fast_ns);
  * by settime or adjtimex with an offset. The user of this function has to deal
  * with the possibility of wrong timestamps in post processing.
  */
+/*
+ * 一个例子:
+ * CLOCK_REALTIME: Initialized at boot from RTC
+ * CLOCK_MONOTONIC: CLOCK_REALTIME - wall_to_monotonic
+ * CLOCK_TAI: CLOCK_REALTIME + tai_offset
+ */
 u64 notrace ktime_get_tai_fast_ns(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -554,6 +1075,11 @@ u64 notrace ktime_get_tai_fast_ns(void)
 }
 EXPORT_SYMBOL_GPL(ktime_get_tai_fast_ns);
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1096| <<ktime_get_real_fast_ns>> return __ktime_get_real_fast(&tk_fast_mono, NULL);
+ *   - kernel/time/timekeeping.c|1150| <<ktime_get_fast_timestamps>> snapshot->real = __ktime_get_real_fast(&tk_fast_mono, &snapshot->mono);
+ */
 static __always_inline u64 __ktime_get_real_fast(struct tk_fast *tkf, u64 *mono)
 {
 	struct tk_read_base *tkr;
@@ -578,6 +1104,13 @@ static __always_inline u64 __ktime_get_real_fast(struct tk_fast *tkf, u64 *mono)
  *
  * See ktime_get_fast_ns() for documentation of the time stamp ordering.
  */
+/*
+ * called by:
+ *   - fs/pstore/platform.c|379| <<pstore_record_init>> record->time = ns_to_timespec64(ktime_get_real_fast_ns());
+ *   - kernel/rcu/refscale.c|501| <<ref_clock_section>> x += ktime_get_real_fast_ns();
+ *   - kernel/rcu/refscale.c|513| <<ref_clock_delay_section>> x += ktime_get_real_fast_ns();
+ *   - kernel/time/clocksource-wdtest.c|64| <<wdtest_ktime_read>> ret = ktime_get_real_fast_ns();
+ */
 u64 ktime_get_real_fast_ns(void)
 {
 	return __ktime_get_real_fast(&tk_fast_mono, NULL);
@@ -630,6 +1163,9 @@ EXPORT_SYMBOL_GPL(ktime_get_real_fast_ns);
  * trivial as on early boot because it needs some careful protection
  * against the clock monotonic timestamp jumping backwards on resume.
  */
+/*
+ * 没见到调用
+ */
 void ktime_get_fast_timestamps(struct ktime_timestamps *snapshot)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -648,6 +1184,10 @@ void ktime_get_fast_timestamps(struct ktime_timestamps *snapshot)
  * number of cycles every time until timekeeping is resumed at which time the
  * proper readout base for the fast timekeeper will be restored automatically.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2706| <<timekeeping_suspend>> halt_fast_timekeeper(tk);
+ */
 static void halt_fast_timekeeper(const struct timekeeper *tk)
 {
 	static struct tk_read_base tkr_dummy;
@@ -665,8 +1205,28 @@ static void halt_fast_timekeeper(const struct timekeeper *tk)
 	update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
 }
 
+/*
+ * 在以下使用pvclock_gtod_chain:
+ *   - kernel/time/timekeeping.c|716| <<global>> static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
+ *   - kernel/time/timekeeping.c|720| <<update_pvclock_gtod>> raw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);
+ *   - kernel/time/timekeeping.c|734| <<pvclock_gtod_register_notifier>> ret = raw_notifier_chain_register(&pvclock_gtod_chain, nb);
+ *   - kernel/time/timekeeping.c|753| <<pvclock_gtod_unregister_notifier>> ret = raw_notifier_chain_unregister(&pvclock_gtod_chain, nb);
+ */
 static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
 
+/*
+ * tk_core的声明.
+ * 111 static struct {
+ * 112         seqcount_raw_spinlock_t seq;
+ * 113         struct timekeeper       timekeeper;
+ * 114 } tk_core ____cacheline_aligned = {
+ * 115         .seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),
+ * 116 };
+ *
+ * called by:
+ *   - kernel/time/timekeeping.c|735| <<pvclock_gtod_register_notifier>> update_pvclock_gtod(tk, true);
+ *   - kernel/time/timekeeping.c|841| <<timekeeping_update>> update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
 {
 	raw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);
@@ -676,6 +1236,12 @@ static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
  * pvclock_gtod_register_notifier - register a pvclock timedata update listener
  * @nb: Pointer to the notifier block to register
  */
+/*
+ * called by:
+ *   - arch/arm/xen/enlighten.c|530| <<xen_guest_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|9910| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/xen/time.c|527| <<xen_time_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ */
 int pvclock_gtod_register_notifier(struct notifier_block *nb)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -696,6 +1262,10 @@ EXPORT_SYMBOL_GPL(pvclock_gtod_register_notifier);
  * timedata update listener
  * @nb: Pointer to the notifier block to unregister
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10255| <<kvm_arch_exit>> pvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);
+ */
 int pvclock_gtod_unregister_notifier(struct notifier_block *nb)
 {
 	unsigned long flags;
@@ -712,6 +1282,11 @@ EXPORT_SYMBOL_GPL(pvclock_gtod_unregister_notifier);
 /*
  * tk_update_leap_state - helper to update the next_leap_ktime
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1317| <<timekeeping_update>> tk_update_leap_state(tk);
+ *   - kernel/time/timekeeping.c|3485| <<do_adjtimex>> tk_update_leap_state(tk);
+ */
 static inline void tk_update_leap_state(struct timekeeper *tk)
 {
 	tk->next_leap_ktime = ntp_get_next_leap();
@@ -723,6 +1298,21 @@ static inline void tk_update_leap_state(struct timekeeper *tk)
 /*
  * Update the ktime_t based scalar nsec members of the timekeeper
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|779| <<timekeeping_update>> tk_update_ktime_data(tk);
+ *
+ * 一个例子:
+ * update_wall_time() or do_adjtimex()
+ * -> timekeeping_advance()
+ *    -> timekeeping_update()
+ *       -> tk_update_ktime_data()
+ *
+ * 部分更新的:
+ * - tk->tkr_mono.base
+ * - tk->ktime_sec
+ * - tk->tkr_raw.base
+ */
 static inline void tk_update_ktime_data(struct timekeeper *tk)
 {
 	u64 seconds;
@@ -735,6 +1325,15 @@ static inline void tk_update_ktime_data(struct timekeeper *tk)
 	 *	nsec = base_mono + now();
 	 * ==> base_mono = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec
 	 */
+	/*
+	 * 在以下使用timekeeper->xtime_sec:
+	 *   - kernel/time/timekeeping.c|121| <<tk_normalize_xtime>> tk->xtime_sec++;
+	 *   - kernel/time/timekeeping.c|140| <<tk_set_xtime>> tk->xtime_sec = ts->tv_sec;
+	 *   - kernel/time/timekeeping.c|146| <<tk_xtime_add>> tk->xtime_sec += ts->tv_sec;
+	 *   - kernel/time/timekeeping.c|2072| <<timekeeping_adjust>> tk->xtime_sec--;
+	 *   - kernel/time/timekeeping.c|2093| <<accumulate_nsecs_to_secs>> tk->xtime_sec++;
+	 *   - kernel/time/timekeeping.c|2109| <<accumulate_nsecs_to_secs>> tk->xtime_sec += leap;
+	 */
 	seconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
 	nsec = (u32) tk->wall_to_monotonic.tv_nsec;
 	tk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);
@@ -749,11 +1348,48 @@ static inline void tk_update_ktime_data(struct timekeeper *tk)
 		seconds++;
 	tk->ktime_sec = seconds;
 
+	/*
+	 * 在以下使用timekeeper->raw_sec:
+	 *   - kernel/time/timekeeping.c|225| <<tk_normalize_xtime>> tk->raw_sec++;
+	 *   - kernel/time/timekeeping.c|1031| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+	 *   - kernel/time/timekeeping.c|1934| <<ktime_get_raw_ts64>> ts->tv_sec = tk->raw_sec;
+	 *   - kernel/time/timekeeping.c|2089| <<timekeeping_init>> tk->raw_sec = 0;
+	 *   - kernel/time/timekeeping.c|2583| <<logarithmic_accumulation>> tk->raw_sec++;
+	 *   - kernel/time/vsyscall.c|63| <<update_vdso_data>> vdso_ts->sec = tk->raw_sec;
+	 *
+	 * 只在这里修改tk->tkr_raw.base:
+	 *   - kernel/time/timekeeping.c|1150| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+	 */
 	/* Update the monotonic raw base */
 	tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
 }
 
 /* must hold timekeeper_lock */
+/*
+ * tk_core的声明.
+ * 111 static struct {
+ * 112         seqcount_raw_spinlock_t seq;
+ * 113         struct timekeeper       timekeeper;
+ * 114 } tk_core ____cacheline_aligned = {
+ * 115         .seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),
+ * 116 };
+ *
+ * called by:
+ *   - kernel/time/timekeeping.c|1381| <<do_settimeofday64>> timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);
+ *   - kernel/time/timekeeping.c|1431| <<timekeeping_inject_offset>> timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);
+ *   - kernel/time/timekeeping.c|1520| <<change_clocksource>> timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);
+ *   - kernel/time/timekeeping.c|1713| <<timekeeping_init>> timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
+ *   - kernel/time/timekeeping.c|1806| <<timekeeping_inject_sleeptime64>> timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);
+ *   - kernel/time/timekeeping.c|1869| <<timekeeping_resume>> timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
+ *   - kernel/time/timekeeping.c|1938| <<timekeeping_suspend>> timekeeping_update(tk, TK_MIRROR);
+ *   - kernel/time/timekeeping.c|2266| <<timekeeping_advance>> timekeeping_update(tk, clock_set);
+ *   - kernel/time/timekeeping.c|2521| <<do_adjtimex>> timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
+ *
+ * 一个例子:
+ * update_wall_time() or do_adjtimex() 
+ * -> timekeeping_advance()
+ *    -> timekeeping_update()
+ */
 static void timekeeping_update(struct timekeeper *tk, unsigned int action)
 {
 	if (action & TK_CLEAR_NTP) {
@@ -762,9 +1398,27 @@ static void timekeeping_update(struct timekeeper *tk, unsigned int action)
 	}
 
 	tk_update_leap_state(tk);
+	/*
+	 * called by:
+	 *   - kernel/time/timekeeping.c|779| <<timekeeping_update>> tk_update_ktime_data(tk);
+	 *
+	 * 一个例子:
+	 * update_wall_time() or do_adjtimex()
+	 * -> timekeeping_advance()
+	 *    -> timekeeping_update()
+	 *       -> tk_update_ktime_data()
+	 *
+	 * 部分更新的:
+	 * - tk->tkr_mono.base
+	 * - tk->ktime_sec
+	 * - tk->tkr_raw.base
+	 */
 	tk_update_ktime_data(tk);
 
 	update_vsyscall(tk);
+	/*
+	 * 调用call chain !!!
+	 */
 	update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);
 
 	tk->tkr_mono.base_real = tk->tkr_mono.base + tk->offs_real;
@@ -791,6 +1445,24 @@ static void timekeeping_update(struct timekeeper *tk, unsigned int action)
  * update_wall_time(). This is useful before significant clock changes,
  * as it avoids having to deal with this time offset explicitly.
  */
+/*
+ * [0] timekeeping_forward_now
+ * [0] change_clocksource
+ * [0] multi_cpu_stop
+ * [0] cpu_stopper_thread
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - kernel/time/timekeeping.c|1438| <<do_settimeofday64>> timekeeping_forward_now(tk);
+ *   - kernel/time/timekeeping.c|1488| <<timekeeping_inject_offset>> timekeeping_forward_now(tk);
+ *   - kernel/time/timekeeping.c|1584| <<change_clocksource>> timekeeping_forward_now(tk);
+ *   - kernel/time/timekeeping.c|1891| <<timekeeping_inject_sleeptime64>> timekeeping_forward_now(tk);
+ *   - kernel/time/timekeeping.c|1993| <<timekeeping_suspend>> timekeeping_forward_now(tk);
+ *
+ * 简单的测试只在change clocksource的时候调用
+ */
 static void timekeeping_forward_now(struct timekeeper *tk)
 {
 	u64 cycle_now, delta;
@@ -812,6 +1484,9 @@ static void timekeeping_forward_now(struct timekeeper *tk)
  *
  * Returns the time of day in a timespec64 (WARN if suspended).
  */
+/*
+ * 特别多调用
+ */
 void ktime_get_real_ts64(struct timespec64 *ts)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -823,6 +1498,9 @@ void ktime_get_real_ts64(struct timespec64 *ts)
 	do {
 		seq = read_seqcount_begin(&tk_core.seq);
 
+		/*
+		 * Current CLOCK_REALTIME time in seconds
+		 */
 		ts->tv_sec = tk->xtime_sec;
 		nsecs = timekeeping_get_ns(&tk->tkr_mono);
 
@@ -940,6 +1618,43 @@ EXPORT_SYMBOL_GPL(ktime_mono_to_any);
 /**
  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2418| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|551| <<intel_dp_hdcp2_read_msg>> msg_end = ktime_add_ms(ktime_get_raw(),
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|568| <<intel_dp_hdcp2_read_msg>> msg_expired = ktime_after(ktime_get_raw(), msg_end);
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|78| <<live_nop_switch>> times[0] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|103| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|114| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|158| <<live_nop_switch>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/gt/intel_rps.c|1738| <<vlv_c0_read>> ei->ktime = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3757| <<nop_virtual_engine>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3812| <<nop_virtual_engine>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/i915_pmu.c|164| <<ktime_since_raw>> return ktime_to_ns(ktime_sub(ktime_get_raw(), kt));
+ *   - drivers/gpu/drm/i915/i915_pmu.c|216| <<init_rc6>> pmu->sleep_last = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_pmu.c|225| <<park_rc6>> pmu->sleep_last = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_utils.h|268| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/i915/i915_utils.h|273| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|573| <<live_nop_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|601| <<live_nop_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1052| <<live_empty_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1065| <<live_empty_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|312| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|317| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|792| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|797| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/ptp/ptp_clockmatrix.c|438| <<_idtcm_gettime>> idtcm->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|750| <<_idtcm_set_dpll_hw_tod>> ktime_t diff = ktime_sub(ktime_get_raw(),
+ *   - drivers/ptp/ptp_clockmatrix.c|1006| <<set_tod_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|1013| <<set_tod_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|143| <<_idt82p33_gettime>> idt82p33->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|182| <<_idt82p33_settime>> dynamic_overhead_ns = ktime_to_ns(ktime_get_raw())
+ *   - drivers/ptp/ptp_idt82p33.c|292| <<idt82p33_measure_one_byte_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|297| <<idt82p33_measure_one_byte_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|325| <<idt82p33_measure_tod_write_9_byte_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|336| <<idt82p33_measure_tod_write_9_byte_overhead>> stop = ktime_get_raw();
+ *   - include/linux/timekeeping.h|174| <<ktime_get_raw_ns>> return ktime_to_ns(ktime_get_raw());
+ */
 ktime_t ktime_get_raw(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -948,6 +1663,17 @@ ktime_t ktime_get_raw(void)
 	u64 nsecs;
 
 	do {
+		/*
+		 * struct tk_read_base - base structure for timekeeping readout
+		 * @clock:      Current clocksource used for timekeeping.
+		 * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+		 * @cycle_last: @clock cycle value at last update
+		 * @mult:       (NTP adjusted) multiplier for scaled math conversion
+		 * @shift:      Shift value for scaled math conversion
+		 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+		 * @base:       ktime_t (nanoseconds) base time for readout
+		 * @base_real:  Nanoseconds base value for clock REALTIME readout
+		 */
 		seq = read_seqcount_begin(&tk_core.seq);
 		base = tk->tkr_raw.base;
 		nsecs = timekeeping_get_ns(&tk->tkr_raw);
@@ -966,8 +1692,21 @@ EXPORT_SYMBOL_GPL(ktime_get_raw);
  * clock and the wall_to_monotonic offset and stores the result
  * in normalized timespec64 format in the variable pointed to by @ts.
  */
+/*
+ * 被很多地方调用
+ */
 void ktime_get_ts64(struct timespec64 *ts)
 {
+	/*
+	 * kernel/time/timekeeping.c
+	 *
+	 * 50 static struct {
+	 * 51         seqcount_raw_spinlock_t seq;
+	 * 52         struct timekeeper       timekeeper;
+	 * 53 } tk_core ____cacheline_aligned = {
+	 * 54         .seq = SEQCNT_RAW_SPINLOCK_ZERO(tk_core.seq, &timekeeper_lock),
+	 * 55 };
+	 */
 	struct timekeeper *tk = &tk_core.timekeeper;
 	struct timespec64 tomono;
 	unsigned int seq;
@@ -977,8 +1716,20 @@ void ktime_get_ts64(struct timespec64 *ts)
 
 	do {
 		seq = read_seqcount_begin(&tk_core.seq);
+		/*
+		 * Current CLOCK_REALTIME time in seconds
+		 */
 		ts->tv_sec = tk->xtime_sec;
+		/*
+		 * 不会更新参数中结构的信息
+		 *
+		 * 应该是获得距离上一次采样的ns delta (基于MONOTONIC)
+		 * 实际是当前MONOTONIC的ns
+		 */
 		nsec = timekeeping_get_ns(&tk->tkr_mono);
+		/*
+		 * CLOCK_REALTIME to CLOCK_MONOTONIC offset
+		 */
 		tomono = tk->wall_to_monotonic;
 
 	} while (read_seqcount_retry(&tk_core.seq, seq));
@@ -1052,6 +1803,11 @@ noinstr time64_t __ktime_get_real_seconds(void)
  * ktime_get_snapshot - snapshots the realtime/monotonic raw clocks with counter
  * @systime_snapshot:	pointer to struct receiving the system time snapshot
  */
+/*
+ * called by:
+ *   - drivers/net/ethernet/intel/igc/igc_ptp.c|828| <<igc_phc_get_syncdevicetime>> ktime_get_snapshot(&adapter->snapshot);
+ *   - include/linux/pps_kernel.h|104| <<pps_get_ts>> ktime_get_snapshot(&snap);
+ */
 void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1083,6 +1839,11 @@ void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)
 }
 EXPORT_SYMBOL_GPL(ktime_get_snapshot);
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1616| <<adjust_historical_crosststamp>> ret = scale64_check_overflow(partial_history_cycles,
+ *   - kernel/time/timekeeping.c|1634| <<adjust_historical_crosststamp>> ret = scale64_check_overflow(partial_history_cycles,
+ */
 /* Scale base by mult/div checking for overflow */
 static int scale64_check_overflow(u64 mult, u64 div, u64 *base)
 {
@@ -1117,6 +1878,10 @@ static int scale64_check_overflow(u64 mult, u64 div, u64 *base)
  * count between the driver timestamp point and the start of the current
  * interval is partial_history_cycles.
  */
+/*
+ * 除了自己, 没人调用:
+ *   - kernel/time/timekeeping.c|2050| <<adjust_historical_crosststamp>> ret = adjust_historical_crosststamp(history_begin,
+ */
 static int adjust_historical_crosststamp(struct system_time_snapshot *history,
 					 u64 partial_history_cycles,
 					 u64 total_history_cycles,
@@ -1182,6 +1947,11 @@ static int adjust_historical_crosststamp(struct system_time_snapshot *history,
 /*
  * cycle_between - true if test occurs chronologically between before and after
  */
+/*
+ * 除了自己没人调用:
+ *   - kernel/time/timekeeping.c|2005| <<cycle_between>> if (!cycle_between(interval_start, cycles, now)) {
+ *   - kernel/time/timekeeping.c|2041| <<cycle_between>> !cycle_between(history_begin->cycles,
+ */
 static bool cycle_between(u64 before, u64 test, u64 after)
 {
 	if (test > before && test < after)
@@ -1202,6 +1972,15 @@ static bool cycle_between(u64 before, u64 test, u64 after)
  *
  * Reads a timestamp from a device and correlates it to system time
  */
+/*
+ * called by:
+ *   - drivers/net/ethernet/intel/e1000e/ptp.c|156| <<e1000e_phc_getcrosststamp>> return get_device_system_crosststamp(e1000e_phc_get_syncdevicetime,
+ *   - drivers/net/ethernet/intel/ice/ice_ptp.c|1686| <<ice_ptp_getcrosststamp_e822>> return get_device_system_crosststamp(ice_ptp_get_syncdevicetime,
+ *   - drivers/net/ethernet/intel/igc/igc_ptp.c|905| <<igc_ptp_getcrosststamp>> return get_device_system_crosststamp(igc_phc_get_syncdevicetime,
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_ptp.c|258| <<stmmac_getcrosststamp>> return get_device_system_crosststamp(stmmac_get_syncdevicetime,
+ *   - drivers/ptp/ptp_kvm_common.c|61| <<ptp_kvm_getcrosststamp>> return get_device_system_crosststamp(ptp_kvm_get_time_fn, NULL,
+ *   - sound/pci/hda/hda_controller.c|476| <<azx_get_crosststamp>> return get_device_system_crosststamp(azx_get_sync_time,
+ */
 int get_device_system_crosststamp(int (*get_time_fn)
 				  (ktime_t *device_time,
 				   struct system_counterval_t *sys_counterval,
@@ -1309,6 +2088,16 @@ EXPORT_SYMBOL_GPL(get_device_system_crosststamp);
  *
  * Sets the time of day to the new time and update NTP and notify hrtimers
  */
+/*
+ * called by:
+ *   - arch/arm/xen/enlighten.c|548| <<xen_pm_init>> do_settimeofday64(&ts);
+ *   - arch/x86/xen/time.c|506| <<xen_time_init>> do_settimeofday64(&tp);
+ *   - drivers/hv/hv_util.c|344| <<hv_set_host_time>> do_settimeofday64(&ts);
+ *   - drivers/rtc/class.c|83| <<rtc_hctosys>> err = do_settimeofday64(&tv64);
+ *   - kernel/time/time.c|95| <<SYSCALL_DEFINE1(stime)>> do_settimeofday64(&tv);
+ *   - kernel/time/time.c|133| <<SYSCALL_DEFINE1(stime32)>> do_settimeofday64(&tv);
+ *   - kernel/time/time.c|195| <<do_sys_settimeofday64>> return do_settimeofday64(tv);
+ */
 int do_settimeofday64(const struct timespec64 *ts)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1359,6 +2148,11 @@ EXPORT_SYMBOL(do_settimeofday64);
  *
  * Adds or subtracts an offset value from the current time.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|1847| <<timekeeping_warp_clock>> timekeeping_inject_offset(&adjust);
+ *   - kernel/time/timekeeping.c|3050| <<do_adjtimex>> ret = timekeeping_inject_offset(&delta);
+ */
 static int timekeeping_inject_offset(const struct timespec64 *ts)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1401,6 +2195,11 @@ static int timekeeping_inject_offset(const struct timespec64 *ts)
  * Indicates if there is an offset between the system clock and the hardware
  * clock/persistent clock/rtc.
  */
+/*
+ * 在以下使用persistent_clock_is_local:
+ *   - kernel/time/ntp.c|694| <<sync_hw_clock>> if (persistent_clock_is_local)
+ *   - kernel/time/timekeeping.c|2202| <<timekeeping_warp_clock>> persistent_clock_is_local = 1;
+ */
 int persistent_clock_is_local;
 
 /*
@@ -1419,6 +2218,10 @@ int persistent_clock_is_local;
  * as real UNIX machines always do it. This avoids all headaches about
  * daylight saving times and warping kernel clocks.
  */
+/*
+ * called by:
+ *   - kernel/time/time.c|191| <<do_sys_settimeofday64>> timekeeping_warp_clock();
+ */
 void timekeeping_warp_clock(void)
 {
 	if (sys_tz.tz_minuteswest != 0) {
@@ -1434,6 +2237,11 @@ void timekeeping_warp_clock(void)
 /*
  * __timekeeping_set_tai_offset - Sets the TAI offset from UTC and monotonic
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2970| <<accumulate_nsecs_to_secs>> __timekeeping_set_tai_offset(tk, tk->tai_offset - leap);
+ *   - kernel/time/timekeeping.c|3595| <<do_adjtimex>> __timekeeping_set_tai_offset(tk, tai);
+ */
 static void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)
 {
 	tk->tai_offset = tai_offset;
@@ -1445,6 +2253,10 @@ static void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)
  *
  * Accumulates current time interval and initializes new clocksource
  */
+/*
+ * 在以下使用change_clocksource():
+ *   - kernel/time/timekeeping.c|1797| <<timekeeping_notify>> stop_machine(change_clocksource, clock, NULL);
+ */
 static int change_clocksource(void *data)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1472,6 +2284,13 @@ static int change_clocksource(void *data)
 
 	if (change) {
 		old = tk->tkr_mono.clock;
+		/*
+		 * called by:
+		 *   - kernel/time/timekeeping.c|1588| <<change_clocksource>> tk_setup_internals(tk, new);
+		 *   - kernel/time/timekeeping.c|1795| <<timekeeping_init>> tk_setup_internals(tk, clock);
+		 *
+		 * new的类型是clocksource
+		 */
 		tk_setup_internals(tk, new);
 	}
 
@@ -1497,6 +2316,10 @@ static int change_clocksource(void *data)
  * This function is called from clocksource.c after a new, better clock
  * source has been registered. The caller holds the clocksource_mutex.
  */
+/*
+ * called by:
+ *   - kernel/time/clocksource.c|1094| <<__clocksource_select>> if (curr_clocksource != best && !timekeeping_notify(best)) {
+ */
 int timekeeping_notify(struct clocksource *clock)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1514,6 +2337,24 @@ int timekeeping_notify(struct clocksource *clock)
  *
  * Returns the raw monotonic time (completely un-modified by ntp)
  */
+/*
+ * called by:
+ *   - drivers/firewire/core-cdev.c|1226| <<ioctl_get_cycle_timer2>> case CLOCK_MONOTONIC_RAW: ktime_get_raw_ts64(&ts); break;
+ *   - drivers/net/wireless/ath/ath9k/channel.c|238| <<chanctx_event_delta>> ktime_get_raw_ts64(&ts);
+ *   - drivers/net/wireless/ath/ath9k/channel.c|349| <<ath_chanctx_adjust_tbtt_delta>> ktime_get_raw_ts64(&ts);
+ *   - drivers/net/wireless/ath/ath9k/channel.c|1263| <<ath_chanctx_set_next>> ktime_get_raw_ts64(&ts);
+ *   - drivers/net/wireless/ath/ath9k/channel.c|1280| <<ath_chanctx_set_next>> ktime_get_raw_ts64(&sc->cur_chan->tsf_ts);
+ *   - drivers/net/wireless/ath/ath9k/hw.c|1856| <<ath9k_hw_get_tsf_offset>> ktime_get_raw_ts64(&ts);
+ *   - drivers/net/wireless/ath/ath9k/hw.c|1920| <<ath9k_hw_reset>> ktime_get_raw_ts64(&tsf_ts);
+ *   - drivers/net/wireless/ath/ath9k/main.c|1978| <<ath9k_set_tsf>> ktime_get_raw_ts64(&avp->chanctx->tsf_ts);
+ *   - drivers/net/wireless/ath/ath9k/main.c|1994| <<ath9k_reset_tsf>> ktime_get_raw_ts64(&avp->chanctx->tsf_ts);
+ *   - drivers/rtc/rtc-meson-vrtc.c|26| <<meson_vrtc_read_time>> ktime_get_raw_ts64(&time);
+ *   - drivers/rtc/rtc-meson-vrtc.c|99| <<meson_vrtc_suspend>> ktime_get_raw_ts64(&time);
+ *   - fs/nfsd/nfssvc.c|374| <<nfsd_reset_write_verifier_locked>> ktime_get_raw_ts64(&now);
+ *   - include/sound/pcm.h|1238| <<snd_pcm_gettime>> ktime_get_raw_ts64(tv);
+ *   - kernel/time/posix-timers.c|214| <<posix_get_monotonic_raw>> ktime_get_raw_ts64(tp);
+ *   - sound/core/rawmidi.c|1091| <<get_framing_tstamp>> ktime_get_raw_ts64(&ts64);
+ */
 void ktime_get_raw_ts64(struct timespec64 *ts)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1536,6 +2377,11 @@ EXPORT_SYMBOL(ktime_get_raw_ts64);
 /**
  * timekeeping_valid_for_hres - Check if timekeeping is suitable for hres
  */
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|174| <<tick_handle_periodic>> if (timekeeping_valid_for_hres())
+ *   - kernel/time/tick-sched.c|1598| <<tick_check_oneshot_change>> if (!timekeeping_valid_for_hres() || !tick_is_oneshot_available())
+ */
 int timekeeping_valid_for_hres(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1555,6 +2401,10 @@ int timekeeping_valid_for_hres(void)
 /**
  * timekeeping_max_deferment - Returns max time the clocksource can be deferred
  */
+/*
+ * called by:
+ *   - kernel/time/tick-sched.c|851| <<tick_nohz_next_event>> delta = timekeeping_max_deferment();
+ */
 u64 timekeeping_max_deferment(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1629,6 +2479,10 @@ static bool persistent_clock_exists;
 /*
  * timekeeping_init - Initializes the clocksource and common timekeeping values
  */
+/*
+ * called by:
+ *   - init/main.c|1037| <<start_kernel>> timekeeping_init();
+ */
 void __init timekeeping_init(void)
 {
 	struct timespec64 wall_time, boot_offset, wall_to_mono;
@@ -1658,9 +2512,17 @@ void __init timekeeping_init(void)
 	write_seqcount_begin(&tk_core.seq);
 	ntp_init();
 
+	/*
+	 * 似乎x86返回clocksource_jiffies
+	 */
 	clock = clocksource_default_clock();
 	if (clock->enable)
 		clock->enable(clock);
+	/*
+	 * called by:
+	 *   - kernel/time/timekeeping.c|1588| <<change_clocksource>> tk_setup_internals(tk, new);
+	 *   - kernel/time/timekeeping.c|1795| <<timekeeping_init>> tk_setup_internals(tk, clock);
+	 */
 	tk_setup_internals(tk, clock);
 
 	tk_set_xtime(tk, &wall_time);
@@ -1685,6 +2547,11 @@ static struct timespec64 timekeeping_suspend_time;
  * Takes a timespec offset measuring a suspend interval and properly
  * adds the sleep offset to the timekeeping variables.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2257| <<timekeeping_inject_sleeptime64>> __timekeeping_inject_sleeptime(tk, delta);
+ *   - kernel/time/timekeeping.c|2313| <<timekeeping_resume>> __timekeeping_inject_sleeptime(tk, &ts_delta);
+ */
 static void __timekeeping_inject_sleeptime(struct timekeeper *tk,
 					   const struct timespec64 *delta)
 {
@@ -1747,6 +2614,10 @@ bool timekeeping_rtc_skipsuspend(void)
  * This function should only be called by rtc_resume(), and allows
  * a suspend offset to be injected into the timekeeping values.
  */
+/*
+ * called by:
+ *   - drivers/rtc/class.c|191| <<rtc_resume>> timekeeping_inject_sleeptime64(&sleep_time);
+ */
 void timekeeping_inject_sleeptime64(const struct timespec64 *delta)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1774,6 +2645,11 @@ void timekeeping_inject_sleeptime64(const struct timespec64 *delta)
 /**
  * timekeeping_resume - Resumes the generic timekeeping subsystem.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2780| <<global>> struct syscore_ops timekeeping_syscore_ops.resume = timekeeping_resume,
+ *   - kernel/time/tick-common.c|590| <<tick_unfreeze>> timekeeping_resume();
+ */
 void timekeeping_resume(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1836,6 +2712,10 @@ void timekeeping_resume(void)
 	timerfd_resume();
 }
 
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|564| <<tick_freeze>> timekeeping_suspend();
+ */
 int timekeeping_suspend(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1921,6 +2801,14 @@ device_initcall(timekeeping_init_ops);
 /*
  * Apply a multiplier adjustment to the timekeeper
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2480| <<timekeeping_adjust>> timekeeping_apply_adjustment(tk, offset, mult - tk->tkr_mono.mult);
+ *
+ * timekeeping_advance()
+ * -> timekeeping_adjust()
+ *    -> timekeeping_apply_adjustment()
+ */
 static __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,
 							 s64 offset,
 							 s32 mult_adj)
@@ -1999,6 +2887,16 @@ static __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,
  * Adjust the timekeeper's multiplier to the correct frequency
  * and also to reduce the accumulated error value.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2699| <<timekeeping_advance>> timekeeping_adjust(tk, offset);
+ *
+ * timekeeping_advance()
+ * -> timekeeping_adjust()
+ *    -> timekeeping_apply_adjustment()
+ *
+ * 大概率只是修改mono的mult, 没有raw
+ */
 static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
 {
 	u32 mult;
@@ -2024,6 +2922,17 @@ static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
 	tk->ntp_err_mult = tk->ntp_error > 0 ? 1 : 0;
 	mult += tk->ntp_err_mult;
 
+	/*
+	 * 在以下设置tk->tkr_raw.mult:
+	 *   - kernel/time/timekeeping.c|711| <<tk_setup_internals>> tk->tkr_raw.mult = clock->mult;
+	 * 在以下设置tk->tkr_raw.shift:
+	 *   - kernel/time/timekeeping.c|699| <<tk_setup_internals>> tk->tkr_raw.shift = clock->shift;
+	 * 在以下设置tk->tkr_mono.mult:
+	 *   - kernel/time/timekeeping.c|710| <<tk_setup_internals>> tk->tkr_mono.mult = clock->mult;
+	 *   - kernel/time/timekeeping.c|2659| <<timekeeping_apply_adjustment>> tk->tkr_mono.mult += mult_adj;
+	 * 在以下设置tk->tkr_mono.shift:
+	 *   - kernel/time/timekeeping.c|698| <<tk_setup_internals>> tk->tkr_mono.shift = clock->shift;
+	 */
 	timekeeping_apply_adjustment(tk, offset, mult - tk->tkr_mono.mult);
 
 	if (unlikely(tk->tkr_mono.clock->maxadj &&
@@ -2048,6 +2957,9 @@ static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
 	if (unlikely((s64)tk->tkr_mono.xtime_nsec < 0)) {
 		tk->tkr_mono.xtime_nsec += (u64)NSEC_PER_SEC <<
 							tk->tkr_mono.shift;
+		/*
+		 * raw的是raw_sec
+		 */
 		tk->xtime_sec--;
 		tk->skip_second_overflow = 1;
 	}
@@ -2060,6 +2972,13 @@ static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
  * from the xtime_nsec field to the xtime_secs field.
  * It also calls into the NTP code to handle leapsecond processing.
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2424| <<logarithmic_accumulation>> *clock_set |= accumulate_nsecs_to_secs(tk);
+ *   - kernel/time/timekeeping.c|2503| <<timekeeping_advance>> clock_set |= accumulate_nsecs_to_secs(tk);
+ *
+ * 大概率只修改mono, 没有raw (from the xtime_nsec field to the xtime_secs field)
+ */
 static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)
 {
 	u64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
@@ -2069,6 +2988,9 @@ static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)
 		int leap;
 
 		tk->tkr_mono.xtime_nsec -= nsecps;
+		/*
+		 * raw是raw_sec
+		 */
 		tk->xtime_sec++;
 
 		/*
@@ -2100,6 +3022,34 @@ static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)
 	return clock_set;
 }
 
+/*
+ * struct tk_read_base - base structure for timekeeping readout
+ * @clock:      Current clocksource used for timekeeping.
+ * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+ * @cycle_last: @clock cycle value at last update
+ * @mult:       (NTP adjusted) multiplier for scaled math conversion
+ * @shift:      Shift value for scaled math conversion
+ * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+ * @base:       ktime_t (nanoseconds) base time for readout
+ * @base_real:  Nanoseconds base value for clock REALTIME readout
+ *
+ * 在以下修改timekeeper->tkr_raw的内容:
+ *   - kernel/time/timekeeping.c|255| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+ *   - kernel/time/timekeeping.c|536| <<tk_setup_internals>> tk->tkr_raw.clock = clock;
+ *   - kernel/time/timekeeping.c|537| <<tk_setup_internals>> tk->tkr_raw.mask = clock->mask;
+ *   - kernel/time/timekeeping.c|538| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+ *   - kernel/time/timekeeping.c|567| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+ *   - kernel/time/timekeeping.c|570| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+ *   - kernel/time/timekeeping.c|575| <<tk_setup_internals>> tk->tkr_raw.shift = clock->shift;
+ *   - kernel/time/timekeeping.c|587| <<tk_setup_internals>> tk->tkr_raw.mult = clock->mult;
+ *   - kernel/time/timekeeping.c|1062| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+ *   - kernel/time/timekeeping.c|1138| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+ *   - kernel/time/timekeeping.c|1141| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+ *   - kernel/time/timekeeping.c|2276| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now;
+ *   - kernel/time/timekeeping.c|2599| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+ *   - kernel/time/timekeeping.c|2610| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+ *   - kernel/time/timekeeping.c|2613| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+ */
 /*
  * logarithmic_accumulation - shifted accumulation of cycles
  *
@@ -2109,9 +3059,40 @@ static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)
  *
  * Returns the unconsumed cycles.
  */
+/*
+ * 在以下设置tk->tkr_raw.mult:
+ *   - kernel/time/timekeeping.c|711| <<tk_setup_internals>> tk->tkr_raw.mult = clock->mult;
+ * 在以下设置tk->tkr_raw.shift:
+ *   - kernel/time/timekeeping.c|699| <<tk_setup_internals>> tk->tkr_raw.shift = clock->shift;
+ * 在以下设置tk->tkr_mono.mult:
+ *   - kernel/time/timekeeping.c|710| <<tk_setup_internals>> tk->tkr_mono.mult = clock->mult;
+ *   - kernel/time/timekeeping.c|2659| <<timekeeping_apply_adjustment>> tk->tkr_mono.mult += mult_adj;
+ * 在以下设置tk->tkr_mono.shift:
+ *   - kernel/time/timekeeping.c|698| <<tk_setup_internals>> tk->tkr_mono.shift = clock->shift;
+ *
+ * called by (唯一的调用!!!!!!):
+ *   - kernel/time/timekeeping.c|2189| <<timekeeping_advance>> offset = logarithmic_accumulation(tk, offset, shift,
+ *
+ * do_adjtimex() or update_wall_time()
+ * -> timekeeping_advance()
+ *    -> logarithmic_accumulation()
+ */
 static u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,
 				    u32 shift, unsigned int *clock_set)
 {
+	/*
+	 * 在以下使用cycle_interval:
+	 *   - kernel/time/timekeeping.c|555| <<tk_setup_internals>> tk->cycle_interval = interval;
+	 *   - kernel/time/timekeeping.c|2381| <<timekeeping_apply_adjustment>> s64 interval = tk->cycle_interval;
+	 *   - kernel/time/timekeeping.c|2468| <<timekeeping_adjust>> tk->xtime_remainder, tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2577| <<logarithmic_accumulation>> u64 interval = tk->cycle_interval << shift;
+	 *   - kernel/time/timekeeping.c|2665| <<timekeeping_advance>> if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
+	 *   - kernel/time/timekeeping.c|2682| <<timekeeping_advance>> shift = ilog2(offset) - ilog2(tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2687| <<timekeeping_advance>> while (offset >= tk->cycle_interval) {
+	 *   - kernel/time/timekeeping.c|2694| <<timekeeping_advance>> if (offset < tk->cycle_interval<<shift)
+	 *
+	 * 这个shift是倍数
+	 */
 	u64 interval = tk->cycle_interval << shift;
 	u64 snsec_per_sec;
 
@@ -2119,37 +3100,196 @@ static u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,
 	if (offset < interval)
 		return offset;
 
+	/*
+	 * 在以下修改tk_read_base->cycle_last:
+	 *   - kernel/time/timekeeping.c|327| <<tk_setup_internals>> tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
+	 *   - kernel/time/timekeeping.c|331| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+	 *   - kernel/time/timekeeping.c|814| <<timekeeping_forward_now>> tk->tkr_mono.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1864| <<timekeeping_resume>> tk->tkr_mono.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1865| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now
+	 *   - kernel/time/timekeeping.c|2170| <<logarithmic_accumulation>> tk->tkr_mono.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2171| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2217| <<timekeeping_advance>> tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
+	 *
+	 * struct tk_read_base {
+	 *     struct clocksource      *clock;
+	 *     u64                     mask;
+	 *     u64                     cycle_last;
+	 *     u32                     mult;
+	 *     u32                     shift;
+	 *     u64                     xtime_nsec;
+	 *     ktime_t                 base;
+	 *     u64                     base_real;
+	 * };
+	 */
 	/* Accumulate one shifted interval */
+	/*
+	 * 只在这里更新offset
+	 */
 	offset -= interval;
 	tk->tkr_mono.cycle_last += interval;
 	tk->tkr_raw.cycle_last  += interval;
 
+	/*
+	 * 在以下使用timekeeper->xtime_interval:
+	 *   - kernel/time/timekeeping.c|558| <<tk_setup_internals>> tk->xtime_interval = interval * clock->mult;
+	 *   - kernel/time/timekeeping.c|559| <<tk_setup_internals>> tk->xtime_remainder = ntpinterval - tk->xtime_interval;
+	 *   - kernel/time/timekeeping.c|2459| <<timekeeping_apply_adjustment>> tk->xtime_interval += interval;
+	 *   - kernel/time/timekeeping.c|2667| <<logarithmic_accumulation>> tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
+	 *   - kernel/time/timekeeping.c|2694| <<logarithmic_accumulation>> tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<
+	 *
+	 * @cycle_last: @clock cycle value at last update
+	 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+	 *
+	 * shift是倍数
+	 * tk->xtime_interval是cycle*shift(adjusted后的???)
+	 */
 	tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
+	/*
+	 * called by:
+	 *   - kernel/time/timekeeping.c|2424| <<logarithmic_accumulation>> *clock_set |= accumulate_nsecs_to_secs(tk);
+	 *   - kernel/time/timekeeping.c|2503| <<timekeeping_advance>> clock_set |= accumulate_nsecs_to_secs(tk);
+	 *
+	 * 似乎主要是mono, 没raw
+	 */
 	*clock_set |= accumulate_nsecs_to_secs(tk);
 
+	/*
+	 * 在以下设置tk_read_base->xtime_nsec (非mono):
+	 *   - kernel/time/timekeeping.c|129| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+	 *   - kernel/time/timekeeping.c|384| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+	 *   - kernel/time/timekeeping.c|387| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+	 *   - kernel/time/timekeeping.c|902| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+	 *   - kernel/time/timekeeping.c|2279| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 *   - kernel/time/timekeeping.c|2282| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+	 *
+	 * 在以下使用timekeeper->raw_interval:
+	 *   - kernel/time/timekeeping.c|499| <<tk_setup_internals>> tk->raw_interval = interval * clock->mult;
+	 *   - kernel/time/timekeeping.c|2432| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+	 */
 	/* Accumulate raw time */
+	/*
+	 * shift是倍数
+	 */
 	tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
 	snsec_per_sec = (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
 	while (tk->tkr_raw.xtime_nsec >= snsec_per_sec) {
 		tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+		/*
+		 * 只在以下修改tk->tkr_raw.base:
+		 *   - kernel/time/timekeeping.c|1150| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+		 *
+		 * 在以下使用timekeeper->raw_sec:
+		 *   - kernel/time/timekeeping.c|225| <<tk_normalize_xtime>> tk->raw_sec++;
+		 *   - kernel/time/timekeeping.c|1031| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+		 *   - kernel/time/timekeeping.c|1934| <<ktime_get_raw_ts64>> ts->tv_sec = tk->raw_sec;
+		 *   - kernel/time/timekeeping.c|2089| <<timekeeping_init>> tk->raw_sec = 0;
+		 *   - kernel/time/timekeeping.c|2583| <<logarithmic_accumulation>> tk->raw_sec++;
+		 *   - kernel/time/vsyscall.c|63| <<update_vdso_data>> vdso_ts->sec = tk->raw_sec;
+		 */
 		tk->raw_sec++;
 	}
 
+	/*
+	 * 在以下使用timekeeper->ntp_error:
+	 *   - kernel/time/timekeeping.c|792| <<tk_setup_internals>> tk->ntp_error = 0;
+	 *   - kernel/time/timekeeping.c|1394| <<timekeeping_update>> tk->ntp_error = 0;
+	 *   - kernel/time/timekeeping.c|2699| <<timekeeping_resume>> tk->ntp_error = 0;
+	 *   - kernel/time/timekeeping.c|2920| <<timekeeping_adjust>> tk->ntp_err_mult = tk->ntp_error > 0 ? 1 : 0;
+	 *   - kernel/time/timekeeping.c|3187| <<logarithmic_accumulation>> tk->ntp_error += tk->ntp_tick << shift;
+	 *   - kernel/time/timekeeping.c|3188| <<logarithmic_accumulation>> tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<
+	 */
 	/* Accumulate error between NTP and clock interval */
 	tk->ntp_error += tk->ntp_tick << shift;
 	tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<
 						(tk->ntp_error_shift + shift);
 
+	/*
+	 * 返回的是剩下的没加上的cycles
+	 */
 	return offset;
 }
 
+/*
+ * 一个计算的例子.
+ *
+ * struct timekeeper *tk_local = &tk_core.timekeeper;
+ * static u64 old_time = 0;
+ * u64 nsec, time;
+ * u64 tsc;
+ *
+ * write_seqcount_begin(&tk_core.seq);
+ *
+ * tsc = (u64)rdtsc_ordered();
+ * nsec = timekeeping_cycles_to_ns(&tk_local->tkr_raw, tsc);
+ * time = ktime_add_ns(tk_local->tkr_raw.base, nsec);
+ *
+ * write_seqcount_end(&tk_core.seq);
+ *
+ *
+ * 1. 获得tsc.
+ * 2. 计算tsc和cycle_last
+ * 3. nsec = delta * tkr->mult + tkr->xtime_nsec;
+ * 4. nsec >>= tkr->shift;
+ * 5. 把nsec加到tk_local->tkr_raw.base返回
+ *
+ * 在以下修改tk_read_base->cycle_last (非mono!!!):
+ *   - kernel/time/timekeeping.c|331| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+ *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+ *   - kernel/time/timekeeping.c|1865| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now
+ *   - kernel/time/timekeeping.c|2171| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+ *
+ * 在以下设置tk_read_base->xtime_nsec (非mono!!!):
+ *   - kernel/time/timekeeping.c|129| <<tk_normalize_xtime>> tk->tkr_raw.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_raw.shift;
+ *   - kernel/time/timekeeping.c|384| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec >>= -shift_change;
+ *   - kernel/time/timekeeping.c|387| <<tk_setup_internals>> tk->tkr_raw.xtime_nsec <<= shift_change;
+ *   - kernel/time/timekeeping.c|902| <<timekeeping_forward_now>> tk->tkr_raw.xtime_nsec += delta * tk->tkr_raw.mult;
+ *   - kernel/time/timekeeping.c|2279| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;
+ *   - kernel/time/timekeeping.c|2282| <<logarithmic_accumulation>> tk->tkr_raw.xtime_nsec -= snsec_per_sec;
+ *
+ * 在以下修改tk->tkr_raw.base:
+ *   - kernel/time/timekeeping.c|1150| <<tk_update_ktime_data>> tk->tkr_raw.base = ns_to_ktime(tk->raw_sec * NSEC_PER_SEC);
+ *
+ * 在以下使用timekeeper->raw_sec:
+ *   - kernel/time/timekeeping.c|225| <<tk_normalize_xtime>> tk->raw_sec++;
+ *   - kernel/time/timekeeping.c|2089| <<timekeeping_init>> tk->raw_sec = 0;
+ *   - kernel/time/timekeeping.c|2583| <<logarithmic_accumulation>> tk->raw_sec++;
+ *
+ *
+ * 1. tk_normalize_xtime()在简单的测试中只在change clocksource的时候调用
+ *
+ * 2. timekeeping_forward_now()在简单的测试中只在change clocksource的时候调用
+ *
+ *
+ *
+ * 在以下设置tk->tkr_raw.mult:
+ *   - kernel/time/timekeeping.c|711| <<tk_setup_internals>> tk->tkr_raw.mult = clock->mult;
+ * 在以下设置tk->tkr_raw.shift:
+ *   - kernel/time/timekeeping.c|699| <<tk_setup_internals>> tk->tkr_raw.shift = clock->shift;
+ * 在以下设置tk->tkr_mono.mult:
+ *   - kernel/time/timekeeping.c|710| <<tk_setup_internals>> tk->tkr_mono.mult = clock->mult;
+ *   - kernel/time/timekeeping.c|2659| <<timekeeping_apply_adjustment>> tk->tkr_mono.mult += mult_adj;
+ * 在以下设置tk->tkr_mono.shift:
+ *   - kernel/time/timekeeping.c|698| <<tk_setup_internals>> tk->tkr_mono.shift = clock->shift;
+ */
 /*
  * timekeeping_advance - Updates the timekeeper to the current time and
  * current NTP tick length
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2231| <<update_wall_time>> if (timekeeping_advance(TK_ADV_TICK))
+ *   - kernel/time/timekeeping.c|2476| <<do_adjtimex>> clock_set |= timekeeping_advance(TK_ADV_FREQ);
+ */
 static bool timekeeping_advance(enum timekeeping_adv_mode mode)
 {
 	struct timekeeper *real_tk = &tk_core.timekeeper;
+	/*
+	 * 在以下使用shadow_timekeeper:
+	 *   - kernel/time/timekeeping.c|872| <<timekeeping_update>> memcpy(&shadow_timekeeper, &tk_core.timekeeper,
+	 *   - kernel/time/timekeeping.c|2306| <<timekeeping_advance>> struct timekeeper *tk = &shadow_timekeeper;
+	 */
 	struct timekeeper *tk = &shadow_timekeeper;
 	u64 offset;
 	int shift = 0, maxshift;
@@ -2162,9 +3302,34 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
 	if (unlikely(timekeeping_suspended))
 		goto out;
 
+	/*
+	 * 在以下修改tk_read_base->cycle_last:
+	 *   - kernel/time/timekeeping.c|327| <<tk_setup_internals>> tk->tkr_mono.cycle_last = tk_clock_read(&tk->tkr_mono);
+	 *   - kernel/time/timekeeping.c|331| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+	 *   - kernel/time/timekeeping.c|814| <<timekeeping_forward_now>> tk->tkr_mono.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1864| <<timekeeping_resume>> tk->tkr_mono.cycle_last = cycle_now;
+	 *   - kernel/time/timekeeping.c|1865| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now
+	 *   - kernel/time/timekeeping.c|2170| <<logarithmic_accumulation>> tk->tkr_mono.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2171| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+	 *   - kernel/time/timekeeping.c|2217| <<timekeeping_advance>> tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
+	 */
 	offset = clocksource_delta(tk_clock_read(&tk->tkr_mono),
 				   tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
 
+	/*
+	 * 在以下使用timekeeper->cycle_interval:
+	 *   - kernel/time/timekeeping.c|555| <<tk_setup_internals>> tk->cycle_interval = interval;
+	 *   - kernel/time/timekeeping.c|2381| <<timekeeping_apply_adjustment>> s64 interval = tk->cycle_interval;
+	 *   - kernel/time/timekeeping.c|2468| <<timekeeping_adjust>> tk->xtime_remainder, tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2577| <<logarithmic_accumulation>> u64 interval = tk->cycle_interval << shift;
+	 *   - kernel/time/timekeeping.c|2665| <<timekeeping_advance>> if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
+	 *   - kernel/time/timekeeping.c|2682| <<timekeeping_advance>> shift = ilog2(offset) - ilog2(tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2687| <<timekeeping_advance>> while (offset >= tk->cycle_interval) {
+	 *   - kernel/time/timekeeping.c|2694| <<timekeeping_advance>> if (offset < tk->cycle_interval<<shift)
+	 *
+	 * cycle_interval是没有乘以过mult的
+	 */
 	/* Check if there's really nothing to do */
 	if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
 		goto out;
@@ -2180,18 +3345,53 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
 	 * chunk in one go, and then try to consume the next smaller
 	 * doubled multiple.
 	 */
+	/*
+	 * log base 2 of 32-bit or a 64-bit unsigned value
+	 */
 	shift = ilog2(offset) - ilog2(tk->cycle_interval);
 	shift = max(0, shift);
 	/* Bound shift to one less than what overflows tick_length */
 	maxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;
 	shift = min(shift, maxshift);
+	/*
+	 * 在以下使用timekeeper->cycle_interval:
+	 *   - kernel/time/timekeeping.c|555| <<tk_setup_internals>> tk->cycle_interval = interval;
+	 *   - kernel/time/timekeeping.c|2381| <<timekeeping_apply_adjustment>> s64 interval = tk->cycle_interval;
+	 *   - kernel/time/timekeeping.c|2468| <<timekeeping_adjust>> tk->xtime_remainder, tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2577| <<logarithmic_accumulation>> u64 interval = tk->cycle_interval << shift;
+	 *   - kernel/time/timekeeping.c|2665| <<timekeeping_advance>> if (offset < real_tk->cycle_interval && mode == TK_ADV_TICK)
+	 *   - kernel/time/timekeeping.c|2682| <<timekeeping_advance>> shift = ilog2(offset) - ilog2(tk->cycle_interval);
+	 *   - kernel/time/timekeeping.c|2687| <<timekeeping_advance>> while (offset >= tk->cycle_interval) {
+	 *   - kernel/time/timekeeping.c|2694| <<timekeeping_advance>> if (offset < tk->cycle_interval<<shift)
+	 */
 	while (offset >= tk->cycle_interval) {
+		/*
+		 * 在以下修改tk_read_base->cycle_last (非mono!!!):
+		 *   - kernel/time/timekeeping.c|331| <<tk_setup_internals>> tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;
+		 *   - kernel/time/timekeeping.c|815| <<timekeeping_forward_now>> tk->tkr_raw.cycle_last = cycle_now;
+		 *   - kernel/time/timekeeping.c|1865| <<timekeeping_resume>> tk->tkr_raw.cycle_last = cycle_now
+		 *   - kernel/time/timekeeping.c|2171| <<logarithmic_accumulation>> tk->tkr_raw.cycle_last += interval;
+		 *
+		 * called by:
+		 *   - kernel/time/timekeeping.c|2189| <<timekeeping_advance>> offset = logarithmic_accumulation(tk, offset, shift,
+		 *
+		 * logarithmic_accumulation - shifted accumulation of cycles
+		 *
+		 * This functions accumulates a shifted interval of cycles into
+		 * a shifted interval nanoseconds. Allows for O(log) accumulation
+		 * loop.
+		 *
+		 * Returns the unconsumed cycles. 返回的是剩下的没加上的cycles
+		 */
 		offset = logarithmic_accumulation(tk, offset, shift,
 							&clock_set);
 		if (offset < tk->cycle_interval<<shift)
 			shift--;
 	}
 
+	/*
+	 * 大概率只是修改mono的mult, 没有raw
+	 */
 	/* Adjust the multiplier to correct NTP error */
 	timekeeping_adjust(tk, offset);
 
@@ -2199,6 +3399,13 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
 	 * Finally, make sure that after the rounding
 	 * xtime_nsec isn't larger than NSEC_PER_SEC
 	 */
+	/*
+	 * called by:
+	 *   - kernel/time/timekeeping.c|2424| <<logarithmic_accumulation>> *clock_set |= accumulate_nsecs_to_secs(tk);
+	 *   - kernel/time/timekeeping.c|2503| <<timekeeping_advance>> clock_set |= accumulate_nsecs_to_secs(tk);
+	 *
+	 * 大概率只修改mono, 没有raw (from the xtime_nsec field to the xtime_secs field)
+	 */
 	clock_set |= accumulate_nsecs_to_secs(tk);
 
 	write_seqcount_begin(&tk_core.seq);
@@ -2212,6 +3419,18 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
 	 * memcpy under the tk_core.seq against one before we start
 	 * updating.
 	 */
+	/*
+	 * 一个例子:
+	 * update_wall_time() or do_adjtimex()
+	 * -> timekeeping_advance()
+	 *    -> timekeeping_update()
+	 *       -> tk_update_ktime_data()
+	 *
+	 * 部分更新的:
+	 * - tk->tkr_mono.base
+	 * - tk->ktime_sec
+	 * - tk->tkr_raw.base
+	 */
 	timekeeping_update(tk, clock_set);
 	memcpy(real_tk, tk, sizeof(*tk));
 	/* The memcpy must come last. Do not put anything here! */
@@ -2226,6 +3445,12 @@ static bool timekeeping_advance(enum timekeeping_adv_mode mode)
  * update_wall_time - Uses the current clocksource to increment the wall time
  *
  */
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|97| <<tick_periodic>> update_wall_time();
+ *   - kernel/time/tick-legacy.c|33| <<legacy_timer_tick>> update_wall_time();
+ *   - kernel/time/tick-sched.c|151| <<tick_do_update_jiffies64>> update_wall_time();
+ */
 void update_wall_time(void)
 {
 	if (timekeeping_advance(TK_ADV_TICK))
@@ -2243,6 +3468,14 @@ void update_wall_time(void)
  * basically means that however wrong your real time clock is at boot time,
  * you get the right time here).
  */
+/*
+ * called by:
+ *   - fs/proc/stat.c|121| <<show_stat>> getboottime64(&boottime);
+ *   - include/linux/sunrpc/cache.h|160| <<seconds_since_boot>> getboottime64(&boot);
+ *   - include/linux/sunrpc/cache.h|167| <<convert_to_wallclock>> getboottime64(&boot);
+ *   - include/linux/sunrpc/cache.h|312| <<get_expiry>> getboottime64(&boot);
+ *   - net/sunrpc/auth_gss/svcauth_gss.c|1331| <<gss_proxy_save_rsc>> getboottime64(&boot);
+ */
 void getboottime64(struct timespec64 *ts)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -2286,6 +3519,11 @@ EXPORT_SYMBOL(ktime_get_coarse_ts64);
 /*
  * Must hold jiffies_lock
  */
+/*
+ * called by:
+ *   - kernel/time/tick-common.c|117| <<tick_periodic>> do_timer(1);
+ *   - kernel/time/tick-legacy.c|30| <<legacy_timer_tick>> do_timer(ticks);
+ */
 void do_timer(unsigned long ticks)
 {
 	jiffies_64 += ticks;
@@ -2339,6 +3577,10 @@ ktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,
 /*
  * timekeeping_validate_timex - Ensures the timex is ok for use in do_adjtimex
  */
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|3149| <<do_adjtimex>> ret = timekeeping_validate_timex(txc);
+ */
 static int timekeeping_validate_timex(const struct __kernel_timex *txc)
 {
 	if (txc->modes & ADJ_ADJTIME) {
@@ -2405,6 +3647,16 @@ static int timekeeping_validate_timex(const struct __kernel_timex *txc)
  * random_get_entropy_fallback - Returns the raw clock source value,
  * used by random.c for platforms with no valid random_get_entropy().
  */
+/*
+ * called by:
+ *   - arch/arm/include/asm/timex.h|14| <<random_get_entropy>> #define random_get_entropy() (((unsigned long )get_cycles()) ?: random_get_entropy_fallback())
+ *   - arch/m68k/include/asm/timex.h|38| <<random_get_entropy>> return random_get_entropy_fallback();
+ *   - arch/mips/include/asm/timex.h|96| <<random_get_entropy>> return (random_get_entropy_fallback() << 6) | (0x3f - c0_random);
+ *   - arch/nios2/include/asm/timex.h|13| <<random_get_entropy>> #define random_get_entropy() (((unsigned long )get_cycles()) ?: random_get_entropy_fallback())
+ *   - arch/riscv/include/asm/timex.h|44| <<random_get_entropy>> return random_get_entropy_fallback();
+ *   - arch/x86/include/asm/timex.h|12| <<random_get_entropy>> return random_get_entropy_fallback();
+ *   - include/linux/timex.h|85| <<random_get_entropy>> #define random_get_entropy() random_get_entropy_fallback()
+ */
 unsigned long random_get_entropy_fallback(void)
 {
 	struct tk_read_base *tkr = &tk_core.timekeeper.tkr_mono;
@@ -2419,6 +3671,18 @@ EXPORT_SYMBOL_GPL(random_get_entropy_fallback);
 /**
  * do_adjtimex() - Accessor function to NTP __do_adjtimex function
  */
+/*
+ * called by:
+ *   - arch/alpha/kernel/osf_sys.c|1201| <<SYSCALL_DEFINE1(old_adjtimex)>> ret = do_adjtimex(&txc);
+ *   - arch/s390/kernel/time.c|595| <<stp_clear_leap>> ret = do_adjtimex(&txc);
+ *   - arch/s390/kernel/time.c|601| <<stp_clear_leap>> return do_adjtimex(&txc);
+ *   - arch/s390/kernel/time.c|641| <<stp_check_leap>> ret = do_adjtimex(&txc);
+ *   - arch/s390/kernel/time.c|650| <<stp_check_leap>> ret = do_adjtimex(&txc);
+ *   - arch/sparc/kernel/sys_sparc_64.c|569| <<SYSCALL_DEFINE1(sparc_adjtimex)>> ret = do_adjtimex(&txc);
+ *   - kernel/time/posix-timers.c|191| <<posix_clock_realtime_adj>> return do_adjtimex(t);
+ *   - kernel/time/time.c|280| <<SYSCALL_DEFINE1(adjtimex)>> ret = do_adjtimex(&txc);
+ *   - kernel/time/time.c|358| <<SYSCALL_DEFINE1(adjtimex_time32)>> ret = do_adjtimex(&txc);
+ */
 int do_adjtimex(struct __kernel_timex *txc)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -2487,6 +3751,10 @@ int do_adjtimex(struct __kernel_timex *txc)
 /**
  * hardpps() - Accessor function to NTP __hardpps function
  */
+/*
+ * called by:
+ *   - drivers/pps/kc.c|107| <<pps_kc_event>> hardpps(&ts->ts_real, &ts->ts_raw);
+ */
 void hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)
 {
 	unsigned long flags;
diff --git a/kernel/time/timekeeping_debug.c b/kernel/time/timekeeping_debug.c
index b73e8850e..c2c9ef159 100644
--- a/kernel/time/timekeeping_debug.c
+++ b/kernel/time/timekeeping_debug.c
@@ -43,6 +43,10 @@ static int __init tk_debug_sleep_time_init(void)
 }
 late_initcall(tk_debug_sleep_time_init);
 
+/*
+ * called by:
+ *   - kernel/time/timekeeping.c|2191| <<__timekeeping_inject_sleeptime>> tk_debug_account_sleep_time(delta);
+ */
 void tk_debug_account_sleep_time(const struct timespec64 *t)
 {
 	/* Cap bin index so we don't overflow the array */
diff --git a/kernel/time/timekeeping_internal.h b/kernel/time/timekeeping_internal.h
index 4ca2787d1..e16ff6dbd 100644
--- a/kernel/time/timekeeping_internal.h
+++ b/kernel/time/timekeeping_internal.h
@@ -15,6 +15,21 @@ extern void tk_debug_account_sleep_time(const struct timespec64 *t);
 #define tk_debug_account_sleep_time(x)
 #endif
 
+/*
+ * 在以下调用clocksource_delta():
+ *   - kernel/time/clocksource.c|302| <<cs_watchdog_read>> wd_delta = clocksource_delta(wd_end, *wdnow, watchdog->mask);
+ *   - kernel/time/clocksource.c|331| <<cs_watchdog_read>> wd_delta = clocksource_delta(wd_end2, wd_end, watchdog->mask);
+ *   - kernel/time/clocksource.c|443| <<clocksource_verify_percpu>> delta = clocksource_delta(csnow_end, csnow_begin, cs->mask);
+ *   - kernel/time/clocksource.c|510| <<clocksource_watchdog>> delta = clocksource_delta(wdnow, cs->wd_last, watchdog->mask);
+ *   - kernel/time/clocksource.c|514| <<clocksource_watchdog>> delta = clocksource_delta(csnow, cs->cs_last, cs->mask);
+ *   - kernel/time/clocksource.c|891| <<clocksource_stop_suspend_timing>> delta = clocksource_delta(now, suspend_start,
+ *   - kernel/time/timekeeping.c|436| <<timekeeping_get_delta>> delta = clocksource_delta(now, last, mask);
+ *   - kernel/time/timekeeping.c|490| <<timekeeping_get_delta>> delta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);
+ *   - kernel/time/timekeeping.c|652| <<timekeeping_cycles_to_ns>> delta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);
+ *   - kernel/time/timekeeping.c|705| <<fast_tk_get_delta_ns>> delta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);
+ *   - kernel/time/timekeeping.c|1152| <<timekeeping_forward_now>> delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
+ *   - kernel/time/timekeeping.c|2789| <<timekeeping_advance>> offset = clocksource_delta(tk_clock_read(&tk->tkr_mono),
+ */
 #ifdef CONFIG_CLOCKSOURCE_VALIDATE_LAST_CYCLE
 static inline u64 clocksource_delta(u64 now, u64 last, u64 mask)
 {
diff --git a/kernel/watchdog.c b/kernel/watchdog.c
index 8e61f21e7..16b412cda 100644
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@ -41,6 +41,26 @@ unsigned long __read_mostly watchdog_enabled;
 int __read_mostly watchdog_user_enabled = 1;
 int __read_mostly nmi_watchdog_user_enabled = NMI_WATCHDOG_DEFAULT;
 int __read_mostly soft_watchdog_user_enabled = 1;
+/*
+ * 在以下使用watchdog_thresh:
+ *   - kernel/watchdog.c|44| <<global>> int __read_mostly watchdog_thresh = 10;
+ *   - kernel/watchdog.c|863| <<global>> .data = &watchdog_thresh,
+ *   - kernel/workqueue.c|5929| <<global>> module_param_cb(watchdog_thresh, &wq_watchdog_thresh_ops, &wq_watchdog_thresh,
+ *   - arch/powerpc/kernel/setup_64.c|882| <<hw_nmi_get_sample_period>> u64 hw_nmi_get_sample_period(int watchdog_thresh)
+ *   - arch/powerpc/kernel/setup_64.c|884| <<hw_nmi_get_sample_period>> return ppc_proc_freq * watchdog_thresh;
+ *   - arch/powerpc/kernel/watchdog.c|534| <<watchdog_calc_timeouts>> u64 threshold = watchdog_thresh;
+ *   - arch/powerpc/kernel/watchdog.c|546| <<watchdog_calc_timeouts>> wd_timer_period_ms = watchdog_thresh * 1000 * 2 / 5;
+ *   - arch/x86/kernel/apic/hw_nmi.c|25| <<hw_nmi_get_sample_period>> u64 hw_nmi_get_sample_period(int watchdog_thresh)
+ *   - arch/x86/kernel/apic/hw_nmi.c|27| <<hw_nmi_get_sample_period>> return (u64)(cpu_khz) * 1000 * watchdog_thresh;
+ *   - kernel/watchdog.c|227| <<watchdog_thresh_setup>> get_option(&str, &watchdog_thresh);
+ *   - kernel/watchdog.c|243| <<get_softlockup_thresh>> return watchdog_thresh * 2;
+ *   - kernel/watchdog.c|345| <<is_softlockup>> if ((watchdog_enabled & SOFT_WATCHDOG_ENABLED) && watchdog_thresh){
+ *   - kernel/watchdog.c|642| <<__lockup_detector_reconfigure>> if (watchdog_enabled && watchdog_thresh)
+ *   - kernel/watchdog.c|673| <<lockup_detector_setup>> !(watchdog_enabled && watchdog_thresh))
+ *   - kernel/watchdog.c|818| <<proc_watchdog_thresh>> old = READ_ONCE(watchdog_thresh);
+ *   - kernel/watchdog.c|821| <<proc_watchdog_thresh>> if (!err && write && old != READ_ONCE(watchdog_thresh))
+ *   - kernel/watchdog_hld.c|182| <<hardlockup_detector_event_create>> wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
+ */
 int __read_mostly watchdog_thresh = 10;
 static int __read_mostly nmi_watchdog_available;
 
@@ -95,6 +115,10 @@ __setup("nmi_watchdog=", hardlockup_panic_setup);
  * softlockup watchdog start and stop. The arch must select the
  * SOFTLOCKUP_DETECTOR Kconfig.
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|539| <<watchdog_enable>> watchdog_nmi_enable(cpu);
+ */
 int __weak watchdog_nmi_enable(unsigned int cpu)
 {
 	hardlockup_detector_perf_enable();
@@ -152,6 +176,9 @@ static void lockup_detector_update_enable(void)
 		watchdog_enabled |= SOFT_WATCHDOG_ENABLED;
 }
 
+/*
+ * 一般都是支持CONFIG_SOFTLOCKUP_DETECTOR的
+ */
 #ifdef CONFIG_SOFTLOCKUP_DETECTOR
 
 /*
@@ -177,9 +204,27 @@ static u64 __read_mostly sample_period;
 static DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);
 /* Timestamp of the last softlockup report. */
 static DEFINE_PER_CPU(unsigned long, watchdog_report_ts);
+/*
+ * 在以下使用watchdog_hrtimer:
+ *   - kernel/watchdog.c|183| <<global>> static DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);
+ *   - kernel/watchdog.c|464| <<watchdog_enable>> struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
+ *   - kernel/watchdog.c|490| <<watchdog_disable>> struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
+ */
 static DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);
 static DEFINE_PER_CPU(bool, softlockup_touch_sync);
+/*
+ * 在以下使用percpu的hrtimer_interrupts:
+ *   - kernel/watchdog.c|191| <<global>> static DEFINE_PER_CPU(unsigned long , hrtimer_interrupts);
+ *   - kernel/watchdog.c|340| <<is_hardlockup>> unsigned long hrint = __this_cpu_read(hrtimer_interrupts);
+ *   - kernel/watchdog.c|351| <<watchdog_interrupt_count>> __this_cpu_inc(hrtimer_interrupts);
+ */
 static DEFINE_PER_CPU(unsigned long, hrtimer_interrupts);
+/*
+ * 在以下使用percpu的hrtimer_interrupts_saved:
+ *   - kernel/watchdog.c|192| <<global>> static DEFINE_PER_CPU(unsigned long , hrtimer_interrupts_saved);
+ *   - kernel/watchdog.c|342| <<is_hardlockup>> if (__this_cpu_read(hrtimer_interrupts_saved) == hrint)
+ *   - kernel/watchdog.c|345| <<is_hardlockup>> __this_cpu_write(hrtimer_interrupts_saved, hrint);
+ */
 static DEFINE_PER_CPU(unsigned long, hrtimer_interrupts_saved);
 static unsigned long soft_lockup_nmi_warn;
 
@@ -241,12 +286,23 @@ static void set_sample_period(void)
 	watchdog_update_hrtimer_threshold(sample_period);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|256| <<update_touch_ts>> update_report_ts();
+ *   - kernel/watchdog.c|414| <<watchdog_timer_fn>> update_report_ts();
+ *   - kernel/watchdog.c|432| <<watchdog_timer_fn>> update_report_ts();
+ */
 static void update_report_ts(void)
 {
 	__this_cpu_write(watchdog_report_ts, get_timestamp());
 }
 
 /* Commands for resetting the watchdog */
+/*
+ * called by:
+ *   - kernel/watchdog.c|350| <<softlockup_fn>> update_touch_ts();
+ *   - kernel/watchdog.c|482| <<watchdog_enable>> update_touch_ts();
+ */
 static void update_touch_ts(void)
 {
 	__this_cpu_write(watchdog_touch_ts, get_timestamp());
@@ -326,12 +382,20 @@ bool is_hardlockup(void)
 	return false;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|397| <<watchdog_timer_fn>> watchdog_interrupt_count();
+ */
 static void watchdog_interrupt_count(void)
 {
 	__this_cpu_inc(hrtimer_interrupts);
 }
 
 static DEFINE_PER_CPU(struct completion, softlockup_completion);
+/*
+ * 在以下使用softlockup_stop_work:
+ *   - kernel/watchdog.c|378| <<watchdog_timer_fn>> stop_one_cpu_nowait(smp_processor_id(), softlockup_fn, NULL, this_cpu_ptr(&softlockup_stop_work));
+ */
 static DEFINE_PER_CPU(struct cpu_stop_work, softlockup_stop_work);
 
 /*
@@ -342,6 +406,10 @@ static DEFINE_PER_CPU(struct cpu_stop_work, softlockup_stop_work);
  * for more than 2*watchdog_thresh seconds then the debug-printout
  * triggers in watchdog_timer_fn().
  */
+/*
+ * 在以下使用softlockup_fn():
+ *   - kernel/watchdog.c|378| <<watchdog_timer_fn>> stop_one_cpu_nowait(smp_processor_id(), softlockup_fn, NULL, this_cpu_ptr(&softlockup_stop_work));
+ */
 static int softlockup_fn(void *data)
 {
 	update_touch_ts();
@@ -351,6 +419,10 @@ static int softlockup_fn(void *data)
 }
 
 /* watchdog kicker functions */
+/*
+ * 在以下使用watchdog_timer_fn():
+ *   - kernel/watchdog.c|465| <<watchdog_enable>> hrtimer->function = watchdog_timer_fn;
+ */
 static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 {
 	unsigned long touch_ts, period_ts, now;
@@ -364,6 +436,13 @@ static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 	/* kick the hardlockup detector */
 	watchdog_interrupt_count();
 
+	/*
+	 * completion_done - Test to see if a completion has any waiters
+	 *      @x:     completion structure
+	 *
+	 *      Return: 0 if there are waiters (wait_for_completion() in progress)
+	 *               1 if there are no waiters.
+	 */
 	/* kick the softlockup detector */
 	if (completion_done(this_cpu_ptr(&softlockup_completion))) {
 		reinit_completion(this_cpu_ptr(&softlockup_completion));
@@ -447,8 +526,19 @@ static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 	return HRTIMER_RESTART;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|513| <<softlockup_start_fn>> watchdog_enable(smp_processor_id());
+ *   - kernel/watchdog.c|529| <<lockup_detector_online_cpu>> watchdog_enable(cpu);
+ */
 static void watchdog_enable(unsigned int cpu)
 {
+	/*
+	 * 在以下使用watchdog_hrtimer:
+	 *   - kernel/watchdog.c|183| <<global>> static DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);
+	 *   - kernel/watchdog.c|464| <<watchdog_enable>> struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
+	 *   - kernel/watchdog.c|490| <<watchdog_disable>> struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
+	 */
 	struct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);
 	struct completion *done = this_cpu_ptr(&softlockup_completion);
 
@@ -508,12 +598,20 @@ static void softlockup_stop_all(void)
 	cpumask_clear(&watchdog_allowed_mask);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|523| <<softlockup_start_all>> smp_call_on_cpu(cpu, softlockup_start_fn, NULL, false);
+ */
 static int softlockup_start_fn(void *data)
 {
 	watchdog_enable(smp_processor_id());
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|549| <<__lockup_detector_reconfigure>> softlockup_start_all();
+ */
 static void softlockup_start_all(void)
 {
 	int cpu;
@@ -523,6 +621,14 @@ static void softlockup_start_all(void)
 		smp_call_on_cpu(cpu, softlockup_start_fn, NULL, false);
 }
 
+/*
+ * 在以下使用lockup_detector_online_cpu():
+ * 1803         [CPUHP_AP_WATCHDOG_ONLINE] = {
+ * 1804                 .name                   = "lockup_detector:online",
+ * 1805                 .startup.single         = lockup_detector_online_cpu,
+ * 1806                 .teardown.single        = lockup_detector_offline_cpu,
+ * 1807         }, 
+ */
 int lockup_detector_online_cpu(unsigned int cpu)
 {
 	if (cpumask_test_cpu(cpu, &watchdog_allowed_mask))
@@ -537,6 +643,14 @@ int lockup_detector_offline_cpu(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|563| <<lockup_detector_reconfigure>> __lockup_detector_reconfigure();
+ *   - kernel/watchdog.c|583| <<lockup_detector_setup>> __lockup_detector_reconfigure();
+ *   - kernel/watchdog.c|643| <<proc_watchdog_update>> __lockup_detector_reconfigure();
+ *
+ * 一般都是支持CONFIG_SOFTLOCKUP_DETECTOR的, 这个!!!
+ */
 static void __lockup_detector_reconfigure(void)
 {
 	cpus_read_lock();
diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 247bf0b15..b5035fbff 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -21,6 +21,13 @@
 #include <linux/perf_event.h>
 
 static DEFINE_PER_CPU(bool, hard_watchdog_warn);
+/*
+ * 在以下使用percpu的watchdog_nmi_touch:
+ *   - kernel/watchdog_hld.c|24| <<global>> static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
+ *   - kernel/watchdog_hld.c|41| <<arch_touch_nmi_watchdog>> raw_cpu_write(watchdog_nmi_touch, true);
+ *   - kernel/watchdog_hld.c|121| <<watchdog_overflow_callback>> if (__this_cpu_read(watchdog_nmi_touch) == true) {
+ *   - kernel/watchdog_hld.c|122| <<watchdog_overflow_callback>> __this_cpu_write(watchdog_nmi_touch, false);
+ */
 static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
 static DEFINE_PER_CPU(struct perf_event *, dead_event);
@@ -38,6 +45,13 @@ notrace void arch_touch_nmi_watchdog(void)
 	 * case we shouldn't have to worry about the watchdog
 	 * going off.
 	 */
+	/*
+	 * 在以下使用percpu的watchdog_nmi_touch:
+	 *   - kernel/watchdog_hld.c|24| <<global>> static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
+	 *   - kernel/watchdog_hld.c|41| <<arch_touch_nmi_watchdog>> raw_cpu_write(watchdog_nmi_touch, true);
+	 *   - kernel/watchdog_hld.c|121| <<watchdog_overflow_callback>> if (__this_cpu_read(watchdog_nmi_touch) == true) {
+	 *   - kernel/watchdog_hld.c|122| <<watchdog_overflow_callback>> __this_cpu_write(watchdog_nmi_touch, false);
+	 */
 	raw_cpu_write(watchdog_nmi_touch, true);
 }
 EXPORT_SYMBOL(arch_touch_nmi_watchdog);
@@ -107,6 +121,10 @@ static struct perf_event_attr wd_hw_attr = {
 };
 
 /* Callback function for perf event subsystem */
+/*
+ * 在以下使用watchdog_overflow_callback():
+ *   - kernel/watchdog_hld.c|177| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+ */
 static void watchdog_overflow_callback(struct perf_event *event,
 				       struct perf_sample_data *data,
 				       struct pt_regs *regs)
@@ -114,6 +132,13 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	/* Ensure the watchdog never gets throttled */
 	event->hw.interrupts = 0;
 
+	/*
+	 * 在以下使用percpu的watchdog_nmi_touch:
+	 *   - kernel/watchdog_hld.c|24| <<global>> static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
+	 *   - kernel/watchdog_hld.c|41| <<arch_touch_nmi_watchdog>> raw_cpu_write(watchdog_nmi_touch, true);
+	 *   - kernel/watchdog_hld.c|121| <<watchdog_overflow_callback>> if (__this_cpu_read(watchdog_nmi_touch) == true) {
+	 *   - kernel/watchdog_hld.c|122| <<watchdog_overflow_callback>> __this_cpu_write(watchdog_nmi_touch, false);
+	 */
 	if (__this_cpu_read(watchdog_nmi_touch) == true) {
 		__this_cpu_write(watchdog_nmi_touch, false);
 		return;
@@ -163,6 +188,11 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	return;
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog_hld.c|192| <<hardlockup_detector_perf_enable>> if (hardlockup_detector_event_create())
+ *   - kernel/watchdog_hld.c|287| <<hardlockup_detector_perf_init>> int ret = hardlockup_detector_event_create();
+ */
 static int hardlockup_detector_event_create(void)
 {
 	unsigned int cpu = smp_processor_id();
@@ -187,6 +217,10 @@ static int hardlockup_detector_event_create(void)
 /**
  * hardlockup_detector_perf_enable - Enable the local event
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|100| <<watchdog_nmi_enable>> hardlockup_detector_perf_enable();
+ */
 void hardlockup_detector_perf_enable(void)
 {
 	if (hardlockup_detector_event_create())
@@ -282,6 +316,10 @@ void __init hardlockup_detector_perf_restart(void)
 /**
  * hardlockup_detector_perf_init - Probe whether NMI event is available at all
  */
+/*
+ * called by:
+ *   - kernel/watchdog.c|112| <<watchdog_nmi_probe>> return hardlockup_detector_perf_init();
+ */
 int __init hardlockup_detector_perf_init(void)
 {
 	int ret = hardlockup_detector_event_create();
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index 4b7fce72e..dae401be0 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -1489,6 +1489,22 @@ static ssize_t __iov_iter_get_pages_alloc(struct iov_iter *i,
 	return -EFAULT;
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1221| <<__bio_iov_iter_get_pages>> size = iov_iter_get_pages2(iter, pages, UINT_MAX - bio->bi_iter.bi_size,
+ *   - crypto/af_alg.c|407| <<af_alg_make_sg>> n = iov_iter_get_pages2(iter, sgl->pages, len, ALG_MAX_PAGES, &off);
+ *   - drivers/vhost/scsi.c|829| <<vhost_scsi_map_to_sgl>> bytes = iov_iter_get_pages2(iter, pages, LONG_MAX,
+ *   - fs/ceph/file.c|98| <<__iter_get_bvecs>> bytes = iov_iter_get_pages2(iter, pages, maxsize - size,
+ *   - fs/cifs/misc.c|1029| <<setup_aio_ctx_iter>> rc = iov_iter_get_pages2(iter, pages, count, max_pages, &start);
+ *   - fs/direct-io.c|172| <<dio_refill_pages>> ret = iov_iter_get_pages2(sdio->iter, dio->pages, LONG_MAX, DIO_PAGES,
+ *   - fs/fuse/dev.c|733| <<fuse_copy_fill>> err = iov_iter_get_pages2(cs->iter, &page, PAGE_SIZE, 1, &off);
+ *   - fs/fuse/file.c|1417| <<fuse_get_user_pages>> ret = iov_iter_get_pages2(ii, &ap->pages[ap->num_pages],
+ *   - fs/splice.c|1168| <<iter_to_pipe>> left = iov_iter_get_pages2(from, pages, ~0UL, 16, &start);
+ *   - net/core/datagram.c|635| <<__zerocopy_sg_from_iter>> copied = iov_iter_get_pages2(from, pages, length,
+ *   - net/core/skmsg.c|327| <<sk_msg_zerocopy_from_iter>> copied = iov_iter_get_pages2(from, pages, bytes, maxpages,
+ *   - net/rds/message.c|394| <<rds_message_zcopy_from_user>> copied = iov_iter_get_pages2(from, &pages, PAGE_SIZE,
+ *   - net/tls/tls_sw.c|1355| <<tls_setup_from_iter>> copied = iov_iter_get_pages2(from, pages,
+ */
 ssize_t iov_iter_get_pages2(struct iov_iter *i,
 		   struct page **pages, size_t maxsize, unsigned maxpages,
 		   size_t *start)
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index f42bb51e0..60b2623c2 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -70,6 +70,16 @@ static atomic_t huge_zero_refcount;
 struct page *huge_zero_page __read_mostly;
 unsigned long huge_zero_pfn __read_mostly = ~0UL;
 
+/*
+ * called by:
+ *   - fs/proc/task_mmu.c|867| <<show_smap>> hugepage_vma_check(vma, vma->vm_flags, true, false));
+ *   - mm/khugepaged.c|469| <<khugepaged_enter_vma>> if (hugepage_vma_check(vma, vm_flags, false, false))
+ *   - mm/khugepaged.c|923| <<hugepage_vma_revalidate>> if (!hugepage_vma_check(vma, vma->vm_flags, false, false))
+ *   - mm/khugepaged.c|1410| <<collapse_pte_mapped_thp>> if (!hugepage_vma_check(vma, vma->vm_flags | VM_HUGEPAGE, false, false))
+ *   - mm/khugepaged.c|2100| <<__acquires>> if (!hugepage_vma_check(vma, vma->vm_flags, false, false)) {
+ *   - mm/memory.c|5002| <<__handle_mm_fault>> hugepage_vma_check(vma, vm_flags, false, true)) {
+ *   - mm/memory.c|5036| <<__handle_mm_fault>> hugepage_vma_check(vma, vm_flags, false, true)) {
+ */
 bool hugepage_vma_check(struct vm_area_struct *vma,
 			unsigned long vm_flags,
 			bool smaps, bool in_pf)
@@ -778,6 +788,10 @@ static void set_huge_zero_page(pgtable_t pgtable, struct mm_struct *mm,
 	mm_inc_nr_ptes(mm);
 }
 
+/*
+ * called by:
+ *   - mm/memory.c|4782| <<create_huge_pmd>> return do_huge_pmd_anonymous_page(vmf);
+ */
 vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 {
 	struct vm_area_struct *vma = vmf->vma;
diff --git a/mm/memblock.c b/mm/memblock.c
index b5d302697..8baf56377 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -1539,6 +1539,10 @@ static void * __init memblock_alloc_internal(
  * Return:
  * Virtual address of allocated memory block on success, NULL on failure.
  */
+/*
+ * called by:
+ *   - mm/page_alloc.c|7009| <<memmap_alloc>> ptr = memblock_alloc_exact_nid_raw(size, align, min_addr,
+ */
 void * __init memblock_alloc_exact_nid_raw(
 			phys_addr_t size, phys_addr_t align,
 			phys_addr_t min_addr, phys_addr_t max_addr,
@@ -2124,6 +2128,11 @@ void __init reset_all_zones_managed_pages(void)
 /**
  * memblock_free_all - release free pages to the buddy allocator
  */
+/*
+ * x86在以下使用:
+ *   - arch/x86/mm/init_32.c|749| <<mem_init>> memblock_free_all();
+ *   - arch/x86/mm/init_64.c|1337| <<mem_init>> memblock_free_all();
+ */
 void __init memblock_free_all(void)
 {
 	unsigned long pages;
diff --git a/mm/memory.c b/mm/memory.c
index a78814413..d78816abf 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4776,6 +4776,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	goto out;
 }
 
+/*
+ * called by:
+ *   - mm/memory.c|5029| <<__handle_mm_fault>> ret = create_huge_pmd(&vmf);
+ */
 static inline vm_fault_t create_huge_pmd(struct vm_fault *vmf)
 {
 	if (vma_is_anonymous(vmf->vma))
@@ -4964,6 +4968,10 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * The mmap_lock may have been released depending on flags and our
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
+/*
+ * called by:
+ *   - mm/memory.c|5157| <<handle_mm_fault>> ret = __handle_mm_fault(vma, address, flags);
+ */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		unsigned long address, unsigned int flags)
 {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d04211f0e..ead24fd52 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -7000,6 +7000,12 @@ static void __init memmap_init(void)
 		init_unavailable_range(hole_pfn, end_pfn, zone_id, nid);
 }
 
+/*
+ * called by:
+ *   - mm/page_alloc.c|7823| <<alloc_node_mem_map>> map = memmap_alloc(size, SMP_CACHE_BYTES, MEMBLOCK_LOW_LIMIT,
+ *   - mm/sparse.c|440| <<__populate_section_memmap>> map = memmap_alloc(size, size, addr, nid, false);
+ *   - mm/sparse.c|467| <<sparse_buffer_init>> sparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);
+ */
 void __init *memmap_alloc(phys_addr_t size, phys_addr_t align,
 			  phys_addr_t min_addr, int nid, bool exact_nid)
 {
diff --git a/net/core/sock.c b/net/core/sock.c
index 788c13726..577f63e2d 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -2770,6 +2770,20 @@ DEFINE_STATIC_KEY_FALSE(net_high_order_alloc_disable_key);
  * no guarantee that allocations succeed. Therefore, @sz MUST be
  * less or equal than PAGE_SIZE.
  */
+/*
+ * called by:
+ *   - drivers/net/tun.c|1668| <<tun_build_skb>> if (unlikely(!skb_page_frag_refill(buflen, alloc_frag, GFP_KERNEL)))
+ *   - drivers/net/virtio_net.c|1313| <<add_recvbuf_small>> if (unlikely(!skb_page_frag_refill(len, alloc_frag, gfp)))
+ *   - drivers/net/virtio_net.c|1410| <<add_recvbuf_mergeable>> if (unlikely(!skb_page_frag_refill(len + room, alloc_frag, gfp)))
+ *   - net/core/sock.c|2809| <<sk_page_frag_refill>> if (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))
+ *   - net/ipv4/esp4.c|481| <<esp_output_head>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/ipv4/esp4.c|590| <<esp_output_tail>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/ipv6/esp6.c|516| <<esp6_output_head>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/ipv6/esp6.c|627| <<esp6_output_tail>> if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+ *   - net/mptcp/protocol.c|1091| <<mptcp_page_frag_refill>> if (likely(skb_page_frag_refill(32U + sizeof(struct mptcp_data_frag),
+ *   - net/tls/tls_device.c|331| <<tls_device_record_close>> if (likely(skb_page_frag_refill(prot->tag_size, pfrag,
+ *   - net/tls/tls_device.c|381| <<tls_do_allocation>> if (unlikely(!skb_page_frag_refill(prepend_size, pfrag,
+ */
 bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)
 {
 	if (pfrag->page) {
@@ -2804,6 +2818,16 @@ bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)
 }
 EXPORT_SYMBOL(skb_page_frag_refill);
 
+/*
+ * called by:
+ *   - net/ipv4/ip_output.c|1216| <<global>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/ipv6/ip6_output.c|1770| <<global>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/core/skbuff.c|2494| <<linear_to_page>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/core/skmsg.c|39| <<sk_msg_alloc>> if (!sk_page_frag_refill(sk, pfrag)) {
+ *   - net/ipv4/tcp.c|1352| <<tcp_sendmsg_locked>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/kcm/kcmsock.c|960| <<kcm_sendmsg>> if (!sk_page_frag_refill(sk, pfrag))
+ *   - net/tls/tls_device.c|396| <<tls_do_allocation>> if (!sk_page_frag_refill(sk, pfrag))
+ */
 bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag)
 {
 	if (likely(skb_page_frag_refill(32U, pfrag, sk->sk_allocation)))
diff --git a/net/ethernet/eth.c b/net/ethernet/eth.c
index 62b89d6f5..9d2c11376 100644
--- a/net/ethernet/eth.c
+++ b/net/ethernet/eth.c
@@ -301,6 +301,24 @@ EXPORT_SYMBOL(eth_prepare_mac_addr_change);
  * @dev: network device
  * @p: socket address
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_netdev.c|151| <<opa_vnic_process_vema_config>> eth_commit_mac_addr_change(netdev, &saddr);
+ *   - drivers/net/ethernet/broadcom/bgmac.c|1247| <<bgmac_set_mac_address>> eth_commit_mac_addr_change(net_dev, addr);
+ *   - drivers/net/ethernet/davicom/dm9051.c|1086| <<dm9051_set_mac_address>> eth_commit_mac_addr_change(ndev, p);
+ *   - drivers/net/ethernet/faraday/ftgmac100.c|220| <<ftgmac100_set_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/ethernet/marvell/mvneta.c|3890| <<mvneta_set_mac_addr>> eth_commit_mac_addr_change(dev, addr);
+ *   - drivers/net/ethernet/netronome/nfp/nfp_net_common.c|1920| <<nfp_net_set_mac_address>> eth_commit_mac_addr_change(netdev, addr);
+ *   - drivers/net/ethernet/pensando/ionic/ionic_lif.c|1654| <<ionic_set_mac_address>> eth_commit_mac_addr_change(netdev, addr);
+ *   - drivers/net/ethernet/pensando/ionic/ionic_lif.c|3288| <<ionic_station_set>> eth_commit_mac_addr_change(netdev, &addr);
+ *   - drivers/net/ethernet/ti/am65-cpsw-nuss.c|1222| <<am65_cpsw_nuss_ndo_slave_set_mac_address>> eth_commit_mac_addr_change(ndev, sockaddr);
+ *   - drivers/net/hyperv/netvsc_drv.c|1420| <<netvsc_set_mac_addr>> eth_commit_mac_addr_change(ndev, p);
+ *   - drivers/net/usb/qmi_wwan.c|636| <<qmi_wwan_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/virtio_net.c|2048| <<virtnet_set_mac_address>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/wireless/microchip/wilc1000/netdev.c|649| <<wilc_set_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - drivers/net/wireless/microchip/wilc1000/netdev.c|673| <<wilc_set_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ *   - net/ethernet/eth.c|329| <<eth_mac_addr>> eth_commit_mac_addr_change(dev, p);
+ */
 void eth_commit_mac_addr_change(struct net_device *dev, void *p)
 {
 	struct sockaddr *addr = p;
diff --git a/security/integrity/ima/ima_efi.c b/security/integrity/ima/ima_efi.c
index 9db66fe31..205bd10ba 100644
--- a/security/integrity/ima/ima_efi.c
+++ b/security/integrity/ima/ima_efi.c
@@ -11,8 +11,20 @@
 #define arch_ima_efi_boot_mode efi_secureboot_mode_unset
 #endif
 
+/*
+ * called by:
+ *   - security/integrity/ima/ima_efi.c|42| <<arch_ima_get_secureboot>> sb_mode = get_sb_mode();
+ */
 static enum efi_secureboot_mode get_sb_mode(void)
 {
+	/*
+	 * enum efi_secureboot_mode {
+	 *     efi_secureboot_mode_unset,
+	 *     efi_secureboot_mode_unknown,
+	 *     efi_secureboot_mode_disabled,
+	 *     efi_secureboot_mode_enabled,
+	 * };
+	 */
 	enum efi_secureboot_mode mode;
 
 	if (!efi_rt_services_supported(EFI_RT_SUPPORTED_GET_VARIABLE)) {
@@ -30,6 +42,13 @@ static enum efi_secureboot_mode get_sb_mode(void)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - security/integrity/ima/ima_appraise.c|29| <<ima_appraise_parse_cmdline>> bool sb_state = arch_ima_get_secureboot();
+ *   - security/integrity/ima/ima_efi.c|67| <<arch_get_ima_policy>> if (IS_ENABLED(CONFIG_IMA_ARCH_POLICY) && arch_ima_get_secureboot()) {
+ *   - security/integrity/ima/ima_main.c|803| <<ima_load_data>> && arch_ima_get_secureboot()) {
+ *   - security/integrity/platform_certs/load_uefi.c|214| <<load_uefi_certs>> if (!arch_ima_get_secureboot())
+ */
 bool arch_ima_get_secureboot(void)
 {
 	static enum efi_secureboot_mode sb_mode;
diff --git a/security/integrity/ima/ima_main.c b/security/integrity/ima/ima_main.c
index 040b03ddc..389d11546 100644
--- a/security/integrity/ima/ima_main.c
+++ b/security/integrity/ima/ima_main.c
@@ -790,6 +790,10 @@ int ima_post_read_file(struct file *file, void *buf, loff_t size,
  *
  * For permission return 0, otherwise return -EACCES.
  */
+/*
+ * called by:
+ *   - security/security.c|1777| <<security_kernel_load_data>> return ima_load_data(id, contents);
+ */
 int ima_load_data(enum kernel_load_data_id id, bool contents)
 {
 	bool ima_enforce, sig_enforce;
diff --git a/tools/include/linux/ring_buffer.h b/tools/include/linux/ring_buffer.h
index 6c0261737..0a6488b9d 100644
--- a/tools/include/linux/ring_buffer.h
+++ b/tools/include/linux/ring_buffer.h
@@ -50,6 +50,13 @@
 
 static inline u64 ring_buffer_read_head(struct perf_event_mmap_page *base)
 {
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 /*
  * Architectures where smp_load_acquire() does not fallback to
  * READ_ONCE() + smp_mb() pair.
diff --git a/tools/include/uapi/linux/perf_event.h b/tools/include/uapi/linux/perf_event.h
index 581ed4bdc..8f0bd7c60 100644
--- a/tools/include/uapi/linux/perf_event.h
+++ b/tools/include/uapi/linux/perf_event.h
@@ -673,6 +673,13 @@ struct perf_event_mmap_page {
 	 * data_{offset,size} indicate the location and size of the perf record
 	 * buffer within the mmapped area.
 	 */
+	/*
+	 * 在以下使用perf_event_mmap_page->data_head:
+	 *   - kernel/events/core.c|13494| <<perf_event_init>> BUILD_BUG_ON((offsetof(struct perf_event_mmap_page, data_head))
+	 *   - kernel/events/ring_buffer.c|110| <<perf_output_put_handle>> WRITE_ONCE(rb->user_page->data_head, head);
+	 *   - tools/include/linux/ring_buffer.h|59| <<ring_buffer_read_head>> return smp_load_acquire(&base->data_head);
+	 *   - tools/include/linux/ring_buffer.h|61| <<ring_buffer_read_head>> u64 head = READ_ONCE(base->data_head);
+	 */
 	__u64   data_head;		/* head in the data section */
 	__u64	data_tail;		/* user-space written tail */
 	__u64	data_offset;		/* where the buffer starts */
diff --git a/tools/lib/perf/evlist.c b/tools/lib/perf/evlist.c
index 8ec5b9f34..dcf1cc742 100644
--- a/tools/lib/perf/evlist.c
+++ b/tools/lib/perf/evlist.c
@@ -430,6 +430,12 @@ static void perf_evlist__set_mmap_first(struct perf_evlist *evlist, struct perf_
 		evlist->mmap_first = map;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/evlist.c|542| <<mmap_per_thread>> if (mmap_per_evsel(evlist, ops, idx, mp, 0, thread, &output,
+ *   - tools/lib/perf/evlist.c|552| <<mmap_per_thread>> if (mmap_per_evsel(evlist, ops, idx, mp, cpu, 0, &output,
+ *   - tools/lib/perf/evlist.c|583| <<mmap_per_cpu>> if (mmap_per_evsel(evlist, ops, cpu, mp, cpu,
+ */
 static int
 mmap_per_evsel(struct perf_evlist *evlist, struct perf_evlist_mmap_ops *ops,
 	       int idx, struct perf_mmap_param *mp, int cpu_idx,
diff --git a/tools/lib/perf/evsel.c b/tools/lib/perf/evsel.c
index 8ce5bbd09..cca5460aa 100644
--- a/tools/lib/perf/evsel.c
+++ b/tools/lib/perf/evsel.c
@@ -242,6 +242,11 @@ void perf_evsel__munmap(struct perf_evsel *evsel)
 	evsel->mmap = NULL;
 }
 
+/*
+ * called by:
+ *   - tools/lib/perf/tests/test-evsel.c|153| <<test_stat_user_read>> err = perf_evsel__mmap(evsel, 0);
+ *   - tools/perf/tests/mmap-basic.c|204| <<test_stat_user_read>> err = perf_evsel__mmap(evsel, 0);
+ */
 int perf_evsel__mmap(struct perf_evsel *evsel, int pages)
 {
 	int ret, idx, thread;
@@ -384,6 +389,23 @@ static void perf_evsel__adjust_values(struct perf_evsel *evsel, u64 *buf,
 		count->lost = buf[n++];
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_evsel__read (evsel=evsel@entry=0x7c6130, cpu_map_idx=cpu_map_idx@entry=0, thread=thread@entry=0, count=0x7c63c8) at evsel.c:389
+ * #1  0x00000000004c42ff in evsel__read_one (thread=0, cpu_map_idx=0, evsel=0x7c6130) at util/evsel.c:1540
+ * #2  evsel__read_counter (evsel=evsel@entry=0x7c6130, cpu_map_idx=cpu_map_idx@entry=0, thread=thread@entry=0) at util/evsel.c:1629
+ * #3  0x000000000042b0b1 in read_single_counter (rs=0x7fffffff8fb0, thread=0, cpu_map_idx=0, counter=0x7c6130) at builtin-stat.c:367
+ * #4  read_counter_cpu (cpu_map_idx=0, rs=0x7fffffff8fb0, counter=0x7c6130) at builtin-stat.c:397
+ * #5  read_affinity_counters (rs=0x7fffffff8fb0) at builtin-stat.c:449
+ * #6  read_counters (rs=0x7fffffff8fb0) at builtin-stat.c:483
+ * #7  0x000000000042e01c in __run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1046
+ * #8  run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1071
+ * #9  cmd_stat (argc=2, argv=<optimized out>) at builtin-stat.c:2577
+ * #10 0x00000000004a544b in run_builtin (p=p@entry=0x719650 <commands+336>, argc=argc@entry=6, argv=argv@entry=0x7fffffffdf70) at perf.c:316
+ * #11 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #12 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #13 main (argc=6, argv=0x7fffffffdf70) at perf.c:544
+ */
 int perf_evsel__read(struct perf_evsel *evsel, int cpu_map_idx, int thread,
 		     struct perf_counts_values *count)
 {
@@ -405,6 +427,20 @@ int perf_evsel__read(struct perf_evsel *evsel, int cpu_map_idx, int thread,
 	    !perf_mmap__read_self(MMAP(evsel, cpu_map_idx, thread), count))
 		return 0;
 
+	/*
+	 * 15 struct perf_counts_values {
+	 * 16         union {
+	 * 17                 struct {
+	 * 18                         uint64_t val;
+	 * 19                         uint64_t ena;
+	 * 20                         uint64_t run;
+	 * 21                         uint64_t id;
+	 * 22                         uint64_t lost;
+	 * 23                 };
+	 * 24                 uint64_t values[5];
+	 * 25         };
+	 * 26 };
+	 */
 	if (readn(*fd, buf.values, size) <= 0)
 		return -errno;
 
diff --git a/tools/lib/perf/mmap.c b/tools/lib/perf/mmap.c
index 0d1634ced..9446bd8c6 100644
--- a/tools/lib/perf/mmap.c
+++ b/tools/lib/perf/mmap.c
@@ -32,6 +32,44 @@ size_t perf_mmap__mmap_len(struct perf_mmap *map)
 	return map->mask + 1 + page_size;
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7ffff7eeb010, mp=mp@entry=0x7fffffff8840, fd=fd@entry=5, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7ffff7eeb010, mp=0x7fffffff8840, fd=5, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7a7480, ops=0x7fffffff8820, idx=0, mp=0x7fffffff8840, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87d0, _output_overwrite=0x7fffffff87d4,
+ *     nr_mmaps=0x7fffffff87cc) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8840, ops=<optimized out>, evlist=0x7a7480) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7a7480, ops=ops@entry=0x7fffffff8820, mp=mp@entry=0x7fffffff8840) at evlist.c:642
+ * #5  0x00000000004bcecc in evlist__mmap_ex (evlist=evlist@entry=0x7a7480, pages=<optimized out>, auxtrace_pages=0, auxtrace_overwrite=auxtrace_overwrite@entry=false, nr_cblocks=<optimized out>,
+ *     affinity=<optimized out>, flush=1, comp_level=0) at util/evlist.c:1001
+ * #6  0x0000000000422a28 in record__mmap_evlist (rec=0x707020 <record>, evlist=0x7a7480) at builtin-record.c:1159
+ * #7  record__mmap (rec=0x707020 <record>) at builtin-record.c:1208
+ * #8  record__open (rec=0x707020 <record>) at builtin-record.c:1292
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2304
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7fffec363010, mp=mp@entry=0x7fffffff8820, fd=fd@entry=21, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7fffec363010, mp=0x7fffffff8820, fd=21, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7c9850, ops=0x7fffffff8800, idx=0, mp=0x7fffffff8820, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87b0, _output_overwrite=0x7fffffff87b4,
+ *     nr_mmaps=0x7fffffff87ac) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8820, ops=<optimized out>, evlist=0x7c9850) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7c9850, ops=ops@entry=0x7fffffff8800, mp=mp@entry=0x7fffffff8820) at evlist.c:642
+ * #5  0x00000000004bcfeb in evlist__mmap_ex (comp_level=0, flush=1, affinity=0, nr_cblocks=0, auxtrace_overwrite=false, auxtrace_pages=0, pages=4294967295, evlist=0x7c9850) at util/evlist.c:1001
+ * #6  evlist__mmap (evlist=evlist@entry=0x7c9850, pages=pages@entry=4294967295) at util/evlist.c:1006
+ * #7  0x00000000004c0745 in evlist__start_sb_thread (evlist=0x7c9850, target=target@entry=0x707160 <record+320>) at util/sideband_evlist.c:122
+ * #8  0x0000000000422578 in record__setup_sb_evlist (rec=0x707020 <record>) at builtin-record.c:1984
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2359
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ */
 int perf_mmap__mmap(struct perf_mmap *map, struct perf_mmap_param *mp,
 		    int fd, struct perf_cpu cpu)
 {
@@ -138,6 +176,10 @@ static int overwrite_rb_find_range(void *buf, int mask, u64 *start, u64 *end)
 /*
  * Report the start and end of the available data in ringbuffer
  */
+/*
+ * called by:
+ *   - tools/lib/perf/mmap.c|221| <<perf_mmap__read_init>> return __perf_mmap__read_init(map);
+ */
 static int __perf_mmap__read_init(struct perf_mmap *md)
 {
 	u64 head = perf_mmap__read_head(md);
diff --git a/tools/perf/builtin-record.c b/tools/perf/builtin-record.c
index 0f711f888..8a89d6959 100644
--- a/tools/perf/builtin-record.c
+++ b/tools/perf/builtin-record.c
@@ -110,6 +110,10 @@ struct record_thread {
 	struct mmap		**maps;
 	struct mmap		**overwrite_maps;
 	struct record		*rec;
+	/*
+	 * 似乎只在这一处增加record_thread->samples:
+	 * - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+	 */
 	unsigned long long	samples;
 	unsigned long		waking;
 	u64			bytes_written;
@@ -232,6 +236,32 @@ static bool record__output_max_size_exceeded(struct record *rec)
 	       (record__bytes_written(rec) >= rec->output_max_size);
 }
 
+/*
+ * (gdb) bt
+ * #0  record__write (size=528, bf=0x7ca940, map=0x0, rec=0x707020 <record>) at builtin-record.c:608
+ * #1  process_synthesized_event (tool=tool@entry=0x707020 <record>, event=event@entry=0x7ca940, sample=sample@entry=0x0, machine=machine@entry=0x7a8148) at builtin-record.c:608
+ * #2  0x000000000054e929 in __perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148,
+ *     from=from@entry=0) at util/synthetic-events.c:1873
+ * #3  0x000000000054eb2f in perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148)
+ *     at util/synthetic-events.c:1883
+ * #4  0x0000000000421225 in record__synthesize (tail=tail@entry=false, rec=0x707020 <record>) at builtin-record.c:1853
+ * #5  0x0000000000422519 in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2356
+ * #6  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #7  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #8  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #9  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #10 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/builtin-record.c|661| <<process_synthesized_event>> return record__write(rec, NULL, event, event->header.size);
+ *   - tools/perf/builtin-record.c|688| <<record__pushfn>> return record__write(rec, map, bf, size);
+ *   - tools/perf/builtin-record.c|768| <<record__process_auxtrace>> record__write(rec, map, event, event->header.size);
+ *   - tools/perf/builtin-record.c|769| <<record__process_auxtrace>> record__write(rec, map, data1, len1);
+ *   - tools/perf/builtin-record.c|771| <<record__process_auxtrace>> record__write(rec, map, data2, len2);
+ *   - tools/perf/builtin-record.c|772| <<record__process_auxtrace>> record__write(rec, map, &pad, padding);
+ *   - tools/perf/builtin-record.c|1607| <<record__mmap_read_evlist>> rc = record__write(rec, NULL, &finished_round_event, sizeof(finished_round_event));
+ *   - tools/perf/builtin-record.c|1792| <<write_finished_init>> return record__write(rec, NULL, &finished_init_event, sizeof(finished_init_event));
+ */
 static int record__write(struct record *rec, struct mmap *map __maybe_unused,
 			 void *bf, size_t size)
 {
@@ -426,6 +456,10 @@ static int record__aio_pushfn(struct mmap *map, void *to, void *buf, size_t size
 	return size;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1615| <<record__mmap_read_evlist>> if (record__aio_push(rec, map, &off) < 0) {
+ */
 static int record__aio_push(struct record *rec, struct mmap *map, off_t *off)
 {
 	int ret, idx;
@@ -439,6 +473,12 @@ static int record__aio_push(struct record *rec, struct mmap *map, off_t *off)
 
 	idx = record__aio_sync(map, false);
 	aio.data = map->aio.data[idx];
+	/*
+	 * called by:
+	 *   - 定义tools/perf/util/mmap.c|324| <<global>> int perf_mmap__push(struct mmap *md, void *to,
+	 *   - tools/perf/builtin-record.c|468| <<record__aio_push>> ret = perf_mmap__push(map, &aio, record__aio_pushfn);
+	 *   - tools/perf/builtin-record.c|1608| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+	 */
 	ret = perf_mmap__push(map, &aio, record__aio_pushfn);
 	if (ret != 0) /* ret > 0 - no data, ret < 0 - error */
 		return ret;
@@ -599,6 +639,43 @@ static int record__comp_enabled(struct record *rec)
 	return rec->opts.comp_level > 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  process_synthesized_event (tool=tool@entry=0x707020 <record>, event=event@entry=0x7ca940, sample=sample@entry=0x0, machine=machine@entry=0x7a8148) at builtin-record.c:606
+ * #1  0x000000000054e859 in __perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148, 
+ *     from=from@entry=0) at util/synthetic-events.c:1873
+ * #2  0x000000000054ea5f in perf_event__synthesize_id_index (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, evlist=<optimized out>, machine=machine@entry=0x7a8148)
+ *     at util/synthetic-events.c:1883
+ * #3  0x0000000000421225 in record__synthesize (tail=tail@entry=false, rec=0x707020 <record>) at builtin-record.c:1853
+ * #4  0x000000000042251c in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2355
+ * #5  0x00000000004255d5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4079
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * callled by:
+ *   - tools/perf/builtin-record.c|620| <<process_locked_synthesized_event>> ret = process_synthesized_event(tool, event, sample, machine);
+ *   - tools/perf/builtin-record.c|1369| <<perf_event__synthesize_guest_os>> err = perf_event__synthesize_modules(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1379| <<perf_event__synthesize_guest_os>> err = perf_event__synthesize_kernel_mmap(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1696| <<record__synthesize_workload>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1833| <<record__synthesize>> event_op f = process_synthesized_event;
+ *   - tools/perf/builtin-record.c|1840| <<record__synthesize>> process_synthesized_event);
+ *   - tools/perf/builtin-record.c|1848| <<record__synthesize>> process_synthesized_event, machine);
+ *   - tools/perf/builtin-record.c|1854| <<record__synthesize>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1861| <<record__synthesize>> session, process_synthesized_event);
+ *   - tools/perf/builtin-record.c|1867| <<record__synthesize>> err = perf_event__synthesize_kernel_mmap(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1873| <<record__synthesize>> err = perf_event__synthesize_modules(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1887| <<record__synthesize>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1893| <<record__synthesize>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1901| <<record__synthesize>> process_synthesized_event, NULL);
+ *   - tools/perf/builtin-record.c|1907| <<record__synthesize>> err = perf_event__synthesize_bpf_events(session, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|1915| <<record__synthesize>> err = perf_event__synthesize_cgroups(tool, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2407| <<__cmd_record>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2427| <<__cmd_record>> tgid, process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2407| <<__cmd_record>> process_synthesized_event,
+ *   - tools/perf/builtin-record.c|2427| <<__cmd_record>> tgid, process_synthesized_event,
+ */
 static int process_synthesized_event(struct perf_tool *tool,
 				     union perf_event *event,
 				     struct perf_sample *sample __maybe_unused,
@@ -622,16 +699,44 @@ static int process_locked_synthesized_event(struct perf_tool *tool,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1599| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+ *
+ * __cmd_record() or record__thread()
+ * -> record__mmap_read_all()
+ *    -> record__mmap_read_evlist()
+ *       -> perf_mmap__push()
+ *          -> record__pushfn()
+ */
 static int record__pushfn(struct mmap *map, void *to, void *bf, size_t size)
 {
 	struct record *rec = to;
 
+	/*
+	 * 似乎默认是0
+	 */
 	if (record__comp_enabled(rec)) {
 		size = zstd_compress(rec->session, map, map->data, mmap__mmap_len(map), bf, size);
 		bf   = map->data;
 	}
 
+	/*
+	 * 似乎只在这一处增加record_thread->samples:
+	 * - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+	 */
 	thread->samples++;
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|661| <<process_synthesized_event>> return record__write(rec, NULL, event, event->header.size);
+	 *   - tools/perf/builtin-record.c|688| <<record__pushfn>> return record__write(rec, map, bf, size);
+	 *   - tools/perf/builtin-record.c|768| <<record__process_auxtrace>> record__write(rec, map, event, event->header.size);
+	 *   - tools/perf/builtin-record.c|769| <<record__process_auxtrace>> record__write(rec, map, data1, len1);
+	 *   - tools/perf/builtin-record.c|771| <<record__process_auxtrace>> record__write(rec, map, data2, len2);
+	 *   - tools/perf/builtin-record.c|772| <<record__process_auxtrace>> record__write(rec, map, &pad, padding);
+	 *   - tools/perf/builtin-record.c|1607| <<record__mmap_read_evlist>> rc = record__write(rec, NULL, &finished_round_event, sizeof(finished_round_event));
+	 *   - tools/perf/builtin-record.c|1792| <<write_finished_init>> return record__write(rec, NULL, &finished_init_event, sizeof(finished_init_event));
+	 */
 	return record__write(rec, map, bf, size);
 }
 
@@ -683,6 +788,11 @@ static void record__sig_exit(void)
 
 #ifdef HAVE_AUXTRACE_SUPPORT
 
+/*
+ * 在以下使用record__process_auxtrace():
+ *   - tools/perf/builtin-record.c|793| <<record__auxtrace_mmap_read>> record__process_auxtrace);
+ *   - tools/perf/builtin-record.c|809| <<record__auxtrace_mmap_read_snapshot>> record__process_auxtrace,
+ */
 static int record__process_auxtrace(struct perf_tool *tool,
 				    struct mmap *map,
 				    union perf_event *event, void *data1,
@@ -1144,6 +1254,15 @@ static int record__alloc_thread_data(struct record *rec, struct evlist *evlist)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1261| <<record__mmap>> return record__mmap_evlist(rec, rec->evlist);
+ *
+ * __cmd_record()
+ * -> record__open()
+ *    -> record__mmap()
+ *       -> record__mmap_evlist()
+ */
 static int record__mmap_evlist(struct record *rec,
 			       struct evlist *evlist)
 {
@@ -1203,11 +1322,29 @@ static int record__mmap_evlist(struct record *rec,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1344| <<record__open>> rc = record__mmap(rec);
+ *
+ * __cmd_record()
+ * -> record__open()
+ *    -> record__mmap()
+ *       -> record__mmap_evlist()
+ */
 static int record__mmap(struct record *rec)
 {
 	return record__mmap_evlist(rec, rec->evlist);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2366| <<__cmd_record>> if (record__open(rec) != 0) {
+ *
+ * __cmd_record()
+ * -> record__open()
+ *    -> record__mmap()
+ *       -> record__mmap_evlist()
+ */
 static int record__open(struct record *rec)
 {
 	char msg[BUFSIZ];
@@ -1248,6 +1385,19 @@ static int record__open(struct record *rec)
 
 	evlist__for_each_entry(evlist, pos) {
 try_again:
+		/*
+		 * called by:
+		 *   - tools/perf/builtin-record.c|1251| <<record__open>> if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
+		 *   - tools/perf/builtin-top.c|1024| <<perf_top__start_counters>> if (evsel__open(counter, top->evlist->core.user_requested_cpus,
+		 *   - tools/perf/tests/mmap-basic.c|98| <<test__basic_mmap>> if (evsel__open(evsels[i], cpus, threads) < 0) {
+		 *   - tools/perf/tests/openat-syscall-all-cpus.c|56| <<test__openat_syscall_event_on_all_cpus>> if (evsel__open(evsel, cpus, threads) < 0) {
+		 *   - tools/perf/util/evlist.c|1361| <<evlist__open>> err = evsel__open(evsel, evsel->core.cpus, evsel->core.threads);
+		 *   - tools/perf/util/evsel.c|2207| <<evsel__open_per_thread>> return evsel__open(evsel, NULL, threads);
+		 *   - tools/perf/util/parse-events.c|178| <<is_event_supported>> open_return = evsel__open(evsel, NULL, tmap);
+		 *   - tools/perf/util/parse-events.c|190| <<is_event_supported>> ret = evsel__open(evsel, NULL, tmap) >= 0;
+		 *   - tools/perf/util/python.c|904| <<pyrf_evsel__open>> if (evsel__open(evsel, cpus, threads) < 0) {
+		 *   - tools/perf/util/sideband_evlist.c|117| <<evlist__start_sb_thread>> if (evsel__open(counter, evlist->core.user_requested_cpus,
+		 */
 		if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
 			if (evsel__fallback(pos, errno, msg, sizeof(msg))) {
 				if (verbose > 0)
@@ -1288,6 +1438,15 @@ static int record__open(struct record *rec)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1344| <<record__open>> rc = record__mmap(rec);
+	 *
+	 * __cmd_record()
+	 * -> record__open()
+	 *    -> record__mmap()
+	 *       -> record__mmap_evlist()
+	 */
 	rc = record__mmap(rec);
 	if (rc)
 		goto out;
@@ -1307,6 +1466,57 @@ static void set_timestamp_boundary(struct record *rec, u64 sample_time)
 		rec->evlist->last_sample_time = sample_time;
 }
 
+/*
+ * 3433 static struct record record = {
+ * 3434         .opts = {
+ * 3435                 .sample_time         = true,
+ * 3436                 .mmap_pages          = UINT_MAX,
+ * 3437                 .user_freq           = UINT_MAX,
+ * 3438                 .user_interval       = ULLONG_MAX,
+ * 3439                 .freq                = 4000,
+ * 3440                 .target              = {
+ * 3441                         .uses_mmap   = true,
+ * 3442                         .default_per_cpu = true,
+ * 3443                 },
+ * 3444                 .mmap_flush          = MMAP_FLUSH_DEFAULT,
+ * 3445                 .nr_threads_synthesize = 1,
+ * 3446                 .ctl_fd              = -1,
+ * 3447                 .ctl_fd_ack          = -1,
+ * 3448                 .synth               = PERF_SYNTH_ALL,
+ * 3449         },
+ * 3450         .tool = {
+ * 3451                 .sample         = process_sample_event,
+ * 3452                 .fork           = perf_event__process_fork,
+ * 3453                 .exit           = perf_event__process_exit,
+ * 3454                 .comm           = perf_event__process_comm,
+ * 3455                 .namespaces     = perf_event__process_namespaces,
+ * 3456                 .mmap           = build_id__process_mmap,
+ * 3457                 .mmap2          = build_id__process_mmap2,
+ * 3458                 .itrace_start   = process_timestamp_boundary,
+ * 3459                 .aux            = process_timestamp_boundary,
+ * 3460                 .ordered_events = true,
+ * 3461         },
+ * 3461 };
+ *
+ * (gdb) bt
+ * #0  process_sample_event (tool=0x707020 <record>, event=0x7ffff7e69da8, sample=0x7fffffff8100, evsel=0x7c6130, machine=0x7a8148) at builtin-record.c:1318
+ * #1  0x0000000000508e80 in perf_session__deliver_event (session=0x7a7f40, event=0x7ffff7e69da8, tool=0x707020 <record>, file_offset=7592, file_path=0x7c6640 "perf.data") at util/session.c:1635
+ * #2  0x000000000050dbb6 in do_flush (show_progress=true, oe=0x7aeac0) at util/ordered-events.c:245
+ * #3  __ordered_events__flush (oe=0x7aeac0, oe@entry=0x0, how=how@entry=OE_FLUSH__FINAL, timestamp=timestamp@entry=0) at util/ordered-events.c:324
+ * #4  0x000000000050e3e5 in __ordered_events__flush (timestamp=<optimized out>, how=<optimized out>, oe=<optimized out>) at util/ordered-events.c:340
+ * #5  ordered_events__flush (how=OE_FLUSH__FINAL, oe=0x0) at util/ordered-events.c:342
+ * #6  ordered_events__flush (oe=oe@entry=0x7aeac0, how=how@entry=OE_FLUSH__FINAL) at util/ordered-events.c:340
+ * #7  0x000000000050b38d in __perf_session__process_events (session=0x7a7f40) at util/session.c:2456
+ * #8  perf_session__process_events (session=session@entry=0x7a7f40) at util/session.c:2618
+ * #9  0x000000000041f1f2 in process_buildids (rec=0x707020 <record>) at builtin-record.c:1357
+ * #10 record__finish_output (rec=0x707020 <record>) at builtin-record.c:1675
+ * #11 0x00000000004223c0 in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2649
+ * #12 0x00000000004256c5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4089
+ * #13 0x00000000004a547b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #14 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #15 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #16 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ */
 static int process_sample_event(struct perf_tool *tool,
 				union perf_event *event,
 				struct perf_sample *sample,
@@ -1450,6 +1660,23 @@ static size_t zstd_compress(struct perf_session *session, struct mmap *map,
 	return compressed;
 }
 
+/*
+ * (gdb) bt
+ * #0  record__mmap_read_evlist (rec=rec@entry=0x707020 <record>, evlist=0x7a7480, overwrite=overwrite@entry=false, 
+ *     synch=synch@entry=true) at builtin-record.c:1456
+ * #1  0x000000000042352e in record__mmap_read_all (synch=true, rec=0x707020 <record>) at builtin-record.c:2620
+ * #2  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2620
+ * #3  0x0000000000425705 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4097
+ * #4  0x00000000004a55bb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, 
+ *     argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #5  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #6  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #7  main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/builtin-record.c|1619| <<record__mmap_read_all>> err = record__mmap_read_evlist(rec, rec->evlist, false, synch);
+ *   - tools/perf/builtin-record.c|1623| <<record__mmap_read_all>> return record__mmap_read_evlist(rec, rec->evlist, true, synch);
+ */
 static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 				    bool overwrite, bool synch)
 {
@@ -1458,12 +1685,28 @@ static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 	int rc = 0;
 	int nr_mmaps;
 	struct mmap **maps;
+	/*
+	 * struct record *rec:
+	 * -> struct perf_data data;
+	 *    -> const char *path;
+	 *    -> struct perf_data_file file;
+	 *         char            *path;
+	 *         union {                      
+	 *             int      fd;
+	 *             FILE    *fptr;
+	 *         };
+	 *         unsigned long    size;
+	 */
 	int trace_fd = rec->data.file.fd;
 	off_t off = 0;
 
 	if (!evlist)
 		return 0;
 
+	/*
+	 * 文件的开头声明:
+	 * static __thread struct record_thread *thread;
+	 */
 	nr_mmaps = thread->nr_mmaps;
 	maps = overwrite ? thread->overwrite_maps : thread->maps;
 
@@ -1487,6 +1730,12 @@ static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 				map->core.flush = 1;
 			}
 			if (!record__aio_enabled(rec)) {
+				/*
+				 * called by:
+				 *   - 定义tools/perf/util/mmap.c|324| <<global>> int perf_mmap__push(struct mmap *md, void *to,
+				 *   - tools/perf/builtin-record.c|468| <<record__aio_push>> ret = perf_mmap__push(map, &aio, record__aio_pushfn);
+				 *   - tools/perf/builtin-record.c|1608| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+				 */
 				if (perf_mmap__push(map, rec, record__pushfn) < 0) {
 					if (synch)
 						map->core.flush = flush;
@@ -1534,6 +1783,13 @@ static int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1658| <<record__thread>> if (record__mmap_read_all(thread->rec, false) < 0 || terminate)
+ *   - tools/perf/builtin-record.c|1687| <<record__thread>> record__mmap_read_all(thread->rec, true);
+ *   - tools/perf/builtin-record.c|2601| <<__cmd_record>> if (record__mmap_read_all(rec, false) < 0) {
+ *   - tools/perf/builtin-record.c|2738| <<__cmd_record>> record__mmap_read_all(rec, true);
+ */
 static int record__mmap_read_all(struct record *rec, bool synch)
 {
 	int err;
@@ -1542,6 +1798,11 @@ static int record__mmap_read_all(struct record *rec, bool synch)
 	if (err)
 		return err;
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1619| <<record__mmap_read_all>> err = record__mmap_read_evlist(rec, rec->evlist, false, synch);
+	 *   - tools/perf/builtin-record.c|1623| <<record__mmap_read_all>> return record__mmap_read_evlist(rec, rec->evlist, true, synch);
+	 */
 	return record__mmap_read_evlist(rec, rec->evlist, true, synch);
 }
 
@@ -1554,6 +1815,10 @@ static void record__thread_munmap_filtered(struct fdarray *fda, int fd,
 		perf_mmap__put(map);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2282| <<record__start_threads>> if (pthread_create(&handle, &attrs, record__thread, &thread_data[t])) {
+ */
 static void *record__thread(void *arg)
 {
 	enum thread_msg msg = THREAD_MSG__READY;
@@ -1577,6 +1842,13 @@ static void *record__thread(void *arg)
 	for (;;) {
 		unsigned long long hits = thread->samples;
 
+		/*
+		 * called by:
+		 *   - tools/perf/builtin-record.c|1658| <<record__thread>> if (record__mmap_read_all(thread->rec, false) < 0 || terminate)
+		 *   - tools/perf/builtin-record.c|1687| <<record__thread>> record__mmap_read_all(thread->rec, true);
+		 *   - tools/perf/builtin-record.c|2601| <<__cmd_record>> if (record__mmap_read_all(rec, false) < 0) {
+		 *   - tools/perf/builtin-record.c|2738| <<__cmd_record>> record__mmap_read_all(rec, true);
+		 */
 		if (record__mmap_read_all(thread->rec, false) < 0 || terminate)
 			break;
 
@@ -1651,6 +1923,11 @@ static void record__init_features(struct record *rec)
 	perf_header__clear_feat(&session->header, HEADER_STAT);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1791| <<record__switch_output>> record__finish_output(rec);
+ *   - tools/perf/builtin-record.c|2714| <<__cmd_record>> record__finish_output(rec);
+ */
 static void
 record__finish_output(struct record *rec)
 {
@@ -1679,6 +1956,12 @@ record__finish_output(struct record *rec)
 	return;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2002| <<record__switch_output>> record__synthesize_workload(rec, true);
+ *   - tools/perf/builtin-record.c|2053| <<record__switch_output>> record__synthesize_workload(rec, false);
+ *   - tools/perf/builtin-record.c|2965| <<__cmd_record>> record__synthesize_workload(rec, true);
+ */
 static int record__synthesize_workload(struct record *rec, bool tail)
 {
 	int err;
@@ -1701,6 +1984,13 @@ static int record__synthesize_workload(struct record *rec, bool tail)
 	return err;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1814| <<record__switch_output>> write_finished_init(rec, true);
+ *   - tools/perf/builtin-record.c|1870| <<record__switch_output>> write_finished_init(rec, false);
+ *   - tools/perf/builtin-record.c|2580| <<__cmd_record>> err = write_finished_init(rec, false);
+ *   - tools/perf/builtin-record.c|2731| <<__cmd_record>> write_finished_init(rec, true);
+ */
 static int write_finished_init(struct record *rec, bool tail)
 {
 	if (rec->opts.tail_synthesize != tail)
@@ -1711,6 +2001,11 @@ static int write_finished_init(struct record *rec, bool tail)
 
 static int record__synthesize(struct record *rec, bool tail);
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2578| <<__cmd_record>> fd = record__switch_output(rec, false);
+ *   - tools/perf/builtin-record.c|2711| <<__cmd_record>> fd = record__switch_output(rec, true);
+ */
 static int
 record__switch_output(struct record *rec, bool at_exit)
 {
@@ -1822,6 +2117,13 @@ static const struct perf_event_mmap_page *record__pick_pc(struct record *rec)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1765| <<record__switch_output>> record__synthesize(rec, true);
+ *   - tools/perf/builtin-record.c|1806| <<record__switch_output>> record__synthesize(rec, false);
+ *   - tools/perf/builtin-record.c|2396| <<__cmd_record>> err = record__synthesize(rec, false);
+ *   - tools/perf/builtin-record.c|2674| <<__cmd_record>> record__synthesize(rec, true);
+ */
 static int record__synthesize(struct record *rec, bool tail)
 {
 	struct perf_session *session = rec->session;
@@ -1948,6 +2250,10 @@ static int record__process_signal_event(union perf_event *event __maybe_unused,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2587| <<__cmd_record>> err = record__setup_sb_evlist(rec);
+ */
 static int record__setup_sb_evlist(struct record *rec)
 {
 	struct record_opts *opts = &rec->opts;
@@ -2033,6 +2339,10 @@ static void hit_auxtrace_snapshot_trigger(struct record *rec)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2364| <<__cmd_record>> record__uniquify_name(rec);
+ */
 static void record__uniquify_name(struct record *rec)
 {
 	struct evsel *pos;
@@ -2040,6 +2350,9 @@ static void record__uniquify_name(struct record *rec)
 	char *new_name;
 	int ret;
 
+	/*
+	 * 一般不用hybrid
+	 */
 	if (!perf_pmu__has_hybrid())
 		return;
 
@@ -2077,6 +2390,10 @@ static int record__terminate_thread(struct record_thread *thread_data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2593| <<__cmd_record>> if (record__start_threads(rec))
+ */
 static int record__start_threads(struct record *rec)
 {
 	int t, tt, err, ret = 0, nr_threads = rec->nr_threads;
@@ -2140,6 +2457,10 @@ static int record__start_threads(struct record *rec)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2913| <<__cmd_record>> record__stop_threads(rec);
+ */
 static int record__stop_threads(struct record *rec)
 {
 	int t;
@@ -2178,6 +2499,32 @@ static unsigned long record__waking(struct record *rec)
 	return waking;
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at ut    il/ses          sion.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ *
+ * called by:
+ *   - tools/perf/builtin-record.c|4079| <<cmd_record>> err = __cmd_record(&record, argc, argv);
+ *
+ * struct record:
+ * -> struct evlist *evlist;
+ * -> struct perf_session *session;
+ */
 static int __cmd_record(struct record *rec, int argc, const char **argv)
 {
 	int err;
@@ -2237,6 +2584,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		}
 	}
 
+	/*
+	 * 在open_file_write()创建
+	 * (gdb) p data->file.path
+	 * $1 = 0x7c6640 "perf.data"
+	 *
+	 * int fd;
+	 */
 	fd = perf_data__fd(data);
 	rec->session = session;
 
@@ -2294,6 +2648,14 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 
 	record__uniquify_name(rec);
 
+	/*
+	 * __cmd_record()
+	 * -> record__open()
+	 *    -> record__mmap()
+	 *       -> record__mmap_evlist()
+	 *
+	 * 这里打开那些fd
+	 */
 	if (record__open(rec) != 0) {
 		err = -1;
 		goto out_free_threads;
@@ -2330,11 +2692,21 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	if (!rec->evlist->core.nr_groups)
 		perf_header__clear_feat(&session->header, HEADER_GROUP_DESC);
 
+	/*
+	 * 一般的record的is_pipe是zero
+	 */
 	if (data->is_pipe) {
 		err = perf_header__write_pipe(fd);
 		if (err < 0)
 			goto out_free_threads;
 	} else {
+		/*
+		 * 在open_file_write()创建
+		 * (gdb) p data->file.path
+		 * $1 = 0x7c6640 "perf.data"
+		 *
+		 * int fd;
+		 */
 		err = perf_session__write_header(session, rec->evlist, fd, false);
 		if (err < 0)
 			goto out_free_threads;
@@ -2352,6 +2724,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	if (err)
 		goto out_free_threads;
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1765| <<record__switch_output>> record__synthesize(rec, true);
+	 *   - tools/perf/builtin-record.c|1806| <<record__switch_output>> record__synthesize(rec, false);
+	 *   - tools/perf/builtin-record.c|2396| <<__cmd_record>> err = record__synthesize(rec, false);
+	 *   - tools/perf/builtin-record.c|2674| <<__cmd_record>> record__synthesize(rec, true);
+	 */
 	err = record__synthesize(rec, false);
 	if (err < 0)
 		goto out_free_threads;
@@ -2367,8 +2746,17 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		}
 	}
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|2593| <<__cmd_record>> if (record__start_threads(rec))
+	 *
+	 * 每个thread的函数是record__thread() !!!!!!!!!!
+	 */
 	if (record__start_threads(rec))
 		goto out_free_threads;
+	/*
+	 * 这里还没有hang
+	 */
 
 	/*
 	 * When perf is starting the traced process, all the events
@@ -2386,6 +2774,15 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		union perf_event *event;
 		pid_t tgid;
 
+		/*
+		 * union perf_event {
+		 *     struct perf_event_header                header;
+		 *     struct perf_record_mmap                 mmap;
+		 *     struct perf_record_mmap2                mmap2;
+		 *     struct perf_record_comm                 comm;
+		 *     struct perf_record_namespaces           namespaces;
+		 *     ... ...
+		 */
 		event = malloc(sizeof(event->comm) + machine->id_hdr_size);
 		if (event == NULL) {
 			err = -ENOMEM;
@@ -2440,6 +2837,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	trigger_ready(&switch_output_trigger);
 	perf_hooks__invoke_record_start();
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1814| <<record__switch_output>> write_finished_init(rec, true);
+	 *   - tools/perf/builtin-record.c|1870| <<record__switch_output>> write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2580| <<__cmd_record>> err = write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2731| <<__cmd_record>> write_finished_init(rec, true);
+	 */
 	/*
 	 * Must write FINISHED_INIT so it will be seen after all other
 	 * synthesized user events, but before any regular events.
@@ -2448,7 +2852,19 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 	if (err < 0)
 		goto out_child;
 
+	/*
+	 * 时间都花在for loop
+	 */
 	for (;;) {
+		/*
+		 * __cmd_record() or record__thread()
+		 * -> record__mmap_read_all()
+		 *    -> record__mmap_read_evlist()
+		 *       -> record__pushfn()
+		 *
+		 * 似乎只在这一处增加record_thread->samples:
+		 *   - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+		 */
 		unsigned long long hits = thread->samples;
 
 		/*
@@ -2462,6 +2878,20 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		if (trigger_is_hit(&switch_output_trigger) || done || draining)
 			evlist__toggle_bkw_mmap(rec->evlist, BKW_MMAP_DATA_PENDING);
 
+		/*
+		 * __cmd_record() or record__thread()
+		 * -> record__mmap_read_all()
+		 *    -> record__mmap_read_evlist()
+		 *       -> perf_mmap__push()
+		 *          -> record__pushfn()
+		 *
+		 * record__mmap_read_all()
+		 * -> record__mmap_read_evlist()
+		 *    -> perf_mmap__push(map, rec, record__pushfn)
+		 *       -> perf_mmap__read_init()
+		 *       -> push = record__pushfn()
+		 *       -> perf_mmap__consume()
+		 */
 		if (record__mmap_read_all(rec, false) < 0) {
 			trigger_error(&auxtrace_snapshot_trigger);
 			trigger_error(&switch_output_trigger);
@@ -2518,7 +2948,16 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 				alarm(rec->switch_output.time);
 		}
 
+		/*
+		 * 似乎只在这一处增加record_thread->samples:
+		 *   - tools/perf/builtin-record.c|711| <<record__pushfn>> thread->samples++;
+		 *
+		 * struct record_thread
+		 */
 		if (hits == thread->samples) {
+			/*
+			 * failed的测试是在sig_handler()设置的done, 可能进程被杀死了
+			 */
 			if (done || draining)
 				break;
 			err = fdarray__poll(&thread->pollfd, -1);
@@ -2592,6 +3031,13 @@ static int __cmd_record(struct record *rec, int argc, const char **argv)
 		fprintf(stderr, "[ perf record: Woken up %ld times to write data ]\n",
 			record__waking(rec));
 
+	/*
+	 * called by:
+	 *   - tools/perf/builtin-record.c|1814| <<record__switch_output>> write_finished_init(rec, true);
+	 *   - tools/perf/builtin-record.c|1870| <<record__switch_output>> write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2580| <<__cmd_record>> err = write_finished_init(rec, false);
+	 *   - tools/perf/builtin-record.c|2731| <<__cmd_record>> write_finished_init(rec, true);
+	 */
 	write_finished_init(rec, true);
 
 	if (target__none(&rec->opts.target))
@@ -3765,6 +4211,25 @@ static int record__init_thread_masks(struct record *rec)
 	return ret;
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at ut    il/ses     sion.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ */
 int cmd_record(int argc, const char **argv)
 {
 	int err;
diff --git a/tools/perf/builtin-report.c b/tools/perf/builtin-report.c
index 91ed41cc7..65fbb6551 100644
--- a/tools/perf/builtin-report.c
+++ b/tools/perf/builtin-report.c
@@ -463,6 +463,14 @@ static size_t hists__fprintf_nr_sample_events(struct hists *hists, struct report
 	size_t ret;
 	char unit;
 	unsigned long nr_samples = hists->stats.nr_samples;
+	/*
+	 * struct hists *hists:
+	 * -> struct hists_stats stats;
+	 *    -> u64 total_period;
+	 *    -> u64 total_non_filtered_period;
+	 *    -> u32 nr_samples;
+	 *    -> u32 nr_non_filtered_samples;
+	 */
 	u64 nr_events = hists->stats.total_period;
 	struct evsel *evsel = hists_to_evsel(hists);
 	char buf[512];
@@ -726,6 +734,10 @@ static int hists__resort_cb(struct hist_entry *he, void *arg)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-report.c|1017| <<__cmd_report>> report__output_resort(rep);
+ */
 static void report__output_resort(struct report *rep)
 {
 	struct ui_progress prog;
@@ -920,6 +932,21 @@ static int tasks_print(struct report *rep, FILE *fp)
 	return 0;
 }
 
+/*
+ * (gdb) bt
+ * #0  hists__inc_stats (h=0xda58e0, hists=0x7ae5e0) at util/hist.c:1933
+ * #1  output_resort (hists=hists@entry=0x7ae5e0, prog=<optimized out>, use_callchain=<optimized out>, cb=cb@entry=0x426bb0 <hists__resort_cb>, cb_arg=<optimized out>)
+ *     at util/hist.c:1933
+ * #2  0x0000000000533bd5 in evsel__output_resort_cb (evsel=evsel@entry=0x7ae380, prog=prog@entry=0x7fffffffb640, cb=cb@entry=0x426bb0 <hists__resort_cb>,
+ *     cb_arg=cb_arg@entry=0x7fffffffb7f0) at util/hist.h:246
+ * #3  0x00000000004295cb in report__output_resort (rep=0x7fffffffb7f0) at builtin-report.c:739
+ * #4  __cmd_report (rep=0x7fffffffb7f0) at builtin-report.c:1019
+ * #5  cmd_report (argc=<optimized out>, argv=<optimized out>) at builtin-report.c:1659
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719620 <commands+288>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdfe0) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=2, argv=0x7fffffffdfe0) at perf.c:544
+ */
 static int __cmd_report(struct report *rep)
 {
 	int ret;
diff --git a/tools/perf/builtin-stat.c b/tools/perf/builtin-stat.c
index 0b4a62e4f..a5b7c280c 100644
--- a/tools/perf/builtin-stat.c
+++ b/tools/perf/builtin-stat.c
@@ -336,6 +336,10 @@ static int evsel__write_stat_event(struct evsel *counter, int cpu_map_idx, u32 t
 					   process_synthesized_event, NULL);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|395| <<read_counter_cpu>> read_single_counter(counter, cpu_map_idx, thread, rs)) {
+ */
 static int read_single_counter(struct evsel *counter, int cpu_map_idx,
 			       int thread, struct timespec *rs)
 {
@@ -374,6 +378,10 @@ static int read_single_counter(struct evsel *counter, int cpu_map_idx,
  * Read out the results of a single counter:
  * do not aggregate counts across CPUs in system-wide mode
  */
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|450| <<read_affinity_counters>> counter->err = read_counter_cpu(counter, rs,
+ */
 static int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_map_idx)
 {
 	int nthreads = perf_thread_map__nr(evsel_list->core.threads);
@@ -421,6 +429,10 @@ static int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|487| <<read_counters>> read_affinity_counters(rs))
+ */
 static int read_affinity_counters(struct timespec *rs)
 {
 	struct evlist_cpu_iterator evlist_cpu_itr;
@@ -469,6 +481,11 @@ static int read_bpf_map_counters(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|540| <<process_interval>> read_counters(&rs);
+ *   - tools/perf/builtin-stat.c|1075| <<__run_perf_stat>> read_counters(&(struct timespec) { .tv_nsec = t1-t0 });
+ */
 static void read_counters(struct timespec *rs)
 {
 	struct evsel *counter;
@@ -760,6 +777,13 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 		 * errored is a sticky flag that means one of the counter's
 		 * cpu event had a problem and needs to be reexamined.
 		 */
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		counter->errored = true;
 
 		if ((evsel__leader(counter) != counter) ||
@@ -791,6 +815,10 @@ static enum counter_recovery stat_handle_error(struct evsel *counter)
 	return COUNTER_FATAL;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|1100| <<run_perf_stat>> ret = __run_perf_stat(argc, argv, run_idx);
+ */
 static int __run_perf_stat(int argc, const char **argv, int run_idx)
 {
 	int interval = stat_config.interval;
@@ -826,6 +854,16 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	}
 
 	evlist__for_each_entry(evsel_list, counter) {
+		/*
+		 * 在以下使用evsel->reset_group:
+		 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+		 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true;
+		 */
 		counter->reset_group = false;
 		if (bpf_counter__load(counter, &target))
 			return -1;
@@ -833,6 +871,16 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 			all_counters_use_bpf = false;
 	}
 
+	/*
+	 * evlist__for_each_cpu - without affinity, iterate over the evlist. With
+	 *                        affinity, iterate over all CPUs and then the evlist
+	 *                        for each evsel on that CPU. When switching between
+	 *                        CPUs the affinity is set to the CPU to avoid IPIs
+	 *                        during syscalls.
+	 * @evlist_cpu_itr: the iterator instance.
+	 * @evlist: evlist instance to iterate.
+	 * @affinity: NULL or used to set the affinity to the current CPU.
+	 */
 	evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 		counter = evlist_cpu_itr.evsel;
 
@@ -843,6 +891,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		if (target.use_bpf)
 			break;
 
+		/*
+		 * 在以下使用evsel->errored:
+		 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+		 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+		 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+		 */
 		if (counter->reset_group || counter->errored)
 			continue;
 		if (evsel__is_bpf(counter))
@@ -892,6 +947,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 
@@ -901,6 +963,13 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 		evlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {
 			counter = evlist_cpu_itr.evsel;
 
+			/*
+			 * 在以下使用evsel->errored:
+			 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+			 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+			 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+			 */
 			if (!counter->reset_group && !counter->errored)
 				continue;
 			if (!counter->reset_group)
@@ -1042,6 +1111,10 @@ static int __run_perf_stat(int argc, const char **argv, int run_idx)
 	return WEXITSTATUS(status);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|2606| <<cmd_stat>> status = run_perf_stat(argc, argv, run_idx);
+ */
 static int run_perf_stat(int argc, const char **argv, int run_idx)
 {
 	int ret;
@@ -2233,6 +2306,10 @@ static void setup_system_wide(int forks)
 	}
 }
 
+/*
+ * 在以下使用cmd_stat():
+ *   - tools/perf/perf.c|72| <<global>> { "stat", cmd_stat, 0 },
+ */
 int cmd_stat(int argc, const char **argv)
 {
 	const char * const stat_usage[] = {
diff --git a/tools/perf/builtin-top.c b/tools/perf/builtin-top.c
index fd8fd913c..64574b73e 100644
--- a/tools/perf/builtin-top.c
+++ b/tools/perf/builtin-top.c
@@ -1021,6 +1021,19 @@ static int perf_top__start_counters(struct perf_top *top)
 
 	evlist__for_each_entry(evlist, counter) {
 try_again:
+		/*
+		 * called by:
+		 *   - tools/perf/builtin-record.c|1251| <<record__open>> if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
+		 *   - tools/perf/builtin-top.c|1024| <<perf_top__start_counters>> if (evsel__open(counter, top->evlist->core.user_requested_cpus,
+		 *   - tools/perf/tests/mmap-basic.c|98| <<test__basic_mmap>> if (evsel__open(evsels[i], cpus, threads) < 0) {
+		 *   - tools/perf/tests/openat-syscall-all-cpus.c|56| <<test__openat_syscall_event_on_all_cpus>> if (evsel__open(evsel, cpus, threads) < 0) {
+		 *   - tools/perf/util/evlist.c|1361| <<evlist__open>> err = evsel__open(evsel, evsel->core.cpus, evsel->core.threads);
+		 *   - tools/perf/util/evsel.c|2207| <<evsel__open_per_thread>> return evsel__open(evsel, NULL, threads);
+		 *   - tools/perf/util/parse-events.c|178| <<is_event_supported>> open_return = evsel__open(evsel, NULL, tmap);
+		 *   - tools/perf/util/parse-events.c|190| <<is_event_supported>> ret = evsel__open(evsel, NULL, tmap) >= 0;
+		 *   - tools/perf/util/python.c|904| <<pyrf_evsel__open>> if (evsel__open(evsel, cpus, threads) < 0) {
+		 *   - tools/perf/util/sideband_evlist.c|117| <<evlist__start_sb_thread>> if (evsel__open(counter, evlist->core.user_requested_cpus,
+		 */
 		if (evsel__open(counter, top->evlist->core.user_requested_cpus,
 				     top->evlist->core.threads) < 0) {
 
diff --git a/tools/perf/util/counts.c b/tools/perf/util/counts.c
index 7a447d918..9b2cbc1b1 100644
--- a/tools/perf/util/counts.c
+++ b/tools/perf/util/counts.c
@@ -7,6 +7,11 @@
 #include <perf/threadmap.h>
 #include <linux/zalloc.h>
 
+/*
+ * called by:
+ *   - tools/perf/util/counts.c|64| <<evsel__alloc_counts>> evsel->counts = perf_counts__new(perf_cpu_map__nr(cpus), nthreads);
+ *   - tools/perf/util/stat.c|164| <<evsel__alloc_prev_raw_counts>> counts = perf_counts__new(cpu_map_nr, nthreads);
+ */
 struct perf_counts *perf_counts__new(int ncpus, int nthreads)
 {
 	struct perf_counts *counts = zalloc(sizeof(*counts));
@@ -51,11 +56,21 @@ void perf_counts__reset(struct perf_counts *counts)
 	memset(&counts->aggr, 0, sizeof(struct perf_counts_values));
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|1640| <<__evsel__read_on_cpu>> if (evsel->counts == NULL && evsel__alloc_counts(evsel) < 0)
+ *   - tools/perf/util/stat.c|186| <<evsel__alloc_stats>> evsel__alloc_counts(evsel) < 0 ||
+ */
 void evsel__reset_counts(struct evsel *evsel)
 {
 	perf_counts__reset(evsel->counts);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|1640| <<__evsel__read_on_cpu>> if (evsel->counts == NULL && evsel__alloc_counts(evsel) < 0)
+ *   - tools/perf/util/stat.c|190| <<evsel__alloc_stats>> evsel__alloc_counts(evsel) < 0 ||
+ */
 int evsel__alloc_counts(struct evsel *evsel)
 {
 	struct perf_cpu_map *cpus = evsel__cpus(evsel);
diff --git a/tools/perf/util/data.c b/tools/perf/util/data.c
index a7f68c309..3ae98ddca 100644
--- a/tools/perf/util/data.c
+++ b/tools/perf/util/data.c
@@ -284,11 +284,34 @@ static int open_file_read(struct perf_data *data)
 	return -1;
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at util/session.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ */
 static int open_file_write(struct perf_data *data)
 {
 	int fd;
 	char sbuf[STRERR_BUFSIZE];
 
+	/*
+	 * (gdb) p data->file.path
+	 * $1 = 0x7c6640 "perf.data"
+	 */
 	fd = open(data->file.path, O_CREAT|O_RDWR|O_TRUNC|O_CLOEXEC,
 		  S_IRUSR|S_IWUSR);
 
@@ -348,6 +371,12 @@ static int open_dir(struct perf_data *data)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-inject.c|2202| <<cmd_inject>> if (perf_data__open(&inject.output)) {
+ *   - tools/perf/util/data.c|436| <<perf_data__switch>> ret = perf_data__open(data);
+ *   - tools/perf/util/session.c|213| <<__perf_session__new>> ret = perf_data__open(data);
+ */
 int perf_data__open(struct perf_data *data)
 {
 	if (check_pipe(data))
diff --git a/tools/perf/util/data.h b/tools/perf/util/data.h
index effcc195d..9b2c535e0 100644
--- a/tools/perf/util/data.h
+++ b/tools/perf/util/data.h
@@ -68,6 +68,43 @@ static inline bool perf_data__is_single_file(struct perf_data *data)
 	return data->dir.version == PERF_DIR_SINGLE_FILE;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-inject.c|260| <<perf_event__repipe_auxtrace>> ret = copy_bytes(inject, perf_data__fd(session->data),
+ *   - tools/perf/builtin-inject.c|1737| <<save_section_info>> int fd = perf_data__fd(inject->session->data);
+ *   - tools/perf/builtin-inject.c|1797| <<feat_copy>> int fd = perf_data__fd(inject->session->data);
+ *   - tools/perf/builtin-inject.c|1871| <<output_fd>> return inject->in_place_update ? -1 : perf_data__fd(&inject->output);
+ *   - tools/perf/builtin-inject.c|2237| <<cmd_inject>> ret = perf_header__write_pipe(perf_data__fd(&inject.output));
+ *   - tools/perf/builtin-record.c|698| <<record__process_auxtrace>> int fd = perf_data__fd(data);
+ *   - tools/perf/builtin-record.c|1659| <<record__finish_output>> int fd = perf_data__fd(data);
+ *   - tools/perf/builtin-record.c|1665| <<record__finish_output>> data->file.size = lseek(perf_data__fd(data), 0, SEEK_CUR);
+ *   - tools/perf/builtin-record.c|2240| <<__cmd_record>> fd = perf_data__fd(data);
+ *   - tools/perf/builtin-stat.c|1011| <<__run_perf_stat>> int fd = perf_data__fd(&perf_stat.data);
+ *   - tools/perf/builtin-stat.c|1014| <<__run_perf_stat>> err = perf_header__write_pipe(perf_data__fd(&perf_stat.data));
+ *   - tools/perf/builtin-stat.c|2656| <<cmd_stat>> int fd = perf_data__fd(&perf_stat.data);
+ *   - tools/perf/builtin-timechart.c|1612| <<__cmd_timechart>> perf_data__fd(session->data),
+ *   - tools/perf/util/arm-spe.c|170| <<arm_spe_get_trace>> int fd = perf_data__fd(speq->spe->session->data);
+ *   - tools/perf/util/arm-spe.c|938| <<arm_spe_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/auxtrace.c|264| <<auxtrace_copy_data>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/cs-etm.c|1076| <<cs_etm__get_trace>> int fd = perf_data__fd(etmq->etm->session->data);
+ *   - tools/perf/util/cs-etm.c|2451| <<cs_etm__process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/data-convert-json.c|229| <<output_headers>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/header.c|3446| <<perf_header__fprintf_info>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/header.c|4137| <<perf_session__read_header>> int fd = perf_data__fd(data);
+ *   - tools/perf/util/header.c|4427| <<perf_event__process_tracing_data>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/intel-bts.c|484| <<intel_bts_process_queue>> int fd = perf_data__fd(btsq->bts->session->data);
+ *   - tools/perf/util/intel-bts.c|648| <<intel_bts_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/intel-pt.c|401| <<intel_pt_get_buffer>> int fd = perf_data__fd(ptq->pt->session->data);
+ *   - tools/perf/util/intel-pt.c|3519| <<intel_pt_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/s390-cpumsf.c|730| <<s390_cpumsf_run_decoder>> int fd = perf_data__fd(sfq->sf->session->data);
+ *   - tools/perf/util/s390-cpumsf.c|968| <<s390_cpumsf_process_auxtrace_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/session.c|397| <<process_event_auxtrace_stub>> skipn(perf_data__fd(session->data), event->auxtrace.size);
+ *   - tools/perf/util/session.c|1652| <<perf_session__process_user_event>> int fd = perf_data__fd(session->data);
+ *   - tools/perf/util/session.c|1769| <<perf_session__peek_event>> fd = perf_data__fd(session->data);
+ *   - tools/perf/util/session.c|2433| <<__perf_session__process_events>> .fd = perf_data__fd(session->data),
+ *   - tools/perf/util/session.c|2514| <<__perf_session__process_dir_events>> .fd = perf_data__fd(session->data),
+ *   - tools/perf/util/synthetic-events.c|2342| <<perf_event__synthesize_for_pipe>> int fd = perf_data__fd(data);
+ */
 static inline int perf_data__fd(struct perf_data *data)
 {
 	if (data->use_stdio)
diff --git a/tools/perf/util/evlist.c b/tools/perf/util/evlist.c
index 48167f394..4f06ed23e 100644
--- a/tools/perf/util/evlist.c
+++ b/tools/perf/util/evlist.c
@@ -579,6 +579,29 @@ static void __evlist__enable(struct evlist *evlist, char *evsel_name)
 	evlist->enabled = true;
 }
 
+/*
+ * called by:
+ *   - tools/perf/bench/evlist-open-close.c|131| <<bench__do_evlist_open_close>> evlist__enable(evlist);
+ *   - tools/perf/builtin-kvm.c|983| <<kvm_events_live_report>> evlist__enable(kvm->evlist);
+ *   - tools/perf/builtin-record.c|2512| <<__cmd_record>> evlist__enable(rec->evlist);
+ *   - tools/perf/builtin-record.c|2567| <<__cmd_record>> evlist__enable(rec->evlist);
+ *   - tools/perf/builtin-stat.c|610| <<enable_counters>> evlist__enable(evsel_list);
+ *   - tools/perf/builtin-top.c|1305| <<__cmd_top>> evlist__enable(top->evlist);
+ *   - tools/perf/builtin-trace.c|4094| <<trace__run>> evlist__enable(evlist);
+ *   - tools/perf/builtin-trace.c|4101| <<trace__run>> evlist__enable(evlist);
+ *   - tools/perf/tests/backward-ring-buffer.c|75| <<do_test>> evlist__enable(evlist);
+ *   - tools/perf/tests/bpf.c|170| <<do_test>> evlist__enable(evlist);
+ *   - tools/perf/tests/code-reading.c|689| <<do_test_code_reading>> evlist__enable(evlist);
+ *   - tools/perf/tests/keep-tracking.c|116| <<test__keep_tracking>> evlist__enable(evlist);
+ *   - tools/perf/tests/keep-tracking.c|134| <<test__keep_tracking>> evlist__enable(evlist);
+ *   - tools/perf/tests/openat-syscall-tp-fields.c|82| <<test__syscall_openat_tp_fields>> evlist__enable(evlist);
+ *   - tools/perf/tests/perf-record.c|159| <<test__PERF_RECORD>> evlist__enable(evlist);
+ *   - tools/perf/tests/perf-time-to-tsc.c|135| <<test__perf_time_to_tsc>> evlist__enable(evlist);
+ *   - tools/perf/tests/sw-clock.c|91| <<__test__sw_clock_freq>> evlist__enable(evlist);
+ *   - tools/perf/tests/switch-tracking.c|483| <<test__switch_tracking>> evlist__enable(evlist);
+ *   - tools/perf/util/evlist.c|594| <<evlist__toggle_enable>> (evlist->enabled ? evlist__disable : evlist__enable)(evlist);
+ *   - tools/perf/util/evlist.c|2058| <<evlist__ctlfd_enable>> evlist__enable(evlist);
+ */
 void evlist__enable(struct evlist *evlist)
 {
 	__evlist__enable(evlist, NULL);
@@ -989,6 +1012,9 @@ int evlist__mmap_ex(struct evlist *evlist, unsigned int pages,
 	struct perf_evlist_mmap_ops ops = {
 		.idx  = perf_evlist__mmap_cb_idx,
 		.get  = perf_evlist__mmap_cb_get,
+		/*
+		 * 核心是这个!!!
+		 */
 		.mmap = perf_evlist__mmap_cb_mmap,
 	};
 
@@ -1490,6 +1516,18 @@ int evlist__prepare_workload(struct evlist *evlist, struct target *target, const
 	return -1;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-ftrace.c|633| <<__cmd_ftrace>> evlist__start_workload(ftrace->evlist);
+ *   - tools/perf/builtin-ftrace.c|908| <<__cmd_latency>> evlist__start_workload(ftrace->evlist);
+ *   - tools/perf/builtin-lock.c|1686| <<__cmd_contention>> evlist__start_workload(con.evlist);
+ *   - tools/perf/builtin-record.c|2560| <<__cmd_record>> evlist__start_workload(rec->evlist);
+ *   - tools/perf/builtin-stat.c|1035| <<__run_perf_stat>> evlist__start_workload(evsel_list);
+ *   - tools/perf/builtin-trace.c|4097| <<trace__run>> evlist__start_workload(evlist);
+ *   - tools/perf/tests/event-times.c|50| <<attach__enable_on_exec>> return evlist__start_workload(evlist) == 1 ? TEST_OK : TEST_FAIL;
+ *   - tools/perf/tests/perf-record.c|164| <<test__PERF_RECORD>> evlist__start_workload(evlist);
+ *   - tools/perf/tests/task-exit.c|115| <<test__task_exit>> evlist__start_workload(evlist);
+ */
 int evlist__start_workload(struct evlist *evlist)
 {
 	if (evlist->workload.cork_fd > 0) {
@@ -1889,6 +1927,11 @@ void evlist__close_control(int ctl_fd, int ctl_fd_ack, bool *ctl_fd_close)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1281| <<record__mmap_evlist>> if (evlist__initialize_ctlfd(evlist, opts->ctl_fd, opts->ctl_fd_ack))
+ *   - tools/perf/builtin-stat.c|2617| <<cmd_stat>> if (evlist__initialize_ctlfd(evsel_list, stat_config.ctl_fd, stat_config.ctl_fd_ack))
+ */
 int evlist__initialize_ctlfd(struct evlist *evlist, int fd, int ack)
 {
 	if (fd == -1) {
@@ -2100,6 +2143,11 @@ static int evlist__ctlfd_list(struct evlist *evlist, char *cmd_data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2846| <<__cmd_record>> if (evlist__ctlfd_process(rec->evlist, &cmd) > 0) {
+ *   - tools/perf/builtin-stat.c|678| <<process_evlist>> if (evlist__ctlfd_process(evlist, &cmd) > 0) {
+ */
 int evlist__ctlfd_process(struct evlist *evlist, enum evlist_ctl_cmd *cmd)
 {
 	int err = 0;
@@ -2147,6 +2195,10 @@ int evlist__ctlfd_process(struct evlist *evlist, enum evlist_ctl_cmd *cmd)
 	return err;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2825| <<__cmd_record>> evlist__ctlfd_update(rec->evlist,
+ */
 int evlist__ctlfd_update(struct evlist *evlist, struct pollfd *update)
 {
 	int ctlfd_pos = evlist->ctl_fd.pos;
diff --git a/tools/perf/util/evsel.c b/tools/perf/util/evsel.c
index 18c3eb864..50710d507 100644
--- a/tools/perf/util/evsel.c
+++ b/tools/perf/util/evsel.c
@@ -1429,6 +1429,10 @@ int evsel__append_addr_filter(struct evsel *evsel, const char *filter)
 }
 
 /* Caller has to clear disabled after going through all CPUs. */
+/*
+ * called by:
+ *   - tools/perf/util/evlist.c|563| <<__evlist__enable>> evsel__enable_cpu(pos, evlist_cpu_itr.cpu_map_idx);
+ */
 int evsel__enable_cpu(struct evsel *evsel, int cpu_map_idx)
 {
 	return perf_evsel__enable_cpu(&evsel->core, cpu_map_idx);
@@ -1533,6 +1537,27 @@ void evsel__compute_deltas(struct evsel *evsel, int cpu_map_idx, int thread,
 	count->run = count->run - tmp.run;
 }
 
+/*
+ * (gdb) bt
+ * #0  evsel__read_one (thread=0, cpu_map_idx=0, evsel=0x7c6130) at util/evsel.c:1538
+ * #1  evsel__read_counter (evsel=evsel@entry=0x7c6130, cpu_map_idx=cpu_map_idx@entry=0, thread=thread@entry=0) at util/evsel.c:1629
+ * #2  0x000000000042b0b1 in read_single_counter (rs=0x7fffffff8fb0, thread=0, cpu_map_idx=0, counter=0x7c6130) at builtin-stat.c:367
+ * #3  read_counter_cpu (cpu_map_idx=0, rs=0x7fffffff8fb0, counter=0x7c6130) at builtin-stat.c:397
+ * #4  read_affinity_counters (rs=0x7fffffff8fb0) at builtin-stat.c:449
+ * #5  read_counters (rs=0x7fffffff8fb0) at builtin-stat.c:483
+ * #6  0x000000000042e01c in __run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1046
+ * #7  run_perf_stat (run_idx=0, argv=0x7fffffffdf70, argc=2) at builtin-stat.c:1071
+ * #8  cmd_stat (argc=2, argv=<optimized out>) at builtin-stat.c:2577
+ * #9  0x00000000004a544b in run_builtin (p=p@entry=0x719650 <commands+336>, argc=argc@entry=6, argv=argv@entry=0x7fffffffdf70) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=6, argv=0x7fffffffdf70) at perf.c:544
+ * (gdb) p evsel->name
+ * $1 = 0x7c5e90 "cycles"
+ *
+ * called by:
+ *   - tools/perf/util/evsel.c|1629| <<evsel__read_counter>> return evsel__read_one(evsel, cpu_map_idx, thread);
+ */
 static int evsel__read_one(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	struct perf_counts_values *count = perf_counts(evsel->counts, cpu_map_idx, thread);
@@ -1619,6 +1644,10 @@ static int evsel__read_group(struct evsel *leader, int cpu_map_idx, int thread)
 	return evsel__process_group_data(leader, cpu_map_idx, thread, data);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|366| <<read_single_counter>> return evsel__read_counter(counter, cpu_map_idx, thread);
+ */
 int evsel__read_counter(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	u64 read_format = evsel->core.attr.read_format;
@@ -1629,6 +1658,11 @@ int evsel__read_counter(struct evsel *evsel, int cpu_map_idx, int thread)
 	return evsel__read_one(evsel, cpu_map_idx, thread);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.h|402| <<evsel__read_on_cpu>> return __evsel__read_on_cpu(evsel, cpu_map_idx, thread, false);
+ *   - tools/perf/util/evsel.h|414| <<evsel__read_on_cpu_scaled>> return __evsel__read_on_cpu(evsel, cpu_map_idx, thread, true);
+ */
 int __evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread, bool scale)
 {
 	struct perf_counts_values count;
@@ -2031,6 +2065,12 @@ bool evsel__increase_rlimit(enum rlimit_action *set_rlimit)
 	return false;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/evsel.c|2188| <<evsel__open>> return evsel__open_cpu(evsel, cpus, threads, 0, perf_cpu_map__nr(cpus));
+ *   - tools/perf/util/evsel.c|2200| <<evsel__open_per_cpu>> return evsel__open_cpu(evsel, cpus, NULL, 0, perf_cpu_map__nr(cpus));
+ *   - tools/perf/util/evsel.c|2202| <<evsel__open_per_cpu>> return evsel__open_cpu(evsel, cpus, NULL, cpu_map_idx, cpu_map_idx + 1);
+ */
 static int evsel__open_cpu(struct evsel *evsel, struct perf_cpu_map *cpus,
 		struct perf_thread_map *threads,
 		int start_cpu_map_idx, int end_cpu_map_idx)
@@ -2177,6 +2217,19 @@ static int evsel__open_cpu(struct evsel *evsel, struct perf_cpu_map *cpus,
 	return err;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1251| <<record__open>> if (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {
+ *   - tools/perf/builtin-top.c|1024| <<perf_top__start_counters>> if (evsel__open(counter, top->evlist->core.user_requested_cpus,
+ *   - tools/perf/tests/mmap-basic.c|98| <<test__basic_mmap>> if (evsel__open(evsels[i], cpus, threads) < 0) {
+ *   - tools/perf/tests/openat-syscall-all-cpus.c|56| <<test__openat_syscall_event_on_all_cpus>> if (evsel__open(evsel, cpus, threads) < 0) {
+ *   - tools/perf/util/evlist.c|1361| <<evlist__open>> err = evsel__open(evsel, evsel->core.cpus, evsel->core.threads);
+ *   - tools/perf/util/evsel.c|2207| <<evsel__open_per_thread>> return evsel__open(evsel, NULL, threads);
+ *   - tools/perf/util/parse-events.c|178| <<is_event_supported>> open_return = evsel__open(evsel, NULL, tmap);
+ *   - tools/perf/util/parse-events.c|190| <<is_event_supported>> ret = evsel__open(evsel, NULL, tmap) >= 0;
+ *   - tools/perf/util/python.c|904| <<pyrf_evsel__open>> if (evsel__open(evsel, cpus, threads) < 0) {
+ *   - tools/perf/util/sideband_evlist.c|117| <<evlist__start_sb_thread>> if (evsel__open(counter, evlist->core.user_requested_cpus,
+ */
 int evsel__open(struct evsel *evsel, struct perf_cpu_map *cpus,
 		struct perf_thread_map *threads)
 {
@@ -2189,6 +2242,15 @@ void evsel__close(struct evsel *evsel)
 	perf_evsel__free_id(&evsel->core);
 }
 
+/*
+ * called by:
+ *   - tools/perf/tests/event-times.c|128| <<attach__cpu_disabled>> err = evsel__open_per_cpu(evsel, cpus, -1);
+ *   - tools/perf/tests/event-times.c|155| <<attach__cpu_enabled>> err = evsel__open_per_cpu(evsel, cpus, -1);
+ *   - tools/perf/util/bpf_counter.c|454| <<bperf_reload_leader_program>> evsel__open_per_cpu(evsel, all_cpu_map, -1);
+ *   - tools/perf/util/bpf_counter_cgroup.c|91| <<bperf_load_program>> if (evsel__open_per_cpu(cgrp_switch, evlist->core.all_cpus, -1) < 0) {
+ *   - tools/perf/util/bpf_counter_cgroup.c|118| <<bperf_load_program>> err = evsel__open_per_cpu(evsel, evsel->core.cpus, -1);
+ *   - tools/perf/util/stat.c|629| <<create_perf_stat_counter>> return evsel__open_per_cpu(evsel, evsel__cpus(evsel), cpu_map_idx);
+ */
 int evsel__open_per_cpu(struct evsel *evsel, struct perf_cpu_map *cpus, int cpu_map_idx)
 {
 	if (cpu_map_idx == -1)
@@ -2197,6 +2259,13 @@ int evsel__open_per_cpu(struct evsel *evsel, struct perf_cpu_map *cpus, int cpu_
 	return evsel__open_cpu(evsel, cpus, NULL, cpu_map_idx, cpu_map_idx + 1);
 }
 
+/*
+ * called by:
+ *   - tools/perf/tests/event-times.c|75| <<attach__current_disabled>> err = evsel__open_per_thread(evsel, threads);
+ *   - tools/perf/tests/event-times.c|99| <<attach__current_enabled>> err = evsel__open_per_thread(evsel, threads);
+ *   - tools/perf/tests/openat-syscall.c|39| <<test__openat_syscall_event>> if (evsel__open_per_thread(evsel, threads) < 0) {
+ *   - tools/perf/util/stat.c|631| <<create_perf_stat_counter>> return evsel__open_per_thread(evsel, evsel->core.threads);
+ */
 int evsel__open_per_thread(struct evsel *evsel, struct perf_thread_map *threads)
 {
 	return evsel__open(evsel, NULL, threads);
@@ -2940,6 +3009,12 @@ static bool is_amd_ibs(struct evsel *evsel)
 	    || (evsel->pmu_name && !strncmp(evsel->pmu_name, "ibs", 3));
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1414| <<record__open>> evsel__open_strerror(pos, &opts->target, errno, msg, sizeof(msg));
+ *   - tools/perf/builtin-stat.c|810| <<stat_handle_error>> evsel__open_strerror(counter, &target, errno, msg, sizeof(msg));
+ *   - tools/perf/builtin-top.c|1047| <<perf_top__start_counters>> evsel__open_strerror(counter, &opts->target, errno, msg, sizeof(msg));
+ */
 int evsel__open_strerror(struct evsel *evsel, struct target *target,
 			 int err, char *msg, size_t size)
 {
diff --git a/tools/perf/util/evsel.h b/tools/perf/util/evsel.h
index d927713b5..675f8d049 100644
--- a/tools/perf/util/evsel.h
+++ b/tools/perf/util/evsel.h
@@ -118,6 +118,27 @@ struct evsel {
 	void			*priv;
 	u64			db_id;
 	bool			uniquified_name;
+	/*
+	 * 在以下使用counter->supported:
+	 *   - tools/perf/builtin-record.c|1269| <<record__open>> pos->supported = true;
+	 *   - tools/perf/builtin-stat.c|382| <<read_counter_cpu>> if (!counter->supported)
+	 *   - tools/perf/builtin-stat.c|758| <<stat_handle_error>> counter->supported = false;
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|962| <<__run_perf_stat>> counter->supported = true;
+	 *   - tools/perf/builtin-stat.c|968| <<__run_perf_stat>> if (!counter->supported) {
+	 *   - tools/perf/util/bpf_counter_cgroup.c|139| <<bperf_load_program>> evsel->supported = true;
+	 *   - tools/perf/util/mem-events.c|136| <<perf_mem_events__init>> e->supported = perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|141| <<perf_mem_events__init>> e->supported |= perf_mem_event__supported(mnt, sysfs_name);
+	 *   - tools/perf/util/mem-events.c|145| <<perf_mem_events__init>> if (e->supported)
+	 *   - tools/perf/util/mem-events.c|163| <<perf_mem_events__list>> e->supported ? ": available" : "");
+	 *   - tools/perf/util/mem-events.c|198| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/mem-events.c|207| <<perf_mem_events__record_args>> if (!e->supported) {
+	 *   - tools/perf/util/stat-display.c|587| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED);
+	 *   - tools/perf/util/stat-display.c|591| <<printout>> counter->supported ? CNTR_NOT_COUNTED : CNTR_NOT_SUPPORTED,
+	 *   - tools/perf/util/stat-display.c|595| <<printout>> if (counter->supported) {
+	 *   - tools/perf/util/stat.c|502| <<perf_event__process_stat_event>> counter->supported = true;
+	 *   - tools/perf/util/synthetic-events.c|2066| <<perf_event__synthesize_extra_attr>> if (!evsel->supported)
+	 */
 	bool 			supported;
 	bool 			needs_swap;
 	bool 			disabled;
@@ -128,7 +149,24 @@ struct evsel {
 	bool			forced_leader;
 	bool			cmdline_group_boundary;
 	bool			merged_stat;
+	/*
+	 * 在以下使用evsel->reset_group:
+	 *   - tools/perf/builtin-stat.c|829| <<__run_perf_stat>> counter->reset_group = false;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|865| <<__run_perf_stat>> assert(counter->reset_group);
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|906| <<__run_perf_stat>> if (!counter->reset_group)
+	 *   - tools/perf/util/evlist.c|1798| <<evlist__reset_weak_group>> c2->reset_group = true; 
+	 */
 	bool			reset_group;
+	/*
+	 * 在以下使用evsel->errored:
+	 *   - tools/perf/builtin-stat.c|763| <<stat_handle_error>> counter->errored = true;
+	 *   - tools/perf/builtin-stat.c|846| <<__run_perf_stat>> if (counter->reset_group || counter->errored)
+	 *   - tools/perf/builtin-stat.c|895| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 *   - tools/perf/builtin-stat.c|904| <<__run_perf_stat>> if (!counter->reset_group && !counter->errored)
+	 */
 	bool			errored;
 	bool			needs_auxtrace_mmap;
 	struct hashmap		*per_pkg_mask;
@@ -359,6 +397,11 @@ int __evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread, bool
  * @cpu_map_idx - CPU of interest
  * @thread - thread of interest
  */
+/*
+ * called by:
+ *   - tools/perf/tests/openat-syscall-all-cpus.c|101| <<test__openat_syscall_event_on_all_cpus>> if (evsel__read_on_cpu(evsel, idx, 0) < 0) {
+ *   - tools/perf/tests/openat-syscall.c|52| <<test__openat_syscall_event>> if (evsel__read_on_cpu(evsel, 0, 0) < 0) {
+ */
 static inline int evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread)
 {
 	return __evsel__read_on_cpu(evsel, cpu_map_idx, thread, false);
diff --git a/tools/perf/util/hist.c b/tools/perf/util/hist.c
index 1c085ab56..3d844ca5b 100644
--- a/tools/perf/util/hist.c
+++ b/tools/perf/util/hist.c
@@ -1720,6 +1720,27 @@ static void hists__inc_filter_stats(struct hists *hists, struct hist_entry *h)
 	hists->stats.total_non_filtered_period += h->stat.period;
 }
 
+/*
+ * (gdb) bt
+ * #0  hists__inc_stats (h=0xda58e0, hists=0x7ae5e0) at util/hist.c:1933
+ * #1  output_resort (hists=hists@entry=0x7ae5e0, prog=<optimized out>, use_callchain=<optimized out>, cb=cb@entry=0x426bb0 <hists__resort_cb>, cb_arg=<optimized out>)
+ *     at util/hist.c:1933
+ * #2  0x0000000000533bd5 in evsel__output_resort_cb (evsel=evsel@entry=0x7ae380, prog=prog@entry=0x7fffffffb640, cb=cb@entry=0x426bb0 <hists__resort_cb>, 
+ *     cb_arg=cb_arg@entry=0x7fffffffb7f0) at util/hist.h:246
+ * #3  0x00000000004295cb in report__output_resort (rep=0x7fffffffb7f0) at builtin-report.c:739
+ * #4  __cmd_report (rep=0x7fffffffb7f0) at builtin-report.c:1019
+ * #5  cmd_report (argc=<optimized out>, argv=<optimized out>) at builtin-report.c:1659
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719620 <commands+288>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdfe0) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=2, argv=0x7fffffffdfe0) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/util/hist.c|1933| <<output_resort>> hists__inc_stats(hists, n);
+ *   - tools/perf/util/hist.c|2380| <<hists__add_dummy_entry>> hists__inc_stats(hists, he);
+ *   - tools/perf/util/hist.c|2428| <<add_dummy_hierarchy_entry>> hists__inc_stats(hists, he);
+ *   - tools/perf/util/hist.h|201| <<add_dummy_hierarchy_entry>> void hists__inc_stats(struct hists *hists, struct hist_entry *h);
+ */
 void hists__inc_stats(struct hists *hists, struct hist_entry *h)
 {
 	if (!h->filtered)
@@ -1885,6 +1906,12 @@ static void __hists__insert_output_entry(struct rb_root_cached *entries,
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/hist.c|1955| <<evsel__output_resort_cb>> output_resort(evsel__hists(evsel), prog, use_callchain, cb, cb_arg);
+ *   - tools/perf/util/hist.c|1965| <<hists__output_resort>> output_resort(hists, prog, symbol_conf.use_callchain, NULL, NULL);
+ *   - tools/perf/util/hist.c|1971| <<hists__output_resort_cb>> output_resort(hists, prog, symbol_conf.use_callchain, cb, NULL);
+ */
 static void output_resort(struct hists *hists, struct ui_progress *prog,
 			  bool use_callchain, hists__resort_cb_t cb,
 			  void *cb_arg)
@@ -1940,6 +1967,21 @@ static void output_resort(struct hists *hists, struct ui_progress *prog,
 	}
 }
 
+/*
+ * (gdb) bt
+ * #0  evsel__output_resort_cb (evsel=evsel@entry=0x7ae380, prog=prog@entry=0x7fffffffb640, cb=cb@entry=0x426bb0 <hists__resort_cb>, cb_arg=cb_arg@entry=0x7fffffffb7f0) at util/hist.c:1945
+ * #1  0x00000000004295cb in report__output_resort (rep=0x7fffffffb7f0) at builtin-report.c:739
+ * #2  __cmd_report (rep=0x7fffffffb7f0) at builtin-report.c:1019
+ * #3  cmd_report (argc=<optimized out>, argv=<optimized out>) at builtin-report.c:1659
+ * #4  0x00000000004a548b in run_builtin (p=p@entry=0x719620 <commands+288>, argc=argc@entry=2, argv=argv@entry=0x7fffffffdfe0) at perf.c:316
+ * #5  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #6  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #7  main (argc=2, argv=0x7fffffffdfe0) at perf.c:544
+ *
+ * called by:
+ *   - tools/perf/builtin-report.c|737| <<report__output_resort>> evsel__output_resort_cb(pos, &prog, hists__resort_cb, rep);
+ *   - tools/perf/util/hist.c|1960| <<evsel__output_resort>> return evsel__output_resort_cb(evsel, prog, NULL, NULL);
+ */
 void evsel__output_resort_cb(struct evsel *evsel, struct ui_progress *prog,
 			     hists__resort_cb_t cb, void *cb_arg)
 {
diff --git a/tools/perf/util/mmap.c b/tools/perf/util/mmap.c
index a4dff881b..d89950a6c 100644
--- a/tools/perf/util/mmap.c
+++ b/tools/perf/util/mmap.c
@@ -321,9 +321,78 @@ int mmap__mmap(struct mmap *map, struct mmap_params *mp, int fd, struct perf_cpu
 	return perf_mmap__aio_mmap(map, mp);
 }
 
+/*
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7ffff7eeb010, mp=mp@entry=0x7fffffff8840, fd=fd@entry=5, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7ffff7eeb010, mp=0x7fffffff8840, fd=5, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7a7480, ops=0x7fffffff8820, idx=0, mp=0x7fffffff8840, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87d0, _output_overwrite=0x7fffffff87d4,
+ *     nr_mmaps=0x7fffffff87cc) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8840, ops=<optimized out>, evlist=0x7a7480) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7a7480, ops=ops@entry=0x7fffffff8820, mp=mp@entry=0x7fffffff8840) at evlist.c:642
+ * #5  0x00000000004bcecc in evlist__mmap_ex (evlist=evlist@entry=0x7a7480, pages=<optimized out>, auxtrace_pages=0, auxtrace_overwrite=auxtrace_overwrite@entry=false, nr_cblocks=<optimized out>,
+ *     affinity=<optimized out>, flush=1, comp_level=0) at util/evlist.c:1001
+ * #6  0x0000000000422a28 in record__mmap_evlist (rec=0x707020 <record>, evlist=0x7a7480) at builtin-record.c:1159
+ * #7  record__mmap (rec=0x707020 <record>) at builtin-record.c:1208
+ * #8  record__open (rec=0x707020 <record>) at builtin-record.c:1292
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2304
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * (gdb) bt
+ * #0  perf_mmap__mmap (map=map@entry=0x7fffec363010, mp=mp@entry=0x7fffffff8820, fd=fd@entry=21, cpu=...) at mmap.c:37
+ * #1  0x00000000004cc23e in mmap__mmap (map=0x7fffec363010, mp=0x7fffffff8820, fd=21, cpu=...) at util/mmap.c:280
+ * #2  0x00000000005baf08 in mmap_per_evsel (evlist=0x7c9850, ops=0x7fffffff8800, idx=0, mp=0x7fffffff8820, cpu_idx=<optimized out>, thread=0, _output=0x7fffffff87b0, _output_overwrite=0x7fffffff87b4,
+ *     nr_mmaps=0x7fffffff87ac) at evlist.c:491
+ * #3  0x00000000005bb654 in mmap_per_cpu (mp=0x7fffffff8820, ops=<optimized out>, evlist=0x7c9850) at evlist.c:583
+ * #4  perf_evlist__mmap_ops (evlist=evlist@entry=0x7c9850, ops=ops@entry=0x7fffffff8800, mp=mp@entry=0x7fffffff8820) at evlist.c:642
+ * #5  0x00000000004bcfeb in evlist__mmap_ex (comp_level=0, flush=1, affinity=0, nr_cblocks=0, auxtrace_overwrite=false, auxtrace_pages=0, pages=4294967295, evlist=0x7c9850) at util/evlist.c:1001
+ * #6  evlist__mmap (evlist=evlist@entry=0x7c9850, pages=pages@entry=4294967295) at util/evlist.c:1006
+ * #7  0x00000000004c0745 in evlist__start_sb_thread (evlist=0x7c9850, target=target@entry=0x707160 <record+320>) at util/sideband_evlist.c:122
+ * #8  0x0000000000422578 in record__setup_sb_evlist (rec=0x707020 <record>) at builtin-record.c:1984
+ * #9  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2359
+ * #10 0x0000000000425715 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4102
+ * #11 0x00000000004a55cb in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #12 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #13 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #14 main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ *
+ * 上面是md->core.base的来源
+ *
+ * called by:
+ *   - 定义tools/perf/util/mmap.c|324| <<global>> int perf_mmap__push(struct mmap *md, void *to,
+ *   - tools/perf/builtin-record.c|468| <<record__aio_push>> ret = perf_mmap__push(map, &aio, record__aio_pushfn);
+ *   - tools/perf/builtin-record.c|1608| <<record__mmap_read_evlist>> if (perf_mmap__push(map, rec, record__pushfn) < 0) {
+ */
 int perf_mmap__push(struct mmap *md, void *to,
 		    int push(struct mmap *map, void *to, void *buf, size_t size))
 {
+	/*
+	 * struct mmap *md:
+	 * -> struct perf_mmap core:
+	 *        void                    *base;
+	 *        int                      mask;
+	 *        int                      fd;
+	 *        struct perf_cpu          cpu;
+	 *        refcount_t               refcnt;
+	 *        u64                      prev;
+	 *        u64                      start;
+	 *        u64                      end;
+	 *        bool                     overwrite;
+	 *        u64                      flush;
+	 *        libperf_unmap_cb_t       unmap_cb;
+	 *        char                     event_copy[PERF_SAMPLE_MAX_SIZE] __aligned(8);
+	 *        struct perf_mmap        *next;
+	 *
+	 *
+	 * struct perf_event_mmap_page *base;
+	 *   -> __u64   data_head;              // head in the data section
+	 *   -> __u64   data_tail;              // user-space written tail
+	 *   -> __u64   data_offset;            // where the buffer starts
+	 *   -> __u64   data_size;              // data buffer size
+	 */
 	u64 head = perf_mmap__read_head(&md->core);
 	unsigned char *data = md->core.base + page_size;
 	unsigned long size;
@@ -334,6 +403,24 @@ int perf_mmap__push(struct mmap *md, void *to,
 	if (rc < 0)
 		return (rc == -EAGAIN) ? 1 : -1;
 
+	/*
+	 * struct mmap *md:
+	 * -> struct perf_mmap        core;
+	 *        void                    *base;
+	 *        int                      mask;
+	 *        int                      fd;
+	 *        struct perf_cpu          cpu;
+	 *        refcount_t               refcnt;
+	 *        u64                      prev;
+	 *        u64                      start;
+	 *        u64                      end;
+	 *        bool                     overwrite;
+	 *        u64                      flush;
+	 *        libperf_unmap_cb_t       unmap_cb;
+	 *        char                     event_copy[PERF_SAMPLE_MAX_SIZE] __aligned(8);
+	 *        struct perf_mmap        *next;
+	 * -> struct auxtrace_mmap auxtrace_mmap;
+	 */
 	size = md->core.end - md->core.start;
 
 	if ((md->core.start & md->core.mask) + size != (md->core.end & md->core.mask)) {
@@ -341,6 +428,9 @@ int perf_mmap__push(struct mmap *md, void *to,
 		size = md->core.mask + 1 - (md->core.start & md->core.mask);
 		md->core.start += size;
 
+		/*
+		 * 一个例子是record__pushfn()
+		 */
 		if (push(md, to, buf, size) < 0) {
 			rc = -1;
 			goto out;
@@ -351,6 +441,9 @@ int perf_mmap__push(struct mmap *md, void *to,
 	size = md->core.end - md->core.start;
 	md->core.start += size;
 
+	/*
+	 * 一个例子是record__pushfn()
+	 */
 	if (push(md, to, buf, size) < 0) {
 		rc = -1;
 		goto out;
diff --git a/tools/perf/util/session.c b/tools/perf/util/session.c
index 192c9274f..33ca0c491 100644
--- a/tools/perf/util/session.c
+++ b/tools/perf/util/session.c
@@ -189,6 +189,25 @@ static int ordered_events__deliver_event(struct ordered_events *oe,
 					   event->file_path);
 }
 
+/*
+ * (gdb) bt
+ * #0  open_file_write (data=0x707268 <record+584>) at util/data.c:292
+ * #1  open_file (data=data@entry=0x707268 <record+584>) at util/data.c:307
+ * #2  0x0000000000550b2d in open_file_dup (data=0x707268 <record+584>) at util/data.c:324
+ * #3  perf_data__open (data=0x707268 <record+584>) at util/data.c:369
+ * #4  perf_data__open (data=data@entry=0x707268 <record+584>) at util/data.c:351
+ * #5  0x0000000000509f26 in __perf_session__new (data=data@entry=0x707268 <record+584>, repipe=repipe@entry=false, repipe_fd=repipe_fd@entry=-1, tool=0x7a7f40, tool@entry=0x707020 <record>) at ut    il/session.c:213
+ * #6  0x0000000000421a15 in perf_session__new (tool=0x707020 <record>, data=0x707268 <record+584>) at util/session.h:71
+ * #7  __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2223
+ * #8  0x00000000004256a5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4094
+ * #9  0x00000000004a555b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #10 0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #11 run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #12 main (argc=7, argv=0x7fffffffdf60) at perf.c:54
+ *
+ * (gdb) p data->file.path
+ * $1 = 0x7c6640 "perf.data"
+ */
 struct perf_session *__perf_session__new(struct perf_data *data,
 					 bool repipe, int repipe_fd,
 					 struct perf_tool *tool)
diff --git a/tools/perf/util/stat-display.c b/tools/perf/util/stat-display.c
index b82844cb0..fa66885c7 100644
--- a/tools/perf/util/stat-display.c
+++ b/tools/perf/util/stat-display.c
@@ -531,6 +531,15 @@ static bool is_mixed_hw_group(struct evsel *counter)
 	return false;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|862| <<print_counter_aggrdata>> printout(config, id, nr, counter, uval,
+ *   - tools/perf/util/stat-display.c|983| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|987| <<print_aggr_thread>> printout(config, id, 0, buf[thread].counter, buf[thread].uval,
+ *   - tools/perf/util/stat-display.c|1031| <<print_counter_aggr>> printout(config, aggr_cpu_id__empty(), 0, counter, uval, prefix, cd.avg_running,
+ *   - tools/perf/util/stat-display.c|1076| <<print_counter>> printout(config, id, 0, counter, uval, prefix,
+ *   - tools/perf/util/stat-display.c|1115| <<print_no_aggr_metric>> printout(config, id, 0, counter, uval, prefix,
+ */
 static void printout(struct perf_stat_config *config, struct aggr_cpu_id id, int nr,
 		     struct evsel *counter, double uval,
 		     char *prefix, u64 run, u64 ena, double noise,
@@ -1049,6 +1058,10 @@ static void counter_cb(struct perf_stat_config *config __maybe_unused,
  * Print out the results of a single counter:
  * does not use aggregated count in system-wide
  */
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1519| <<evlist__print_counters>> print_counter(config, counter, prefix);
+ */
 static void print_counter(struct perf_stat_config *config,
 			  struct evsel *counter, char *prefix)
 {
@@ -1268,6 +1281,10 @@ static void print_interval(struct perf_stat_config *config,
 		num_print_interval = 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat-display.c|1471| <<evlist__print_counters>> print_header(config, _target, argc, argv);
+ */
 static void print_header(struct perf_stat_config *config,
 			 struct target *_target,
 			 int argc, const char **argv)
@@ -1445,6 +1462,10 @@ static void print_percore(struct perf_stat_config *config,
 		fputc('\n', output);
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|1117| <<print_counters>> evlist__print_counters(evsel_list, &stat_config, &target, ts, argc, argv);
+ */
 void evlist__print_counters(struct evlist *evlist, struct perf_stat_config *config,
 			    struct target *_target, struct timespec *ts, int argc, const char **argv)
 {
diff --git a/tools/perf/util/stat.c b/tools/perf/util/stat.c
index 0882b4754..290a80f5c 100644
--- a/tools/perf/util/stat.c
+++ b/tools/perf/util/stat.c
@@ -180,6 +180,10 @@ static void evsel__reset_prev_raw_counts(struct evsel *evsel)
 		perf_counts__reset(evsel->prev_raw_counts);
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|198| <<evlist__alloc_stats>> if (evsel__alloc_stats(evsel, alloc_raw))
+ */
 static int evsel__alloc_stats(struct evsel *evsel, bool alloc_raw)
 {
 	if (evsel__alloc_stat_priv(evsel) < 0 ||
@@ -190,6 +194,15 @@ static int evsel__alloc_stats(struct evsel *evsel, bool alloc_raw)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-script.c|2052| <<perf_sample__fprint_metric>> evlist__alloc_stats(script->session->evlist, false);
+ *   - tools/perf/builtin-script.c|3638| <<set_maps>> if (evlist__alloc_stats(evlist, true))
+ *   - tools/perf/builtin-stat.c|2138| <<set_maps>> if (evlist__alloc_stats(evsel_list, true))
+ *   - tools/perf/builtin-stat.c|2570| <<cmd_stat>> if (evlist__alloc_stats(evsel_list, interval))
+ *   - tools/perf/tests/parse-metric.c|106| <<__compute_metric>> err = evlist__alloc_stats(evlist, false);
+ *   - tools/perf/tests/pmu-events.c|892| <<test__parsing_callback>> err = evlist__alloc_stats(evlist, false);
+ */
 int evlist__alloc_stats(struct evlist *evlist, bool alloc_raw)
 {
 	struct evsel *evsel;
@@ -355,6 +368,10 @@ static int check_per_pkg(struct evsel *counter, struct perf_counts_values *vals,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|424| <<process_counter_maps>> if (process_counter_values(config, counter, idx, thread,
+ */
 static int
 process_counter_values(struct perf_stat_config *config, struct evsel *evsel,
 		       int cpu_map_idx, int thread,
@@ -409,6 +426,10 @@ process_counter_values(struct perf_stat_config *config, struct evsel *evsel,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/util/stat.c|446| <<perf_stat_process_counter>> ret = process_counter_maps(config, counter);
+ */
 static int process_counter_maps(struct perf_stat_config *config,
 				struct evsel *counter)
 {
@@ -430,6 +451,12 @@ static int process_counter_maps(struct perf_stat_config *config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-script.c|3611| <<process_stat_round_event>> perf_stat_process_counter(&stat_config, counter);
+ *   - tools/perf/builtin-stat.c|485| <<read_counters>> if (counter->err == 0 && perf_stat_process_counter(&stat_config, counter))
+ *   - tools/perf/builtin-stat.c|2087| <<process_stat_round_event>> perf_stat_process_counter(&stat_config, counter);
+ */
 int perf_stat_process_counter(struct perf_stat_config *config,
 			      struct evsel *counter)
 {
@@ -542,6 +569,11 @@ size_t perf_event__fprintf_stat_config(union perf_event *event, FILE *fp)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-stat.c|879| <<__run_perf_stat>> if (create_perf_stat_counter(counter, &stat_config, &target,
+ *   - tools/perf/builtin-stat.c|952| <<__run_perf_stat>> if (create_perf_stat_counter(counter, &stat_config, &target,
+ */
 int create_perf_stat_counter(struct evsel *evsel,
 			     struct perf_stat_config *config,
 			     struct target *target,
diff --git a/tools/perf/util/synthetic-events.c b/tools/perf/util/synthetic-events.c
index 538790758..ed8ed2a4b 100644
--- a/tools/perf/util/synthetic-events.c
+++ b/tools/perf/util/synthetic-events.c
@@ -47,6 +47,19 @@
 
 unsigned int proc_map_timeout = DEFAULT_PROC_MAP_PARSE_TIMEOUT;
 
+/*
+ * (gdb) bt
+ * #0  perf_tool__process_synth_event (tool=0x707020 <record>, event=0x7ca940, machine=0x7a8148, process=0x420aa0 <process_synthesized_event>) at util/synthetic-events.c:54
+ * #1  0x000000000054d98f in __perf_event__synthesize_kernel_mmap (machine=0x7a8148, process=0x420aa0 <process_synthesized_event>, tool=0x707020 <record>) at util/synthetic-events.c:1143
+ * #2  perf_event__synthesize_kernel_mmap (tool=tool@entry=0x707020 <record>, process=process@entry=0x420aa0 <process_synthesized_event>, machine=machine@entry=0x7a8148) at util/synthetic-events.c:1155
+ * #3  0x0000000000421412 in record__synthesize (tail=tail@entry=false, rec=0x707020 <record>) at builtin-record.c:1867
+ * #4  0x000000000042251c in __cmd_record (argc=argc@entry=2, argv=argv@entry=0x7fffffffdf60, rec=0x707020 <record>) at builtin-record.c:2355
+ * #5  0x00000000004255d5 in cmd_record (argc=<optimized out>, argv=<optimized out>) at builtin-record.c:4079
+ * #6  0x00000000004a548b in run_builtin (p=p@entry=0x719608 <commands+264>, argc=argc@entry=7, argv=argv@entry=0x7fffffffdf60) at perf.c:316
+ * #7  0x0000000000409509 in handle_internal_command (argv=<optimized out>, argc=<optimized out>) at perf.c:370
+ * #8  run_argv (argv=<optimized out>, argcp=<optimized out>) at perf.c:414
+ * #9  main (argc=7, argv=0x7fffffffdf60) at perf.c:544
+ */
 int perf_tool__process_synth_event(struct perf_tool *tool,
 				   union perf_event *event,
 				   struct machine *machine,
@@ -218,6 +231,12 @@ static void perf_event__get_ns_link_info(pid_t pid, const char *ns,
 	}
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|2554| <<__cmd_record>> perf_event__synthesize_namespaces(tool, event,
+ *   - tools/perf/util/synthetic-events.c|759| <<__event__synthesize_thread>> if (perf_event__synthesize_namespaces(tool, namespaces_event, pid,
+ *   - tools/perf/util/synthetic-events.c|804| <<__event__synthesize_thread>> if (perf_event__synthesize_namespaces(tool, namespaces_event, _pid,
+ */
 int perf_event__synthesize_namespaces(struct perf_tool *tool,
 				      union perf_event *event,
 				      pid_t pid, pid_t tgid,
@@ -815,6 +834,13 @@ static int __event__synthesize_thread(union perf_event *comm_event,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - tools/perf/builtin-record.c|1748| <<record__synthesize_workload>> err = perf_event__synthesize_thread_map(&rec->tool, thread_map,
+ *   - tools/perf/tests/code-reading.c|608| <<do_test_code_reading>> ret = perf_event__synthesize_thread_map(NULL, threads,
+ *   - tools/perf/tests/mmap-thread-lookup.c|148| <<synth_process>> err = perf_event__synthesize_thread_map(NULL, map,
+ *   - tools/perf/util/synthetic-events.c|1924| <<__machine__synthesize_threads>> return perf_event__synthesize_thread_map(tool, threads, process, machine,
+ */
 int perf_event__synthesize_thread_map(struct perf_tool *tool,
 				      struct perf_thread_map *threads,
 				      perf_event__handler_t process,
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 2a3ed401c..359b8abed 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -109,6 +109,10 @@ irqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)
 /*
  * Race-free decouple logic (ordering is critical)
  */
+/*
+ * 在以下使用irqfd_shutdown():
+ *   - virt/kvm/eventfd.c|318| <<kvm_irqfd_assign>> INIT_WORK(&irqfd->shutdown, irqfd_shutdown);
+ */
 static void
 irqfd_shutdown(struct work_struct *work)
 {
@@ -160,6 +164,12 @@ irqfd_is_active(struct kvm_kernel_irqfd *irqfd)
  *
  * assumes kvm->irqfds.lock is held
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|231| <<irqfd_wakeup>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|572| <<kvm_irqfd_deassign>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|613| <<kvm_irqfd_release>> irqfd_deactivate(irqfd);
+ */
 static void
 irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 {
@@ -246,6 +256,11 @@ irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,
 }
 
 /* Must be called under irqfds.lock */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|414| <<kvm_irqfd_assign>> irqfd_update(kvm, irqfd);
+ *   - virt/kvm/eventfd.c|663| <<kvm_irq_routing_update>> irqfd_update(kvm, irqfd);
+ */
 static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
 {
 	struct kvm_kernel_irq_routing_entry *e;
@@ -291,6 +306,15 @@ bool __attribute__((weak)) kvm_arch_irqfd_route_changed(
 }
 #endif
 
+/*
+ * struct kvm_irqfd {
+ *     __u32 fd;
+ *     __u32 gsi;
+ *     __u32 flags;
+ *     __u32 resamplefd;
+ *     __u8  pad[16];
+ * };
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -418,7 +442,17 @@ kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
 	if (kvm_arch_has_irq_bypass()) {
+		/*
+		 * struct kvm_kernel_irqfd *irqfd:
+		 * -> struct eventfd_ctx *eventfd;
+		 */
 		irqfd->consumer.token = (void *)irqfd->eventfd;
+		/*
+		 * 下面的例子:
+		 *   - arch/arm64/kvm/arm.c|2199| <<kvm_arch_irq_bypass_add_producer>> int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
+		 *   - arch/powerpc/kvm/powerpc.c|880| <<kvm_arch_irq_bypass_add_producer>> int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
+		 *   - arch/x86/kvm/x86.c|14099| <<kvm_arch_irq_bypass_add_producer>> int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
+		 */
 		irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
 		irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
 		irqfd->consumer.stop = kvm_arch_irq_bypass_stop;
@@ -536,6 +570,10 @@ kvm_eventfd_init(struct kvm *kvm)
 /*
  * shutdown any irqfd's that match fd+gsi
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|596| <<kvm_irqfd>> return kvm_irqfd_deassign(kvm, args);
+ */
 static int
 kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -576,6 +614,15 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * struct kvm_irqfd {
+ *     __u32 fd;
+ *     __u32 gsi;
+ *     __u32 flags;
+ *     __u32 resamplefd;
+ *     __u8  pad[16];
+ * };
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -592,6 +639,10 @@ kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
  * This function is called as the kvm VM fd is being released. Shutdown all
  * irqfds that still remain open
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1386| <<kvm_vm_release>> kvm_irqfd_release(kvm);
+ */
 void
 kvm_irqfd_release(struct kvm *kvm)
 {
@@ -616,6 +667,10 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
  */
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|222| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
@@ -743,6 +798,9 @@ ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 	return _val == p->datamatch;
 }
 
+/*
+ * struct kvm_io_device_ops ioeventfd_ops.ioeventfd_write = ioeventfd_write()
+ */
 /* MMIO/PIO writes trigger an event if the addr/val match */
 static int
 ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
@@ -769,6 +827,10 @@ ioeventfd_destructor(struct kvm_io_device *this)
 	ioeventfd_release(p);
 }
 
+/*
+ * 在以下使用ioeventfd_ops():
+ *   - virt/kvm/eventfd.c|843| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static const struct kvm_io_device_ops ioeventfd_ops = {
 	.write      = ioeventfd_write,
 	.destructor = ioeventfd_destructor,
@@ -801,6 +863,11 @@ static enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)
 	return KVM_MMIO_BUS;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|955| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);
+ *   - virt/kvm/eventfd.c|963| <<kvm_assign_ioeventfd>> ret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);
+ */
 static int kvm_assign_ioeventfd_idx(struct kvm *kvm,
 				enum kvm_bus bus_idx,
 				struct kvm_ioeventfd *args)
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 58e4f88b2..f51c50b66 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -166,6 +166,15 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|152| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - arch/powerpc/kvm/mpic.c|1649| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+ *   - arch/s390/kvm/kvm-s390.c|2820| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> r = kvm_set_irq_routing(kvm, &routing, 0, 0);
+ *   - arch/x86/kvm/irq_comm.c|388| <<kvm_setup_default_irq_routing>> return kvm_set_irq_routing(kvm, default_routing,
+ *   - arch/x86/kvm/irq_comm.c|396| <<kvm_setup_empty_irq_routing>> return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
+ *   - virt/kvm/kvm_main.c|4832| <<kvm_vm_ioctl(KVM_SET_GSI_ROUTING)>> r = kvm_set_irq_routing(kvm, entries, routing.nr,
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 584a5bab3..abf49959f 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -110,6 +110,17 @@ static atomic_t hardware_enable_failed;
 static struct kmem_cache *kvm_vcpu_cache;
 
 static __read_mostly struct preempt_ops kvm_preempt_ops;
+/*
+ * 在以下使用percpu的kvm_running_vcpu:
+ *   - virt/kvm/kvm_main.c|113| <<global>> static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|224| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|236| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|3656| <<kvm_vcpu_kick>> if (vcpu == __this_cpu_read(kvm_running_vcpu)) {
+ *   - virt/kvm/kvm_main.c|5839| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|5854| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5871| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|5883| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+ */
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
 struct dentry *kvm_debugfs_dir;
@@ -1803,6 +1814,11 @@ static void kvm_update_flags_memslot(struct kvm *kvm,
 	kvm_activate_memslot(kvm, old, new);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1974| <<__kvm_set_memory_region>> return kvm_set_memslot(kvm, old, NULL, KVM_MR_DELETE);
+ *   - virt/kvm/kvm_main.c|2019| <<__kvm_set_memory_region>> r = kvm_set_memslot(kvm, old, new, change);
+ */
 static int kvm_set_memslot(struct kvm *kvm,
 			   struct kvm_memory_slot *old,
 			   struct kvm_memory_slot *new,
@@ -1919,6 +1935,11 @@ static bool kvm_check_memslot_overlap(struct kvm_memslots *slots, int id,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13328| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|2032| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem)
 {
@@ -2023,6 +2044,10 @@ int __kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(__kvm_set_memory_region);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2044| <<kvm_vm_ioctl_set_memory_region(KVM_SET_USER_MEMORY_REGION)>> return kvm_set_memory_region(kvm, mem);
+ */
 int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem)
 {
@@ -2035,6 +2060,10 @@ int kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_set_memory_region);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4692| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem);
+ */
 static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 					  struct kvm_userspace_memory_region *mem)
 {
@@ -3298,6 +3327,23 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1304| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1041| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3278| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|234| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|375| <<handle_changed_spte_dirty_log>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3453| <<kvm_setup_guest_pvclock>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/x86.c|3928| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|5197| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|314| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|397| <<kvm_xen_inject_pending_events>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3093| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3231| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3330| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3339| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3340,6 +3386,15 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|881| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/mips/kvm/mips.c|441| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1869| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|955| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4886| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/x86/kvm/x86.c|11967| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ */
 void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3354,6 +3409,15 @@ void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 	sigprocmask(SIG_SETMASK, &vcpu->sigset, &current->real_blocked);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1053| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/mips/kvm/mips.c|486| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1876| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|1062| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4925| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/x86/kvm/x86.c|12050| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ */
 void kvm_sigset_deactivate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3404,6 +3468,11 @@ static void shrink_halt_poll_ns(struct kvm_vcpu *vcpu)
 	trace_kvm_halt_poll_ns_shrink(vcpu->vcpu_id, val, old);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3466| <<kvm_vcpu_block>> if (kvm_vcpu_check_block(vcpu) < 0)
+ *   - virt/kvm/kvm_main.c|3528| <<kvm_vcpu_halt>> if (kvm_vcpu_check_block(vcpu) < 0)
+ */
 static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
 {
 	int ret = -EINTR;
@@ -3431,6 +3500,12 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11644| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|11866| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ *   - virt/kvm/kvm_main.c|3535| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -4018,6 +4093,11 @@ static int kvm_vcpu_ioctl_get_stats_fd(struct kvm_vcpu *vcpu)
 	return fd;
 }
 
+/*
+ * 在以下使用kvm_vcpu_ioctl():
+ *   - virt/kvm/kvm_main.c|3870| <<global>> struct file_operations kvm_vcpu_fops.unlocked_ioctl = kvm_vcpu_ioctl,
+ *   - virt/kvm/kvm_main.c|4317| <<kvm_vcpu_compat_ioctl>> r = kvm_vcpu_ioctl(filp, ioctl, arg);
+ */
 static long kvm_vcpu_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -4049,6 +4129,10 @@ static long kvm_vcpu_ioctl(struct file *filp,
 		r = -EINVAL;
 		if (arg)
 			goto out;
+		/*
+		 * struct kvm_vcpu *vcpu = filp->private_data;
+		 * -> struct pid __rcu *pid;
+		 */
 		oldpid = rcu_access_pointer(vcpu->pid);
 		if (unlikely(oldpid != task_pid(current))) {
 			/* The thread running this VCPU changed. */
@@ -4435,6 +4519,11 @@ static int kvm_ioctl_create_device(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4802| <<kvm_vm_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(kvm, arg);
+ *   - virt/kvm/kvm_main.c|4957| <<kvm_dev_ioctl(KVM_CHECK_EXTENSION)>> r = kvm_vm_ioctl_check_extension_generic(NULL, arg);
+ */
 static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 {
 	switch (arg) {
@@ -4696,6 +4785,15 @@ static long kvm_vm_ioctl(struct file *filp,
 	}
 #endif
 	case KVM_IRQFD: {
+		/*
+		 * struct kvm_irqfd {
+		 *     __u32 fd;
+		 *     __u32 gsi;
+		 *     __u32 flags;
+		 *     __u32 resamplefd;
+		 *     __u8  pad[16];
+		 * };
+		 */
 		struct kvm_irqfd data;
 
 		r = -EFAULT;
@@ -5162,6 +5260,11 @@ static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 	return off;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5223| <<kvm_io_bus_write>> r = __kvm_io_bus_write(vcpu, bus, &range, val);
+ *   - virt/kvm/kvm_main.c|5255| <<kvm_io_bus_write_cookie>> return __kvm_io_bus_write(vcpu, bus, &range, val);
+ */
 static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 			      struct kvm_io_range *range, const void *val)
 {
@@ -5182,6 +5285,18 @@ static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 	return -EOPNOTSUPP;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmio.c|166| <<io_mem_abort>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
+ *   - arch/mips/kvm/emulate.c|1254| <<kvm_mips_emulate_store>> r = kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/powerpc/kvm/book3s.c|993| <<kvmppc_h_logical_ci_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|442| <<kvmppc_hv_emulate_mmio>> ret = kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, (gpa_t) gpa, 0,
+ *   - arch/powerpc/kvm/powerpc.c|1398| <<kvmppc_handle_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
+ *   - arch/riscv/kvm/vcpu_insn.c|679| <<kvm_riscv_vcpu_mmio_store>> if (!kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/vmx/vmx.c|5692| <<handle_ept_misconfig>> !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
+ *   - arch/x86/kvm/x86.c|7573| <<vcpu_mmio_write>> && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))
+ *   - arch/x86/kvm/x86.c|8202| <<emulator_pio_in_out>> r = kvm_io_bus_write(vcpu, KVM_PIO_BUS, port, size, data);
+ */
 /* kvm_io_bus_write - called under kvm->slots_lock */
 int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val)
@@ -5195,6 +5310,19 @@ int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		.len = len,
 	};
 
+	/*
+	 * enum kvm_bus {
+	 *     KVM_MMIO_BUS,
+	 *     KVM_PIO_BUS,
+	 *     KVM_VIRTIO_CCW_NOTIFY_BUS,
+	 *     KVM_FAST_MMIO_BUS,
+	 *     KVM_NR_BUSES
+	 * };
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm *kvm;
+	 *    -> struct kvm_io_bus __rcu *buses[KVM_NR_BUSES];
+	 */
 	bus = srcu_dereference(vcpu->kvm->buses[bus_idx], &vcpu->kvm->srcu);
 	if (!bus)
 		return -ENOMEM;
@@ -5203,6 +5331,10 @@ int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 }
 EXPORT_SYMBOL_GPL(kvm_io_bus_write);
 
+/*
+ * called by:
+ *   - arch/s390/kvm/diag.c|265| <<__diag_virtio_hypercall>> ret = kvm_io_bus_write_cookie(vcpu, KVM_VIRTIO_CCW_NOTIFY_BUS,
+ */
 /* kvm_io_bus_write_cookie - called under kvm->slots_lock */
 int kvm_io_bus_write_cookie(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx,
 			    gpa_t addr, int len, const void *val, long cookie)
@@ -5273,6 +5405,22 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 	return r < 0 ? r : 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1877| <<vgic_register_its_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|803| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|1115| <<vgic_register_dist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,
+ *   - arch/mips/kvm/loongson_ipi.c|209| <<kvm_init_loongson_ipi>> kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, addr, 0x400, device);
+ *   - arch/powerpc/kvm/mpic.c|1449| <<map_mmio>> kvm_io_bus_register_dev(opp->kvm, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/i8254.c|706| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
+ *   - arch/x86/kvm/i8254.c|713| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
+ *   - arch/x86/kvm/i8259.c|607| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
+ *   - arch/x86/kvm/i8259.c|616| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_elcr);
+ *   - arch/x86/kvm/ioapic.c|698| <<kvm_ioapic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, ioapic->base_address,
+ *   - virt/kvm/coalesced_mmio.c|156| <<kvm_vm_ioctl_register_coalesced_mmio>> ret = kvm_io_bus_register_dev(kvm,
+ *   - virt/kvm/eventfd.c|845| <<kvm_assign_ioeventfd_idx>> ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
+ */
 /* Caller must hold slots_lock. */
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev)
diff --git a/virt/lib/irqbypass.c b/virt/lib/irqbypass.c
index 28fda42e4..585be896e 100644
--- a/virt/lib/irqbypass.c
+++ b/virt/lib/irqbypass.c
@@ -22,11 +22,38 @@
 MODULE_LICENSE("GPL v2");
 MODULE_DESCRIPTION("IRQ bypass manager utility module");
 
+/*
+ * When a physical I/O device is assigned to a virtual machine through
+ * facilities like VFIO and KVM, the interrupt for the device generally
+ * bounces through the host system before being injected into the VM.
+ * However, hardware technologies exist that often allow the host to be
+ * bypassed for some of these scenarios.  Intel Posted Interrupts allow
+ * the specified physical edge interrupts to be directly injected into a
+ * guest when delivered to a physical processor while the vCPU is
+ * running.  ARM IRQ Forwarding allows forwarded physical interrupts to
+ * be directly deactivated by the guest.
+ *
+ * The IRQ bypass manager here is meant to provide the shim to connect
+ * interrupt producers, generally the host physical device driver, with
+ * interrupt consumers, generally the hypervisor, in order to configure
+ * these bypass mechanism.  To do this, we base the connection on a
+ * shared, opaque token.  For KVM-VFIO this is expected to be an
+ * eventfd_ctx since this is the connection we already use to connect an
+ * eventfd to an irqfd on the in-kernel path.  When a producer and
+ * consumer with matching tokens is found, callbacks via both registered
+ * participants allow the bypass facilities to be automatically enabled.
+ */
+
 static LIST_HEAD(producers);
 static LIST_HEAD(consumers);
 static DEFINE_MUTEX(lock);
 
 /* @lock must be held when calling connect */
+/*
+ * called by:
+ *   - virt/lib/irqbypass.c|136| <<irq_bypass_register_producer>> ret = __connect(producer, consumer);
+ *   - virt/lib/irqbypass.c|236| <<irq_bypass_register_consumer>> ret = __connect(producer, consumer);
+ */
 static int __connect(struct irq_bypass_producer *prod,
 		     struct irq_bypass_consumer *cons)
 {
@@ -55,6 +82,11 @@ static int __connect(struct irq_bypass_producer *prod,
 }
 
 /* @lock must be held when calling disconnect */
+/*
+ * called by:
+ *   - virt/lib/irqbypass.c|188| <<irq_bypass_unregister_producer>> __disconnect(producer, consumer);
+ *   - virt/lib/irqbypass.c|288| <<irq_bypass_unregister_consumer>> __disconnect(producer, consumer);
+ */
 static void __disconnect(struct irq_bypass_producer *prod,
 			 struct irq_bypass_consumer *cons)
 {
@@ -81,6 +113,11 @@ static void __disconnect(struct irq_bypass_producer *prod,
  * Add the provided IRQ producer to the list of producers and connect
  * with any matching token found on the IRQ consumers list.
  */
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|354| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&vdev->ctx[vector].producer);
+ *   - drivers/vhost/vdpa.c|196| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer);
+ */
 int irq_bypass_register_producer(struct irq_bypass_producer *producer)
 {
 	struct irq_bypass_producer *tmp;
@@ -132,6 +169,12 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_producer);
  * Remove a previously registered IRQ producer from the list of producers
  * and disconnect it from any connected IRQ consumer.
  */
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|311| <<vfio_msi_set_vector_signal>> irq_bypass_unregister_producer(&vdev->ctx[vector].producer);
+ *   - drivers/vhost/vdpa.c|190| <<vhost_vdpa_setup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ *   - drivers/vhost/vdpa.c|206| <<vhost_vdpa_unsetup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ */
 void irq_bypass_unregister_producer(struct irq_bypass_producer *producer)
 {
 	struct irq_bypass_producer *tmp;
@@ -176,6 +219,10 @@ EXPORT_SYMBOL_GPL(irq_bypass_unregister_producer);
  * Add the provided IRQ consumer to the list of consumers and connect
  * with any matching token found on the IRQ producer list.
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|426| <<kvm_irqfd_assign>> ret = irq_bypass_register_consumer(&irqfd->consumer);
+ */
 int irq_bypass_register_consumer(struct irq_bypass_consumer *consumer)
 {
 	struct irq_bypass_consumer *tmp;
@@ -228,6 +275,10 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_consumer);
  * Remove a previously registered IRQ consumer from the list of consumers
  * and disconnect it from any connected IRQ producer.
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|144| <<irqfd_shutdown>> irq_bypass_unregister_consumer(&irqfd->consumer);
+ */
 void irq_bypass_unregister_consumer(struct irq_bypass_consumer *consumer)
 {
 	struct irq_bypass_consumer *tmp;
-- 
2.34.1

