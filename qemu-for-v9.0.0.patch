From 8f0aec83869faf173153f8d408d01233b062dd1e Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sat, 10 Aug 2024 17:09:36 -0700
Subject: [PATCH 1/1] qemu for v9.0.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 accel/kvm/kvm-all.c               | 128 ++++++++++++
 block.c                           | 316 ++++++++++++++++++++++++++++++
 block/aio_task.c                  |   6 +
 block/block-backend.c             | 123 ++++++++++++
 block/file-posix.c                | 121 ++++++++++++
 block/io.c                        |  86 ++++++++
 block/mirror.c                    |  95 +++++++++
 block/qcow2-threads.c             |   5 +
 block/qcow2.c                     |   6 +
 blockdev.c                        | 123 ++++++++++++
 hw/block/virtio-blk.c             | 285 +++++++++++++++++++++++++++
 hw/core/vm-change-state-handler.c |  87 ++++++++
 hw/i386/kvm/apic.c                |  14 ++
 hw/net/virtio-net.c               |  22 +++
 hw/pci/msi.c                      |  10 +
 hw/pci/msix.c                     |  60 ++++++
 hw/pci/pci.c                      |  85 ++++++++
 hw/pci/pcie.c                     |   4 +
 hw/pci/pcie_port.c                |  14 ++
 hw/scsi/scsi-disk.c               |  11 ++
 hw/vfio/helpers.c                 |  17 ++
 hw/vfio/pci.c                     | 132 +++++++++++++
 hw/vfio/pci.h                     |   9 +
 hw/virtio/virtio-bus.c            |  22 +++
 hw/virtio/virtio-pci.c            |  34 ++++
 hw/virtio/virtio.c                | 105 ++++++++++
 include/block/block_int-common.h  |  27 +++
 include/hw/hotplug.h              |  23 +++
 include/hw/i386/apic_internal.h   |  10 +
 include/hw/pci/pci_bus.h          |   8 +
 include/hw/pci/pci_device.h       |  51 +++++
 include/hw/scsi/scsi.h            |  10 +
 include/hw/virtio/virtio-blk.h    |  39 ++++
 include/hw/virtio/virtio.h        |   7 +
 include/sysemu/kvm_int.h          |  39 ++++
 iothread.c                        |   8 +
 linux-headers/linux/kvm.h         |   4 +
 net/tap-linux.c                   |   8 +
 net/tap.c                         |  43 ++++
 system/cpus.c                     |   4 +
 system/memory.c                   |  29 +++
 target/arm/kvm.c                  |   5 +
 target/arm/tcg/tlb_helper.c       |  21 ++
 target/i386/kvm/kvm.c             |  14 ++
 util/aio-posix.c                  |  23 +++
 util/event_notifier-posix.c       |  55 ++++++
 util/main-loop.c                  |  33 ++++
 util/osdep.c                      |  29 +++
 48 files changed, 2410 insertions(+)

diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index 931f74256..79e9fbf32 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -1182,6 +1182,11 @@ static uint32_t adjust_ioeventfd_endianness(uint32_t val, uint32_t size)
     return val;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1655| <<kvm_mem_ioeventfd_add>> r = kvm_set_ioeventfd_mmio(fd, section->offset_within_address_space, data, true, int128_get64(section->size), match_data);
+ *   - accel/kvm/kvm-all.c|1673| <<kvm_mem_ioeventfd_del>> r = kvm_set_ioeventfd_mmio(fd, section->offset_within_address_space, data, false, int128_get64(section->size), match_data);
+ */
 static int kvm_set_ioeventfd_mmio(int fd, hwaddr addr, uint32_t val,
                                   bool assign, uint32_t size, bool datamatch)
 {
@@ -1818,6 +1823,22 @@ void kvm_init_irq_routing(KVMState *s)
     kvm_arch_init_irq_routing(s);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2156| <<kvm_irqchip_add_hv_sint_route>> kvm_irqchip_commit_routes(s);
+ *   - hw/i386/kvm/ioapic.c|46| <<kvm_pc_setup_irq_routing>> kvm_irqchip_commit_routes(s);
+ *   - hw/intc/arm_gic_kvm.c|590| <<kvm_arm_gic_realize>> kvm_irqchip_commit_routes(kvm_state);
+ *   - hw/intc/arm_gicv3_kvm.c|874| <<kvm_arm_gicv3_realize>> kvm_irqchip_commit_routes(kvm_state);
+ *   - hw/intc/ioapic.c|208| <<ioapic_update_kvm_routes>> kvm_irqchip_commit_routes(kvm_state);
+ *   - hw/intc/openpic_kvm.c|254| <<kvm_openpic_realize>> kvm_irqchip_commit_routes(s);
+ *   - hw/intc/s390_flic_kvm.c|345| <<kvm_s390_add_adapter_routes>> kvm_irqchip_commit_routes(kvm_state);
+ *   - hw/misc/ivshmem.c|294| <<ivshmem_vector_unmask>> kvm_irqchip_commit_routes(kvm_state);
+ *   - hw/vfio/pci.c|510| <<vfio_update_kvm_msi_virq>> kvm_irqchip_commit_routes(kvm_state);
+ *   - hw/virtio/virtio-pci.c|1005| <<virtio_pci_one_vector_unmask>> kvm_irqchip_commit_routes(kvm_state);
+ *   - include/sysemu/kvm.h|468| <<kvm_irqchip_commit_route_changes>> kvm_irqchip_commit_routes(c->s);
+ *   - target/i386/kvm/kvm.c|5552| <<kvm_update_msi_routes_all>> kvm_irqchip_commit_routes(kvm_state);
+ *   - target/riscv/kvm/kvm-cpu.c|1660| <<kvm_riscv_aia_create>> kvm_irqchip_commit_routes(kvm_state);
+ */
 void kvm_irqchip_commit_routes(KVMState *s)
 {
     int ret;
@@ -1836,6 +1857,13 @@ void kvm_irqchip_commit_routes(KVMState *s)
     assert(ret == 0);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1919| <<kvm_irqchip_add_irq_route>> kvm_add_routing_entry(s, &e);
+ *   - accel/kvm/kvm-all.c|2060| <<kvm_irqchip_add_msi_route>> kvm_add_routing_entry(s, &kroute);
+ *   - accel/kvm/kvm-all.c|2177| <<kvm_irqchip_add_adapter_route>> kvm_add_routing_entry(s, &kroute);
+ *   - accel/kvm/kvm-all.c|2204| <<kvm_irqchip_add_hv_sint_route>> kvm_add_routing_entry(s, &kroute);
+ */
 static void kvm_add_routing_entry(KVMState *s,
                                   struct kvm_irq_routing_entry *entry)
 {
@@ -1860,6 +1888,10 @@ static void kvm_add_routing_entry(KVMState *s,
     set_gsi(s, entry->gsi);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2100| <<kvm_irqchip_update_msi_route>> return kvm_update_routing_entry(s, &kroute);
+ */
 static int kvm_update_routing_entry(KVMState *s,
                                     struct kvm_irq_routing_entry *new_entry)
 {
@@ -1884,6 +1916,17 @@ static int kvm_update_routing_entry(KVMState *s,
     return -ESRCH;
 }
 
+/*
+ * called by:
+ *   - hw/i386/kvm/ioapic.c|32| <<kvm_pc_setup_irq_routing>> kvm_irqchip_add_irq_route(s, i, KVM_IRQCHIP_PIC_MASTER, i);
+ *   - hw/i386/kvm/ioapic.c|35| <<kvm_pc_setup_irq_routing>> kvm_irqchip_add_irq_route(s, i, KVM_IRQCHIP_PIC_SLAVE, i - 8);
+ *   - hw/i386/kvm/ioapic.c|40| <<kvm_pc_setup_irq_routing>> kvm_irqchip_add_irq_route(s, i, KVM_IRQCHIP_IOAPIC, 2);
+ *   - hw/i386/kvm/ioapic.c|42| <<kvm_pc_setup_irq_routing>> kvm_irqchip_add_irq_route(s, i, KVM_IRQCHIP_IOAPIC, i);
+ *   - hw/intc/arm_gic_kvm.c|585| <<kvm_arm_gic_realize>> kvm_irqchip_add_irq_route(kvm_state, i, 0, i);
+ *   - hw/intc/arm_gicv3_kvm.c|869| <<kvm_arm_gicv3_realize>> kvm_irqchip_add_irq_route(kvm_state, i, 0, i);
+ *   - hw/intc/openpic_kvm.c|248| <<kvm_openpic_realize>> kvm_irqchip_add_irq_route(kvm_state, i, 0, i);
+ *   - target/riscv/kvm/kvm-cpu.c|1657| <<kvm_riscv_aia_create>> kvm_irqchip_add_irq_route(kvm_state, idx, 0, idx);
+ */
 void kvm_irqchip_add_irq_route(KVMState *s, int irq, int irqchip, int pin)
 {
     struct kvm_irq_routing_entry e = {};
@@ -1947,6 +1990,11 @@ static int kvm_irqchip_get_virq(KVMState *s)
     }
 }
 
+/*
+ * called by:
+ *   - hw/i386/kvm/apic.c|193| <<kvm_send_msi>> ret = kvm_irqchip_send_msi(kvm_state, *msg);
+ *   - target/i386/kvm/xen-emu.c|456| <<kvm_xen_inject_vcpu_callback_vector>> kvm_irqchip_send_msi(kvm_state, msg);
+ */
 int kvm_irqchip_send_msi(KVMState *s, MSIMessage msg)
 {
     struct kvm_msi msi;
@@ -1960,6 +2008,13 @@ int kvm_irqchip_send_msi(KVMState *s, MSIMessage msg)
     return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
 }
 
+/*
+ * called by:
+ *   - hw/misc/ivshmem.c|434| <<ivshmem_add_kvm_msi_virq>> ret = kvm_irqchip_add_msi_route(&c, vector, pdev);
+ *   - hw/vfio/pci.c|469| <<vfio_add_kvm_msi_virq>> vector->virq = kvm_irqchip_add_msi_route(&vfio_route_change, vector_n, &vdev->pdev);
+ *   - hw/virtio/virtio-pci.c|819| <<kvm_virtio_pci_vq_vector_use>> ret = kvm_irqchip_add_msi_route(&c, vector, &proxy->pci_dev);
+ *   - target/i386/kvm/kvm.c|5400| <<kvm_arch_init_irq_routing>> if (kvm_irqchip_add_msi_route(&c, 0, NULL) < 0) {
+ */
 int kvm_irqchip_add_msi_route(KVMRouteChange *c, int vector, PCIDevice *dev)
 {
     struct kvm_irq_routing_entry kroute = {};
@@ -1984,6 +2039,32 @@ int kvm_irqchip_add_msi_route(KVMRouteChange *c, int vector, PCIDevice *dev)
         return virq;
     }
 
+    /*
+     * 1162 struct kvm_irq_routing_msi {
+     * 1163         __u32 address_lo;
+     * 1164         __u32 address_hi;
+     * 1165         __u32 data;
+     * 1166         union {
+     * 1167                 __u32 pad;
+     * 1168                 __u32 devid;
+     * 1169         };
+     * 1170 };
+     *
+     * 1200 struct kvm_irq_routing_entry {
+     * 1201         __u32 gsi;
+     * 1202         __u32 type;
+     * 1203         __u32 flags;
+     * 1204         __u32 pad;
+     * 1205         union {
+     * 1206                 struct kvm_irq_routing_irqchip irqchip;
+     * 1207                 struct kvm_irq_routing_msi msi;
+     * 1208                 struct kvm_irq_routing_s390_adapter adapter;
+     * 1209                 struct kvm_irq_routing_hv_sint hv_sint;
+     * 1210                 struct kvm_irq_routing_xen_evtchn xen_evtchn;
+     * 1211                 __u32 pad[8];
+     * 1212         } u;
+     * 1213 };
+     */
     kroute.gsi = virq;
     kroute.type = KVM_IRQ_ROUTING_MSI;
     kroute.flags = 0;
@@ -1994,6 +2075,11 @@ int kvm_irqchip_add_msi_route(KVMRouteChange *c, int vector, PCIDevice *dev)
         kroute.flags = KVM_MSI_VALID_DEVID;
         kroute.u.msi.devid = pci_requester_id(dev);
     }
+    /*
+     * called by:
+     *   - accel/kvm/kvm-all.c|2078| <<kvm_irqchip_add_msi_route>> if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
+     *   - accel/kvm/kvm-all.c|2129| <<kvm_irqchip_update_msi_route>> if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
+     */
     if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
         kvm_irqchip_release_virq(s, virq);
         return -EINVAL;
@@ -2014,6 +2100,14 @@ int kvm_irqchip_add_msi_route(KVMRouteChange *c, int vector, PCIDevice *dev)
     return virq;
 }
 
+/*
+ * called by:
+ *   - hw/intc/ioapic.c|205| <<ioapic_update_kvm_routes>> kvm_irqchip_update_msi_route(kvm_state, i, msg, NULL);
+ *   - hw/misc/ivshmem.c|290| <<ivshmem_vector_unmask>> ret = kvm_irqchip_update_msi_route(kvm_state, v->virq, msg, dev);
+ *   - hw/vfio/pci.c|519| <<vfio_update_kvm_msi_virq>> kvm_irqchip_update_msi_route(kvm_state, vector->virq, msg, pdev);
+ *   - hw/virtio/virtio-pci.c|1027| <<virtio_pci_one_vector_unmask>> ret = kvm_irqchip_update_msi_route(kvm_state, irqfd->virq, msg, &proxy->pci_dev);
+ *   - target/i386/kvm/kvm.c|5550| <<kvm_update_msi_routes_all>> kvm_irqchip_update_msi_route(kvm_state, entry->virq, msg, dev);
+ */
 int kvm_irqchip_update_msi_route(KVMState *s, int virq, MSIMessage msg,
                                  PCIDevice *dev)
 {
@@ -2037,15 +2131,28 @@ int kvm_irqchip_update_msi_route(KVMState *s, int virq, MSIMessage msg,
         kroute.flags = KVM_MSI_VALID_DEVID;
         kroute.u.msi.devid = pci_requester_id(dev);
     }
+    /*
+     * called by:
+     *   - accel/kvm/kvm-all.c|2078| <<kvm_irqchip_add_msi_route>> if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
+     *   - accel/kvm/kvm-all.c|2129| <<kvm_irqchip_update_msi_route>> if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
+     */
     if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
         return -EINVAL;
     }
 
     trace_kvm_irqchip_update_msi_route(virq);
 
+    /*
+     * 只在此处调用
+     */
     return kvm_update_routing_entry(s, &kroute);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2202| <<kvm_irqchip_add_irqfd_notifier_gsi>> return kvm_irqchip_assign_irqfd(s, n, rn, virq, true);
+ *   - accel/kvm/kvm-all.c|2208| <<kvm_irqchip_remove_irqfd_notifier_gsi>> return kvm_irqchip_assign_irqfd(s, n, NULL, virq, false);
+ */
 static int kvm_irqchip_assign_irqfd(KVMState *s, EventNotifier *event,
                                     EventNotifier *resample, int virq,
                                     bool assign)
@@ -2092,6 +2199,10 @@ static int kvm_irqchip_assign_irqfd(KVMState *s, EventNotifier *event,
     return kvm_vm_ioctl(s, KVM_IRQFD, &irqfd);
 }
 
+/*
+ * called by:
+ *   - hw/intc/s390_flic_kvm.c|338| <<kvm_s390_add_adapter_routes>> ret = kvm_irqchip_add_adapter_route(kvm_state, &routes->adapter);
+ */
 int kvm_irqchip_add_adapter_route(KVMState *s, AdapterInfo *adapter)
 {
     struct kvm_irq_routing_entry kroute = {};
@@ -2191,6 +2302,19 @@ int kvm_irqchip_update_msi_route(KVMState *s, int virq, MSIMessage msg)
 }
 #endif /* !KVM_CAP_IRQ_ROUTING */
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2220| <<kvm_irqchip_add_irqfd_notifier>> return kvm_irqchip_add_irqfd_notifier_gsi(s, n, rn, GPOINTER_TO_INT(gsi));
+ *   - accel/stubs/kvm-stub.c|86| <<kvm_irqchip_add_irqfd_notifier_gsi>> int kvm_irqchip_add_irqfd_notifier_gsi(KVMState *s, EventNotifier *n,
+ *   - hw/hyperv/hyperv.c|438| <<hyperv_sint_route_new>> r = kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, &sint_route->sint_set_notifier, ack_notifier, gsi);
+ *   - hw/misc/ivshmem.c|296| <<ivshmem_vector_unmask>> ret = kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, n, NULL, v->virq);
+ *   - hw/misc/ivshmem.c|467| <<setup_interrupt>> kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, n, NULL, s->msi_vectors[vector].virq);
+ *   - hw/remote/proxy.c|45| <<proxy_intx_update>> kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, &dev->intr, &dev->resample, dev->virq);
+ *   - hw/s390x/virtio-ccw.c|1003| <<virtio_ccw_add_irqfd>> return kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, notifier, NULL, dev->routes.gsi[n]);
+ *   - hw/vfio/pci.c|142| <<vfio_intx_enable_kvm>> if (kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, &vdev->intx.interrupt, &vdev->intx.unmask, vdev->intx.route.irq)) {
+ *   - hw/vfio/pci.c|483| <<vfio_connect_kvm_msi_virq>> if (kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, &vector->kvm_interrupt, NULL, vector->virq) < 0) {
+ *   - hw/virtio/virtio-pci.c|844| <<kvm_virtio_pci_irqfd_use>> return kvm_irqchip_add_irqfd_notifier_gsi(kvm_state, n, NULL, irqfd->virq);
+ */
 int kvm_irqchip_add_irqfd_notifier_gsi(KVMState *s, EventNotifier *n,
                                        EventNotifier *rn, int virq)
 {
@@ -3387,6 +3511,10 @@ int kvm_on_sigbus_vcpu(CPUState *cpu, int code, void *addr)
 #endif
 }
 
+/*
+ * 在以下使用kvm_on_sigbus():
+ *   - system/cpus.c|373| <<sigbus_handler>> if (kvm_on_sigbus(siginfo->si_code, siginfo->si_addr)) {
+ */
 /* Called synchronously (via signalfd) in main thread.  */
 int kvm_on_sigbus(int code, void *addr)
 {
diff --git a/block.c b/block.c
index 468cf5e67..d005c38c2 100644
--- a/block.c
+++ b/block.c
@@ -68,6 +68,16 @@
 
 #define NOT_DONE 0x7fffffff /* used while emulated sync operation in progress */
 
+/*
+ * 在以下使用graph_bdrv_states:
+ *   - block.c|72| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockDriverState) graph_bdrv_states =
+ *   - block.c|1636| <<bdrv_assign_node_name>> QTAILQ_INSERT_TAIL(&graph_bdrv_states, bs, node_list);
+ *   - block.c|5682| <<bdrv_delete>> QTAILQ_REMOVE(&graph_bdrv_states, bs, node_list);
+ *   - block.c|6360| <<bdrv_find_node>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+ *   - block.c|6379| <<bdrv_named_nodes_list>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+ *   - block.c|6514| <<bdrv_get_xdbg_block_graph>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+ *   - block.c|6579| <<bdrv_next_node>> return QTAILQ_FIRST(&graph_bdrv_states);
+ */
 /* Protected by BQL */
 static QTAILQ_HEAD(, BlockDriverState) graph_bdrv_states =
     QTAILQ_HEAD_INITIALIZER(graph_bdrv_states);
@@ -1594,6 +1604,10 @@ static void update_options_from_flags(QDict *options, int flags)
     }
 }
 
+/*
+ * called by:
+ *   - block.c|1649| <<bdrv_open_driver>> bdrv_assign_node_name(bs, node_name, &local_err);
+ */
 static void bdrv_assign_node_name(BlockDriverState *bs,
                                   const char *node_name,
                                   Error **errp)
@@ -1633,6 +1647,16 @@ static void bdrv_assign_node_name(BlockDriverState *bs,
 
     /* copy node name into the bs and insert it into the graph list */
     pstrcpy(bs->node_name, sizeof(bs->node_name), node_name);
+    /*
+     * 在以下使用graph_bdrv_states:
+     *   - block.c|72| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockDriverState) graph_bdrv_states =
+     *   - block.c|1636| <<bdrv_assign_node_name>> QTAILQ_INSERT_TAIL(&graph_bdrv_states, bs, node_list);
+     *   - block.c|5682| <<bdrv_delete>> QTAILQ_REMOVE(&graph_bdrv_states, bs, node_list);
+     *   - block.c|6360| <<bdrv_find_node>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+     *   - block.c|6379| <<bdrv_named_nodes_list>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+     *   - block.c|6514| <<bdrv_get_xdbg_block_graph>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+     *   - block.c|6579| <<bdrv_next_node>> return QTAILQ_FIRST(&graph_bdrv_states);
+     */
     QTAILQ_INSERT_TAIL(&graph_bdrv_states, bs, node_list);
 out:
     g_free(gen_node_name);
@@ -2379,10 +2403,39 @@ TransactionActionDrv bdrv_drv_set_perm_drv = {
     .commit = bdrv_drv_set_perm_commit,
 };
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ */
 /*
  * After calling this function, the transaction @tran may only be completed
  * while holding a reader lock for the graph.
  */
+/*
+ * called by:
+ *   - block.c|2546| <<bdrv_node_refresh_perm>> ret = bdrv_drv_set_perm(bs, cumulative_perms, cumulative_shared_perms, tran, errp);
+ */
 static int GRAPH_RDLOCK
 bdrv_drv_set_perm(BlockDriverState *bs, uint64_t perm, uint64_t shared_perm,
                   Transaction *tran, Error **errp)
@@ -2392,6 +2445,10 @@ bdrv_drv_set_perm(BlockDriverState *bs, uint64_t perm, uint64_t shared_perm,
         return 0;
     }
 
+    /*
+     * block/file-posix.c|3955| <<global>> BlockDriver bdrv_file.bdrv_check_perm = raw_check_perm,
+     * block/file-posix.c|4324| <<global>> BlockDriver bdrv_host_device.bdrv_check_perm = raw_check_perm,
+     */
     if (bs->drv->bdrv_check_perm) {
         int ret = bs->drv->bdrv_check_perm(bs, perm, shared_perm, errp);
         if (ret < 0) {
@@ -2489,6 +2546,31 @@ bdrv_replace_child_tran(BdrvChild *child, BlockDriverState *new_bs,
     /* old_bs reference is transparently moved from @child to @s */
 }
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ */
 /*
  * Refresh permissions in @bs subtree. The function is intended to be called
  * after some graph modification that was done without permission update.
@@ -2496,6 +2578,10 @@ bdrv_replace_child_tran(BdrvChild *child, BlockDriverState *new_bs,
  * After calling this function, the transaction @tran may only be completed
  * while holding a reader lock for the graph.
  */
+/*
+ * called by:
+ *   - block.c|2597| <<bdrv_do_refresh_perms>> ret = bdrv_node_refresh_perm(bs, q, tran, errp);
+ */
 static int GRAPH_RDLOCK
 bdrv_node_refresh_perm(BlockDriverState *bs, BlockReopenQueue *q,
                        Transaction *tran, Error **errp)
@@ -2543,6 +2629,10 @@ bdrv_node_refresh_perm(BlockDriverState *bs, BlockReopenQueue *q,
         return 0;
     }
 
+    /*
+     * called by:
+     *   - block.c|2546| <<bdrv_node_refresh_perm>> ret = bdrv_drv_set_perm(bs, cumulative_perms, cumulative_shared_perms, tran, errp);
+     */
     ret = bdrv_drv_set_perm(bs, cumulative_perms, cumulative_shared_perms, tran,
                             errp);
     if (ret < 0) {
@@ -2575,6 +2665,11 @@ bdrv_node_refresh_perm(BlockDriverState *bs, BlockReopenQueue *q,
  * After calling this function, the transaction @tran may only be completed
  * while holding a reader lock for the graph.
  */
+/*
+ * called by:
+ *   - block.c|2710| <<bdrv_list_refresh_perms>> return bdrv_do_refresh_perms(refresh_list, q, tran, errp);
+ *   - block.c|2778| <<bdrv_refresh_perms>> ret = bdrv_do_refresh_perms(list, NULL, tran, errp);
+ */
 static int GRAPH_RDLOCK
 bdrv_do_refresh_perms(GSList *list, BlockReopenQueue *q, Transaction *tran,
                       Error **errp)
@@ -2599,6 +2694,31 @@ bdrv_do_refresh_perms(GSList *list, BlockReopenQueue *q, Transaction *tran,
     return 0;
 }
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ */
 /*
  * @list is any list of nodes. List is completed by all subtrees and
  * topologically sorted. It's not a problem if some node occurs in the @list
@@ -2607,6 +2727,12 @@ bdrv_do_refresh_perms(GSList *list, BlockReopenQueue *q, Transaction *tran,
  * After calling this function, the transaction @tran may only be completed
  * while holding a reader lock for the graph.
  */
+/*
+ * called by:
+ *   - block.c|4577| <<bdrv_reopen_multiple>> ret = bdrv_list_refresh_perms(refresh_list, bs_queue, tran, errp);
+ *   - block.c|5391| <<bdrv_replace_node_common>> ret = bdrv_list_refresh_perms(refresh_list, NULL, tran, errp);
+ *   - block.c|5506| <<bdrv_replace_child_bs>> ret = bdrv_list_refresh_perms(refresh_list, NULL, tran, errp);
+ */
 static int GRAPH_RDLOCK
 bdrv_list_refresh_perms(GSList *list, BlockReopenQueue *q, Transaction *tran,
                         Error **errp)
@@ -2618,6 +2744,11 @@ bdrv_list_refresh_perms(GSList *list, BlockReopenQueue *q, Transaction *tran,
         refresh_list = bdrv_topological_dfs(refresh_list, found, list->data);
     }
 
+    /*
+     * called by:
+     *   - block.c|2710| <<bdrv_list_refresh_perms>> return bdrv_do_refresh_perms(refresh_list, q, tran, errp);
+     *   - block.c|2778| <<bdrv_refresh_perms>> ret = bdrv_do_refresh_perms(list, NULL, tran, errp);
+     */
     return bdrv_do_refresh_perms(refresh_list, q, tran, errp);
 }
 
@@ -5326,6 +5457,31 @@ bdrv_replace_node_noperm(BlockDriverState *from,
     return 0;
 }
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ */
 /*
  * Switch all parents of @from to point to @to instead. @from and @to must be in
  * the same AioContext and both must be drained.
@@ -5339,6 +5495,12 @@ bdrv_replace_node_noperm(BlockDriverState *from,
  * With @detach_subchain=true @to must be in a backing chain of @from. In this
  * case backing link of the cow-parent of @to is removed.
  */
+/*
+ * called by:
+ *   - block.c|5406| <<bdrv_replace_node>> return bdrv_replace_node_common(from, to, true, false, errp);
+ *   - block.c|5422| <<bdrv_drop_filter>> ret = bdrv_replace_node_common(bs, child_bs, true, true, errp);
+ *   - block.c|5889| <<bdrv_drop_intermediate>> bdrv_replace_node_common(top, base, false, false, &local_err);
+ */
 static int GRAPH_WRLOCK
 bdrv_replace_node_common(BlockDriverState *from, BlockDriverState *to,
                          bool auto_skip, bool detach_subchain, Error **errp)
@@ -5384,6 +5546,12 @@ bdrv_replace_node_common(BlockDriverState *from, BlockDriverState *to,
     refresh_list = g_slist_prepend(refresh_list, to);
     refresh_list = g_slist_prepend(refresh_list, from);
 
+    /*
+     * called by:
+     *   - block.c|4577| <<bdrv_reopen_multiple>> ret = bdrv_list_refresh_perms(refresh_list, bs_queue, tran, errp);
+     *   - block.c|5391| <<bdrv_replace_node_common>> ret = bdrv_list_refresh_perms(refresh_list, NULL, tran, errp);
+     *   - block.c|5506| <<bdrv_replace_child_bs>> ret = bdrv_list_refresh_perms(refresh_list, NULL, tran, errp);
+     */
     ret = bdrv_list_refresh_perms(refresh_list, NULL, tran, errp);
     if (ret < 0) {
         goto out;
@@ -5396,9 +5564,25 @@ out:
     return ret;
 }
 
+/*
+ * called by:
+ *   - block.c|5709| <<bdrv_insert_node>> ret = bdrv_replace_node(bs, new_node_bs, errp);
+ *   - block/commit.c|106| <<commit_abort>> bdrv_replace_node(s->commit_top_bs, commit_top_backing_bs, &error_abort);
+ *   - block/commit.c|442| <<commit_start>> bdrv_replace_node(commit_top_bs, top, &error_abort);
+ *   - block/mirror.c|766| <<mirror_exit_common>> bdrv_replace_node(to_replace, target_bs, &local_err);
+ *   - block/mirror.c|794| <<mirror_exit_common>> bdrv_replace_node(mirror_top_bs, mirror_top_bs->backing->bs, &error_abort);
+ *   - block/mirror.c|2029| <<mirror_start_job>> bdrv_replace_node(mirror_top_bs, bs, &error_abort);
+ *   - blockdev.c|1546| <<external_snapshot_abort>> bdrv_replace_node(state->new_bs, state->old_bs, &error_abort);
+ */
 int bdrv_replace_node(BlockDriverState *from, BlockDriverState *to,
                       Error **errp)
 {
+    /*
+     * called by:
+     *   - block.c|5406| <<bdrv_replace_node>> return bdrv_replace_node_common(from, to, true, false, errp);
+     *   - block.c|5422| <<bdrv_drop_filter>> ret = bdrv_replace_node_common(bs, child_bs, true, true, errp);
+     *   - block.c|5889| <<bdrv_drop_intermediate>> bdrv_replace_node_common(top, base, false, false, &local_err);
+     */
     return bdrv_replace_node_common(from, to, true, false, errp);
 }
 
@@ -5683,6 +5867,15 @@ bdrv_co_change_backing_file(BlockDriverState *bs, const char *backing_file,
  *
  * Returns the bottommost base image if bs == NULL.
  */
+/*
+ * called by:
+ *   - block.c|5900| <<bdrv_find_base>> return bdrv_find_overlay(bs, NULL);
+ *   - block/commit.c|346| <<commit_start>> s->base_overlay = bdrv_find_overlay(top, base);
+ *   - block/mirror.c|1917| <<mirror_start_job>> s->base_overlay = bdrv_find_overlay(bs, base);
+ *   - block/mirror.c|1962| <<mirror_start_job>> filtered_target = bdrv_cow_bs(bdrv_find_overlay(bs, target));
+ *   - block/stream.c|292| <<stream_start>> base_overlay = bdrv_find_overlay(bs, base);
+ *   - blockdev.c|2718| <<qmp_block_commit>> BlockDriverState *overlay_bs = bdrv_find_overlay(bs, top_bs);
+ */
 BlockDriverState *bdrv_find_overlay(BlockDriverState *active,
                                     BlockDriverState *bs)
 {
@@ -5703,11 +5896,25 @@ BlockDriverState *bdrv_find_overlay(BlockDriverState *active,
     return NULL;
 }
 
+/*
+ * called by:
+ *   - blockdev.c|2622| <<qmp_block_commit>> base_bs = bdrv_find_base(top_bs);
+ *   - blockdev.c|3380| <<qmp_change_backing_file>> if (bdrv_find_base(image_bs) == image_bs) {
+ */
 /* Given a BDS, searches for the base layer. */
 BlockDriverState *bdrv_find_base(BlockDriverState *bs)
 {
     GLOBAL_STATE_CODE();
 
+    /*
+     * called by:
+     *   - block.c|5900| <<bdrv_find_base>> return bdrv_find_overlay(bs, NULL);
+     *   - block/commit.c|346| <<commit_start>> s->base_overlay = bdrv_find_overlay(top, base);
+     *   - block/mirror.c|1917| <<mirror_start_job>> s->base_overlay = bdrv_find_overlay(bs, base);
+     *   - block/mirror.c|1962| <<mirror_start_job>> filtered_target = bdrv_cow_bs(bdrv_find_overlay(bs, target));
+     *   - block/stream.c|292| <<stream_start>> base_overlay = bdrv_find_overlay(bs, base);
+     *   - blockdev.c|2718| <<qmp_block_commit>> BlockDriverState *overlay_bs = bdrv_find_overlay(bs, top_bs);
+     */
     return bdrv_find_overlay(bs, NULL);
 }
 
@@ -6189,6 +6396,24 @@ void bdrv_iterate_format(void (*it)(void *opaque, const char *name),
     g_free(formats);
 }
 
+/*
+ * called by:
+ *   - block.c|1623| <<bdrv_assign_node_name>> if (bdrv_find_node(node_name)) {
+ *   - block.c|6548| <<bdrv_lookup_bs>> bs = bdrv_find_node(node_name);
+ *   - block.c|7935| <<check_to_replace_node>> BlockDriverState *to_replace_bs = bdrv_find_node(node_name);
+ *   - block/block-backend.c|733| <<monitor_add_blk>> if (bdrv_find_node(name)) {
+ *   - block/copy-on-read.c|66| <<cor_open>> bottom_bs = bdrv_find_node(bottom_node);
+ *   - block/mirror.c|1184| <<mirror_complete>> s->to_replace = bdrv_find_node(s->replaces);
+ *   - block/monitor/block-hmp-cmds.c|149| <<hmp_drive_del>> bs = bdrv_find_node(id);
+ *   - block/qapi-sysemu.c|287| <<blockdev_insert_medium>> bs = bdrv_find_node(node_name);
+ *   - block/snapshot.c|495| <<bdrv_all_get_snapshot_devices>> BlockDriverState *bs = bdrv_find_node(devices->value);
+ *   - block/write-threshold.c|37| <<qmp_block_set_write_threshold>> bs = bdrv_find_node(node_name);
+ *   - blockdev.c|3476| <<qmp_blockdev_reopen>> bs = bdrv_find_node(options->node_name);
+ *   - blockdev.c|3511| <<qmp_blockdev_del>> bs = bdrv_find_node(node_name);
+ *   - blockdev.c|3588| <<qmp_x_blockdev_change>> new_bs = bdrv_find_node(node);
+ *   - blockdev.c|3633| <<qmp_x_blockdev_set_iothread>> bs = bdrv_find_node(node_name);
+ *   - tests/unit/test-block-iothread.c|766| <<test_propagate_mirror>> filter = bdrv_find_node("filter_node");
+ */
 /* This function is to find a node in the bs graph */
 BlockDriverState *bdrv_find_node(const char *node_name)
 {
@@ -6197,6 +6422,16 @@ BlockDriverState *bdrv_find_node(const char *node_name)
     assert(node_name);
     GLOBAL_STATE_CODE();
 
+    /*
+     * 在以下使用graph_bdrv_states:
+     *   - block.c|72| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockDriverState) graph_bdrv_states =
+     *   - block.c|1636| <<bdrv_assign_node_name>> QTAILQ_INSERT_TAIL(&graph_bdrv_states, bs, node_list);
+     *   - block.c|5682| <<bdrv_delete>> QTAILQ_REMOVE(&graph_bdrv_states, bs, node_list);
+     *   - block.c|6360| <<bdrv_find_node>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+     *   - block.c|6379| <<bdrv_named_nodes_list>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+     *   - block.c|6514| <<bdrv_get_xdbg_block_graph>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+     *   - block.c|6579| <<bdrv_next_node>> return QTAILQ_FIRST(&graph_bdrv_states);
+     */
     QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
         if (!strcmp(node_name, bs->node_name)) {
             return bs;
@@ -6362,6 +6597,39 @@ XDbgBlockGraph *bdrv_get_xdbg_block_graph(Error **errp)
     return xdbg_graph_finalize(gr);
 }
 
+/*
+ * called by:
+ *   - block.c|4097| <<bdrv_open_inherit>> bs = bdrv_lookup_bs(reference, reference, errp);
+ *   - block.c|4833| <<bdrv_reopen_parse_file_or_backing>> new_child_bs = bdrv_lookup_bs(NULL, str, errp);
+ *   - block/amend.c|102| <<qmp_x_blockdev_amend>> bs = bdrv_lookup_bs(NULL, node_name, errp);
+ *   - block/export/export.c|103| <<blk_exp_add>> bs = bdrv_lookup_bs(NULL, export->node_name, errp);
+ *   - block/monitor/bitmap-qmp-cmds.c|71| <<block_dirty_bitmap_lookup>> bs = bdrv_lookup_bs(node, node, NULL);
+ *   - block/monitor/bitmap-qmp-cmds.c|104| <<qmp_block_dirty_bitmap_add>> bs = bdrv_lookup_bs(node, node, errp);
+ *   - block/monitor/block-hmp-cmds.c|564| <<hmp_qemu_io>> bs = bdrv_lookup_bs(NULL, device, &err);
+ *   - block/replication.c|408| <<backup_job_cleanup>> top_bs = bdrv_lookup_bs(s->top_id, s->top_id, NULL);
+ *   - block/replication.c|570| <<replication_start>> top_bs = bdrv_lookup_bs(s->top_id, s->top_id, NULL);
+ *   - blockdev-nbd.c|194| <<qmp_nbd_server_add>> bs = bdrv_lookup_bs(arg->device, arg->device, errp);
+ *   - blockdev.c|1064| <<qmp_get_root_bs>> bs = bdrv_lookup_bs(name, name, errp);
+ *   - blockdev.c|1408| <<external_snapshot_action>> state->old_bs = bdrv_lookup_bs(device, node_name, errp);
+ *   - blockdev.c|1446| <<external_snapshot_action>> bdrv_lookup_bs(snapshot_node_name, snapshot_node_name, NULL)) {
+ *   - blockdev.c|1631| <<drive_backup_action>> bs = bdrv_lookup_bs(backup->device, backup->device, errp);
+ *   - blockdev.c|1802| <<blockdev_backup_action>> bs = bdrv_lookup_bs(backup->device, backup->device, errp);
+ *   - blockdev.c|1807| <<blockdev_backup_action>> target_bs = bdrv_lookup_bs(backup->target, backup->target, errp);
+ *   - blockdev.c|2263| <<qmp_block_resize>> bs = bdrv_lookup_bs(device, node_name, &local_err);
+ *   - blockdev.c|2345| <<qmp_block_stream>> bs = bdrv_lookup_bs(device, device, errp);
+ *   - blockdev.c|2363| <<qmp_block_stream>> base_bs = bdrv_lookup_bs(NULL, base_node, errp);
+ *   - blockdev.c|2378| <<qmp_block_stream>> bottom_bs = bdrv_lookup_bs(NULL, bottom, errp);
+ *   - blockdev.c|2552| <<qmp_block_commit>> bs = bdrv_lookup_bs(device, device, NULL);
+ *   - blockdev.c|2576| <<qmp_block_commit>> top_bs = bdrv_lookup_bs(NULL, top_node, errp);
+ *   - blockdev.c|2606| <<qmp_block_commit>> base_bs = bdrv_lookup_bs(NULL, base_node, errp);
+ *   - blockdev.c|3169| <<qmp_blockdev_mirror>> target_bs = bdrv_lookup_bs(target, target, errp);
+ *   - blockdev.c|3369| <<qmp_change_backing_file>> image_bs = bdrv_lookup_bs(NULL, image_node_name, &local_err);
+ *   - blockdev.c|3563| <<qmp_x_blockdev_change>> parent_bs = bdrv_lookup_bs(parent, parent, errp);
+ *   - hw/core/qdev-properties-system.c|114| <<set_drive_helper>> bs = bdrv_lookup_bs(NULL, str, errp);
+ *   - hw/core/qdev-properties-system.c|136| <<set_drive_helper>> bs = bdrv_lookup_bs(NULL, str, NULL);
+ *   - migration/block-dirty-bitmap.c|1075| <<dirty_bitmap_load_header>> s->bs = bdrv_lookup_bs(NULL, amin->string, &local_err);
+ *   - migration/block-dirty-bitmap.c|1078| <<dirty_bitmap_load_header>> s->bs = bdrv_lookup_bs(s->node_alias, s->node_alias,
+ */
 BlockDriverState *bdrv_lookup_bs(const char *device,
                                  const char *node_name,
                                  Error **errp)
@@ -6385,6 +6653,18 @@ BlockDriverState *bdrv_lookup_bs(const char *device,
     }
 
     if (node_name) {
+        /*
+	 * 在以下使用graph_bdrv_states:
+	 *   - block.c|72| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockDriverState) graph_bdrv_states =
+	 *   - block.c|1636| <<bdrv_assign_node_name>> QTAILQ_INSERT_TAIL(&graph_bdrv_states, bs, node_list);
+	 *   - block.c|5682| <<bdrv_delete>> QTAILQ_REMOVE(&graph_bdrv_states, bs, node_list);
+	 *   - block.c|6360| <<bdrv_find_node>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+	 *   - block.c|6379| <<bdrv_named_nodes_list>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+	 *   - block.c|6514| <<bdrv_get_xdbg_block_graph>> QTAILQ_FOREACH(bs, &graph_bdrv_states, node_list) {
+	 *   - block.c|6579| <<bdrv_next_node>> return QTAILQ_FIRST(&graph_bdrv_states);
+         *
+	 * 从graph_bdrv_states寻找!
+	 */
         bs = bdrv_find_node(node_name);
 
         if (bs) {
@@ -8270,6 +8550,11 @@ BdrvChild *bdrv_primary_child(BlockDriverState *bs)
     return found;
 }
 
+/*
+ * called by:
+ *   - block.c|8527| <<bdrv_skip_implicit_filters>> return bdrv_do_skip_filters(bs, true);
+ *   - block.c|8537| <<bdrv_skip_filters>> return bdrv_do_skip_filters(bs, false);
+ */
 static BlockDriverState * GRAPH_RDLOCK
 bdrv_do_skip_filters(BlockDriverState *bs, bool stop_on_explicit_filter)
 {
@@ -8315,6 +8600,32 @@ BlockDriverState *bdrv_skip_implicit_filters(BlockDriverState *bs)
     return bdrv_do_skip_filters(bs, true);
 }
 
+/*
+ * called by:
+ *   - block.c|5876| <<bdrv_find_overlay>> bs = bdrv_skip_filters(bs);
+ *   - block.c|5877| <<bdrv_find_overlay>> active = bdrv_skip_filters(active);
+ *   - block.c|6973| <<bdrv_find_backing_image>> for (curr_bs = bdrv_skip_filters(bs);
+ *   - block.c|8602| <<bdrv_backing_chain_next>> return bdrv_skip_filters(bdrv_cow_bs(bdrv_skip_filters(bs)));
+ *   - block/block-backend.c|2840| <<blk_commit_all>> BlockDriverState *unfiltered_bs = bdrv_skip_filters(blk_bs(blk));
+ *   - block/commit.c|274| <<commit_start>> if (bdrv_skip_filters(top) == bdrv_skip_filters(base)) {
+ *   - block/commit.c|354| <<commit_start>> assert(bdrv_skip_filters(filtered_base) == bdrv_skip_filters(base));
+ *   - block/mirror.c|726| <<mirror_exit_common>> BlockDriverState *unfiltered_target = bdrv_skip_filters(target_bs);
+ *   - block/mirror.c|738| <<mirror_exit_common>> ret = bdrv_open_backing_file(bdrv_skip_filters(target_bs), NULL,
+ *   - block/mirror.c|1785| <<mirror_start_job>> if (bdrv_skip_filters(bs) == bdrv_skip_filters(target)) {
+ *   - block/mirror.c|1877| <<mirror_start_job>> if (bdrv_chain_contains(bs, bdrv_skip_filters(target))) {
+ *   - block/mirror.c|1964| <<mirror_start_job>> assert(bdrv_skip_filters(filtered_target) ==
+ *   - block/mirror.c|1965| <<mirror_start_job>> bdrv_skip_filters(target));
+ *   - block/stream.c|67| <<stream_prepare>> unfiltered_bs = bdrv_skip_filters(s->target_bs);
+ *   - block/stream.c|91| <<stream_prepare>> unfiltered_base = bdrv_skip_filters(base);
+ *   - block/stream.c|165| <<stream_run>> unfiltered_bs = bdrv_skip_filters(s->target_bs);
+ *   - blockdev.c|1672| <<drive_backup_action>> source = bdrv_cow_bs(bdrv_skip_filters(bs));
+ *   - blockdev.c|2685| <<qmp_block_commit>> bdrv_skip_filters(top_bs) == bdrv_skip_filters(bs))
+ *   - blockdev.c|2688| <<qmp_block_commit>> if (bdrv_skip_filters(top_bs) == bdrv_skip_filters(bs)) {
+ *   - blockdev.c|3054| <<qmp_drive_mirror>> target_backing_bs = bdrv_cow_bs(bdrv_skip_filters(bs));
+ *   - qemu-img.c|1746| <<convert_iteration_sectors>> base = bdrv_cow_bs(bdrv_skip_filters(src_bs));
+ *   - qemu-img.c|3163| <<get_block_status>> bs = bdrv_skip_filters(bs);
+ *   - qemu-img.c|3673| <<img_rebase>> unfiltered_bs = bdrv_skip_filters(bs);
+ */
 /*
  * Return the first BDS that does not have a filtered child down the
  * chain starting from @bs (including @bs itself).
@@ -8322,6 +8633,11 @@ BlockDriverState *bdrv_skip_implicit_filters(BlockDriverState *bs)
 BlockDriverState *bdrv_skip_filters(BlockDriverState *bs)
 {
     IO_CODE();
+    /*
+     * called by:
+     *   - block.c|8527| <<bdrv_skip_implicit_filters>> return bdrv_do_skip_filters(bs, true);
+     *   - block.c|8537| <<bdrv_skip_filters>> return bdrv_do_skip_filters(bs, false);
+     */
     return bdrv_do_skip_filters(bs, false);
 }
 
diff --git a/block/aio_task.c b/block/aio_task.c
index 9bd17ea2c..8abec7c1b 100644
--- a/block/aio_task.c
+++ b/block/aio_task.c
@@ -27,6 +27,12 @@
 #include "block/aio_task.h"
 
 struct AioTaskPool {
+    /*
+     * 在以下使用AioTaskPool->main_co:
+     *   - block/aio_task.c|57| <<aio_task_co>> aio_co_wake(pool->main_co);
+     *   - block/aio_task.c|64| <<aio_task_pool_wait_one>> assert(qemu_coroutine_self() == pool->main_co);
+     *   - block/aio_task.c|103| <<aio_task_pool_new>> pool->main_co = qemu_coroutine_self();
+     */
     Coroutine *main_co;
     int status;
     int max_busy_tasks;
diff --git a/block/block-backend.c b/block/block-backend.c
index db6f9b92a..106d1b7cc 100644
--- a/block/block-backend.c
+++ b/block/block-backend.c
@@ -115,6 +115,13 @@ static QTAILQ_HEAD(, BlockBackend) block_backends =
  * All BlockBackends referenced by the monitor and which are iterated through by
  * blk_next(). Protected by BQL.
  */
+/*
+ * 在以下使用monitor_block_backends:
+ *   - block/block-backend.c|118| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) monitor_block_backends =
+ *   - block/block-backend.c|590| <<blk_next>> : QTAILQ_FIRST(&monitor_block_backends);
+ *   - block/block-backend.c|707| <<monitor_add_blk>> QTAILQ_INSERT_TAIL(&monitor_block_backends, blk, monitor_link);
+ *   - block/block-backend.c|723| <<monitor_remove_blk>> QTAILQ_REMOVE(&monitor_block_backends, blk, monitor_link);
+ */
 static QTAILQ_HEAD(, BlockBackend) monitor_block_backends =
     QTAILQ_HEAD_INITIALIZER(monitor_block_backends);
 
@@ -318,6 +325,14 @@ static AioContext *blk_root_get_parent_aio_context(BdrvChild *c)
     return blk_get_aio_context(blk);
 }
 
+/*
+ * 在以下使用child_root:
+ *   - block/block-backend.c|328| <<global>> static const BdrvChildClass child_root = {
+ *   - block/block-backend.c|821| <<bdrv_first_blk>> if (child->klass == &child_root) {
+ *   - block/block-backend.c|849| <<bdrv_is_root_node>> if (c->klass != &child_root) {
+ *   - block/block-backend.c|964| <<blk_insert_bs>> blk->root = bdrv_root_attach_child(bs, "root", &child_root, BDRV_CHILD_FILTERED | BDRV_CHILD_PRIMARY,
+ *                                                  blk->perm, blk->shared_perm, blk, errp);
+ */
 static const BdrvChildClass child_root = {
     .inherit_options    = blk_root_inherit_options,
 
@@ -583,9 +598,28 @@ void blk_remove_all_bs(void)
  *     ...
  * }
  */
+/*
+ * called by:
+ *   - block/block-backend.c|748| <<blk_by_name>> while ((blk = blk_next(blk)) != NULL) {
+ *   - block/block-backend.c|839| <<blk_by_legacy_dinfo>> while ((blk = blk_next(blk)) != NULL) {
+ *   - blockdev.c|123| <<override_max_devs>> for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+ *   - blockdev.c|228| <<drive_get>> for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+ *   - blockdev.c|252| <<drive_check_orphaned>> for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+ *   - blockdev.c|300| <<drive_get_max_bus>> for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+ *   - migration/block-dirty-bitmap.c|629| <<init_dirty_bitmap_migration>> for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+ *   - monitor/hmp.c|1335| <<monitor_find_completion_by_table>> while ((blk = blk_next(blk)) != NULL) {
+ *   - monitor/qmp-cmds.c|89| <<qmp_cont>> for (blk = blk_next(NULL); blk; blk = blk_next(blk)) {
+ */
 BlockBackend *blk_next(BlockBackend *blk)
 {
     GLOBAL_STATE_CODE();
+    /*
+     * 在以下使用monitor_block_backends:
+     *   - block/block-backend.c|118| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) monitor_block_backends =
+     *   - block/block-backend.c|590| <<blk_next>> : QTAILQ_FIRST(&monitor_block_backends);
+     *   - block/block-backend.c|707| <<monitor_add_blk>> QTAILQ_INSERT_TAIL(&monitor_block_backends, blk, monitor_link);
+     *   - block/block-backend.c|723| <<monitor_remove_blk>> QTAILQ_REMOVE(&monitor_block_backends, blk, monitor_link);
+     */
     return blk ? QTAILQ_NEXT(blk, monitor_link)
                : QTAILQ_FIRST(&monitor_block_backends);
 }
@@ -682,6 +716,14 @@ void bdrv_next_cleanup(BdrvNextIterator *it)
  * Returns true on success and false on failure. In the latter case, an Error
  * object is returned through @errp.
  */
+/*
+ * called by:
+ *   - blockdev.c|641| <<blockdev_init>> if (!monitor_add_blk(blk, id, errp)) {
+ *   - tests/unit/test-blockjob.c|84| <<create_blk>> monitor_add_blk(blk, name, &err);
+ *   - tests/unit/test-replication.c|192| <<start_primary>> monitor_add_blk(blk, P_ID, &error_abort);
+ *   - tests/unit/test-replication.c|307| <<start_secondary>> monitor_add_blk(blk, S_LOCAL_DISK_ID, &error_abort);
+ *   - tests/unit/test-replication.c|333| <<start_secondary>> monitor_add_blk(blk, S_ID, &error_abort)
+ */
 bool monitor_add_blk(BlockBackend *blk, const char *name, Error **errp)
 {
     assert(!blk->name);
@@ -704,6 +746,13 @@ bool monitor_add_blk(BlockBackend *blk, const char *name, Error **errp)
     }
 
     blk->name = g_strdup(name);
+    /*
+     * 在以下使用monitor_block_backends:
+     *   - block/block-backend.c|118| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) monitor_block_backends =
+     *   - block/block-backend.c|590| <<blk_next>> : QTAILQ_FIRST(&monitor_block_backends);
+     *   - block/block-backend.c|707| <<monitor_add_blk>> QTAILQ_INSERT_TAIL(&monitor_block_backends, blk, monitor_link);
+     *   - block/block-backend.c|723| <<monitor_remove_blk>> QTAILQ_REMOVE(&monitor_block_backends, blk, monitor_link);
+     */
     QTAILQ_INSERT_TAIL(&monitor_block_backends, blk, monitor_link);
     return true;
 }
@@ -720,6 +769,13 @@ void monitor_remove_blk(BlockBackend *blk)
         return;
     }
 
+    /*
+     * 在以下使用monitor_block_backends:
+     *   - block/block-backend.c|118| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) monitor_block_backends =
+     *   - block/block-backend.c|590| <<blk_next>> : QTAILQ_FIRST(&monitor_block_backends);
+     *   - block/block-backend.c|707| <<monitor_add_blk>> QTAILQ_INSERT_TAIL(&monitor_block_backends, blk, monitor_link);
+     *   - block/block-backend.c|723| <<monitor_remove_blk>> QTAILQ_REMOVE(&monitor_block_backends, blk, monitor_link);
+     */
     QTAILQ_REMOVE(&monitor_block_backends, blk, monitor_link);
     g_free(blk->name);
     blk->name = NULL;
@@ -770,6 +826,14 @@ static BlockBackend * GRAPH_RDLOCK bdrv_first_blk(BlockDriverState *bs)
     assert_bdrv_graph_readable();
 
     QLIST_FOREACH(child, &bs->parents, next_parent) {
+        /*
+	 * 在以下使用child_root:
+	 *   - block/block-backend.c|328| <<global>> static const BdrvChildClass child_root = {
+	 *   - block/block-backend.c|821| <<bdrv_first_blk>> if (child->klass == &child_root) {
+	 *   - block/block-backend.c|849| <<bdrv_is_root_node>> if (c->klass != &child_root) {
+	 *   - block/block-backend.c|964| <<blk_insert_bs>> blk->root = bdrv_root_attach_child(bs, "root", &child_root, BDRV_CHILD_FILTERED | BDRV_CHILD_PRIMARY,
+	 *                                                  blk->perm, blk->shared_perm, blk, errp);
+	 */
         if (child->klass == &child_root) {
             return child->opaque;
         }
@@ -798,6 +862,14 @@ bool bdrv_is_root_node(BlockDriverState *bs)
     assert_bdrv_graph_readable();
 
     QLIST_FOREACH(c, &bs->parents, next_parent) {
+        /*
+	 * 在以下使用child_root:
+	 *   - block/block-backend.c|328| <<global>> static const BdrvChildClass child_root = {
+	 *   - block/block-backend.c|821| <<bdrv_first_blk>> if (child->klass == &child_root) {
+	 *   - block/block-backend.c|849| <<bdrv_is_root_node>> if (c->klass != &child_root) {
+	 *   - block/block-backend.c|964| <<blk_insert_bs>> blk->root = bdrv_root_attach_child(bs, "root", &child_root, BDRV_CHILD_FILTERED | BDRV_CHILD_PRIMARY,
+	 *                                                  blk->perm, blk->shared_perm, blk, errp);
+	 */
         if (c->klass != &child_root) {
             return false;
         }
@@ -913,6 +985,14 @@ int blk_insert_bs(BlockBackend *blk, BlockDriverState *bs, Error **errp)
     GLOBAL_STATE_CODE();
     bdrv_ref(bs);
     bdrv_graph_wrlock();
+    /*
+     * 在以下使用child_root:
+     *   - block/block-backend.c|328| <<global>> static const BdrvChildClass child_root = {
+     *   - block/block-backend.c|821| <<bdrv_first_blk>> if (child->klass == &child_root) {
+     *   - block/block-backend.c|849| <<bdrv_is_root_node>> if (c->klass != &child_root) {
+     *   - block/block-backend.c|964| <<blk_insert_bs>> blk->root = bdrv_root_attach_child(bs, "root", &child_root, BDRV_CHILD_FILTERED | BDRV_CHILD_PRIMARY,
+     *                                                  blk->perm, blk->shared_perm, blk, errp);
+     */
     blk->root = bdrv_root_attach_child(bs, "root", &child_root,
                                        BDRV_CHILD_FILTERED | BDRV_CHILD_PRIMARY,
                                        blk->perm, blk->shared_perm,
@@ -1565,6 +1645,10 @@ static void blk_aio_complete_bh(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * 比如cb是virtio_blk_rw_complete()
+ *     co_entry是blk_aio_write_entry()
+ */
 static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset,
                                 int64_t bytes,
                                 void *iobuf, CoroutineEntry co_entry,
@@ -1610,6 +1694,11 @@ static void coroutine_fn blk_aio_read_entry(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * 在以下使用blk_aio_write_entry():
+ *   - block/block-backend.c|1630| <<blk_aio_pwrite_zeroes>> return blk_aio_prwv(blk, offset, bytes, NULL, blk_aio_write_entry, flags | BDRV_REQ_ZERO_WRITE, cb, opaque);
+ *   - block/block-backend.c|1712| <<blk_aio_pwritev>> return blk_aio_prwv(blk, offset, qiov->size, qiov, blk_aio_write_entry, flags, cb, opaque);
+ */
 static void coroutine_fn blk_aio_write_entry(void *opaque)
 {
     BlkAioEmAIOCB *acb = opaque;
@@ -1622,6 +1711,19 @@ static void coroutine_fn blk_aio_write_entry(void *opaque)
     blk_aio_complete(acb);
 }
 
+/*
+ * 在以下使用blk_aio_pwrite_zeroes():
+ *   - hw/block/virtio-blk.c|568| <<virtio_blk_handle_discard_write_zeroes>> blk_aio_pwrite_zeroes(s->blk, sector << BDRV_SECTOR_BITS, bytes, blk_aio_flags, virtio_blk_discard_write_zeroes_complete, req);
+ *   - hw/nvme/ctrl.c|2192| <<nvme_rw_cb>> req->aiocb = blk_aio_pwrite_zeroes(blk, offset, mlen, BDRV_REQ_MAY_UNMAP, nvme_rw_complete_cb, req);
+ *   - hw/nvme/ctrl.c|2539| <<nvme_dsm_md_cb>> iocb->aiocb = blk_aio_pwrite_zeroes(ns->blkconf.blk, nvme_moff(ns, slba), nvme_m2b(ns, nlb), BDRV_REQ_MAY_UNMAP, nvme_dsm_cb, iocb);
+ *   - hw/nvme/ctrl.c|3637| <<nvme_do_write>> req->aiocb = blk_aio_pwrite_zeroes(blk, data_offset, data_size, BDRV_REQ_MAY_UNMAP, nvme_rw_cb, req);
+ *   - hw/nvme/ctrl.c|3911| <<nvme_zone_reset_epilogue_cb>> iocb->aiocb = blk_aio_pwrite_zeroes(ns->blkconf.blk, moff, count, BDRV_REQ_MAY_UNMAP, nvme_zone_reset_cb, iocb);
+ *   - hw/nvme/ctrl.c|3965| <<nvme_zone_reset_cb>> iocb->aiocb = blk_aio_pwrite_zeroes(ns->blkconf.blk, nvme_l2b(ns, zone->d.zslba), nvme_l2b(ns, ns->zone_size), BDRV_REQ_MAY_UNMAP, nvme_zone_reset_epilogue_cb, iocb);
+ *   - hw/nvme/ctrl.c|6523| <<nvme_format_ns_cb>> iocb->aiocb = blk_aio_pwrite_zeroes(ns->blkconf.blk, iocb->offset, bytes, BDRV_REQ_MAY_UNMAP, nvme_format_ns_cb, iocb);
+ *   - hw/nvme/dif.c|632| <<nvme_dif_rw>> req->aiocb = blk_aio_pwrite_zeroes(blk, offset, len, flags, nvme_dif_rw_mdata_out_cb, ctx);
+ *   - hw/scsi/scsi-disk.c|1889| <<scsi_disk_emulate_write_same>> r->req.aiocb = blk_aio_pwrite_zeroes(s->qdev.conf.blk, r->req.cmd.lba * s->qdev.blocksize, nb_sectors * s->qdev.blocksize, flags, scsi_aio_complete, r);
+ *   - qemu-io-cmds.c|1683| <<aio_write_f>> blk_aio_pwrite_zeroes(blk, ctx->offset, count, ctx->flags, aio_write_done, ctx);
+ */
 BlockAIOCB *blk_aio_pwrite_zeroes(BlockBackend *blk, int64_t offset,
                                   int64_t bytes, BdrvRequestFlags flags,
                                   BlockCompletionFunc *cb, void *opaque)
@@ -1702,6 +1804,27 @@ BlockAIOCB *blk_aio_preadv(BlockBackend *blk, int64_t offset,
                         blk_aio_read_entry, flags, cb, opaque);
 }
 
+/*
+ * called by:
+ *   - hw/block/dataplane/xen-block.c|393| <<xen_block_do_aio>> blk_aio_pwritev(dataplane->blk, request->start, &request->v, 0, xen_block_complete_aio, request);
+ *   - hw/block/m25p80.c|564| <<flash_sync_page>> blk_aio_pwritev(s->blk, page * s->pi->page_size, iov, 0, blk_sync_complete, iov);
+ *   - hw/block/m25p80.c|580| <<flash_sync_area>> blk_aio_pwritev(s->blk, off, iov, 0, blk_sync_complete, iov);
+ *   - hw/block/virtio-blk.c|399| <<submit_requests>> blk_aio_pwritev(blk, sector_num << BDRV_SECTOR_BITS, qiov, flags, virtio_blk_rw_complete, mrb->reqs[start]);
+ *   - hw/ide/core.c|1101| <<ide_sector_write>> s->pio_aiocb = blk_aio_pwritev(s->blk, sector_num << BDRV_SECTOR_BITS, &s->qiov, 0, ide_sector_write_cb, s);
+ *   - hw/nvme/ctrl.c|1465| <<nvme_blk_write>> req->aiocb = blk_aio_pwritev(blk, offset, &req->sg.iov, 0, cb, req);
+ *   - hw/nvme/ctrl.c|2917| <<nvme_copy_out_cb>> iocb->aiocb = blk_aio_pwritev(ns->blkconf.blk, nvme_moff(ns, iocb->slba), &iocb->iov, 0, nvme_copy_out_completed_cb, iocb);
+ *   - hw/nvme/ctrl.c|3011| <<nvme_copy_in_completed_cb>> iocb->aiocb = blk_aio_pwritev(ns->blkconf.blk, nvme_l2b(ns, iocb->slba), &iocb->iov, 0, nvme_copy_out_cb, iocb);
+ *   - hw/nvme/dif.c|530| <<nvme_dif_rw_mdata_out_cb>> req->aiocb = blk_aio_pwritev(blk, offset, &ctx->mdata.iov, 0, nvme_dif_rw_cb, ctx);
+ *   - hw/nvme/dif.c|701| <<nvme_dif_rw>> req->aiocb = blk_aio_pwritev(ns->blkconf.blk, offset, &ctx->data.iov, 0, nvme_dif_rw_mdata_out_cb, ctx);
+ *   - hw/scsi/scsi-disk.c|1842| <<scsi_write_same_complete>> r->req.aiocb = blk_aio_pwritev(s->qdev.conf.blk, data->sector << BDRV_SECTOR_BITS, &data->qiov, 0, scsi_write_same_complete, data);
+ *   - hw/scsi/scsi-disk.c|1914| <<scsi_disk_emulate_write_same>> r->req.aiocb = blk_aio_pwritev(s->qdev.conf.blk, data->sector << BDRV_SECTOR_BITS, &data->qiov, 0, scsi_write_same_complete, data);
+ *   - hw/scsi/scsi-disk.c|3089| <<scsi_dma_writev>> return blk_aio_pwritev(s->qdev.conf.blk, offset, iov, 0, cb, cb_opaque);
+ *   - qemu-img.c|4493| <<bench_cb>> acb = blk_aio_pwritev(b->blk, offset, b->qiov, 0, bench_cb, b);
+ *   - qemu-io-cmds.c|663| <<do_aio_writev>> blk_aio_pwritev(blk, offset, qiov, flags, aio_rw_done, &async_ret);
+ *   - qemu-io-cmds.c|1699| <<aio_write_f>> blk_aio_pwritev(blk, ctx->offset, &ctx->qiov, ctx->flags, aio_write_done, ctx);
+ *   - system/dma-helpers.c|265| <<dma_blk_write_io_func>> return blk_aio_pwritev(blk, offset, iov, 0, cb, cb_opaque);
+ *   - tests/unit/test-replication.c|113| <<test_blk_write>> blk_aio_pwritev(blk, offset, &qiov, 0, blk_rw_done, &async_ret);
+ */
 BlockAIOCB *blk_aio_pwritev(BlockBackend *blk, int64_t offset,
                             QEMUIOVector *qiov, BdrvRequestFlags flags,
                             BlockCompletionFunc *cb, void *opaque)
diff --git a/block/file-posix.c b/block/file-posix.c
index 35684f7e2..335fd0bef 100644
--- a/block/file-posix.c
+++ b/block/file-posix.c
@@ -839,6 +839,15 @@ typedef enum {
  * file; if @unlock == true, also unlock the unneeded bytes.
  * @shared_perm_lock_bits is the mask of all permissions that are NOT shared.
  */
+/*
+ * called by:
+ *   - block/file-posix.c|1020| <<raw_handle_perm_lock>> ret = raw_apply_lock_bytes(s, s->fd, s->perm | new_perm,
+ *   - block/file-posix.c|1034| <<raw_handle_perm_lock>> raw_apply_lock_bytes(s, s->fd, s->perm, ~s->shared_perm,
+ *   - block/file-posix.c|1044| <<raw_handle_perm_lock>> raw_apply_lock_bytes(s, s->fd, new_perm, ~new_shared,
+ *   - block/file-posix.c|2963| <<raw_co_create>> result = raw_apply_lock_bytes(NULL, fd, perm, ~shared, false, errp);
+ *   - block/file-posix.c|3028| <<raw_co_create>> raw_apply_lock_bytes(NULL, fd, 0, 0, true, &local_err);
+ *   - block/file-posix.c|3886| <<raw_check_perm>> ret = raw_apply_lock_bytes(NULL, s->perm_change_fd, perm, ~shared,
+ */
 static int raw_apply_lock_bytes(BDRVRawState *s, int fd,
                                 uint64_t perm_lock_bits,
                                 uint64_t shared_perm_lock_bits,
@@ -955,6 +964,37 @@ static int raw_check_lock_bytes(int fd, uint64_t perm, uint64_t shared_perm,
     return 0;
 }
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ *
+ * called by:
+ *   - block/file-posix.c|3820| <<raw_check_perm>> ret = raw_handle_perm_lock(bs, RAW_PL_PREPARE, perm, shared, errp);
+ *   - block/file-posix.c|3830| <<raw_check_perm>> raw_handle_perm_lock(bs, RAW_PL_ABORT, 0, 0, NULL);
+ *   - block/file-posix.c|3857| <<raw_set_perm>> raw_handle_perm_lock(bs, RAW_PL_COMMIT, perm, shared, NULL);
+ *   - block/file-posix.c|3873| <<raw_abort_perm_update>> raw_handle_perm_lock(bs, RAW_PL_ABORT, 0, 0, NULL);
+ */
 static int raw_handle_perm_lock(BlockDriverState *bs,
                                 RawPermLockOp op,
                                 uint64_t new_perm, uint64_t new_shared,
@@ -2404,6 +2444,18 @@ out:
     return result;
 }
 
+/*
+ * called by:
+ *   - block/file-posix.c|2533| <<raw_co_prw>> ret = raw_thread_pool_submit(handle_aiocb_rw, &acb);
+ *   - block/file-posix.c|2603| <<raw_co_flush_to_disk>> return raw_thread_pool_submit(handle_aiocb_flush, &acb);
+ *   - block/file-posix.c|2642| <<raw_regular_truncate>> return raw_thread_pool_submit(handle_aiocb_truncate, &acb);
+ *   - block/file-posix.c|3400| <<raw_co_zone_report>> return raw_thread_pool_submit(handle_aiocb_zone_report, &acb);
+ *   - block/file-posix.c|3477| <<raw_co_zone_mgmt>> ret = raw_thread_pool_submit(handle_aiocb_zone_mgmt, &acb);
+ *   - block/file-posix.c|3559| <<raw_do_pdiscard>> ret = raw_thread_pool_submit(handle_aiocb_discard, &acb);
+ *   - block/file-posix.c|3665| <<raw_do_pwrite_zeroes>> return raw_thread_pool_submit(handler, &acb);
+ *   - block/file-posix.c|3906| <<raw_co_copy_range_to>> return raw_thread_pool_submit(handle_aiocb_copy_range, &acb);
+ *   - block/file-posix.c|4249| <<hdev_co_ioctl>> return raw_thread_pool_submit(handle_aiocb_ioctl, &acb);
+ */
 static int coroutine_fn raw_thread_pool_submit(ThreadPoolFunc func, void *arg)
 {
     return thread_pool_submit_co(func, arg);
@@ -3567,6 +3619,37 @@ raw_co_pdiscard(BlockDriverState *bs, int64_t offset, int64_t bytes)
     return raw_do_pdiscard(bs, offset, bytes, false);
 }
 
+/*
+ * (gdb) bt
+ * #0  bdrv_co_get_self_request (bs=0x629000005200) at ../block/io.c:720
+ * #1  0x00005555570f6929 in raw_do_pwrite_zeroes (bs=0x629000005200, offset=458752, bytes=65536, flags=BDRV_REQ_NO_FALLBACK, blkdev=false) at ../block/file-posix.c:3598
+ * #2  0x00005555570f6f1d in raw_co_pwrite_zeroes (bs=0x629000005200, offset=458752, bytes=65536, flags=BDRV_REQ_NO_FALLBACK) at ../block/file-posix.c:3641
+ * #3  0x0000555556fb193d in bdrv_co_do_pwrite_zeroes (bs=0x629000005200, offset=458752, bytes=65536, flags=258) at ../block/io.c:1901
+ * #4  0x0000555556fb2b50 in bdrv_aligned_pwritev (child=0x608000002920, req=0x7ffd14df80d0, offset=458752, bytes=65536, align=512, qiov=0x0, qiov_offset=0, flags=258) at ../block/io.c:2100
+ * #5  0x0000555556fb33a7 in bdrv_co_do_zero_pwritev (child=0x608000002920, offset=458752, bytes=65536, flags=258, req=0x7ffd14df80d0) at ../block/io.c:2183
+ * #6  0x0000555556fb3c43 in bdrv_co_pwritev_part (child=0x608000002920, offset=458752, bytes=65536, qiov=0x0, qiov_offset=0, flags=258) at ../block/io.c:2283
+ * #7  0x0000555556fb362c in bdrv_co_pwritev (child=0x608000002920, offset=458752, bytes=65536, qiov=0x0, flags=258) at ../block/io.c:2216
+ * #8  0x0000555556fb3f19 in bdrv_co_pwrite_zeroes (child=0x608000002920, offset=458752, bytes=65536, flags=BDRV_REQ_NO_FALLBACK) at ../block/io.c:2322
+ * #9  0x0000555556ffae77 in handle_alloc_space (bs=0x62900000a200, l2meta=0x60b0011bb730) at ../block/qcow2.c:2557
+ * #10 0x0000555556ffb3de in qcow2_co_pwritev_task (bs=0x62900000a200, host_offset=458752, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, l2meta=0x60b0011bb730) at ../block/qcow2.c:2609
+ * #11 0x0000555556ffb828 in qcow2_co_pwritev_task_entry (task=0x7ffd14c07320) at ../block/qcow2.c:2657
+ * #12 0x0000555556ff97cd in qcow2_add_task
+ *     (bs=0x62900000a200, pool=0x0, func=0x555556ffb653 <qcow2_co_pwritev_task_entry>, subcluster_type=QCOW2_SUBCLUSTER_UNALLOCATED_PLAIN, host_offset=458752, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, l2meta=0x60b0011bb730) at ../block/qcow2.c:2307
+ * #13 0x0000555556ffbd90 in qcow2_co_pwritev_part (bs=0x62900000a200, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/qcow2.c:2709
+ * #14 0x0000555556face86 in bdrv_driver_pwritev (bs=0x62900000a200, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/io.c:1070
+ * #15 0x0000555556fb2bcb in bdrv_aligned_pwritev (child=0x608000002e20, req=0x7ffd14df7cd0, offset=131072, bytes=8192, align=1, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/io.c:2106
+ * #16 0x0000555556fb3d48 in bdrv_co_pwritev_part (child=0x608000002e20, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/io.c:2299
+ * #17 0x0000555556f7f9bd in blk_co_do_pwritev_part (blk=0x6190000d8e80, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/block-backend.c:1425
+ * #18 0x0000555556f80d1d in blk_aio_write_entry (opaque=0x6080016c0920) at ../block/block-backend.c:1620
+ * #19 0x000055555734236e in coroutine_trampoline (i0=87277888, i1=24864) at ../util/coroutine-ucontext.c:175
+ * #20 0x00007ffff70b4830 in __start_context () at /lib64/libc.so.6
+ * #21 0x00007fffef7e01c0 in  ()
+ * #22 0x0000000000000000 in  ()
+ *
+ * called by:
+ *   - block/file-posix.c|3641| <<raw_co_pwrite_zeroes>> return raw_do_pwrite_zeroes(bs, offset, bytes, flags, false);
+ *   - block/file-posix.c|4243| <<hdev_co_pwrite_zeroes>> return raw_do_pwrite_zeroes(bs, offset, bytes, flags, true);
+ */
 static int coroutine_fn
 raw_do_pwrite_zeroes(BlockDriverState *bs, int64_t offset, int64_t bytes,
                      BdrvRequestFlags flags, bool blkdev)
@@ -3634,6 +3717,9 @@ raw_do_pwrite_zeroes(BlockDriverState *bs, int64_t offset, int64_t bytes,
     return raw_thread_pool_submit(handler, &acb);
 }
 
+/*
+ * BlockDriver bdrv_file.block/file-posix.c|3923| <<global>> .bdrv_co_pwrite_zeroes = raw_co_pwrite_zeroes,
+ */
 static int coroutine_fn raw_co_pwrite_zeroes(
     BlockDriverState *bs, int64_t offset,
     int64_t bytes, BdrvRequestFlags flags)
@@ -3739,6 +3825,34 @@ static QemuOptsList raw_create_opts = {
     }
 };
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ *
+ * block/file-posix.c|3955| <<global>> BlockDriver bdrv_file.bdrv_check_perm = raw_check_perm,
+ * block/file-posix.c|4324| <<global>> BlockDriver bdrv_host_device.bdrv_check_perm = raw_check_perm,
+ */
 static int raw_check_perm(BlockDriverState *bs, uint64_t perm, uint64_t shared,
                           Error **errp)
 {
@@ -3771,6 +3885,13 @@ static int raw_check_perm(BlockDriverState *bs, uint64_t perm, uint64_t shared,
 
     /* Prepare permissions on old fd to avoid conflicts between old and new,
      * but keep everything locked that new will need. */
+    /*
+     * called by:
+     *   - block/file-posix.c|3820| <<raw_check_perm>> ret = raw_handle_perm_lock(bs, RAW_PL_PREPARE, perm, shared, errp);
+     *   - block/file-posix.c|3830| <<raw_check_perm>> raw_handle_perm_lock(bs, RAW_PL_ABORT, 0, 0, NULL);
+     *   - block/file-posix.c|3857| <<raw_set_perm>> raw_handle_perm_lock(bs, RAW_PL_COMMIT, perm, shared, NULL);
+     *   - block/file-posix.c|3873| <<raw_abort_perm_update>> raw_handle_perm_lock(bs, RAW_PL_ABORT, 0, 0, NULL);
+     */
     ret = raw_handle_perm_lock(bs, RAW_PL_PREPARE, perm, shared, errp);
     if (ret < 0) {
         goto fail;
diff --git a/block/io.c b/block/io.c
index 7217cf811..86aa006d6 100644
--- a/block/io.c
+++ b/block/io.c
@@ -582,6 +582,15 @@ void bdrv_drain_all(void)
  *
  * This function should be called when a tracked request is completing.
  */
+/*
+ * called by:
+ *   - block/io.c|1822| <<bdrv_co_preadv_part>> tracked_request_end(&req);
+ *   - block/io.c|2305| <<bdrv_co_pwritev_part>> tracked_request_end(&req);
+ *   - block/io.c|3187| <<bdrv_co_pdiscard>> tracked_request_end(&req);
+ *   - block/io.c|3445| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+ *   - block/io.c|3461| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+ *   - block/io.c|3661| <<bdrv_co_truncate>> tracked_request_end(&req);
+ */
 static void coroutine_fn tracked_request_end(BdrvTrackedRequest *req)
 {
     if (req->serialising) {
@@ -603,6 +612,15 @@ static void coroutine_fn tracked_request_end(BdrvTrackedRequest *req)
 /**
  * Add an active request to the tracked requests list
  */
+/*
+ * called by:
+ *   - block/io.c|1818| <<bdrv_co_preadv_part>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_READ);
+ *   - block/io.c|2279| <<bdrv_co_pwritev_part>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_WRITE);
+ *   - block/io.c|3118| <<bdrv_co_pdiscard>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_DISCARD);
+ *   - block/io.c|3432| <<bdrv_co_copy_range_internal>> tracked_request_begin(&req, src->bs, src_offset, bytes, BDRV_TRACKED_READ);
+ *   - block/io.c|3449| <<bdrv_co_copy_range_internal>> tracked_request_begin(&req, dst->bs, dst_offset, bytes, BDRV_TRACKED_WRITE);
+ *   - block/io.c|3584| <<bdrv_co_truncate>> tracked_request_begin(&req, bs, offset - new_bytes, new_bytes, BDRV_TRACKED_TRUNCATE);
+ */
 static void coroutine_fn tracked_request_begin(BdrvTrackedRequest *req,
                                                BlockDriverState *bs,
                                                int64_t offset,
@@ -625,6 +643,13 @@ static void coroutine_fn tracked_request_begin(BdrvTrackedRequest *req,
     qemu_co_queue_init(&req->wait_queue);
 
     qemu_mutex_lock(&bs->reqs_lock);
+    /*
+     * 在以下使用BlockDriverState->tracked_requests:
+     *   - block/io.c|628| <<tracked_request_begin>> QLIST_INSERT_HEAD(&bs->tracked_requests, req, list);
+     *   - block/io.c|654| <<bdrv_find_conflicting_request>> QLIST_FOREACH(req, &self->bs->tracked_requests, list) {
+     *   - block/io.c|724| <<bdrv_co_get_self_request>> QLIST_FOREACH(req, &bs->tracked_requests, list) {
+     *   - block/mirror.c|1130| <<mirror_run>> assert(QLIST_EMPTY(&bs->tracked_requests));
+     */
     QLIST_INSERT_HEAD(&bs->tracked_requests, req, list);
     qemu_mutex_unlock(&bs->reqs_lock);
 }
@@ -715,6 +740,10 @@ static void tracked_request_set_serialising(BdrvTrackedRequest *req,
  * Return the tracked request on @bs for the current coroutine, or
  * NULL if there is none.
  */
+/*
+ * called by:
+ *   - block/file-posix.c|3598| <<raw_do_pwrite_zeroes>> req = bdrv_co_get_self_request(bs);
+ */
 BdrvTrackedRequest *coroutine_fn bdrv_co_get_self_request(BlockDriverState *bs)
 {
     BdrvTrackedRequest *req;
@@ -722,6 +751,13 @@ BdrvTrackedRequest *coroutine_fn bdrv_co_get_self_request(BlockDriverState *bs)
     IO_CODE();
 
     QLIST_FOREACH(req, &bs->tracked_requests, list) {
+        /*
+	 * 在以下使用BlockDriverState->tracked_requests:
+	 *   - block/io.c|628| <<tracked_request_begin>> QLIST_INSERT_HEAD(&bs->tracked_requests, req, list);
+	 *   - block/io.c|654| <<bdrv_find_conflicting_request>> QLIST_FOREACH(req, &self->bs->tracked_requests, list) {
+	 *   - block/io.c|724| <<bdrv_co_get_self_request>> QLIST_FOREACH(req, &bs->tracked_requests, list) {
+	 *   - block/mirror.c|1130| <<mirror_run>> assert(QLIST_EMPTY(&bs->tracked_requests));
+	 */
         if (req->co == self) {
             return req;
         }
@@ -1828,6 +1864,11 @@ fail:
     return ret;
 }
 
+/*
+ * called by:
+ *   - block/io.c|1301| <<bdrv_co_do_copy_on_readv>> ret = bdrv_co_do_pwrite_zeroes(bs, align_offset, pnum, BDRV_REQ_WRITE_UNCHANGED);
+ *   - block/io.c|2129| <<bdrv_aligned_pwritev>> ret = bdrv_co_do_pwrite_zeroes(bs, offset, bytes, flags);
+ */
 static int coroutine_fn GRAPH_RDLOCK
 bdrv_co_do_pwrite_zeroes(BlockDriverState *bs, int64_t offset, int64_t bytes,
                          BdrvRequestFlags flags)
@@ -2276,6 +2317,15 @@ int coroutine_fn bdrv_co_pwritev_part(BdrvChild *child,
     }
 
     bdrv_inc_in_flight(bs);
+    /*
+     * called by:
+     *   - block/io.c|1818| <<bdrv_co_preadv_part>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_READ);
+     *   - block/io.c|2279| <<bdrv_co_pwritev_part>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_WRITE);
+     *   - block/io.c|3118| <<bdrv_co_pdiscard>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_DISCARD);
+     *   - block/io.c|3432| <<bdrv_co_copy_range_internal>> tracked_request_begin(&req, src->bs, src_offset, bytes, BDRV_TRACKED_READ);
+     *   - block/io.c|3449| <<bdrv_co_copy_range_internal>> tracked_request_begin(&req, dst->bs, dst_offset, bytes, BDRV_TRACKED_WRITE);
+     *   - block/io.c|3584| <<bdrv_co_truncate>> tracked_request_begin(&req, bs, offset - new_bytes, new_bytes, BDRV_TRACKED_TRUNCATE);
+     */
     tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_WRITE);
 
     if (flags & BDRV_REQ_ZERO_WRITE) {
@@ -3442,6 +3492,15 @@ static int coroutine_fn GRAPH_RDLOCK bdrv_co_copy_range_internal(
                                                     bytes,
                                                     read_flags, write_flags);
 
+        /*
+	 * called by:
+	 *   - block/io.c|1822| <<bdrv_co_preadv_part>> tracked_request_end(&req);
+	 *   - block/io.c|2305| <<bdrv_co_pwritev_part>> tracked_request_end(&req);
+	 *   - block/io.c|3187| <<bdrv_co_pdiscard>> tracked_request_end(&req);
+	 *   - block/io.c|3445| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+	 *   - block/io.c|3461| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+	 *   - block/io.c|3661| <<bdrv_co_truncate>> tracked_request_end(&req);
+	 */
         tracked_request_end(&req);
         bdrv_dec_in_flight(src->bs);
     } else {
@@ -3458,6 +3517,15 @@ static int coroutine_fn GRAPH_RDLOCK bdrv_co_copy_range_internal(
                                                       read_flags, write_flags);
         }
         bdrv_co_write_req_finish(dst, dst_offset, bytes, &req, ret);
+        /*
+	 * called by:
+	 *   - block/io.c|1822| <<bdrv_co_preadv_part>> tracked_request_end(&req);
+	 *   - block/io.c|2305| <<bdrv_co_pwritev_part>> tracked_request_end(&req);
+	 *   - block/io.c|3187| <<bdrv_co_pdiscard>> tracked_request_end(&req);
+	 *   - block/io.c|3445| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+	 *   - block/io.c|3461| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+	 *   - block/io.c|3661| <<bdrv_co_truncate>> tracked_request_end(&req);
+	 */
         tracked_request_end(&req);
         bdrv_dec_in_flight(dst->bs);
     }
@@ -3581,6 +3649,15 @@ int coroutine_fn bdrv_co_truncate(BdrvChild *child, int64_t offset, bool exact,
     }
 
     bdrv_inc_in_flight(bs);
+    /*
+     * called by:
+     *   - block/io.c|1818| <<bdrv_co_preadv_part>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_READ);
+     *   - block/io.c|2279| <<bdrv_co_pwritev_part>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_WRITE);
+     *   - block/io.c|3118| <<bdrv_co_pdiscard>> tracked_request_begin(&req, bs, offset, bytes, BDRV_TRACKED_DISCARD);
+     *   - block/io.c|3432| <<bdrv_co_copy_range_internal>> tracked_request_begin(&req, src->bs, src_offset, bytes, BDRV_TRACKED_READ);
+     *   - block/io.c|3449| <<bdrv_co_copy_range_internal>> tracked_request_begin(&req, dst->bs, dst_offset, bytes, BDRV_TRACKED_WRITE);
+     *   - block/io.c|3584| <<bdrv_co_truncate>> tracked_request_begin(&req, bs, offset - new_bytes, new_bytes, BDRV_TRACKED_TRUNCATE);
+     */
     tracked_request_begin(&req, bs, offset - new_bytes, new_bytes,
                           BDRV_TRACKED_TRUNCATE);
 
@@ -3658,6 +3735,15 @@ int coroutine_fn bdrv_co_truncate(BdrvChild *child, int64_t offset, bool exact,
     bdrv_co_write_req_finish(child, offset - new_bytes, new_bytes, &req, 0);
 
 out:
+    /*
+     * called by:
+     *   - block/io.c|1822| <<bdrv_co_preadv_part>> tracked_request_end(&req);
+     *   - block/io.c|2305| <<bdrv_co_pwritev_part>> tracked_request_end(&req);
+     *   - block/io.c|3187| <<bdrv_co_pdiscard>> tracked_request_end(&req);
+     *   - block/io.c|3445| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+     *   - block/io.c|3461| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+     *   - block/io.c|3661| <<bdrv_co_truncate>> tracked_request_end(&req);
+     */
     tracked_request_end(&req);
     bdrv_dec_in_flight(bs);
 
diff --git a/block/mirror.c b/block/mirror.c
index 1bdce3b65..0870ead8c 100644
--- a/block/mirror.c
+++ b/block/mirror.c
@@ -1701,6 +1701,44 @@ static BlockDriver bdrv_mirror_top = {
     .filtered_child_is_backing  = true,
 };
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ *
+ * called by:
+ *   - block/mirror.c|2028| <<mirror_start>> mirror_start_job(job_id, bs, creation_flags, target, replaces,
+ *                                                       speed, granularity, buf_size, backing_mode, zero_target,
+ *                                                       on_source_error, on_target_error, unmap, NULL, NULL,
+ *                                                       &mirror_job_driver, is_none_mode, base, false,
+ *                                                       filter_node_name, true, copy_mode, errp);
+ *   - block/mirror.c|2055| <<commit_active_start>> job = mirror_start_job(job_id, bs, creation_flags, base, NULL, speed, 0, 0,
+ *                                                       MIRROR_LEAVE_BACKING_CHAIN, false,
+ *                                                       on_error, on_error, true, cb, opaque,
+ *                                                       &commit_active_job_driver, false, base, auto_complete,
+ *                                                       filter_node_name, false, MIRROR_COPY_MODE_BACKGROUND,
+ *                                                       errp);
+ */
 static BlockJob *mirror_start_job(
                              const char *job_id, BlockDriverState *bs,
                              int creation_flags, BlockDriverState *target,
@@ -1988,6 +2026,16 @@ fail:
     assert(mirror_top_bs->backing->bs == bs);
     bdrv_child_refresh_perms(mirror_top_bs, mirror_top_bs->backing,
                              &error_abort);
+    /*
+     * called by:
+     *   - block.c|5709| <<bdrv_insert_node>> ret = bdrv_replace_node(bs, new_node_bs, errp);
+     *   - block/commit.c|106| <<commit_abort>> bdrv_replace_node(s->commit_top_bs, commit_top_backing_bs, &error_abort);
+     *   - block/commit.c|442| <<commit_start>> bdrv_replace_node(commit_top_bs, top, &error_abort);
+     *   - block/mirror.c|766| <<mirror_exit_common>> bdrv_replace_node(to_replace, target_bs, &local_err);
+     *   - block/mirror.c|794| <<mirror_exit_common>> bdrv_replace_node(mirror_top_bs, mirror_top_bs->backing->bs, &error_abort);
+     *   - block/mirror.c|2029| <<mirror_start_job>> bdrv_replace_node(mirror_top_bs, bs, &error_abort);
+     *   - blockdev.c|1546| <<external_snapshot_abort>> bdrv_replace_node(state->new_bs, state->old_bs, &error_abort);
+     */
     bdrv_replace_node(mirror_top_bs, bs, &error_abort);
     bdrv_graph_wrunlock();
     bdrv_drained_end(bs);
@@ -2032,6 +2080,39 @@ void mirror_start(const char *job_id, BlockDriverState *bs,
                      filter_node_name, true, copy_mode, errp);
 }
 
+/*
+ * [0] qemu_lock_fd  
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh     
+ * [0] aio_bh_poll
+ * [0] aio_dispatch           
+ * [0] aio_ctx_dispatch       
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll      
+ * [0] os_host_main_loop_wait 
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ *
+ * called by:
+ *   - block/replication.c|711| <<replication_stop>> s->commit_job = commit_active_start(NULL, bs->file->bs, s->secondary_disk->bs,
+ *                                    JOB_INTERNAL, 0, BLOCKDEV_ON_ERROR_REPORT, NULL, replication_done, bs, true, errp);
+ *   - blockdev.c|2622| <<qmp_block_commit>> commit_active_start(job_id, top_bs, base_bs, job_flags, speed, on_error,
+ *                                    filter_node_name, NULL, NULL, false, &local_err);
+ *   - qemu-img.c|1080| <<img_commit>> commit_active_start("commit", bs, base_bs, JOB_DEFAULT, rate_limit,
+ *                                    BLOCKDEV_ON_ERROR_REPORT, NULL, common_block_job_cb, &cbi, false, &local_err);
+ */
 BlockJob *commit_active_start(const char *job_id, BlockDriverState *bs,
                               BlockDriverState *base, int creation_flags,
                               int64_t speed, BlockdevOnError on_error,
@@ -2052,6 +2133,20 @@ BlockJob *commit_active_start(const char *job_id, BlockDriverState *bs,
         }
     }
 
+    /*
+     * called by:
+     *   - block/mirror.c|2028| <<mirror_start>> mirror_start_job(job_id, bs, creation_flags, target, replaces,
+     *                                                       speed, granularity, buf_size, backing_mode, zero_target,
+     *                                                       on_source_error, on_target_error, unmap, NULL, NULL,
+     *                                                       &mirror_job_driver, is_none_mode, base, false,
+     *                                                       filter_node_name, true, copy_mode, errp);
+     *   - block/mirror.c|2055| <<commit_active_start>> job = mirror_start_job(job_id, bs, creation_flags, base, NULL, speed, 0, 0,
+     *                                                       MIRROR_LEAVE_BACKING_CHAIN, false,
+     *                                                       on_error, on_error, true, cb, opaque,
+     *                                                       &commit_active_job_driver, false, base, auto_complete,
+     *                                                       filter_node_name, false, MIRROR_COPY_MODE_BACKGROUND,
+     *                                                       errp);
+     */
     job = mirror_start_job(
                      job_id, bs, creation_flags, base, NULL, speed, 0, 0,
                      MIRROR_LEAVE_BACKING_CHAIN, false,
diff --git a/block/qcow2-threads.c b/block/qcow2-threads.c
index d6071a1ea..03c3e07a5 100644
--- a/block/qcow2-threads.c
+++ b/block/qcow2-threads.c
@@ -38,6 +38,11 @@
 #include "block/thread-pool.h"
 #include "crypto.h"
 
+/*
+ * called by:
+ *   - block/qcow2-threads.c|350| <<qcow2_co_do_compress>> qcow2_co_process(bs, qcow2_compress_pool_func, &arg);
+ *   - block/qcow2-threads.c|478| <<qcow2_co_encdec>> return len == 0 ? 0 : qcow2_co_process(bs, qcow2_encdec_pool_func, &arg);
+ */
 static int coroutine_fn
 qcow2_co_process(BlockDriverState *bs, ThreadPoolFunc *func, void *arg)
 {
diff --git a/block/qcow2.c b/block/qcow2.c
index 956128b40..73b00efb4 100644
--- a/block/qcow2.c
+++ b/block/qcow2.c
@@ -2272,6 +2272,12 @@ typedef struct Qcow2AioTask {
 } Qcow2AioTask;
 
 static coroutine_fn int qcow2_co_preadv_task_entry(AioTask *task);
+/*
+ * called by:
+ *   - block/qcow2.c|2410| <<qcow2_co_preadv_part>> ret = qcow2_add_task(bs, aio, qcow2_co_preadv_task_entry, type, host_offset, offset, cur_bytes, qiov, qiov_offset, NULL);
+ *   - block/qcow2.c|2709| <<qcow2_co_pwritev_part>> ret = qcow2_add_task(bs, aio, qcow2_co_pwritev_task_entry, 0, host_offset, offset, cur_bytes, qiov, qiov_offset, l2meta);
+ *   - block/qcow2.c|4786| <<qcow2_co_pwritev_compressed_part>> ret = qcow2_add_task(bs, aio, qcow2_co_pwritev_compressed_task_entry, 0, 0, offset, chunk_size, qiov, qiov_offset, NULL);
+ */
 static coroutine_fn int qcow2_add_task(BlockDriverState *bs,
                                        AioTaskPool *pool,
                                        AioTaskFunc func,
diff --git a/blockdev.c b/blockdev.c
index 057601dcf..cfc30eaec 100644
--- a/blockdev.c
+++ b/blockdev.c
@@ -475,6 +475,10 @@ static OnOffAuto account_get_opt(QemuOpts *opts, const char *name)
     return ON_OFF_AUTO_OFF;
 }
 
+/*
+ * called by:
+ *   - blockdev.c|1006| <<drive_new>> blk = blockdev_init(filename, bs_opts, errp);
+ */
 /* Takes the ownership of bs_opts */
 static BlockBackend *blockdev_init(const char *file, QDict *bs_opts,
                                    Error **errp)
@@ -772,6 +776,12 @@ QemuOptsList qemu_legacy_drive_opts = {
     },
 };
 
+/*
+ * called by:
+ *   - block/monitor/block-hmp-cmds.c|110| <<hmp_drive_add>> dinfo = drive_new(opts, mc->block_default_type, &err);
+ *   - system/vl.c|648| <<drive_init_func>> return drive_new(opts, *block_default_type, errp) == NULL;
+ *   - system/vl.c|674| <<default_drive>> dinfo = drive_new(opts, type, &error_abort);
+ */
 DriveInfo *drive_new(QemuOpts *all_opts, BlockInterfaceType block_default_type,
                      Error **errp)
 {
@@ -1036,6 +1046,15 @@ fail:
     return dinfo;
 }
 
+/*
+ * called by:
+ *   - blockdev.c|1137| <<qmp_blockdev_snapshot_delete_internal_sync>> bs = qmp_get_root_bs(device, errp);
+ *   - blockdev.c|1221| <<internal_snapshot_action>> bs = qmp_get_root_bs(device, errp);
+ *   - blockdev.c|2521| <<qmp_block_commit>> bs = qmp_get_root_bs(device, &local_err);
+ *   - blockdev.c|2971| <<qmp_drive_mirror>> bs = qmp_get_root_bs(arg->device, errp);
+ *   - blockdev.c|3135| <<qmp_blockdev_mirror>> bs = qmp_get_root_bs(device, errp);
+ *   - blockdev.c|3333| <<qmp_change_backing_file>> bs = qmp_get_root_bs(device, errp);
+ */
 static BlockDriverState *qmp_get_root_bs(const char *name, Error **errp)
 {
     BlockDriverState *bs;
@@ -2421,6 +2440,56 @@ out_rdlock:
     bdrv_graph_rdunlock_main_loop();
 }
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ *
+ * (QEMU) blockdev-snapshot-sync node-name=drive01 snapshot-file=/tmp/overlay01.qcow2 snapshot-node-name=over01
+ * (QEMU) block-commit device=over01 job-id=jobA
+ * (QEMU) block-job-complete device=jobA
+ *
+ * (gdb) bt
+ * #0  qmp_block_commit (job_id=0x555557400e10 "jobA", device=0x555557400d70 "over01",
+ *                base_node=0x0, base=0x0, top_node=0x0, top=0x0, backing_file=0x0,
+ *                has_backing_mask_protocol=false, backing_mask_protocol=false,
+ *                has_speed=false, speed=0, has_on_error=false, on_error=BLOCKDEV_ON_ERROR_REPORT,
+ *                filter_node_name=0x0, has_auto_finalize=false, auto_finalize=false,
+ *                has_auto_dismiss=false, auto_dismiss=false, errp=0x7fffffffd688) at ../blockdev.c:2438
+ * #1  0x0000555556010f4a in qmp_marshal_block_commit (args=0x7fffd8005c90, ret=0x7ffff7f9eda8, errp=0x7ffff7f9eda0) at qapi/qapi-commands-block-core.c:408
+ * #2  0x00005555560a4413 in do_qmp_dispatch_bh (opaque=0x7ffff7f9ee40) at ../qapi/qmp-dispatch.c:128
+ * #3  0x00005555560d3305 in aio_bh_call (bh=0x555557c93d70) at ../util/async.c:171
+ * #4  0x00005555560d3453 in aio_bh_poll (ctx=0x555557169480) at ../util/async.c:218
+ * #5  0x00005555560b2a26 in aio_dispatch (ctx=0x555557169480) at ../util/aio-posix.c:423
+ * #6  0x00005555560d3922 in aio_ctx_dispatch (source=0x555557169480, callback=0x0, user_data=0x0) at ../util/async.c:360
+ * #7  0x00007ffff6c53aed in g_main_context_dispatch () at /lib/../lib64/libglib-2.0.so.0
+ * #8  0x00005555560d4ff3 in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #9  0x00005555560d5081 in os_host_main_loop_wait (timeout=0) at ../util/main-loop.c:310
+ * #10 0x00005555560d51b0 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #11 0x0000555555bc3abf in qemu_main_loop () at ../system/runstate.c:783
+ * #12 0x0000555555e8a1bb in qemu_default_main () at ../system/main.c:37
+ * #13 0x0000555555e8a1f8 in main (argc=32, argv=0x7fffffffdaf8) at ../system/main.c:48
+ */
 void qmp_block_commit(const char *job_id, const char *device,
                       const char *base_node,
                       const char *base,
@@ -2436,6 +2505,13 @@ void qmp_block_commit(const char *job_id, const char *device,
                       bool has_auto_dismiss, bool auto_dismiss,
                       Error **errp)
 {
+    /* #0  qmp_block_commit (job_id=0x555557400e10 "jobA", device=0x555557400d70 "over01",
+     *                base_node=0x0, base=0x0, top_node=0x0, top=0x0, backing_file=0x0,
+     *                has_backing_mask_protocol=false, backing_mask_protocol=false,
+     *                has_speed=false, speed=0, has_on_error=false, on_error=BLOCKDEV_ON_ERROR_REPORT,
+     *                filter_node_name=0x0, has_auto_finalize=false, auto_finalize=false,
+     *                has_auto_dismiss=false, auto_dismiss=false, errp=0x7fffffffd688) at ../blockdev.c:2438
+     */
     BlockDriverState *bs;
     BlockDriverState *iter;
     BlockDriverState *base_bs, *top_bs;
@@ -2447,6 +2523,9 @@ void qmp_block_commit(const char *job_id, const char *device,
     /* TODO We'll eventually have to take a writer lock in this function */
     GRAPH_RDLOCK_GUARD_MAINLOOP();
 
+    /*
+     * 默认false
+     */
     if (!has_speed) {
         speed = 0;
     }
@@ -2468,6 +2547,15 @@ void qmp_block_commit(const char *job_id, const char *device,
      *  live commit feature versions; for this to work, we must make sure to
      *  perform the device lookup before any generic errors that may occur in a
      *  scenario in which all optional arguments are omitted. */
+    /*
+     * called by:
+     *   - blockdev.c|1137| <<qmp_blockdev_snapshot_delete_internal_sync>> bs = qmp_get_root_bs(device, errp);
+     *   - blockdev.c|1221| <<internal_snapshot_action>> bs = qmp_get_root_bs(device, errp);
+     *   - blockdev.c|2521| <<qmp_block_commit>> bs = qmp_get_root_bs(device, &local_err);
+     *   - blockdev.c|2971| <<qmp_drive_mirror>> bs = qmp_get_root_bs(arg->device, errp);
+     *   - blockdev.c|3135| <<qmp_blockdev_mirror>> bs = qmp_get_root_bs(device, errp);
+     *   - blockdev.c|3333| <<qmp_change_backing_file>> bs = qmp_get_root_bs(device, errp);
+     */
     bs = qmp_get_root_bs(device, &local_err);
     if (!bs) {
         bs = bdrv_lookup_bs(device, device, NULL);
@@ -2481,6 +2569,13 @@ void qmp_block_commit(const char *job_id, const char *device,
         return;
     }
 
+    /*
+     * 7658 AioContext *bdrv_get_aio_context(BlockDriverState *bs)
+     * 7659 {
+     * 7660     IO_CODE();
+     * 7661     return bs ? bs->aio_context : qemu_get_aio_context();
+     * 7662 }
+     */
     aio_context = bdrv_get_aio_context(bs);
 
     if (bdrv_op_is_blocked(bs, BLOCK_OP_TYPE_COMMIT_SOURCE, errp)) {
@@ -2490,6 +2585,9 @@ void qmp_block_commit(const char *job_id, const char *device,
     /* default top_bs is the active layer */
     top_bs = bs;
 
+    /*
+     * 猜测一般top_node和top都是0
+     */
     if (top_node && top) {
         error_setg(errp, "'top-node' and 'top' are mutually exclusive");
         return;
@@ -2520,6 +2618,9 @@ void qmp_block_commit(const char *job_id, const char *device,
 
     assert(bdrv_get_aio_context(top_bs) == aio_context);
 
+    /*
+     * base_node和base一般也都是0
+     */
     if (base_node && base) {
         error_setg(errp, "'base-node' and 'base' are mutually exclusive");
         return;
@@ -2540,6 +2641,14 @@ void qmp_block_commit(const char *job_id, const char *device,
             return;
         }
     } else {
+        /*
+	 * 因为base_node和base一般也都是0
+	 * 所以这个else很重要
+	 *
+	 * called by:
+	 *   - blockdev.c|2622| <<qmp_block_commit>> base_bs = bdrv_find_base(top_bs);
+	 *   - blockdev.c|3380| <<qmp_change_backing_file>> if (bdrv_find_base(image_bs) == image_bs) {
+	 */
         base_bs = bdrv_find_base(top_bs);
         if (base_bs == NULL) {
             error_setg(errp, "There is no backimg image");
@@ -2594,6 +2703,15 @@ void qmp_block_commit(const char *job_id, const char *device,
              */
             job_id = bdrv_get_device_name(bs);
         }
+        /*
+	 * called by:
+	 *   - block/replication.c|711| <<replication_stop>> s->commit_job = commit_active_start(NULL, bs->file->bs, s->secondary_disk->bs,
+	 *                                    JOB_INTERNAL, 0, BLOCKDEV_ON_ERROR_REPORT, NULL, replication_done, bs, true, errp);
+	 *   - blockdev.c|2622| <<qmp_block_commit>> commit_active_start(job_id, top_bs, base_bs, job_flags, speed, on_error,
+	 *                                    filter_node_name, NULL, NULL, false, &local_err);
+	 *   - qemu-img.c|1080| <<img_commit>> commit_active_start("commit", bs, base_bs, JOB_DEFAULT, rate_limit,
+	 *                                    BLOCKDEV_ON_ERROR_REPORT, NULL, common_block_job_cb, &cbi, false, &local_err);
+	 */
         commit_active_start(job_id, top_bs, base_bs, job_flags, speed, on_error,
                             filter_node_name, NULL, NULL, false, &local_err);
     } else {
@@ -3289,6 +3407,11 @@ void qmp_change_backing_file(const char *device,
         goto out_rdlock;
     }
 
+    /*
+     * called by:
+     *   - blockdev.c|2622| <<qmp_block_commit>> base_bs = bdrv_find_base(top_bs);
+     *   - blockdev.c|3380| <<qmp_change_backing_file>> if (bdrv_find_base(image_bs) == image_bs) {
+     */
     if (bdrv_find_base(image_bs) == image_bs) {
         error_setg(errp, "not allowing backing file change on an image "
                          "without a backing file");
diff --git a/hw/block/virtio-blk.c b/hw/block/virtio-blk.c
index bb86e65f6..ac5396ac6 100644
--- a/hw/block/virtio-blk.c
+++ b/hw/block/virtio-blk.c
@@ -100,6 +100,20 @@ static int virtio_blk_handle_rw_error(VirtIOBlockReq *req, int error,
     return action != BLOCK_ERROR_ACTION_IGNORE;
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_blk_rw_complete (opaque=0x6120019435c0, ret=0) at ../hw/block/virtio-blk.c:105
+ * #1  0x0000555556f805a4 in blk_aio_complete (acb=0x608000f026a0) at ../block/block-backend.c:1555
+ * #2  0x0000555556f80d66 in blk_aio_write_entry (opaque=0x608000f026a0) at ../block/block-backend.c:1622
+ * #3  0x000055555734236e in coroutine_trampoline (i0=106945984, i1=24864) at ../util/coroutine-ucontext.c:175
+ * #4  0x00007ffff70b4830 in __start_context () at /lib64/libc.so.6
+ * #5  0x00007ffde38341c0 in  ()
+ * #6  0x0000000000000000 in  ()
+ *
+ * 在以下使用virtio_blk_rw_complete():
+ *   - hw/block/virtio-blk.c|434| <<submit_requests>> blk_aio_pwritev(blk, sector_num << BDRV_SECTOR_BITS, qiov, flags, virtio_blk_rw_complete, mrb->reqs[start]);
+ *   - hw/block/virtio-blk.c|438| <<submit_requests>> blk_aio_preadv(blk, sector_num << BDRV_SECTOR_BITS, qiov, flags, virtio_blk_rw_complete, mrb->reqs[start]);
+ */
 static void virtio_blk_rw_complete(void *opaque, int ret)
 {
     VirtIOBlockReq *next = opaque;
@@ -353,6 +367,52 @@ static void virtio_blk_handle_scsi(VirtIOBlockReq *req)
     }
 }
 
+/*
+ * (qemu) qemu-system-x86_64: ../block/file-posix.c:3599: raw_do_pwrite_zeroes: Assertion `req' failed.
+ * run.sh: line 11: 74885 Aborted                 (core dumped) /home/opc/qemu/build/qemu-system-x86_64 -hda ol89.qcow2 -m 8G -smp 4 -vnc :4 -enable-kvm -cpu host -net nic -net user,hostfwd=tcp::5028-:22 -monitor stdio -name debug-threads=on -object iothread,id=t0 -object iothread,id=t1 -object iothread,id=t2 -object iothread,id=t3 -blockdev node-name=file01,driver=file,filename=test.qcow2,cache.direct=on,cache.no-flush=off -blockdev node-name=drive01,driver=qcow2,cache.direct=on,cache.no-flush=off,file=file01 -device '{"driver": "virtio-blk-pci", "drive": "drive01", "iothread-vq-mapping": [{"iothread": "t0", "vqs": [0]}, {"iothread": "t1", "vqs": [1]}, {"iothread": "t2", "vqs": [2]}, {"iothread": "t3", "vqs": [3]}]}'
+ *
+ * 在以下调用tracked_request_end():
+ *   - block/io.c|1822| <<bdrv_co_preadv_part>> tracked_request_end(&req);
+ *   - block/io.c|2305| <<bdrv_co_pwritev_part>> tracked_request_end(&req);
+ *   - block/io.c|3187| <<bdrv_co_pdiscard>> tracked_request_end(&req);
+ *   - block/io.c|3445| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+ *   - block/io.c|3461| <<bdrv_co_copy_range_internal>> tracked_request_end(&req);
+ *   - block/io.c|3661| <<bdrv_co_truncate>> tracked_request_end(&req);
+ *
+ * (gdb) bt
+ * #0  bdrv_co_get_self_request (bs=0x629000005200) at ../block/io.c:720
+ * #1  0x00005555570f6929 in raw_do_pwrite_zeroes (bs=0x629000005200, offset=458752, bytes=65536, flags=BDRV_REQ_NO_FALLBACK, blkdev=false) at ../block/file-posix.c:3598
+ * #2  0x00005555570f6f1d in raw_co_pwrite_zeroes (bs=0x629000005200, offset=458752, bytes=65536, flags=BDRV_REQ_NO_FALLBACK) at ../block/file-posix.c:3641
+ * #3  0x0000555556fb193d in bdrv_co_do_pwrite_zeroes (bs=0x629000005200, offset=458752, bytes=65536, flags=258) at ../block/io.c:1901
+ * #4  0x0000555556fb2b50 in bdrv_aligned_pwritev (child=0x608000002920, req=0x7ffd14df80d0, offset=458752, bytes=65536, align=512, qiov=0x0, qiov_offset=0, flags=258) at ../block/io.c:2100
+ * #5  0x0000555556fb33a7 in bdrv_co_do_zero_pwritev (child=0x608000002920, offset=458752, bytes=65536, flags=258, req=0x7ffd14df80d0) at ../block/io.c:2183
+ * #6  0x0000555556fb3c43 in bdrv_co_pwritev_part (child=0x608000002920, offset=458752, bytes=65536, qiov=0x0, qiov_offset=0, flags=258) at ../block/io.c:2283
+ * #7  0x0000555556fb362c in bdrv_co_pwritev (child=0x608000002920, offset=458752, bytes=65536, qiov=0x0, flags=258) at ../block/io.c:2216
+ * #8  0x0000555556fb3f19 in bdrv_co_pwrite_zeroes (child=0x608000002920, offset=458752, bytes=65536, flags=BDRV_REQ_NO_FALLBACK) at ../block/io.c:2322
+ * #9  0x0000555556ffae77 in handle_alloc_space (bs=0x62900000a200, l2meta=0x60b0011bb730) at ../block/qcow2.c:2557
+ * #10 0x0000555556ffb3de in qcow2_co_pwritev_task (bs=0x62900000a200, host_offset=458752, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, l2meta=0x60b0011bb730) at ../block/qcow2.c:2609
+ * #11 0x0000555556ffb828 in qcow2_co_pwritev_task_entry (task=0x7ffd14c07320) at ../block/qcow2.c:2657
+ * #12 0x0000555556ff97cd in qcow2_add_task (bs=0x62900000a200, pool=0x0, func=0x555556ffb653 <qcow2_co_pwritev_task_entry>, subcluster_type=QCOW2_SUBCLUSTER_UNALLOCATED_PLAIN,
+ *                           host_offset=458752, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, l2meta=0x60b0011bb730) at ../block/qcow2.c:2307
+ * #13 0x0000555556ffbd90 in qcow2_co_pwritev_part (bs=0x62900000a200, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/qcow2.c:2709
+ * #14 0x0000555556face86 in bdrv_driver_pwritev (bs=0x62900000a200, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/io.c:1070
+ * #15 0x0000555556fb2bcb in bdrv_aligned_pwritev (child=0x608000002e20, req=0x7ffd14df7cd0, offset=131072, bytes=8192, align=1, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF)
+ *                           at ../block/io.c:2106
+ * #16 0x0000555556fb3d48 in bdrv_co_pwritev_part (child=0x608000002e20, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/io.c:2299
+ * #17 0x0000555556f7f9bd in blk_co_do_pwritev_part (blk=0x6190000d8e80, offset=131072, bytes=8192, qiov=0x61200532c758, qiov_offset=0, flags=BDRV_REQ_REGISTERED_BUF) at ../block/block-backend.c:1425
+ * #18 0x0000555556f80d1d in blk_aio_write_entry (opaque=0x6080016c0920) at ../block/block-backend.c:1620
+ * #19 0x000055555734236e in coroutine_trampoline (i0=87277888, i1=24864) at ../util/coroutine-ucontext.c:175
+ * #20 0x00007ffff70b4830 in __start_context () at /lib64/libc.so.6
+ * #21 0x00007fffef7e01c0 in  ()
+ * #22 0x0000000000000000 in  ()
+ */
+
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|462| <<virtio_blk_submit_multireq>> submit_requests(s, mrb, 0, 1, -1);
+ *   - hw/block/virtio-blk.c|486| <<virtio_blk_submit_multireq>> submit_requests(s, mrb, start, num_reqs, niov);
+ *   - hw/block/virtio-blk.c|502| <<virtio_blk_submit_multireq>> submit_requests(s, mrb, start, num_reqs, niov);
+ */
 static inline void submit_requests(VirtIOBlock *s, MultiReqBuffer *mrb,
                                    int start, int num_reqs, int niov)
 {
@@ -424,6 +484,19 @@ static int multireq_compare(const void *a, const void *b)
     }
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|517| <<virtio_blk_handle_flush>> virtio_blk_submit_multireq(s, mrb);
+ *   - hw/block/virtio-blk.c|1034| <<virtio_blk_handle_request>> virtio_blk_submit_multireq(s, mrb);
+ *   - hw/block/virtio-blk.c|1166| <<virtio_blk_handle_vq>> virtio_blk_submit_multireq(s, &mrb);
+ *   - hw/block/virtio-blk.c|1214| <<virtio_blk_dma_restart_bh>> virtio_blk_submit_multireq(s, &mrb);
+ *
+ * typedef struct MultiReqBuffer {
+ *     VirtIOBlockReq *reqs[VIRTIO_BLK_MAX_MERGE_REQS];
+ *     unsigned int num_reqs;
+ *     bool is_write;
+ * } MultiReqBuffer;
+ */
 static void virtio_blk_submit_multireq(VirtIOBlock *s, MultiReqBuffer *mrb)
 {
     int i = 0, start = 0, num_reqs = 0, niov = 0, nb_sectors = 0;
@@ -928,6 +1001,11 @@ out:
     return err_status;
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1153| <<virtio_blk_handle_vq>> if (virtio_blk_handle_request(req, &mrb)) {
+ *   - hw/block/virtio-blk.c|1198| <<virtio_blk_dma_restart_bh>> if (virtio_blk_handle_request(req, &mrb)) {
+ */
 static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
 {
     uint32_t type;
@@ -1108,6 +1186,10 @@ static int virtio_blk_handle_request(VirtIOBlockReq *req, MultiReqBuffer *mrb)
     return 0;
 }
 
+/*
+ * 在以下使用virtio_blk_handle_vq():
+ *   - hw/block/virtio-blk.c|1186| <<virtio_blk_handle_output>> virtio_blk_handle_vq(s, vq);
+ */
 void virtio_blk_handle_vq(VirtIOBlock *s, VirtQueue *vq)
 {
     VirtIOBlockReq *req;
@@ -1134,17 +1216,56 @@ void virtio_blk_handle_vq(VirtIOBlock *s, VirtQueue *vq)
         }
     } while (!virtio_queue_empty(vq));
 
+    /*
+     * typedef struct MultiReqBuffer {
+     *     VirtIOBlockReq *reqs[VIRTIO_BLK_MAX_MERGE_REQS];
+     *     unsigned int num_reqs;
+     *     bool is_write;
+     * } MultiReqBuffer;
+     */
     if (mrb.num_reqs) {
+        /*
+	 * called by:
+	 *   - hw/block/virtio-blk.c|517| <<virtio_blk_handle_flush>> virtio_blk_submit_multireq(s, mrb);
+	 *   - hw/block/virtio-blk.c|1034| <<virtio_blk_handle_request>> virtio_blk_submit_multireq(s, mrb);
+	 *   - hw/block/virtio-blk.c|1166| <<virtio_blk_handle_vq>> virtio_blk_submit_multireq(s, &mrb);
+	 *   - hw/block/virtio-blk.c|1214| <<virtio_blk_dma_restart_bh>> virtio_blk_submit_multireq(s, &mrb);
+	 */
         virtio_blk_submit_multireq(s, &mrb);
     }
 
     defer_call_end();
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_blk_handle_output (vdev=0x62d00001c830, vq=0x7ffddbda4800) at ../hw/block/virtio-blk.c:1010
+ * #1  0x0000555556ca8da7 in virtio_queue_notify_vq (vq=0x7ffddbda4800) at ../hw/virtio/virtio.c:2277
+ * #2  0x0000555556cb03f4 in virtio_queue_host_notifier_read (n=0x7ffddbda4874) at ../hw/virtio/virtio.c:3641
+ * #3  0x00005555572f3b2f in aio_dispatch_handler (ctx=0x613000121880, node=0x60d000036c20) at ../util/aio-posix.c:372
+ * #4  0x00005555572f3d80 in aio_dispatch_ready_handlers (ctx=0x613000121880, ready_list=0x7ffff1947aa0) at ../util/aio-posix.c:401
+ * #5  0x00005555572f5845 in aio_poll (ctx=0x613000121880, blocking=true) at ../util/aio-posix.c:723
+ * #6  0x0000555556ef1830 in iothread_run (opaque=0x611000024440) at ../iothread.c:63
+ * #7  0x00005555573005c6 in qemu_thread_start (args=0x603000090d30) at ../util/qemu-thread-posix.c:541
+ * #8  0x00007ffff70eac12 in start_thread () at /lib64/libc.so.6
+ * #9  0x00007ffff716fcc0 in clone3 () at /lib64/libc.so.6
+ *
+ * 在以下使用virtio_blk_handle_output():
+ *   - hw/block/virtio-blk.c|2079| <<virtio_blk_device_realize>> virtio_add_queue(vdev, conf->queue_size, virtio_blk_handle_output);
+ */
 static void virtio_blk_handle_output(VirtIODevice *vdev, VirtQueue *vq)
 {
     VirtIOBlock *s = (VirtIOBlock *)vdev;
 
+    /*
+     * 在以下使用VirtIOBlock->ioeventfd_disabled:
+     *   - hw/block/virtio-blk.c|1202| <<virtio_blk_handle_output>> if (!s->ioeventfd_disabled && !s->ioeventfd_started) {
+     *   - hw/block/virtio-blk.c|1207| <<virtio_blk_handle_output>> if (!s->ioeventfd_disabled) {
+     *   - hw/block/virtio-blk.c|2057| <<virtio_blk_start_ioeventfd>> s->ioeventfd_disabled = true;
+     *   - hw/block/virtio-blk.c|2094| <<virtio_blk_stop_ioeventfd>> if (s->ioeventfd_disabled) {
+     *   - hw/block/virtio-blk.c|2095| <<virtio_blk_stop_ioeventfd>> s->ioeventfd_disabled = false;
+     *   - hw/block/virtio-blk.c|2246| <<virtio_blk_device_realize>> s->ioeventfd_disabled = true;
+     */
     if (!s->ioeventfd_disabled && !s->ioeventfd_started) {
         /* Some guests kick before setting VIRTIO_CONFIG_S_DRIVER_OK so start
          * ioeventfd here instead of waiting for .set_status().
@@ -1155,6 +1276,9 @@ static void virtio_blk_handle_output(VirtIODevice *vdev, VirtQueue *vq)
         }
     }
 
+    /*
+     * 只在此处调用
+     */
     virtio_blk_handle_vq(s, vq);
 }
 
@@ -1190,6 +1314,31 @@ static void virtio_blk_dma_restart_bh(void *opaque)
     blk_dec_in_flight(s->conf.conf.blk);
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_blk_dma_restart_cb (opaque=0x62d00001c830, running=false, state=RUN_STATE_PAUSED) at ../hw/block/virtio-blk.c:1059
+ * #1  0x000055555677f0e8 in vm_state_notify (running=false, state=RUN_STATE_PAUSED) at ../system/runstate.c:380
+ * #2  0x000055555675eefe in do_vm_stop (state=RUN_STATE_PAUSED, send_stop=true) at ../system/cpus.c:290
+ * #3  0x00005555567612ab in vm_stop (state=RUN_STATE_PAUSED) at ../system/cpus.c:697
+ * #4  0x0000555556856668 in qmp_stop (errp=0x0) at ../monitor/qmp-cmds.c:61
+ * #5  0x00005555568490c0 in hmp_stop (mon=0x610000003540, qdict=0x621000231500) at ../monitor/hmp-cmds.c:119
+ * #6  0x0000555556852256 in handle_hmp_command_exec (mon=0x610000003540, cmd=0x555558959120 <hmp_cmds+7680>, qdict=0x621000231500) at ../monitor/hmp.c:1106
+ * #7  0x00005555568526f2 in handle_hmp_command (mon=0x610000003540, cmdline=0x0) at ../monitor/hmp.c:1158
+ * #8  0x000055555684b248 in monitor_command_cb (opaque=0x610000003540, cmdline=0x626000000100 "stop", readline_opaque=0x0) at ../monitor/hmp.c:47
+ * #9  0x0000555557369303 in readline_handle_byte (rs=0x626000000100, ch=13) at ../util/readline.c:419
+ * #10 0x00005555568542ab in monitor_read (opaque=0x610000003540, buf=0x7ffff537e020 "\r", size=1) at ../monitor/hmp.c:1390
+ * #11 0x00005555571298fd in qemu_chr_be_write_impl (s=0x610000003440, buf=0x7ffff537e020 "\r", len=1) at ../chardev/char.c:214
+ * #12 0x00005555571299a8 in qemu_chr_be_write (s=0x610000003440, buf=0x7ffff537e020 "\r", len=1) at ../chardev/char.c:226
+ * #13 0x000055555712e942 in fd_chr_read (chan=0x60b000068160, cond=G_IO_IN, opaque=0x610000003440) at ../chardev/char-fd.c:72
+ * #14 0x0000555556e85ff5 in qio_channel_fd_source_dispatch (source=0x60c000054dc0, callback=0x55555712e6b1 <fd_chr_read>, user_data=0x610000003440) at ../io/channel-watch.c:84
+ * #15 0x00007ffff7533f4f in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #16 0x000055555733abaa in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #17 0x000055555733ad23 in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:310
+ * #18 0x000055555733b03c in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #19 0x0000555556780465 in qemu_main_loop () at ../system/runstate.c:795
+ * #20 0x0000555556e39fe2 in qemu_default_main () at ../system/main.c:37
+ * #21 0x0000555556e3a01f in main (argc=42, argv=0x7fffffffda58) at ../system/main.c:48
+ */
 static void virtio_blk_dma_restart_cb(void *opaque, bool running,
                                       RunState state)
 {
@@ -1512,6 +1661,10 @@ static void virtio_blk_resize(void *opaque)
     aio_bh_schedule_oneshot(qemu_get_aio_context(), virtio_resize_cb, vdev);
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1707| <<virtio_blk_drained_begin>> virtio_blk_ioeventfd_detach(s);
+ */
 static void virtio_blk_ioeventfd_detach(VirtIOBlock *s)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
@@ -1522,12 +1675,115 @@ static void virtio_blk_ioeventfd_detach(VirtIOBlock *s)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  virtio_blk_dma_restart_cb (opaque=0x62d00001c830, running=false, state=RUN_STATE_PAUSED) at ../hw/block/virtio-blk.c:1059
+ * #1  0x000055555677f0e8 in vm_state_notify (running=false, state=RUN_STATE_PAUSED) at ../system/runstate.c:380
+ * #2  0x000055555675eefe in do_vm_stop (state=RUN_STATE_PAUSED, send_stop=true) at ../system/cpus.c:290
+ * #3  0x00005555567612ab in vm_stop (state=RUN_STATE_PAUSED) at ../system/cpus.c:697
+ * #4  0x0000555556856668 in qmp_stop (errp=0x0) at ../monitor/qmp-cmds.c:61
+ * #5  0x00005555568490c0 in hmp_stop (mon=0x610000003540, qdict=0x621000231500) at ../monitor/hmp-cmds.c:119
+ * #6  0x0000555556852256 in handle_hmp_command_exec (mon=0x610000003540, cmd=0x555558959120 <hmp_cmds+7680>, qdict=0x621000231500) at ../monitor/hmp.c:1106
+ * #7  0x00005555568526f2 in handle_hmp_command (mon=0x610000003540, cmdline=0x0) at ../monitor/hmp.c:1158
+ * #8  0x000055555684b248 in monitor_command_cb (opaque=0x610000003540, cmdline=0x626000000100 "stop", readline_opaque=0x0) at ../monitor/hmp.c:47
+ * #9  0x0000555557369303 in readline_handle_byte (rs=0x626000000100, ch=13) at ../util/readline.c:419
+ * #10 0x00005555568542ab in monitor_read (opaque=0x610000003540, buf=0x7ffff537e020 "\r", size=1) at ../monitor/hmp.c:1390
+ * #11 0x00005555571298fd in qemu_chr_be_write_impl (s=0x610000003440, buf=0x7ffff537e020 "\r", len=1) at ../chardev/char.c:214
+ * #12 0x00005555571299a8 in qemu_chr_be_write (s=0x610000003440, buf=0x7ffff537e020 "\r", len=1) at ../chardev/char.c:226
+ * #13 0x000055555712e942 in fd_chr_read (chan=0x60b000068160, cond=G_IO_IN, opaque=0x610000003440) at ../chardev/char-fd.c:72
+ * #14 0x0000555556e85ff5 in qio_channel_fd_source_dispatch (source=0x60c000054dc0, callback=0x55555712e6b1 <fd_chr_read>, user_data=0x610000003440) at ../io/channel-watch.c:84
+ * #15 0x00007ffff7533f4f in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #16 0x000055555733abaa in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #17 0x000055555733ad23 in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:310
+ * #18 0x000055555733b03c in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #19 0x0000555556780465 in qemu_main_loop () at ../system/runstate.c:795
+ * #20 0x0000555556e39fe2 in qemu_default_main () at ../system/main.c:37
+ * #21 0x0000555556e3a01f in main (argc=42, argv=0x7fffffffda58) at ../system/main.c:48
+ *
+ *
+ * 初始化(BIOS/Kernel)会调用
+ * system_powerdown会调用
+ *
+ * (gdb) bt
+ * #0  virtio_queue_aio_attach_host_notifier (vq=0x7ffddbda4800, ctx=0x613000121880) at ../hw/virtio/virtio.c:3575
+ * #1  0x0000555556ba8095 in virtio_blk_ioeventfd_attach (s=0x62d00001c830) at ../hw/block/virtio-blk.c:1391
+ * #2  0x0000555556ba9e98 in virtio_blk_start_ioeventfd (vdev=0x62d00001c830) at ../hw/block/virtio-blk.c:1721
+ * #3  0x00005555566934ae in virtio_bus_start_ioeventfd (bus=0x62d00001c7b0) at ../hw/virtio/virtio-bus.c:236
+ * #4  0x000055555669686b in virtio_pci_start_ioeventfd (proxy=0x62d000014400) at ../hw/virtio/virtio-pci.c:375
+ * #5  0x000055555669cafe in virtio_pci_common_write (opaque=0x62d000014400, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1616
+ * #6  0x0000555556d1538b in memory_region_write_accessor (mr=0x62d000014fa0, addr=20, value=0x7ffde5865020, size=1, shift=0, mask=255, attrs=...) at ../system/memory.c:497
+ * #7  0x0000555556d159cf in access_with_adjusted_size (addr=20, value=0x7ffde5865020, size=1, access_size_min=1, access_size_max=4, access_fn=0x555556d15177 <memory_region_write_accessor>, mr=0x62d000014fa0, attrs=...)
+ *     at ../system/memory.c:573
+ * #8  0x0000555556d1dcf0 in memory_region_dispatch_write (mr=0x62d000014fa0, addr=20, data=15, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #9  0x0000555556d41d5c in flatview_write_continue_step (attrs=..., buf=0x7ffff6205028 "\017", len=1, mr_addr=20, l=0x7ffde5960540, mr=0x62d000014fa0) at ../system/physmem.c:2756
+ * #10 0x0000555556d41f50 in flatview_write_continue (fv=0x606000085e20, addr=4261412884, attrs=..., ptr=0x7ffff6205028, len=1, mr_addr=20, l=1, mr=0x62d000014fa0) at ../system/physmem.c:2786
+ * #11 0x0000555556d42254 in flatview_write (fv=0x606000085e20, addr=4261412884, attrs=..., buf=0x7ffff6205028, len=1) at ../system/physmem.c:2817
+ * #12 0x0000555556d42e3d in address_space_write (as=0x555558d5fda0 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7ffff6205028, len=1) at ../system/physmem.c:2937
+ * #13 0x0000555556d42f00 in address_space_rw (as=0x555558d5fda0 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7ffff6205028, len=1, is_write=true) at ../system/physmem.c:2947
+ * #14 0x0000555556e104c0 in kvm_cpu_exec (cpu=0x62b000000200) at ../accel/kvm/kvm-all.c:3031
+ * #15 0x0000555556e1981b in kvm_vcpu_thread_fn (arg=0x62b000000200) at ../accel/kvm/kvm-accel-ops.c:50
+ * #16 0x00005555573005c6 in qemu_thread_start (args=0x6030000a4b60) at ../util/qemu-thread-posix.c:541
+ * #17 0x00007ffff70eac12 in start_thread () at /lib64/libc.so.6
+ * #18 0x00007ffff716fcc0 in clone3 () at /lib64/libc.so.6
+ *
+ * stop/cont会调用
+ *
+ * (gdb) bt
+ * #0  virtio_queue_aio_attach_host_notifier (vq=0x7ffddbda4c28, ctx=0x613000123100) at ../hw/virtio/virtio.c:3575
+ * #1  0x0000555556ba8095 in virtio_blk_ioeventfd_attach (s=0x62d00001c830) at ../hw/block/virtio-blk.c:1391
+ * #2  0x0000555556ba9e98 in virtio_blk_start_ioeventfd (vdev=0x62d00001c830) at ../hw/block/virtio-blk.c:1721
+ * #3  0x00005555566934ae in virtio_bus_start_ioeventfd (bus=0x62d00001c7b0) at ../hw/virtio/virtio-bus.c:236
+ * #4  0x000055555669686b in virtio_pci_start_ioeventfd (proxy=0x62d000014400) at ../hw/virtio/virtio-pci.c:375
+ * #5  0x000055555669b223 in virtio_pci_vmstate_change (d=0x62d000014400, running=true) at ../hw/virtio/virtio-pci.c:1361
+ * #6  0x0000555556cae295 in virtio_vmstate_change (opaque=0x62d00001c830, running=true, state=RUN_STATE_RUNNING) at ../hw/virtio/virtio.c:3207
+ * #7  0x000055555677ee82 in vm_state_notify (running=true, state=RUN_STATE_RUNNING) at ../system/runstate.c:370
+ * #8  0x00005555567614b9 in vm_prepare_start (step_pending=false) at ../system/cpus.c:740
+ * #9  0x0000555556761535 in vm_start () at ../system/cpus.c:747
+ * #10 0x0000555556856af7 in qmp_cont (errp=0x7ffff4c74e60) at ../monitor/qmp-cmds.c:114
+ * #11 0x00005555568495d0 in hmp_cont (mon=0x610000003540, qdict=0x621000138900) at ../monitor/hmp-cmds.c:171
+ * #12 0x0000555556852256 in handle_hmp_command_exec (mon=0x610000003540, cmd=0x5555589579b0 <hmp_cmds+1680>, qdict=0x621000138900) at ../monitor/hmp.c:1106
+ * #13 0x00005555568526f2 in handle_hmp_command (mon=0x610000003540, cmdline=0x0) at ../monitor/hmp.c:1158
+ * #14 0x000055555684b248 in monitor_command_cb (opaque=0x610000003540, cmdline=0x626000000100 "cont", readline_opaque=0x0) at ../monitor/hmp.c:47
+ * #15 0x0000555557369303 in readline_handle_byte (rs=0x626000000100, ch=13) at ../util/readline.c:419
+ * #16 0x00005555568542ab in monitor_read (opaque=0x610000003540, buf=0x7ffff535a020 "\r", size=1) at ../monitor/hmp.c:1390
+ * #17 0x00005555571298fd in qemu_chr_be_write_impl (s=0x610000003440, buf=0x7ffff535a020 "\r", len=1) at ../chardev/char.c:214
+ * #18 0x00005555571299a8 in qemu_chr_be_write (s=0x610000003440, buf=0x7ffff535a020 "\r", len=1) at ../chardev/char.c:226
+ * #19 0x000055555712e942 in fd_chr_read (chan=0x60b000068160, cond=G_IO_IN, opaque=0x610000003440) at ../chardev/char-fd.c:72
+ * #20 0x0000555556e85ff5 in qio_channel_fd_source_dispatch (source=0x60c000054dc0, callback=0x55555712e6b1 <fd_chr_read>, user_data=0x610000003440) at ../io/channel-watch.c:84
+ * #21 0x00007ffff7533f4f in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #22 0x000055555733abaa in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #23 0x000055555733ad23 in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:310
+ * #24 0x000055555733b03c in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #25 0x0000555556780465 in qemu_main_loop () at ../system/runstate.c:795
+ * #26 0x0000555556e39fe2 in qemu_default_main () at ../system/main.c:37
+ * #27 0x0000555556e3a01f in main (argc=42, argv=0x7fffffffda58) at ../system/main.c:48
+ *
+ * called by:
+ *   - hw/block/virtio-blk.c|1605| <<virtio_blk_drained_end>> virtio_blk_ioeventfd_attach(s);
+ *   - hw/block/virtio-blk.c|1934| <<virtio_blk_start_ioeventfd>> virtio_blk_ioeventfd_attach(s);
+ */
 static void virtio_blk_ioeventfd_attach(VirtIOBlock *s)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
 
     for (uint16_t i = 0; i < s->conf.num_queues; i++) {
         VirtQueue *vq = virtio_get_queue(vdev, i);
+        /*
+	 * 在以下使用vq_aio_context[][]:
+	 *   - hw/block/virtio-blk.c|1261| <<virtio_blk_dma_restart_cb>> aio_bh_schedule_oneshot(s->vq_aio_context[i], virtio_blk_dma_restart_bh, vq_rq[i]);
+	 *   - hw/block/virtio-blk.c|1549| <<virtio_blk_ioeventfd_detach>> virtio_queue_aio_detach_host_notifier(vq, s->vq_aio_context[i]);
+	 *   - hw/block/virtio-blk.c|1559| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+	 *   - hw/block/virtio-blk.c|1749| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = g_new(AioContext *, conf->num_queues);
+	 *   - hw/block/virtio-blk.c|1753| <<virtio_blk_vq_aio_context_init>> if (!apply_iothread_vq_mapping(conf->iothread_vq_mapping_list, s->vq_aio_context, conf->num_queues, errp)) {
+	 *   - hw/block/virtio-blk.c|1756| <<virtio_blk_vq_aio_context_init>> g_free(s->vq_aio_context);
+	 *   - hw/block/virtio-blk.c|1757| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = NULL;
+	 *   - hw/block/virtio-blk.c|1763| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+	 *   - hw/block/virtio-blk.c|1771| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+	 *   - hw/block/virtio-blk.c|1798| <<virtio_blk_vq_aio_context_cleanup>> g_free(s->vq_aio_context);
+	 *   - hw/block/virtio-blk.c|1799| <<virtio_blk_vq_aio_context_cleanup>> s->vq_aio_context = NULL;
+	 *   - hw/block/virtio-blk.c|1864| <<virtio_blk_start_ioeventfd>> r = blk_set_aio_context(s->conf.conf.blk, s->vq_aio_context[0], &local_err);
+	 *   - hw/block/virtio-blk.c|1943| <<virtio_blk_stop_ioeventfd>> AioContext *ctx = s->vq_aio_context[i];
+	 */
         virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
     }
 }
@@ -1632,6 +1888,10 @@ validate_iothread_vq_mapping_list(IOThreadVirtQueueMappingList *list,
  *
  * Returns: %true on success, %false on failure.
  **/
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1752| <<virtio_blk_vq_aio_context_init>> if (!apply_iothread_vq_mapping(conf->iothread_vq_mapping_list, s->vq_aio_context, conf->num_queues, errp)) {
+ */
 static bool apply_iothread_vq_mapping(
         IOThreadVirtQueueMappingList *iothread_vq_mapping_list,
         AioContext **vq_aio_context,
@@ -1652,6 +1912,14 @@ static bool apply_iothread_vq_mapping(
     }
 
     for (node = iothread_vq_mapping_list; node; node = node->next) {
+        /*
+	 * called by:
+	 *   - block/export/export.c|123| <<blk_exp_add>> iothread = iothread_by_id(export->iothread);
+	 *   - blockdev.c|3560| <<qmp_x_blockdev_set_iothread>> IOThread *obj = iothread_by_id(iothread->u.s);
+	 *   - hw/block/virtio-blk.c|1601| <<validate_iothread_vq_mapping_list>> if (!iothread_by_id(name)) {
+	 *   - hw/block/virtio-blk.c|1683| <<apply_iothread_vq_mapping>> IOThread *iothread = iothread_by_id(node->value->iothread);
+	 *   - hw/block/virtio-blk.c|1789| <<virtio_blk_vq_aio_context_cleanup>> IOThread *iothread = iothread_by_id(node->value->iothread);
+	 */
         IOThread *iothread = iothread_by_id(node->value->iothread);
         AioContext *ctx = iothread_get_aio_context(iothread);
 
@@ -1680,6 +1948,13 @@ static bool apply_iothread_vq_mapping(
     return true;
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|2088| <<virtio_blk_device_realize>> virtio_blk_vq_aio_context_init(s, &err);
+ *
+ * VirtIOBlock *s:
+ * -> AioContext **vq_aio_context;
+ */
 /* Context: BQL held */
 static bool virtio_blk_vq_aio_context_init(VirtIOBlock *s, Error **errp)
 {
@@ -1771,6 +2046,10 @@ static void virtio_blk_vq_aio_context_cleanup(VirtIOBlock *s)
     s->vq_aio_context = NULL;
 }
 
+/*
+ * 在以下使用virtio_blk_start_ioeventfd():
+ *   - hw/block/virtio-blk.c|2254| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+ */
 /* Context: BQL held */
 static int virtio_blk_start_ioeventfd(VirtIODevice *vdev)
 {
@@ -2071,6 +2350,12 @@ static void virtio_blk_device_realize(DeviceState *dev, Error **errp)
      * This must be after virtio_init() so virtio_blk_dma_restart_cb() gets
      * called after ->start_ioeventfd() has already set blk's AioContext.
      */
+    /*
+     * called by:
+     *   - hw/block/virtio-blk.c|2157| <<virtio_blk_device_realize>> qdev_add_vm_change_state_handler(dev, virtio_blk_dma_restart_cb, s);
+     *   - hw/scsi/scsi-bus.c|358| <<scsi_qdev_realize>> dev->vmsentry = qdev_add_vm_change_state_handler(DEVICE(dev), scsi_dma_restart_cb, dev);
+     *   - hw/virtio/virtio.c|3285| <<virtio_init>> vdev->vmstate = qdev_add_vm_change_state_handler(DEVICE(vdev), virtio_vmstate_change, vdev);
+     */
     s->change =
         qdev_add_vm_change_state_handler(dev, virtio_blk_dma_restart_cb, s);
 
diff --git a/hw/core/vm-change-state-handler.c b/hw/core/vm-change-state-handler.c
index 8e2639224..5f1c759b4 100644
--- a/hw/core/vm-change-state-handler.c
+++ b/hw/core/vm-change-state-handler.c
@@ -52,6 +52,88 @@ static int qdev_get_dev_tree_depth(DeviceState *dev)
  *
  * Returns: an entry to be freed with qemu_del_vm_change_state_handler()
  */
+/*
+ * 先
+ * (gdb) bt
+ * #0  qdev_add_vm_change_state_handler (dev=0x62d00001c830, cb=0x555556cae0c9 <virtio_vmstate_change>, opaque=0x62d00001c830) at ../hw/core/vm-change-state-handler.c:59
+ * #1  0x0000555556caea60 in virtio_init (vdev=0x62d00001c830, device_id=2, config_size=57) at ../hw/virtio/virtio.c:3263
+ * #2  0x0000555556baaf85 in virtio_blk_device_realize (dev=0x62d00001c830, errp=0x7ffff4cdeda0) at ../hw/block/virtio-blk.c:1902
+ * #3  0x0000555556cb0bab in virtio_device_realize (dev=0x62d00001c830, errp=0x7ffff4cdec20) at ../hw/virtio/virtio.c:3718
+ * #4  0x0000555556e45110 in device_set_realized (obj=0x62d00001c830, value=true, errp=0x7ffff4cde0a0) at ../hw/core/qdev.c:510
+ * #5  0x0000555556e5c5b5 in property_set_bool (obj=0x62d00001c830, v=0x61100009bcc0, name=0x555557875960 "realized", opaque=0x60200000c5f0, errp=0x7ffff4cde0a0) at ../qom/object.c:2354
+ * #6  0x0000555556e57a50 in object_property_set (obj=0x62d00001c830, name=0x555557875960 "realized", v=0x61100009bcc0, errp=0x7ffff4cde0a0) at ../qom/object.c:1463
+ * #7  0x0000555556e6148d in object_property_set_qobject (obj=0x62d00001c830, name=0x555557875960 "realized", value=0x6030000d1560, errp=0x7ffff4cde0a0) at ../qom/qom-qobject.c:28
+ * #8  0x0000555556e57fca in object_property_set_bool (obj=0x62d00001c830, name=0x555557875960 "realized", value=true, errp=0x7ffff4cde0a0) at ../qom/object.c:1533
+ * #9  0x0000555556e43fba in qdev_realize (dev=0x62d00001c830, bus=0x62d00001c7b0, errp=0x7ffff4cde0a0) at ../hw/core/qdev.c:291
+ * #10 0x0000555556cfb5c4 in virtio_blk_pci_realize (vpci_dev=0x62d000014400, errp=0x7ffff4cde0a0) at ../hw/virtio/virtio-blk-pci.c:64
+ * #11 0x00005555566a0878 in virtio_pci_realize (pci_dev=0x62d000014400, errp=0x7ffff4cde0a0) at ../hw/virtio/virtio-pci.c:2259
+ * #12 0x0000555556475b08 in pci_qdev_realize (qdev=0x62d000014400, errp=0x7ffff4cddde0) at ../hw/pci/pci.c:2093
+ * #13 0x00005555566a10f7 in virtio_pci_dc_realize (qdev=0x62d000014400, errp=0x7ffff4cddde0) at ../hw/virtio/virtio-pci.c:2351
+ * #14 0x0000555556e45110 in device_set_realized (obj=0x62d000014400, value=true, errp=0x7ffff4de17c0) at ../hw/core/qdev.c:510
+ * #15 0x0000555556e5c5b5 in property_set_bool (obj=0x62d000014400, v=0x61100009bb80, name=0x555557875960 "realized", opaque=0x60200000c5f0, errp=0x7ffff4de17c0) at ../qom/object.c:2354
+ * #16 0x0000555556e57a50 in object_property_set (obj=0x62d000014400, name=0x555557875960 "realized", v=0x61100009bb80, errp=0x7ffff4de17c0) at ../qom/object.c:1463
+ * #17 0x0000555556e6148d in object_property_set_qobject (obj=0x62d000014400, name=0x555557875960 "realized", value=0x6030000d0fc0, errp=0x7ffff4de17c0) at ../qom/qom-qobject.c:28
+ * #18 0x0000555556e57fca in object_property_set_bool (obj=0x62d000014400, name=0x555557875960 "realized", value=true, errp=0x7ffff4de17c0) at ../qom/object.c:1533
+ * #19 0x0000555556e43fba in qdev_realize (dev=0x62d000014400, bus=0x61d000102c80, errp=0x7ffff4de17c0) at ../hw/core/qdev.c:291
+ * #20 0x0000555556771f7e in qdev_device_add_from_qdict (opts=0x62100000dd00, from_json=true, errp=0x7ffff4de17c0) at ../system/qdev-monitor.c:719
+ * #21 0x000055555678ec8b in qemu_create_cli_devices () at ../system/vl.c:2656
+ * #22 0x000055555678f001 in qmp_x_exit_preconfig (errp=0x555558dc1a00 <error_fatal>) at ../system/vl.c:2713
+ * #23 0x0000555556793f88 in qemu_init (argc=34, argv=0x7fffffffdb78) at ../system/vl.c:3758
+ * #24 0x0000555556e3a016 in main (argc=34, argv=0x7fffffffdb78) at ../system/main.c:47
+ *
+ * 再
+ * (gdb) bt
+ * #0  qdev_add_vm_change_state_handler (dev=0x62d00001c830, cb=0x555556ba5184 <virtio_blk_dma_restart_cb>, opaque=0x62d00001c830) at ../hw/core/vm-change-state-handler.c:59
+ * #1  0x0000555556bab377 in virtio_blk_device_realize (dev=0x62d00001c830, errp=0x7ffff4cdeda0) at ../hw/block/virtio-blk.c:1935
+ * #2  0x0000555556cb0bab in virtio_device_realize (dev=0x62d00001c830, errp=0x7ffff4cdec20) at ../hw/virtio/virtio.c:3718
+ * #3  0x0000555556e45110 in device_set_realized (obj=0x62d00001c830, value=true, errp=0x7ffff4cde0a0) at ../hw/core/qdev.c:510
+ * #4  0x0000555556e5c5b5 in property_set_bool (obj=0x62d00001c830, v=0x61100009bcc0, name=0x555557875960 "realized", opaque=0x60200000c5f0, errp=0x7ffff4cde0a0) at ../qom/object.c:2354
+ * #5  0x0000555556e57a50 in object_property_set (obj=0x62d00001c830, name=0x555557875960 "realized", v=0x61100009bcc0, errp=0x7ffff4cde0a0) at ../qom/object.c:1463
+ * #6  0x0000555556e6148d in object_property_set_qobject (obj=0x62d00001c830, name=0x555557875960 "realized", value=0x6030000d1560, errp=0x7ffff4cde0a0) at ../qom/qom-qobject.c:28
+ * #7  0x0000555556e57fca in object_property_set_bool (obj=0x62d00001c830, name=0x555557875960 "realized", value=true, errp=0x7ffff4cde0a0) at ../qom/object.c:1533
+ * #8  0x0000555556e43fba in qdev_realize (dev=0x62d00001c830, bus=0x62d00001c7b0, errp=0x7ffff4cde0a0) at ../hw/core/qdev.c:291
+ * #9  0x0000555556cfb5c4 in virtio_blk_pci_realize (vpci_dev=0x62d000014400, errp=0x7ffff4cde0a0) at ../hw/virtio/virtio-blk-pci.c:64
+ * #10 0x00005555566a0878 in virtio_pci_realize (pci_dev=0x62d000014400, errp=0x7ffff4cde0a0) at ../hw/virtio/virtio-pci.c:2259
+ * #11 0x0000555556475b08 in pci_qdev_realize (qdev=0x62d000014400, errp=0x7ffff4cddde0) at ../hw/pci/pci.c:2093
+ * #12 0x00005555566a10f7 in virtio_pci_dc_realize (qdev=0x62d000014400, errp=0x7ffff4cddde0) at ../hw/virtio/virtio-pci.c:2351
+ * #13 0x0000555556e45110 in device_set_realized (obj=0x62d000014400, value=true, errp=0x7ffff4de17c0) at ../hw/core/qdev.c:510
+ * #14 0x0000555556e5c5b5 in property_set_bool (obj=0x62d000014400, v=0x61100009bb80, name=0x555557875960 "realized", opaque=0x60200000c5f0, errp=0x7ffff4de17c0) at ../qom/object.c:2354
+ * #15 0x0000555556e57a50 in object_property_set (obj=0x62d000014400, name=0x555557875960 "realized", v=0x61100009bb80, errp=0x7ffff4de17c0) at ../qom/object.c:1463
+ * #16 0x0000555556e6148d in object_property_set_qobject (obj=0x62d000014400, name=0x555557875960 "realized", value=0x6030000d0fc0, errp=0x7ffff4de17c0) at ../qom/qom-qobject.c:28
+ * #17 0x0000555556e57fca in object_property_set_bool (obj=0x62d000014400, name=0x555557875960 "realized", value=true, errp=0x7ffff4de17c0) at ../qom/object.c:1533
+ * #18 0x0000555556e43fba in qdev_realize (dev=0x62d000014400, bus=0x61d000102c80, errp=0x7ffff4de17c0) at ../hw/core/qdev.c:291
+ * #19 0x0000555556771f7e in qdev_device_add_from_qdict (opts=0x62100000dd00, from_json=true, errp=0x7ffff4de17c0) at ../system/qdev-monitor.c:719
+ * #20 0x000055555678ec8b in qemu_create_cli_devices () at ../system/vl.c:2656
+ * #21 0x000055555678f001 in qmp_x_exit_preconfig (errp=0x555558dc1a00 <error_fatal>) at ../system/vl.c:2713
+ * #22 0x0000555556793f88 in qemu_init (argc=34, argv=0x7fffffffdb78) at ../system/vl.c:3758
+ * #23 0x0000555556e3a016 in main (argc=34, argv=0x7fffffffdb78) at ../system/main.c:47
+ *
+ * 先
+ * (gdb) bt
+ * #0  virtio_vmstate_change (opaque=0x62d00001c830, running=true, state=RUN_STATE_RUNNING) at ../hw/virtio/virtio.c:3196
+ * #1  0x000055555677ee82 in vm_state_notify (running=true, state=RUN_STATE_RUNNING) at ../system/runstate.c:370
+ * #2  0x00005555567614b9 in vm_prepare_start (step_pending=false) at ../system/cpus.c:740
+ * #3  0x0000555556761535 in vm_start () at ../system/cpus.c:747
+ * #4  0x0000555556856af7 in qmp_cont (errp=0x0) at ../monitor/qmp-cmds.c:114
+ * #5  0x000055555678f234 in qmp_x_exit_preconfig (errp=0x555558dc1a00 <error_fatal>) at ../system/vl.c:2738
+ * #6  0x0000555556793f88 in qemu_init (argc=34, argv=0x7fffffffdb78) at ../system/vl.c:3758
+ * #7  0x0000555556e3a016 in main (argc=34, argv=0x7fffffffdb78) at ../system/main.c:47
+ *
+ * 再
+ * #0  virtio_blk_dma_restart_cb (opaque=0x62d00001c830, running=true, state=RUN_STATE_RUNNING) at ../hw/block/virtio-blk.c:1059
+ * #1  0x000055555677ee82 in vm_state_notify (running=true, state=RUN_STATE_RUNNING) at ../system/runstate.c:370
+ * #2  0x00005555567614b9 in vm_prepare_start (step_pending=false) at ../system/cpus.c:740
+ * #3  0x0000555556761535 in vm_start () at ../system/cpus.c:747
+ * #4  0x0000555556856af7 in qmp_cont (errp=0x0) at ../monitor/qmp-cmds.c:114
+ * #5  0x000055555678f234 in qmp_x_exit_preconfig (errp=0x555558dc1a00 <error_fatal>) at ../system/vl.c:2738
+ * #6  0x0000555556793f88 in qemu_init (argc=34, argv=0x7fffffffdb78) at ../system/vl.c:3758
+ * #7  0x0000555556e3a016 in main (argc=34, argv=0x7fffffffdb78) at ../system/main.c:47
+ *
+ * called by:
+ *   - hw/block/virtio-blk.c|2157| <<virtio_blk_device_realize>> qdev_add_vm_change_state_handler(dev, virtio_blk_dma_restart_cb, s);
+ *   - hw/scsi/scsi-bus.c|358| <<scsi_qdev_realize>> dev->vmsentry = qdev_add_vm_change_state_handler(DEVICE(dev), scsi_dma_restart_cb, dev);
+ *   - hw/virtio/virtio.c|3285| <<virtio_init>> vdev->vmstate = qdev_add_vm_change_state_handler(DEVICE(vdev), virtio_vmstate_change, vdev);
+ */
 VMChangeStateEntry *qdev_add_vm_change_state_handler(DeviceState *dev,
                                                      VMChangeStateHandler *cb,
                                                      void *opaque)
@@ -63,6 +145,11 @@ VMChangeStateEntry *qdev_add_vm_change_state_handler(DeviceState *dev,
  * Exactly like qdev_add_vm_change_state_handler() but passes a prepare_cb
  * argument too.
  */
+/*
+ * 在以下使用qdev_add_vm_change_state_handler_full():
+ *   - hw/core/vm-change-state-handler.c|59| <<qdev_add_vm_change_state_handler>> return qdev_add_vm_change_state_handler_full(dev, cb, NULL, opaque);
+ *   - hw/vfio/migration.c|859| <<vfio_migration_init>> migration->vm_state = qdev_add_vm_change_state_handler_full(vbasedev->dev, vfio_vmstate_change, prepare_cb, vbasedev);
+ */
 VMChangeStateEntry *qdev_add_vm_change_state_handler_full(
     DeviceState *dev, VMChangeStateHandler *cb,
     VMChangeStateHandler *prepare_cb, void *opaque)
diff --git a/hw/i386/kvm/apic.c b/hw/i386/kvm/apic.c
index a72c28e8a..69ffa20bd 100644
--- a/hw/i386/kvm/apic.c
+++ b/hw/i386/kvm/apic.c
@@ -179,6 +179,20 @@ static void kvm_apic_external_nmi(APICCommonState *s)
     run_on_cpu(CPU(s->cpu), do_inject_external_nmi, RUN_ON_CPU_HOST_PTR(s));
 }
 
+/*
+ * 在以下使用APICommonClass->send_msi:
+ *   - hw/i386/amd_iommu.c|1382| <<amdvi_mem_ir_write>> apic_get_class(NULL)->send_msi(&to);
+ *   - hw/i386/intel_iommu.c|400| <<vtd_generate_interrupt>> apic_get_class(NULL)->send_msi(&msi);
+ *   - hw/i386/intel_iommu.c|3628| <<vtd_mem_ir_write>> apic_get_class(NULL)->send_msi(&to);
+ *   - hw/i386/kvm/apic.c|257| <<kvm_apic_class_init>> k->send_msi = kvm_send_msi;
+ *   - hw/i386/xen/xen_apic.c|89| <<xen_apic_class_init>> k->send_msi = xen_send_msi;
+ *   - hw/intc/apic.c|1193| <<apic_class_init>> k->send_msi = apic_send_msi;
+ *   - target/i386/whpx/whpx-apic.c|267| <<whpx_apic_class_init>> k->send_msi = whpx_send_msi;
+ *
+ * 在以下使用kvm_send_msi():
+ *   - hw/i386/kvm/apic.c|211| <<kvm_apic_mem_write>> kvm_send_msi(&msg);
+ *   - hw/i386/kvm/apic.c|257| <<kvm_apic_class_init>> k->send_msi = kvm_send_msi;
+ */
 static void kvm_send_msi(MSIMessage *msg)
 {
     int ret;
diff --git a/hw/net/virtio-net.c b/hw/net/virtio-net.c
index 24e5e7d34..e76eeb42c 100644
--- a/hw/net/virtio-net.c
+++ b/hw/net/virtio-net.c
@@ -2717,6 +2717,14 @@ static void virtio_net_tx_complete(NetClientState *nc, ssize_t len)
     }
 }
 
+/*
+ * called by:
+ *   - hw/net/virtio-net.c|2702| <<virtio_net_tx_complete>> ret = virtio_net_flush_tx(q);
+ *   - hw/net/virtio-net.c|2894| <<virtio_net_tx_timer>> ret = virtio_net_flush_tx(q);
+ *   - hw/net/virtio-net.c|2914| <<virtio_net_tx_timer>> ret = virtio_net_flush_tx(q);
+ *   - hw/net/virtio-net.c|2944| <<virtio_net_tx_bh>> ret = virtio_net_flush_tx(q);
+ *   - hw/net/virtio-net.c|2962| <<virtio_net_tx_bh>> ret = virtio_net_flush_tx(q);
+ */
 /* TX */
 static int32_t virtio_net_flush_tx(VirtIONetQueue *q)
 {
@@ -2740,6 +2748,20 @@ static int32_t virtio_net_flush_tx(VirtIONetQueue *q)
         struct iovec sg[VIRTQUEUE_MAX_SIZE], sg2[VIRTQUEUE_MAX_SIZE + 1], *out_sg;
         struct virtio_net_hdr_v1_hash vhdr;
 
+        /*
+	 * typedef struct VirtQueueElement
+	 * {
+	 *     unsigned int index;
+	 *     unsigned int len;
+	 *     unsigned int ndescs;
+	 *     unsigned int out_num;
+	 *     unsigned int in_num;
+	 *     hwaddr *in_addr;
+	 *     hwaddr *out_addr;
+	 *     struct iovec *in_sg;
+	 *     struct iovec *out_sg;
+	 * } VirtQueueElement;
+	 */
         elem = virtqueue_pop(q->tx_vq, sizeof(VirtQueueElement));
         if (!elem) {
             break;
diff --git a/hw/pci/msi.c b/hw/pci/msi.c
index 8104ac1d9..618d2091e 100644
--- a/hw/pci/msi.c
+++ b/hw/pci/msi.c
@@ -375,6 +375,16 @@ void msi_notify(PCIDevice *dev, unsigned int vector)
     msi_send_message(dev, msg);
 }
 
+/*
+ * 在以下使用APICommonClass->send_msi:
+ *   - hw/i386/amd_iommu.c|1382| <<amdvi_mem_ir_write>> apic_get_class(NULL)->send_msi(&to);
+ *   - hw/i386/intel_iommu.c|400| <<vtd_generate_interrupt>> apic_get_class(NULL)->send_msi(&msi);
+ *   - hw/i386/intel_iommu.c|3628| <<vtd_mem_ir_write>> apic_get_class(NULL)->send_msi(&to);
+ *   - hw/i386/kvm/apic.c|257| <<kvm_apic_class_init>> k->send_msi = kvm_send_msi;
+ *   - hw/i386/xen/xen_apic.c|89| <<xen_apic_class_init>> k->send_msi = xen_send_msi;
+ *   - hw/intc/apic.c|1193| <<apic_class_init>> k->send_msi = apic_send_msi;
+ *   - target/i386/whpx/whpx-apic.c|267| <<whpx_apic_class_init>> k->send_msi = whpx_send_msi;
+ */
 void msi_send_message(PCIDevice *dev, MSIMessage msg)
 {
     dev->msi_trigger(dev, msg);
diff --git a/hw/pci/msix.c b/hw/pci/msix.c
index 487e49834..a4b563fda 100644
--- a/hw/pci/msix.c
+++ b/hw/pci/msix.c
@@ -71,11 +71,24 @@ static uint8_t *msix_pending_byte(PCIDevice *dev, int vector)
     return dev->msix_pba + vector / 8;
 }
 
+/*
+ * called by:
+ *   - hw/pci/msix.c|142| <<msix_handle_mask_update>> if (!is_masked && msix_is_pending(dev, vector)) {
+ */
 static int msix_is_pending(PCIDevice *dev, int vector)
 {
     return *msix_pending_byte(dev, vector) & msix_pending_mask(vector);
 }
 
+/*
+ * called by:
+ *   - hw/misc/ivshmem.c|346| <<ivshmem_vector_poll>> msix_set_pending(dev, vector);
+ *   - hw/pci/msix.c|536| <<msix_notify>> msix_set_pending(dev, vector);
+ *   - hw/virtio/virtio-pci.c|1143| <<virtio_pci_vector_poll>> msix_set_pending(dev, vector);
+ *   - hw/virtio/virtio-pci.c|1146| <<virtio_pci_vector_poll>> msix_set_pending(dev, vector);
+ *   - hw/virtio/virtio-pci.c|1161| <<virtio_pci_vector_poll>> msix_set_pending(dev, vector);
+ *   - hw/virtio/virtio-pci.c|1164| <<virtio_pci_vector_poll>> msix_set_pending(dev, vector);
+ */
 void msix_set_pending(PCIDevice *dev, unsigned int vector)
 {
     *msix_pending_byte(dev, vector) |= msix_pending_mask(vector);
@@ -104,12 +117,22 @@ bool msix_is_masked(PCIDevice *dev, unsigned int vector)
     return msix_vector_masked(dev, vector, dev->msix_function_masked);
 }
 
+/*
+ * 在以下使用msix_fire_vector_notifier():
+ *   - hw/pci/msix.c|140| <<msix_handle_mask_update>> msix_fire_vector_notifier(dev, vector, is_masked);
+ */
 static void msix_fire_vector_notifier(PCIDevice *dev,
                                       unsigned int vector, bool is_masked)
 {
     MSIMessage msg;
     int ret;
 
+    /*
+     * 在以下调用msix_set_vector_notifiers():
+     *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev, ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+     *   - hw/vfio/pci.c|683| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+     *   - hw/virtio/virtio-pci.c|1291| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev, virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+     */
     if (!dev->msix_vector_use_notifier) {
         return;
     }
@@ -117,11 +140,27 @@ static void msix_fire_vector_notifier(PCIDevice *dev,
         dev->msix_vector_release_notifier(dev, vector);
     } else {
         msg = msix_get_message(dev, vector);
+	/*
+	 * 在以下调用msix_set_vector_notifiers():
+	 *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev, ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+	 *   - hw/vfio/pci.c|683| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+	 *   - hw/virtio/virtio-pci.c|1291| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev, virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+	 */
         ret = dev->msix_vector_use_notifier(dev, vector, msg);
         assert(ret >= 0);
     }
 }
 
+/*
+ * called by:
+ *   - hw/pci/msix.c|165| <<msix_set_mask>> msix_handle_mask_update(dev, vector, was_masked);
+ *   - hw/pci/msix.c|206| <<msix_write_config>> msix_handle_mask_update(dev, vector,
+ *   - hw/pci/msix.c|231| <<msix_table_mmio_write>> msix_handle_mask_update(dev, vector, was_masked);
+ *   - hw/pci/msix.c|288| <<msix_mask_all>> msix_handle_mask_update(dev, vector, was_masked);
+ *   - hw/pci/msix.c|506| <<msix_load>> msix_handle_mask_update(dev, vector, true);
+ *
+ * 这里的vector是设备的第几个line
+ */
 static void msix_handle_mask_update(PCIDevice *dev, int vector, bool was_masked)
 {
     bool is_masked = msix_is_masked(dev, vector);
@@ -248,6 +287,21 @@ static uint64_t msix_pba_mmio_read(void *opaque, hwaddr addr,
                                    unsigned size)
 {
     PCIDevice *dev = opaque;
+    /*
+     * 在以下调用msix_set_vector_notifiers():
+     *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev, ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+     *   - hw/vfio/pci.c|683| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+     *   - hw/virtio/virtio-pci.c|1291| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev, virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+     *
+     * 在以下使用PCIDevice->msix_vector_poll_notifier:
+     *   - hw/pci/msix.c|251| <<msix_pba_mmio_read>> if (dev->msix_vector_poll_notifier) {
+     *   - hw/pci/msix.c|254| <<msix_pba_mmio_read>> dev->msix_vector_poll_notifier(dev, vector_start, vector_end);
+     *   - hw/pci/msix.c|629| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier = poll_notifier;
+     *   - hw/pci/msix.c|640| <<msix_set_vector_notifiers>> if (dev->msix_vector_poll_notifier) {
+     *   - hw/pci/msix.c|641| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier(dev, 0, dev->msix_entries_nr);
+     *   - hw/pci/msix.c|651| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier = NULL;
+     *   - hw/pci/msix.c|670| <<msix_unset_vector_notifiers>> dev->msix_vector_poll_notifier = NULL;
+     */
     if (dev->msix_vector_poll_notifier) {
         unsigned vector_start = addr * 8;
         unsigned vector_end = MIN(addr + size * 8, dev->msix_entries_nr);
@@ -615,6 +669,12 @@ static void msix_unset_notifier_for_vector(PCIDevice *dev, unsigned int vector)
     dev->msix_vector_release_notifier(dev, vector);
 }
 
+/*
+ * 在以下调用msix_set_vector_notifiers():
+ *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev, ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+ *   - hw/vfio/pci.c|683| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+ *   - hw/virtio/virtio-pci.c|1291| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev, virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+ */
 int msix_set_vector_notifiers(PCIDevice *dev,
                               MSIVectorUseNotifier use_notifier,
                               MSIVectorReleaseNotifier release_notifier,
diff --git a/hw/pci/pci.c b/hw/pci/pci.c
index e7a39cb20..36b203b8b 100644
--- a/hw/pci/pci.c
+++ b/hw/pci/pci.c
@@ -1110,31 +1110,92 @@ uint16_t pci_requester_id(PCIDevice *dev)
     return pci_req_id_cache_extract(&dev->requester_id_cache);
 }
 
+/*
+ * called by:
+ *   - hw/pci/pci.c|1166| <<do_pci_register_device>> if (pci_bus_devfn_available(bus, devfn) && !pci_bus_devfn_reserved(bus, devfn)) {
+ *   - hw/pci/pci.c|1180| <<do_pci_register_device>> } else if (!pci_bus_devfn_available(bus, devfn)) {
+ */
 static bool pci_bus_devfn_available(PCIBus *bus, int devfn)
 {
     return !(bus->devices[devfn]);
 }
 
+/*
+ * called by:
+ *   - hw/pci/pci.c|1167| <<do_pci_register_device>> if (pci_bus_devfn_available(bus, devfn) && !pci_bus_devfn_reserved(bus, devfn)) {
+ *   - hw/pci/pci.c|1175| <<do_pci_register_device>> } else if (pci_bus_devfn_reserved(bus, devfn)) {
+ */
 static bool pci_bus_devfn_reserved(PCIBus *bus, int devfn)
 {
+    /*
+     * 在以下使用PCIBus->slot_reserved_mask:
+     *   - hw/pci/pci.c|511| <<pci_root_bus_internal_init>> bus->slot_reserved_mask = 0x0;
+     *   - hw/pci/pci.c|1130| <<pci_bus_devfn_reserved>> return bus->slot_reserved_mask & (1UL << PCI_SLOT(devfn));
+     *   - hw/pci/pci.c|1135| <<pci_bus_get_slot_reserved_mask>> return bus->slot_reserved_mask;
+     *   - hw/pci/pci.c|1140| <<pci_bus_set_slot_reserved_mask>> bus->slot_reserved_mask |= mask;
+     *   - hw/pci/pci.c|1149| <<pci_bus_clear_slot_reserved_mask>> bus->slot_reserved_mask &= ~mask;
+     */
     return bus->slot_reserved_mask & (1UL << PCI_SLOT(devfn));
 }
 
+/*
+ * called by:
+ *   - hw/xen/xen_pt.c|974| <<xen_igd_clear_slot>> if (!(pci_bus_get_slot_reserved_mask(pci_bus) & XEN_PCI_IGD_SLOT_MASK)) {
+ */
 uint32_t pci_bus_get_slot_reserved_mask(PCIBus *bus)
 {
+    /*
+     * 在以下使用PCIBus->slot_reserved_mask:
+     *   - hw/pci/pci.c|511| <<pci_root_bus_internal_init>> bus->slot_reserved_mask = 0x0;
+     *   - hw/pci/pci.c|1130| <<pci_bus_devfn_reserved>> return bus->slot_reserved_mask & (1UL << PCI_SLOT(devfn));
+     *   - hw/pci/pci.c|1135| <<pci_bus_get_slot_reserved_mask>> return bus->slot_reserved_mask;
+     *   - hw/pci/pci.c|1140| <<pci_bus_set_slot_reserved_mask>> bus->slot_reserved_mask |= mask;
+     *   - hw/pci/pci.c|1149| <<pci_bus_clear_slot_reserved_mask>> bus->slot_reserved_mask &= ~mask;
+     */
     return bus->slot_reserved_mask;
 }
 
+/*
+ * called by:
+ *   - hw/sparc64/sun4u.c|613| <<sun4uv_init>> pci_bus_set_slot_reserved_mask(pci_bus, 0xfffffffc);
+ *   - hw/sparc64/sun4u.c|614| <<sun4uv_init>> pci_bus_set_slot_reserved_mask(pci_busA, 0xfffffff1);
+ *   - hw/sparc64/sun4u.c|615| <<sun4uv_init>> pci_bus_set_slot_reserved_mask(pci_busB, 0xfffffff0);
+ *   - hw/xen/xen_pt.c|954| <<xen_igd_reserve_slot>> pci_bus_set_slot_reserved_mask(pci_bus, XEN_PCI_IGD_SLOT_MASK);
+ */
 void pci_bus_set_slot_reserved_mask(PCIBus *bus, uint32_t mask)
 {
+    /*
+     * 在以下使用PCIBus->slot_reserved_mask:
+     *   - hw/pci/pci.c|511| <<pci_root_bus_internal_init>> bus->slot_reserved_mask = 0x0;
+     *   - hw/pci/pci.c|1130| <<pci_bus_devfn_reserved>> return bus->slot_reserved_mask & (1UL << PCI_SLOT(devfn));
+     *   - hw/pci/pci.c|1135| <<pci_bus_get_slot_reserved_mask>> return bus->slot_reserved_mask;
+     *   - hw/pci/pci.c|1140| <<pci_bus_set_slot_reserved_mask>> bus->slot_reserved_mask |= mask;
+     *   - hw/pci/pci.c|1149| <<pci_bus_clear_slot_reserved_mask>> bus->slot_reserved_mask &= ~mask;
+     */
     bus->slot_reserved_mask |= mask;
 }
 
+/*
+ * called by:
+ *   - hw/xen/xen_pt.c|985| <<xen_igd_clear_slot>> pci_bus_clear_slot_reserved_mask(pci_bus, XEN_PCI_IGD_SLOT_MASK);
+ */
 void pci_bus_clear_slot_reserved_mask(PCIBus *bus, uint32_t mask)
 {
+    /*
+     * 在以下使用PCIBus->slot_reserved_mask:
+     *   - hw/pci/pci.c|511| <<pci_root_bus_internal_init>> bus->slot_reserved_mask = 0x0;
+     *   - hw/pci/pci.c|1130| <<pci_bus_devfn_reserved>> return bus->slot_reserved_mask & (1UL << PCI_SLOT(devfn));
+     *   - hw/pci/pci.c|1135| <<pci_bus_get_slot_reserved_mask>> return bus->slot_reserved_mask;
+     *   - hw/pci/pci.c|1140| <<pci_bus_set_slot_reserved_mask>> bus->slot_reserved_mask |= mask;
+     *   - hw/pci/pci.c|1149| <<pci_bus_clear_slot_reserved_mask>> bus->slot_reserved_mask &= ~mask;
+     */
     bus->slot_reserved_mask &= ~mask;
 }
 
+/*
+ * called by:
+ *   - hw/pci/pci.c|2086| <<pci_qdev_realize>> pci_dev = do_pci_register_device(pci_dev, object_get_typename(OBJECT(qdev)), pci_dev->devfn, errp);
+ */
 /* -1 for devfn means auto assign */
 static PCIDevice *do_pci_register_device(PCIDevice *pci_dev,
                                          const char *name, int devfn,
@@ -2028,6 +2089,10 @@ PCIDevice *pci_find_device(PCIBus *bus, int bus_num, uint8_t devfn)
 
 #define ONBOARD_INDEX_MAX (16 * 1024 - 1)
 
+/*
+ * 在以下使用pci_qdev_realize():
+ *   - hw/pci/pci.c|2632| <<pci_device_class_init>> k->realize = pci_qdev_realize;
+ */
 static void pci_qdev_realize(DeviceState *qdev, Error **errp)
 {
     PCIDevice *pci_dev = (PCIDevice *)qdev;
@@ -2767,6 +2832,12 @@ void pci_bus_get_w64_range(PCIBus *bus, Range *range)
     pci_for_each_device_under_bus(bus, pci_dev_get_w64, range);
 }
 
+/*
+ * called by:
+ *   - hw/pci/pci.c|2178| <<pci_qdev_realize>> if (pci_is_express(pci_dev) &&
+ *                     !pcie_find_capability(pci_dev, PCI_EXT_CAP_ID_ARI) && pcie_has_upstream_port(pci_dev) && PCI_SLOT(pci_dev->devfn)) {
+ *   - hw/pci/pci.c|2867| <<pci_get_function_0>> if(pcie_has_upstream_port(pci_dev)) {
+ */
 static bool pcie_has_upstream_port(PCIDevice *dev)
 {
     PCIDevice *parent_dev = pci_bridge_get_device(pci_get_bus(dev));
@@ -2783,6 +2854,20 @@ static bool pcie_has_upstream_port(PCIDevice *dev)
          pcie_cap_get_type(parent_dev) == PCI_EXP_TYPE_DOWNSTREAM);
 }
 
+/*
+ * called by:
+ *   - hw/isa/vt82c686.c|658| <<via_isa_set_irq>> ViaISAState *s = VIA_ISA(pci_get_function_0(d));
+ *   - hw/pci/pci.c|1207| <<do_pci_register_device>> else if (dev->hotplugged && !pci_is_vf(pci_dev) && pci_get_function_0(pci_dev)) {
+ *   - hw/pci/pci.c|1210| <<do_pci_register_device>> error_setg(errp, "PCI: slot %d function 0 already occupied by %s,"
+ *                              " new func %s cannot be exposed to guest.", PCI_SLOT(pci_get_function_0(pci_dev)->devfn), pci_get_function_0(pci_dev)->name, name);
+ *   - hw/pci/pci.c|1211| <<do_pci_register_device>> error_setg(errp, "PCI: slot %d function 0 already occupied by %s,"
+ *                              " new func %s cannot be exposed to guest.", PCI_SLOT(pci_get_function_0(pci_dev)->devfn), pci_get_function_0(pci_dev)->name, name);
+ *   - hw/pci/pci_host.c|88| <<pci_host_config_write_common>> if ((pci_dev->qdev.hotplugged && !pci_get_function_0(pci_dev)) || !pci_dev->has_power || is_pci_dev_ejected(pci_dev)) {
+ *   - hw/pci/pci_host.c|113| <<pci_host_config_read_common>> if ((pci_dev->qdev.hotplugged && !pci_get_function_0(pci_dev)) || !pci_dev->has_power || is_pci_dev_ejected(pci_dev)) {
+ *   - hw/pci/pcie.c|524| <<pcie_cap_slot_plug_cb>> if (pci_get_function_0(pci_dev)) {
+ *   - hw/ppc/spapr_pci.c|1582| <<spapr_pci_pre_plug>> pci_get_function_0(PCI_DEVICE(drc->dev))->name);
+ *   - hw/rdma/vmw/pvrdma_main.c|628| <<pvrdma_realize>> func0 = pci_get_function_0(pdev);
+ */
 PCIDevice *pci_get_function_0(PCIDevice *pci_dev)
 {
     PCIBus *bus = pci_get_bus(pci_dev);
diff --git a/hw/pci/pcie.c b/hw/pci/pcie.c
index 4b2f0805c..cfdf84d70 100644
--- a/hw/pci/pcie.c
+++ b/hw/pci/pcie.c
@@ -572,6 +572,10 @@ static void pcie_cap_slot_do_unplug(PCIDevice *dev)
                                PCI_EXP_SLTSTA_PDC);
 }
 
+/*
+ * 在以下使用pcie_cap_slot_unplug_request_cb():
+ *   - hw/pci/pcie_port.c|235| <<pcie_slot_class_init>> hc->unplug_request = pcie_cap_slot_unplug_request_cb;
+ */
 void pcie_cap_slot_unplug_request_cb(HotplugHandler *hotplug_dev,
                                      DeviceState *dev, Error **errp)
 {
diff --git a/hw/pci/pcie_port.c b/hw/pci/pcie_port.c
index 20ff2b39e..a6f17ae07 100644
--- a/hw/pci/pcie_port.c
+++ b/hw/pci/pcie_port.c
@@ -102,8 +102,22 @@ PCIESlot *pcie_chassis_find_slot(uint8_t chassis_number, uint16_t slot)
     return pcie_chassis_find_slot_with_chassis(c, slot);
 }
 
+/*
+ * called by:
+ *   - hw/pci-bridge/cxl_downstream.c|169| <<cxl_dsp_realize>> rc = pcie_chassis_add_slot(s);
+ *   - hw/pci-bridge/pcie_root_port.c|106| <<rp_realize>> rc = pcie_chassis_add_slot(s);
+ *   - hw/pci-bridge/xio3130_downstream.c|102| <<xio3130_downstream_realize>> rc = pcie_chassis_add_slot(s);
+ */
 int pcie_chassis_add_slot(struct PCIESlot *slot)
 {
+    /*
+     * struct PCIEChassis {
+     *     uint8_t     number;
+     *
+     *     QLIST_HEAD(, PCIESlot) slots;
+     *     QLIST_ENTRY(PCIEChassis) next;
+     * };
+     */
     struct PCIEChassis *c;
     c = pcie_chassis_find(slot->chassis);
     if (!c) {
diff --git a/hw/scsi/scsi-disk.c b/hw/scsi/scsi-disk.c
index 4bd7af9d0..1f02721ca 100644
--- a/hw/scsi/scsi-disk.c
+++ b/hw/scsi/scsi-disk.c
@@ -2887,6 +2887,17 @@ static BlockAIOCB *scsi_block_do_sgio(SCSIBlockReq *req,
     /* The rest is as in scsi-generic.c.  */
     io_header->mx_sb_len = sizeof(r->req.sense);
     io_header->sbp = r->req.sense;
+    /*
+     * sg_io_hd_t *io_header:
+     * -> unsigned int timeout;
+     *
+     * SCSIDiskState *s:
+     * -> SCSIDevice qdev;
+     *    -> QTAILQ_HEAD(, SCSIRequest) requests;
+     *    -> uint32_t channel;
+     *    -> uint32_t lun;
+     *    -> uint32_t io_timeout;
+     */
     io_header->timeout = s->qdev.io_timeout * 1000;
     io_header->usr_ptr = r;
     io_header->flags |= SG_FLAG_DIRECT_IO;
diff --git a/hw/vfio/helpers.c b/hw/vfio/helpers.c
index 47b4096c0..f3b43ed52 100644
--- a/hw/vfio/helpers.c
+++ b/hw/vfio/helpers.c
@@ -107,6 +107,23 @@ static const char *index_to_str(VFIODevice *vbasedev, int index)
     }
 }
 
+/*
+ * called by:
+ *   - hw/vfio/ap.c|120| <<vfio_ap_register_irq_notifier>> if (vfio_set_irq_signaling(vdev, irq, 0, VFIO_IRQ_SET_ACTION_TRIGGER, fd, errp)) {
+ *   - hw/vfio/ap.c|146| <<vfio_ap_unregister_irq_notifier>> if (vfio_set_irq_signaling(&vapdev->vdev, irq, 0, VFIO_IRQ_SET_ACTION_TRIGGER, -1, &err)) {
+ *   - hw/vfio/ccw.c|437| <<vfio_ccw_register_irq_notifier>> if (vfio_set_irq_signaling(vdev, irq, 0, VFIO_IRQ_SET_ACTION_TRIGGER, fd, errp)) {
+ *   - hw/vfio/ccw.c|468| <<vfio_ccw_unregister_irq_notifier>> if (vfio_set_irq_signaling(&vcdev->vdev, irq, 0, VFIO_IRQ_SET_ACTION_TRIGGER, -1, &err)) {
+ *   - hw/vfio/pci.c|150| <<vfio_intx_enable_kvm>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_INTX_IRQ_INDEX, 0, VFIO_IRQ_SET_ACTION_UNMASK, event_notifier_get_fd(&vdev->intx.unmask), errp)) {
+ *   - hw/vfio/pci.c|298| <<vfio_intx_enable>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_INTX_IRQ_INDEX, 0, VFIO_IRQ_SET_ACTION_TRIGGER, fd, errp)) {
+ *   - hw/vfio/pci.c|593| <<vfio_msix_vector_do_use>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_MSIX_IRQ_INDEX, nr, VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err)) {
+ *   - hw/vfio/pci.c|647| <<vfio_msix_vector_release>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_MSIX_IRQ_INDEX, nr, VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err)) {
+ *   - hw/vfio/pci.c|2860| <<vfio_register_err_notifier>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_ERR_IRQ_INDEX, 0, VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err)) {
+ *   - hw/vfio/pci.c|2877| <<vfio_unregister_err_notifier>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_ERR_IRQ_INDEX, 0, VFIO_IRQ_SET_ACTION_TRIGGER, -1, &err)) {
+ *   - hw/vfio/pci.c|2925| <<vfio_register_req_notifier>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_REQ_IRQ_INDEX, 0, VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err)) {
+ *   - hw/vfio/pci.c|2943| <<vfio_unregister_req_notifier>> if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_REQ_IRQ_INDEX, 0, VFIO_IRQ_SET_ACTION_TRIGGER, -1, &err)) {
+ *   - hw/vfio/platform.c|122| <<vfio_set_trigger_eventfd>> ret = vfio_set_irq_signaling(vbasedev, intp->pin, 0, VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err);
+ *   - hw/vfio/platform.c|361| <<vfio_set_resample_eventfd>> ret = vfio_set_irq_signaling(vbasedev, intp->pin, 0, VFIO_IRQ_SET_ACTION_UNMASK, fd, &err);
+ */
 int vfio_set_irq_signaling(VFIODevice *vbasedev, int index, int subindex,
                            int action, int fd, Error **errp)
 {
diff --git a/hw/vfio/pci.c b/hw/vfio/pci.c
index 64780d1b7..aab0eb393 100644
--- a/hw/vfio/pci.c
+++ b/hw/vfio/pci.c
@@ -356,7 +356,19 @@ static void vfio_msi_interrupt(void *opaque)
         /* A masked vector firing needs to use the PBA, enable it */
         if (msix_is_masked(&vdev->pdev, nr)) {
             set_bit(nr, vdev->msix->pending);
+            /*
+	     * 在以下使用PCIDevice->msix_pba_mmio:
+	     *   - hw/pci/msix.c|416| <<msix_init>> memory_region_init_io(&dev->msix_pba_mmio, OBJECT(dev), &msix_pba_mmio_ops, dev, "msix-pba", pba_size);
+	     *   - hw/pci/msix.c|418| <<msix_init>> memory_region_add_subregion(pba_bar, pba_offset, &dev->msix_pba_mmio);
+	     *   - hw/pci/msix.c|498| <<msix_uninit>> memory_region_del_subregion(pba_bar, &dev->msix_pba_mmio);
+	     *   - hw/vfio/pci.c|359| <<vfio_msi_interrupt>> memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, true);
+	     *   - hw/vfio/pci.c|632| <<vfio_msix_vector_do_use>> memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, false);
+	     *   - hw/vfio/pci.c|1737| <<vfio_msix_setup>> memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, false);
+	     */
             memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, true);
+            /*
+	     * 只在这里trace!
+	     */
             trace_vfio_msix_pba_enable(vdev->vbasedev.name);
         }
     } else if (vdev->interrupt == VFIO_INT_MSI) {
@@ -470,6 +482,11 @@ static void vfio_add_kvm_msi_virq(VFIOPCIDevice *vdev, VFIOMSIVector *vector,
                                              vector_n, &vdev->pdev);
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|565| <<vfio_msix_vector_do_use>> vfio_connect_kvm_msi_virq(vector);
+ *   - hw/vfio/pci.c|692| <<vfio_commit_kvm_msi_virq_batch>> vfio_connect_kvm_msi_virq(&vdev->msi_vectors[i]);
+ */
 static void vfio_connect_kvm_msi_virq(VFIOMSIVector *vector)
 {
     if (vector->virq < 0) {
@@ -503,6 +520,11 @@ static void vfio_remove_kvm_msi_virq(VFIOMSIVector *vector)
     event_notifier_cleanup(&vector->kvm_interrupt);
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|550| <<vfio_msix_vector_do_use>> vfio_update_kvm_msi_virq(vector, *msg, pdev);
+ *   - hw/vfio/pci.c|897| <<vfio_update_msi>> vfio_update_kvm_msi_virq(vector, msg, &vdev->pdev);
+ */
 static void vfio_update_kvm_msi_virq(VFIOMSIVector *vector, MSIMessage msg,
                                      PCIDevice *pdev)
 {
@@ -510,6 +532,10 @@ static void vfio_update_kvm_msi_virq(VFIOMSIVector *vector, MSIMessage msg,
     kvm_irqchip_commit_routes(kvm_state);
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|615| <<vfio_msix_vector_use>> return vfio_msix_vector_do_use(pdev, nr, &msg, vfio_msi_interrupt);
+ */
 static int vfio_msix_vector_do_use(PCIDevice *pdev, unsigned int nr,
                                    MSIMessage *msg, IOHandler *handler)
 {
@@ -547,6 +573,15 @@ static int vfio_msix_vector_do_use(PCIDevice *pdev, unsigned int nr,
         }
     } else {
         if (msg) {
+            /*
+	     * 在以下使用VFIOPCIDevice->defer_kvm_irq_routing:
+	     *   - hw/vfio/pci.c|576| <<vfio_msix_vector_do_use>> if (vdev->defer_kvm_irq_routing) {
+	     *   - hw/vfio/pci.c|602| <<vfio_msix_vector_do_use>> if (!vdev->defer_kvm_irq_routing) {
+	     *   - hw/vfio/pci.c|711| <<vfio_prepare_kvm_msi_virq_batch>> assert(!vdev->defer_kvm_irq_routing);
+	     *   - hw/vfio/pci.c|712| <<vfio_prepare_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = true;
+	     *   - hw/vfio/pci.c|720| <<vfio_commit_kvm_msi_virq_batch>> assert(vdev->defer_kvm_irq_routing);
+	     *   - hw/vfio/pci.c|721| <<vfio_commit_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = false;
+	     */
             if (vdev->defer_kvm_irq_routing) {
                 vfio_add_kvm_msi_virq(vdev, vector, nr, true);
             } else {
@@ -573,6 +608,15 @@ static int vfio_msix_vector_do_use(PCIDevice *pdev, unsigned int nr,
         vdev->nr_vectors = nr + 1;
     }
 
+    /*
+     * 在以下使用VFIOPCIDevice->defer_kvm_irq_routing:
+     *   - hw/vfio/pci.c|576| <<vfio_msix_vector_do_use>> if (vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|602| <<vfio_msix_vector_do_use>> if (!vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|711| <<vfio_prepare_kvm_msi_virq_batch>> assert(!vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|712| <<vfio_prepare_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = true;
+     *   - hw/vfio/pci.c|720| <<vfio_commit_kvm_msi_virq_batch>> assert(vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|721| <<vfio_commit_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = false;
+     */
     if (!vdev->defer_kvm_irq_routing) {
         if (vdev->msix->noresize && resizing) {
             vfio_disable_irqindex(&vdev->vbasedev, VFIO_PCI_MSIX_IRQ_INDEX);
@@ -584,6 +628,16 @@ static int vfio_msix_vector_do_use(PCIDevice *pdev, unsigned int nr,
             Error *err = NULL;
             int32_t fd;
 
+	    /*
+	     * 注释
+	     *  Two interrupt paths are configured per vector.  The first, is only used
+	     * for interrupts injected via QEMU.  This is typically the non-accel path,
+	     * but may also be used when we want QEMU to handle masking and pending
+	     * bits.  The KVM path bypasses QEMU and is therefore higher performance,
+	     * but requires masking at the device.  virq is used to track the MSI route
+	     * through KVM, thus kvm_interrupt is only available when virq is set to a
+	     * valid (>= 0) value.
+	     */
             if (vector->virq >= 0) {
                 fd = event_notifier_get_fd(&vector->kvm_interrupt);
             } else {
@@ -598,6 +652,13 @@ static int vfio_msix_vector_do_use(PCIDevice *pdev, unsigned int nr,
         }
     }
 
+    /*
+     * 这里会改变memslots!!!
+     *
+     * VFIOPCIDevice:
+     * -> VFIOMSIXInfo *msix;
+     *    -> unsigned long *pending;
+     */
     /* Disable PBA emulation when nothing more is pending. */
     clear_bit(nr, vdev->msix->pending);
     if (find_first_bit(vdev->msix->pending,
@@ -609,12 +670,34 @@ static int vfio_msix_vector_do_use(PCIDevice *pdev, unsigned int nr,
     return 0;
 }
 
+/*
+ * 在以下调用msix_set_vector_notifiers():
+ *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev, ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+ *   - hw/vfio/pci.c|683| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+ *   - hw/virtio/virtio-pci.c|1291| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev, virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+ *
+ * 在以下使用vfio_msix_vector_use():
+ *   - hw/vfio/pci.c|693| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+ */
 static int vfio_msix_vector_use(PCIDevice *pdev,
                                 unsigned int nr, MSIMessage msg)
 {
+    /*
+     * 只在此处调用
+     */
     return vfio_msix_vector_do_use(pdev, nr, &msg, vfio_msi_interrupt);
 }
 
+/*
+ * 在以下调用msix_set_vector_notifiers():
+ *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev, ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+ *   - hw/vfio/pci.c|683| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+ *   - hw/virtio/virtio-pci.c|1291| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev, virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+ *
+ * 在以下使用vfio_msix_vector_release():
+ *   - hw/vfio/pci.c|684| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev, vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+ *   - hw/vfio/pci.c|824| <<vfio_msix_disable>> vfio_msix_vector_release(&vdev->pdev, i);
+ */
 static void vfio_msix_vector_release(PCIDevice *pdev, unsigned int nr)
 {
     VFIOPCIDevice *vdev = VFIO_PCI(pdev);
@@ -634,6 +717,10 @@ static void vfio_msix_vector_release(PCIDevice *pdev, unsigned int nr)
         int32_t fd = event_notifier_get_fd(&vector->interrupt);
         Error *err = NULL;
 
+        /*
+	 * 这是use的情况.
+	 * vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_MSIX_IRQ_INDEX, nr, VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err)
+	 */
         if (vfio_set_irq_signaling(&vdev->vbasedev, VFIO_PCI_MSIX_IRQ_INDEX, nr,
                                    VFIO_IRQ_SET_ACTION_TRIGGER, fd, &err)) {
             error_reportf_err(err, VFIO_MSG_PREFIX, vdev->vbasedev.name);
@@ -641,8 +728,22 @@ static void vfio_msix_vector_release(PCIDevice *pdev, unsigned int nr)
     }
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|759| <<vfio_msix_enable>> vfio_prepare_kvm_msi_virq_batch(vdev);
+ *   - hw/vfio/pci.c|808| <<vfio_msi_enable>> vfio_prepare_kvm_msi_virq_batch(vdev);
+ */
 static void vfio_prepare_kvm_msi_virq_batch(VFIOPCIDevice *vdev)
 {
+    /*
+     * 在以下使用VFIOPCIDevice->defer_kvm_irq_routing:
+     *   - hw/vfio/pci.c|576| <<vfio_msix_vector_do_use>> if (vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|602| <<vfio_msix_vector_do_use>> if (!vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|711| <<vfio_prepare_kvm_msi_virq_batch>> assert(!vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|712| <<vfio_prepare_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = true;
+     *   - hw/vfio/pci.c|720| <<vfio_commit_kvm_msi_virq_batch>> assert(vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|721| <<vfio_commit_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = false;
+     */
     assert(!vdev->defer_kvm_irq_routing);
     vdev->defer_kvm_irq_routing = true;
     vfio_route_change = kvm_irqchip_begin_route_changes(kvm_state);
@@ -652,6 +753,15 @@ static void vfio_commit_kvm_msi_virq_batch(VFIOPCIDevice *vdev)
 {
     int i;
 
+    /*
+     * 在以下使用VFIOPCIDevice->defer_kvm_irq_routing:
+     *   - hw/vfio/pci.c|576| <<vfio_msix_vector_do_use>> if (vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|602| <<vfio_msix_vector_do_use>> if (!vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|711| <<vfio_prepare_kvm_msi_virq_batch>> assert(!vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|712| <<vfio_prepare_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = true;
+     *   - hw/vfio/pci.c|720| <<vfio_commit_kvm_msi_virq_batch>> assert(vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|721| <<vfio_commit_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = false;
+     */
     assert(vdev->defer_kvm_irq_routing);
     vdev->defer_kvm_irq_routing = false;
 
@@ -662,6 +772,11 @@ static void vfio_commit_kvm_msi_virq_batch(VFIOPCIDevice *vdev)
     }
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|1373| <<vfio_pci_write_config>> vfio_msix_enable(vdev);
+ *   - hw/vfio/pci.c|2712| <<vfio_pci_load_config>> vfio_msix_enable(vdev);
+ */
 static void vfio_msix_enable(VFIOPCIDevice *vdev)
 {
     int ret;
@@ -858,6 +973,10 @@ static void vfio_msi_disable(VFIOPCIDevice *vdev)
     trace_vfio_msi_disable(vdev->vbasedev.name);
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci.c|1302| <<vfio_pci_write_config>> vfio_update_msi(vdev);
+ */
 static void vfio_update_msi(VFIOPCIDevice *vdev)
 {
     int i;
@@ -1244,6 +1363,19 @@ uint32_t vfio_pci_read_config(PCIDevice *pdev, uint32_t addr, int len)
     return val;
 }
 
+/*
+ * called by:
+ *   - hw/vfio/pci-quirks.c|171| <<vfio_generic_window_quirk_data_write>> vfio_pci_write_config(&vdev->pdev, window->address_val, data, size);
+ *   - hw/vfio/pci-quirks.c|225| <<vfio_generic_quirk_mirror_write>> vfio_pci_write_config(&vdev->pdev, addr, data, size);
+ *   - hw/vfio/pci-quirks.c|647| <<vfio_nvidia_3d0_quirk_write>> vfio_pci_write_config(&vdev->pdev, offset, data, size);
+ *   - hw/vfio/pci-quirks.c|1396| <<vfio_radeon_reset>> vfio_pci_write_config(pdev, PCI_COMMAND, PCI_COMMAND_MEMORY, 2);
+ *   - hw/vfio/pci-quirks.c|1409| <<vfio_radeon_reset>> vfio_pci_write_config(pdev, 0x7c, 0x39d5e86b, 4);
+ *   - hw/vfio/pci-quirks.c|1439| <<vfio_radeon_reset>> vfio_pci_write_config(pdev, PCI_COMMAND, 0, 2);
+ *   - hw/vfio/pci.c|2480| <<vfio_pci_pre_reset>> vfio_pci_write_config(pdev, vdev->pm_cap + PCI_PM_CTRL, pmcsr, 2);
+ *   - hw/vfio/pci.c|2498| <<vfio_pci_pre_reset>> vfio_pci_write_config(pdev, PCI_COMMAND, cmd, 2);
+ *   - hw/vfio/pci.c|2694| <<vfio_pci_load_config>> vfio_pci_write_config(pdev, PCI_COMMAND,
+ *   - hw/vfio/pci.c|3500| <<vfio_pci_dev_class_init>> pdc->config_write = vfio_pci_write_config;
+ */
 void vfio_pci_write_config(PCIDevice *pdev,
                            uint32_t addr, uint32_t val, int len)
 {
diff --git a/hw/vfio/pci.h b/hw/vfio/pci.h
index 6e64a2654..d1793ae10 100644
--- a/hw/vfio/pci.h
+++ b/hw/vfio/pci.h
@@ -175,6 +175,15 @@ struct VFIOPCIDevice {
     bool no_vfio_ioeventfd;
     bool enable_ramfb;
     OnOffAuto ramfb_migrate;
+    /*
+     * 在以下使用VFIOPCIDevice->defer_kvm_irq_routing:
+     *   - hw/vfio/pci.c|576| <<vfio_msix_vector_do_use>> if (vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|602| <<vfio_msix_vector_do_use>> if (!vdev->defer_kvm_irq_routing) {
+     *   - hw/vfio/pci.c|711| <<vfio_prepare_kvm_msi_virq_batch>> assert(!vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|712| <<vfio_prepare_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = true;
+     *   - hw/vfio/pci.c|720| <<vfio_commit_kvm_msi_virq_batch>> assert(vdev->defer_kvm_irq_routing);
+     *   - hw/vfio/pci.c|721| <<vfio_commit_kvm_msi_virq_batch>> vdev->defer_kvm_irq_routing = false;
+     */
     bool defer_kvm_irq_routing;
     bool clear_parent_atomics_on_exit;
     VFIODisplay *dpy;
diff --git a/hw/virtio/virtio-bus.c b/hw/virtio/virtio-bus.c
index 896feb37a..81b9d782d 100644
--- a/hw/virtio/virtio-bus.c
+++ b/hw/virtio/virtio-bus.c
@@ -216,6 +216,14 @@ void virtio_bus_release_ioeventfd(VirtioBusState *bus)
     }
 }
 
+/*
+ * 在以下使用virtio_bus_start_ioeventfd():
+ *   - hw/s390x/virtio-ccw.c|136| <<virtio_ccw_start_ioeventfd>> virtio_bus_start_ioeventfd(&dev->bus);
+ *   - hw/virtio/virtio-bus.c|215| <<virtio_bus_release_ioeventfd>> virtio_bus_start_ioeventfd(bus);
+ *   - hw/virtio/virtio-mmio.c|63| <<virtio_mmio_start_ioeventfd>> virtio_bus_start_ioeventfd(&proxy->bus);
+ *   - hw/virtio/virtio-pci.c|375| <<virtio_pci_start_ioeventfd>> virtio_bus_start_ioeventfd(&proxy->bus);
+ *   - hw/virtio/virtio.c|3902| <<virtio_device_start_ioeventfd>> return virtio_bus_start_ioeventfd(vbus);
+ */
 int virtio_bus_start_ioeventfd(VirtioBusState *bus)
 {
     VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(bus);
@@ -269,6 +277,20 @@ bool virtio_bus_ioeventfd_enabled(VirtioBusState *bus)
     return k->ioeventfd_assign && k->ioeventfd_enabled(proxy);
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1807| <<virtio_blk_start_ioeventfd>> r = virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, true);
+ *   - hw/block/virtio-blk.c|1813| <<virtio_blk_start_ioeventfd>> virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, false);
+ *   - hw/block/virtio-blk.c|1928| <<virtio_blk_stop_ioeventfd>> virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, false);
+ *   - hw/scsi/virtio-scsi-dataplane.c|57| <<virtio_scsi_set_host_notifier>> rc = virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), n, true);
+ *   - hw/scsi/virtio-scsi-dataplane.c|168| <<virtio_scsi_dataplane_start>> virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, false);
+ *   - hw/scsi/virtio-scsi-dataplane.c|222| <<virtio_scsi_dataplane_stop>> virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), i, false);
+ *   - hw/virtio/vhost.c|1624| <<vhost_dev_disable_notifiers_nvqs>> r = virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), hdev->vq_index + i, false);
+ *   - hw/virtio/vhost.c|1668| <<vhost_dev_enable_notifiers>> r = virtio_bus_set_host_notifier(VIRTIO_BUS(qbus), hdev->vq_index + i, true);
+ *   - hw/virtio/virtio.c|3850| <<virtio_device_start_ioeventfd_impl>> r = virtio_bus_set_host_notifier(qbus, n, true);
+ *   - hw/virtio/virtio.c|3879| <<virtio_device_start_ioeventfd_impl>> r = virtio_bus_set_host_notifier(qbus, n, false);
+ *   - hw/virtio/virtio.c|3922| <<virtio_device_stop_ioeventfd_impl>> r = virtio_bus_set_host_notifier(qbus, n, false);
+ */
 /*
  * This function switches ioeventfd on/off in the device.
  * The caller must set or clear the handlers for the EventNotifier.
diff --git a/hw/virtio/virtio-pci.c b/hw/virtio/virtio-pci.c
index cb159fd07..c660e8b68 100644
--- a/hw/virtio/virtio-pci.c
+++ b/hw/virtio/virtio-pci.c
@@ -333,6 +333,12 @@ static int virtio_pci_ioeventfd_assign(DeviceState *d, EventNotifier *notifier,
     bool legacy = virtio_pci_legacy(proxy);
     bool modern = virtio_pci_modern(proxy);
     bool modern_pio = proxy->flags & VIRTIO_PCI_FLAG_MODERN_PIO_NOTIFY;
+    /*
+     * VirtIOPCIProxy *proxy:
+     * -> MemoryRegion bar;
+     * -> VirtIOPCIRegion notify;
+     * -> VirtIOPCIRegion notify_pio;
+     */
     MemoryRegion *modern_mr = &proxy->notify.mr;
     MemoryRegion *modern_notify_mr = &proxy->notify_pio.mr;
     MemoryRegion *legacy_mr = &proxy->bar;
@@ -370,6 +376,12 @@ static int virtio_pci_ioeventfd_assign(DeviceState *d, EventNotifier *notifier,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio-pci.c|423| <<virtio_ioport_write(VIRTIO_PCI_STATUS/VIRTIO_CONFIG_S_DRIVER_OK)>> virtio_pci_start_ioeventfd(proxy);
+ *   - hw/virtio/virtio-pci.c|1368| <<virtio_pci_vmstate_change>> virtio_pci_start_ioeventfd(proxy);
+ *   - hw/virtio/virtio-pci.c|1623| <<virtio_pci_common_write(VIRTIO_PCI_COMMON_STATUS/VIRTIO_CONFIG_S_DRIVER_OK)>> virtio_pci_start_ioeventfd(proxy);
+ */
 static void virtio_pci_start_ioeventfd(VirtIOPCIProxy *proxy)
 {
     virtio_bus_start_ioeventfd(&proxy->bus);
@@ -836,6 +848,11 @@ static void kvm_virtio_pci_vq_vector_release(VirtIOPCIProxy *proxy,
     }
 }
 
+/*
+ * 在以下使用kvm_virtio_pci_irqfd_use():
+ *   - hw/virtio/virtio-pci.c|902| <<kvm_virtio_pci_vector_use_one>> ret = kvm_virtio_pci_irqfd_use(proxy, n, vector);
+ *   - hw/virtio/virtio-pci.c|1020| <<virtio_pci_one_vector_unmask>> ret = kvm_virtio_pci_irqfd_use(proxy, n, vector);
+ */
 static int kvm_virtio_pci_irqfd_use(VirtIOPCIProxy *proxy,
                                  EventNotifier *n,
                                  unsigned int vector)
@@ -874,6 +891,12 @@ static int virtio_pci_get_notifier(VirtIOPCIProxy *proxy, int queue_no,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio-pci.c|935| <<kvm_virtio_pci_vector_vq_use>> ret = kvm_virtio_pci_vector_use_one(proxy, queue_no);
+ *   - hw/virtio/virtio-pci.c|942| <<kvm_virtio_pci_vector_config_use>> return kvm_virtio_pci_vector_use_one(proxy, VIRTIO_CONFIG_IRQ_IDX);
+ *   - hw/virtio/virtio-pci.c|1462| <<virtio_pci_set_vector>> kvm_virtio_pci_vector_use_one(proxy, queue_no);
+ */
 static int kvm_virtio_pci_vector_use_one(VirtIOPCIProxy *proxy, int queue_no)
 {
     unsigned int vector;
@@ -922,6 +945,10 @@ undo:
     }
     return ret;
 }
+/*
+ * called by:
+ *   - hw/virtio/virtio-pci.c|1288| <<virtio_pci_set_guest_notifiers>> r = kvm_virtio_pci_vector_vq_use(proxy, nvqs);
+ */
 static int kvm_virtio_pci_vector_vq_use(VirtIOPCIProxy *proxy, int nvqs)
 {
     int queue_no;
@@ -1176,6 +1203,13 @@ void virtio_pci_set_guest_notifier_fd_handler(VirtIODevice *vdev, VirtQueue *vq,
     }
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio-pci.c|1263| <<virtio_pci_set_guest_notifiers>> r = virtio_pci_set_guest_notifier(d, n, assign, with_irqfd);
+ *   - hw/virtio/virtio-pci.c|1268| <<virtio_pci_set_guest_notifiers>> r = virtio_pci_set_guest_notifier(d, VIRTIO_CONFIG_IRQ_IDX, assign, with_irqfd);
+ *   - hw/virtio/virtio-pci.c|1311| <<virtio_pci_set_guest_notifiers>> virtio_pci_set_guest_notifier(d, VIRTIO_CONFIG_IRQ_IDX, !assign, with_irqfd);
+ *   - hw/virtio/virtio-pci.c|1317| <<virtio_pci_set_guest_notifiers>> virtio_pci_set_guest_notifier(d, n, !assign, with_irqfd);
+ */
 static int virtio_pci_set_guest_notifier(DeviceState *d, int n, bool assign,
                                          bool with_irqfd)
 {
diff --git a/hw/virtio/virtio.c b/hw/virtio/virtio.c
index 871674f9b..a889259a6 100644
--- a/hw/virtio/virtio.c
+++ b/hw/virtio/virtio.c
@@ -148,7 +148,30 @@ struct VirtQueue
     uint16_t vector;
     VirtIOHandleOutput handle_output;
     VirtIODevice *vdev;
+    /*
+     * 在以下使用VirtQueue->guest_notifier:
+     *   - hw/virtio/virtio.c|2493| <<virtio_notify_irqfd>> defer_call(virtio_notify_irqfd_deferred_fn, &vq->guest_notifier);
+     *   - hw/virtio/virtio.c|3501| <<virtio_queue_set_guest_notifier_fd_handler>> event_notifier_set_handler(&vq->guest_notifier, virtio_queue_guest_notifier_read);
+     *   - hw/virtio/virtio.c|3504| <<virtio_queue_set_guest_notifier_fd_handler>> event_notifier_set_handler(&vq->guest_notifier, NULL);
+     *   - hw/virtio/virtio.c|3509| <<virtio_queue_set_guest_notifier_fd_handler>> virtio_queue_guest_notifier_read(&vq->guest_notifier);
+     *   - hw/virtio/virtio.c|3532| <<virtio_queue_get_guest_notifier>> return &vq->guest_notifier;
+     */
     EventNotifier guest_notifier;
+    /*
+     * 在以下使用VirtQueue->host_notifier:
+     *   - hw/virtio/virtio.c|2294| <<virtio_queue_notify>> event_notifier_set(&vq->host_notifier);
+     *   - hw/virtio/virtio.c|3578| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *   - hw/virtio/virtio.c|3582| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier_poll(ctx, &vq->host_notifier,
+     *   - hw/virtio/virtio.c|3591| <<virtio_queue_aio_attach_host_notifier>> event_notifier_set(&vq->host_notifier);
+     *   - hw/virtio/virtio.c|3627| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx, &vq->host_notifier,
+     *   - hw/virtio/virtio.c|3637| <<virtio_queue_aio_attach_host_notifier_no_poll>> event_notifier_set(&vq->host_notifier);
+     *   - hw/virtio/virtio.c|3642| <<virtio_queue_aio_detach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier, NULL, NULL, NULL);
+     *   - hw/virtio/virtio.c|3666| <<virtio_queue_get_host_notifier>> return &vq->host_notifier;
+     *   - hw/virtio/virtio.c|3827| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier,
+     *   - hw/virtio/virtio.c|3837| <<virtio_device_start_ioeventfd_impl>> event_notifier_set(&vq->host_notifier);
+     *   - hw/virtio/virtio.c|3850| <<virtio_device_start_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier, NULL);
+     *   - hw/virtio/virtio.c|3893| <<virtio_device_stop_ioeventfd_impl>> event_notifier_set_handler(&vq->host_notifier, NULL);
+     */
     EventNotifier host_notifier;
     bool host_notifier_enabled;
     QLIST_ENTRY(VirtQueue) node;
@@ -995,6 +1018,46 @@ void virtqueue_flush(VirtQueue *vq, unsigned int count)
     }
 }
 
+/*
+ * called by:
+ *   - hw/9pfs/virtio-9p-device.c|38| <<virtio_9p_push_and_notify>> virtqueue_push(v->vq, elem, pdu->size);
+ *   - hw/audio/virtio-snd.c|749| <<process_cmd>> virtqueue_push(cmd->vq, cmd->elem, sizeof(virtio_snd_hdr) + cmd->payload_size);
+ *   - hw/audio/virtio-snd.c|852| <<empty_invalid_queue>> virtqueue_push(vq, buffer->elem, sizeof(virtio_snd_pcm_status));
+ *   - hw/audio/virtio-snd.c|1149| <<return_tx_buffer>> virtqueue_push(buffer->vq, buffer->elem, sizeof(virtio_snd_pcm_status));
+ *   - hw/audio/virtio-snd.c|1242| <<return_rx_buffer>> virtqueue_push(buffer->vq, buffer->elem, sizeof(virtio_snd_pcm_status) + buffer->size);
+ *   - hw/block/virtio-blk.c|68| <<virtio_blk_req_complete>> virtqueue_push(req->vq, &req->elem, req->in_len);
+ *   - hw/char/virtio-serial-bus.c|125| <<write_to_port>> virtqueue_push(vq, elem, len);
+ *   - hw/char/virtio-serial-bus.c|145| <<discard_vq_data>> virtqueue_push(vq, elem, 0);
+ *   - hw/char/virtio-serial-bus.c|207| <<do_flush_queued_data>> virtqueue_push(vq, port->elem, 0);
+ *   - hw/char/virtio-serial-bus.c|242| <<send_control_msg>> virtqueue_push(vq, elem, len);
+ *   - hw/char/virtio-serial-bus.c|491| <<control_out>> virtqueue_push(vq, elem, 0);
+ *   - hw/display/virtio-gpu.c|179| <<virtio_gpu_ctrl_response>> virtqueue_push(cmd->vq, &cmd->elem, s);
+ *   - hw/display/virtio-gpu.c|1157| <<virtio_gpu_handle_cursor>> virtqueue_push(vq, elem, 0);
+ *   - hw/input/virtio-input.c|65| <<virtio_input_send>> virtqueue_push(vinput->evt, elem, len);
+ *   - hw/input/virtio-input.c|97| <<virtio_input_handle_sts>> virtqueue_push(vinput->sts, elem, len);
+ *   - hw/net/virtio-net.c|1638| <<virtio_net_handle_ctrl>> virtqueue_push(vq, elem, written);
+ *   - hw/net/virtio-net.c|2695| <<virtio_net_tx_complete>> virtqueue_push(q->tx_vq, q->async_tx.elem, 0);
+ *   - hw/net/virtio-net.c|2813| <<virtio_net_flush_tx>> virtqueue_push(q->tx_vq, elem, 0);
+ *   - hw/scsi/virtio-scsi.c|112| <<virtio_scsi_complete_req>> virtqueue_push(vq, &req->elem, req->qsgl.size + req->resp_iov.size);
+ *   - hw/virtio/vhost-shadow-virtqueue.c|465| <<vhost_svq_push_elem>> virtqueue_push(svq->vq, elem, len);
+ *   - hw/virtio/vhost-vsock-common.c|185| <<vhost_vsock_common_send_transport_reset>> virtqueue_push(vq, elem, sizeof(event));
+ *   - hw/virtio/virtio-balloon.c|234| <<balloon_stats_poll_cb>> virtqueue_push(s->svq, s->stats_vq_elem, 0);
+ *   - hw/virtio/virtio-balloon.c|381| <<virtio_balloon_handle_report>> virtqueue_push(vq, elem, 0);
+ *   - hw/virtio/virtio-balloon.c|439| <<virtio_balloon_handle_output>> virtqueue_push(vq, elem, 0);
+ *   - hw/virtio/virtio-balloon.c|460| <<virtio_balloon_receive_stats>> virtqueue_push(vq, s->stats_vq_elem, 0);
+ *   - hw/virtio/virtio-balloon.c|546| <<get_free_page_hints>> virtqueue_push(vq, elem, 0);
+ *   - hw/virtio/virtio-crypto.c|300| <<virtio_crypto_create_session_completion>> virtqueue_push(vq, elem, sizeof(input));
+ *   - hw/virtio/virtio-crypto.c|330| <<virtio_crypto_destroy_session_completion>> virtqueue_push(vq, elem, sizeof(status));
+ *   - hw/virtio/virtio-crypto.c|443| <<virtio_crypto_handle_ctrl>> virtqueue_push(vq, elem, sizeof(input));
+ *   - hw/virtio/virtio-crypto.c|582| <<virtio_crypto_req_complete>> virtqueue_push(req->vq, &req->elem, req->in_len);
+ *   - hw/virtio/virtio-iommu.c|823| <<virtio_iommu_handle_command>> virtqueue_push(vq, elem, sz);
+ *   - hw/virtio/virtio-iommu.c|867| <<virtio_iommu_report_fault>> virtqueue_push(vq, elem, sz);
+ *   - hw/virtio/virtio-mem.c|460| <<virtio_mem_send_response>> virtqueue_push(vq, elem, sizeof(*resp));
+ *   - hw/virtio/virtio-pmem.c|62| <<done_cb>> virtqueue_push(req_data->pmem->rq_vq, &req_data->elem, len);
+ *   - hw/virtio/virtio-rng.c|81| <<chr_read>> virtqueue_push(vrng->vq, elem, len);
+ *   - hw/virtio/virtio.c|1859| <<virtqueue_packed_drop_all>> virtqueue_push(vq, &elem, 0);
+ *   - hw/virtio/virtio.c|1892| <<virtqueue_split_drop_all>> virtqueue_push(vq, &elem, 0);
+ */
 void virtqueue_push(VirtQueue *vq, const VirtQueueElement *elem,
                     unsigned int len)
 {
@@ -3527,6 +3590,19 @@ void virtio_config_set_guest_notifier_fd_handler(VirtIODevice *vdev,
     }
 }
 
+/*
+ * called by:
+ *   - hw/s390x/virtio-ccw.c|1001| <<virtio_ccw_add_irqfd>> EventNotifier *notifier = virtio_queue_get_guest_notifier(vq);
+ *   - hw/s390x/virtio-ccw.c|1011| <<virtio_ccw_remove_irqfd>> EventNotifier *notifier = virtio_queue_get_guest_notifier(vq);
+ *   - hw/s390x/virtio-ccw.c|1024| <<virtio_ccw_set_guest_notifier>> EventNotifier *notifier = virtio_queue_get_guest_notifier(vq);
+ *   - hw/virtio/vhost.c|1718| <<vhost_virtqueue_mask>> file.fd = event_notifier_get_wfd(virtio_queue_get_guest_notifier(vvq));
+ *   - hw/virtio/virtio-mmio.c|655| <<virtio_mmio_set_guest_notifier>> EventNotifier *notifier = virtio_queue_get_guest_notifier(vq);
+ *   - hw/virtio/virtio-pci.c|872| <<virtio_pci_get_notifier>> *n = virtio_queue_get_guest_notifier(vq);
+ *   - hw/virtio/virtio-pci.c|1058| <<virtio_pci_vector_unmask>> n = virtio_queue_get_guest_notifier(vq);
+ *   - hw/virtio/virtio-pci.c|1085| <<virtio_pci_vector_unmask>> n = virtio_queue_get_guest_notifier(vq);
+ *   - hw/virtio/virtio-pci.c|1104| <<virtio_pci_vector_mask>> n = virtio_queue_get_guest_notifier(vq);
+ *   - hw/virtio/virtio-pci.c|1192| <<virtio_pci_set_guest_notifier>> notifier = virtio_queue_get_guest_notifier(vq);
+ */
 EventNotifier *virtio_queue_get_guest_notifier(VirtQueue *vq)
 {
     return &vq->guest_notifier;
@@ -3562,6 +3638,13 @@ static void virtio_queue_host_notifier_aio_poll_end(EventNotifier *n)
     virtio_queue_set_notification(vq, 1);
 }
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1590| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+ *   - hw/scsi/virtio-scsi-dataplane.c|157| <<virtio_scsi_dataplane_start>> virtio_queue_aio_attach_host_notifier(vs->ctrl_vq, s->ctx);
+ *   - hw/scsi/virtio-scsi-dataplane.c|161| <<virtio_scsi_dataplane_start>> virtio_queue_aio_attach_host_notifier(vs->cmd_vqs[i], s->ctx);
+ *   - hw/scsi/virtio-scsi.c|1173| <<virtio_scsi_drained_end>> virtio_queue_aio_attach_host_notifier(vq, s->ctx);
+ */
 void virtio_queue_aio_attach_host_notifier(VirtQueue *vq, AioContext *ctx)
 {
     /*
@@ -3641,6 +3724,18 @@ void virtio_queue_host_notifier_read(EventNotifier *n)
     }
 }
 
+/*
+ * called by:
+ *   - hw/block/vhost-user-blk.c|304| <<vhost_user_blk_handle_output>> event_notifier_set(virtio_queue_get_host_notifier(kick_vq));
+  3 hw/block/virtio-blk.c|1880| <<virtio_blk_ioeventfd_stop_vq_bh>> EventNotifier *host_notifier = virtio_queue_get_host_notifier(vq);
+  4 hw/scsi/vhost-user-scsi.c|136| <<vhost_user_scsi_handle_output>> event_notifier_set(virtio_queue_get_host_notifier(kick_vq));
+  5 hw/scsi/virtio-scsi-dataplane.c|77| <<virtio_scsi_dataplane_stop_bh>> host_notifier = virtio_queue_get_host_notifier(vs->ctrl_vq);
+  6 hw/scsi/virtio-scsi-dataplane.c|86| <<virtio_scsi_dataplane_stop_bh>> host_notifier = virtio_queue_get_host_notifier(vs->event_vq);
+  7 hw/scsi/virtio-scsi-dataplane.c|91| <<virtio_scsi_dataplane_stop_bh>> host_notifier = virtio_queue_get_host_notifier(vs->cmd_vqs[i]);
+  8 hw/virtio/vhost.c|1251| <<vhost_virtqueue_start>> file.fd = event_notifier_get_fd(virtio_queue_get_host_notifier(vvq));
+  9 hw/virtio/virtio-bus.c|282| <<virtio_bus_set_host_notifier>> EventNotifier *notifier = virtio_queue_get_host_notifier(vq);
+ 10 hw/virtio/virtio-bus.c|316| <<virtio_bus_cleanup_host_notifier>> EventNotifier *notifier = virtio_queue_get_host_notifier(vq);
+ */
 EventNotifier *virtio_queue_get_host_notifier(VirtQueue *vq)
 {
     return &vq->host_notifier;
@@ -3784,6 +3879,16 @@ static Property virtio_properties[] = {
     DEFINE_PROP_END_OF_LIST(),
 };
 
+/*
+ * 在以下使用VirtioDeviceClass->start_ioeventfd:
+ *   - hw/block/virtio-blk.c|2181| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+ *   - hw/scsi/virtio-scsi.c|1342| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+ *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+ *   - hw/virtio/virtio.c|3965| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+ *
+ * 在以下使用virtio_device_start_ioeventfd_impl():
+ *   - hw/virtio/virtio.c|3965| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+ */
 static int virtio_device_start_ioeventfd_impl(VirtIODevice *vdev)
 {
     VirtioBusState *qbus = VIRTIO_BUS(qdev_get_parent_bus(DEVICE(vdev)));
diff --git a/include/block/block_int-common.h b/include/block/block_int-common.h
index 761276127..3a48f9eef 100644
--- a/include/block/block_int-common.h
+++ b/include/block/block_int-common.h
@@ -1238,8 +1238,35 @@ struct BlockDriverState {
 
     unsigned int write_gen;               /* Current data generation */
 
+    /*
+     * 在以下使用BlockDriverState->reqs_lock:
+     *   - block.c|422| <<bdrv_new>> qemu_mutex_init(&bs->reqs_lock);
+     *   - block.c|5528| <<bdrv_delete>> qemu_mutex_destroy(&bs->reqs_lock);
+     *   - block/io.c|591| <<tracked_request_end>> qemu_mutex_lock(&req->bs->reqs_lock);
+     *   - block/io.c|593| <<tracked_request_end>> qemu_mutex_unlock(&req->bs->reqs_lock);
+     *   - block/io.c|627| <<tracked_request_begin>> qemu_mutex_lock(&bs->reqs_lock);
+     *   - block/io.c|629| <<tracked_request_begin>> qemu_mutex_unlock(&bs->reqs_lock);
+     *   - block/io.c|690| <<bdrv_wait_serialising_requests_locked>> qemu_co_queue_wait(&req->wait_queue, &self->bs->reqs_lock);
+     *   - block/io.c|793| <<bdrv_wait_serialising_requests>> qemu_mutex_lock(&bs->reqs_lock);
+     *   - block/io.c|795| <<bdrv_wait_serialising_requests>> qemu_mutex_unlock(&bs->reqs_lock);
+     *   - block/io.c|803| <<bdrv_make_request_serialising>> qemu_mutex_lock(&req->bs->reqs_lock);
+     *   - block/io.c|808| <<bdrv_make_request_serialising>> qemu_mutex_unlock(&req->bs->reqs_lock);
+     *   - block/io.c|1973| <<bdrv_co_write_req_prepare>> QEMU_LOCK_GUARD(&bs->reqs_lock);
+     *   - block/io.c|2962| <<bdrv_co_flush>> qemu_mutex_lock(&bs->reqs_lock);
+     *   - block/io.c|2967| <<bdrv_co_flush>> qemu_co_queue_wait(&bs->flush_queue, &bs->reqs_lock);
+     *   - block/io.c|2972| <<bdrv_co_flush>> qemu_mutex_unlock(&bs->reqs_lock);
+     *   - block/io.c|3060| <<bdrv_co_flush>> qemu_mutex_lock(&bs->reqs_lock);
+     *   - block/io.c|3064| <<bdrv_co_flush>> qemu_mutex_unlock(&bs->reqs_lock);
+     */
     /* Protected by reqs_lock.  */
     QemuMutex reqs_lock;
+    /*
+     * 在以下使用BlockDriverState->tracked_requests:
+     *   - block/io.c|628| <<tracked_request_begin>> QLIST_INSERT_HEAD(&bs->tracked_requests, req, list);
+     *   - block/io.c|654| <<bdrv_find_conflicting_request>> QLIST_FOREACH(req, &self->bs->tracked_requests, list) {
+     *   - block/io.c|724| <<bdrv_co_get_self_request>> QLIST_FOREACH(req, &bs->tracked_requests, list) {
+     *   - block/mirror.c|1130| <<mirror_run>> assert(QLIST_EMPTY(&bs->tracked_requests));
+     */
     QLIST_HEAD(, BdrvTrackedRequest) tracked_requests;
     CoQueue flush_queue;                  /* Serializing flush queue */
     bool active_flush_req;                /* Flush request in flight? */
diff --git a/include/hw/hotplug.h b/include/hw/hotplug.h
index a9840ed48..5bf50cd32 100644
--- a/include/hw/hotplug.h
+++ b/include/hw/hotplug.h
@@ -57,6 +57,29 @@ struct HotplugHandlerClass {
     /* <public> */
     hotplug_fn pre_plug;
     hotplug_fn plug;
+    /*
+     * 在以下设置HotplugHandlerClass->unplug_request:
+     *   - hw/acpi/generic_event_device.c|416| <<acpi_ged_class_init>> hc->unplug_request = acpi_ged_unplug_request_cb;
+     *   - hw/acpi/piix4.c|648| <<piix4_pm_class_init>> hc->unplug_request = piix4_device_unplug_request_cb;
+     *   - hw/arm/virt.c|3015| <<virt_machine_class_init>> hc->unplug_request = virt_machine_device_unplug_request_cb;
+     *   - hw/i386/microvm.c|667| <<microvm_class_init>> hc->unplug_request = microvm_device_unplug_request_cb;
+     *   - hw/i386/pc.c|1831| <<pc_machine_class_init>> hc->unplug_request = pc_machine_device_unplug_request_cb;
+     *   - hw/isa/lpc_ich9.c|891| <<ich9_lpc_class_init>> hc->unplug_request = ich9_pm_device_unplug_request_cb;
+     *   - hw/loongarch/virt.c|1191| <<loongarch_class_init>> hc->unplug_request = virt_machine_device_unplug_request;
+     *   - hw/pci-bridge/pci_bridge_dev.c|263| <<pci_bridge_dev_class_init>> hc->unplug_request = pci_bridge_dev_unplug_request_cb;
+     *   - hw/pci-bridge/pcie_pci_bridge.c|159| <<pcie_pci_bridge_class_init>> hc->unplug_request = pci_bridge_dev_unplug_request_cb;
+     *   - hw/pci/pcie_port.c|235| <<pcie_slot_class_init>> hc->unplug_request = pcie_cap_slot_unplug_request_cb;
+     *   - hw/ppc/spapr.c|4692| <<spapr_machine_class_init>> hc->unplug_request = spapr_machine_device_unplug_request;
+     *   - hw/ppc/spapr_pci.c|2261| <<spapr_phb_class_init>> hp->unplug_request = spapr_pci_unplug_request;
+     *   - hw/s390x/s390-pci-bus.c|1331| <<s390_pcihost_class_init>> hc->unplug_request = s390_pcihost_unplug_request;
+     *   - hw/s390x/s390-virtio-ccw.c|778| <<ccw_machine_class_init>> hc->unplug_request = s390_machine_device_unplug_request;
+     *   - hw/xen/xen-bus.c|389| <<xen_bus_class_init>> hotplug_class->unplug_request = xen_bus_unplug_request;
+     * 在以下使用HotplugHandlerClass->unplug_request:
+     *   - hw/core/hotplug.c|44| <<hotplug_handler_unplug_request>> if (hdc->unplug_request) {
+     *   - hw/core/hotplug.c|45| <<hotplug_handler_unplug_request>> hdc->unplug_request(plug_handler, plugged_dev, errp);
+     *   - hw/virtio/virtio-md-pci.c|100| <<virtio_md_pci_unplug_request>> if (hdc->unplug_request) {
+     *   - system/qdev-monitor.c|938| <<qdev_unplug>> if (hdc->unplug_request) {
+     */
     hotplug_fn unplug_request;
     hotplug_fn unplug;
     bool (*is_hotpluggable_bus)(HotplugHandler *plug_handler, BusState *bus);
diff --git a/include/hw/i386/apic_internal.h b/include/hw/i386/apic_internal.h
index d6e85833d..b762e01bf 100644
--- a/include/hw/i386/apic_internal.h
+++ b/include/hw/i386/apic_internal.h
@@ -149,6 +149,16 @@ struct APICCommonClass {
     /* send_msi emulates an APIC bus and its proper place would be in a new
      * device, but it's convenient to have it here for now.
      */
+    /*
+     * 在以下使用APICommonClass->send_msi:
+     *   - hw/i386/amd_iommu.c|1382| <<amdvi_mem_ir_write>> apic_get_class(NULL)->send_msi(&to);
+     *   - hw/i386/intel_iommu.c|400| <<vtd_generate_interrupt>> apic_get_class(NULL)->send_msi(&msi);
+     *   - hw/i386/intel_iommu.c|3628| <<vtd_mem_ir_write>> apic_get_class(NULL)->send_msi(&to);
+     *   - hw/i386/kvm/apic.c|257| <<kvm_apic_class_init>> k->send_msi = kvm_send_msi;
+     *   - hw/i386/xen/xen_apic.c|89| <<xen_apic_class_init>> k->send_msi = xen_send_msi;
+     *   - hw/intc/apic.c|1193| <<apic_class_init>> k->send_msi = apic_send_msi;
+     *   - target/i386/whpx/whpx-apic.c|267| <<whpx_apic_class_init>> k->send_msi = whpx_send_msi;
+     */
     void (*send_msi)(MSIMessage *msi);
 };
 
diff --git a/include/hw/pci/pci_bus.h b/include/hw/pci/pci_bus.h
index 226131254..dd6817a18 100644
--- a/include/hw/pci/pci_bus.h
+++ b/include/hw/pci/pci_bus.h
@@ -36,6 +36,14 @@ struct PCIBus {
     const PCIIOMMUOps *iommu_ops;
     void *iommu_opaque;
     uint8_t devfn_min;
+    /*
+     * 在以下使用PCIBus->slot_reserved_mask:
+     *   - hw/pci/pci.c|511| <<pci_root_bus_internal_init>> bus->slot_reserved_mask = 0x0;
+     *   - hw/pci/pci.c|1130| <<pci_bus_devfn_reserved>> return bus->slot_reserved_mask & (1UL << PCI_SLOT(devfn));
+     *   - hw/pci/pci.c|1135| <<pci_bus_get_slot_reserved_mask>> return bus->slot_reserved_mask;
+     *   - hw/pci/pci.c|1140| <<pci_bus_set_slot_reserved_mask>> bus->slot_reserved_mask |= mask;
+     *   - hw/pci/pci.c|1149| <<pci_bus_clear_slot_reserved_mask>> bus->slot_reserved_mask &= ~mask;
+     */
     uint32_t slot_reserved_mask;
     pci_set_irq_fn set_irq;
     pci_map_irq_fn map_irq;
diff --git a/include/hw/pci/pci_device.h b/include/hw/pci/pci_device.h
index d3dd0f64b..bb61a3f96 100644
--- a/include/hw/pci/pci_device.h
+++ b/include/hw/pci/pci_device.h
@@ -112,6 +112,18 @@ struct PCIDevice {
 
     /* Space to store MSIX table & pending bit array */
     uint8_t *msix_table;
+    /*
+     * 在以下使用PCIDevice->msix_pba:
+     *   - hw/pci/msix.c|71| <<msix_pending_byte>> return dev->msix_pba + vector / 8;
+     *   - hw/pci/msix.c|311| <<msix_pba_mmio_read>> return pci_get_long(dev->msix_pba + addr);
+     *   - hw/pci/msix.c|423| <<msix_init>> dev->msix_pba = g_malloc0(pba_size);
+     *   - hw/pci/msix.c|514| <<msix_uninit>> g_free(dev->msix_pba);
+     *   - hw/pci/msix.c|515| <<msix_uninit>> dev->msix_pba = NULL;
+     *   - hw/pci/msix.c|541| <<msix_save>> qemu_put_buffer(f, dev->msix_pba, DIV_ROUND_UP(n, 8));
+     *   - hw/pci/msix.c|556| <<msix_load>> qemu_get_buffer(f, dev->msix_pba, DIV_ROUND_UP(n, 8));
+     *   - hw/pci/msix.c|608| <<msix_reset>> memset(dev->msix_pba, 0, QEMU_ALIGN_UP(dev->msix_entries_nr, 64) / 8);
+     *   - hw/usb/hcd-xhci-pci.c|171| <<usb_xhci_pci_exit>> if (dev->msix_table && dev->msix_pba && dev->msix_entry_used) {
+     */
     uint8_t *msix_pba;
 
     /* May be used by INTx or MSI during interrupt notification */
@@ -125,6 +137,15 @@ struct PCIDevice {
     MemoryRegion msix_exclusive_bar;
     /* Memory Regions for MSIX table and pending bit entries. */
     MemoryRegion msix_table_mmio;
+    /*
+     * 在以下使用PCIDevice->msix_pba_mmio:
+     *   - hw/pci/msix.c|416| <<msix_init>> memory_region_init_io(&dev->msix_pba_mmio, OBJECT(dev), &msix_pba_mmio_ops, dev, "msix-pba", pba_size);
+     *   - hw/pci/msix.c|418| <<msix_init>> memory_region_add_subregion(pba_bar, pba_offset, &dev->msix_pba_mmio);
+     *   - hw/pci/msix.c|498| <<msix_uninit>> memory_region_del_subregion(pba_bar, &dev->msix_pba_mmio);
+     *   - hw/vfio/pci.c|359| <<vfio_msi_interrupt>> memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, true);
+     *   - hw/vfio/pci.c|632| <<vfio_msix_vector_do_use>> memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, false);
+     *   - hw/vfio/pci.c|1737| <<vfio_msix_setup>> memory_region_set_enabled(&vdev->pdev.msix_pba_mmio, false);
+     */
     MemoryRegion msix_pba_mmio;
     /* Reference-count for entries actually in use by driver. */
     unsigned *msix_entry_used;
@@ -153,8 +174,38 @@ struct PCIDevice {
     PCIINTxRoutingNotifier intx_routing_notifier;
 
     /* MSI-X notifiers */
+    /*
+     * 在以下使用PCIDevice->msix_vector_use_notifier:
+     *   - hw/misc/ivshmem.c|795| <<ivshmem_disable_irqfd>> if (!pdev->msix_vector_use_notifier) {
+     *   - hw/pci/msix.c|113| <<msix_fire_vector_notifier>> if (!dev->msix_vector_use_notifier) {
+     *   - hw/pci/msix.c|120| <<msix_fire_vector_notifier>> ret = dev->msix_vector_use_notifier(dev, vector, msg);
+     *   - hw/pci/msix.c|607| <<msix_set_notifier_for_vector>> return dev->msix_vector_use_notifier(dev, vector, msg);
+     *   - hw/pci/msix.c|627| <<msix_set_vector_notifiers>> dev->msix_vector_use_notifier = use_notifier;
+     *   - hw/pci/msix.c|649| <<msix_set_vector_notifiers>> dev->msix_vector_use_notifier = NULL;
+     *   - hw/pci/msix.c|659| <<msix_unset_vector_notifiers>> assert(dev->msix_vector_use_notifier &&
+     *   - hw/pci/msix.c|668| <<msix_unset_vector_notifiers>> dev->msix_vector_use_notifier = NULL;
+     */
     MSIVectorUseNotifier msix_vector_use_notifier;
+    /*
+     * 在以下使用PCIDevice->msix_vector_release_notifier:
+     *   - hw/pci/msix.c|117| <<msix_fire_vector_notifier>> dev->msix_vector_release_notifier(dev, vector);
+     *   - hw/pci/msix.c|615| <<msix_unset_notifier_for_vector>> dev->msix_vector_release_notifier(dev, vector);
+     *   - hw/pci/msix.c|628| <<msix_set_vector_notifiers>> dev->msix_vector_release_notifier = release_notifier;
+     *   - hw/pci/msix.c|650| <<msix_set_vector_notifiers>> dev->msix_vector_release_notifier = NULL;
+     *   - hw/pci/msix.c|660| <<msix_unset_vector_notifiers>> dev->msix_vector_release_notifier);
+     *   - hw/pci/msix.c|669| <<msix_unset_vector_notifiers>> dev->msix_vector_release_notifier = NULL;
+     */
     MSIVectorReleaseNotifier msix_vector_release_notifier;
+    /*
+     * 在以下使用PCIDevice->msix_vector_poll_notifier:
+     *   - hw/pci/msix.c|251| <<msix_pba_mmio_read>> if (dev->msix_vector_poll_notifier) {
+     *   - hw/pci/msix.c|254| <<msix_pba_mmio_read>> dev->msix_vector_poll_notifier(dev, vector_start, vector_end);
+     *   - hw/pci/msix.c|629| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier = poll_notifier;
+     *   - hw/pci/msix.c|640| <<msix_set_vector_notifiers>> if (dev->msix_vector_poll_notifier) {
+     *   - hw/pci/msix.c|641| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier(dev, 0, dev->msix_entries_nr);
+     *   - hw/pci/msix.c|651| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier = NULL;
+     *   - hw/pci/msix.c|670| <<msix_unset_vector_notifiers>> dev->msix_vector_poll_notifier = NULL;
+     */
     MSIVectorPollNotifier msix_vector_poll_notifier;
 
     /* ID of standby device in net_failover pair */
diff --git a/include/hw/scsi/scsi.h b/include/hw/scsi/scsi.h
index c3d5e17e3..20e18bfd4 100644
--- a/include/hw/scsi/scsi.h
+++ b/include/hw/scsi/scsi.h
@@ -91,6 +91,16 @@ struct SCSIDevice
     uint64_t port_wwn;
     int scsi_version;
     int default_scsi_version;
+    /*
+     * 在以下使用SCSIDevice->io_timeout:
+     *   - hw/scsi/scsi-disk.c|3234| <<global>> DEFINE_PROP_UINT32("io_timeout", SCSIDiskState, qdev.io_timeout, DEFAULT_IO_TIMEOUT),
+     *   - hw/scsi/scsi-generic.c|778| <<global>> DEFINE_PROP_UINT32("io_timeout", SCSIDevice, io_timeout, DEFAULT_IO_TIMEOUT),
+     *   - hw/scsi/scsi-disk.c|2701| <<get_device_type>> ret = scsi_SG_IO_FROM_DEV(s->qdev.conf.blk, cmd, sizeof(cmd), buf, sizeof(buf), s->qdev.io_timeout);
+     *   - hw/scsi/scsi-disk.c|2901| <<scsi_block_do_sgio>> io_header->timeout = s->qdev.io_timeout * 1000;
+     *   - hw/scsi/scsi-generic.c|133| <<execute_command>> r->io_header.timeout = s->io_timeout * 1000;
+     *   - hw/scsi/scsi-generic.c|577| <<scsi_generic_set_vpd_bl_emulation>> ret = scsi_SG_IO_FROM_DEV(s->conf.blk, cmd, sizeof(cmd), buf, sizeof(buf), s->io_timeout);
+     *   - hw/scsi/scsi-generic.c|613| <<scsi_generic_read_device_identification>> ret = scsi_SG_IO_FROM_DEV(s->conf.blk, cmd, sizeof(cmd), buf, sizeof(buf), s->io_timeout);
+     */
     uint32_t io_timeout;
     bool needs_vpd_bl_emulation;
     bool hba_supports_iothread;
diff --git a/include/hw/virtio/virtio-blk.h b/include/hw/virtio/virtio-blk.h
index 5c14110c4..edb18bd62 100644
--- a/include/hw/virtio/virtio-blk.h
+++ b/include/hw/virtio/virtio-blk.h
@@ -38,6 +38,16 @@ struct VirtIOBlkConf
 {
     BlockConf conf;
     IOThread *iothread;
+    /*
+     * 在以下使用apply_iothread_vq_mapping->iothread_vq_mapping_list:
+     *   - hw/block/virtio-blk.c|2176| <<global>> DEFINE_PROP_IOTHREAD_VQ_MAPPING_LIST("iothread-vq-mapping", VirtIOBlock, conf.iothread_vq_mapping_list),
+     *   - hw/block/virtio-blk.c|1720| <<virtio_blk_vq_aio_context_init>> if (conf->iothread && conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1727| <<virtio_blk_vq_aio_context_init>> if (conf->iothread || conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1751| <<virtio_blk_vq_aio_context_init>> if (conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1752| <<virtio_blk_vq_aio_context_init>> if (!apply_iothread_vq_mapping(conf->iothread_vq_mapping_list,
+     *   - hw/block/virtio-blk.c|1785| <<virtio_blk_vq_aio_context_cleanup>> if (conf->iothread_vq_mapping_list) {
+     *   - hw/block/virtio-blk.c|1788| <<virtio_blk_vq_aio_context_cleanup>> for (node = conf->iothread_vq_mapping_list; node; node = node->next) {
+     */
     IOThreadVirtQueueMappingList *iothread_vq_mapping_list;
     char *serial;
     uint32_t request_merging;
@@ -60,6 +70,15 @@ struct VirtIOBlock {
     unsigned short sector_mask;
     bool original_wce;
     VMChangeStateEntry *change;
+    /*
+     * 在以下使用VirtIOBlock->ioeventfd_disabled:
+     *   - hw/block/virtio-blk.c|1202| <<virtio_blk_handle_output>> if (!s->ioeventfd_disabled && !s->ioeventfd_started) {
+     *   - hw/block/virtio-blk.c|1207| <<virtio_blk_handle_output>> if (!s->ioeventfd_disabled) {
+     *   - hw/block/virtio-blk.c|2057| <<virtio_blk_start_ioeventfd>> s->ioeventfd_disabled = true;
+     *   - hw/block/virtio-blk.c|2094| <<virtio_blk_stop_ioeventfd>> if (s->ioeventfd_disabled) {
+     *   - hw/block/virtio-blk.c|2095| <<virtio_blk_stop_ioeventfd>> s->ioeventfd_disabled = false;
+     *   - hw/block/virtio-blk.c|2246| <<virtio_blk_device_realize>> s->ioeventfd_disabled = true;
+     */
     bool ioeventfd_disabled;
     bool ioeventfd_started;
     bool ioeventfd_starting;
@@ -69,6 +88,26 @@ struct VirtIOBlock {
      * The AioContext for each virtqueue. The BlockDriverState will use the
      * first element as its AioContext.
      */
+    /*
+     * 在以下使用VirtIOBlock->vq_aio_context[][]:
+     *   - hw/block/virtio-blk.c|1261| <<virtio_blk_dma_restart_cb>> aio_bh_schedule_oneshot(s->vq_aio_context[i], virtio_blk_dma_restart_bh, vq_rq[i]);
+     *   - hw/block/virtio-blk.c|1549| <<virtio_blk_ioeventfd_detach>> virtio_queue_aio_detach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1559| <<virtio_blk_ioeventfd_attach>> virtio_queue_aio_attach_host_notifier(vq, s->vq_aio_context[i]);
+     *   - hw/block/virtio-blk.c|1749| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = g_new(AioContext *, conf->num_queues);
+     *   - hw/block/virtio-blk.c|1753| <<virtio_blk_vq_aio_context_init>> if (!apply_iothread_vq_mapping(conf->iothread_vq_mapping_list, s->vq_aio_context, conf->num_queues, errp)) {
+     *   - hw/block/virtio-blk.c|1756| <<virtio_blk_vq_aio_context_init>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1757| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1763| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1771| <<virtio_blk_vq_aio_context_init>> s->vq_aio_context[i] = ctx;
+     *   - hw/block/virtio-blk.c|1798| <<virtio_blk_vq_aio_context_cleanup>> g_free(s->vq_aio_context);
+     *   - hw/block/virtio-blk.c|1799| <<virtio_blk_vq_aio_context_cleanup>> s->vq_aio_context = NULL;
+     *   - hw/block/virtio-blk.c|1864| <<virtio_blk_start_ioeventfd>> r = blk_set_aio_context(s->conf.conf.blk, s->vq_aio_context[0], &local_err);
+     *   - hw/block/virtio-blk.c|1943| <<virtio_blk_stop_ioeventfd>> AioContext *ctx = s->vq_aio_context[i];
+     *
+     * 注释:
+     * The AioContext for each virtqueue. The BlockDriverState will use the
+     * first element as its AioContext.
+     */
     AioContext **vq_aio_context;
 
     uint64_t host_features;
diff --git a/include/hw/virtio/virtio.h b/include/hw/virtio/virtio.h
index 7d5ffdc14..38d0aa447 100644
--- a/include/hw/virtio/virtio.h
+++ b/include/hw/virtio/virtio.h
@@ -206,6 +206,13 @@ struct VirtioDeviceClass {
      * must mask in frontend instead.
      */
     void (*guest_notifier_mask)(VirtIODevice *vdev, int n, bool mask);
+    /*
+     * 在以下使用VirtioDeviceClass->start_ioeventfd:
+     *   - hw/block/virtio-blk.c|2181| <<virtio_blk_class_init>> vdc->start_ioeventfd = virtio_blk_start_ioeventfd;
+     *   - hw/scsi/virtio-scsi.c|1342| <<virtio_scsi_class_init>> vdc->start_ioeventfd = virtio_scsi_dataplane_start;
+     *   - hw/virtio/virtio-bus.c|236| <<virtio_bus_start_ioeventfd>> r = vdc->start_ioeventfd(vdev);
+     *   - hw/virtio/virtio.c|3965| <<virtio_device_class_init>> vdc->start_ioeventfd = virtio_device_start_ioeventfd_impl;
+     */
     int (*start_ioeventfd)(VirtIODevice *vdev);
     void (*stop_ioeventfd)(VirtIODevice *vdev);
     /* Saving and loading of a device; trying to deprecate save/load
diff --git a/include/sysemu/kvm_int.h b/include/sysemu/kvm_int.h
index 882e37e12..495f09599 100644
--- a/include/sysemu/kvm_int.h
+++ b/include/sysemu/kvm_int.h
@@ -95,9 +95,48 @@ struct KVMState
     unsigned int sigmask_len;
     GHashTable *gsimap;
 #ifdef KVM_CAP_IRQ_ROUTING
+    /*
+     * 在以下使用KVMState->irq_routes:
+     *   - accel/kvm/kvm-all.c|1820| <<kvm_init_irq_routing>> s->irq_routes = g_malloc0(sizeof(*s->irq_routes));
+     *   - accel/kvm/kvm-all.c|1854| <<kvm_irqchip_commit_routes>> s->irq_routes->flags = 0;
+     *   - accel/kvm/kvm-all.c|1856| <<kvm_irqchip_commit_routes>> ret = kvm_vm_ioctl(s, KVM_SET_GSI_ROUTING, s->irq_routes);
+     *   - accel/kvm/kvm-all.c|1866| <<kvm_add_routing_entry>> if (s->irq_routes->nr == s->nr_allocated_irq_routes) {
+     *   - accel/kvm/kvm-all.c|1873| <<kvm_add_routing_entry>> s->irq_routes = g_realloc(s->irq_routes, size);
+     *   - accel/kvm/kvm-all.c|1876| <<kvm_add_routing_entry>> n = s->irq_routes->nr++;
+     *   - accel/kvm/kvm-all.c|1877| <<kvm_add_routing_entry>> new = &s->irq_routes->entries[n];
+     *   - accel/kvm/kvm-all.c|1890| <<kvm_update_routing_entry>> for (n = 0; n < s->irq_routes->nr; n++) {
+     *   - accel/kvm/kvm-all.c|1891| <<kvm_update_routing_entry>> entry = &s->irq_routes->entries[n];
+     *   - accel/kvm/kvm-all.c|1931| <<kvm_irqchip_release_virq>> for (i = 0; i < s->irq_routes->nr; i++) {
+     *   - accel/kvm/kvm-all.c|1932| <<kvm_irqchip_release_virq>> e = &s->irq_routes->entries[i];
+     *   - accel/kvm/kvm-all.c|1934| <<kvm_irqchip_release_virq>> s->irq_routes->nr--;
+     *   - accel/kvm/kvm-all.c|1935| <<kvm_irqchip_release_virq>> *e = s->irq_routes->entries[s->irq_routes->nr];
+     *   - accel/kvm/kvm-all.c|2056| <<kvm_irqchip_add_msi_route>> if (s->irq_routes->nr < s->gsi_count) {
+     */
     struct kvm_irq_routing *irq_routes;
+    /*
+     * 在以下使用KVMState->nr_allocated_irq_routes:
+     *   - accel/kvm/kvm-all.c|1821| <<kvm_init_irq_routing>> s->nr_allocated_irq_routes = 0;
+     *   - accel/kvm/kvm-all.c|1866| <<kvm_add_routing_entry>> if (s->irq_routes->nr == s->nr_allocated_irq_routes) {
+     *   - accel/kvm/kvm-all.c|1867| <<kvm_add_routing_entry>> n = s->nr_allocated_irq_routes * 2;
+     *   - accel/kvm/kvm-all.c|1874| <<kvm_add_routing_entry>> s->nr_allocated_irq_routes = n;
+     */
     int nr_allocated_irq_routes;
+    /*
+     * 在以下使用KVMState->used_gsi_bitmap:
+     *   - accel/kvm/kvm-all.c|1801| <<set_gsi>> set_bit(gsi, s->used_gsi_bitmap);
+     *   - accel/kvm/kvm-all.c|1806| <<clear_gsi>> clear_bit(gsi, s->used_gsi_bitmap);
+     *   - accel/kvm/kvm-all.c|1816| <<kvm_init_irq_routing>> s->used_gsi_bitmap = bitmap_new(gsi_count);
+     *   - accel/kvm/kvm-all.c|1963| <<kvm_irqchip_get_virq>> next_virq = find_first_zero_bit(s->used_gsi_bitmap, s->gsi_count);
+     */
     unsigned long *used_gsi_bitmap;
+    /*
+     * 在以下使用KVMState->gsi_count:
+     *   - accel/kvm/kvm-all.c|1817| <<kvm_init_irq_routing>> s->gsi_count = gsi_count;
+     *   - accel/kvm/kvm-all.c|1912| <<kvm_irqchip_add_irq_route>> assert(pin < s->gsi_count);
+     *   - accel/kvm/kvm-all.c|1963| <<kvm_irqchip_get_virq>> next_virq = find_first_zero_bit(s->used_gsi_bitmap, s->gsi_count);
+     *   - accel/kvm/kvm-all.c|1964| <<kvm_irqchip_get_virq>> if (next_virq >= s->gsi_count) {
+     *   - accel/kvm/kvm-all.c|2056| <<kvm_irqchip_add_msi_route>> if (s->irq_routes->nr < s->gsi_count) {
+     */
     unsigned int gsi_count;
 #endif
     KVMMemoryListener memory_listener;
diff --git a/iothread.c b/iothread.c
index e1e9e0473..1313d4d44 100644
--- a/iothread.c
+++ b/iothread.c
@@ -395,6 +395,14 @@ void iothread_destroy(IOThread *iothread)
     object_unparent(OBJECT(iothread));
 }
 
+/*
+ * called by:
+ *   - block/export/export.c|123| <<blk_exp_add>> iothread = iothread_by_id(export->iothread);
+ *   - blockdev.c|3560| <<qmp_x_blockdev_set_iothread>> IOThread *obj = iothread_by_id(iothread->u.s);
+ *   - hw/block/virtio-blk.c|1601| <<validate_iothread_vq_mapping_list>> if (!iothread_by_id(name)) {
+ *   - hw/block/virtio-blk.c|1683| <<apply_iothread_vq_mapping>> IOThread *iothread = iothread_by_id(node->value->iothread);
+ *   - hw/block/virtio-blk.c|1789| <<virtio_blk_vq_aio_context_cleanup>> IOThread *iothread = iothread_by_id(node->value->iothread);
+ */
 /* Lookup IOThread by its id.  Only finds user-created objects, not internal
  * iothread_create() objects. */
 IOThread *iothread_by_id(const char *id)
diff --git a/linux-headers/linux/kvm.h b/linux-headers/linux/kvm.h
index 17839229b..0957effe8 100644
--- a/linux-headers/linux/kvm.h
+++ b/linux-headers/linux/kvm.h
@@ -2011,6 +2011,10 @@ struct kvm_assigned_msix_entry {
 	__u16 padding[3];
 };
 
+/*
+ * 在以下使用KVM_X2APIC_API_USE_32BIT_IDS:
+ *   - target/i386/kvm/kvm.c|209| <<kvm_enable_x2apic>> kvm_x2apic_api_set_flags(KVM_X2APIC_API_USE_32BIT_IDS | KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK),
+ */
 #define KVM_X2APIC_API_USE_32BIT_IDS            (1ULL << 0)
 #define KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK  (1ULL << 1)
 
diff --git a/net/tap-linux.c b/net/tap-linux.c
index c7e514ecb..87d345033 100644
--- a/net/tap-linux.c
+++ b/net/tap-linux.c
@@ -146,6 +146,14 @@ void tap_set_sndbuf(int fd, const NetdevTapOptions *tap, Error **errp)
     }
 }
 
+/*
+ * called by:
+ *   - net/tap.c|675| <<net_init_bridge>> vnet_hdr = tap_probe_vnet_hdr(fd, errp);
+ *   - net/tap.c|892| <<net_init_tap>> vnet_hdr = tap_probe_vnet_hdr(fd, errp);
+ *   - net/tap.c|949| <<net_init_tap>> vnet_hdr = tap_probe_vnet_hdr(fd, errp);
+ *   - net/tap.c|954| <<net_init_tap>> } else if (vnet_hdr != tap_probe_vnet_hdr(fd, NULL)) {
+ *   - net/tap.c|1001| <<net_init_tap>> vnet_hdr = tap_probe_vnet_hdr(fd, errp);
+ */
 int tap_probe_vnet_hdr(int fd, Error **errp)
 {
     struct ifreq ifr;
diff --git a/net/tap.c b/net/tap.c
index baaa2f7a9..d16574445 100644
--- a/net/tap.c
+++ b/net/tap.c
@@ -60,6 +60,25 @@ typedef struct TAPState {
     bool has_uso;
     bool enabled;
     VHostNetState *vhost_net;
+    /*
+     * 在以下设置TAPState->host_vnet_hdr_len:
+     *   - net/tap.c|286| <<tap_set_vnet_hdr_len>> s->host_vnet_hdr_len = len;
+     *   - net/tap.c|425| <<net_tap_fd_init>> s->host_vnet_hdr_len = vnet_hdr ? sizeof(struct virtio_net_hdr) : 0;
+     * 在以下使用TAPState->host_vnet_hdr_len:
+     *   - net/tap.c|124| <<tap_receive_iov>> if (s->host_vnet_hdr_len && !s->using_vnet_hdr) {
+     *   - net/tap.c|127| <<tap_receive_iov>> iov_copy[0].iov_len = s->host_vnet_hdr_len;
+     *   - net/tap.c|143| <<tap_receive_raw>> if (s->host_vnet_hdr_len) {
+     *   - net/tap.c|145| <<tap_receive_raw>> iov[iovcnt].iov_len = s->host_vnet_hdr_len;
+     *   - net/tap.c|161| <<tap_receive>> if (s->host_vnet_hdr_len && !s->using_vnet_hdr) {
+     *   - net/tap.c|200| <<tap_send>> if (s->host_vnet_hdr_len && !s->using_vnet_hdr) {
+     *   - net/tap.c|201| <<tap_send>> buf += s->host_vnet_hdr_len;
+     *   - net/tap.c|202| <<tap_send>> size -= s->host_vnet_hdr_len;
+     *   - net/tap.c|257| <<tap_has_vnet_hdr>> return !!s->host_vnet_hdr_len;
+     *   - net/tap.c|273| <<tap_get_vnet_hdr_len>> return s->host_vnet_hdr_len;
+     *   - net/tap.c|301| <<tap_using_vnet_hdr>> assert(!!s->host_vnet_hdr_len == using_vnet_hdr);
+     *   - net/tap.c|435| <<net_tap_fd_init>> if (tap_probe_vnet_hdr_len(s->fd, s->host_vnet_hdr_len)) {
+     *   - net/tap.c|436| <<net_tap_fd_init>> tap_fd_set_vnet_hdr_len(s->fd, s->host_vnet_hdr_len);
+     */
     unsigned host_vnet_hdr_len;
     Notifier exit;
 } TAPState;
@@ -181,6 +200,18 @@ static void tap_send_completed(NetClientState *nc, ssize_t len)
     tap_read_poll(s, true);
 }
 
+/*
+ * 在以下使用tap_send():
+ *   - net/tap.c|95| <<tap_update_fd_handler>> s->read_poll && s->enabled ? tap_send : NULL,
+ *
+ * 92 static void tap_update_fd_handler(TAPState *s)
+ * 93 {                         
+ * 94     qemu_set_fd_handler(s->fd,
+ * 95                         s->read_poll && s->enabled ? tap_send : NULL,
+ * 96                         s->write_poll && s->enabled ? tap_writable : NULL,
+ * 97                         s);
+ * 98 }
+ */
 static void tap_send(void *opaque)
 {
     TAPState *s = opaque;
@@ -408,6 +439,11 @@ static NetClientInfo net_tap_info = {
     .set_steering_ebpf = tap_set_steering_ebpf,
 };
 
+/*
+ * called by;
+ *   - net/tap.c|675| <<net_init_bridge>> s = net_tap_fd_init(peer, "bridge", name, fd, vnet_hdr);
+ *   - net/tap.c|726| <<net_init_tap_one>> TAPState *s = net_tap_fd_init(peer, model, name, fd, vnet_hdr);
+ */
 static TAPState *net_tap_fd_init(NetClientState *peer,
                                  const char *model,
                                  const char *name,
@@ -697,6 +733,13 @@ static int net_tap_init(const NetdevTapOptions *tap, int *vnet_hdr,
 
 #define MAX_TAP_QUEUES 1024
 
+/*
+ * called by:
+ *   - net/tap.c|886| <<net_init_tap>> net_init_tap_one(tap, peer, "tap", name, NULL, script, downscript, vhostfdname, vnet_hdr, fd, &err);
+ *   - net/tap.c|949| <<net_init_tap>> net_init_tap_one(tap, peer, "tap", name, ifname, script, downscript, tap->vhostfds ? vhost_fds[i] : NULL, vnet_hdr, fd, &err);
+ *   - net/tap.c|995| <<net_init_tap>> net_init_tap_one(tap, peer, "bridge", name, ifname, script, downscript, vhostfdname, vnet_hdr, fd, &err);
+ *   - net/tap.c|1040| <<net_init_tap>> net_init_tap_one(tap, peer, "tap", name, ifname, i >= 1 ? "no" : script, i >= 1 ? "no" : downscript, vhostfdname, vnet_hdr, fd, &err);
+ */
 static void net_init_tap_one(const NetdevTapOptions *tap, NetClientState *peer,
                              const char *model, const char *name,
                              const char *ifname, const char *script,
diff --git a/system/cpus.c b/system/cpus.c
index 68d161d96..c3ed8ede1 100644
--- a/system/cpus.c
+++ b/system/cpus.c
@@ -357,6 +357,10 @@ static void sigbus_reraise(void)
     abort();
 }
 
+/*
+ * 在以下使用sigbug_handler():
+ *   - system/cpus.c|389| <<qemu_init_sigbus>> action.sa_sigaction = sigbus_handler;
+ */
 static void sigbus_handler(int n, siginfo_t *siginfo, void *ctx)
 {
     if (siginfo->si_code != BUS_MCEERR_AO && siginfo->si_code != BUS_MCEERR_AR) {
diff --git a/system/memory.c b/system/memory.c
index a229a7998..3f4965fd6 100644
--- a/system/memory.c
+++ b/system/memory.c
@@ -39,6 +39,14 @@
 
 static unsigned memory_region_transaction_depth;
 static bool memory_region_update_pending;
+/*
+ * 在以下使用ioeventfd_update_pending():
+ *   - system/memory.c|1140| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+ *   - system/memory.c|1142| <<memory_region_transaction_commit>> } else if (ioeventfd_update_pending) {
+ *   - system/memory.c|1146| <<memory_region_transaction_commit>> ioeventfd_update_pending = false;
+ *   - system/memory.c|2606| <<memory_region_add_eventfd>> ioeventfd_update_pending |= mr->enabled;
+ *   - system/memory.c|2641| <<memory_region_del_eventfd>> ioeventfd_update_pending |= mr->enabled;
+ */
 static bool ioeventfd_update_pending;
 unsigned int global_dirty_tracking;
 
@@ -776,6 +784,10 @@ static FlatView *generate_memory_topology(MemoryRegion *mr)
     return view;
 }
 
+/*
+ * called by:
+ *   - system/memory.c|883| <<address_space_update_ioeventfds>> address_space_add_del_ioeventfds(as, ioeventfds, ioeventfd_nb, as->ioeventfds, as->ioeventfd_nb);
+ */
 static void address_space_add_del_ioeventfds(AddressSpace *as,
                                              MemoryRegionIoeventfd *fds_new,
                                              unsigned fds_new_nb,
@@ -2556,6 +2568,18 @@ void memory_region_clear_flush_coalesced(MemoryRegion *mr)
     }
 }
 
+/*
+ * called by:
+ *   - hw/misc/ivshmem.c|365| <<ivshmem_add_eventfd>> memory_region_add_eventfd(&s->ivshmem_mmio, DOORBELL, 4, true, (posn << 16) | i, &s->peers[posn].eventfds[i]);
+ *   - hw/misc/pci-testdev.c|113| <<pci_testdev_start>> memory_region_add_eventfd(test->mr, le32_to_cpu(test->hdr->offset), test->size, test->match_data, test->hdr->data, &test->notifier);
+ *   - hw/nvme/ctrl.c|4523| <<nvme_init_cq_ioeventfd>> memory_region_add_eventfd(&n->iomem, 0x1000 + offset, 4, false, 0, &cq->notifier);
+ *   - hw/nvme/ctrl.c|4552| <<nvme_init_sq_ioeventfd>> memory_region_add_eventfd(&n->iomem, 0x1000 + offset, 4, false, 0, &sq->notifier);
+ *   - hw/vfio/pci-quirks.c|401| <<vfio_ioeventfd_init>> memory_region_add_eventfd(ioeventfd->mr, ioeventfd->addr, ioeventfd->size, true, ioeventfd->data, &ioeventfd->e);
+ *   - hw/virtio/virtio-mmio.c|52| <<virtio_mmio_ioeventfd_assign>> memory_region_add_eventfd(&proxy->iomem, VIRTIO_MMIO_QUEUE_NOTIFY, 4, true, n, notifier);
+ *   - hw/virtio/virtio-pci.c|345| <<virtio_pci_ioeventfd_assign>> memory_region_add_eventfd(modern_mr, modern_addr, 0, false, n, notifier);
+ *   - hw/virtio/virtio-pci.c|348| <<virtio_pci_ioeventfd_assign>> memory_region_add_eventfd(modern_notify_mr, 0, 2, true, n, notifier);
+ *   - hw/virtio/virtio-pci.c|353| <<virtio_pci_ioeventfd_assign>> memory_region_add_eventfd(legacy_mr, legacy_addr, 2, true, n, notifier);
+ */
 void memory_region_add_eventfd(MemoryRegion *mr,
                                hwaddr addr,
                                unsigned size,
@@ -2581,6 +2605,11 @@ void memory_region_add_eventfd(MemoryRegion *mr,
             break;
         }
     }
+    /*
+     * MemoryRegion *mr:
+     * -> unsigned ioeventfd_nb;
+     * -> MemoryRegionIoeventfd *ioeventfds;
+     */
     ++mr->ioeventfd_nb;
     mr->ioeventfds = g_realloc(mr->ioeventfds,
                                   sizeof(*mr->ioeventfds) * mr->ioeventfd_nb);
diff --git a/target/arm/kvm.c b/target/arm/kvm.c
index ab85d628a..0db43fe9d 100644
--- a/target/arm/kvm.c
+++ b/target/arm/kvm.c
@@ -2356,6 +2356,11 @@ int kvm_arch_get_registers(CPUState *cs)
     return ret;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2876| <<kvm_cpu_exec>> kvm_arch_on_sigbus_vcpu(cpu, pending_sigbus_code, pending_sigbus_addr);
+ *   - accel/kvm/kvm-all.c|3399| <<kvm_on_sigbus>> kvm_arch_on_sigbus_vcpu(first_cpu, code, addr);
+ */
 void kvm_arch_on_sigbus_vcpu(CPUState *c, int code, void *addr)
 {
     ram_addr_t ram_addr;
diff --git a/target/arm/tcg/tlb_helper.c b/target/arm/tcg/tlb_helper.c
index 885bf4ec1..075c58a35 100644
--- a/target/arm/tcg/tlb_helper.c
+++ b/target/arm/tcg/tlb_helper.c
@@ -116,6 +116,10 @@ static uint32_t compute_fsr_fsc(CPUARMState *env, ARMMMUFaultInfo *fi,
     return fsr;
 }
 
+/*
+ * called by:
+ *   - target/arm/tcg/tlb_helper.c|195| <<arm_deliver_fault>> if (report_as_gpc_exception(cpu, current_el, fi)) {
+ */
 static bool report_as_gpc_exception(ARMCPU *cpu, int current_el,
                                     ARMMMUFaultInfo *fi)
 {
@@ -167,6 +171,13 @@ static unsigned encode_gpcsc(ARMMMUFaultInfo *fi)
     return gpcsc[fi->gpcf] | fi->level;
 }
 
+/*
+ * called by:
+ *   - target/arm/tcg/tlb_helper.c|277| <<arm_cpu_do_unaligned_access>> arm_deliver_fault(cpu, vaddr, access_type, mmu_idx, &fi);
+ *   - target/arm/tcg/tlb_helper.c|318| <<arm_cpu_do_transaction_failed>> arm_deliver_fault(cpu, addr, access_type, mmu_idx, &fi);
+ *   - target/arm/tcg/tlb_helper.c|371| <<arm_cpu_tlb_fill>> arm_deliver_fault(cpu, address, access_type, mmu_idx, fi);
+ *   - target/arm/tcg/tlb_helper.c|390| <<arm_cpu_record_sigsegv>> arm_deliver_fault(cpu, addr, access_type, MMU_USER_IDX, &fi);
+ */
 static G_NORETURN
 void arm_deliver_fault(ARMCPU *cpu, vaddr addr,
                        MMUAccessType access_type,
@@ -262,6 +273,16 @@ void arm_deliver_fault(ARMCPU *cpu, vaddr addr,
     raise_exception(env, exc, syn, target_el);
 }
 
+/*
+ * 在以下使用arm_cpu_do_unaligned_access():
+ *   - target/arm/cpu.c|2491| <<global>> TCGCPUOps arm_tcg_ops.do_unaligned_access = arm_cpu_do_unaligned_access,
+ *   - target/arm/tcg/cpu-v7m.c|249| <<global>> TCGCPUOps arm_v7m_tcg_ops.do_unaligned_access = arm_cpu_do_unaligned_access,
+ *   - target/arm/tcg/helper-a64.c|968| <<HELPER(unaligned_access)>> arm_cpu_do_unaligned_access(env_cpu(env), addr, access_type,
+ *   - target/arm/tcg/helper-a64.c|1203| <<check_setg_alignment>> arm_cpu_do_unaligned_access(env_cpu(env), ptr, MMU_DATA_STORE,
+ *   - target/arm/tcg/mte_helper.c|313| <<check_tag_aligned>> arm_cpu_do_unaligned_access(env_cpu(env), ptr, MMU_DATA_STORE,
+ *   - target/arm/tcg/mte_helper.c|917| <<HELPER(mte_check)>> arm_cpu_do_unaligned_access(env_cpu(env), ptr, type, idx, GETPC());
+ *   - target/arm/tcg/tlb_helper.c|396| <<arm_cpu_record_sigbus>> arm_cpu_do_unaligned_access(cs, addr, access_type, MMU_USER_IDX, ra);
+ */
 /* Raise a data fault alignment exception for the specified virtual address */
 void arm_cpu_do_unaligned_access(CPUState *cs, vaddr vaddr,
                                  MMUAccessType access_type,
diff --git a/target/i386/kvm/kvm.c b/target/i386/kvm/kvm.c
index e68cbe929..16f54b989 100644
--- a/target/i386/kvm/kvm.c
+++ b/target/i386/kvm/kvm.c
@@ -203,6 +203,11 @@ bool kvm_has_x2apic_api(void)
     return has_x2apic_api;
 }
 
+/*
+ * called by:
+ *   - hw/i386/intel_iommu.c|4127| <<vtd_decide_config>> if (kvm_irqchip_is_split() && !kvm_enable_x2apic()) {
+ *   - hw/i386/x86.c|140| <<x86_cpus_init>> if (kvm_enabled() && x86ms->apic_id_limit > 255 && kvm_irqchip_in_kernel() && !kvm_enable_x2apic()) {
+ */
 bool kvm_enable_x2apic(void)
 {
     return MEMORIZE(
@@ -5381,6 +5386,10 @@ bool kvm_arch_stop_on_emulation_error(CPUState *cs)
            ((env->segs[R_CS].selector  & 3) != 3);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1823| <<kvm_init_irq_routing>> kvm_arch_init_irq_routing(s);
+ */
 void kvm_arch_init_irq_routing(KVMState *s)
 {
     /* We know at this point that we're using the in-kernel
@@ -5453,6 +5462,11 @@ uint64_t kvm_swizzle_msi_ext_dest_id(uint64_t address)
     return address;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2078| <<kvm_irqchip_add_msi_route>> if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
+ *   - accel/kvm/kvm-all.c|2129| <<kvm_irqchip_update_msi_route>> if (kvm_arch_fixup_msi_route(&kroute, msg.address, msg.data, dev)) {
+ */
 int kvm_arch_fixup_msi_route(struct kvm_irq_routing_entry *route,
                              uint64_t address, uint32_t data, PCIDevice *dev)
 {
diff --git a/util/aio-posix.c b/util/aio-posix.c
index 266c9dd35..4ec189cd3 100644
--- a/util/aio-posix.c
+++ b/util/aio-posix.c
@@ -192,6 +192,29 @@ static void aio_set_fd_poll(AioContext *ctx, int fd,
     node->io_poll_end = io_poll_end;
 }
 
+/*
+ * called by:
+ *   - block/linux-aio.c|435| <<laio_detach_aio_context>> aio_set_event_notifier(old_context, &s->e, NULL, NULL, NULL);
+  2 block/linux-aio.c|444| <<laio_attach_aio_context>> aio_set_event_notifier(new_context, &s->e,
+  3 block/nvme.c|873| <<nvme_init>> aio_set_event_notifier(bdrv_get_aio_context(bs),
+  4 block/nvme.c|959| <<nvme_close>> aio_set_event_notifier(bdrv_get_aio_context(bs),
+  5 block/nvme.c|1557| <<nvme_detach_aio_context>> aio_set_event_notifier(bdrv_get_aio_context(bs),
+  6 block/nvme.c|1568| <<nvme_attach_aio_context>> aio_set_event_notifier(new_context, &s->irq_notifier[MSIX_SHARED_IRQ_IDX],
+  7 block/win32-aio.c|177| <<win32_aio_detach_aio_context>> aio_set_event_notifier(old_context, &aio->e, NULL, NULL, NULL);
+  8 block/win32-aio.c|185| <<win32_aio_attach_aio_context>> aio_set_event_notifier(new_context, &aio->e, win32_aio_completion_cb,
+  9 hw/virtio/virtio.c|3614| <<virtio_queue_aio_attach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier,
+ 10 hw/virtio/virtio.c|3643| <<virtio_queue_aio_attach_host_notifier_no_poll>> aio_set_event_notifier(ctx, &vq->host_notifier,
+ *   - hw/virtio/virtio.c|3658| <<virtio_queue_aio_detach_host_notifier>> aio_set_event_notifier(ctx, &vq->host_notifier, NULL, NULL, NULL);
+ 12 include/block/aio.h|477| <<QSLIST_HEAD>> void aio_set_event_notifier(AioContext *ctx,
+ 13 tests/unit/test-aio.c|106| <<set_event_notifier>> aio_set_event_notifier(nctx, notifier, handler, NULL, NULL);
+ 14 tests/unit/test-nested-aio-poll.c|94| <<test>> aio_set_event_notifier(td.ctx, &td.poll_notifier,
+ 15 tests/unit/test-nested-aio-poll.c|99| <<test>> aio_set_event_notifier(td.ctx, &td.dummy_notifier,
+ 16 tests/unit/test-nested-aio-poll.c|117| <<test>> aio_set_event_notifier(td.ctx, &td.dummy_notifier, NULL, NULL, NULL);
+ 17 tests/unit/test-nested-aio-poll.c|118| <<test>> aio_set_event_notifier(td.ctx, &td.poll_notifier, NULL, NULL, NULL);
+ *   - util/async.c|414| <<aio_ctx_finalize>> aio_set_event_notifier(ctx, &ctx->notifier, NULL, NULL, NULL);
+ 21 util/async.c|595| <<aio_context_new>> aio_set_event_notifier(ctx, &ctx->notifier,
+ 22 util/main-loop.c|685| <<event_notifier_set_handler>> aio_set_event_notifier(iohandler_ctx, e, handler, NULL, NULL);
+ */
 void aio_set_event_notifier(AioContext *ctx,
                             EventNotifier *notifier,
                             EventNotifierHandler *io_read,
diff --git a/util/event_notifier-posix.c b/util/event_notifier-posix.c
index 76420c5b5..1dfe9252a 100644
--- a/util/event_notifier-posix.c
+++ b/util/event_notifier-posix.c
@@ -32,6 +32,61 @@ void event_notifier_init_fd(EventNotifier *e, int fd)
 }
 #endif
 
+/*
+ * called by:
+ *   - block/linux-aio.c|456| <<laio_init>> rc = event_notifier_init(&s->e, false);
+ *   - block/nvme.c|761| <<nvme_init>> ret = event_notifier_init(&s->irq_notifier[MSIX_SHARED_IRQ_IDX], 0);
+ *   - block/win32-aio.c|194| <<win32_aio_init>> if (event_notifier_init(&s->e, false) < 0) {
+ *   - contrib/ivshmem-server/ivshmem-server.c|176| <<ivshmem_server_handle_new_conn>> if (event_notifier_init(&peer->vectors[i], FALSE) < 0) {
+ *   - hw/hyperv/hyperv.c|414| <<hyperv_sint_route_new>> r = event_notifier_init(ack_notifier, false);
+ *   - hw/hyperv/hyperv.c|428| <<hyperv_sint_route_new>> r = event_notifier_init(&sint_route->sint_set_notifier, false);
+ *   - hw/hyperv/hyperv_testdev.c|216| <<evt_conn_create>> assert(!event_notifier_init(&conn->notifier, false));
+ *   - hw/hyperv/vmbus.c|1426| <<open_channel>> if (event_notifier_init(&chan->notifier, 0)) {
+ *   - hw/hyperv/vmbus.c|2421| <<vmbus_realize>> ret = event_notifier_init(&vmbus->notifier, 0);
+ *   - hw/misc/pci-testdev.c|294| <<pci_testdev_realize>> r = event_notifier_init(&test->notifier, 0);
+ *   - hw/nvme/ctrl.c|4517| <<nvme_init_cq_ioeventfd>> ret = event_notifier_init(&cq->notifier, 0);
+ *   - hw/nvme/ctrl.c|4546| <<nvme_init_sq_ioeventfd>> ret = event_notifier_init(&sq->notifier, 0);
+ *   - hw/remote/proxy.c|56| <<setup_irqfd>> event_notifier_init(&dev->intr, 0);
+ *   - hw/remote/proxy.c|57| <<setup_irqfd>> event_notifier_init(&dev->resample, 0);
+ *   - hw/s390x/virtio-ccw.c|1028| <<virtio_ccw_set_guest_notifier>> int r = event_notifier_init(notifier, 0);
+ *   - hw/usb/ccid-card-emulated.c|406| <<init_event_notifier>> if (event_notifier_init(&card->notifier, false) < 0) {
+ *   - hw/usb/u2f-emulated.c|325| <<u2f_emulated_realize>> if (event_notifier_init(&key->notifier, false) < 0) {
+ *   - hw/vfio/ap.c|110| <<vfio_ap_register_irq_notifier>> if (event_notifier_init(notifier, 0)) {
+ *   - hw/vfio/ccw.c|427| <<vfio_ccw_register_irq_notifier>> if (event_notifier_init(notifier, 0)) {
+ *   - hw/vfio/pci-quirks.c|361| <<vfio_ioeventfd_init>> if (event_notifier_init(&ioeventfd->e, 0)) {
+ *   - hw/vfio/pci.c|137| <<vfio_intx_enable_kvm>> if (event_notifier_init(&vdev->intx.unmask, 0)) {
+ *   - hw/vfio/pci.c|290| <<vfio_intx_enable>> ret = event_notifier_init(&vdev->intx.interrupt, 0);
+ *   - hw/vfio/pci.c|479| <<vfio_connect_kvm_msi_virq>> if (event_notifier_init(&vector->kvm_interrupt, 0)) {
+ *   - hw/vfio/pci.c|528| <<vfio_msix_vector_do_use>> if (event_notifier_init(&vector->interrupt, 0)) {
+ *   - hw/vfio/pci.c|751| <<vfio_msi_enable>> if (event_notifier_init(&vector->interrupt, 0)) {
+ *   - hw/vfio/pci.c|2851| <<vfio_register_err_notifier>> if (event_notifier_init(&vdev->err_notifier, 0)) {
+ *   - hw/vfio/pci.c|2917| <<vfio_register_req_notifier>> if (event_notifier_init(&vdev->req_notifier, 0)) {
+ *   - hw/vfio/platform.c|77| <<vfio_init_intp>> ret = event_notifier_init(intp->interrupt, 0);
+ *   - hw/vfio/platform.c|88| <<vfio_init_intp>> ret = event_notifier_init(intp->unmask, 0);
+ *   - hw/virtio/vhost-vdpa.c|1054| <<vhost_vdpa_svq_set_fds>> r = event_notifier_init(&svq->hdev_kick, 0);
+ *   - hw/virtio/vhost-vdpa.c|1060| <<vhost_vdpa_svq_set_fds>> r = event_notifier_init(&svq->hdev_call, 0);
+ *   - hw/virtio/vhost.c|1385| <<vhost_virtqueue_init>> int r = event_notifier_init(&vq->masked_notifier, 0);
+ *   - hw/virtio/vhost.c|1400| <<vhost_virtqueue_init>> r = event_notifier_init(&vq->error_notifier, 0);
+ *   - hw/virtio/vhost.c|2031| <<vhost_dev_start>> r = event_notifier_init(
+ *   - hw/virtio/virtio-bus.c|290| <<virtio_bus_set_host_notifier>> r = event_notifier_init(notifier, 1);
+ *   - hw/virtio/virtio-mmio.c|658| <<virtio_mmio_set_guest_notifier>> int r = event_notifier_init(notifier, 0);
+ *   - hw/virtio/virtio-mmio.c|684| <<virtio_mmio_set_config_guest_notifier>> r = event_notifier_init(notifier, 0);
+ *   - hw/virtio/virtio-pci.c|1196| <<virtio_pci_set_guest_notifier>> int r = event_notifier_init(notifier, 0);
+ *   - tests/unit/test-aio.c|261| <<test_set_event_notifier>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|275| <<test_wait_event_notifier>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|300| <<test_flush_event_notifier>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|327| <<test_wait_event_notifier_noflush>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|340| <<test_wait_event_notifier_noflush>> event_notifier_init(&dummy.e, false);
+ *   - tests/unit/test-aio.c|381| <<test_timer_schedule>> event_notifier_init(&e, false);
+ *   - tests/unit/test-aio.c|592| <<test_source_set_event_notifier>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|606| <<test_source_wait_event_notifier>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|631| <<test_source_flush_event_notifier>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|658| <<test_source_wait_event_notifier_noflush>> event_notifier_init(&data.e, false);
+ *   - tests/unit/test-aio.c|671| <<test_source_wait_event_notifier_noflush>> event_notifier_init(&dummy.e, false);
+ *   - tests/unit/test-nested-aio-poll.c|93| <<test>> event_notifier_init(&td.poll_notifier, 1);
+ *   - tests/unit/test-nested-aio-poll.c|98| <<test>> event_notifier_init(&td.dummy_notifier, 0);
+ *   - util/async.c|584| <<aio_context_new>> ret = event_notifier_init(&ctx->notifier, false);
+ */
 int event_notifier_init(EventNotifier *e, int active)
 {
     int fds[2];
diff --git a/util/main-loop.c b/util/main-loop.c
index a0386cfeb..1bce9c2fa 100644
--- a/util/main-loop.c
+++ b/util/main-loop.c
@@ -635,6 +635,39 @@ GSource *iohandler_get_g_source(void)
     return aio_get_g_source(iohandler_ctx);
 }
 
+/*
+ * 一些例子:
+ *   - hw/net/vhost_net.c|284| <<vhost_net_start_one>> qemu_set_fd_handler(net->backend, NULL, NULL, NULL);
+ *   - hw/vfio/ap.c|118| <<vfio_ap_register_irq_notifier>> qemu_set_fd_handler(fd, fd_read, NULL, vapdev);
+ *   - hw/vfio/ap.c|122| <<vfio_ap_register_irq_notifier>> qemu_set_fd_handler(fd, NULL, NULL, vapdev);
+ *   - hw/vfio/ap.c|151| <<vfio_ap_unregister_irq_notifier>> qemu_set_fd_handler(event_notifier_get_fd(notifier), NULL, NULL, vapdev);
+ *   - hw/vfio/ccw.c|435| <<vfio_ccw_register_irq_notifier>> qemu_set_fd_handler(fd, fd_read, NULL, vcdev);
+ *   - hw/vfio/ccw.c|439| <<vfio_ccw_register_irq_notifier>> qemu_set_fd_handler(fd, NULL, NULL, vcdev);
+ *   - hw/vfio/ccw.c|473| <<vfio_ccw_unregister_irq_notifier>> qemu_set_fd_handler(event_notifier_get_fd(notifier), NULL, NULL, vcdev);
+ *   - hw/vfio/pci-quirks.c|312| <<vfio_ioeventfd_exit>> qemu_set_fd_handler(event_notifier_get_fd(&ioeventfd->e), NULL, NULL, NULL);
+ *   - hw/vfio/pci-quirks.c|397| <<vfio_ioeventfd_init>> qemu_set_fd_handler(event_notifier_get_fd(&ioeventfd->e), vfio_ioeventfd_handler, NULL, ioeventfd);
+ *   - hw/vfio/pci.c|131| <<vfio_intx_enable_kvm>> qemu_set_fd_handler(irq_fd, NULL, NULL, vdev);
+ *   - hw/vfio/pci.c|172| <<vfio_intx_enable_kvm>> qemu_set_fd_handler(irq_fd, vfio_intx_interrupt, NULL, vdev);
+ *   - hw/vfio/pci.c|202| <<vfio_intx_disable_kvm>> qemu_set_fd_handler(event_notifier_get_fd(&vdev->intx.interrupt), vfio_intx_interrupt, NULL, vdev);
+ *   - hw/vfio/pci.c|296| <<vfio_intx_enable>> qemu_set_fd_handler(fd, vfio_intx_interrupt, NULL, vdev);
+ *   - hw/vfio/pci.c|300| <<vfio_intx_enable>> qemu_set_fd_handler(fd, NULL, NULL, vdev);
+ *   - hw/vfio/pci.c|328| <<vfio_intx_disable>> qemu_set_fd_handler(fd, NULL, NULL, vdev);
+ *   - hw/vfio/pci.c|549| <<vfio_msix_vector_do_use>> qemu_set_fd_handler(event_notifier_get_fd(&vector->interrupt), handler, NULL, vector);
+ *   - hw/vfio/pci.c|795| <<vfio_msi_enable>> qemu_set_fd_handler(event_notifier_get_fd(&vector->interrupt), vfio_msi_interrupt, NULL, vector);
+ *   - hw/vfio/pci.c|849| <<vfio_msi_disable_common>> qemu_set_fd_handler(event_notifier_get_fd(&vector->interrupt), NULL, NULL, NULL);
+ *   - hw/vfio/pci.c|2902| <<vfio_register_err_notifier>> qemu_set_fd_handler(fd, vfio_err_notifier_handler, NULL, vdev);
+ *   - hw/vfio/pci.c|2907| <<vfio_register_err_notifier>> qemu_set_fd_handler(fd, NULL, NULL, vdev);
+ *   - hw/vfio/pci.c|2925| <<vfio_unregister_err_notifier>> qemu_set_fd_handler(event_notifier_get_fd(&vdev->err_notifier), NULL, NULL, vdev);
+ *   - hw/vfio/pci.c|2967| <<vfio_register_req_notifier>> qemu_set_fd_handler(fd, vfio_req_notifier_handler, NULL, vdev);
+ *   - hw/vfio/pci.c|2972| <<vfio_register_req_notifier>> qemu_set_fd_handler(fd, NULL, NULL, vdev);
+ *   - hw/vfio/pci.c|2991| <<vfio_unregister_req_notifier>> qemu_set_fd_handler(event_notifier_get_fd(&vdev->req_notifier), NULL, NULL, vdev);
+ *   - hw/vfio/platform.c|120| <<vfio_set_trigger_eventfd>> qemu_set_fd_handler(fd, (IOHandler *)handler, NULL, intp);
+ *   - hw/vfio/platform.c|126| <<vfio_set_trigger_eventfd>> qemu_set_fd_handler(fd, NULL, NULL, NULL);
+ *   - hw/vfio/platform.c|360| <<vfio_set_resample_eventfd>> qemu_set_fd_handler(fd, NULL, NULL, NULL);
+ *   - hw/virtio/vhost-backend.c|313| <<vhost_kernel_set_iotlb_callback>> qemu_set_fd_handler((uintptr_t)dev->opaque, vhost_kernel_iotlb_read, NULL, dev);
+ *   - hw/virtio/vhost-backend.c|316| <<vhost_kernel_set_iotlb_callback>> qemu_set_fd_handler((uintptr_t)dev->opaque, NULL, NULL, NULL);
+ *   - net/tap.c|75| <<tap_update_fd_handler>> qemu_set_fd_handler(s->fd, s->read_poll && s->enabled ? tap_send : NULL, s->write_poll && s->enabled ? tap_writable : NULL, s);
+ */
 void qemu_set_fd_handler(int fd,
                          IOHandler *fd_read,
                          IOHandler *fd_write,
diff --git a/util/osdep.c b/util/osdep.c
index e996c4744..8d074e7a2 100644
--- a/util/osdep.c
+++ b/util/osdep.c
@@ -248,6 +248,35 @@ static int qemu_lock_fcntl(int fd, int64_t start, int64_t len, int fl_type)
     return ret == -1 ? -errno : 0;
 }
 
+/*
+ * [0] qemu_lock_fd
+ * [0] raw_handle_perm_lock
+ * [0] raw_check_perm
+ * [0] bdrv_drv_set_perm
+ * [0] bdrv_node_refresh_perm
+ * [0] bdrv_list_refresh_perms
+ * [0] bdrv_replace_node_common
+ * [0] mirror_start_job
+ * [0] commit_active_start
+ * [0] qmp_block_commit
+ * [0] qmp_marshal_block_commit
+ * [0] do_qmp_dispatch_bh
+ * [0] aio_bh_poll
+ * [0] aio_dispatch
+ * [0] aio_ctx_dispatch
+ * [0] g_main_context_dispatch
+ * [0] glib_pollfds_poll
+ * [0] os_host_main_loop_wait
+ * [0] main_loop_wait
+ * [0] qemu_main_loop
+ * [0] qemu_default_main
+ * [0] __libc_start_main
+ * [0] _start
+ *
+ * called by:
+ *   - block/file-posix.c|872| <<raw_apply_lock_bytes>> ret = qemu_lock_fd(fd, off, 1, false);
+ *   - block/file-posix.c|894| <<raw_apply_lock_bytes>> ret = qemu_lock_fd(fd, off, 1, false);
+ */
 int qemu_lock_fd(int fd, int64_t start, int64_t len, bool exclusive)
 {
     return qemu_lock_fcntl(fd, start, len, exclusive ? F_WRLCK : F_RDLCK);
-- 
2.43.5

