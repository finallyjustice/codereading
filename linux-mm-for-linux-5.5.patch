From 4757cad7190d35cd8d2968d9b70fa1952eca47c7 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Thu, 26 Mar 2020 14:33:29 -0700
Subject: [PATCH 1/1] linux mm for linux-5.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 mm/slab.h |   4 +++
 mm/slub.c | 106 ++++++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 110 insertions(+)

diff --git a/mm/slab.h b/mm/slab.h
index 7e94700aa78c..c49e2b037c4d 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -625,6 +625,10 @@ struct kmem_cache_node {
 
 };
 
+/*
+ * struct kmem_cache_node *node[MAX_NUMNODES];
+ * 返回s->node[node]
+ */
 static inline struct kmem_cache_node *get_node(struct kmem_cache *s, int node)
 {
 	return s->node[node];
diff --git a/mm/slub.c b/mm/slub.c
index 8eafccf75940..ea0db2cbe495 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -131,6 +131,19 @@ void *fixup_red_left(struct kmem_cache *s, void *p)
 	return p;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1883| <<get_partial_node>> if (!kmem_cache_has_cpu_partial(s)
+ *   - mm/slub.c|2889| <<__slab_free>> if (kmem_cache_has_cpu_partial(s) && !prior) {
+ *   - mm/slub.c|2946| <<__slab_free>> if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
+ *   - mm/slub.c|3505| <<set_cpu_partial>> if (!kmem_cache_has_cpu_partial(s))
+ *   - mm/slub.c|5030| <<cpu_partial_store>> if (objects && !kmem_cache_has_cpu_partial(s))
+ *
+ * 使用了CONFIG_SLUB_CPU_PARTIAL的情况下, 当kmem_cache->flags设置了debug的任何flag的时候返回false
+ * 没有CONFIG_SLUB_CPU_PARTIAL直接返回false
+ * ol支持CONFIG_SLUB_CPU_PARTIAL
+ * 说明用debug的时候不支持kmem_cache_has_cpu_partial
+ */
 static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
@@ -360,6 +373,11 @@ static __always_inline void slab_unlock(struct page *page)
 }
 
 /* Interrupts must be disabled (for the fallback code to work right) */
+/*
+ * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+ * 核心思想是判断page->freelist和page->counters是否和old的相等
+ * 如果相等,则吧page->freelist和page->counters都更新成新的
+ */
 static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -397,6 +415,11 @@ static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page
 	return false;
 }
 
+/*
+ * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+ * 核心思想是判断page->freelist和page->counters是否和old的相等
+ * 如果相等,则吧page->freelist和page->counters都更新成新的
+ */
 static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -1756,6 +1779,16 @@ static void discard_slab(struct kmem_cache *s, struct page *page)
 /*
  * Management of partially allocated slabs.
  */
+/*
+ * called by:
+ *   - mm/slub.c|1796| <<add_partial>> __add_partial(n, page, tail);
+ *   - mm/slub.c|3476| <<early_kmem_cache_node_alloc>> __add_partial(n, page, DEACTIVATE_TO_HEAD);
+ *
+ * 把page通过page->slab_list加入kmem_cache_node->partial
+ * 如果tail==DEACTIVATE_TO_TAIL就放入尾部
+ * 否则放入头部
+ * 增加n->nr_partial++
+ */
 static inline void
 __add_partial(struct kmem_cache_node *n, struct page *page, int tail)
 {
@@ -1766,6 +1799,17 @@ __add_partial(struct kmem_cache_node *n, struct page *page, int tail)
 		list_add(&page->slab_list, &n->partial);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2218| <<deactivate_slab>> add_partial(n, page, tail);
+ *   - mm/slub.c|2301| <<unfreeze_partials>> add_partial(n, page, DEACTIVATE_TO_TAIL);
+ *   - mm/slub.c|2994| <<__slab_free>> add_partial(n, page, DEACTIVATE_TO_TAIL);
+ *
+ * 把page通过page->slab_list加入kmem_cache_node->partial
+ * 如果tail==DEACTIVATE_TO_TAIL就放入尾部
+ * 否则放入头部
+ * 增加n->nr_partial++
+ */
 static inline void add_partial(struct kmem_cache_node *n,
 				struct page *page, int tail)
 {
@@ -1773,6 +1817,16 @@ static inline void add_partial(struct kmem_cache_node *n,
 	__add_partial(n, page, tail);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1848| <<acquire_slab>> remove_partial(n, page);
+ *   - mm/slub.c|2213| <<deactivate_slab>> remove_partial(n, page);
+ *   - mm/slub.c|3005| <<__slab_free>> remove_partial(n, page);
+ *   - mm/slub.c|3786| <<free_partial>> remove_partial(n, page);
+ *
+ * 把page->slab_list从kmem_cache_node->partial移走
+ * 减少n->nr_partial--
+ */
 static inline void remove_partial(struct kmem_cache_node *n,
 					struct page *page)
 {
@@ -2036,10 +2090,32 @@ static void init_kmem_cache_cpus(struct kmem_cache *s)
 /*
  * Remove the cpu slab
  */
+/*
+ * called by:
+ *   - mm/slub.c|2307| <<flush_slab>> deactivate_slab(s, c->page, c->freelist, c);
+ *   - mm/slub.c|2559| <<___slab_alloc>> deactivate_slab(s, page, c->freelist, c);
+ *   - mm/slub.c|2570| <<___slab_alloc>> deactivate_slab(s, page, c->freelist, c);
+ *   - mm/slub.c|2625| <<___slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist), c);
+ *
+ * Remove the cpu slab
+ * 该函数会将CPU缓存对象的freelist中的对象,还给slab对象(page)
+ * 然后把这个page放入partial或者根据情况释放
+ * 关键最后会进行以下:
+ *   c->page = NULL;
+ *   c->freelist = NULL;
+ *
+ * 假设从___slab_alloc()第一次进入这里, page->freelist=NULL, c->page=page,
+ * freelist是第一个(要分配出去的)object
+ * 第3个参数是get_freepointer(s, freelist), 也就是分配出去的下一个
+ */
 static void deactivate_slab(struct kmem_cache *s, struct page *page,
 				void *freelist, struct kmem_cache_cpu *c)
 {
 	enum slab_modes { M_NONE, M_PARTIAL, M_FULL, M_FREE };
+	/*
+	 * struct kmem_cache_node *node[MAX_NUMNODES];
+	 * 返回s->node[node]
+	 */
 	struct kmem_cache_node *n = get_node(s, page_to_nid(page));
 	int lock = 0;
 	enum slab_modes l = M_NONE, m = M_NONE;
@@ -2050,6 +2126,7 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 
 	if (page->freelist) {
 		stat(s, DEACTIVATE_REMOTE_FREES);
+		/* DEACTIVATE_TO_TAIL: Cpu slab was moved to the tail of partials */
 		tail = DEACTIVATE_TO_TAIL;
 	}
 
@@ -2068,11 +2145,35 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 		do {
 			prior = page->freelist;
 			counters = page->counters;
+			/*
+			 * freelist的下一个指向prior
+			 */
 			set_freepointer(s, freelist, prior);
+			/*
+			 * struct kmem_cache *slab_cache; // not slob
+			 * // Double-word boundary
+			 * void *freelist;         // first free object
+			 * union {
+			 *	void *s_mem;    // slab: first object
+			 *	unsigned long counters;         // SLUB
+			 *	struct {                        // SLUB
+			 *		unsigned inuse:16;
+			 *		unsigned objects:15;
+			 *		unsigned frozen:1;
+			 *	};
+			 * };
+			 *
+			 * 下面的inuse是counters的一部分
+			 */
 			new.counters = counters;
 			new.inuse--;
 			VM_BUG_ON(!new.frozen);
 
+			/*
+			 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+			 * 核心思想是判断page->freelist和page->counters是否和old的相等
+			 * 如果相等,则吧page->freelist和page->counters都更新成新的
+			 */
 		} while (!__cmpxchg_double_slab(s, page,
 			prior, counters,
 			freelist, new.counters,
@@ -2151,6 +2252,11 @@ static void deactivate_slab(struct kmem_cache *s, struct page *page,
 	}
 
 	l = m;
+	/*
+	 * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+	 * 核心思想是判断page->freelist和page->counters是否和old的相等
+	 * 如果相等,则吧page->freelist和page->counters都更新成新的
+	 */
 	if (!__cmpxchg_double_slab(s, page,
 				old.freelist, old.counters,
 				new.freelist, new.counters,
-- 
2.17.1

