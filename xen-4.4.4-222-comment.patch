From c0528dec5374a11925b673e99c6e923f52f5e603 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 3 Feb 2020 22:42:14 -0800
Subject: [PATCH 1/1] xen-4.4.4-222-comment

xen-4.4.4-222

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 tools/libxc/xc_dom_x86.c       |   4 +
 tools/libxc/xc_domain.c        |  20 ++
 tools/libxc/xc_hvm_build_x86.c |  20 ++
 xen/arch/x86/domain.c          |  38 +++
 xen/arch/x86/domain_page.c     |  17 ++
 xen/arch/x86/mm.c              |  16 ++
 xen/common/domain.c            |   4 +
 xen/common/memory.c            |  21 ++
 xen/common/page_alloc.c        | 463 +++++++++++++++++++++++++++++++++
 xen/include/asm-x86/domain.h   |  15 ++
 xen/include/asm-x86/guest_pt.h |   9 +
 xen/include/asm-x86/mm.h       |  34 +++
 xen/include/xen/mm.h           |  10 +
 xen/include/xen/sched.h        |  15 ++
 14 files changed, 686 insertions(+)

diff --git a/tools/libxc/xc_dom_x86.c b/tools/libxc/xc_dom_x86.c
index b2256f2c5e..6ea4408480 100644
--- a/tools/libxc/xc_dom_x86.c
+++ b/tools/libxc/xc_dom_x86.c
@@ -757,6 +757,10 @@ static int x86_shadow(xc_interface *xch, domid_t domid)
     return rc;
 }
 
+/*
+ * x86下调用的例子:
+ *   - libxc/xc_dom_boot.c|149| <<xc_dom_boot_mem_init>> rc = arch_setup_meminit(dom);
+ */
 int arch_setup_meminit(struct xc_dom_image *dom)
 {
     int rc;
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 251b49e728..d26b02b3f1 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -902,6 +902,12 @@ int xc_domain_claim_pages(xc_interface *xch,
     return err;
 }
 
+/*
+ * called by:
+ *   - libxc/xc_domain.c|943| <<xc_domain_populate_physmap_exact>> err = xc_domain_populate_physmap(xch, domid, nr_extents,
+ *   - libxc/xc_hvm_build_x86.c|484| <<setup_guest>> done = xc_domain_populate_physmap(xch, dom, nr_extents,
+ *   - libxc/xc_hvm_build_x86.c|525| <<setup_guest>> done = xc_domain_populate_physmap(xch, dom, nr_extents,
+ */
 int xc_domain_populate_physmap(xc_interface *xch,
                                uint32_t domid,
                                unsigned long nr_extents,
@@ -931,6 +937,20 @@ int xc_domain_populate_physmap(xc_interface *xch,
     return err;
 }
 
+/*
+ * x86下调用的例子:
+ *   - libxc/xc_dom_x86.c|796| <<arch_setup_meminit>> rc = xc_domain_populate_physmap_exact(dom->xch, dom->guest_domid,
+ *   - libxc/xc_dom_x86.c|901| <<arch_setup_meminit>> rc = xc_domain_populate_physmap_exact(dom->xch,
+ *   - libxc/xc_domain_restore.c|147| <<alloc_superpage_mfns>> if (xc_domain_populate_physmap_exact(xch, dom, max, SUPERPAGE_PFN_SHIFT,
+ *   - libxc/xc_domain_restore.c|211| <<uncanonicalize_pagetable>> rc = xc_domain_populate_physmap_exact(xch, dom, nr_mfns, 0, 0,
+ *   - libxc/xc_domain_restore.c|1149| <<apply_batch>> if ( xc_domain_populate_physmap_exact(xch, dom, 1,
+ *   - libxc/xc_domain_restore.c|1221| <<apply_batch>> rc = xc_domain_populate_physmap_exact(xch, dom, nr_mfns, 0, 0,
+ *   - libxc/xc_hvm_build_x86.c|418| <<setup_guest>> rc = xc_domain_populate_physmap_exact(
+ *   - libxc/xc_hvm_build_x86.c|543| <<setup_guest>> rc = xc_domain_populate_physmap_exact(
+ *   - libxc/xc_hvm_build_x86.c|583| <<setup_guest>> rc = xc_domain_populate_physmap_exact(xch, dom, 1, 0, 0, &pfn);
+ *   - tests/xen-access/xen-access.c|266| <<xenaccess_init>> rc = xc_domain_populate_physmap_exact(xenaccess->xc_handle,
+ *   - xenpaging/xenpaging.c|351| <<xenpaging_init>> rc = xc_domain_populate_physmap_exact(paging->xc_handle,
+ */
 int xc_domain_populate_physmap_exact(xc_interface *xch,
                                      uint32_t domid,
                                      unsigned long nr_extents,
diff --git a/tools/libxc/xc_hvm_build_x86.c b/tools/libxc/xc_hvm_build_x86.c
index 245f0d88fd..9426f22cce 100644
--- a/tools/libxc/xc_hvm_build_x86.c
+++ b/tools/libxc/xc_hvm_build_x86.c
@@ -33,8 +33,24 @@
 
 #include <xen/libelf/libelf.h>
 
+/*
+ * 在以下使用SUPERPAGE_2MB_SHIFT:
+ *   - libxc/xc_hvm_build_x86.c|37| <<SUPERPAGE_2MB_NR_PFNS>> #define SUPERPAGE_2MB_NR_PFNS (1UL << SUPERPAGE_2MB_SHIFT)
+ *   - libxc/xc_hvm_build_x86.c|522| <<setup_guest>> unsigned long nr_extents = count >> SUPERPAGE_2MB_SHIFT;
+ *   - libxc/xc_hvm_build_x86.c|527| <<setup_guest>> page_array[cur_pages+(i<<SUPERPAGE_2MB_SHIFT)];
+ *   - libxc/xc_hvm_build_x86.c|530| <<setup_guest>> SUPERPAGE_2MB_SHIFT,
+ *   - libxc/xc_hvm_build_x86.c|537| <<setup_guest>> done <<= SUPERPAGE_2MB_SHIFT;
+ */
 #define SUPERPAGE_2MB_SHIFT   9
 #define SUPERPAGE_2MB_NR_PFNS (1UL << SUPERPAGE_2MB_SHIFT)
+/*
+ * 在以下使用SUPERPAGE_1GB_SHIFT:
+ *   - libxc/xc_hvm_build_x86.c|39| <<SUPERPAGE_1GB_NR_PFNS>> #define SUPERPAGE_1GB_NR_PFNS (1UL << SUPERPAGE_1GB_SHIFT)
+ *   - libxc/xc_hvm_build_x86.c|481| <<setup_guest>> unsigned long nr_extents = count >> SUPERPAGE_1GB_SHIFT;
+ *   - libxc/xc_hvm_build_x86.c|486| <<setup_guest>> page_array[cur_pages+(i<<SUPERPAGE_1GB_SHIFT)];
+ *   - libxc/xc_hvm_build_x86.c|489| <<setup_guest>> SUPERPAGE_1GB_SHIFT,
+ *   - libxc/xc_hvm_build_x86.c|496| <<setup_guest>> done <<= SUPERPAGE_1GB_SHIFT;
+ */
 #define SUPERPAGE_1GB_SHIFT   18
 #define SUPERPAGE_1GB_NR_PFNS (1UL << SUPERPAGE_1GB_SHIFT)
 
@@ -229,6 +245,10 @@ static int check_mmio_hole(uint64_t start, uint64_t memsize,
         return 1;
 }
 
+/*
+ * called by:
+ *   - libxc/xc_hvm_build_x86.c|675| <<xc_hvm_build>> sts = setup_guest(xch, domid, &args, image, image_size);
+ */
 static int setup_guest(xc_interface *xch,
                        uint32_t dom, struct xc_hvm_build_args *args,
                        char *image, unsigned long image_size)
diff --git a/xen/arch/x86/domain.c b/xen/arch/x86/domain.c
index 8acc0c7d0c..6b30b10e59 100644
--- a/xen/arch/x86/domain.c
+++ b/xen/arch/x86/domain.c
@@ -119,6 +119,11 @@ static void play_dead(void)
     (*dead_idle)();
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|148| <<startup_cpu_idle_loop>> reset_stack_and_jump(idle_loop);
+ *   - arch/x86/domain.c|153| <<continue_idle_domain>> reset_stack_and_jump(idle_loop);
+ */
 static void idle_loop(void)
 {
     for ( ; ; )
@@ -1983,6 +1988,18 @@ int hypercall_xlat_continuation(unsigned int *id, unsigned int nr,
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|2168| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->xenpage_list, ~0UL);
+ *   - arch/x86/domain.c|2175| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ *   - arch/x86/domain.c|2182| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l3_page_table);
+ *   - arch/x86/domain.c|2189| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l2_page_table);
+ *
+ * page_list被很多地方使用,这里只是三个例子:
+ *   - common/page_alloc.c|2130| <<assign_pages>> page_list_add_tail(&pg[i], &d->page_list);
+ *   - arch/x86/domain.c|180| <<dump_pageframe_info>> page_list_for_each ( page, &d->page_list )
+ *   - arch/x86/numa.c|392| <<dump_numa>> page_list_for_each(page, &d->page_list)
+ */
 static int relinquish_memory(
     struct domain *d, struct page_list_head *list, unsigned long type)
 {
@@ -2003,6 +2020,17 @@ static int relinquish_memory(
             continue;
         }
 
+        /*
+	 * Clear a bit and return its old value
+	 *
+	 * 使用_PGT_pinned的地方:
+	 *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+	 *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+	 */
         if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
             ret = put_page_and_type_preemptible(page);
         switch ( ret )
@@ -2095,6 +2123,10 @@ static int relinquish_memory(
     return ret;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> rc = domain_relinquish_resources(d);
+ */
 int domain_relinquish_resources(struct domain *d)
 {
     int ret;
@@ -2172,6 +2204,12 @@ int domain_relinquish_resources(struct domain *d)
         /* fallthrough */
 
     case RELMEM_l4:
+        /*
+	 * page_list被很多地方使用,这里只是三个例子:
+	 *   - common/page_alloc.c|2130| <<assign_pages>> page_list_add_tail(&pg[i], &d->page_list);
+	 *   - arch/x86/domain.c|180| <<dump_pageframe_info>> page_list_for_each ( page, &d->page_list )
+	 *   - arch/x86/numa.c|392| <<dump_numa>> page_list_for_each(page, &d->page_list)
+	 */
         ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
         if ( ret )
             return ret;
diff --git a/xen/arch/x86/domain_page.c b/xen/arch/x86/domain_page.c
index 56f9bb1244..6ec449cbea 100644
--- a/xen/arch/x86/domain_page.c
+++ b/xen/arch/x86/domain_page.c
@@ -172,6 +172,23 @@ void *map_domain_page(unsigned long mfn)
     return (void *)MAPCACHE_VIRT_START + pfn_to_paddr(idx);
 }
 
+/*
+ * MAPCACHE_VCPU_ENTRIES = 0x00000010
+ * MAPCACHE_ENTRIES      = 0x0000000000020000
+ * MAPCACHE_VIRT_START   = 0xffff820040000000
+ * MAPCACHE_VIRT_END     = 0xffff820060000000
+ *
+ * DIRECTMAP_VIRT_START = 0xffff830000000000
+ * DIRECTMAP_SIZE       = 0x00007c8000000000
+ * DIRECTMAP_VIRT_END   = 0xffffff8000000000
+ *
+ * LINEAR_PT_VIRT_START = 0xffff810000000000
+ * LINEAR_PT_VIRT_END   = 0xffff818000000000
+ *
+ * HYPERVISOR_VIRT_START = 0xffff800000000000
+ * HYPERVISOR_VIRT_END   = 0xffff880000000000
+ */
+
 void unmap_domain_page(const void *ptr)
 {
     unsigned int idx;
diff --git a/xen/arch/x86/mm.c b/xen/arch/x86/mm.c
index 176893d7c5..ac35f743a7 100644
--- a/xen/arch/x86/mm.c
+++ b/xen/arch/x86/mm.c
@@ -158,6 +158,22 @@ struct rangeset *__read_mostly mmio_ro_ranges;
 
 bool_t __read_mostly opt_allow_superpage;
 bool_t __read_mostly opt_allow_hugepage;
+/*
+ * 在以下使用opt_allow_superpage:
+ *   - arch/x86/domain_build.c|1029| <<construct_dom0>> if ( opt_allow_superpage )
+ *   - arch/x86/mm.c|169| <<L2_DISALLOW_MASK>> #define L2_DISALLOW_MASK (unlikely(opt_allow_superpage) \
+ *   - arch/x86/mm.c|252| <<init_frametable>> opt_allow_superpage = 1;
+ *   - arch/x86/mm.c|273| <<init_frametable>> if (opt_allow_superpage)
+ *   - arch/x86/mm.c|1071| <<get_page_from_l2e>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|2679| <<mark_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2722| <<unmark_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2754| <<clear_superpage_mark>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|2769| <<get_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2809| <<put_superpage>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|3537| <<do_mmuext_op(MMUEXT_MARK_SUPER)>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm/shadow/common.c|57| <<shadow_domain_init>> if ( !is_pv_domain(d) || opt_allow_superpage )
+ *   - include/asm-x86/guest_pt.h|191| <<guest_supports_superpages>> ? opt_allow_superpage
+ */
 boolean_param("allowsuperpage", opt_allow_superpage);
 boolean_param("allowhugepage", opt_allow_hugepage);
 
diff --git a/xen/common/domain.c b/xen/common/domain.c
index e44d403d96..735834ff6a 100644
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -612,6 +612,10 @@ int rcu_lock_live_remote_domain_by_id(domid_t dom, struct domain **d)
     return 0;
 }
 
+/*
+ * called by:
+ *   - common/domctl.c|726| <<do_domctl(XEN_DOMCTL_destroydomain)>> ret = domain_kill(d);
+ */
 int domain_kill(struct domain *d)
 {
     int rc = 0;
diff --git a/xen/common/memory.c b/xen/common/memory.c
index a07b18a9dc..bdc111d8ad 100644
--- a/xen/common/memory.c
+++ b/xen/common/memory.c
@@ -1100,6 +1100,27 @@ long do_memory_op(unsigned long cmd, XEN_GUEST_HANDLE_PARAM(void) arg)
     }
 
     case XENMEM_claim_pages:
+    /*
+     * Attempt to stake a claim for a domain on a quantity of pages
+     * of system RAM, but _not_ assign specific pageframes.  Only
+     * arithmetic is performed so the hypercall is very fast and need
+     * not be preemptible, thus sidestepping time-of-check-time-of-use
+     * races for memory allocation.  Returns 0 if the hypervisor page
+     * allocator has atomically and successfully claimed the requested
+     * number of pages, else non-zero.
+     *   
+     * Any domain may have only one active claim.  When sufficient memory
+     * has been allocated to resolve the claim, the claim silently expires.
+     * Claiming zero pages effectively resets any outstanding claim and
+     * is always successful.
+     *           
+     * Note that a valid claim may be staked even after memory has been
+     * allocated for a domain.  In this case, the claim is not incremental,
+     * i.e. if the domain's tot_pages is 3, and a claim is staked for 10,
+     * only 7 additional pages are claimed.
+     *       
+     * Caller must be privileged or the hypercall fails.
+     */
         if ( copy_from_guest(&reservation, arg, 1) )
             return -EFAULT;
 
diff --git a/xen/common/page_alloc.c b/xen/common/page_alloc.c
index 95db602e7c..6a32b9da5c 100644
--- a/xen/common/page_alloc.c
+++ b/xen/common/page_alloc.c
@@ -51,6 +51,112 @@
 #define p2m_pod_offline_or_broken_replace(pg) BUG_ON(pg != NULL)
 #endif
 
+/*
+ * Here is how we expect memory pages to be reclaimed during VM destroy, that
+ * is, when we run "xm destroy" on dom0.
+ *
+ * "XendDomainInfo.destroy" is printed by below line 3842.
+ *
+ * 3830     def destroy(self):
+ * 3831         """Cleanup VM and destroy domain.  Nothrow guarantee."""
+ * 3832
+ * 3833         if self.domid is None:
+ * 3834             return
+ * 3835
+ * 3836         if self.destroying == False:
+ * 3837             self.destroying = True
+ * 3838         else:
+ * 3839             raise VmError("Domain (domid=%s) is destroying, please wait!", str(self.domid))
+ * 3840
+ * 3841         from xen.xend import XendDomain
+ * 3842         log.debug("XendDomainInfo.destroy: domid=%s", str(self.domid))
+ * 3843
+ * 3844         paths = self._prepare_phantom_paths()
+ * 3845
+ * 3846         if self.dompath is not None:
+ * 3847             try:
+ * 3848                 xc.domain_destroy_hook(self.domid)
+ * 3849                 xc.domain_pause(self.domid)
+ * 3850                 do_FLR(self.domid, self.info.is_hvm())
+ * 3851                 xc.domain_destroy(self.domid)
+ * 3852                 for state in DOM_STATES_OLD:
+ * 3853                     self.info[state] = 0
+ * 3854                 self._stateSet(DOM_STATE_HALTED)
+ *
+ * xend would run destroy() in python/xen/xend/XendDomainInfo.py, which would
+ * call xc_domain_destroy() in libxc, after printing "DEBUG
+ * (XendDomainInfo:3789) XendDomainInfo.destroy: domid=18".
+ *
+ * 83 int xc_domain_destroy(xc_interface *xch,
+ * 84                       uint32_t domid)
+ * 85 {
+ * 86     int ret;
+ * 87     DECLARE_DOMCTL;
+ * 88     domctl.cmd = XEN_DOMCTL_destroydomain;
+ * 89     domctl.domain = (domid_t)domid;
+ * 90     do {
+ * 91         ret = do_domctl(xch, &domctl);
+ * 92     } while ( ret && (errno == EAGAIN) );
+ * 93     return ret;
+ * 94 }
+ *
+ * xc_domain_destroy() traps to hypervisor via XEN_DOMCTL_destroydomain() and
+ * this would persists until the destroy is completed by hypervisor.
+ *
+ * This can be confirmed by oswatcher in sosreport that cpu usage of xend is
+ * always 100% in top command.
+ *
+ * In hypervisor side, XEN_DOMCTL_destroydomain is handled by
+ * do_domctl()->domain_kill():
+ *
+ * domain_kill()
+ *  -> domain_relinquish_resources()
+ *
+ * As shown in below code, if domain_relinquish_resources() cannot return 0 with
+ * all memory pages reclaimed, xend would trap to run again until all pages are
+ * reclaimed. I have verified with mini-os whose memory is only 16MB.
+ *
+ * 637     case DOMDYING_dying:
+ * 638         rc = domain_relinquish_resources(d);
+ * 639         if ( rc != 0 )
+ * 640         {
+ * 641             if ( rc == -ERESTART )
+ * 642                 rc = -EAGAIN;
+ * 643             break;
+ * 644         }
+ *
+ * Most memory pages (indeed regular pages) are reclaimed at relinquish_memory()
+ * at line 2175. Although the argument at 2175 is PGT_l4_page_table, regular
+ * pages are reclaimed as well.
+ *
+ * 2098 int domain_relinquish_resources(struct domain *d)
+ * 2099 {
+ * ... ...
+ * 2174     case RELMEM_l4:
+ * 2175         ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ * 2176         if ( ret )
+ * 2177             return ret;
+ * 2178         d->arch.relmem = RELMEM_l3;
+ * 2179         // fallthrough
+ *
+ * According to my test, line 2040 should return true so that the page type does
+ * not need to be PGT_l4_page_table.
+ *
+ * 1986 static int relinquish_memory(
+ * 1987     struct domain *d, struct page_list_head *list, unsigned long type)
+ * 1988 {
+ * ... ...
+ * 2040             if ( likely((x & PGT_type_mask) != type) ||
+ * 2041                  likely(!(x & (PGT_validated|PGT_partial))) )
+ * 2042                 break;
+ * ... ...
+ * 2079         // Put the page on the list and /then/ potentially free it.
+ * 2080         page_list_add_tail(page, &d->arch.relmem_list);
+ * 2081         put_page(page);
+ *
+ * Until after line 2081, the d->tot_pages would not be decremented.
+ */
+
 /*
  * Comma-separated list of hexadecimal page numbers containing bad bytes.
  * e.g. 'badpage=0x3f45,0x8a321'.
@@ -81,6 +187,13 @@ integer_param("dma_bits", dma_bitsize);
 #define round_pgdown(_p)  ((_p)&PAGE_MASK)
 #define round_pgup(_p)    (((_p)+(PAGE_SIZE-1))&PAGE_MASK)
 
+/*
+ * 在以下使用pglist_lock:
+ *   - common/page_alloc.c|877| <<reserve_offlined_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|883| <<reserve_offlined_page>> spin_unlock(&pglist_lock);
+ *   - common/page_alloc.c|1210| <<online_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|1241| <<online_page>> spin_unlock(&pglist_lock);
+ */
 static DEFINE_SPINLOCK(pglist_lock);
 /* Offlined page list, protected by pglist_lock. */
 static PAGE_LIST_HEAD(page_offlined_list);
@@ -88,9 +201,35 @@ static PAGE_LIST_HEAD(page_offlined_list);
 static PAGE_LIST_HEAD(page_broken_list);
 
 /* A rough flag to indicate whether a node have need_scrub pages */
+/*
+ * 修改node_need_scrub[]的地方:
+ *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+ *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+ *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+ * 在以下使用node_need_scrub:
+ *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+ *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+ *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+ */
 static unsigned long node_need_scrub[MAX_NUMNODES];
+/*
+ * 在以下使用is_scrubbing:
+ *   - common/page_alloc.c|1673| <<scrub_free_pages>> if ( test_and_set_bool(per_cpu(is_scrubbing, cpumask_first(per_cpu(cpu_sibling_mask, cpu)))) )
+ *   - common/page_alloc.c|1736| <<scrub_free_pages>> was_scrubbing = test_and_clear_bool(per_cpu(is_scrubbing,
+ */
 static DEFINE_PER_CPU(bool_t, is_scrubbing);
+/*
+ * 在以下使用scrub_list_cpu:
+ *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, scrub_list_cpu);
+/*
+ * 在以下使用free_list_cpu:
+ *   - common/page_alloc.c|1612| <<__scrub_free_pages>> struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, free_list_cpu);
 
 /*************************
@@ -120,6 +259,12 @@ static void __init boot_bug(int line)
 }
 #define BOOT_BUG_ON(p) if ( p ) boot_bug(__LINE__);
 
+/*
+ * called by:
+ *   - common/page_alloc.c|203| <<bootmem_region_zap>> bootmem_region_add(e, _e);
+ *   - common/page_alloc.c|224| <<init_boot_pages>> bootmem_region_add(ps >> PAGE_SHIFT, pe >> PAGE_SHIFT);
+ *   - common/page_alloc.c|302| <<alloc_boot_pages>> bootmem_region_add(pg + nr_pfns, _e);
+ */
 static void __init bootmem_region_add(unsigned long s, unsigned long e)
 {
     unsigned int i;
@@ -301,8 +446,31 @@ static long midsize_alloc_zone_pages;
 static DEFINE_SPINLOCK(heap_lock_globals);
 static spinlock_t heap_lock[MAX_NUMNODES];
 
+/*
+ * 在以下使用outstanding_claims:
+ *   - common/page_alloc.c|380| <<domain_adjust_tot_pages>> sys_before = outstanding_claims;
+ *   - common/page_alloc.c|383| <<domain_adjust_tot_pages>> outstanding_claims = sys_after;
+ *   - common/page_alloc.c|406| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+ *   - common/page_alloc.c|439| <<domain_set_outstanding_pages>> avail_pages -= outstanding_claims;
+ *   - common/page_alloc.c|451| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+ *   - common/page_alloc.c|463| <<get_outstanding_claims>> *outstanding_pages = outstanding_claims;
+ *   - common/page_alloc.c|709| <<alloc_heap_pages>> if ( (outstanding_claims + request >
+ *   - common/page_alloc.c|737| <<alloc_heap_pages>> (opt_tmem ? tmem_freeable_pages() : 0) - outstanding_claims;
+ */
 static long outstanding_claims; /* total outstanding claims by all domains */
 
+/*
+ * called by:
+ *   - arch/x86/mm.c|4419| <<donate_page>> domain_adjust_tot_pages(d, 1);
+ *   - arch/x86/mm.c|4483| <<steal_page>> if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )
+ *   - arch/x86/mm/mem_sharing.c|659| <<page_make_sharable>> drop_dom_ref = !domain_adjust_tot_pages(d, -1);
+ *   - arch/x86/mm/mem_sharing.c|706| <<page_make_private>> if ( domain_adjust_tot_pages(d, 1) == 1 )
+ *   - common/grant_table.c|1995| <<gnttab_transfer>> if ( unlikely(domain_adjust_tot_pages(e, 1) == 1) )
+ *   - common/grant_table.c|2010| <<gnttab_transfer>> bool_t drop_dom_ref = !domain_adjust_tot_pages(e, -1);
+ *   - common/memory.c|636| <<memory_exchange>> !domain_adjust_tot_pages(d, -dec_count));
+ *   - common/page_alloc.c|1967| <<assign_pages>> domain_adjust_tot_pages(d, 1 << order);
+ *   - common/page_alloc.c|2067| <<free_domheap_pages>> drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
+ */
 unsigned long domain_adjust_tot_pages(struct domain *d, long pages)
 {
     long dom_before, dom_after, dom_claimed, sys_before, sys_after;
@@ -336,6 +504,11 @@ out:
     return d->tot_pages;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> domain_set_outstanding_pages(d, 0);
+ *   - common/memory.c|1122| <<do_mem_op(XENMEM_claim_pages)>> rc = domain_set_outstanding_pages(d, reservation.nr_extents);
+ */
 int domain_set_outstanding_pages(struct domain *d, unsigned long pages)
 {
     int ret = -ENOMEM;
@@ -406,14 +579,36 @@ out:
     return ret;
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|268| <<do_sysctl(XEN_SYSCTL_physinfo)>> get_outstanding_claims(&pi->free_pages, &pi->outstanding_pages);
+ */
 void get_outstanding_claims(uint64_t *free_pages, uint64_t *outstanding_pages)
 {
     spin_lock(&heap_lock_globals);
+    /*
+     * 在以下使用outstanding_claims:
+     *   - common/page_alloc.c|380| <<domain_adjust_tot_pages>> sys_before = outstanding_claims;
+     *   - common/page_alloc.c|383| <<domain_adjust_tot_pages>> outstanding_claims = sys_after;
+     *   - common/page_alloc.c|406| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+     *   - common/page_alloc.c|439| <<domain_set_outstanding_pages>> avail_pages -= outstanding_claims;
+     *   - common/page_alloc.c|451| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+     *   - common/page_alloc.c|463| <<get_outstanding_claims>> *outstanding_pages = outstanding_claims;
+     *   - common/page_alloc.c|709| <<alloc_heap_pages>> if ( (outstanding_claims + request >
+     *   - common/page_alloc.c|737| <<alloc_heap_pages>> (opt_tmem ? tmem_freeable_pages() : 0) - outstanding_claims;
+     */
     *outstanding_pages = outstanding_claims;
+    /* 把从MEMZONE_XEN + 1到NR_ZONES - 1的每个node的page全加起来 */
     *free_pages =  avail_domheap_pages();
     spin_unlock(&heap_lock_globals);
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|306| <<do_sysctl(XEN_SYSCTL_numainfo)>> get_dirty_pages(i) << PAGE_SHIFT : 0ul;
+ *
+ * 返回对应参数node的node_need_scrub[node]
+ */
 unsigned long get_dirty_pages(unsigned int node)
 {
      unsigned long dirty_pages;
@@ -432,6 +627,10 @@ static unsigned int __read_mostly xenheap_bits;
 #define xenheap_bits 0
 #endif
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1551| <<init_heap_pages>> n = init_node_heap(nid, page_to_mfn(pg+i), nr_pages - i,
+ */
 static unsigned long init_node_heap(int node, unsigned long mfn,
                                     unsigned long nr, bool_t *use_tail)
 {
@@ -519,6 +718,10 @@ static unsigned long low_mem_virq_orig      = 0;
 static unsigned int  low_mem_virq_th_order  = 0;
 
 /* Perform bootstrapping checks and set bounds */
+/*
+ * called by:
+ *   - common/page_alloc.c|1873| <<scrub_heap_pages>> setup_low_mem_virq();
+ */
 static void __init setup_low_mem_virq(void)
 {
     unsigned int order;
@@ -570,6 +773,10 @@ static void __init setup_low_mem_virq(void)
             low_mem_virq_th);
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|943| <<alloc_heap_pages>> check_low_mem_virq(avail_pages);
+ */
 static void check_low_mem_virq(unsigned long avail_pages)
 {
     if ( unlikely(avail_pages <= low_mem_virq_th) )
@@ -602,6 +809,12 @@ static void check_low_mem_virq(unsigned long avail_pages)
 }
 
 /* Allocate 2^@order contiguous pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1812| <<alloc_xenheap_pages>> pg = alloc_heap_pages(MEMZONE_XEN, MEMZONE_XEN,
+ *   - common/page_alloc.c|1988| <<alloc_domheap_pages>> pg = alloc_heap_pages(dma_zone + 1, zone_hi, order, memflags, d);
+ *   - common/page_alloc.c|1992| <<alloc_domheap_pages>> ((pg = alloc_heap_pages(MEMZONE_XEN + 1, zone_hi, order,
+ */
 static struct page_info *alloc_heap_pages(
     unsigned int zone_lo, unsigned int zone_hi,
     unsigned int order, unsigned int memflags,
@@ -697,6 +910,9 @@ static struct page_info *alloc_heap_pages(
                 continue;
 
             /* Find smallest order which can satisfy the request. */
+	    /*
+	     * 感觉就是在这里从对应的heap移除的!!!
+	     */
             for ( j = order; j <= MAX_ORDER; j++ )
                 if ( (pg = page_list_remove_head(&heap(node, zone, j))) )
                     goto found;
@@ -765,6 +981,17 @@ static struct page_info *alloc_heap_pages(
 
     for ( i = 0; i < (1 << order); i++ )
     {
+        /*
+	 * 在以下使用_PGC_need_scrub:
+	 *   - common/page_alloc.c|947| <<alloc_heap_pages>> if ( test_and_clear_bit(_PGC_need_scrub, &pg[i].count_info) )
+	 *   - common/page_alloc.c|1093| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+	 *   - common/page_alloc.c|1098| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+	 *   - common/page_alloc.c|1114| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+	 *   - common/page_alloc.c|1119| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+	 *   - common/page_alloc.c|1829| <<__scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+	 *   - common/page_alloc.c|1900| <<scrub_free_pages>> if ( !test_bit(_PGC_need_scrub, &(pg->count_info)) )
+	 *   - common/page_alloc.c|1919| <<scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+	 */
         if ( test_and_clear_bit(_PGC_need_scrub, &pg[i].count_info) )
             need_scrub = 1;
 
@@ -786,6 +1013,18 @@ static struct page_info *alloc_heap_pages(
         flush_page_to_ram(page_to_mfn(&pg[i]));
     }
 
+    /*
+     * 修改node_need_scrub[]的地方:
+     *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+     *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+     *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+     * 在以下使用node_need_scrub:
+     *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+     *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+     *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+     *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+     *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+     */
     if ( need_scrub )
         node_need_scrub[node] -= (1 << order);
 
@@ -793,6 +1032,10 @@ static struct page_info *alloc_heap_pages(
 
     if ( need_scrub )
     {
+        /*
+	 * scrub_one_page():
+	 * 把page给scrub了, 不改变任何flag
+	 */
         for ( i = 0; i < (1 << order); i++ )
             scrub_one_page(&pg[i]);
     }
@@ -804,6 +1047,11 @@ static struct page_info *alloc_heap_pages(
 }
 
 /* Remove any offlined page in the buddy pointed to by head. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1253| <<free_heap_pages>> reserve_offlined_page(pg);
+ *   - common/page_alloc.c|1310| <<reserve_heap_page>> return reserve_offlined_page(head);
+ */
 static int reserve_offlined_page(struct page_info *head)
 {
     unsigned int node = phys_to_nid(page_to_maddr(head));
@@ -888,6 +1136,11 @@ static int reserve_offlined_page(struct page_info *head)
     return count;
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1205| <<free_heap_pages>> merge_free_trunks(pg, order, node, zone, need_scrub);
+ *   - common/page_alloc.c|1850| <<__scrub_free_pages>> merge_free_trunks(pg, order, node, page_to_zone(pg), 0);
+ */
 static void merge_free_trunks(struct page_info *pg, unsigned int order,
     unsigned int node, unsigned int zone, bool_t need_scrub)
 {
@@ -951,6 +1204,23 @@ static void merge_free_trunks(struct page_info *pg, unsigned int order,
 }
 
 /* Free 2^@order set of pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1295| <<online_page>> free_heap_pages(pg, 0, 0);
+ *   - common/page_alloc.c|1366| <<init_heap_pages>> free_heap_pages(pg+i, 0, 0);
+ *   - common/page_alloc.c|1863| <<free_xenheap_pages>> free_heap_pages(virt_to_page(v), order, 0);
+ *   - common/page_alloc.c|1921| <<free_xenheap_pages>> free_heap_pages(pg, order, 1);
+ *   - common/page_alloc.c|2030| <<alloc_domheap_pages>> free_heap_pages(pg, order, 0);
+ *   - common/page_alloc.c|2089| <<free_domheap_pages>> free_heap_pages(pg, order, 1);
+ *   - common/page_alloc.c|2091| <<free_domheap_pages>> free_heap_pages(pg, order, 0);
+ *   - common/page_alloc.c|2096| <<free_domheap_pages>> free_heap_pages(pg, 0, 1);
+ *   - common/page_alloc.c|2102| <<free_domheap_pages>> free_heap_pages(pg, order, 1);
+ *
+ * 关于参数的need_scrub什么时候是0,什么时候是1...
+ * Normally we expect a domain to clear pages before freeing them, if
+ * it cares about the secrecy of their contents. However, after a
+ * domain has died we assume responsibility for erasure.
+ */
 static void free_heap_pages(
     struct page_info *pg, unsigned int order, bool_t need_scrub)
 {
@@ -977,6 +1247,13 @@ static void free_heap_pages(
          * In all the above cases there can be no guest mappings of this page.
          */
         ASSERT(!page_state_is(&pg[i], offlined));
+	/*
+	 * 设置pg[i].count_info为:
+	 *  - (pg[i].count_info & PGC_broken) |
+	 *  - (page_state_is(&pg[i], offlining) ? PGC_state_offlined : PGC_state_free)
+	 *
+	 * 就是说这里有可能设置成PGC_state_free
+	 */
         pg[i].count_info =
             ((pg[i].count_info & PGC_broken) |
              (page_state_is(&pg[i], offlining)
@@ -1011,6 +1288,11 @@ static void free_heap_pages(
         node_need_scrub[node] += (1 << order);
     }
 
+    /*
+     * called by:
+     *   - common/page_alloc.c|1205| <<free_heap_pages>> merge_free_trunks(pg, order, node, zone, need_scrub);
+     *   - common/page_alloc.c|1850| <<__scrub_free_pages>> merge_free_trunks(pg, order, node, page_to_zone(pg), 0);
+     */
     merge_free_trunks(pg, order, node, zone, need_scrub);
 
     if ( tainted )
@@ -1317,6 +1599,17 @@ static void init_heap_pages(
     }
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|2357| <<avail_domheap_pages_region>> return avail_heap_pages(zone_lo, zone_hi, node);
+ *   - common/page_alloc.c|2362| <<avail_domheap_pages>> return avail_heap_pages(MEMZONE_XEN + 1,
+ *   - common/page_alloc.c|2369| <<avail_node_heap_pages>> return avail_heap_pages(MEMZONE_XEN, NR_ZONES -1, nodeid);
+ *   - common/page_alloc.c|2380| <<pagealloc_info>> avail_heap_pages(zone, zone, -1) << (PAGE_SHIFT-10));
+ *   - common/page_alloc.c|2390| <<pagealloc_info>> if ( (n = avail_heap_pages(zone, zone, -1)) != 0 )
+ *
+ * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+ * 如果node是-1就把所有node的加起来
+ */
 static unsigned long avail_heap_pages(
     unsigned int zone_lo, unsigned int zone_hi, unsigned int node)
 {
@@ -1338,6 +1631,10 @@ static unsigned long avail_heap_pages(
     return free_pages;
 }
 
+/*
+ * called by:
+ *   - common/tmem.c|1425| <<tmem_ensure_avail_pages>> free_mem = (tmem_page_list_pages + total_free_pages())
+ */
 unsigned long total_free_pages(void)
 {
     long ret;
@@ -1391,6 +1688,15 @@ void __init end_boot_allocator(void)
     printk("\n");
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1561| <<scrub_heap_pages>> on_selected_cpus(&all_worker_cpus, smp_scrub_heap_pages, NULL, 1);
+ *   - common/page_alloc.c|1618| <<scrub_heap_pages>> on_selected_cpus(&node_cpus, smp_scrub_heap_pages, &region[i], 1);
+ *
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ */
 static void __init smp_scrub_heap_pages(void *data)
 {
     unsigned long mfn, start, end;
@@ -1464,6 +1770,10 @@ int __init find_non_smt(unsigned int node, cpumask_t *dest)
  * Scrub all unallocated pages in all heap zones. This function uses all
  * online cpu's to scrub the memory in parallel.
  */
+/*
+ * x86下的调用:
+ *   - arch/x86/setup.c|1545| <<__start_xen>> scrub_heap_pages();
+ */
 void __init scrub_heap_pages(void)
 {
     cpumask_t node_cpus, all_worker_cpus;
@@ -1599,12 +1909,29 @@ void __init scrub_heap_pages(void)
 
 #define SCRUB_BATCH_ORDER 12
 /* return 1 if should continue */
+/*
+ * idle_loop()
+ *  -> scrub_free_pages()
+ *      -> __scrub_free_pages()
+ *
+ * called by:
+ *   - common/page_alloc.c|1726| <<scrub_free_pages>> } while ( __scrub_free_pages(node, cpu) );
+ */
 static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 {
     struct page_info *pg, *tmp;
     unsigned int i;
     int order;
+    /*
+     * 在以下使用scrub_list_cpu:
+     *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     */
     struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+    /*
+     * 在以下使用free_list_cpu:
+     *   - common/page_alloc.c|1612| <<__scrub_free_pages>> struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
+     */
     struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
 
     if ( page_list_empty(local_scrub_list) && page_list_empty(local_free_list) )
@@ -1622,6 +1949,10 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
             scrub_one_page(&pg[i]);
         }
         page_list_add_tail(pg, local_free_list);
+        /*
+	 * 如果有挂载的softirq, 就先退出,
+	 * 这样local_free_list和local_scrub_list都有可能有没处理的page
+	 */
         if ( softirq_pending(cpu) )
             return 0;
     }
@@ -1636,10 +1967,29 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
             page_list_del(pg, local_free_list);
             for ( i = 0; i < (1 << order); i++ )
             {
+                /*
+		 * 在以下使用PGC_state_free:
+		 *   - common/page_alloc.c|962| <<alloc_heap_pages>> BUG_ON(pg[i].count_info != PGC_state_free);
+		 *   - common/page_alloc.c|1202| <<free_heap_pages>> ? PGC_state_offlined : PGC_state_free));
+		 *   - common/page_alloc.c|1262| <<mark_page_offline>> nx |= (((x & PGC_state) == PGC_state_free)
+		 *   - common/page_alloc.c|1892| <<__scrub_free_pages>> pg[i].count_info |= PGC_state_free;
+		 */
                 pg[i].count_info |= PGC_state_free;
                 pg[i].count_info &= ~PGC_need_scrub;
             }
             merge_free_trunks(pg, order, node, page_to_zone(pg), 0);
+	    /*
+	     * 修改node_need_scrub[]的地方:
+	     *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+	     *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+	     *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+	     * 在以下使用node_need_scrub:
+	     *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+	     *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+	     *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+	     *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+	     *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+	     */
             node_need_scrub[node] -= (1 << order);
         }
         spin_unlock(&heap_lock[node]);
@@ -1648,13 +1998,25 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 }
 
 /* return 1 if should continue */
+/*
+ * called by:
+ *   - arch/x86/domain.c|128| <<idle_loop>> if ( !scrub_free_pages() )
+ */
 int scrub_free_pages(void)
 {
     int order;
     struct page_info *pg, *tmp;
     unsigned int i, zone, nr_delisted = 0;
+    /*
+     * 每一个cpu都有一个idle_loop()
+     */
     unsigned int cpu = smp_processor_id();
     unsigned int node = cpu_to_node(cpu);
+    /*
+     * 在以下使用scrub_list_cpu:
+     *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     */
     struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
     bool_t was_scrubbing;
     static unsigned node_is_scrubbed = 0;
@@ -1874,6 +2236,13 @@ void init_domheap_pages(paddr_t ps, paddr_t pe)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/domain_build.c|674| <<construct_dom0>> if ( assign_pages(d, mfn_to_page(mfn++), 0, 0) )
+ *   - common/memory.c|617| <<memory_exchaeng>> if ( assign_pages(d, page, exch.out.extent_order,
+ *   - common/memory.c|682| <<memory_exchange>> if ( assign_pages(d, page, 0, MEMF_no_refcount) )
+ *   - common/page_alloc.c|2169| <<alloc_domheap_pages>> assign_pages(d, pg, order, memflags) )
+ */
 int assign_pages(
     struct domain *d,
     struct page_info *pg,
@@ -1963,6 +2332,16 @@ struct page_info *alloc_domheap_pages(
     return pg;
 }
 
+/*
+ * x86在以下使用:
+ *   - arch/x86/domain_build.c|288| <<alloc_chunk>> free_domheap_pages(page, free_order);
+ *   - arch/x86/domain_build.c|293| <<alloc_chunk>> free_domheap_pages(pg2, order);
+ *   - arch/x86/domain_build.c|661| <<construct_dom0>> free_domheap_pages(page, order);
+ *   - common/memory.c|642| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - common/memory.c|693| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - include/xen/mm.h|107| <<free_domheap_page>> #define free_domheap_page(p) (free_domheap_pages(p,0))
+ *   - include/xen/tmem_xen.h|140| <<__tmem_free_page_thispool>> free_domheap_pages(pi,0);
+ */
 void free_domheap_pages(struct page_info *pg, unsigned int order)
 {
     struct domain *d = page_get_owner(pg);
@@ -1971,6 +2350,9 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
 
     ASSERT(!in_irq());
 
+    /*
+     * 判断((page)->count_info & PGC_xen_heap)
+     */
     if ( unlikely(is_xen_heap_page(pg)) )
     {
         /* NB. May recursively lock from relinquish_memory(). */
@@ -1995,6 +2377,18 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
             page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
         }
 
+	/*
+	 * called by:
+	 *   - arch/x86/mm.c|4419| <<donate_page>> domain_adjust_tot_pages(d, 1);
+	 *   - arch/x86/mm.c|4483| <<steal_page>> if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )
+	 *   - arch/x86/mm/mem_sharing.c|659| <<page_make_sharable>> drop_dom_ref = !domain_adjust_tot_pages(d, -1);
+	 *   - arch/x86/mm/mem_sharing.c|706| <<page_make_private>> if ( domain_adjust_tot_pages(d, 1) == 1 )
+	 *   - common/grant_table.c|1995| <<gnttab_transfer>> if ( unlikely(domain_adjust_tot_pages(e, 1) == 1) )
+	 *   - common/grant_table.c|2010| <<gnttab_transfer>> bool_t drop_dom_ref = !domain_adjust_tot_pages(e, -1);
+	 *   - common/memory.c|636| <<memory_exchange>> !domain_adjust_tot_pages(d, -dec_count));
+	 *   - common/page_alloc.c|1967| <<assign_pages>> domain_adjust_tot_pages(d, 1 << order);
+	 *   - common/page_alloc.c|2067| <<free_domheap_pages>> drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
+	 */
         drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
 
         spin_unlock_recursive(&d->page_alloc_lock);
@@ -2026,6 +2420,10 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
         put_domain(d);
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|169| <<do_sysctl(XEN_SYSCTL_availheap)>> op->u.availheap.avail_bytes = avail_domheap_pages_region(
+ */
 unsigned long avail_domheap_pages_region(
     unsigned int node, unsigned int min_width, unsigned int max_width)
 {
@@ -2037,18 +2435,42 @@ unsigned long avail_domheap_pages_region(
     zone_hi = max_width ? bits_to_zone(max_width) : (NR_ZONES - 1);
     zone_hi = max_t(int, MEMZONE_XEN + 1, min_t(int, NR_ZONES - 1, zone_hi));
 
+    /*
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     */
     return avail_heap_pages(zone_lo, zone_hi, node);
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain_build.c|301| <<compute_dom0_nr_pages>> unsigned long avail = avail_domheap_pages() + initial_images_nrpages();
+ *   - common/page_alloc.c|586| <<get_outstanding_claims>> *free_pages = avail_domheap_pages();
+ *
+ * 把从MEMZONE_XEN + 1到NR_ZONES - 1的每个node的page全加起来
+ */
 unsigned long avail_domheap_pages(void)
 {
+    /*
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     */
     return avail_heap_pages(MEMZONE_XEN + 1,
                             NR_ZONES - 1,
                             -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/numa.c|369| <<dump_numa>> avail_node_heap_pages(i));
+ *   - common/sysctl.c|299| <<do_sysctl(XEN_SYSCTL_numainfo)>> avail_node_heap_pages(i) << PAGE_SHIFT : 0ul;
+ */
 unsigned long avail_node_heap_pages(unsigned int nodeid)
 {
+    /*
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     */
     return avail_heap_pages(MEMZONE_XEN, NR_ZONES -1, nodeid);
 }
 
@@ -2058,6 +2480,13 @@ static void pagealloc_info(unsigned char key)
     unsigned int zone = MEMZONE_XEN;
     unsigned long n, total = 0;
 
+    /*
+     * avail_heap_pages():
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     *
+     * 开始zone是MEMZONE_XEN=0
+     */
     printk("Physical memory information:\n");
     printk("    Xen heap: %lukB free\n",
            avail_heap_pages(zone, zone, -1) << (PAGE_SHIFT-10));
@@ -2070,6 +2499,10 @@ static void pagealloc_info(unsigned char key)
             total = 0;
         }
 
+        /*
+	 * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+	 * 如果node是-1就把所有node的加起来
+	 */
         if ( (n = avail_heap_pages(zone, zone, -1)) != 0 )
         {
             total += n;
@@ -2094,6 +2527,24 @@ static __init int pagealloc_keyhandler_init(void)
 __initcall(pagealloc_keyhandler_init);
 
 
+/*
+ * called by:
+ *   - arch/x86/mm/p2m.c|1005| <<p2m_mem_paging_evict>> scrub_one_page(page);
+ *   - common/page_alloc.c|797| <<alloc_heap_pages>> scrub_one_page(&pg[i]);
+ *   - common/page_alloc.c|1442| <<smp_scrub_heap_pages>> scrub_one_page(pg);
+ *   - common/page_alloc.c|1622| <<__scrub_free_pages>> scrub_one_page(&pg[i]);
+ *   - common/tmem.c|1401| <<tmem_flush_npages>> scrub_one_page(pg);
+ *   - common/tmem.c|2795| <<tmem_relinquish_pages>> scrub_one_page(pfp);
+ *   - include/xen/tmem_xen.h|138| <<__tmem_free_page_thispool>> scrub_one_page(pi);
+ *
+ * 从smp_scrub_heap_pages()进来的话只能是初始化的时候:
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ *          -> scrub_one_page()
+ *
+ * 把page给scrub了, 不改变任何flag
+ */
 void scrub_one_page(struct page_info *pg)
 {
     void *p;
@@ -2140,6 +2591,18 @@ static void dump_heap(unsigned char key)
     }
     for ( i = 0; i < MAX_NUMNODES; i++ )
     {
+        /*
+	 * 修改node_need_scrub[]的地方:
+         *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+         *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+         *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+         * 在以下使用node_need_scrub:
+         *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+         *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+         *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+         *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+         *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+	 */
         if ( !node_need_scrub[i] )
             continue;
         printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index 9e94eabfb7..c7c81f487a 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -322,6 +322,21 @@ struct arch_domain
         RELMEM_l2,
         RELMEM_done,
     } relmem;
+    /*
+     * 在以下修改relmem_list:
+     *   - arch/x86/domain.c|536| <<arch_domain_create>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/domain.c|2009| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2087| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2098| <<relinquish_memory>> page_list_move(list, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2173| <<domain_relinquish_resources>> page_list_splice(&d->arch.relmem_list, &d->page_list);
+     *   - arch/x86/domain.c|2174| <<domain_relinquish_resources>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/mm/p2m-pod.c|471| <<p2m_pod_offline_or_broken_hit>> page_list_add(p, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1980| <<free_domheap_pages>> page_list_del2(&pg[i], &d->xenpage_list, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1995| <<free_domheap_pages>> page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|404| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|419| <<iommu_populate_page_table>> page_list_move(&d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|424| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     */
     struct page_list_head relmem_list;
 
     cpuid_input_t *cpuids;
diff --git a/xen/include/asm-x86/guest_pt.h b/xen/include/asm-x86/guest_pt.h
index 6ce65892f8..d940924b83 100644
--- a/xen/include/asm-x86/guest_pt.h
+++ b/xen/include/asm-x86/guest_pt.h
@@ -181,6 +181,15 @@ static inline guest_l4e_t guest_l4e_from_gfn(gfn_t gfn, u32 flags)
 
 /* Which pagetable features are supported on this vcpu? */
 
+/*
+ * called by:
+ *   - arch/x86/mm/guest_walk.c|290| <<guest_walk_tables>> pse2M = (gflags & _PAGE_PSE) && guest_supports_superpages(v);
+ *   - arch/x86/mm/shadow/multi.c|243| <<shadow_check_gwalk>> if ( !(guest_supports_superpages(v) &&
+ *   - arch/x86/mm/shadow/multi.c|314| <<gw_remove_write_accesses>> if ( !(guest_supports_superpages(v) &&
+ *   - arch/x86/mm/shadow/multi.c|653| <<_sh_propagate>> guest_supports_superpages(v)))
+ *   - arch/x86/mm/shadow/multi.c|1860| <<shadow_get_and_create_l1e>> if ( guest_supports_superpages(v) && (flags & _PAGE_PSE) )
+ *   - arch/x86/mm/shadow/multi.c|2268| <<validate_gl2e>> if ( guest_supports_superpages(v) &&
+ */
 static inline int
 guest_supports_superpages(struct vcpu *v)
 {
diff --git a/xen/include/asm-x86/mm.h b/xen/include/asm-x86/mm.h
index 56f2968d39..d37f14bd1f 100644
--- a/xen/include/asm-x86/mm.h
+++ b/xen/include/asm-x86/mm.h
@@ -184,13 +184,24 @@ struct page_info
 #define PGT_l1_page_table PG_mask(1, 4)  /* using as an L1 page table?     */
 #define PGT_l2_page_table PG_mask(2, 4)  /* using as an L2 page table?     */
 #define PGT_l3_page_table PG_mask(3, 4)  /* using as an L3 page table?     */
+/* 0x4000000000000000 */
 #define PGT_l4_page_table PG_mask(4, 4)  /* using as an L4 page table?     */
 #define PGT_seg_desc_page PG_mask(5, 4)  /* using this page in a GDT/LDT?  */
+/* 0x7000000000000000 */
 #define PGT_writable_page PG_mask(7, 4)  /* has writable mappings?         */
 #define PGT_shared_page   PG_mask(8, 4)  /* CoW sharable page              */
 #define PGT_type_mask     PG_mask(15, 4) /* Bits 28-31 or 60-63.           */
 
  /* Owning guest has pinned this page to its current type? */
+/*
+ * 使用_PGT_pinned的地方:
+ *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+ *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+ */
 #define _PGT_pinned       PG_shift(5)
 #define PGT_pinned        PG_mask(1, 5)
  /* Has this page been validated for use as its current type? */
@@ -233,10 +244,33 @@ struct page_info
 #define PGC_state_inuse   PG_mask(0, 9)
 #define PGC_state_offlining PG_mask(1, 9)
 #define PGC_state_offlined PG_mask(2, 9)
+/*
+ * 在以下使用PGC_state_free:
+ *   - common/page_alloc.c|962| <<alloc_heap_pages>> BUG_ON(pg[i].count_info != PGC_state_free);
+ *   - common/page_alloc.c|1202| <<free_heap_pages>> ? PGC_state_offlined : PGC_state_free));
+ *   - common/page_alloc.c|1262| <<mark_page_offline>> nx |= (((x & PGC_state) == PGC_state_free)
+ *   - common/page_alloc.c|1892| <<__scrub_free_pages>> pg[i].count_info |= PGC_state_free;
+ */
 #define PGC_state_free    PG_mask(3, 9)
 #define page_state_is(pg, st) (((pg)->count_info&PGC_state) == PGC_state_##st)
 /* Page need to be scrubbed */
+/*
+ * 在以下使用_PGC_need_scrub:
+ *   - common/page_alloc.c|947| <<alloc_heap_pages>> if ( test_and_clear_bit(_PGC_need_scrub, &pg[i].count_info) )
+ *   - common/page_alloc.c|1093| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+ *   - common/page_alloc.c|1098| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+ *   - common/page_alloc.c|1114| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+ *   - common/page_alloc.c|1119| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+ *   - common/page_alloc.c|1829| <<__scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+ *   - common/page_alloc.c|1900| <<scrub_free_pages>> if ( !test_bit(_PGC_need_scrub, &(pg->count_info)) )
+ *   - common/page_alloc.c|1919| <<scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+ */
 #define _PGC_need_scrub   PG_shift(10)
+/*
+ * 在以下使用PGC_need_scrub:
+ *   - common/page_alloc.c|1201| <<free_heap_pages>> pg[i].count_info |= PGC_need_scrub;
+ *   - common/page_alloc.c|1848| <<__scrub_free_pages>> pg[i].count_info &= ~PGC_need_scrub;
+ */
 #define PGC_need_scrub    PG_mask(1, 10)
 
  /* Count of references to this frame. */
diff --git a/xen/include/xen/mm.h b/xen/include/xen/mm.h
index bad1ef3edc..c0dd7d97e0 100644
--- a/xen/include/xen/mm.h
+++ b/xen/include/xen/mm.h
@@ -410,6 +410,11 @@ int page_is_ram_type(unsigned long mfn, unsigned long mem_type);
 
 #include <asm/flushtlb.h>
 
+/*
+ * called by:
+ *   - common/memory.c|230| <<populate_physmap>> accumulate_tlbflush(&need_tlbflush, &page[j],
+ *   - common/page_alloc.c|966| <<alloc_heap_pages>> accumulate_tlbflush(&need_tlbflush, &pg[i],
+ */
 static inline void accumulate_tlbflush(bool_t *need_tlbflush,
                                        const struct page_info *page,
                                        uint32_t *tlbflush_timestamp)
@@ -424,6 +429,11 @@ static inline void accumulate_tlbflush(bool_t *need_tlbflush,
     }
 }
 
+/*
+ * called by:
+ *   - common/memory.c|251| <<populate_physmap>> filtered_flush_tlb_mask(tlbflush_timestamp);
+ *   - common/page_alloc.c|1003| <<alloc_heap_pages>> filtered_flush_tlb_mask(tlbflush_timestamp);
+ */
 static inline void filtered_flush_tlb_mask(uint32_t tlbflush_timestamp)
 {
     cpumask_t mask = cpu_online_map;
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index d8006bb0b7..69e89b2f85 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -292,6 +292,21 @@ struct domain
     struct page_list_head page_list;  /* linked list */
     struct page_list_head xenpage_list; /* linked list (size xenheap_pages) */
     unsigned int     tot_pages;       /* number of pages currently possesed */
+    /*
+     * 在以下修改d->outstanding_pages:
+     *   - common/page_alloc.c|495| <<domain_adjust_tot_pages>> d->outstanding_pages = dom_claimed;
+     *   - common/page_alloc.c|529| <<domain_set_outstanding_pages>> d->outstanding_pages = 0;
+     *   - common/page_alloc.c|572| <<domain_set_outstanding_pages>> d->outstanding_pages = claim;
+     *
+     * 在以下使用d->outstanding_pages:
+     *   - common/domctl.c|209| <<getdomaininfo>> info->outstanding_pages = d->outstanding_pages;
+     *   - common/page_alloc.c|486| <<domain_adjust_tot_pages>> if ( !d->outstanding_pages )
+     *   - common/page_alloc.c|491| <<domain_adjust_tot_pages>> dom_before = d->outstanding_pages;
+     *   - common/page_alloc.c|528| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+     *   - common/page_alloc.c|535| <<domain_set_outstanding_pages>> if ( d->outstanding_pages )
+     *   - common/page_alloc.c|573| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+     *   - common/page_alloc.c|868| <<alloc_heap_pages>> !d || d->outstanding_pages < request) )
+     */
     unsigned int     outstanding_pages; /* pages claimed but not possessed  */
     unsigned int     max_pages;       /* maximum value for tot_pages        */
     atomic_t         shr_pages;       /* number of shared pages             */
-- 
2.17.1

