From 4d485233ae0e1139ebe7343f1d8b56fb6c9acd73 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 4 May 2020 15:59:53 -0700
Subject: [PATCH 1/1] xen-4.4.4-222-comment

xen-4.4.4-222

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 tools/libxc/xc_dom_x86.c                |   4 +
 tools/libxc/xc_domain.c                 |  20 +
 tools/libxc/xc_domain_save.c            |  27 +
 tools/libxc/xc_hvm_build_x86.c          |  20 +
 tools/libxc/xc_misc.c                   |  14 +
 tools/python/xen/xend/XendDomainInfo.py |   2 +
 tools/xenstore/talloc.c                 |  13 +
 tools/xenstore/xenstored_core.c         | 367 +++++++++++
 tools/xenstore/xenstored_core.h         |  43 ++
 tools/xenstore/xenstored_domain.c       |  87 +++
 tools/xenstore/xenstored_transaction.c  | 232 +++++++
 tools/xenstore/xenstored_watch.c        |  11 +
 tools/xenstore/xs.c                     | 102 ++++
 xen/arch/x86/domain.c                   |  66 ++
 xen/arch/x86/domain_page.c              |  17 +
 xen/arch/x86/hvm/hvm.c                  |  11 +
 xen/arch/x86/hvm/vmx/vmcs.c             |  36 ++
 xen/arch/x86/mm.c                       |  16 +
 xen/common/domain.c                     |   4 +
 xen/common/memory.c                     |  40 ++
 xen/common/page_alloc.c                 | 768 ++++++++++++++++++++++++
 xen/common/softirq.c                    |  19 +
 xen/include/asm-x86/domain.h            |  15 +
 xen/include/asm-x86/event.h             |  13 +
 xen/include/asm-x86/guest_pt.h          |   9 +
 xen/include/asm-x86/mm.h                |  34 ++
 xen/include/xen/mm.h                    |  77 +++
 xen/include/xen/sched.h                 |  42 ++
 28 files changed, 2109 insertions(+)

diff --git a/tools/libxc/xc_dom_x86.c b/tools/libxc/xc_dom_x86.c
index b2256f2c5e..6ea4408480 100644
--- a/tools/libxc/xc_dom_x86.c
+++ b/tools/libxc/xc_dom_x86.c
@@ -757,6 +757,10 @@ static int x86_shadow(xc_interface *xch, domid_t domid)
     return rc;
 }
 
+/*
+ * x86下调用的例子:
+ *   - libxc/xc_dom_boot.c|149| <<xc_dom_boot_mem_init>> rc = arch_setup_meminit(dom);
+ */
 int arch_setup_meminit(struct xc_dom_image *dom)
 {
     int rc;
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 251b49e728..d26b02b3f1 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -902,6 +902,12 @@ int xc_domain_claim_pages(xc_interface *xch,
     return err;
 }
 
+/*
+ * called by:
+ *   - libxc/xc_domain.c|943| <<xc_domain_populate_physmap_exact>> err = xc_domain_populate_physmap(xch, domid, nr_extents,
+ *   - libxc/xc_hvm_build_x86.c|484| <<setup_guest>> done = xc_domain_populate_physmap(xch, dom, nr_extents,
+ *   - libxc/xc_hvm_build_x86.c|525| <<setup_guest>> done = xc_domain_populate_physmap(xch, dom, nr_extents,
+ */
 int xc_domain_populate_physmap(xc_interface *xch,
                                uint32_t domid,
                                unsigned long nr_extents,
@@ -931,6 +937,20 @@ int xc_domain_populate_physmap(xc_interface *xch,
     return err;
 }
 
+/*
+ * x86下调用的例子:
+ *   - libxc/xc_dom_x86.c|796| <<arch_setup_meminit>> rc = xc_domain_populate_physmap_exact(dom->xch, dom->guest_domid,
+ *   - libxc/xc_dom_x86.c|901| <<arch_setup_meminit>> rc = xc_domain_populate_physmap_exact(dom->xch,
+ *   - libxc/xc_domain_restore.c|147| <<alloc_superpage_mfns>> if (xc_domain_populate_physmap_exact(xch, dom, max, SUPERPAGE_PFN_SHIFT,
+ *   - libxc/xc_domain_restore.c|211| <<uncanonicalize_pagetable>> rc = xc_domain_populate_physmap_exact(xch, dom, nr_mfns, 0, 0,
+ *   - libxc/xc_domain_restore.c|1149| <<apply_batch>> if ( xc_domain_populate_physmap_exact(xch, dom, 1,
+ *   - libxc/xc_domain_restore.c|1221| <<apply_batch>> rc = xc_domain_populate_physmap_exact(xch, dom, nr_mfns, 0, 0,
+ *   - libxc/xc_hvm_build_x86.c|418| <<setup_guest>> rc = xc_domain_populate_physmap_exact(
+ *   - libxc/xc_hvm_build_x86.c|543| <<setup_guest>> rc = xc_domain_populate_physmap_exact(
+ *   - libxc/xc_hvm_build_x86.c|583| <<setup_guest>> rc = xc_domain_populate_physmap_exact(xch, dom, 1, 0, 0, &pfn);
+ *   - tests/xen-access/xen-access.c|266| <<xenaccess_init>> rc = xc_domain_populate_physmap_exact(xenaccess->xc_handle,
+ *   - xenpaging/xenpaging.c|351| <<xenpaging_init>> rc = xc_domain_populate_physmap_exact(paging->xc_handle,
+ */
 int xc_domain_populate_physmap_exact(xc_interface *xch,
                                      uint32_t domid,
                                      unsigned long nr_extents,
diff --git a/tools/libxc/xc_domain_save.c b/tools/libxc/xc_domain_save.c
index 549bebf51c..6c0cb96fd1 100644
--- a/tools/libxc/xc_domain_save.c
+++ b/tools/libxc/xc_domain_save.c
@@ -280,6 +280,14 @@ struct time_stats {
     long long d0_cpu, d1_cpu;
 };
 
+/*
+ * called by:
+ *   - libxc/xc_domain_save.c|1101| <<xc_domain_save>> print_stats(xch, dom, 0, &time_stats, &shadow_stats, 0);
+ *   - libxc/xc_domain_save.c|1512| <<xc_domain_save>> print_stats( xch, dom, sent_this_iter, &time_stats, &shadow_stats, 1);
+ *   - libxc/xc_domain_save.c|1622| <<xc_domain_save>> print_stats(xch, dom, sent_this_iter, &time_stats, &shadow_stats, print);
+ *   - libxc/xc_domain_save.c|2121| <<xc_domain_save>> print_stats(xch, dom, 0, &time_stats, &shadow_stats, 0);
+ *   - libxc/xc_domain_save.c|2132| <<xc_domain_save>> print_stats(xch, dom, 0, &time_stats, &shadow_stats, 1);
+ */
 static int print_stats(xc_interface *xch, uint32_t domid, int pages_sent,
                        struct time_stats *last,
                        xc_shadow_op_stats_t *stats, int print)
@@ -307,6 +315,19 @@ static int print_stats(xc_interface *xch, uint32_t domid, int pages_sent,
         d0_cpu_delta = (now.d0_cpu - last->d0_cpu)/1000;
         d1_cpu_delta = (now.d1_cpu - last->d1_cpu)/1000;
 
+        /*
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: delta 71944ms, dom0 0%, target 0%, sent 934Mb/s, dirtied 1Mb/s 2406 pages --> 0x1f4a59 = 2050649.9
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: delta 65ms, dom0 0%, target 0%, sent 1183Mb/s, dirtied 57Mb/s 114 pages --> 0x92A = 2346.6
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: delta 7ms, dom0 0%, target 0%, sent 533Mb/s, dirtied 0Mb/s 0 pages --> 0x71 = 113.8
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: Start last iteration
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: delta 185ms, dom0 11%, target 1%, sent 0Mb/s, dirtied 181Mb/s 1022 pages
+	 * DEBUG (XendCheckpoint:158) Written done
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: delta 13ms, dom0 7%, target 0%, sent 2576Mb/s, dirtied 2576Mb/s 1022 pages --> 0x3fd = 1021.9
+	 * INFO (XendCheckpoint:430) [24087] xc: detail: Total pages sent= 2054555 (0.97x) --> 2054555 / 2113537 = 97.2%
+	 *
+	 * INFO (XendCheckpoint:430) [5560] xc: detail: xc_domain_restore: p2m_size = 204001 (16进制)
+	 */
+
         DPRINTF("delta %lldms, dom0 %d%%, target %d%%, sent %dMb/s, "
                 "dirtied %dMb/s %" PRId32 " pages\n",
                 wall_delta,
@@ -1119,6 +1140,9 @@ int xc_domain_save(xc_interface *xch, int io_fd, uint32_t dom, uint32_t max_iter
 #define wrcompressed(fd) write_compressed(xch, compress_ctx, last_iter, ob, (fd))
 
     ob = &ob_pagebuf; /* Holds pfn_types, pages/compressed pages */
+    /*
+     * 下面的for循环很大, 一直到DPRINTF("All memory is saved\n");
+     */
     /* Now write out each data page, canonicalising page tables as we go... */
     for ( ; ; )
     {
@@ -1575,6 +1599,9 @@ int xc_domain_save(xc_interface *xch, int io_fd, uint32_t dom, uint32_t max_iter
                 else  //We can stop migrating as long as we have converged
                 {
                     DPRINTF("Continue migration NO\n");
+                    /*
+		     * 除了一开始初始化, 这里是唯一设置成0的地方
+		     */
                     continue_migration = 0;
                     print = 1;
                 }
diff --git a/tools/libxc/xc_hvm_build_x86.c b/tools/libxc/xc_hvm_build_x86.c
index 245f0d88fd..9426f22cce 100644
--- a/tools/libxc/xc_hvm_build_x86.c
+++ b/tools/libxc/xc_hvm_build_x86.c
@@ -33,8 +33,24 @@
 
 #include <xen/libelf/libelf.h>
 
+/*
+ * 在以下使用SUPERPAGE_2MB_SHIFT:
+ *   - libxc/xc_hvm_build_x86.c|37| <<SUPERPAGE_2MB_NR_PFNS>> #define SUPERPAGE_2MB_NR_PFNS (1UL << SUPERPAGE_2MB_SHIFT)
+ *   - libxc/xc_hvm_build_x86.c|522| <<setup_guest>> unsigned long nr_extents = count >> SUPERPAGE_2MB_SHIFT;
+ *   - libxc/xc_hvm_build_x86.c|527| <<setup_guest>> page_array[cur_pages+(i<<SUPERPAGE_2MB_SHIFT)];
+ *   - libxc/xc_hvm_build_x86.c|530| <<setup_guest>> SUPERPAGE_2MB_SHIFT,
+ *   - libxc/xc_hvm_build_x86.c|537| <<setup_guest>> done <<= SUPERPAGE_2MB_SHIFT;
+ */
 #define SUPERPAGE_2MB_SHIFT   9
 #define SUPERPAGE_2MB_NR_PFNS (1UL << SUPERPAGE_2MB_SHIFT)
+/*
+ * 在以下使用SUPERPAGE_1GB_SHIFT:
+ *   - libxc/xc_hvm_build_x86.c|39| <<SUPERPAGE_1GB_NR_PFNS>> #define SUPERPAGE_1GB_NR_PFNS (1UL << SUPERPAGE_1GB_SHIFT)
+ *   - libxc/xc_hvm_build_x86.c|481| <<setup_guest>> unsigned long nr_extents = count >> SUPERPAGE_1GB_SHIFT;
+ *   - libxc/xc_hvm_build_x86.c|486| <<setup_guest>> page_array[cur_pages+(i<<SUPERPAGE_1GB_SHIFT)];
+ *   - libxc/xc_hvm_build_x86.c|489| <<setup_guest>> SUPERPAGE_1GB_SHIFT,
+ *   - libxc/xc_hvm_build_x86.c|496| <<setup_guest>> done <<= SUPERPAGE_1GB_SHIFT;
+ */
 #define SUPERPAGE_1GB_SHIFT   18
 #define SUPERPAGE_1GB_NR_PFNS (1UL << SUPERPAGE_1GB_SHIFT)
 
@@ -229,6 +245,10 @@ static int check_mmio_hole(uint64_t start, uint64_t memsize,
         return 1;
 }
 
+/*
+ * called by:
+ *   - libxc/xc_hvm_build_x86.c|675| <<xc_hvm_build>> sts = setup_guest(xch, domid, &args, image, image_size);
+ */
 static int setup_guest(xc_interface *xch,
                        uint32_t dom, struct xc_hvm_build_args *args,
                        char *image, unsigned long image_size)
diff --git a/tools/libxc/xc_misc.c b/tools/libxc/xc_misc.c
index 8ce8fc4125..87ea843514 100644
--- a/tools/libxc/xc_misc.c
+++ b/tools/libxc/xc_misc.c
@@ -159,6 +159,20 @@ int xc_send_debug_keys(xc_interface *xch, char *keys)
     return ret;
 }
 
+/*
+ * 在以下调用:
+ *   - libxc/xc_misc.c|32| <<xc_get_max_cpus>> if ( !xc_physinfo(xch, &physinfo) )
+ *   - libxc/xc_misc.c|45| <<xc_get_online_cpus>> if ( !xc_physinfo(xch, &physinfo) )
+ *   - libxc/xc_misc.c|59| <<xc_get_max_nodes>> if ( !xc_physinfo(xch, &physinfo) )
+ *   - libxl/libxl.c|4428| <<libxl_get_physinfo>> rc = xc_physinfo(ctx->xch, &xcphysinfo);
+ *   - misc/xen-lowmemd.c|45| <<handle_low_mem>> if (xc_physinfo(xch, &info) < 0)
+ *   - misc/xenpm.c|1181| <<main>> ret = xc_physinfo(xc_handle, &physinfo);
+ *   - ocaml/libs/xc/xenctrl_stubs.c|609| <<stub_xc_physinfo>> r = xc_physinfo(_H(xch), &c_physinfo);
+ *   - python/xen/lowlevel/xc/xc.c|1349| <<pyxc_physinfo>> if ( xc_physinfo(self->xc_handle, &pinfo) != 0 )
+ *   - xenmon/xenbaked.c|442| <<get_num_cpus>> ret = xc_physinfo(xc_handle, &physinfo);
+ *   - xenstat/libxenstat/src/xenstat.c|179| <<xenstat_get_node>> if (xc_physinfo(handle->xc_handle, &physinfo) < 0) {
+ *   - xentrace/xentrace.c|557| <<get_num_cpus>> ret = xc_physinfo(xc_handle, &physinfo);
+ */
 int xc_physinfo(xc_interface *xch,
                 xc_physinfo_t *put_info)
 {
diff --git a/tools/python/xen/xend/XendDomainInfo.py b/tools/python/xen/xend/XendDomainInfo.py
index 6d52be1ffa..6452299551 100644
--- a/tools/python/xen/xend/XendDomainInfo.py
+++ b/tools/python/xen/xend/XendDomainInfo.py
@@ -2325,6 +2325,8 @@ class XendDomainInfo:
             self.destroy()
             return
 
+        # 不管是soft_reset还是reboot都会执行_restart()
+
         old_domid = self.domid
         self._writeVm(RESTART_IN_PROGRESS, 'True')
 
diff --git a/tools/xenstore/talloc.c b/tools/xenstore/talloc.c
index a3d85e34e7..22b7ef6a38 100644
--- a/tools/xenstore/talloc.c
+++ b/tools/xenstore/talloc.c
@@ -675,6 +675,19 @@ void *_talloc_realloc(const void *context, void *ptr, size_t size, const char *n
    ptr on success, or NULL if it could not be transferred.
    passing NULL as ptr will always return NULL with no side effects.
 */
+/*
+ * called by:
+ *   - xenstore/talloc.c|367| <<talloc_unlink>> talloc_steal(new_parent, ptr);
+ *   - xenstore/talloc.c|530| <<talloc_free_children>> talloc_steal(new_parent, child);
+ *   - xenstore/talloc.h|117| <<talloc_init>> void *talloc_steal(const void *new_ctx, const void *ptr);
+ *   - xenstore/xenstored_core.c|166| <<replace_tdb>> tdb_ctx = talloc_steal(talloc_autofree_context(), newtdb);
+ *   - xenstore/xenstored_core.c|501| <<read_node>> talloc_steal(node, data.dptr);
+ *   - xenstore/xenstored_domain.c|450| <<do_introduce>> talloc_steal(domain->conn, domain);
+ *   - xenstore/xenstored_domain.c|680| <<dom0_init>> talloc_steal(dom0->conn, dom0);
+ *   - xenstore/xenstored_transaction.c|247| <<do_transaction_start>> talloc_steal(trans, trans->tdb);
+ *   - xenstore/xenstored_transaction.c|268| <<do_transaction_start>> talloc_steal(conn, trans);
+ *   - xenstore/xenstored_transaction.c|347| <<do_transaction_end>> talloc_steal(arg, trans);
+ */
 void *talloc_steal(const void *new_ctx, const void *ptr)
 {
 	struct talloc_chunk *tc, *new_tc;
diff --git a/tools/xenstore/xenstored_core.c b/tools/xenstore/xenstored_core.c
index 6a28161523..dd7ac584b2 100644
--- a/tools/xenstore/xenstored_core.c
+++ b/tools/xenstore/xenstored_core.c
@@ -54,15 +54,95 @@
 
 #include "hashtable.h"
 
+/*
+ * // We hand errors as strings, for portability.
+ * struct xsd_errors
+ * {
+ *      int errnum;
+ *      const char *errstring;
+ * };
+ * static struct xsd_errors xsd_errors[]
+ * = {
+ *      {EINVAL, "EINVAL"},
+ *      {EACCES, "EACCES"},
+ *      {EEXIST, "EEXIST"}
+ *      {EISDIR, "EISDIR"},
+ *      {ENOENT, "ENOENT"},
+ *      {ENOMEM, "ENOMEM"},
+ *      {ENOSPC, "ENOSPC"},
+ *      {EIO, "EIO"},
+ *      {ENOTEMPTY, "ENOTEMPTY"},
+ *      {ENOSYS, "ENOSYS"},
+ *      {EROFS, "EROFS"},
+ *      {EBUSY, "EBUSY"},
+ *      {EAGAIN, "EAGAIN"},
+ *      {EISCONN, "EISCONN"},
+ *      {E2BIG, "E2BIG"}
+ * };
+ */
+
 extern xc_evtchn *xce_handle; /* in xenstored_domain.c */
 static int xce_pollfd_idx = -1;
+/*
+ * 在以下使用fds:
+ *   - xenstore/xenstored_core.c|390| <<set_fd>> new_fds = realloc(fds, sizeof(struct pollfd)*newsize);
+ *   - xenstore/xenstored_core.c|393| <<set_fd>> fds = new_fds;
+ *   - xenstore/xenstored_core.c|395| <<set_fd>> memset(&fds[0] + current_array_size, 0,
+ *   - xenstore/xenstored_core.c|400| <<set_fd>> fds[nr_fds].fd = fd;
+ *   - xenstore/xenstored_core.c|401| <<set_fd>> fds[nr_fds].events = events;
+ *   - xenstore/xenstored_core.c|418| <<initialize_fds>> if (fds)
+ *   - xenstore/xenstored_core.c|419| <<initialize_fds>> memset(fds, 0, sizeof(struct pollfd) * current_array_size);
+ *   - xenstore/xenstored_core.c|2283| <<main>> if (poll(fds, nr_fds, timeout) < 0) {
+ *   - xenstore/xenstored_core.c|2290| <<main>> if (fds[reopen_log_pipe0_pollfd_idx].revents
+ *   - xenstore/xenstored_core.c|2295| <<main>> } else if (fds[reopen_log_pipe0_pollfd_idx].revents
+ *   - xenstore/xenstored_core.c|2306| <<main>> if (fds[sock_pollfd_idx].revents & ~POLLIN) {
+ *   - xenstore/xenstored_core.c|2309| <<main>> } else if (fds[sock_pollfd_idx].revents & POLLIN) {
+ *   - xenstore/xenstored_core.c|2321| <<main>> if (fds[ro_sock_pollfd_idx].revents & ~POLLIN) {
+ *   - xenstore/xenstored_core.c|2324| <<main>> } else if (fds[ro_sock_pollfd_idx].revents & POLLIN) {
+ *   - xenstore/xenstored_core.c|2336| <<main>> if (fds[xce_pollfd_idx].revents & ~POLLIN) {
+ *   - xenstore/xenstored_core.c|2339| <<main>> } else if (fds[xce_pollfd_idx].revents & POLLIN) {
+ *   - xenstore/xenstored_core.c|2385| <<main>> if (fds[conn->pollfd_idx].revents
+ *   - xenstore/xenstored_core.c|2388| <<main>> else if (fds[conn->pollfd_idx].revents
+ *   - xenstore/xenstored_core.c|2398| <<main>> if (fds[conn->pollfd_idx].revents
+ *   - xenstore/xenstored_core.c|2401| <<main>> else if (fds[conn->pollfd_idx].revents
+ */
 static struct pollfd *fds;
 static unsigned int current_array_size;
+/*
+ * 在以下使用nr_fds:
+ *   - xenstore/xenstored_core.c|381| <<set_fd>> if (current_array_size < nr_fds + 1) {
+ *   - xenstore/xenstored_core.c|388| <<set_fd>> newsize = ROUNDUP(nr_fds + 1, 8);
+ *   - xenstore/xenstored_core.c|400| <<set_fd>> fds[nr_fds].fd = fd;
+ *   - xenstore/xenstored_core.c|401| <<set_fd>> fds[nr_fds].events = events;
+ *   - xenstore/xenstored_core.c|402| <<set_fd>> ret = nr_fds;
+ *   - xenstore/xenstored_core.c|403| <<set_fd>> nr_fds++;
+ *   - xenstore/xenstored_core.c|420| <<initialize_fds>> nr_fds = 0;
+ *   - xenstore/xenstored_core.c|2283| <<main>> if (poll(fds, nr_fds, timeout) < 0) {
+ */
 static unsigned int nr_fds;
 
 #define ROUNDUP(_x, _w) (((unsigned long)(_x)+(1UL<<(_w))-1) & ~((1UL<<(_w))-1))
 
+/*
+ * 在以下使用xenstored的verbose:
+ *   - xenstore/xenstored_core.c|263| <<write_messages>> if (verbose)
+ *   - xenstore/xenstored_core.c|1415| <<consider_message>> if (verbose)
+ *   - xenstore/xenstored_core.c|2051| <<main>> verbose = true; --> 大写的V
+ *
+ * xenstored --pid-file /var/run/xenstored.pid -V -T /var/log/xen/xenstored-trace.log
+ */
 static bool verbose = false;
+/*
+ * 在以下使用connections:
+ *   - xenstore/xenstored_core.c|66| <<global>> LIST_HEAD(connections);
+ *   - xenstore/xenstored_core.c|369| <<initialize_fds>> list_for_each_entry(conn, &connections, list) {
+ *   - xenstore/xenstored_core.c|1438| <<new_connection>> list_add_tail(&new->list, &connections);
+ *   - xenstore/xenstored_core.c|2099| <<main>> next = list_entry(connections.next, typeof(*conn), list);
+ *   - xenstore/xenstored_core.c|2100| <<main>> if (&next->list != &connections)
+ *   - xenstore/xenstored_core.c|2102| <<main>> while (&next->list != &connections) {
+ *   - xenstore/xenstored_core.c|2107| <<main>> if (&next->list != &connections)
+ *   - xenstore/xenstored_watch.c|109| <<fire_watches>> list_for_each_entry(i, &connections, list) {
+ */
 LIST_HEAD(connections);
 static int tracefd = -1;
 static bool recovery = true;
@@ -71,6 +151,16 @@ static int reopen_log_pipe[2];
 static int reopen_log_pipe0_pollfd_idx = -1;
 static char *tracefile = NULL;
 static TDB_CONTEXT *tdb_ctx = NULL;
+/*
+ * 在以下使用trigger_talloc_report:
+ *   - xenstore/xenstored_core.c|74| <<global>> static bool trigger_talloc_report = false;
+ *   - xenstore/xenstored_core.c|1827| <<do_talloc_report>> trigger_talloc_report = true;
+ *   - xenstore/xenstored_core.c|2035| <<main>> if (trigger_talloc_report) {
+ *   - xenstore/xenstored_core.c|2039| <<main>> trigger_talloc_report = false;
+ *
+ * (gdb) p trigger_talloc_report
+ * $1 = false
+ */
 static bool trigger_talloc_report = false;
 
 static void corrupt(struct connection *conn, const char *fmt, ...);
@@ -88,6 +178,12 @@ static void check_store(void);
 int quota_nb_entry_per_domain = 1000;
 int quota_nb_watch_per_domain = 128;
 int quota_max_entry_size = 2048; /* 2K */
+/*
+ * 在以下使用quota_max_transaction:
+ *   - xenstore/xenstored_core.c|2150| <<main>> quota_max_transaction = strtol(optarg, NULL, 10);
+ *   - xenstore/xenstored_transaction.c|223| <<do_transaction_start>> if (conn->id && conn->transaction_started > quota_max_transaction) {
+ *   - xenstore/xenstored_core.c|144| <<global>> int quota_max_transaction = 10;
+ */
 int quota_max_transaction = 10;
 
 TDB_CONTEXT *tdb_context(struct connection *conn)
@@ -98,6 +194,23 @@ TDB_CONTEXT *tdb_context(struct connection *conn)
 	return tdb_transaction_context(conn->transaction);
 }
 
+/*
+ * (gdb) bt
+ * #0  replace_tdb (newname=0x1a63630 "/var/lib/xenstored/tdb.0x1a62600", newtdb=0x1a632d0) at xenstored_core.c:103
+ * #1  0x0000000000406f91 in do_transaction_end (conn=0x1a62520, in=0x1a61ae0) at xenstored_transaction.c:223
+ * #2  0x0000000000404961 in process_message (conn=0x1a62520) at xenstored_core.c:1294
+ * #3  consider_message (conn=0x1a62520) at xenstored_core.c:1341
+ * #4  handle_input (conn=0x1a62520) at xenstored_core.c:1387
+ * #5  0x00000000004053ba in main (argc=<value optimized out>, argv=<value optimized out>) at xenstored_core.c:2095
+ *
+ * ll -h /var/lib/xenstored/
+ * total 80K
+ * -rw-r----- 1 root root 40K May  4 14:42 tdb
+ * -rw-r----- 1 root root 40K May  4 14:42 tdb.0x1a62600
+ *
+ * called by:
+ *   - xenstore/xenstored_transaction.c|376| <<do_transaction_end>> if (!replace_tdb(trans->tdb_name, trans->tdb)) {
+ */
 bool replace_tdb(const char *newname, TDB_CONTEXT *newtdb)
 {
 	if (!(tdb_ctx->flags & TDB_INTERNAL))
@@ -165,6 +278,11 @@ void trace(const char *fmt, ...)
 	talloc_free(str);
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|400| <<write_messages>> trace_io(conn, out, 1);
+ *   - xenstore/xenstored_core.c|1637| <<handle_input>> trace_io(conn, in, 0);
+ */
 static void trace_io(const struct connection *conn,
 		     const struct buffered_data *data,
 		     int out)
@@ -237,6 +355,17 @@ static bool write_messages(struct connection *conn)
 	int ret;
 	struct buffered_data *out;
 
+	/*
+	 * 在以下使用connection->out_list:
+	 *   - xenstore/xenstored_core.c|293| <<write_messages>> out = list_top(&conn->out_list, struct buffered_data, list);
+	 *   - xenstore/xenstored_core.c|347| <<destroy_conn>> while (!list_empty(&conn->out_list)
+	 *   - xenstore/xenstored_core.c|427| <<initialize_fds>> !list_empty(&conn->out_list)))
+	 *   - xenstore/xenstored_core.c|431| <<initialize_fds>> if (!list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_core.c|746| <<send_reply>> list_add_tail(&bdata->list, &conn->out_list);
+	 *   - xenstore/xenstored_core.c|1588| <<new_connection>> INIT_LIST_HEAD(&new->out_list);
+	 *   - xenstore/xenstored_core.c|2306| <<main>> !list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_domain.c|383| <<domain_conn_reset>> while ((out = list_top(&conn->out_list, struct buffered_data, list))) {
+	 */
 	out = list_top(&conn->out_list, struct buffered_data, list);
 	if (out == NULL)
 		return true;
@@ -650,6 +779,18 @@ unsigned int get_strings(struct buffered_data *data,
 	return i;
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|722| <<send_ack>> send_reply(conn, type, "OK", sizeof("OK"));
+ *   - xenstore/xenstored_core.c|736| <<send_error>> send_reply(conn, XS_ERROR, xsd_errors[i].errstring,
+ *   - xenstore/xenstored_core.c|832| <<send_directory>> send_reply(conn, XS_DIRECTORY, node->children, node->childlen);
+ *   - xenstore/xenstored_core.c|847| <<do_read>> send_reply(conn, XS_READ, node->data, node->datalen);
+ *   - xenstore/xenstored_core.c|1185| <<do_get_perms>> send_reply(conn, XS_GET_PERMS, strings, len);
+ *   - xenstore/xenstored_domain.c|604| <<do_get_domain_path>> send_reply(conn, XS_GET_DOMAIN_PATH, path, strlen(path) + 1);
+ *   - xenstore/xenstored_domain.c|626| <<do_is_domain_introduced>> send_reply(conn, XS_IS_DOMAIN_INTRODUCED, result ? "T" : "F", 2);
+ *   - xenstore/xenstored_transaction.c|238| <<do_transaction_start>> send_reply(conn, XS_TRANSACTION_START, id_str, strlen(id_str)+1);
+ *   - xenstore/xenstored_watch.c|90| <<add_event>> send_reply(conn, XS_WATCH_EVENT, data, len);
+ */
 void send_reply(struct connection *conn, enum xsd_sockmsg_type type,
 		const void *data, unsigned int len)
 {
@@ -677,20 +818,91 @@ void send_reply(struct connection *conn, enum xsd_sockmsg_type type,
 	bdata->hdr.msg.len = len;
 	memcpy(bdata->buffer, data, len);
 
+	/*
+	 * 在以下使用connection->out_list:
+	 *   - xenstore/xenstored_core.c|293| <<write_messages>> out = list_top(&conn->out_list, struct buffered_data, list);
+	 *   - xenstore/xenstored_core.c|347| <<destroy_conn>> while (!list_empty(&conn->out_list)
+	 *   - xenstore/xenstored_core.c|427| <<initialize_fds>> !list_empty(&conn->out_list)))
+	 *   - xenstore/xenstored_core.c|431| <<initialize_fds>> if (!list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_core.c|746| <<send_reply>> list_add_tail(&bdata->list, &conn->out_list);
+	 *   - xenstore/xenstored_core.c|1588| <<new_connection>> INIT_LIST_HEAD(&new->out_list);
+	 *   - xenstore/xenstored_core.c|2306| <<main>> !list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_domain.c|383| <<domain_conn_reset>> while ((out = list_top(&conn->out_list, struct buffered_data, list))) {
+	 */
 	/* Queue for later transmission. */
 	list_add_tail(&bdata->list, &conn->out_list);
 }
 
 /* Some routines (write, mkdir, etc) just need a non-error return */
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|986| <<do_write>> send_ack(conn, XS_WRITE);
+ *   - xenstore/xenstored_core.c|1019| <<do_mkdir>> send_ack(conn, XS_MKDIR);
+ *   - xenstore/xenstored_core.c|1126| <<do_rm>> send_ack(conn, XS_RM);
+ *   - xenstore/xenstored_core.c|1145| <<do_rm>> send_ack(conn, XS_RM);
+ *   - xenstore/xenstored_core.c|1222| <<do_set_perms>> send_ack(conn, XS_SET_PERMS);
+ *   - xenstore/xenstored_core.c|1247| <<do_debug>> send_ack(conn, XS_DEBUG);
+ *   - xenstore/xenstored_domain.c|467| <<do_introduce>> send_ack(conn, XS_INTRODUCE);
+ *   - xenstore/xenstored_domain.c|512| <<do_set_target>> send_ack(conn, XS_SET_TARGET);
+ *   - xenstore/xenstored_domain.c|551| <<do_release>> send_ack(conn, XS_RELEASE);
+ *   - xenstore/xenstored_domain.c|589| <<do_resume>> send_ack(conn, XS_RESUME);
+ *   - xenstore/xenstored_domain.c|635| <<do_reset_watches>> send_ack(conn, XS_RESET_WATCHES);
+ *   - xenstore/xenstored_transaction.c|276| <<do_transaction_end>> send_ack(conn, XS_TRANSACTION_END);
+ *   - xenstore/xenstored_watch.c|180| <<do_watch>> send_ack(conn, XS_WATCH);
+ *   - xenstore/xenstored_watch.c|202| <<do_unwatch>> send_ack(conn, XS_UNWATCH);
+ */
 void send_ack(struct connection *conn, enum xsd_sockmsg_type type)
 {
 	send_reply(conn, type, "OK", sizeof("OK"));
 }
 
+/*
+ * // We hand errors as strings, for portability.
+ * struct xsd_errors
+ * {
+ *      int errnum;
+ *      const char *errstring;
+ * };
+ * static struct xsd_errors xsd_errors[]
+ * = {
+ *      {EINVAL, "EINVAL"},
+ *      {EACCES, "EACCES"},
+ *      {EEXIST, "EEXIST"}
+ *      {EISDIR, "EISDIR"},
+ *      {ENOENT, "ENOENT"},
+ *      {ENOMEM, "ENOMEM"},
+ *      {ENOSPC, "ENOSPC"},
+ *      {EIO, "EIO"},
+ *      {ENOTEMPTY, "ENOTEMPTY"},
+ *      {ENOSYS, "ENOSYS"},
+ *      {EROFS, "EROFS"},
+ *      {EBUSY, "EBUSY"},
+ *      {EAGAIN, "EAGAIN"},
+ *      {EISCONN, "EISCONN"},
+ *      {E2BIG, "E2BIG"}
+ * };
+ */
 void send_error(struct connection *conn, int error)
 {
 	unsigned int i;
 
+	/*
+	 * XSD_ERROR(EINVAL),
+	 * XSD_ERROR(EACCES),
+	 * XSD_ERROR(EEXIST),
+	 * XSD_ERROR(EISDIR),
+	 * XSD_ERROR(ENOENT),
+	 * XSD_ERROR(ENOMEM),
+	 * XSD_ERROR(ENOSPC),
+	 * XSD_ERROR(EIO),
+	 * XSD_ERROR(ENOTEMPTY),
+	 * XSD_ERROR(ENOSYS),
+	 * XSD_ERROR(EROFS),
+	 * XSD_ERROR(EBUSY),
+	 * XSD_ERROR(EAGAIN),
+	 * XSD_ERROR(EISCONN),
+	 * XSD_ERROR(E2BIG)
+	 */
 	for (i = 0; error != xsd_errors[i].errnum; i++) {
 		if (i == ARRAY_SIZE(xsd_errors) - 1) {
 			eprintf("xenstored: error %i untranslatable", error);
@@ -962,6 +1174,12 @@ static void do_write(struct connection *conn, struct buffered_data *in)
 		}
 	}
 
+	/*
+	 * 在以下设置conn->transaction:
+	 *   - xenstore/xenstored_core.c|1400| <<process_message>> conn->transaction = trans;
+	 *   - xenstore/xenstored_core.c|1485| <<process_message>> conn->transaction = NULL;
+	 *   - xenstore/xenstored_transaction.c|284| <<do_transaction_end>> conn->transaction = NULL;
+	 */
 	add_change_node(conn->transaction, name, false);
 	wrl_apply_debit_direct(conn);
 	fire_watches(conn, in, name, false);
@@ -1232,6 +1450,13 @@ static void do_debug(struct connection *conn, struct buffered_data *in)
 /* Process "in" for conn: "in" will vanish after this conversation, so
  * we can talloc off it for temporary variables.  May free "conn".
  */
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|1341| <<consider_message>> process_message(conn, conn->in);
+ *
+ * struct connection:
+ *  -> struct buffered_data *in; //Buffered incoming data.
+ */
 static void process_message(struct connection *conn, struct buffered_data *in)
 {
 	struct transaction *trans;
@@ -1243,6 +1468,20 @@ static void process_message(struct connection *conn, struct buffered_data *in)
 	}
 
 	assert(conn->transaction == NULL);
+	/*
+	 * 在以下使用generation:
+	 *   - xenstore/xenstored_transaction.c|102| <<add_change_node>> generation++;
+	 *   - xenstore/xenstored_transaction.c|256| <<do_transaction_end>> generation++;
+	 *
+	 * 在以下使用transaction->generation:
+	 *   - xenstore/xenstored_transaction.c|166| <<do_transaction_start>> trans->generation = generation;
+	 *   - xenstore/xenstored_transaction.c|235| <<do_transaction_end>> if (trans->generation != generation) {
+	 *
+	 * 在以下设置conn->transaction:
+	 *   - xenstore/xenstored_core.c|1400| <<process_message>> conn->transaction = trans;
+	 *   - xenstore/xenstored_core.c|1485| <<process_message>> conn->transaction = NULL;
+	 *   - xenstore/xenstored_transaction.c|284| <<do_transaction_end>> conn->transaction = NULL;
+	 */
 	conn->transaction = trans;
 
 	switch (in->hdr.msg.type) {
@@ -1331,13 +1570,25 @@ static void process_message(struct connection *conn, struct buffered_data *in)
 	conn->transaction = NULL;
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|1387| <<handle_input>> consider_message(conn);
+ */
 static void consider_message(struct connection *conn)
 {
+	/*
+	 * struct connection:
+	 *  -> struct buffered_data *in; //Buffered incoming data.
+	 */
 	if (verbose)
 		xprintf("Got message %s len %i from %p\n",
 			sockmsg_string(conn->in->hdr.msg.type),
 			conn->in->hdr.msg.len, conn);
 
+	/*
+	 * struct connection:
+	 *  -> struct buffered_data *in; //Buffered incoming data.
+	 */
 	process_message(conn, conn->in);
 
 	talloc_free(conn->in);
@@ -1346,6 +1597,11 @@ static void consider_message(struct connection *conn)
 
 /* Errors in reading or allocating here mean we get out of sync, so we
  * drop the whole client connection. */
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|2078| <<main>> handle_input(conn);
+ *   - xenstore/xenstored_core.c|2095| <<main>> handle_input(conn);
+ */
 static void handle_input(struct connection *conn)
 {
 	int bytes;
@@ -1392,12 +1648,42 @@ bad_client:
 	talloc_free(conn);
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|2161| <<main>> handle_output(conn);
+ *   - xenstore/xenstored_core.c|2184| <<main>> handle_output(conn);
+ */
 static void handle_output(struct connection *conn)
 {
 	if (!write_messages(conn))
 		talloc_free(conn);
 }
 
+/*
+ * 创建一个minios的domU时, new_connection()调用两次
+ *
+ * (gdb) bt
+ * #0  new_connection (write=0x40631a <writechn>, read=0x406249 <readchn>) at xenstored_core.c:1405
+ * #1  0x000000000040607a in new_domain (context=<value optimized out>, domid=1, port=1) at xenstored_domain.c:316
+ * #2  0x00000000004069f7 in do_introduce (conn=0x16a64c0, in=0x16a6b10) at xenstored_domain.c:400
+ * #3  0x0000000000404971 in process_message (conn=0x16a64c0) at xenstored_core.c:1298
+ * #4  consider_message (conn=0x16a64c0) at xenstored_core.c:1341
+ * #5  handle_input (conn=0x16a64c0) at xenstored_core.c:1387
+ * #6  0x00000000004053ba in main (argc=<value optimized out>, argv=<value optimized out>) at xenstored_core.c:2095
+ *
+ * (gdb) bt
+ * #0  new_connection (write=0x4023c3 <writefd>, read=0x40234b <readfd>) at xenstored_core.c:1405
+ * #1  0x0000000000403101 in accept_connection (sock=<value optimized out>, canwrite=false) at xenstored_core.c:1483
+ * #2  0x00000000004052b2 in main (argc=<value optimized out>, argv=<value optimized out>) at xenstored_core.c:2050
+ *
+ * called by:
+ *   - xenstore/xenstored_core.c|1496| <<accept_connection>> conn = new_connection(writefd, readfd);
+ *   - xenstore/xenstored_domain.c|339| <<new_domain>> domain->conn = new_connection(writechn, readchn);
+ *
+ *
+ * struct domain:
+ *  - struct connection *conn;
+ */
 struct connection *new_connection(connwritefn_t *write, connreadfn_t *read)
 {
 	struct connection *new;
@@ -1412,6 +1698,17 @@ struct connection *new_connection(connwritefn_t *write, connreadfn_t *read)
 	new->read = read;
 	new->can_write = true;
 	new->transaction_started = 0;
+	/*
+	 * 在以下使用connection->out_list:
+	 *   - xenstore/xenstored_core.c|293| <<write_messages>> out = list_top(&conn->out_list, struct buffered_data, list);
+	 *   - xenstore/xenstored_core.c|347| <<destroy_conn>> while (!list_empty(&conn->out_list)
+	 *   - xenstore/xenstored_core.c|427| <<initialize_fds>> !list_empty(&conn->out_list)))
+	 *   - xenstore/xenstored_core.c|431| <<initialize_fds>> if (!list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_core.c|746| <<send_reply>> list_add_tail(&bdata->list, &conn->out_list);
+	 *   - xenstore/xenstored_core.c|1588| <<new_connection>> INIT_LIST_HEAD(&new->out_list);
+	 *   - xenstore/xenstored_core.c|2306| <<main>> !list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_domain.c|383| <<domain_conn_reset>> while ((out = list_top(&conn->out_list, struct buffered_data, list))) {
+	 */
 	INIT_LIST_HEAD(&new->out_list);
 	INIT_LIST_HEAD(&new->watches);
 	INIT_LIST_HEAD(&new->transaction_list);
@@ -1471,6 +1768,11 @@ static int readfd(struct connection *conn, void *data, unsigned int len)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|2105| <<main>> accept_connection(*sock, true);
+ *   - xenstore/xenstored_core.c|2115| <<main>> accept_connection(*ro_sock, false);
+ */
 static void accept_connection(int sock, bool canwrite)
 {
 	int fd;
@@ -1761,6 +2063,10 @@ static int destroy_fd(void *_fd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|2219| <<main>> init_sockets(&sock, &ro_sock);
+ */
 static void init_sockets(int **psock, int **pro_sock)
 {
 	struct sockaddr_un addr;
@@ -1809,6 +2115,10 @@ static void init_sockets(int **psock, int **pro_sock)
 
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|2217| <<main>> signal(SIGUSR1, do_talloc_report);
+ */
 static void do_talloc_report(int sig)
 {
 	trigger_talloc_report = true;
@@ -1873,15 +2183,28 @@ int main(int argc, char *argv[])
 	int sock_pollfd_idx = -1, ro_sock_pollfd_idx = -1;
 	bool dofork = true;
 	bool outputpid = false;
+	/*
+	 * 在以下使用no_domain_init:
+	 *   - xenstore/xenstored_core.c|1898| <<main>> no_domain_init = true;
+	 *   - xenstore/xenstored_core.c|1986| <<main>> if (!no_domain_init)
+	 */ 
 	bool no_domain_init = false;
 	const char *pidfile = NULL;
 	const char *memfile = NULL;
 	int timeout;
 
+	/*
+	 * xenstored --pid-file /var/run/xenstored.pid --entry-nb=4096 --transaction=512
+	 */
 	while ((opt = getopt_long(argc, argv, "DE:F:HNPS:t:T:RLVW:M:", options,
 				  NULL)) != -1) {
 		switch (opt) {
 		case 'D':
+			/*
+			 * 在以下使用no_domain_init:
+			 *   - xenstore/xenstored_core.c|1898| <<main>> no_domain_init = true;
+			 *   - xenstore/xenstored_core.c|1986| <<main>> if (!no_domain_init)
+			 */
 			no_domain_init = true;
 			break;
 		case 'E':
@@ -1963,6 +2286,9 @@ int main(int argc, char *argv[])
 		signal(SIGUSR1, do_talloc_report);
 	}
 
+	/*
+	 * sock的定义: int *sock;
+	 */
 	init_sockets(&sock, &ro_sock);
 	init_pipe(reopen_log_pipe);
 
@@ -1970,6 +2296,11 @@ int main(int argc, char *argv[])
 	setup_structure();
 
 	/* Listen to hypervisor. */
+	/*
+	 * 在以下使用no_domain_init:
+	 *   - xenstore/xenstored_core.c|1898| <<main>> no_domain_init = true;
+	 *   - xenstore/xenstored_core.c|1986| <<main>> if (!no_domain_init)
+	 */
 	if (!no_domain_init)
 		domain_init();
 
@@ -1992,12 +2323,23 @@ int main(int argc, char *argv[])
 		       &timeout);
 
 	/* Tell the kernel we're up and running. */
+	/* linux下什么也不做 */
 	xenbus_notify_running();
 
 	/* Main loop. */
 	for (;;) {
 		struct connection *conn, *next;
 
+		/*
+		 * 在以下使用trigger_talloc_report:
+		 *   - xenstore/xenstored_core.c|74| <<global>> static bool trigger_talloc_report = false;
+		 *   - xenstore/xenstored_core.c|1827| <<do_talloc_report>> trigger_talloc_report = true;
+		 *   - xenstore/xenstored_core.c|2035| <<main>> if (trigger_talloc_report) {
+		 *   - xenstore/xenstored_core.c|2039| <<main>> trigger_talloc_report = false;
+		 *
+		 * (gdb) p trigger_talloc_report
+		 * $1 = false
+		 */
 		if (trigger_talloc_report) {
 			FILE *out;
 
@@ -2037,6 +2379,11 @@ int main(int argc, char *argv[])
 				barf_perror("sock poll failed");
 				break;
 			} else if (fds[sock_pollfd_idx].revents & POLLIN) {
+				/*
+				 * called by:
+				 *   - xenstore/xenstored_core.c|2105| <<main>> accept_connection(*sock, true);
+				 *   - xenstore/xenstored_core.c|2115| <<main>> accept_connection(*ro_sock, false);
+				 */
 				accept_connection(*sock, true);
 				sock_pollfd_idx = -1;
 			}
@@ -2047,6 +2394,11 @@ int main(int argc, char *argv[])
 				barf_perror("ro sock poll failed");
 				break;
 			} else if (fds[ro_sock_pollfd_idx].revents & POLLIN) {
+				/*
+				 * called by:
+				 *   - xenstore/xenstored_core.c|2105| <<main>> accept_connection(*sock, true);
+				 *   - xenstore/xenstored_core.c|2115| <<main>> accept_connection(*ro_sock, false);
+				 */
 				accept_connection(*ro_sock, false);
 				ro_sock_pollfd_idx = -1;
 			}
@@ -2062,6 +2414,17 @@ int main(int argc, char *argv[])
 			}
 		}
 
+		/*
+		 * 在以下使用connections:
+		 *   - xenstore/xenstored_core.c|66| <<global>> LIST_HEAD(connections);
+		 *   - xenstore/xenstored_core.c|369| <<initialize_fds>> list_for_each_entry(conn, &connections, list) {
+		 *   - xenstore/xenstored_core.c|1438| <<new_connection>> list_add_tail(&new->list, &connections);
+		 *   - xenstore/xenstored_core.c|2099| <<main>> next = list_entry(connections.next, typeof(*conn), list);
+		 *   - xenstore/xenstored_core.c|2100| <<main>> if (&next->list != &connections)
+		 *   - xenstore/xenstored_core.c|2102| <<main>> while (&next->list != &connections) {
+		 *   - xenstore/xenstored_core.c|2107| <<main>> if (&next->list != &connections)
+		 *   - xenstore/xenstored_watch.c|109| <<fire_watches>> list_for_each_entry(i, &connections, list) {
+		 */
 		next = list_entry(connections.next, typeof(*conn), list);
 		if (&next->list != &connections)
 			talloc_increase_ref_count(next);
@@ -2073,6 +2436,10 @@ int main(int argc, char *argv[])
 			if (&next->list != &connections)
 				talloc_increase_ref_count(next);
 
+			/*
+			 * struct connection:
+			 *  -> struct domain *domain; //The domain I'm associated with, if any.
+			 */
 			if (conn->domain) {
 				if (domain_can_read(conn))
 					handle_input(conn);
diff --git a/tools/xenstore/xenstored_core.h b/tools/xenstore/xenstored_core.h
index 14e761343c..180ccaa69e 100644
--- a/tools/xenstore/xenstored_core.h
+++ b/tools/xenstore/xenstored_core.h
@@ -70,6 +70,10 @@ struct connection
 	int pollfd_idx;
 
 	/* Who am I? 0 for socket connections. */
+	/*
+	 * 找到的唯一设置conn->id的地方:
+	 *   - xenstore/xenstored_domain.c|341| <<new_domain>> domain->conn->id = domid;
+	 */
 	unsigned int id;
 
 	/* Is this a read-only connection? */
@@ -79,14 +83,53 @@ struct connection
 	struct buffered_data *in;
 
 	/* Buffered output data */
+	/*
+	 * 在以下使用connection->out_list:
+	 *   - xenstore/xenstored_core.c|293| <<write_messages>> out = list_top(&conn->out_list, struct buffered_data, list);
+	 *   - xenstore/xenstored_core.c|347| <<destroy_conn>> while (!list_empty(&conn->out_list)
+	 *   - xenstore/xenstored_core.c|427| <<initialize_fds>> !list_empty(&conn->out_list)))
+	 *   - xenstore/xenstored_core.c|431| <<initialize_fds>> if (!list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_core.c|746| <<send_reply>> list_add_tail(&bdata->list, &conn->out_list);
+	 *   - xenstore/xenstored_core.c|1588| <<new_connection>> INIT_LIST_HEAD(&new->out_list);
+	 *   - xenstore/xenstored_core.c|2306| <<main>> !list_empty(&conn->out_list))
+	 *   - xenstore/xenstored_domain.c|383| <<domain_conn_reset>> while ((out = list_top(&conn->out_list, struct buffered_data, list))) {
+	 */
 	struct list_head out_list;
 
 	/* Transaction context for current request (NULL if none). */
+	/*
+	 * 在以下使用generation:
+	 *   - xenstore/xenstored_transaction.c|102| <<add_change_node>> generation++;
+	 *   - xenstore/xenstored_transaction.c|256| <<do_transaction_end>> generation++;
+	 *
+	 * 在以下使用transaction->generation:
+	 *   - xenstore/xenstored_transaction.c|166| <<do_transaction_start>> trans->generation = generation;
+	 *   - xenstore/xenstored_transaction.c|235| <<do_transaction_end>> if (trans->generation != generation) {
+	 *
+	 * 在以下设置conn->transaction:
+	 *   - xenstore/xenstored_core.c|1400| <<process_message>> conn->transaction = trans; 
+	 *   - xenstore/xenstored_core.c|1485| <<process_message>> conn->transaction = NULL;
+	 *   - xenstore/xenstored_transaction.c|284| <<do_transaction_end>> conn->transaction = NULL;
+	 */
 	struct transaction *transaction;
 
 	/* List of in-progress transactions. */
+	/*
+	 * 在以下使用transaction_list:
+	 *   - xenstore/xenstored_transaction.c|135| <<transaction_lookup>> list_for_each_entry(trans, &conn->transaction_list, list)
+	 *   - xenstore/xenstored_transaction.c|184| <<do_transaction_start>> list_add_tail(&trans->list, &conn->transaction_list);
+	 *   - xenstore/xenstored_transaction.c|286| <<conn_delete_all_transactions>> while ((trans = list_top(&conn->transaction_list,
+	 */
 	struct list_head transaction_list;
 	uint32_t next_transaction_id;
+	/*
+	 * 在以下使用transaction_started:
+	 *   - xenstore/xenstored_core.c|1450| <<new_connection>> new->transaction_started = 0;
+	 *   - xenstore/xenstored_transaction.c|157| <<do_transaction_start>> if (conn->id && conn->transaction_started > quota_max_transaction) {
+	 *   - xenstore/xenstored_transaction.c|187| <<do_transaction_start>> conn->transaction_started++;
+	 *   - xenstore/xenstored_transaction.c|217| <<do_transaction_end>> conn->transaction_started--;
+	 *   - xenstore/xenstored_transaction.c|294| <<conn_delete_all_transactions>> conn->transaction_started = 0;
+	 */
 	unsigned int transaction_started;
 
 	/* The domain I'm associated with, if any. */
diff --git a/tools/xenstore/xenstored_domain.c b/tools/xenstore/xenstored_domain.c
index c99311ab3b..2cabf50fae 100644
--- a/tools/xenstore/xenstored_domain.c
+++ b/tools/xenstore/xenstored_domain.c
@@ -38,8 +38,31 @@
 
 static xc_interface **xc_handle;
 xc_gnttab **xcg_handle;
+/*
+ * 在以下使用virq_port:
+ *   - xenstore/xenstored_domain.c|279| <<handle_event>> if (port == virq_port)
+ *   - xenstore/xenstored_domain.c|721| <<domain_init>> virq_port = rc;
+ */
 static evtchn_port_t virq_port;
 
+/*
+ * 在以下使用xce_handle:
+ *   - xenstore/xenstored_domain.c|43| <<global>> xc_evtchn *xce_handle = NULL;
+ *   - xenstore/xenstored_core.c|362| <<initialize_fds>> if (xce_handle != NULL)
+ *   - xenstore/xenstored_core.c|363| <<initialize_fds>> xce_pollfd_idx = set_fd(xc_evtchn_fd(xce_handle),
+ *   - xenstore/xenstored_domain.c|140| <<writechn>> xc_evtchn_notify(xce_handle, conn->domain->port);
+ *   - xenstore/xenstored_domain.c|170| <<readchn>> xc_evtchn_notify(xce_handle, conn->domain->port);
+ *   - xenstore/xenstored_domain.c|202| <<destroy_domain>> if (xc_evtchn_unbind(xce_handle, domain->port) == -1)
+ *   - xenstore/xenstored_domain.c|258| <<handle_event>> if ((port = xc_evtchn_pending(xce_handle)) == -1)
+ *   - xenstore/xenstored_domain.c|264| <<handle_event>> if (xc_evtchn_unmask(xce_handle, port) == -1)
+ *   - xenstore/xenstored_domain.c|311| <<new_domain>> rc = xc_evtchn_bind_interdomain(xce_handle, domid, port);
+ *   - xenstore/xenstored_domain.c|416| <<do_introduce>> xc_evtchn_unbind(xce_handle, domain->port);
+ *   - xenstore/xenstored_domain.c|417| <<do_introduce>> rc = xc_evtchn_bind_interdomain(xce_handle, domid, port);
+ *   - xenstore/xenstored_domain.c|642| <<dom0_init>> xc_evtchn_notify(xce_handle, dom0->port);
+ *   - xenstore/xenstored_domain.c|671| <<domain_init>> xce_handle = xc_evtchn_open(NULL, 0);
+ *   - xenstore/xenstored_domain.c|673| <<domain_init>> if (xce_handle == NULL)
+ *   - xenstore/xenstored_domain.c|679| <<domain_init>> if ((rc = xc_evtchn_bind_virq(xce_handle, VIRQ_DOM_EXC)) == -1)
+ */
 xc_evtchn *xce_handle = NULL;
 
 struct domain
@@ -101,6 +124,10 @@ static void *get_output_chunk(XENSTORE_RING_IDX cons,
 	return buf + MASK_XENSTORE_IDX(prod);
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_domain.c|185| <<readchn>> src = get_input_chunk(cons, prod, intf->req, &avail);
+ */
 static const void *get_input_chunk(XENSTORE_RING_IDX cons,
 				   XENSTORE_RING_IDX prod,
 				   const char *buf, uint32_t *len)
@@ -142,6 +169,10 @@ static int writechn(struct connection *conn,
 	return len;
 }
 
+/*
+ * 在以下使用readchn():
+ *   - xenstore/xenstored_domain.c|352| <<new_domain>> domain->conn = new_connection(writechn, readchn);
+ */
 static int readchn(struct connection *conn, void *data, unsigned int len)
 {
 	uint32_t avail;
@@ -290,6 +321,19 @@ static char *talloc_domain_path(void *context, unsigned int domid)
 	return talloc_asprintf(context, "/local/domain/%u", domid);
 }
 
+/*
+ * (gdb) bt
+ * #0  new_domain (context=0x16a6b10, domid=1, port=1) at xenstored_domain.c:299
+ * #1  0x00000000004069f7 in do_introduce (conn=0x16a64c0, in=0x16a6b10) at xenstored_domain.c:400
+ * #2  0x0000000000404971 in process_message (conn=0x16a64c0) at xenstored_core.c:1298
+ * #3  consider_message (conn=0x16a64c0) at xenstored_core.c:1341
+ * #4  handle_input (conn=0x16a64c0) at xenstored_core.c:1387
+ * #5  0x00000000004053ba in main (argc=<value optimized out>, argv=<value optimized out>) at xenstored_core.c:2095
+ *
+ * called by:
+ *   - xenstore/xenstored_domain.c|423| <<do_introduce>> domain = new_domain(in, domid, port);
+ *   - xenstore/xenstored_domain.c|655| <<dom0_init>> dom0 = new_domain(NULL, 0, port);
+ */
 static struct domain *new_domain(void *context, unsigned int domid,
 				 int port)
 {
@@ -313,6 +357,10 @@ static struct domain *new_domain(void *context, unsigned int domid,
 	    return NULL;
 	domain->port = rc;
 
+	/*
+	 * struct domain:
+	 *  - struct connection *conn;
+	 */
 	domain->conn = new_connection(writechn, readchn);
 	domain->conn->domain = domain;
 	domain->conn->id = domid;
@@ -336,6 +384,10 @@ static struct domain *find_domain_by_domid(unsigned int domid)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_domain.c|465| <<do_introduce>> domain_conn_reset(domain);
+ */
 static void domain_conn_reset(struct domain *domain)
 {
 	struct connection *conn = domain->conn;
@@ -358,6 +410,10 @@ static void domain_conn_reset(struct domain *domain)
 }
 
 /* domid, mfn, evtchn, path */
+/*
+ * 处理XS_INTRODUCE:
+ *   - xenstore/xenstored_core.c|1320| <<process_message>> do_introduce(conn, in);
+ */
 void do_introduce(struct connection *conn, struct buffered_data *in)
 {
 	struct domain *domain;
@@ -620,6 +676,10 @@ void restore_existing_connections(void)
 {
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_domain.c|742| <<domain_init>> if (dom0_init() != 0)
+ */
 static int dom0_init(void) 
 { 
 	evtchn_port_t port;
@@ -644,6 +704,10 @@ static int dom0_init(void)
 	return 0; 
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|1987| <<main>> domain_init();
+ */
 void domain_init(void)
 {
 	int rc;
@@ -668,6 +732,24 @@ void domain_init(void)
 	else
 		talloc_set_destructor(xcg_handle, close_xcg_handle);
 
+	/*
+	 * 在以下使用xce_handle:
+	 *   - xenstore/xenstored_domain.c|43| <<global>> xc_evtchn *xce_handle = NULL;
+	 *   - xenstore/xenstored_core.c|362| <<initialize_fds>> if (xce_handle != NULL)
+	 *   - xenstore/xenstored_core.c|363| <<initialize_fds>> xce_pollfd_idx = set_fd(xc_evtchn_fd(xce_handle),
+	 *   - xenstore/xenstored_domain.c|140| <<writechn>> xc_evtchn_notify(xce_handle, conn->domain->port);
+	 *   - xenstore/xenstored_domain.c|170| <<readchn>> xc_evtchn_notify(xce_handle, conn->domain->port);
+	 *   - xenstore/xenstored_domain.c|202| <<destroy_domain>> if (xc_evtchn_unbind(xce_handle, domain->port) == -1)
+	 *   - xenstore/xenstored_domain.c|258| <<handle_event>> if ((port = xc_evtchn_pending(xce_handle)) == -1)
+	 *   - xenstore/xenstored_domain.c|264| <<handle_event>> if (xc_evtchn_unmask(xce_handle, port) == -1)
+	 *   - xenstore/xenstored_domain.c|311| <<new_domain>> rc = xc_evtchn_bind_interdomain(xce_handle, domid, port);
+	 *   - xenstore/xenstored_domain.c|416| <<do_introduce>> xc_evtchn_unbind(xce_handle, domain->port);
+	 *   - xenstore/xenstored_domain.c|417| <<do_introduce>> rc = xc_evtchn_bind_interdomain(xce_handle, domid, port);
+	 *   - xenstore/xenstored_domain.c|642| <<dom0_init>> xc_evtchn_notify(xce_handle, dom0->port);
+	 *   - xenstore/xenstored_domain.c|671| <<domain_init>> xce_handle = xc_evtchn_open(NULL, 0);
+	 *   - xenstore/xenstored_domain.c|673| <<domain_init>> if (xce_handle == NULL)
+	 *   - xenstore/xenstored_domain.c|679| <<domain_init>> if ((rc = xc_evtchn_bind_virq(xce_handle, VIRQ_DOM_EXC)) == -1)
+	 */
 	xce_handle = xc_evtchn_open(NULL, 0);
 
 	if (xce_handle == NULL)
@@ -678,6 +760,11 @@ void domain_init(void)
 
 	if ((rc = xc_evtchn_bind_virq(xce_handle, VIRQ_DOM_EXC)) == -1)
 		barf_perror("Failed to bind to domain exception virq port");
+	/*
+	 * 在以下使用virq_port:
+	 *   - xenstore/xenstored_domain.c|279| <<handle_event>> if (port == virq_port)
+	 *   - xenstore/xenstored_domain.c|721| <<domain_init>> virq_port = rc;
+	 */
 	virq_port = rc;
 }
 
diff --git a/tools/xenstore/xenstored_transaction.c b/tools/xenstore/xenstored_transaction.c
index 32ae47debf..26c43cb460 100644
--- a/tools/xenstore/xenstored_transaction.c
+++ b/tools/xenstore/xenstored_transaction.c
@@ -69,6 +69,15 @@ struct transaction
 	uint32_t id;
 
 	/* Generation when transaction started. */
+	/*
+	 * 在以下使用generation:
+	 *   - xenstore/xenstored_transaction.c|102| <<add_change_node>> generation++;
+	 *   - xenstore/xenstored_transaction.c|256| <<do_transaction_end>> generation++;
+	 *
+	 * 在以下使用transaction->generation:
+	 *   - xenstore/xenstored_transaction.c|166| <<do_transaction_start>> trans->generation = generation;
+	 *   - xenstore/xenstored_transaction.c|235| <<do_transaction_end>> if (trans->generation != generation) {
+	 */
 	unsigned int generation;
 
 	/* TDB to work on, and filename */
@@ -76,16 +85,51 @@ struct transaction
 	char *tdb_name;
 
 	/* List of changed nodes. */
+	/*
+	 * 在以下使用changes:
+	 *   - xenstore/xenstored_transaction.c|124| <<add_change_node>> list_for_each_entry(i, &trans->changes, list)
+	 *   - xenstore/xenstored_transaction.c|131| <<add_change_node>> list_add_tail(&i->list, &trans->changes);
+	 *   - xenstore/xenstored_transaction.c|192| <<do_transaction_start>> INIT_LIST_HEAD(&trans->changes);
+	 *   - xenstore/xenstored_transaction.c|340| <<do_transaction_end>> list_for_each_entry(i, &trans->changes, list)
+	 */
 	struct list_head changes;
 
 	/* List of changed domains - to record the changed domain entry number */
+	/*
+	 * 在以下使用changed_domains:
+	 *   - xenstore/xenstored_transaction.c|193| <<do_transaction_start>> INIT_LIST_HEAD(&trans->changed_domains);
+	 *   - xenstore/xenstored_transaction.c|336| <<do_transaction_end>> list_for_each_entry(d, &trans->changed_domains, list)
+	 *   - xenstore/xenstored_transaction.c|351| <<transaction_entry_inc>> list_for_each_entry(d, &trans->changed_domains, list)
+	 *   - xenstore/xenstored_transaction.c|360| <<transaction_entry_inc>> list_add_tail(&d->list, &trans->changed_domains);
+	 *   - xenstore/xenstored_transaction.c|367| <<transaction_entry_dec>> list_for_each_entry(d, &trans->changed_domains, list)
+	 *   - xenstore/xenstored_transaction.c|376| <<transaction_entry_dec>> list_add_tail(&d->list, &trans->changed_domains);
+	 */
 	struct list_head changed_domains;
 };
 
+/*
+ * 在以下使用quota_max_transaction:
+ *   - xenstore/xenstored_core.c|2150| <<main>> quota_max_transaction = strtol(optarg, NULL, 10);
+ *   - xenstore/xenstored_transaction.c|223| <<do_transaction_start>> if (conn->id && conn->transaction_started > quota_max_transaction) {
+ *   - xenstore/xenstored_core.c|144| <<global>> int quota_max_transaction = 10;
+ */
 extern int quota_max_transaction;
+/*
+ * 在以下使用transaction->generation:
+ *   - xenstore/xenstored_transaction.c|166| <<do_transaction_start>> trans->generation = generation;
+ *   - xenstore/xenstored_transaction.c|235| <<do_transaction_end>> if (trans->generation != generation) {
+ *
+ * 在以下使用generation:
+ *   - xenstore/xenstored_transaction.c|102| <<add_change_node>> generation++;
+ *   - xenstore/xenstored_transaction.c|256| <<do_transaction_end>> generation++;
+ */
 static unsigned int generation;
 
 /* Return tdb context to use for this connection. */
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|151| <<tdb_context>> return tdb_transaction_context(conn->transaction);
+ */
 TDB_CONTEXT *tdb_transaction_context(struct transaction *trans)
 {
 	return trans->tdb;
@@ -93,6 +137,13 @@ TDB_CONTEXT *tdb_transaction_context(struct transaction *trans)
 
 /* Callers get a change node (which can fail) and only commit after they've
  * finished.  This way they don't have to unwind eg. a write. */
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|1112| <<do_write>> add_change_node(conn->transaction, name, false);
+ *   - xenstore/xenstored_core.c|1144| <<do_mkdir>> add_change_node(conn->transaction, name, false);
+ *   - xenstore/xenstored_core.c|1271| <<do_rm>> add_change_node(conn->transaction, name, true);
+ *   - xenstore/xenstored_core.c|1348| <<do_set_perms>> add_change_node(conn->transaction, name, false);
+ */
 void add_change_node(struct transaction *trans, const char *node, bool recurse)
 {
 	struct changed_node *i;
@@ -113,6 +164,10 @@ void add_change_node(struct transaction *trans, const char *node, bool recurse)
 	list_add_tail(&i->list, &trans->changes);
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_transaction.c|215| <<do_transaction_start>> talloc_set_destructor(trans, destroy_transaction);
+ */
 static int destroy_transaction(void *_transaction)
 {
 	struct transaction *trans = _transaction;
@@ -125,6 +180,11 @@ static int destroy_transaction(void *_transaction)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|1393| <<process_message>> trans = transaction_lookup(conn, in->hdr.msg.tx_id);
+ *   - xenstore/xenstored_transaction.c|230| <<do_transaction_start>> exists = transaction_lookup(conn, conn->next_transaction_id++);
+ */
 struct transaction *transaction_lookup(struct connection *conn, uint32_t id)
 {
 	struct transaction *trans;
@@ -132,6 +192,12 @@ struct transaction *transaction_lookup(struct connection *conn, uint32_t id)
 	if (id == 0)
 		return NULL;
 
+	/*
+	 * 在以下使用transaction_list:
+	 *   - xenstore/xenstored_transaction.c|135| <<transaction_lookup>> list_for_each_entry(trans, &conn->transaction_list, list)
+	 *   - xenstore/xenstored_transaction.c|184| <<do_transaction_start>> list_add_tail(&trans->list, &conn->transaction_list);
+	 *   - xenstore/xenstored_transaction.c|286| <<conn_delete_all_transactions>> while ((trans = list_top(&conn->transaction_list,
+	 */
 	list_for_each_entry(trans, &conn->transaction_list, list)
 		if (trans->id == id)
 			return trans;
@@ -139,12 +205,22 @@ struct transaction *transaction_lookup(struct connection *conn, uint32_t id)
 	return ERR_PTR(-ENOENT);
 }
 
+/*
+ * 处理XS_TRANSACTION_START:
+ *   - xenstore/xenstored_core.c|1290| <<process_message>> do_transaction_start(conn, in);
+ */
 void do_transaction_start(struct connection *conn, struct buffered_data *in)
 {
 	struct transaction *trans, *exists;
 	char id_str[20];
 
 	/* We don't support nested transactions. */
+	/*
+	 * 在以下设置conn->transaction:
+	 *   - xenstore/xenstored_core.c|1400| <<process_message>> conn->transaction = trans;
+	 *   - xenstore/xenstored_core.c|1485| <<process_message>> conn->transaction = NULL;
+	 *   - xenstore/xenstored_transaction.c|284| <<do_transaction_end>> conn->transaction = NULL;
+	 */
 	if (conn->transaction) {
 		send_error(conn, EBUSY);
 		return;
@@ -177,9 +253,28 @@ void do_transaction_start(struct connection *conn, struct buffered_data *in)
 	} while (!IS_ERR(exists));
 
 	/* Now we own it. */
+	/*
+	 * 在以下使用transaction_list:
+	 *   - xenstore/xenstored_transaction.c|135| <<transaction_lookup>> list_for_each_entry(trans, &conn->transaction_list, list)
+	 *   - xenstore/xenstored_transaction.c|184| <<do_transaction_start>> list_add_tail(&trans->list, &conn->transaction_list);
+	 *   - xenstore/xenstored_transaction.c|286| <<conn_delete_all_transactions>> while ((trans = list_top(&conn->transaction_list,
+	 */
 	list_add_tail(&trans->list, &conn->transaction_list);
+	/*
+	 * move a lump of memory from one talloc context to another return the
+	 * ptr on success, or NULL if it could not be transferred.
+	 * passing NULL as ptr will always return NULL with no side effects.
+	 */
 	talloc_steal(conn, trans);
 	talloc_set_destructor(trans, destroy_transaction);
+	/*
+	 * 在以下使用transaction_started:
+	 *   - xenstore/xenstored_core.c|1450| <<new_connection>> new->transaction_started = 0;
+	 *   - xenstore/xenstored_transaction.c|157| <<do_transaction_start>> if (conn->id && conn->transaction_started > quota_max_transaction) {
+	 *   - xenstore/xenstored_transaction.c|187| <<do_transaction_start>> conn->transaction_started++;
+	 *   - xenstore/xenstored_transaction.c|217| <<do_transaction_end>> conn->transaction_started--;
+	 *   - xenstore/xenstored_transaction.c|294| <<conn_delete_all_transactions>> conn->transaction_started = 0;
+	 */
 	conn->transaction_started++;
 	wrl_ntransactions++;
 
@@ -187,6 +282,65 @@ void do_transaction_start(struct connection *conn, struct buffered_data *in)
 	send_reply(conn, XS_TRANSACTION_START, id_str, strlen(id_str)+1);
 }
 
+/*
+ * 一个不是domain的connection的例子:
+ * (gdb) p *conn
+ * $6 = {
+ *   list = {
+ *     next = 0x1a61a00,
+ *     prev = 0x1a61230
+ *   },
+ *   fd = 15,
+ *   pollfd_idx = 6,
+ *   id = 0,
+ *   can_write = true,
+ *   in = 0x1a5f840,
+ *   out_list = {
+ *     next = 0x1a62548,
+ *     prev = 0x1a62548
+ *   },
+ *   transaction = 0x0,
+ *   transaction_list = {
+ *     next = 0x1a62560,
+ *     prev = 0x1a62560
+ *   },
+ *   next_transaction_id = 212,
+ *   transaction_started = 0,
+ *   domain = 0x0,
+ *   target = 0x0,
+ *   watches = {
+ *     next = 0x1a5f9a0,
+ *     prev = 0x1a62020
+ *   },
+ *   write = 0x4023c3 <writefd>,
+ *   read = 0x40234b <readfd>
+ * }
+ *
+ * 967 // End a transaction.  
+ * 968 // If abandon is true, transaction is discarded instead of committed.
+ * 969 // Returns false on failure, which indicates an error: transactions will
+ * 970 // not fail spuriously.
+ * 971 //
+ * 972 bool xs_transaction_end(struct xs_handle *h, xs_transaction_t t,
+ * 973                         bool abort)
+ * 974 {
+ * 975         char abortstr[2];
+ * 976 
+ * 977         if (abort)
+ * 978                 strcpy(abortstr, "F");
+ * 979         else
+ * 980                 strcpy(abortstr, "T");
+ * 981                         
+ * 982         return xs_bool(xs_single(h, t, XS_TRANSACTION_END, abortstr, NULL));
+ * 983 }
+ *
+ * 处理XS_TRANSACTION_END:
+ *   - xenstore/xenstored_core.c|1294| <<process_message>> do_transaction_end(conn, in);
+ *
+ * connection->transaction_list的类型是"struct list_head transaction_list"
+ * 记录着所有的in-progress transaction
+ * 所以do_transaction_end()不是针对某个transaction, 而是整个connection
+ */
 void do_transaction_end(struct connection *conn, struct buffered_data *in)
 {
 	const char *arg = onearg(in);
@@ -199,13 +353,31 @@ void do_transaction_end(struct connection *conn, struct buffered_data *in)
 		return;
 	}
 
+	/*
+	 * 在以下设置conn->transaction:
+	 *   - xenstore/xenstored_core.c|1400| <<process_message>> conn->transaction = trans;
+	 *   - xenstore/xenstored_core.c|1485| <<process_message>> conn->transaction = NULL;
+	 *   - xenstore/xenstored_transaction.c|284| <<do_transaction_end>> conn->transaction = NULL;
+	 */
 	if ((trans = conn->transaction) == NULL) {
 		send_error(conn, ENOENT);
 		return;
 	}
 
 	conn->transaction = NULL;
+	/*
+	 * 这里为什么把transaction从connection上移除了??
+	 * 这样transaction_lookup()就查不到了啊
+	 */
 	list_del(&trans->list);
+	/*
+	 * 在以下使用transaction_started:
+	 *   - xenstore/xenstored_core.c|1450| <<new_connection>> new->transaction_started = 0;
+	 *   - xenstore/xenstored_transaction.c|157| <<do_transaction_start>> if (conn->id && conn->transaction_started > quota_max_transaction) {
+	 *   - xenstore/xenstored_transaction.c|187| <<do_transaction_start>> conn->transaction_started++;
+	 *   - xenstore/xenstored_transaction.c|217| <<do_transaction_end>> conn->transaction_started--;
+	 *   - xenstore/xenstored_transaction.c|294| <<conn_delete_all_transactions>> conn->transaction_started = 0;
+	 */
 	conn->transaction_started--;
 
 	/* Attach transaction to arg for auto-cleanup */
@@ -213,6 +385,20 @@ void do_transaction_end(struct connection *conn, struct buffered_data *in)
 
 	if (streq(arg, "T")) {
 		/* FIXME: Merge, rather failing on any change. */
+		/*
+		 * 在以下使用generation:
+		 *   - xenstore/xenstored_transaction.c|102| <<add_change_node>> generation++;
+		 *   - xenstore/xenstored_transaction.c|256| <<do_transaction_end>> generation++;
+		 *
+		 * 在以下使用transaction->generation:
+		 *   - xenstore/xenstored_transaction.c|166| <<do_transaction_start>> trans->generation = generation;
+		 *   - xenstore/xenstored_transaction.c|235| <<do_transaction_end>> if (trans->generation != generation) {
+		 *
+		 * 
+		 * connection->transaction_list的类型是"struct list_head transaction_list"
+		 * 记录着所有的in-progress transaction
+		 * 所以do_transaction_end()不是针对某个transaction, 而是整个connection
+		 */
 		if (trans->generation != generation) {
 			send_error(conn, EAGAIN);
 			return;
@@ -220,6 +406,10 @@ void do_transaction_end(struct connection *conn, struct buffered_data *in)
 
 		wrl_apply_debit_trans_commit(conn);
 
+		/*
+		 * 在以下调用replace_tdb():
+		 *   - xenstore/xenstored_transaction.c|376| <<do_transaction_end>> if (!replace_tdb(trans->tdb_name, trans->tdb)) {
+		 */
 		if (!replace_tdb(trans->tdb_name, trans->tdb)) {
 			send_error(conn, errno);
 			return;
@@ -228,17 +418,40 @@ void do_transaction_end(struct connection *conn, struct buffered_data *in)
 		trans->tdb = NULL;
 
 		/* fix domain entry for each changed domain */
+		/*
+		 * 在以下使用changed_domains:
+		 *   - xenstore/xenstored_transaction.c|193| <<do_transaction_start>> INIT_LIST_HEAD(&trans->changed_domains);
+		 *   - xenstore/xenstored_transaction.c|336| <<do_transaction_end>> list_for_each_entry(d, &trans->changed_domains, list)
+		 *   - xenstore/xenstored_transaction.c|351| <<transaction_entry_inc>> list_for_each_entry(d, &trans->changed_domains, list)
+		 *   - xenstore/xenstored_transaction.c|360| <<transaction_entry_inc>> list_add_tail(&d->list, &trans->changed_domains);
+		 *   - xenstore/xenstored_transaction.c|367| <<transaction_entry_dec>> list_for_each_entry(d, &trans->changed_domains, list)
+		 *   - xenstore/xenstored_transaction.c|376| <<transaction_entry_dec>> list_add_tail(&d->list, &trans->changed_domains);
+		 */
 		list_for_each_entry(d, &trans->changed_domains, list)
 			domain_entry_fix(d->domid, d->nbentry);
 
 		/* Fire off the watches for everything that changed. */
 		list_for_each_entry(i, &trans->changes, list)
 			fire_watches(conn, in, i->node, i->recurse);
+		/*
+		 * 在以下使用transaction->generation:
+		 *   - xenstore/xenstored_transaction.c|166| <<do_transaction_start>> trans->generation = generation;
+		 *   - xenstore/xenstored_transaction.c|235| <<do_transaction_end>> if (trans->generation != generation) {
+		 *
+		 * 在以下使用generation:
+		 *   - xenstore/xenstored_transaction.c|102| <<add_change_node>> generation++;
+		 *   - xenstore/xenstored_transaction.c|256| <<do_transaction_end>> generation++;
+		 */
 		generation++;
 	}
 	send_ack(conn, XS_TRANSACTION_END);
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_domain.c|760| <<domain_entry_inc>> transaction_entry_inc(conn->transaction,
+ *   - xenstore/xenstored_domain.c|769| <<domain_entry_inc>> transaction_entry_inc(conn->transaction,
+ */
 void transaction_entry_inc(struct transaction *trans, unsigned int domid)
 {
 	struct changed_domain *d;
@@ -255,6 +468,11 @@ void transaction_entry_inc(struct transaction *trans, unsigned int domid)
 	list_add_tail(&d->list, &trans->changed_domains);
 }
 
+/*
+ * called by:
+ *   - xenstore/xenstored_domain.c|786| <<domain_entry_dec>> transaction_entry_dec(conn->transaction,
+ *   - xenstore/xenstored_domain.c|795| <<domain_entry_dec>> transaction_entry_dec(conn->transaction,
+ */
 void transaction_entry_dec(struct transaction *trans, unsigned int domid)
 {
 	struct changed_domain *d;
@@ -271,6 +489,20 @@ void transaction_entry_dec(struct transaction *trans, unsigned int domid)
 	list_add_tail(&d->list, &trans->changed_domains);
 }
 
+/*
+ * (gdb) bt
+ * #0  conn_delete_all_transactions (conn=0x8d1e60) at xenstored_transaction.c:278
+ * #1  0x0000000000406ab9 in domain_conn_reset (conn=0x8d04c0, in=<value optimized out>) at xenstored_domain.c:345
+ * #2  do_introduce (conn=0x8d04c0, in=<value optimized out>) at xenstored_domain.c:425
+ * #3  0x0000000000404971 in process_message (conn=0x8d04c0) at xenstored_core.c:1298
+ * #4  consider_message (conn=0x8d04c0) at xenstored_core.c:1341
+ * #5  handle_input (conn=0x8d04c0) at xenstored_core.c:1387
+ * #6  0x00000000004053ba in main (argc=<value optimized out>, argv=<value optimized out>) at xenstored_core.c:2095
+ *
+ * called by:
+ *   - xenstore/xenstored_domain.c|381| <<domain_conn_reset>> conn_delete_all_transactions(conn);
+ *   - xenstore/xenstored_domain.c|633| <<do_reset_watches>> conn_delete_all_transactions(conn);
+ */
 void conn_delete_all_transactions(struct connection *conn)
 {
 	struct transaction *trans;
diff --git a/tools/xenstore/xenstored_watch.c b/tools/xenstore/xenstored_watch.c
index d2a60a39c6..f0b8d60837 100644
--- a/tools/xenstore/xenstored_watch.c
+++ b/tools/xenstore/xenstored_watch.c
@@ -95,6 +95,17 @@ static void add_event(struct connection *conn,
  * Check whether any watch events are to be sent.
  * Temporary memory allocations are done with ctx.
  */
+/*
+ * called by:
+ *   - xenstore/xenstored_core.c|1163| <<do_write>> fire_watches(conn, in, name, false);
+ *   - xenstore/xenstored_core.c|1195| <<do_mkdir>> fire_watches(conn, in, name, false);
+ *   - xenstore/xenstored_core.c|1322| <<do_rm>> fire_watches(conn, in, name, true);
+ *   - xenstore/xenstored_core.c|1399| <<do_set_perms>> fire_watches(conn, in, name, false);
+ *   - xenstore/xenstored_domain.c|246| <<destroy_domain>> fire_watches(NULL, domain, "@releaseDomain", false);
+ *   - xenstore/xenstored_domain.c|281| <<domain_cleanup>> fire_watches(NULL, NULL, "@releaseDomain", false);
+ *   - xenstore/xenstored_domain.c|468| <<do_introduce>> fire_watches(NULL, in, "@introduceDomain", false);
+ *   - xenstore/xenstored_transaction.c|389| <<do_transaction_end>> fire_watches(conn, in, i->node, i->recurse);
+ */
 void fire_watches(struct connection *conn, void *ctx, const char *name,
 		  bool recurse)
 {
diff --git a/tools/xenstore/xs.c b/tools/xenstore/xs.c
index dd03a85755..fa7b81d136 100644
--- a/tools/xenstore/xs.c
+++ b/tools/xenstore/xs.c
@@ -36,6 +36,33 @@
 #include "list.h"
 #include "utils.h"
 
+/*
+ * // We hand errors as strings, for portability.
+ * struct xsd_errors
+ * {
+ *      int errnum;
+ *      const char *errstring;
+ * };
+ * static struct xsd_errors xsd_errors[]
+ * = {
+ *      {EINVAL, "EINVAL"},
+ *      {EACCES, "EACCES"},
+ *      {EEXIST, "EEXIST"}
+ *      {EISDIR, "EISDIR"},
+ *      {ENOENT, "ENOENT"},
+ *      {ENOMEM, "ENOMEM"},
+ *      {ENOSPC, "ENOSPC"},
+ *      {EIO, "EIO"},
+ *      {ENOTEMPTY, "ENOTEMPTY"},
+ *      {ENOSYS, "ENOSYS"},
+ *      {EROFS, "EROFS"},
+ *      {EBUSY, "EBUSY"},
+ *      {EAGAIN, "EAGAIN"},
+ *      {EISCONN, "EISCONN"},
+ *      {E2BIG, "E2BIG"}
+ * };
+ */
+
 struct xs_stored_msg {
 	struct list_head list;
 	struct xsd_sockmsg hdr;
@@ -409,6 +436,33 @@ out_false:
 #define xs_write_all write_all_choice
 #endif
 
+/*
+ * // We hand errors as strings, for portability.
+ * struct xsd_errors
+ * {
+ *      int errnum;
+ *      const char *errstring;
+ * };
+ * static struct xsd_errors xsd_errors[]
+ * = {
+ *      {EINVAL, "EINVAL"},
+ *      {EACCES, "EACCES"},
+ *      {EEXIST, "EEXIST"}
+ *      {EISDIR, "EISDIR"},
+ *      {ENOENT, "ENOENT"},
+ *      {ENOMEM, "ENOMEM"},
+ *      {ENOSPC, "ENOSPC"},
+ *      {EIO, "EIO"},
+ *      {ENOTEMPTY, "ENOTEMPTY"},
+ *      {ENOSYS, "ENOSYS"},
+ *      {EROFS, "EROFS"},
+ *      {EBUSY, "EBUSY"},
+ *      {EAGAIN, "EAGAIN"},
+ *      {EISCONN, "EISCONN"},
+ *      {E2BIG, "E2BIG"}
+ * };
+ */
+
 static int get_error(const char *errorstring)
 {
 	unsigned int i;
@@ -504,7 +558,42 @@ static void *xs_talkv(struct xs_handle *h, xs_transaction_t t,
 	mutex_unlock(&h->request_mutex);
 
 	sigaction(SIGPIPE, &oldact, NULL);
+	/*
+	 * struct xsd_sockmsg msg;
+	 */
 	if (msg.type == XS_ERROR) {
+		/*
+		 * // We hand errors as strings, for portability.
+		 * struct xsd_errors
+		 * {
+		 *	int errnum;
+		 *	const char *errstring;
+		 * };
+		 * static struct xsd_errors xsd_errors[]
+		 * = {
+		 *	{EINVAL, "EINVAL"},
+		 *	{EACCES, "EACCES"},
+		 *	{EEXIST, "EEXIST"}
+		 *	{EISDIR, "EISDIR"},
+		 *	{ENOENT, "ENOENT"},
+		 *	{ENOMEM, "ENOMEM"},
+		 *	{ENOSPC, "ENOSPC"},
+		 *	{EIO, "EIO"},
+		 *	{ENOTEMPTY, "ENOTEMPTY"},
+		 *	{ENOSYS, "ENOSYS"},
+		 *	{EROFS, "EROFS"},
+		 *	{EBUSY, "EBUSY"},
+		 *	{EAGAIN, "EAGAIN"},
+		 *	{EISCONN, "EISCONN"},
+		 *	{E2BIG, "E2BIG"}
+		 * };
+		 *
+		 * 在xenstore的send_error(), 会用到:
+		 *  773         send_reply(conn, XS_ERROR, xsd_errors[i].errstring,
+		 *  774                           strlen(xsd_errors[i].errstring) + 1);
+		 *
+		 * 假设是EAGAIN, 
+		 */
 		saved_errno = get_error(ret);
 		free(ret);
 		errno = saved_errno;
@@ -947,6 +1036,13 @@ xs_transaction_t xs_transaction_start(struct xs_handle *h)
 	char *id_str;
 	xs_transaction_t id;
 
+	/*
+	 * 在以下使用XS_TRANSACTION_START:
+	 *   - xenstore/xenstored_core.c|120| <<sockmsg_string>> case XS_TRANSACTION_START: return "TRANSACTION_START";
+	 *   - xenstore/xenstored_core.c|1289| <<process_message>> case XS_TRANSACTION_START:
+	 *   - xenstore/xenstored_transaction.c|187| <<do_transaction_start>> send_reply(conn, XS_TRANSACTION_START, id_str, strlen(id_str)+1);
+	 *   - xenstore/xs.c|950| <<xs_transaction_start>> id_str = xs_single(h, XBT_NULL, XS_TRANSACTION_START, "", NULL);
+	 */
 	id_str = xs_single(h, XBT_NULL, XS_TRANSACTION_START, "", NULL);
 	if (id_str == NULL)
 		return XBT_NULL;
@@ -962,6 +1058,12 @@ xs_transaction_t xs_transaction_start(struct xs_handle *h)
  * Returns false on failure, which indicates an error: transactions will
  * not fail spuriously.
  */
+/*
+ * 某一个调用的例子:
+ *   - python/xen/lowlevel/xs/xs.c|587| <<xspy_transaction_end>> result = xs_transaction_end(xh, th, abort);
+ *
+ * 什么时候errno=EAGAIN???
+ */
 bool xs_transaction_end(struct xs_handle *h, xs_transaction_t t,
 			bool abort)
 {
diff --git a/xen/arch/x86/domain.c b/xen/arch/x86/domain.c
index 8acc0c7d0c..b18a6747c2 100644
--- a/xen/arch/x86/domain.c
+++ b/xen/arch/x86/domain.c
@@ -119,6 +119,11 @@ static void play_dead(void)
     (*dead_idle)();
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|148| <<startup_cpu_idle_loop>> reset_stack_and_jump(idle_loop);
+ *   - arch/x86/domain.c|153| <<continue_idle_domain>> reset_stack_and_jump(idle_loop);
+ */
 static void idle_loop(void)
 {
     for ( ; ; )
@@ -159,6 +164,10 @@ static void continue_nonidle_domain(struct vcpu *v)
     reset_stack_and_jump(ret_from_intr);
 }
 
+/*
+ * called by:
+ *   - common/keyhandler.c|285| <<dump_domains>> dump_pageframe_info(d);
+ */
 void dump_pageframe_info(struct domain *d)
 {
     struct page_info *page;
@@ -507,6 +516,10 @@ int bufioreq_evtchn(struct domain *d)
     return hvm_bufioreq_evtchn(d);
 }
 
+/*
+ * x86下的调用:
+ *   - common/domain.c|813| <<complete_domain_destroy>> vcpu_destroy(v);
+ */
 void vcpu_destroy(struct vcpu *v)
 {
     if ( is_pv_32on64_vcpu(v) )
@@ -1983,6 +1996,18 @@ int hypercall_xlat_continuation(unsigned int *id, unsigned int nr,
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|2168| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->xenpage_list, ~0UL);
+ *   - arch/x86/domain.c|2175| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ *   - arch/x86/domain.c|2182| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l3_page_table);
+ *   - arch/x86/domain.c|2189| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l2_page_table);
+ *
+ * page_list被很多地方使用,这里只是三个例子:
+ *   - common/page_alloc.c|2130| <<assign_pages>> page_list_add_tail(&pg[i], &d->page_list);
+ *   - arch/x86/domain.c|180| <<dump_pageframe_info>> page_list_for_each ( page, &d->page_list )
+ *   - arch/x86/numa.c|392| <<dump_numa>> page_list_for_each(page, &d->page_list)
+ */
 static int relinquish_memory(
     struct domain *d, struct page_list_head *list, unsigned long type)
 {
@@ -1993,6 +2018,23 @@ static int relinquish_memory(
     /* Use a recursive lock, as we may enter 'free_domheap_page'. */
     spin_lock_recursive(&d->page_alloc_lock);
 
+    /*
+     * populate_physmap()用来为虚拟机分配内存, 最终会调用以下来分配一段order的内存:
+     *
+     * page = alloc_domheap_pages(d, a->extent_order, a->memflags);
+     *
+     * 然后会assign给domain:
+     *
+     * assign_pages(d, pg, order, memflags)
+     *
+     * assign_pages()会把每一个pg[i]放入链表:
+     *
+     * page_set_owner(&pg[i], d);
+     * pg[i].count_info = PGC_allocated | 1;
+     * page_list_add_tail(&pg[i], &d->page_list);
+     *
+     * 所以d->page_list上放的是一个一个的page,而不是一组一组的page!
+     */
     while ( (page = page_list_remove_head(list)) )
     {
         /* Grab a reference to the page so it won't disappear from under us. */
@@ -2003,6 +2045,17 @@ static int relinquish_memory(
             continue;
         }
 
+        /*
+	 * Clear a bit and return its old value
+	 *
+	 * 使用_PGT_pinned的地方:
+	 *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+	 *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+	 */
         if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
             ret = put_page_and_type_preemptible(page);
         switch ( ret )
@@ -2012,6 +2065,7 @@ static int relinquish_memory(
         case -EAGAIN:
         case -EINTR:
             ret = -EAGAIN;
+	    /* 失败会链接回去!!!! */
             page_list_add(page, list);
             set_bit(_PGT_pinned, &page->u.inuse.type_info);
             put_page(page);
@@ -2051,6 +2105,7 @@ static int relinquish_memory(
                 case 0:
                     break;
                 case -EINTR:
+                    /* 失败会链接回去!!!! */
                     page_list_add(page, list);
                     page->u.inuse.type_info |= PGT_validated;
                     if ( x & PGT_partial )
@@ -2059,6 +2114,7 @@ static int relinquish_memory(
                     ret = -EAGAIN;
                     goto out;
                 case -EAGAIN:
+                    /* 失败会链接回去!!!! */
                     page_list_add(page, list);
                     page->u.inuse.type_info |= PGT_partial;
                     if ( x & PGT_partial )
@@ -2095,6 +2151,10 @@ static int relinquish_memory(
     return ret;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> rc = domain_relinquish_resources(d);
+ */
 int domain_relinquish_resources(struct domain *d)
 {
     int ret;
@@ -2172,6 +2232,12 @@ int domain_relinquish_resources(struct domain *d)
         /* fallthrough */
 
     case RELMEM_l4:
+        /*
+	 * page_list被很多地方使用,这里只是三个例子:
+	 *   - common/page_alloc.c|2130| <<assign_pages>> page_list_add_tail(&pg[i], &d->page_list);
+	 *   - arch/x86/domain.c|180| <<dump_pageframe_info>> page_list_for_each ( page, &d->page_list )
+	 *   - arch/x86/numa.c|392| <<dump_numa>> page_list_for_each(page, &d->page_list)
+	 */
         ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
         if ( ret )
             return ret;
diff --git a/xen/arch/x86/domain_page.c b/xen/arch/x86/domain_page.c
index 56f9bb1244..6ec449cbea 100644
--- a/xen/arch/x86/domain_page.c
+++ b/xen/arch/x86/domain_page.c
@@ -172,6 +172,23 @@ void *map_domain_page(unsigned long mfn)
     return (void *)MAPCACHE_VIRT_START + pfn_to_paddr(idx);
 }
 
+/*
+ * MAPCACHE_VCPU_ENTRIES = 0x00000010
+ * MAPCACHE_ENTRIES      = 0x0000000000020000
+ * MAPCACHE_VIRT_START   = 0xffff820040000000
+ * MAPCACHE_VIRT_END     = 0xffff820060000000
+ *
+ * DIRECTMAP_VIRT_START = 0xffff830000000000
+ * DIRECTMAP_SIZE       = 0x00007c8000000000
+ * DIRECTMAP_VIRT_END   = 0xffffff8000000000
+ *
+ * LINEAR_PT_VIRT_START = 0xffff810000000000
+ * LINEAR_PT_VIRT_END   = 0xffff818000000000
+ *
+ * HYPERVISOR_VIRT_START = 0xffff800000000000
+ * HYPERVISOR_VIRT_END   = 0xffff880000000000
+ */
+
 void unmap_domain_page(const void *ptr)
 {
     unsigned int idx;
diff --git a/xen/arch/x86/hvm/hvm.c b/xen/arch/x86/hvm/hvm.c
index ebdbd9fa83..5a36c706d0 100644
--- a/xen/arch/x86/hvm/hvm.c
+++ b/xen/arch/x86/hvm/hvm.c
@@ -1479,6 +1479,11 @@ void hvm_vcpu_destroy(struct vcpu *v)
     /*free_xen_event_channel(v, v->arch.hvm_vcpu.xen_port);*/
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/hvm.c|1549| <<hvm_hlt>> return hvm_vcpu_down(curr);
+ *   - arch/x86/hvm/vlapic.c|259| <<vlapic_init_sipi_one>> hvm_vcpu_down(target);
+ */
 void hvm_vcpu_down(struct vcpu *v)
 {
     struct domain *d = v->domain;
@@ -1533,6 +1538,12 @@ bool_t hvm_send_assist_req(struct vcpu *v)
     return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/emulate.c|1260| <<hvm_emulate_one>> hvm_hlt(regs->eflags);
+ *   - arch/x86/hvm/svm/svm.c|1814| <<svm_vmexit_do_hlt>> hvm_hlt(regs->eflags);
+ *   - arch/x86/hvm/vmx/vmx.c|2949| <<vmx_vmexit_handler>> hvm_hlt(regs->eflags);
+ */
 void hvm_hlt(unsigned long rflags)
 {
     struct vcpu *curr = current;
diff --git a/xen/arch/x86/hvm/vmx/vmcs.c b/xen/arch/x86/hvm/vmx/vmcs.c
index 0938f08438..cd8500b7a1 100644
--- a/xen/arch/x86/hvm/vmx/vmcs.c
+++ b/xen/arch/x86/hvm/vmx/vmcs.c
@@ -404,6 +404,11 @@ static void vmx_free_vmcs(struct vmcs_struct *vmcs)
     free_xenheap_page(vmcs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vmx/vmcs.c|436| <<vmx_clear_vmcs>> on_selected_cpus(cpumask_of(cpu), __vmx_clear_vmcs, v, 1);
+ *   - arch/x86/hvm/vmx/vmcs.c|588| <<vmx_cpu_down>> __vmx_clear_vmcs(list_entry(active_vmcs_list->next,
+ */
 static void __vmx_clear_vmcs(void *info)
 {
     struct vcpu *v = info;
@@ -428,6 +433,13 @@ static void __vmx_clear_vmcs(void *info)
     }
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/vmx/vmcs.c|629| <<vmx_vmcs_try_enter>> vmx_clear_vmcs(v);
+ *   - arch/x86/hvm/vmx/vmcs.c|661| <<vmx_vmcs_exit>> vmx_clear_vmcs(v);
+ *   - arch/x86/hvm/vmx/vmcs.c|1288| <<vmx_destroy_vmcs>> vmx_clear_vmcs(v);
+ *   - arch/x86/hvm/vmx/vmcs.c|1342| <<vmx_do_resume>> vmx_clear_vmcs(v);
+ */
 static void vmx_clear_vmcs(struct vcpu *v)
 {
     int cpu = v->arch.hvm_vmx.active_cpu;
@@ -644,6 +656,30 @@ void vmx_vmcs_enter(struct vcpu *v)
     ASSERT(okay);
 }
 
+/*
+ * 在以下调用vmx_vmcs_exit():
+ *   - arch/x86/hvm/vmx/vmcs.c|962| <<construct_vmcs>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmcs.c|1148| <<construct_vmcs>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmcs.c|1537| <<vmcs_dump_vcpu>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|468| <<vmx_vmcs_save>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|536| <<vmx_vmcs_restore>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|550| <<vmx_vmcs_restore>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|798| <<vmx_get_segment_register>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|962| <<vmx_set_segment_register>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|978| <<vmx_set_guest_pat>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|990| <<vmx_get_guest_pat>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1054| <<vmx_set_tsc_offset>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1064| <<vmx_set_rdtsc_exiting>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1148| <<vmx_load_pdptrs>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1162| <<vmx_update_host_cr3>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1174| <<vmx_update_debug_state>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1319| <<vmx_update_guest_cr>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1335| <<vmx_update_guest_efer>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1554| <<vmx_set_info_guest>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|1611| <<vmx_process_isr>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|2194| <<vmx_install_vlapic_mapping>> vmx_vmcs_exit(v);
+ *   - arch/x86/hvm/vmx/vmx.c|2256| <<vmx_vlapic_msr_changed>> vmx_vmcs_exit(v);
+ */
 void vmx_vmcs_exit(struct vcpu *v)
 {
     struct foreign_vmcs *fv;
diff --git a/xen/arch/x86/mm.c b/xen/arch/x86/mm.c
index 176893d7c5..ac35f743a7 100644
--- a/xen/arch/x86/mm.c
+++ b/xen/arch/x86/mm.c
@@ -158,6 +158,22 @@ struct rangeset *__read_mostly mmio_ro_ranges;
 
 bool_t __read_mostly opt_allow_superpage;
 bool_t __read_mostly opt_allow_hugepage;
+/*
+ * 在以下使用opt_allow_superpage:
+ *   - arch/x86/domain_build.c|1029| <<construct_dom0>> if ( opt_allow_superpage )
+ *   - arch/x86/mm.c|169| <<L2_DISALLOW_MASK>> #define L2_DISALLOW_MASK (unlikely(opt_allow_superpage) \
+ *   - arch/x86/mm.c|252| <<init_frametable>> opt_allow_superpage = 1;
+ *   - arch/x86/mm.c|273| <<init_frametable>> if (opt_allow_superpage)
+ *   - arch/x86/mm.c|1071| <<get_page_from_l2e>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|2679| <<mark_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2722| <<unmark_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2754| <<clear_superpage_mark>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|2769| <<get_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2809| <<put_superpage>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|3537| <<do_mmuext_op(MMUEXT_MARK_SUPER)>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm/shadow/common.c|57| <<shadow_domain_init>> if ( !is_pv_domain(d) || opt_allow_superpage )
+ *   - include/asm-x86/guest_pt.h|191| <<guest_supports_superpages>> ? opt_allow_superpage
+ */
 boolean_param("allowsuperpage", opt_allow_superpage);
 boolean_param("allowhugepage", opt_allow_hugepage);
 
diff --git a/xen/common/domain.c b/xen/common/domain.c
index e44d403d96..735834ff6a 100644
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -612,6 +612,10 @@ int rcu_lock_live_remote_domain_by_id(domid_t dom, struct domain **d)
     return 0;
 }
 
+/*
+ * called by:
+ *   - common/domctl.c|726| <<do_domctl(XEN_DOMCTL_destroydomain)>> ret = domain_kill(d);
+ */
 int domain_kill(struct domain *d)
 {
     int rc = 0;
diff --git a/xen/common/memory.c b/xen/common/memory.c
index a07b18a9dc..9b2d26f217 100644
--- a/xen/common/memory.c
+++ b/xen/common/memory.c
@@ -151,6 +151,20 @@ static void populate_physmap(struct memop_args *a)
                                      a->nr_extents-1) )
         return;
 
+    /*
+     * struct memop_args {
+     *     // INPUT
+     *     struct domain *domain;     // Domain to be affected.
+     *     XEN_GUEST_HANDLE(xen_pfn_t) extent_list; // List of extent base addrs.
+     *     unsigned int nr_extents;   // Number of extents to allocate or free.
+     *     unsigned int extent_order; // Size of each extent.
+     *     unsigned int memflags;     // Allocation flags.
+     *
+     *     // INPUT/OUTPUT
+     *     unsigned int nr_done;    // Number of extents processed so far.
+     *     int          preempted;  // Was the hypercall preempted?
+     * };
+     */
     if ( a->extent_order > (a->memflags & MEMF_populate_on_demand ? MAX_ORDER :
                             max_order(current->domain)) )
         return;
@@ -166,10 +180,14 @@ static void populate_physmap(struct memop_args *a)
     if ( unlikely(!d->creation_finished) )
         a->memflags |= MEMF_no_tlbflush;
 
+    /*
+     * for循环结束后基本没什么动作了
+     */
     for ( i = a->nr_done; i < a->nr_extents; i++ )
     {
         if ( i != a->nr_done && hypercall_preempt_check() )
         {
+            /* 先临时退出,下次进来继续Was the hypercall preempted? */
             a->preempted = 1;
             goto out;
         }
@@ -185,6 +203,7 @@ static void populate_physmap(struct memop_args *a)
         }
         else
         {
+            /* 对于非arm直接忽略!! */
             if ( is_domain_direct_mapped(d) )
             {
                 mfn = gpfn;
@@ -1100,6 +1119,27 @@ long do_memory_op(unsigned long cmd, XEN_GUEST_HANDLE_PARAM(void) arg)
     }
 
     case XENMEM_claim_pages:
+    /*
+     * Attempt to stake a claim for a domain on a quantity of pages
+     * of system RAM, but _not_ assign specific pageframes.  Only
+     * arithmetic is performed so the hypercall is very fast and need
+     * not be preemptible, thus sidestepping time-of-check-time-of-use
+     * races for memory allocation.  Returns 0 if the hypervisor page
+     * allocator has atomically and successfully claimed the requested
+     * number of pages, else non-zero.
+     *   
+     * Any domain may have only one active claim.  When sufficient memory
+     * has been allocated to resolve the claim, the claim silently expires.
+     * Claiming zero pages effectively resets any outstanding claim and
+     * is always successful.
+     *           
+     * Note that a valid claim may be staked even after memory has been
+     * allocated for a domain.  In this case, the claim is not incremental,
+     * i.e. if the domain's tot_pages is 3, and a claim is staked for 10,
+     * only 7 additional pages are claimed.
+     *       
+     * Caller must be privileged or the hypercall fails.
+     */
         if ( copy_from_guest(&reservation, arg, 1) )
             return -EFAULT;
 
diff --git a/xen/common/page_alloc.c b/xen/common/page_alloc.c
index 95db602e7c..135beb07be 100644
--- a/xen/common/page_alloc.c
+++ b/xen/common/page_alloc.c
@@ -51,6 +51,153 @@
 #define p2m_pod_offline_or_broken_replace(pg) BUG_ON(pg != NULL)
 #endif
 
+/*
+ * 有好多struct page_info [] 是一个数组:
+ * page_info[0]
+ * page_info[1]
+ * ... ...
+ * ... ...
+ * page_info[N]
+ *
+ * 连续的page_info[i]在物理地址上也是连续的.
+ * 不同组的page_info[i]属于不同的order,我们用每组的第一个page_info表示
+ * 这组page的头.如果把头链接入了某个链表,说明整组的page_info都在某个链
+ * 表中了(其实就第一个page_info在,后面的可以用page_info[i+x]推出).
+ *
+ * 每个((*_heap[node])[zone][order])是一个struct page_list_head链表.
+ * 正如上面说的,每组page_info的第一个被放入这个链表,
+ * 表明这组page_info属于这个order.
+ *
+ * 分配的时候尽量从所要求的最小的order分配.如果不够就从大的order分配,
+ * 然后把分配了但实际不用的切割,放入小的order.
+ *
+ * 回收的时候会和前后的邻居尝试merge成更大的order.
+ *
+ * -------------------------------------------
+ *
+ * populate_physmap()用来为虚拟机分配内存, 最终会调用以下来分配一段order的内存:
+ *
+ * page = alloc_domheap_pages(d, a->extent_order, a->memflags);
+ *
+ * 然后会assign给domain:
+ *
+ * assign_pages(d, pg, order, memflags)
+ *
+ * assign_pages()会把每一个pg[i]放入链表:
+ *
+ * page_set_owner(&pg[i], d);
+ * pg[i].count_info = PGC_allocated | 1;
+ * page_list_add_tail(&pg[i], &d->page_list);
+ *
+ * 所以d->page_list上放的是一个一个的page,而不是一组一组的page!
+ */
+
+/*
+ * Here is how we expect memory pages to be reclaimed during VM destroy, that
+ * is, when we run "xm destroy" on dom0.
+ *
+ * "XendDomainInfo.destroy" is printed by below line 3842.
+ *
+ * 3830     def destroy(self):
+ * 3831         """Cleanup VM and destroy domain.  Nothrow guarantee."""
+ * 3832
+ * 3833         if self.domid is None:
+ * 3834             return
+ * 3835
+ * 3836         if self.destroying == False:
+ * 3837             self.destroying = True
+ * 3838         else:
+ * 3839             raise VmError("Domain (domid=%s) is destroying, please wait!", str(self.domid))
+ * 3840
+ * 3841         from xen.xend import XendDomain
+ * 3842         log.debug("XendDomainInfo.destroy: domid=%s", str(self.domid))
+ * 3843
+ * 3844         paths = self._prepare_phantom_paths()
+ * 3845
+ * 3846         if self.dompath is not None:
+ * 3847             try:
+ * 3848                 xc.domain_destroy_hook(self.domid)
+ * 3849                 xc.domain_pause(self.domid)
+ * 3850                 do_FLR(self.domid, self.info.is_hvm())
+ * 3851                 xc.domain_destroy(self.domid)
+ * 3852                 for state in DOM_STATES_OLD:
+ * 3853                     self.info[state] = 0
+ * 3854                 self._stateSet(DOM_STATE_HALTED)
+ *
+ * xend would run destroy() in python/xen/xend/XendDomainInfo.py, which would
+ * call xc_domain_destroy() in libxc, after printing "DEBUG
+ * (XendDomainInfo:3789) XendDomainInfo.destroy: domid=18".
+ *
+ * 83 int xc_domain_destroy(xc_interface *xch,
+ * 84                       uint32_t domid)
+ * 85 {
+ * 86     int ret;
+ * 87     DECLARE_DOMCTL;
+ * 88     domctl.cmd = XEN_DOMCTL_destroydomain;
+ * 89     domctl.domain = (domid_t)domid;
+ * 90     do {
+ * 91         ret = do_domctl(xch, &domctl);
+ * 92     } while ( ret && (errno == EAGAIN) );
+ * 93     return ret;
+ * 94 }
+ *
+ * xc_domain_destroy() traps to hypervisor via XEN_DOMCTL_destroydomain() and
+ * this would persists until the destroy is completed by hypervisor.
+ *
+ * This can be confirmed by oswatcher in sosreport that cpu usage of xend is
+ * always 100% in top command.
+ *
+ * In hypervisor side, XEN_DOMCTL_destroydomain is handled by
+ * do_domctl()->domain_kill():
+ *
+ * domain_kill()
+ *  -> domain_relinquish_resources()
+ *
+ * As shown in below code, if domain_relinquish_resources() cannot return 0 with
+ * all memory pages reclaimed, xend would trap to run again until all pages are
+ * reclaimed. I have verified with mini-os whose memory is only 16MB.
+ *
+ * 637     case DOMDYING_dying:
+ * 638         rc = domain_relinquish_resources(d);
+ * 639         if ( rc != 0 )
+ * 640         {
+ * 641             if ( rc == -ERESTART )
+ * 642                 rc = -EAGAIN;
+ * 643             break;
+ * 644         }
+ *
+ * Most memory pages (indeed regular pages) are reclaimed at relinquish_memory()
+ * at line 2175. Although the argument at 2175 is PGT_l4_page_table, regular
+ * pages are reclaimed as well.
+ *
+ * 2098 int domain_relinquish_resources(struct domain *d)
+ * 2099 {
+ * ... ...
+ * 2174     case RELMEM_l4:
+ * 2175         ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ * 2176         if ( ret )
+ * 2177             return ret;
+ * 2178         d->arch.relmem = RELMEM_l3;
+ * 2179         // fallthrough
+ *
+ * According to my test, line 2040 should return true so that the page type does
+ * not need to be PGT_l4_page_table.
+ *
+ * 1986 static int relinquish_memory(
+ * 1987     struct domain *d, struct page_list_head *list, unsigned long type)
+ * 1988 {
+ * ... ...
+ * 2040             if ( likely((x & PGT_type_mask) != type) ||
+ * 2041                  likely(!(x & (PGT_validated|PGT_partial))) )
+ * 2042                 break;
+ * ... ...
+ * 2079         // Put the page on the list and /then/ potentially free it.
+ * 2080         page_list_add_tail(page, &d->arch.relmem_list);
+ * 2081         put_page(page);
+ *
+ * Until after line 2081, the d->tot_pages would not be decremented.
+ */
+
 /*
  * Comma-separated list of hexadecimal page numbers containing bad bytes.
  * e.g. 'badpage=0x3f45,0x8a321'.
@@ -75,12 +222,30 @@ size_param("bootscrub_chunk", opt_bootscrub_chunk);
  * Bit width of the DMA heap -- used to override NUMA-node-first.
  * allocation strategy, which can otherwise exhaust low memory.
  */
+/*
+ * 使用dma_bitsize的地方:
+ *   - common/page_alloc.c|185| <<global>> integer_param("dma_bits", dma_bitsize);
+ *   - common/page_alloc.c|1683| <<end_boot_allocator>> if ( !dma_bitsize && (num_online_nodes() > 1) )
+ *   - common/page_alloc.c|1686| <<end_boot_allocator>> dma_bitsize = min_t(unsigned int ,
+ *   - common/page_alloc.c|1691| <<end_boot_allocator>> dma_bitsize = 32;
+ *   - common/page_alloc.c|1696| <<end_boot_allocator>> if ( dma_bitsize )
+ *   - common/page_alloc.c|1697| <<end_boot_allocator>> printk(" DMA width %u bits", dma_bitsize);
+ *   - common/page_alloc.c|2329| <<alloc_domheap_pages>> if ( dma_bitsize && ((dma_zone = bits_to_zone(dma_bitsize)) < zone_hi) )
+ *   - common/page_alloc.c|2514| <<pagealloc_info>> if ( (zone + PAGE_SHIFT) == dma_bitsize )
+ */
 static unsigned int dma_bitsize;
 integer_param("dma_bits", dma_bitsize);
 
 #define round_pgdown(_p)  ((_p)&PAGE_MASK)
 #define round_pgup(_p)    (((_p)+(PAGE_SIZE-1))&PAGE_MASK)
 
+/*
+ * 在以下使用pglist_lock:
+ *   - common/page_alloc.c|877| <<reserve_offlined_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|883| <<reserve_offlined_page>> spin_unlock(&pglist_lock);
+ *   - common/page_alloc.c|1210| <<online_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|1241| <<online_page>> spin_unlock(&pglist_lock);
+ */
 static DEFINE_SPINLOCK(pglist_lock);
 /* Offlined page list, protected by pglist_lock. */
 static PAGE_LIST_HEAD(page_offlined_list);
@@ -88,9 +253,35 @@ static PAGE_LIST_HEAD(page_offlined_list);
 static PAGE_LIST_HEAD(page_broken_list);
 
 /* A rough flag to indicate whether a node have need_scrub pages */
+/*
+ * 修改node_need_scrub[]的地方:
+ *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+ *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+ *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+ * 在以下使用node_need_scrub:
+ *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+ *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+ *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+ */
 static unsigned long node_need_scrub[MAX_NUMNODES];
+/*
+ * 在以下使用is_scrubbing:
+ *   - common/page_alloc.c|1673| <<scrub_free_pages>> if ( test_and_set_bool(per_cpu(is_scrubbing, cpumask_first(per_cpu(cpu_sibling_mask, cpu)))) )
+ *   - common/page_alloc.c|1736| <<scrub_free_pages>> was_scrubbing = test_and_clear_bool(per_cpu(is_scrubbing,
+ */
 static DEFINE_PER_CPU(bool_t, is_scrubbing);
+/*
+ * 在以下使用scrub_list_cpu:
+ *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, scrub_list_cpu);
+/*
+ * 在以下使用free_list_cpu:
+ *   - common/page_alloc.c|1612| <<__scrub_free_pages>> struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, free_list_cpu);
 
 /*************************
@@ -120,6 +311,12 @@ static void __init boot_bug(int line)
 }
 #define BOOT_BUG_ON(p) if ( p ) boot_bug(__LINE__);
 
+/*
+ * called by:
+ *   - common/page_alloc.c|203| <<bootmem_region_zap>> bootmem_region_add(e, _e);
+ *   - common/page_alloc.c|224| <<init_boot_pages>> bootmem_region_add(ps >> PAGE_SHIFT, pe >> PAGE_SHIFT);
+ *   - common/page_alloc.c|302| <<alloc_boot_pages>> bootmem_region_add(pg + nr_pfns, _e);
+ */
 static void __init bootmem_region_add(unsigned long s, unsigned long e)
 {
     unsigned int i;
@@ -289,20 +486,131 @@ unsigned long __init alloc_boot_pages(
 
 typedef struct page_list_head heap_by_zone_and_order_t[NR_ZONES][MAX_ORDER+1];
 static heap_by_zone_and_order_t *_heap[MAX_NUMNODES];
+/*
+ * 在以下使用heap():
+ *   - common/page_alloc.c|970| <<alloc_heap_pages>> if ( (pg = page_list_remove_head(&heap(node, zone, j))) )
+ *   - common/page_alloc.c|1023| <<alloc_heap_pages>> page_list_add_tail(pg, &heap(node, zone, j)); 
+ *   - common/page_alloc.c|1119| <<reserve_offlined_page>> page_list_del(head, &heap(node, zone, head_order));
+ *   - common/page_alloc.c|1155| <<reserve_offlined_page>> page_list_add_tail(cur_head, &heap(node, zone, cur_order));
+ *   - common/page_alloc.c|1235| <<merge_free_trunks>> page_list_del(pg, &heap(node, zone, order));
+ *   - common/page_alloc.c|1259| <<merge_free_trunks>> page_list_del(pg + mask, &heap(node, zone, order));
+ *   - common/page_alloc.c|1266| <<merge_free_trunks>> page_list_add_tail(pg, &heap(node, zone, order));
+ *   - common/page_alloc.c|1412| <<reserve_heap_page>> if ( page_list_empty(&heap(node, zone, i)) )
+ *   - common/page_alloc.c|1415| <<reserve_heap_page>> page_list_for_each_safe ( head, tmp, &heap(node, zone, i) )
+ *   - common/page_alloc.c|2111| <<scrub_free_pages>> page_list_for_each_safe( pg, tmp, &heap(node, zone, order) )
+ *   - common/page_alloc.c|2116| <<scrub_free_pages>> page_list_del( pg, &heap(node, zone, order) );
+ *   - common/page_alloc.c|2124| <<scrub_free_pages>> page_list_add_tail(pg, &heap(node, zone, i));
+ *
+ * 这里的*是在_heap[node],和zone/order没关系
+ * 实际就是((*_heap[node])[zone][order]),
+ * 每一个都是一个struct page_list_head
+ */
 #define heap(node, zone, order) ((*_heap[node])[zone][order])
 
+/*
+ * 定义是unsigned long *avail[MAX_NUMNODES]
+ */
 static unsigned long *avail[MAX_NUMNODES];
+/*
+ * 在以下使用total_avail_pages:
+ *   - common/page_alloc.c|560| <<domain_set_outstanding_pages>> avail_pages = total_avail_pages;
+ *   - common/page_alloc.c|757| <<setup_low_mem_virq>> ((paddr_t) total_avail_pages) << PAGE_SHIFT);
+ *   - common/page_alloc.c|765| <<setup_low_mem_virq>> (threshold == (((paddr_t) total_avail_pages) << PAGE_SHIFT)) )
+ *   - common/page_alloc.c|877| <<alloc_heap_pages>> total_avail_pages + tmem_freeable_pages()) &&
+ *   - common/page_alloc.c|892| <<alloc_heap_pages>> (total_avail_pages <= midsize_alloc_zone_pages) &&
+ *   - common/page_alloc.c|900| <<alloc_heap_pages>> total_avail_pages -= request;
+ *   - common/page_alloc.c|903| <<alloc_heap_pages>> avail_pages = total_avail_pages +
+ *   - common/page_alloc.c|972| <<alloc_heap_pages>> total_avail_pages += request;
+ *   - common/page_alloc.c|1133| <<reserve_offlined_page>> total_avail_pages--;
+ *   - common/page_alloc.c|1298| <<free_heap_pages>> total_avail_pages += 1 << order;
+ *   - common/page_alloc.c|1302| <<free_heap_pages>> midsize_alloc_zone_pages, total_avail_pages / MIDSIZE_ALLOC_FRAC);
+ *   - common/page_alloc.c|1664| <<total_free_pages>> ret = total_avail_pages - midsize_alloc_zone_pages;
+ */
 static long total_avail_pages;
 
 /* TMEM: Reserve a fraction of memory for mid-size (0<order<9) allocations.*/
 static long midsize_alloc_zone_pages;
 #define MIDSIZE_ALLOC_FRAC 128
 
+/*
+ * 在以下heap_lock_globals:
+ *   - common/page_alloc.c|578| <<domain_adjust_tot_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|590| <<domain_adjust_tot_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|627| <<domain_set_outstanding_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|681| <<domain_set_outstanding_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|692| <<get_outstanding_claims>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|707| <<get_outstanding_claims>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|964| <<alloc_heap_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|978| <<alloc_heap_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1003| <<alloc_heap_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1059| <<alloc_heap_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1068| <<alloc_heap_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|1070| <<alloc_heap_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1225| <<reserve_offlined_page>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|1231| <<reserve_offlined_page>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1399| <<free_heap_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|1405| <<free_heap_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1774| <<total_free_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|1776| <<total_free_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|1970| <<scrub_heap_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|1972| <<scrub_heap_pages>> spin_unlock(&heap_lock_globals);
+ *   - common/page_alloc.c|2027| <<scrub_heap_pages>> spin_lock(&heap_lock_globals);
+ *   - common/page_alloc.c|2029| <<scrub_heap_pages>> spin_unlock(&heap_lock_globals);
+ */
 static DEFINE_SPINLOCK(heap_lock_globals);
+/*
+ * 在以下使用heap_lock[]:
+ *   - common/page_alloc.c|720| <<get_dirty_pages>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|722| <<get_dirty_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1012| <<alloc_heap_pages>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|1029| <<alloc_heap_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1139| <<alloc_heap_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1170| <<reserve_offlined_page>> ASSERT(spin_is_locked(&heap_lock[node]));
+ *   - common/page_alloc.c|1257| <<merge_free_trunks>> ASSERT(spin_is_locked(&heap_lock[node]));
+ *   - common/page_alloc.c|1357| <<free_heap_pages>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|1424| <<free_heap_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1439| <<mark_page_offline>> ASSERT(spin_is_locked(&heap_lock[phys_to_nid(page_to_maddr(pg))]));
+ *   - common/page_alloc.c|1535| <<offline_page>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|1543| <<offline_page>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1550| <<offline_page>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1614| <<online_page>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|1647| <<online_page>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|1670| <<query_page_offline>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|1679| <<query_page_offline>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|2095| <<__scrub_free_pages>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|2127| <<__scrub_free_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|2179| <<scrub_free_pages>> spin_lock(&heap_lock[node]);
+ *   - common/page_alloc.c|2210| <<scrub_free_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|2214| <<scrub_free_pages>> spin_unlock(&heap_lock[node]);
+ *   - common/page_alloc.c|2734| <<early_heap_init>> spin_lock_init(&heap_lock[i]);
+ */
 static spinlock_t heap_lock[MAX_NUMNODES];
 
+/*
+ * 在以下使用outstanding_claims:
+ *   - common/page_alloc.c|380| <<domain_adjust_tot_pages>> sys_before = outstanding_claims;
+ *   - common/page_alloc.c|383| <<domain_adjust_tot_pages>> outstanding_claims = sys_after;
+ *   - common/page_alloc.c|406| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+ *   - common/page_alloc.c|439| <<domain_set_outstanding_pages>> avail_pages -= outstanding_claims;
+ *   - common/page_alloc.c|451| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+ *   - common/page_alloc.c|463| <<get_outstanding_claims>> *outstanding_pages = outstanding_claims;
+ *   - common/page_alloc.c|709| <<alloc_heap_pages>> if ( (outstanding_claims + request >
+ *   - common/page_alloc.c|737| <<alloc_heap_pages>> (opt_tmem ? tmem_freeable_pages() : 0) - outstanding_claims;
+ */
 static long outstanding_claims; /* total outstanding claims by all domains */
 
+/*
+ * called by:
+ *   - arch/x86/mm.c|4419| <<donate_page>> domain_adjust_tot_pages(d, 1);
+ *   - arch/x86/mm.c|4483| <<steal_page>> if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )
+ *   - arch/x86/mm/mem_sharing.c|659| <<page_make_sharable>> drop_dom_ref = !domain_adjust_tot_pages(d, -1);
+ *   - arch/x86/mm/mem_sharing.c|706| <<page_make_private>> if ( domain_adjust_tot_pages(d, 1) == 1 )
+ *   - common/grant_table.c|1995| <<gnttab_transfer>> if ( unlikely(domain_adjust_tot_pages(e, 1) == 1) )
+ *   - common/grant_table.c|2010| <<gnttab_transfer>> bool_t drop_dom_ref = !domain_adjust_tot_pages(e, -1);
+ *   - common/memory.c|636| <<memory_exchange>> !domain_adjust_tot_pages(d, -dec_count));
+ *   - common/page_alloc.c|1967| <<assign_pages>> domain_adjust_tot_pages(d, 1 << order);
+ *   - common/page_alloc.c|2067| <<free_domheap_pages>> drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
+ */
 unsigned long domain_adjust_tot_pages(struct domain *d, long pages)
 {
     long dom_before, dom_after, dom_claimed, sys_before, sys_after;
@@ -336,11 +644,31 @@ out:
     return d->tot_pages;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> domain_set_outstanding_pages(d, 0);
+ *   - common/memory.c|1122| <<do_mem_op(XENMEM_claim_pages)>> rc = domain_set_outstanding_pages(d, reservation.nr_extents);
+ */
 int domain_set_outstanding_pages(struct domain *d, unsigned long pages)
 {
     int ret = -ENOMEM;
     unsigned long claim, avail_pages;
 
+    /*
+     * 在以下修改d->outstanding_pages:
+     *   - common/page_alloc.c|495| <<domain_adjust_tot_pages>> d->outstanding_pages = dom_claimed;
+     *   - common/page_alloc.c|529| <<domain_set_outstanding_pages>> d->outstanding_pages = 0;
+     *   - common/page_alloc.c|572| <<domain_set_outstanding_pages>> d->outstanding_pages = claim;
+     *
+     * 在以下使用d->outstanding_pages:
+     *   - common/domctl.c|209| <<getdomaininfo>> info->outstanding_pages = d->outstanding_pages;
+     *   - common/page_alloc.c|486| <<domain_adjust_tot_pages>> if ( !d->outstanding_pages )
+     *   - common/page_alloc.c|491| <<domain_adjust_tot_pages>> dom_before = d->outstanding_pages;
+     *   - common/page_alloc.c|528| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+     *   - common/page_alloc.c|535| <<domain_set_outstanding_pages>> if ( d->outstanding_pages )
+     *   - common/page_alloc.c|573| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+     *   - common/page_alloc.c|868| <<alloc_heap_pages>> !d || d->outstanding_pages < request) )
+     */
     /*
      * take the domain's page_alloc_lock, else all d->tot_page adjustments
      * must always take the global heap_lock rather than only in the much
@@ -406,14 +734,36 @@ out:
     return ret;
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|268| <<do_sysctl(XEN_SYSCTL_physinfo)>> get_outstanding_claims(&pi->free_pages, &pi->outstanding_pages);
+ */
 void get_outstanding_claims(uint64_t *free_pages, uint64_t *outstanding_pages)
 {
     spin_lock(&heap_lock_globals);
+    /*
+     * 在以下使用outstanding_claims:
+     *   - common/page_alloc.c|380| <<domain_adjust_tot_pages>> sys_before = outstanding_claims;
+     *   - common/page_alloc.c|383| <<domain_adjust_tot_pages>> outstanding_claims = sys_after;
+     *   - common/page_alloc.c|406| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+     *   - common/page_alloc.c|439| <<domain_set_outstanding_pages>> avail_pages -= outstanding_claims;
+     *   - common/page_alloc.c|451| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+     *   - common/page_alloc.c|463| <<get_outstanding_claims>> *outstanding_pages = outstanding_claims;
+     *   - common/page_alloc.c|709| <<alloc_heap_pages>> if ( (outstanding_claims + request >
+     *   - common/page_alloc.c|737| <<alloc_heap_pages>> (opt_tmem ? tmem_freeable_pages() : 0) - outstanding_claims;
+     */
     *outstanding_pages = outstanding_claims;
+    /* 把从MEMZONE_XEN + 1到NR_ZONES - 1的每个node的page全加起来 */
     *free_pages =  avail_domheap_pages();
     spin_unlock(&heap_lock_globals);
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|306| <<do_sysctl(XEN_SYSCTL_numainfo)>> get_dirty_pages(i) << PAGE_SHIFT : 0ul;
+ *
+ * 返回对应参数node的node_need_scrub[node]
+ */
 unsigned long get_dirty_pages(unsigned int node)
 {
      unsigned long dirty_pages;
@@ -432,6 +782,10 @@ static unsigned int __read_mostly xenheap_bits;
 #define xenheap_bits 0
 #endif
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1551| <<init_heap_pages>> n = init_node_heap(nid, page_to_mfn(pg+i), nr_pages - i,
+ */
 static unsigned long init_node_heap(int node, unsigned long mfn,
                                     unsigned long nr, bool_t *use_tail)
 {
@@ -519,6 +873,10 @@ static unsigned long low_mem_virq_orig      = 0;
 static unsigned int  low_mem_virq_th_order  = 0;
 
 /* Perform bootstrapping checks and set bounds */
+/*
+ * called by:
+ *   - common/page_alloc.c|1873| <<scrub_heap_pages>> setup_low_mem_virq();
+ */
 static void __init setup_low_mem_virq(void)
 {
     unsigned int order;
@@ -570,6 +928,10 @@ static void __init setup_low_mem_virq(void)
             low_mem_virq_th);
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|943| <<alloc_heap_pages>> check_low_mem_virq(avail_pages);
+ */
 static void check_low_mem_virq(unsigned long avail_pages)
 {
     if ( unlikely(avail_pages <= low_mem_virq_th) )
@@ -602,6 +964,12 @@ static void check_low_mem_virq(unsigned long avail_pages)
 }
 
 /* Allocate 2^@order contiguous pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1812| <<alloc_xenheap_pages>> pg = alloc_heap_pages(MEMZONE_XEN, MEMZONE_XEN,
+ *   - common/page_alloc.c|1988| <<alloc_domheap_pages>> pg = alloc_heap_pages(dma_zone + 1, zone_hi, order, memflags, d);
+ *   - common/page_alloc.c|1992| <<alloc_domheap_pages>> ((pg = alloc_heap_pages(MEMZONE_XEN + 1, zone_hi, order,
+ */
 static struct page_info *alloc_heap_pages(
     unsigned int zone_lo, unsigned int zone_hi,
     unsigned int order, unsigned int memflags,
@@ -621,6 +989,7 @@ static struct page_info *alloc_heap_pages(
         memflags &= ~MEMF_exact_node;
         if ( d != NULL )
         {
+            /* node是unsigned int */
             node = next_node(d->last_alloc_node, nodemask);
             if ( node >= MAX_NUMNODES )
                 node = first_node(nodemask);
@@ -649,6 +1018,9 @@ static struct page_info *alloc_heap_pages(
      * Claimed memory is considered unavailable unless the request
      * is made by a domain with sufficient unclaimed pages.
      */
+    /*
+     * request是1UL << order.
+     */
     if ( (outstanding_claims + request >
           total_avail_pages + tmem_freeable_pages()) &&
           ((memflags & MEMF_no_refcount) ||
@@ -697,6 +1069,9 @@ static struct page_info *alloc_heap_pages(
                 continue;
 
             /* Find smallest order which can satisfy the request. */
+	    /*
+	     * 感觉就是在这里从对应的heap移除的!!!
+	     */
             for ( j = order; j <= MAX_ORDER; j++ )
                 if ( (pg = page_list_remove_head(&heap(node, zone, j))) )
                     goto found;
@@ -765,6 +1140,17 @@ static struct page_info *alloc_heap_pages(
 
     for ( i = 0; i < (1 << order); i++ )
     {
+        /*
+	 * 在以下使用_PGC_need_scrub:
+	 *   - common/page_alloc.c|947| <<alloc_heap_pages>> if ( test_and_clear_bit(_PGC_need_scrub, &pg[i].count_info) )
+	 *   - common/page_alloc.c|1093| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+	 *   - common/page_alloc.c|1098| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+	 *   - common/page_alloc.c|1114| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+	 *   - common/page_alloc.c|1119| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+	 *   - common/page_alloc.c|1829| <<__scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+	 *   - common/page_alloc.c|1900| <<scrub_free_pages>> if ( !test_bit(_PGC_need_scrub, &(pg->count_info)) )
+	 *   - common/page_alloc.c|1919| <<scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+	 */
         if ( test_and_clear_bit(_PGC_need_scrub, &pg[i].count_info) )
             need_scrub = 1;
 
@@ -786,6 +1172,18 @@ static struct page_info *alloc_heap_pages(
         flush_page_to_ram(page_to_mfn(&pg[i]));
     }
 
+    /*
+     * 修改node_need_scrub[]的地方:
+     *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+     *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+     *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+     * 在以下使用node_need_scrub:
+     *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+     *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+     *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+     *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+     *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+     */
     if ( need_scrub )
         node_need_scrub[node] -= (1 << order);
 
@@ -793,6 +1191,10 @@ static struct page_info *alloc_heap_pages(
 
     if ( need_scrub )
     {
+        /*
+	 * scrub_one_page():
+	 * 把page给scrub了, 不改变任何flag
+	 */
         for ( i = 0; i < (1 << order); i++ )
             scrub_one_page(&pg[i]);
     }
@@ -804,6 +1206,11 @@ static struct page_info *alloc_heap_pages(
 }
 
 /* Remove any offlined page in the buddy pointed to by head. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1253| <<free_heap_pages>> reserve_offlined_page(pg);
+ *   - common/page_alloc.c|1310| <<reserve_heap_page>> return reserve_offlined_page(head);
+ */
 static int reserve_offlined_page(struct page_info *head)
 {
     unsigned int node = phys_to_nid(page_to_maddr(head));
@@ -888,6 +1295,11 @@ static int reserve_offlined_page(struct page_info *head)
     return count;
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1205| <<free_heap_pages>> merge_free_trunks(pg, order, node, zone, need_scrub);
+ *   - common/page_alloc.c|1850| <<__scrub_free_pages>> merge_free_trunks(pg, order, node, page_to_zone(pg), 0);
+ */
 static void merge_free_trunks(struct page_info *pg, unsigned int order,
     unsigned int node, unsigned int zone, bool_t need_scrub)
 {
@@ -898,6 +1310,9 @@ static void merge_free_trunks(struct page_info *pg, unsigned int order,
     /* Merge chunks as far as possible. */
     while ( order < MAX_ORDER )
     {
+        /*
+	 * 在while的最后会增加order
+	 */
         mask = 1UL << order;
 
         if ( (page_to_mfn(pg) & mask) )
@@ -919,6 +1334,9 @@ static void merge_free_trunks(struct page_info *pg, unsigned int order,
                 if ( test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
                     break;
             }
+            /*
+	     * 这一行很重要!!!
+	     */
             pg -= mask;
             page_list_del(pg, &heap(node, zone, order));
         }
@@ -940,6 +1358,10 @@ static void merge_free_trunks(struct page_info *pg, unsigned int order,
                 if ( test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
                     break;
             }
+            /*
+	     * heap(): ((*_heap[node])[zone][order])
+	     * 每一个order有一个page_list_head
+	     */
             page_list_del(pg + mask, &heap(node, zone, order));
         }
 
@@ -951,6 +1373,28 @@ static void merge_free_trunks(struct page_info *pg, unsigned int order,
 }
 
 /* Free 2^@order set of pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1295| <<online_page>> free_heap_pages(pg, 0, 0);
+ *   - common/page_alloc.c|1366| <<init_heap_pages>> free_heap_pages(pg+i, 0, 0);
+ *   - common/page_alloc.c|1863| <<free_xenheap_pages>> free_heap_pages(virt_to_page(v), order, 0);
+ *   - common/page_alloc.c|1921| <<free_xenheap_pages>> free_heap_pages(pg, order, 1);
+ *   - common/page_alloc.c|2030| <<alloc_domheap_pages>> free_heap_pages(pg, order, 0);
+ *   - common/page_alloc.c|2089| <<free_domheap_pages>> free_heap_pages(pg, order, 1);
+ *   - common/page_alloc.c|2091| <<free_domheap_pages>> free_heap_pages(pg, order, 0);
+ *   - common/page_alloc.c|2096| <<free_domheap_pages>> free_heap_pages(pg, 0, 1);
+ *   - common/page_alloc.c|2102| <<free_domheap_pages>> free_heap_pages(pg, order, 1);
+ *
+ * 关于参数的need_scrub什么时候是0,什么时候是1...
+ * Normally we expect a domain to clear pages before freeing them, if
+ * it cares about the secrecy of their contents. However, after a
+ * domain has died we assume responsibility for erasure.
+ *
+ * put_page()
+ *  -> free_domheap_page()
+ *      -> free_domheap_pages()
+ *          -> free_heap_pages()
+ */
 static void free_heap_pages(
     struct page_info *pg, unsigned int order, bool_t need_scrub)
 {
@@ -977,6 +1421,13 @@ static void free_heap_pages(
          * In all the above cases there can be no guest mappings of this page.
          */
         ASSERT(!page_state_is(&pg[i], offlined));
+	/*
+	 * 设置pg[i].count_info为:
+	 *  - (pg[i].count_info & PGC_broken) |
+	 *  - (page_state_is(&pg[i], offlining) ? PGC_state_offlined : PGC_state_free)
+	 *
+	 * 就是说这里有可能设置成PGC_state_free
+	 */
         pg[i].count_info =
             ((pg[i].count_info & PGC_broken) |
              (page_state_is(&pg[i], offlining)
@@ -1011,6 +1462,11 @@ static void free_heap_pages(
         node_need_scrub[node] += (1 << order);
     }
 
+    /*
+     * called by:
+     *   - common/page_alloc.c|1205| <<free_heap_pages>> merge_free_trunks(pg, order, node, zone, need_scrub);
+     *   - common/page_alloc.c|1850| <<__scrub_free_pages>> merge_free_trunks(pg, order, node, page_to_zone(pg), 0);
+     */
     merge_free_trunks(pg, order, node, zone, need_scrub);
 
     if ( tainted )
@@ -1079,6 +1535,10 @@ static int reserve_heap_page(struct page_info *pg)
 
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|218| <<do_sysctl(XEN_SYSCTL_page_offline_op)>> ret = offline_page(pfn, 0, ptr++);
+ */
 int offline_page(unsigned long mfn, int broken, uint32_t *status)
 {
     unsigned long old_info = 0;
@@ -1282,6 +1742,14 @@ int query_page_offline(unsigned long mfn, uint32_t *status)
  * latter is not on a MAX_ORDER boundary, then we reserve the page by
  * not freeing it to the buddy allocator.
  */
+/*
+ * called by:
+ *   - common/page_alloc.c|1737| <<end_boot_allocator>> init_heap_pages(mfn_to_page(r->s), r->e - r->s);
+ *   - common/page_alloc.c|1746| <<end_boot_allocator>> init_heap_pages(mfn_to_page(r->s), r->e - r->s);
+ *   - common/page_alloc.c|1748| <<end_boot_allocator>> init_heap_pages(virt_to_page(bootmem_region_list), 1);
+ *   - common/page_alloc.c|2201| <<init_xenheap_pages>> init_heap_pages(maddr_to_page(ps), (pe - ps) >> PAGE_SHIFT);
+ *   - common/page_alloc.c|2312| <<init_domheap_pages>> init_heap_pages(mfn_to_page(smfn), emfn - smfn);
+ */
 static void init_heap_pages(
     struct page_info *pg, unsigned long nr_pages)
 {
@@ -1313,10 +1781,22 @@ static void init_heap_pages(
             nr_pages -= n;
         }
 
+	/* 这才是核心调用,初始化相当于free */
         free_heap_pages(pg+i, 0, 0);
     }
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|2357| <<avail_domheap_pages_region>> return avail_heap_pages(zone_lo, zone_hi, node);
+ *   - common/page_alloc.c|2362| <<avail_domheap_pages>> return avail_heap_pages(MEMZONE_XEN + 1,
+ *   - common/page_alloc.c|2369| <<avail_node_heap_pages>> return avail_heap_pages(MEMZONE_XEN, NR_ZONES -1, nodeid);
+ *   - common/page_alloc.c|2380| <<pagealloc_info>> avail_heap_pages(zone, zone, -1) << (PAGE_SHIFT-10));
+ *   - common/page_alloc.c|2390| <<pagealloc_info>> if ( (n = avail_heap_pages(zone, zone, -1)) != 0 )
+ *
+ * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+ * 如果node是-1就把所有node的加起来
+ */
 static unsigned long avail_heap_pages(
     unsigned int zone_lo, unsigned int zone_hi, unsigned int node)
 {
@@ -1338,6 +1818,10 @@ static unsigned long avail_heap_pages(
     return free_pages;
 }
 
+/*
+ * called by:
+ *   - common/tmem.c|1425| <<tmem_ensure_avail_pages>> free_mem = (tmem_page_list_pages + total_free_pages())
+ */
 unsigned long total_free_pages(void)
 {
     long ret;
@@ -1391,6 +1875,15 @@ void __init end_boot_allocator(void)
     printk("\n");
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1561| <<scrub_heap_pages>> on_selected_cpus(&all_worker_cpus, smp_scrub_heap_pages, NULL, 1);
+ *   - common/page_alloc.c|1618| <<scrub_heap_pages>> on_selected_cpus(&node_cpus, smp_scrub_heap_pages, &region[i], 1);
+ *
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ */
 static void __init smp_scrub_heap_pages(void *data)
 {
     unsigned long mfn, start, end;
@@ -1464,6 +1957,10 @@ int __init find_non_smt(unsigned int node, cpumask_t *dest)
  * Scrub all unallocated pages in all heap zones. This function uses all
  * online cpu's to scrub the memory in parallel.
  */
+/*
+ * x86下的调用:
+ *   - arch/x86/setup.c|1545| <<__start_xen>> scrub_heap_pages();
+ */
 void __init scrub_heap_pages(void)
 {
     cpumask_t node_cpus, all_worker_cpus;
@@ -1599,12 +2096,34 @@ void __init scrub_heap_pages(void)
 
 #define SCRUB_BATCH_ORDER 12
 /* return 1 if should continue */
+/*
+ * idle_loop()
+ *  -> scrub_free_pages()
+ *      -> __scrub_free_pages()
+ *
+ * called by:
+ *   - common/page_alloc.c|1726| <<scrub_free_pages>> } while ( __scrub_free_pages(node, cpu) );
+ *
+ * __scrub_free_pages()返回0的时候,
+ * 有可能this_cpu(scrub_list_cpu)和this_cpu(free_list_cpu)都不为empty
+ * __scrub_free_pages()返回1的时候,
+ * this_cpu(scrub_list_cpu)和this_cpu(free_list_cpu)为empty
+ */
 static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 {
     struct page_info *pg, *tmp;
     unsigned int i;
     int order;
+    /*
+     * 在以下使用scrub_list_cpu:
+     *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     */
     struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+    /*
+     * 在以下使用free_list_cpu:
+     *   - common/page_alloc.c|1612| <<__scrub_free_pages>> struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
+     */
     struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
 
     if ( page_list_empty(local_scrub_list) && page_list_empty(local_free_list) )
@@ -1619,9 +2138,14 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
         for ( i = 0; i < (1 << order); i++ )
         {
             ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+	    /* 把page给scrub了, 不改变任何flag */
             scrub_one_page(&pg[i]);
         }
         page_list_add_tail(pg, local_free_list);
+        /*
+	 * 如果有挂载的softirq, 就先退出,
+	 * 这样local_free_list和local_scrub_list都有可能有没处理的page
+	 */
         if ( softirq_pending(cpu) )
             return 0;
     }
@@ -1636,10 +2160,29 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
             page_list_del(pg, local_free_list);
             for ( i = 0; i < (1 << order); i++ )
             {
+                /*
+		 * 在以下使用PGC_state_free:
+		 *   - common/page_alloc.c|962| <<alloc_heap_pages>> BUG_ON(pg[i].count_info != PGC_state_free);
+		 *   - common/page_alloc.c|1202| <<free_heap_pages>> ? PGC_state_offlined : PGC_state_free));
+		 *   - common/page_alloc.c|1262| <<mark_page_offline>> nx |= (((x & PGC_state) == PGC_state_free)
+		 *   - common/page_alloc.c|1892| <<__scrub_free_pages>> pg[i].count_info |= PGC_state_free;
+		 */
                 pg[i].count_info |= PGC_state_free;
                 pg[i].count_info &= ~PGC_need_scrub;
             }
             merge_free_trunks(pg, order, node, page_to_zone(pg), 0);
+	    /*
+	     * 修改node_need_scrub[]的地方:
+	     *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+	     *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+	     *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+	     * 在以下使用node_need_scrub:
+	     *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+	     *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+	     *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+	     *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+	     *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+	     */
             node_need_scrub[node] -= (1 << order);
         }
         spin_unlock(&heap_lock[node]);
@@ -1648,20 +2191,49 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 }
 
 /* return 1 if should continue */
+/*
+ * called by:
+ *   - arch/x86/domain.c|128| <<idle_loop>> if ( !scrub_free_pages() )
+ */
 int scrub_free_pages(void)
 {
     int order;
     struct page_info *pg, *tmp;
     unsigned int i, zone, nr_delisted = 0;
+    /*
+     * 每一个cpu都有一个idle_loop()
+     */
     unsigned int cpu = smp_processor_id();
     unsigned int node = cpu_to_node(cpu);
+    /*
+     * 在以下使用scrub_list_cpu:
+     *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+     */
     struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
     bool_t was_scrubbing;
     static unsigned node_is_scrubbed = 0;
 
+    /*
+     * node_is_scrubbed在这个函数定义为static:
+     *   - common/page_alloc.c|2211| <<scrub_free_pages>> if ( test_and_set_bit(node, &node_is_scrubbed) )
+     *   - common/page_alloc.c|2220| <<scrub_free_pages>> clear_bit(node, &node_is_scrubbed);
+     *   - common/page_alloc.c|2287| <<scrub_free_pages>> clear_bit(node, &node_is_scrubbed);
+     *
+     * Set a bit and return its old value
+     *
+     * 这里是为了保证同一个时刻一个node只有一个cpu能进入下面
+     */
     if ( test_and_set_bit(node, &node_is_scrubbed) )
         return 0;
 
+    /*
+     * 在以下使用is_scrubbing:
+     *   - common/page_alloc.c|1673| <<scrub_free_pages>> if ( test_and_set_bool(per_cpu(is_scrubbing, cpumask_first(per_cpu(cpu_sibling_mask, cpu)))) )
+     *   - common/page_alloc.c|1736| <<scrub_free_pages>> was_scrubbing = test_and_clear_bool(per_cpu(is_scrubbing,
+     *
+     * One of current cpu's siblings already run
+     */
     if ( test_and_set_bool(per_cpu(is_scrubbing, cpumask_first(per_cpu(cpu_sibling_mask, cpu)))) )
     {
         /*
@@ -1675,6 +2247,18 @@ int scrub_free_pages(void)
 
     do
     {
+        /*
+	 * 修改node_need_scrub[]的地方:
+	 *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+	 *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+	 *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+	 *
+	 * 只有真正在this_cpu(free_list_cpu)取出并都scrub了的时候, node_need_scrub[node]才减少
+	 *
+	 * local_scrub_list来自&this_cpu(scrub_list_cpu)
+	 *
+	 * 下面的这一大段if语句就是遍历heap, 把dirty page放入this_cpu(scrub_list_cpu)
+	 */
         if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
         {
             /* Delist a batch of pages from global scrub list */
@@ -1683,6 +2267,11 @@ int scrub_free_pages(void)
                 for ( order = MAX_ORDER; order >= 0; order-- )
                 {
                     spin_lock(&heap_lock[node]);
+		    /*
+		     * 关于heap, 这里的*是在_heap[node],和zone/order没关系
+		     * 实际就是((*_heap[node])[zone][order]),
+		     * 每一个都是一个struct page_list_head
+		     */
                     page_list_for_each_safe( pg, tmp, &heap(node, zone, order) )
                     {
                         if ( !test_bit(_PGC_need_scrub, &(pg->count_info)) )
@@ -1696,18 +2285,25 @@ int scrub_free_pages(void)
                             while ( i != SCRUB_BATCH_ORDER )
                             {
                                 PFN_ORDER(pg) = --i;
+				/*
+				 * 最后一个参数i是order
+				 */
                                 page_list_add_tail(pg, &heap(node, zone, i));
                                 pg += 1 << i;
                             }
                             PFN_ORDER(pg) = SCRUB_BATCH_ORDER;
                         }
 
+			/* #define PFN_ORDER(_pfn) ((_pfn)->v.free.order) */
                         for ( i = 0; i < (1 << PFN_ORDER(pg)); i++ )
                         {
                             ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
                             ASSERT( !test_bit(_PGC_broken, &pg[i].count_info) );
                             mark_page_offline(&pg[i], 0);
                         }
+                        /*
+			 * 非常核心的一行!!!
+			 */
                         page_list_add_tail(pg, local_scrub_list);
                         nr_delisted += ( 1 << PFN_ORDER(pg) );
                         if ( nr_delisted >= (1 << SCRUB_BATCH_ORDER) )
@@ -1723,13 +2319,28 @@ int scrub_free_pages(void)
         }
  start_scrub:
 	; /* To shut up the compiler */
+	/*
+	 * __scrub_free_pages()返回0的时候,
+	 * 有可能this_cpu(scrub_list_cpu)和this_cpu(free_list_cpu)都不为empty
+	 * __scrub_free_pages()返回1的时候,
+	 * this_cpu(scrub_list_cpu)和this_cpu(free_list_cpu)为empty
+	 */
     } while ( __scrub_free_pages(node, cpu) );
 
     was_scrubbing = test_and_clear_bool(per_cpu(is_scrubbing,
                         cpumask_first(per_cpu(cpu_sibling_mask, cpu))));
     ASSERT(was_scrubbing);
 
+    /*
+     * node_is_scrubbed在这个函数定义为static:
+     *   - common/page_alloc.c|2211| <<scrub_free_pages>> if ( test_and_set_bit(node, &node_is_scrubbed) )
+     *   - common/page_alloc.c|2220| <<scrub_free_pages>> clear_bit(node, &node_is_scrubbed);
+     *   - common/page_alloc.c|2287| <<scrub_free_pages>> clear_bit(node, &node_is_scrubbed);
+     */
     clear_bit(node, &node_is_scrubbed);
+    /*
+     * 只有真正在this_cpu(free_list_cpu)取出并都scrub了的时候, node_need_scrub[node]才减少
+     */
     if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
         return 0;
     return 1;
@@ -1780,6 +2391,33 @@ void *alloc_xenheap_pages(unsigned int order, unsigned int memflags)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/bzimage.c|144| <<perform_gunzip>> free_xenheap_pages((void *)free_mem_ptr, HEAPORDER);
+ *   - arch/x86/hvm/nestedhvm.c|177| <<nestedhvm_setup>> free_xenheap_pages(shadow_io_bitmap[i], order);
+ *   - arch/x86/hvm/svm/nestedsvm.c|134| <<nsvm_vcpu_destroy>> free_xenheap_pages(svm->ns_cached_msrpm,
+ *   - arch/x86/hvm/svm/nestedsvm.c|139| <<nsvm_vcpu_destroy>> free_xenheap_pages(svm->ns_merged_msrpm,
+ *   - arch/x86/hvm/svm/vmcb.c|271| <<svm_destroy_vmcb>> free_xenheap_pages(
+ *   - arch/x86/mm/mem_sharing.c|89| <<page_sharing_dispose>> free_xenheap_pages(page->sharing->hash_table.bucket,
+ *   - arch/x86/mm/mem_sharing.c|111| <<page_sharing_dispose>> free_xenheap_pages(page->sharing->hash_table.bucket,
+ *   - arch/x86/mm/mem_sharing.c|239| <<rmap_hash_table_to_list>> free_xenheap_pages(bucket, RMAP_HASHTAB_ORDER);
+ *   - arch/x86/percpu.c|49| <<_free_percpu_area>> free_xenheap_pages(p, PERCPU_ORDER);
+ *   - arch/x86/smpboot.c|658| <<cpu_smpboot_free>> free_xenheap_pages(per_cpu(gdt_table, cpu), order);
+ *   - arch/x86/smpboot.c|660| <<cpu_smpboot_free>> free_xenheap_pages(per_cpu(compat_gdt_table, cpu), order);
+ *   - arch/x86/smpboot.c|663| <<cpu_smpboot_free>> free_xenheap_pages(idt_tables[cpu], order);
+ *   - arch/x86/smpboot.c|669| <<cpu_smpboot_free>> free_xenheap_pages(stack_base[cpu], STACK_ORDER);
+ *   - common/tmem_xen.c|264| <<cpu_callback>> free_xenheap_pages(per_cpu(dstmem, cpu), dstmem_order);
+ *   - common/tmem_xen.c|269| <<cpu_callback>> free_xenheap_pages(per_cpu(workmem, cpu), workmem_order);
+ *   - common/trace.c|279| <<alloc_trace_bufs>> free_xenheap_pages(mfn_to_virt(mfn), 0);
+ *   - common/trace.c|282| <<alloc_trace_bufs>> free_xenheap_pages(t_info, get_order_from_pages(t_info_pages));
+ *   - common/xenoprof.c|287| <<free_xenoprof_pages>> free_xenheap_pages(x->rawbuf, order);
+ *   - common/xmalloc_tlsf.c|375| <<xmem_pool_destroy>> free_xenheap_pages(pool,pool_order);
+ *   - common/xmalloc_tlsf.c|542| <<xmalloc_whole_pages>> free_xenheap_pages(p, i);
+ *   - common/xmalloc_tlsf.c|637| <<xfree>> free_xenheap_pages(p + (size << PAGE_SHIFT), i);
+ *   - drivers/char/console.c|311| <<dump_console_ring_key>> free_xenheap_pages(buf, order);
+ *   - include/asm-x86/hvm/svm/amd-iommu-proto.h|190| <<__free_amd_iommu_tables>> free_xenheap_pages(table, order);
+ *   - include/xen/mm.h|80| <<free_xenheap_page>> #define free_xenheap_page(v) (free_xenheap_pages(v,0))
+ */
 void free_xenheap_pages(void *v, unsigned int order)
 {
     ASSERT(!in_irq());
@@ -1874,6 +2512,13 @@ void init_domheap_pages(paddr_t ps, paddr_t pe)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/domain_build.c|674| <<construct_dom0>> if ( assign_pages(d, mfn_to_page(mfn++), 0, 0) )
+ *   - common/memory.c|617| <<memory_exchaeng>> if ( assign_pages(d, page, exch.out.extent_order,
+ *   - common/memory.c|682| <<memory_exchange>> if ( assign_pages(d, page, 0, MEMF_no_refcount) )
+ *   - common/page_alloc.c|2169| <<alloc_domheap_pages>> assign_pages(d, pg, order, memflags) )
+ */
 int assign_pages(
     struct domain *d,
     struct page_info *pg,
@@ -1915,6 +2560,9 @@ int assign_pages(
         page_set_owner(&pg[i], d);
         smp_wmb(); /* Domain pointer must be visible before updating refcnt. */
         pg[i].count_info = PGC_allocated | 1;
+        /*
+	 * 一个一个page的往d->page_list加
+	 */
         page_list_add_tail(&pg[i], &d->page_list);
     }
 
@@ -1927,6 +2575,27 @@ int assign_pages(
 }
 
 
+/*
+ * x86在以下调用:
+ *   - rch/x86/domain_build.c|259| <<alloc_chunk>> while ( (page = alloc_domheap_pages(d, order, memflags)) == NULL )
+ *   - arch/x86/domain_build.c|285| <<alloc_chunk>> pg2 = alloc_domheap_pages(d, order, 0);
+ *   - arch/x86/domain_build.c|638| <<construct_dom0>> page = alloc_domheap_pages(d, order, 0);
+ *   - arch/x86/domain_build.c|655| <<construct_dom0>> page = alloc_domheap_pages(d, order, 0); 
+ *   - arch/x86/domain_build.c|995| <<construct_dom0>> (page = alloc_domheap_pages(d,
+ *   - arch/x86/domain_build.c|1022| <<construct_dom0>> (page = alloc_domheap_pages(d,
+ *   - arch/x86/mm/p2m-pod.c|227| <<p2m_pod_set_cache_target>> page = alloc_domheap_pages(d, order, PAGE_ORDER_4K);
+ *   - arch/x86/x86_64/mm.c|634| <<paging_init>> (l1_pg = alloc_domheap_pages(NULL, 2 * PAGETABLE_ORDER,
+ *   - arch/x86/x86_64/mm.c|660| <<paging_init>> else if ( (l1_pg = alloc_domheap_pages(NULL, PAGETABLE_ORDER,
+ *   - arch/x86/x86_64/mm.c|726| <<paging_init>> if ( (l1_pg = alloc_domheap_pages(NULL, PAGETABLE_ORDER,
+ *   - common/memory.c|118| <<increase_reservation>> page = alloc_domheap_pages(d, a->extent_order, a->memflags);
+ *   - common/memory.c|234| <<populate_physmap>> page = alloc_domheap_pages(d, a->extent_order, a->memflags);
+ *   - common/memory.c|602| <<XEN_GUEST_HANDLE_PARAM>> page = alloc_domheap_pages(d, exch.out.extent_order,
+ *   - common/page_alloc.c|2195| <<alloc_xenheap_pages>> pg = alloc_domheap_pages(NULL, order, memflags);
+ *   - drivers/passthrough/vtd/iommu.c|200| <<alloc_pgtable_maddr>> pg = alloc_domheap_pages(NULL, get_order_from_pages(npages),
+ *   - include/xen/mm.h|106| <<alloc_domheap_page>> #define alloc_domheap_page(d,f) (alloc_domheap_pages(d,0,f))
+ *   - include/xen/tmem_xen.h|122| <<__tmem_alloc_page_thispool>> pi = alloc_domheap_pages(d,0,MEMF_tmem);
+ *   - include/xen/tmem_xen.h|152| <<__tmem_alloc_page>> pi = alloc_domheap_pages(0,0,MEMF_tmem);
+ */
 struct page_info *alloc_domheap_pages(
     struct domain *d, unsigned int order, unsigned int memflags)
 {
@@ -1963,6 +2632,21 @@ struct page_info *alloc_domheap_pages(
     return pg;
 }
 
+/*
+ * x86在以下使用:
+ *   - arch/x86/domain_build.c|288| <<alloc_chunk>> free_domheap_pages(page, free_order);
+ *   - arch/x86/domain_build.c|293| <<alloc_chunk>> free_domheap_pages(pg2, order);
+ *   - arch/x86/domain_build.c|661| <<construct_dom0>> free_domheap_pages(page, order);
+ *   - common/memory.c|642| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - common/memory.c|693| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - include/xen/mm.h|107| <<free_domheap_page>> #define free_domheap_page(p) (free_domheap_pages(p,0))
+ *   - include/xen/tmem_xen.h|140| <<__tmem_free_page_thispool>> free_domheap_pages(pi,0);
+ *
+ * 一个例子
+ * put_page()
+ *  -> free_domheap_page()
+ *      -> free_domheap_pages()
+ */
 void free_domheap_pages(struct page_info *pg, unsigned int order)
 {
     struct domain *d = page_get_owner(pg);
@@ -1971,6 +2655,9 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
 
     ASSERT(!in_irq());
 
+    /*
+     * 判断((page)->count_info & PGC_xen_heap)
+     */
     if ( unlikely(is_xen_heap_page(pg)) )
     {
         /* NB. May recursively lock from relinquish_memory(). */
@@ -1995,6 +2682,18 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
             page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
         }
 
+	/*
+	 * called by:
+	 *   - arch/x86/mm.c|4419| <<donate_page>> domain_adjust_tot_pages(d, 1);
+	 *   - arch/x86/mm.c|4483| <<steal_page>> if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )
+	 *   - arch/x86/mm/mem_sharing.c|659| <<page_make_sharable>> drop_dom_ref = !domain_adjust_tot_pages(d, -1);
+	 *   - arch/x86/mm/mem_sharing.c|706| <<page_make_private>> if ( domain_adjust_tot_pages(d, 1) == 1 )
+	 *   - common/grant_table.c|1995| <<gnttab_transfer>> if ( unlikely(domain_adjust_tot_pages(e, 1) == 1) )
+	 *   - common/grant_table.c|2010| <<gnttab_transfer>> bool_t drop_dom_ref = !domain_adjust_tot_pages(e, -1);
+	 *   - common/memory.c|636| <<memory_exchange>> !domain_adjust_tot_pages(d, -dec_count));
+	 *   - common/page_alloc.c|1967| <<assign_pages>> domain_adjust_tot_pages(d, 1 << order);
+	 *   - common/page_alloc.c|2067| <<free_domheap_pages>> drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
+	 */
         drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
 
         spin_unlock_recursive(&d->page_alloc_lock);
@@ -2026,6 +2725,10 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
         put_domain(d);
 }
 
+/*
+ * called by:
+ *   - common/sysctl.c|169| <<do_sysctl(XEN_SYSCTL_availheap)>> op->u.availheap.avail_bytes = avail_domheap_pages_region(
+ */
 unsigned long avail_domheap_pages_region(
     unsigned int node, unsigned int min_width, unsigned int max_width)
 {
@@ -2037,18 +2740,42 @@ unsigned long avail_domheap_pages_region(
     zone_hi = max_width ? bits_to_zone(max_width) : (NR_ZONES - 1);
     zone_hi = max_t(int, MEMZONE_XEN + 1, min_t(int, NR_ZONES - 1, zone_hi));
 
+    /*
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     */
     return avail_heap_pages(zone_lo, zone_hi, node);
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain_build.c|301| <<compute_dom0_nr_pages>> unsigned long avail = avail_domheap_pages() + initial_images_nrpages();
+ *   - common/page_alloc.c|586| <<get_outstanding_claims>> *free_pages = avail_domheap_pages();
+ *
+ * 把从MEMZONE_XEN + 1到NR_ZONES - 1的每个node的page全加起来
+ */
 unsigned long avail_domheap_pages(void)
 {
+    /*
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     */
     return avail_heap_pages(MEMZONE_XEN + 1,
                             NR_ZONES - 1,
                             -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/numa.c|369| <<dump_numa>> avail_node_heap_pages(i));
+ *   - common/sysctl.c|299| <<do_sysctl(XEN_SYSCTL_numainfo)>> avail_node_heap_pages(i) << PAGE_SHIFT : 0ul;
+ */
 unsigned long avail_node_heap_pages(unsigned int nodeid)
 {
+    /*
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     */
     return avail_heap_pages(MEMZONE_XEN, NR_ZONES -1, nodeid);
 }
 
@@ -2058,6 +2785,13 @@ static void pagealloc_info(unsigned char key)
     unsigned int zone = MEMZONE_XEN;
     unsigned long n, total = 0;
 
+    /*
+     * avail_heap_pages():
+     * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+     * 如果node是-1就把所有node的加起来
+     *
+     * 开始zone是MEMZONE_XEN=0
+     */
     printk("Physical memory information:\n");
     printk("    Xen heap: %lukB free\n",
            avail_heap_pages(zone, zone, -1) << (PAGE_SHIFT-10));
@@ -2070,6 +2804,10 @@ static void pagealloc_info(unsigned char key)
             total = 0;
         }
 
+        /*
+	 * 把从zone_lo到zone_hi的属于avail[node][zone]的全加起来返回
+	 * 如果node是-1就把所有node的加起来
+	 */
         if ( (n = avail_heap_pages(zone, zone, -1)) != 0 )
         {
             total += n;
@@ -2094,6 +2832,24 @@ static __init int pagealloc_keyhandler_init(void)
 __initcall(pagealloc_keyhandler_init);
 
 
+/*
+ * called by:
+ *   - arch/x86/mm/p2m.c|1005| <<p2m_mem_paging_evict>> scrub_one_page(page);
+ *   - common/page_alloc.c|797| <<alloc_heap_pages>> scrub_one_page(&pg[i]);
+ *   - common/page_alloc.c|1442| <<smp_scrub_heap_pages>> scrub_one_page(pg);
+ *   - common/page_alloc.c|1622| <<__scrub_free_pages>> scrub_one_page(&pg[i]);
+ *   - common/tmem.c|1401| <<tmem_flush_npages>> scrub_one_page(pg);
+ *   - common/tmem.c|2795| <<tmem_relinquish_pages>> scrub_one_page(pfp);
+ *   - include/xen/tmem_xen.h|138| <<__tmem_free_page_thispool>> scrub_one_page(pi);
+ *
+ * 从smp_scrub_heap_pages()进来的话只能是初始化的时候:
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ *          -> scrub_one_page()
+ *
+ * 把page给scrub了, 不改变任何flag
+ */
 void scrub_one_page(struct page_info *pg)
 {
     void *p;
@@ -2140,6 +2896,18 @@ static void dump_heap(unsigned char key)
     }
     for ( i = 0; i < MAX_NUMNODES; i++ )
     {
+        /*
+	 * 修改node_need_scrub[]的地方:
+         *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+         *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+         *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+         * 在以下使用node_need_scrub:
+         *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+         *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+         *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+         *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+         *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+	 */
         if ( !node_need_scrub[i] )
             continue;
         printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
diff --git a/xen/common/softirq.c b/xen/common/softirq.c
index 11e3345397..5fa39fa39d 100644
--- a/xen/common/softirq.c
+++ b/xen/common/softirq.c
@@ -26,6 +26,11 @@ static softirq_handler softirq_handlers[NR_SOFTIRQS];
 static DEFINE_PER_CPU(cpumask_t, batch_mask);
 static DEFINE_PER_CPU(unsigned int, batching);
 
+/*
+ * called by:
+ *   - common/softirq.c|59| <<process_pending_softirqs>> __do_softirq(1ul<<SCHEDULE_SOFTIRQ);
+ *   - common/softirq.c|65| <<do_softirq>> __do_softirq(0);
+ */
 static void __do_softirq(unsigned long ignore_mask)
 {
     unsigned int i, cpu;
@@ -65,6 +70,20 @@ asmlinkage void do_softirq(void)
     __do_softirq(0);
 }
 
+/*
+ * 调用的例子:
+ *   - arch/x86/cpu/mcheck/mce.c|1739| <<mce_handler_init>> open_softirq(MACHINE_CHECK_SOFTIRQ, mce_softirq);
+ *   - arch/x86/domain.c|2377| <<init_vcpu_kick_softirq>> open_softirq(VCPU_KICK_SOFTIRQ, vcpu_kick_softirq);
+ *   - arch/x86/setup.c|1335| <<__start_xen>> open_softirq(NEW_TLBFLUSH_CLOCK_PERIOD_SOFTIRQ, new_tlbflush_clock_period);
+ *   - arch/x86/time.c|1620| <<init_xen_time>> open_softirq(TIME_CALIBRATE_SOFTIRQ, local_time_calibration);
+ *   - arch/x86/traps.c|3652| <<trap_init>> open_softirq(NMI_MCE_SOFTIRQ, nmi_mce_softirq);
+ *   - arch/x86/traps.c|3653| <<trap_init>> open_softirq(PCI_SERR_SOFTIRQ, pci_serr_softirq);
+ *   - common/rcupdate.c|486| <<rcu_init>> open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
+ *   - common/schedule.c|1365| <<scheduler_init>> open_softirq(SCHEDULE_SOFTIRQ, schedule);
+ *   - common/tasklet.c|362| <<tasklet_subsys_init>> open_softirq(TASKLET_SOFTIRQ, tasklet_softirq_action);
+ *   - common/tasklet.c|363| <<tasklet_subsys_init>> open_softirq(TASKLET_SOFTIRQ_PERCPU, tasklet_softirq_percpu_action);
+ *   - common/timer.c|630| <<timer_init>> open_softirq(TIMER_SOFTIRQ, timer_softirq_action);
+ */
 void open_softirq(int nr, softirq_handler handler)
 {
     ASSERT(nr < NR_SOFTIRQS);
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index 9e94eabfb7..c7c81f487a 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -322,6 +322,21 @@ struct arch_domain
         RELMEM_l2,
         RELMEM_done,
     } relmem;
+    /*
+     * 在以下修改relmem_list:
+     *   - arch/x86/domain.c|536| <<arch_domain_create>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/domain.c|2009| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2087| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2098| <<relinquish_memory>> page_list_move(list, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2173| <<domain_relinquish_resources>> page_list_splice(&d->arch.relmem_list, &d->page_list);
+     *   - arch/x86/domain.c|2174| <<domain_relinquish_resources>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/mm/p2m-pod.c|471| <<p2m_pod_offline_or_broken_hit>> page_list_add(p, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1980| <<free_domheap_pages>> page_list_del2(&pg[i], &d->xenpage_list, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1995| <<free_domheap_pages>> page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|404| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|419| <<iommu_populate_page_table>> page_list_move(&d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|424| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     */
     struct page_list_head relmem_list;
 
     cpuid_input_t *cpuids;
diff --git a/xen/include/asm-x86/event.h b/xen/include/asm-x86/event.h
index a82062e339..94a8356aae 100644
--- a/xen/include/asm-x86/event.h
+++ b/xen/include/asm-x86/event.h
@@ -23,11 +23,24 @@ int hvm_local_events_need_delivery(struct vcpu *v);
 static inline int local_events_need_delivery(void)
 {
     struct vcpu *v = current;
+    /*
+     * 部分使用evtchn_upcall_mask的例子:
+     *   - common/domain.c|1113| <<map_vcpu_info>> __vcpu_info(v, new_info, evtchn_upcall_mask) = 1;
+     *   - include/asm-x86/event.h|19| <<vcpu_event_delivery_is_enabled>> return !vcpu_info(v, evtchn_upcall_mask);
+     *   - include/asm-x86/event.h|28| <<local_events_need_delivery>> !vcpu_info(v, evtchn_upcall_mask)));
+     *   - include/asm-x86/event.h|33| <<local_event_delivery_disable>> vcpu_info(current, evtchn_upcall_mask) = 1;
+     *   - include/asm-x86/event.h|38| <<local_event_delivery_enable>> vcpu_info(current, evtchn_upcall_mask) = 0;
+     *   - 在arch/x86/x86_64/entry.S也有用到
+     */
     return (has_hvm_container_vcpu(v) ? hvm_local_events_need_delivery(v) :
             (vcpu_info(v, evtchn_upcall_pending) &&
              !vcpu_info(v, evtchn_upcall_mask)));
 }
 
+/*
+ * called by:
+ *   - arch/x86/hvm/svm/nestedsvm.c|40| <<nestedsvm_vcpu_clgi>> local_event_delivery_disable();
+ */
 static inline void local_event_delivery_disable(void)
 {
     vcpu_info(current, evtchn_upcall_mask) = 1;
diff --git a/xen/include/asm-x86/guest_pt.h b/xen/include/asm-x86/guest_pt.h
index 6ce65892f8..d940924b83 100644
--- a/xen/include/asm-x86/guest_pt.h
+++ b/xen/include/asm-x86/guest_pt.h
@@ -181,6 +181,15 @@ static inline guest_l4e_t guest_l4e_from_gfn(gfn_t gfn, u32 flags)
 
 /* Which pagetable features are supported on this vcpu? */
 
+/*
+ * called by:
+ *   - arch/x86/mm/guest_walk.c|290| <<guest_walk_tables>> pse2M = (gflags & _PAGE_PSE) && guest_supports_superpages(v);
+ *   - arch/x86/mm/shadow/multi.c|243| <<shadow_check_gwalk>> if ( !(guest_supports_superpages(v) &&
+ *   - arch/x86/mm/shadow/multi.c|314| <<gw_remove_write_accesses>> if ( !(guest_supports_superpages(v) &&
+ *   - arch/x86/mm/shadow/multi.c|653| <<_sh_propagate>> guest_supports_superpages(v)))
+ *   - arch/x86/mm/shadow/multi.c|1860| <<shadow_get_and_create_l1e>> if ( guest_supports_superpages(v) && (flags & _PAGE_PSE) )
+ *   - arch/x86/mm/shadow/multi.c|2268| <<validate_gl2e>> if ( guest_supports_superpages(v) &&
+ */
 static inline int
 guest_supports_superpages(struct vcpu *v)
 {
diff --git a/xen/include/asm-x86/mm.h b/xen/include/asm-x86/mm.h
index 56f2968d39..d37f14bd1f 100644
--- a/xen/include/asm-x86/mm.h
+++ b/xen/include/asm-x86/mm.h
@@ -184,13 +184,24 @@ struct page_info
 #define PGT_l1_page_table PG_mask(1, 4)  /* using as an L1 page table?     */
 #define PGT_l2_page_table PG_mask(2, 4)  /* using as an L2 page table?     */
 #define PGT_l3_page_table PG_mask(3, 4)  /* using as an L3 page table?     */
+/* 0x4000000000000000 */
 #define PGT_l4_page_table PG_mask(4, 4)  /* using as an L4 page table?     */
 #define PGT_seg_desc_page PG_mask(5, 4)  /* using this page in a GDT/LDT?  */
+/* 0x7000000000000000 */
 #define PGT_writable_page PG_mask(7, 4)  /* has writable mappings?         */
 #define PGT_shared_page   PG_mask(8, 4)  /* CoW sharable page              */
 #define PGT_type_mask     PG_mask(15, 4) /* Bits 28-31 or 60-63.           */
 
  /* Owning guest has pinned this page to its current type? */
+/*
+ * 使用_PGT_pinned的地方:
+ *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+ *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+ */
 #define _PGT_pinned       PG_shift(5)
 #define PGT_pinned        PG_mask(1, 5)
  /* Has this page been validated for use as its current type? */
@@ -233,10 +244,33 @@ struct page_info
 #define PGC_state_inuse   PG_mask(0, 9)
 #define PGC_state_offlining PG_mask(1, 9)
 #define PGC_state_offlined PG_mask(2, 9)
+/*
+ * 在以下使用PGC_state_free:
+ *   - common/page_alloc.c|962| <<alloc_heap_pages>> BUG_ON(pg[i].count_info != PGC_state_free);
+ *   - common/page_alloc.c|1202| <<free_heap_pages>> ? PGC_state_offlined : PGC_state_free));
+ *   - common/page_alloc.c|1262| <<mark_page_offline>> nx |= (((x & PGC_state) == PGC_state_free)
+ *   - common/page_alloc.c|1892| <<__scrub_free_pages>> pg[i].count_info |= PGC_state_free;
+ */
 #define PGC_state_free    PG_mask(3, 9)
 #define page_state_is(pg, st) (((pg)->count_info&PGC_state) == PGC_state_##st)
 /* Page need to be scrubbed */
+/*
+ * 在以下使用_PGC_need_scrub:
+ *   - common/page_alloc.c|947| <<alloc_heap_pages>> if ( test_and_clear_bit(_PGC_need_scrub, &pg[i].count_info) )
+ *   - common/page_alloc.c|1093| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+ *   - common/page_alloc.c|1098| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg-mask)->count_info) )
+ *   - common/page_alloc.c|1114| <<merge_free_trunks>> if ( !test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+ *   - common/page_alloc.c|1119| <<merge_free_trunks>> if ( test_bit(_PGC_need_scrub, &(pg+mask)->count_info) )
+ *   - common/page_alloc.c|1829| <<__scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+ *   - common/page_alloc.c|1900| <<scrub_free_pages>> if ( !test_bit(_PGC_need_scrub, &(pg->count_info)) )
+ *   - common/page_alloc.c|1919| <<scrub_free_pages>> ASSERT( test_bit(_PGC_need_scrub, &pg[i].count_info) );
+ */
 #define _PGC_need_scrub   PG_shift(10)
+/*
+ * 在以下使用PGC_need_scrub:
+ *   - common/page_alloc.c|1201| <<free_heap_pages>> pg[i].count_info |= PGC_need_scrub;
+ *   - common/page_alloc.c|1848| <<__scrub_free_pages>> pg[i].count_info &= ~PGC_need_scrub;
+ */
 #define PGC_need_scrub    PG_mask(1, 10)
 
  /* Count of references to this frame. */
diff --git a/xen/include/xen/mm.h b/xen/include/xen/mm.h
index bad1ef3edc..9052c902f8 100644
--- a/xen/include/xen/mm.h
+++ b/xen/include/xen/mm.h
@@ -77,6 +77,39 @@ void xenheap_max_mfn(unsigned long mfn);
 void *alloc_xenheap_pages(unsigned int order, unsigned int memflags);
 void free_xenheap_pages(void *v, unsigned int order);
 #define alloc_xenheap_page() (alloc_xenheap_pages(0,0))
+/*
+ * 在以下调用free_xenheap_page():
+ *   - arch/x86/domain.c|250| <<free_domain_struct>> free_xenheap_page(d);
+ *   - arch/x86/domain.c|271| <<free_vcpu_struct>> free_xenheap_page(v);
+ *   - arch/x86/domain.c|670| <<arch_domain_create>> free_xenheap_page(d->shared_info);
+ *   - arch/x86/domain.c|677| <<arch_domain_create>> free_xenheap_page(d->arch.pv_domain.gdt_ldt_l1tab);
+ *   - arch/x86/domain.c|698| <<arch_domain_destroy>> free_xenheap_page(d->arch.pv_domain.gdt_ldt_l1tab);
+ *   - arch/x86/domain.c|700| <<arch_domain_destroy>> free_xenheap_page(d->shared_info);
+ *   - arch/x86/domctl.c|382| <<XEN_GUEST_HANDLE_PARAM>> free_xenheap_page(arr32);
+ *   - arch/x86/hvm/svm/svm.c|1192| <<svm_cpu_dead>> free_xenheap_page(per_cpu(hsa, cpu));
+ *   - arch/x86/hvm/svm/vmcb.c|52| <<free_vmcb>> free_xenheap_page(vmcb);
+ *   - arch/x86/hvm/vmx/vmcs.c|404| <<vmx_free_vmcs>> free_xenheap_page(vmcs);
+ *   - arch/x86/hvm/vmx/vmcs.c|1292| <<vmx_destroy_vmcs>> free_xenheap_page(v->arch.hvm_vmx.host_msr_area);
+ *   - arch/x86/hvm/vmx/vmcs.c|1293| <<vmx_destroy_vmcs>> free_xenheap_page(v->arch.hvm_vmx.msr_area);
+ *   - arch/x86/hvm/vmx/vmcs.c|1294| <<vmx_destroy_vmcs>> free_xenheap_page(v->arch.hvm_vmx.msr_bitmap);
+ *   - arch/x86/hvm/vmx/vmx.c|2177| <<vmx_free_vlapic_mapping>> free_xenheap_page(mfn_to_virt(mfn));
+ *   - arch/x86/hvm/vmx/vvmx.c|140| <<nvmx_vcpu_destroy>> free_xenheap_page(nvcpu->nv_n2vmcx);
+ *   - arch/x86/mm.c|5564| <<free_xen_pagetable>> free_xenheap_page(v);
+ *   - arch/x86/mm.c|6439| <<free_perdomain_mappings>> free_xenheap_page(page_to_virt(l1pg));
+ *   - common/grant_table.c|1507| <<gnttab_populate_status_frames>> free_xenheap_page(gt->status[i]);
+ *   - common/grant_table.c|1575| <<gnttab_unpopulate_status_frames>> free_xenheap_page(gt->status[i]);
+ *   - common/grant_table.c|1635| <<gnttab_grow_table>> free_xenheap_page(gt->shared_raw[i]);
+ *   - common/grant_table.c|1642| <<gnttab_grow_table>> free_xenheap_page(gt->active[i]);
+ *   - common/grant_table.c|3230| <<grant_table_create>> free_xenheap_page(t->shared_raw[i]);
+ *   - common/grant_table.c|3237| <<grant_table_create>> free_xenheap_page(t->active[i]);
+ *   - common/grant_table.c|3400| <<grant_table_destroy>> free_xenheap_page(t->shared_raw[i]);
+ *   - common/grant_table.c|3404| <<grant_table_destroy>> free_xenheap_page(t->maptrack[i]);
+ *   - common/grant_table.c|3408| <<grant_table_destroy>> free_xenheap_page(t->active[i]);
+ *   - common/grant_table.c|3412| <<grant_table_destroy>> free_xenheap_page(t->status[i]);
+ *   - common/tmem_xen.c|274| <<cpu_callback>> free_xenheap_page(per_cpu(scratch_page, cpu));
+ *   - common/wait.c|78| <<destroy_waitqueue_vcpu>> free_xenheap_page(wqv->stack);
+ *   - common/xmalloc_tlsf.c|525| <<xmalloc_pool_put>> free_xenheap_page(p);
+ */
 #define free_xenheap_page(v) (free_xenheap_pages(v,0))
 /* Map machine page range in Xen virtual address space. */
 int map_pages_to_xen(
@@ -104,6 +137,40 @@ unsigned long avail_domheap_pages_region(
 unsigned long avail_domheap_pages(void);
 unsigned long avail_node_heap_pages(unsigned int);
 #define alloc_domheap_page(d,f) (alloc_domheap_pages(d,0,f))
+/*
+ * called by:
+ *   - arch/x86/domain.c|313| <<free_vcpu_guest_context>> free_domheap_page(per_cpu(vgc_pages[i], cpu));
+ *   - arch/x86/domain.c|331| <<setup_compat_l4>> free_domheap_page(pg);
+ *   - arch/x86/domain.c|352| <<release_compat_l4>> free_domheap_page(pagetable_get_page(v->arch.guest_table));
+ *   - arch/x86/domain.c|1248| <<arch_domain_soft_reset>> free_domheap_page(new_page);
+ *   - arch/x86/domain.c|1257| <<arch_domain_soft_reset>> free_domheap_page(new_page);
+ *   - arch/x86/domctl.c|291| <<XEN_GUEST_HANDLE_PARAM>> free_domheap_page(page);
+ *   - arch/x86/hvm/stdvga.c|632| <<stdvga_deinit>> free_domheap_page(s->vram_page[i]);
+ *   - arch/x86/hvm/vlapic.c|1310| <<vlapic_destroy>> free_domheap_page(vlapic->regs_page);
+ *   - arch/x86/hvm/vmx/vvmx.c|154| <<nvmx_vcpu_destroy>> free_domheap_page(v->arch.hvm_vmx.vmread_bitmap);
+ *   - arch/x86/hvm/vmx/vvmx.c|159| <<nvmx_vcpu_destroy>> free_domheap_page(v->arch.hvm_vmx.vmwrite_bitmap);
+ *   - arch/x86/mm.c|2170| <<put_page>> free_domheap_page(page);
+ *   - arch/x86/mm.c|6386| <<destroy_perdomain_mapping>> free_domheap_page(l1e_get_page(l1tab[i]));
+ *   - arch/x86/mm.c|6433| <<free_perdomain_mappings>> free_domheap_page(l1e_get_page(l1tab[k]));
+ *   - arch/x86/mm.c|6441| <<free_perdomain_mappings>> free_domheap_page(l1pg);
+ *   - arch/x86/mm.c|6445| <<free_perdomain_mappings>> free_domheap_page(l2pg);
+ *   - arch/x86/mm.c|6449| <<free_perdomain_mappings>> free_domheap_page(d->arch.perdomain_l3_pg);
+ *   - arch/x86/mm/hap/hap.c|357| <<hap_set_allocation>> free_domheap_page(pg);
+ *   - arch/x86/mm/p2m-pod.c|486| <<p2m_pod_offline_or_broken_replace>> free_domheap_page(p);
+ *   - arch/x86/mm/shadow/common.c|1724| <<sh_set_allocation>> free_domheap_page(sp);
+ *   - common/grant_table.c|1943| <<XEN_GUEST_HANDLE_PARAM>> free_domheap_page(page);
+ *   - common/grant_table.c|1966| <<XEN_GUEST_HANDLE_PARAM>> free_domheap_page(page);
+ *   - common/kimage.c|284| <<kimage_free_page_list>> free_domheap_page(page);
+ *   - common/kimage.c|509| <<kimage_free_entry>> free_domheap_page(page);
+ *   - common/kimage.c|665| <<kimage_alloc_page>> free_domheap_page(page);
+ *   - common/tmem.c|1403| <<tmem_flush_npages>> free_domheap_page(pg);
+ *   - common/vmap.c|120| <<vm_alloc>> free_domheap_page(pg);
+ *   - common/vmap.c|273| <<vmalloc_type>> free_domheap_page(mfn_to_page(mfn_x(mfn[i])));
+ *   - common/vmap.c|330| <<vfree>> free_domheap_page(pg);
+ *   - drivers/passthrough/amd/pci_amd_iommu.c|267| <<amd_iommu_domain_init>> free_domheap_page(hd->root_table);
+ *   - drivers/passthrough/vtd/iommu.c|222| <<free_pgtable_maddr>> free_domheap_page(maddr_to_page(maddr));
+ *   - include/asm-x86/hvm/svm/amd-iommu-proto.h|178| <<free_amd_iommu_pgtable>> free_domheap_page(pg);
+ */
 #define free_domheap_page(p)  (free_domheap_pages(p,0))
 unsigned int online_page(unsigned long mfn, uint32_t *status);
 int offline_page(unsigned long mfn, int broken, uint32_t *status);
@@ -410,6 +477,11 @@ int page_is_ram_type(unsigned long mfn, unsigned long mem_type);
 
 #include <asm/flushtlb.h>
 
+/*
+ * called by:
+ *   - common/memory.c|230| <<populate_physmap>> accumulate_tlbflush(&need_tlbflush, &page[j],
+ *   - common/page_alloc.c|966| <<alloc_heap_pages>> accumulate_tlbflush(&need_tlbflush, &pg[i],
+ */
 static inline void accumulate_tlbflush(bool_t *need_tlbflush,
                                        const struct page_info *page,
                                        uint32_t *tlbflush_timestamp)
@@ -424,6 +496,11 @@ static inline void accumulate_tlbflush(bool_t *need_tlbflush,
     }
 }
 
+/*
+ * called by:
+ *   - common/memory.c|251| <<populate_physmap>> filtered_flush_tlb_mask(tlbflush_timestamp);
+ *   - common/page_alloc.c|1003| <<alloc_heap_pages>> filtered_flush_tlb_mask(tlbflush_timestamp);
+ */
 static inline void filtered_flush_tlb_mask(uint32_t tlbflush_timestamp)
 {
     cpumask_t mask = cpu_online_map;
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index d8006bb0b7..9023111a8b 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -292,6 +292,21 @@ struct domain
     struct page_list_head page_list;  /* linked list */
     struct page_list_head xenpage_list; /* linked list (size xenheap_pages) */
     unsigned int     tot_pages;       /* number of pages currently possesed */
+    /*
+     * 在以下修改d->outstanding_pages:
+     *   - common/page_alloc.c|495| <<domain_adjust_tot_pages>> d->outstanding_pages = dom_claimed;
+     *   - common/page_alloc.c|529| <<domain_set_outstanding_pages>> d->outstanding_pages = 0;
+     *   - common/page_alloc.c|572| <<domain_set_outstanding_pages>> d->outstanding_pages = claim;
+     *
+     * 在以下使用d->outstanding_pages:
+     *   - common/domctl.c|209| <<getdomaininfo>> info->outstanding_pages = d->outstanding_pages;
+     *   - common/page_alloc.c|486| <<domain_adjust_tot_pages>> if ( !d->outstanding_pages )
+     *   - common/page_alloc.c|491| <<domain_adjust_tot_pages>> dom_before = d->outstanding_pages;
+     *   - common/page_alloc.c|528| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+     *   - common/page_alloc.c|535| <<domain_set_outstanding_pages>> if ( d->outstanding_pages )
+     *   - common/page_alloc.c|573| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+     *   - common/page_alloc.c|868| <<alloc_heap_pages>> !d || d->outstanding_pages < request) )
+     */
     unsigned int     outstanding_pages; /* pages claimed but not possessed  */
     unsigned int     max_pages;       /* maximum value for tot_pages        */
     atomic_t         shr_pages;       /* number of shared pages             */
@@ -746,6 +761,33 @@ static inline struct domain *next_domain_in_cpupool(
 #define _VPF_blocked         0
 #define VPF_blocked          (1UL<<_VPF_blocked)
  /* VCPU is offline. */
+/*
+ * x86在以下设置_VPF_down:
+ *   - arch/x86/domain.c|1093| <<arch_set_info_guest>> set_bit(_VPF_down, &v->pause_flags);
+ *   - arch/x86/hvm/hvm.c|1488| <<hvm_vcpu_down>> set_bit(_VPF_down, &v->pause_flags);
+ *   - common/domain.c|227| <<alloc_vcpu>> set_bit(_VPF_down, &v->pause_flags);
+ *   - common/domain.c|1043| <<vcpu_reset>> set_bit(_VPF_down, &v->pause_flags);
+ *   - common/domain.c|1212| <<do_vcpu_op(VCPUOP_down)>> if ( !test_and_set_bit(_VPF_down, &v->pause_flags) )
+ *   - common/hvm/save.c|222| <<hvm_load>> if ( test_and_set_bit(_VPF_down, &v->pause_flags) )
+ * x86在以下清除_VPF_down:
+ *   - arch/x86/domain.c|1091| <<arch_set_info_guest>> clear_bit(_VPF_down, &v->pause_flags);
+ *   - arch/x86/domain_build.c|1162| <<construct_dom0>> clear_bit(_VPF_down, &v->pause_flags);
+ *   - arch/x86/hvm/hvm.c|1027| <<hvm_load_cpu_ctxt>> clear_bit(_VPF_down, &v->pause_flags);
+ *   - arch/x86/hvm/hvm.c|1428| <<hvm_vcpu_initialise>> clear_bit(_VPF_down, &v->pause_flags);
+ *   - arch/x86/hvm/hvm.c|4023| <<hvm_vcpu_reset_state>> clear_bit(_VPF_down, &v->pause_flags);
+ *   - arch/x86/hvm/vlapic.c|335| <<vlapic_accept_irq>> wake = test_and_clear_bit(_VPF_down, &v->pause_flags);
+ *   - common/domain.c|1204| <<do_vcpu_op(VCPUOP_up)>> wake = test_and_clear_bit(_VPF_down, &v->pause_flags);
+ * x86在以下使用_VPF_down:
+ *   - arch/x86/domctl.c|1450| <<arch_get_info_guest>> if ( !test_bit(_VPF_down, &v->pause_flags) )
+ *   - arch/x86/hvm/hvm.c|746| <<hvm_save_cpu_ctxt>> if ( test_bit(_VPF_down, &v->pause_flags) )
+ *   - arch/x86/hvm/hvm.c|1494| <<hvm_vcpu_down>> if ( !test_bit(_VPF_down, &v->pause_flags) )
+ *   - common/domain.c|1087| <<map_vcpu_info>> if ( (v != current) && !test_bit(_VPF_down, &v->pause_flags) )
+ *   - common/domain.c|1217| <<do_vcpu_op(VCPUOP_is_up)>> rc = !test_bit(_VPF_down, &v->pause_flags);
+ *   - common/domctl.c|174| <<getdomaininfo>> if ( !test_bit(_VPF_down, &v->pause_flags) )
+ *   - common/domctl.c|235| <<default_vcpu0_location>> if ( !test_bit(_VPF_down, &v->pause_flags)
+ *   - common/domctl.c|883| <<do_domctl(XEN_DOMCTL_getvcpuinfo)>> op->u.getvcpuinfo.online = !test_bit(_VPF_down, &v->pause_flags);
+ *   - include/xen/sched.h|765| <<VPF_down>> #define VPF_down (1UL<<_VPF_down)
+ */
 #define _VPF_down            1
 #define VPF_down             (1UL<<_VPF_down)
  /* VCPU is blocked awaiting an event to be consumed by Xen. */
-- 
2.17.1

