From c1863863197685b5e01d2eb24dd1be4fc357757a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@oracle.com>
Date: Fri, 1 Nov 2019 16:12:44 +0800
Subject: [PATCH 1/1] xen-4.4.4-222-comment

xen-4.4.4-222

Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
---
 xen/arch/x86/domain.c        | 22 ++++++++++++
 xen/arch/x86/domain_page.c   | 17 ++++++++++
 xen/common/domain.c          |  4 +++
 xen/common/page_alloc.c      | 79 ++++++++++++++++++++++++++++++++++++++++++++
 xen/include/asm-x86/domain.h | 15 +++++++++
 xen/include/asm-x86/mm.h     | 11 ++++++
 6 files changed, 148 insertions(+)

diff --git a/xen/arch/x86/domain.c b/xen/arch/x86/domain.c
index 8acc0c7..f5994e6 100644
--- a/xen/arch/x86/domain.c
+++ b/xen/arch/x86/domain.c
@@ -1983,6 +1983,13 @@ int hypercall_xlat_continuation(unsigned int *id, unsigned int nr,
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|2168| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->xenpage_list, ~0UL);
+ *   - arch/x86/domain.c|2175| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ *   - arch/x86/domain.c|2182| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l3_page_table);
+ *   - arch/x86/domain.c|2189| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l2_page_table);
+ */
 static int relinquish_memory(
     struct domain *d, struct page_list_head *list, unsigned long type)
 {
@@ -2003,6 +2010,17 @@ static int relinquish_memory(
             continue;
         }
 
+        /*
+	 * Clear a bit and return its old value
+	 *
+	 * 使用_PGT_pinned的地方:
+	 *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+	 *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+	 */
         if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
             ret = put_page_and_type_preemptible(page);
         switch ( ret )
@@ -2095,6 +2113,10 @@ static int relinquish_memory(
     return ret;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> rc = domain_relinquish_resources(d);
+ */
 int domain_relinquish_resources(struct domain *d)
 {
     int ret;
diff --git a/xen/arch/x86/domain_page.c b/xen/arch/x86/domain_page.c
index 56f9bb1..6ec449c 100644
--- a/xen/arch/x86/domain_page.c
+++ b/xen/arch/x86/domain_page.c
@@ -172,6 +172,23 @@ void *map_domain_page(unsigned long mfn)
     return (void *)MAPCACHE_VIRT_START + pfn_to_paddr(idx);
 }
 
+/*
+ * MAPCACHE_VCPU_ENTRIES = 0x00000010
+ * MAPCACHE_ENTRIES      = 0x0000000000020000
+ * MAPCACHE_VIRT_START   = 0xffff820040000000
+ * MAPCACHE_VIRT_END     = 0xffff820060000000
+ *
+ * DIRECTMAP_VIRT_START = 0xffff830000000000
+ * DIRECTMAP_SIZE       = 0x00007c8000000000
+ * DIRECTMAP_VIRT_END   = 0xffffff8000000000
+ *
+ * LINEAR_PT_VIRT_START = 0xffff810000000000
+ * LINEAR_PT_VIRT_END   = 0xffff818000000000
+ *
+ * HYPERVISOR_VIRT_START = 0xffff800000000000
+ * HYPERVISOR_VIRT_END   = 0xffff880000000000
+ */
+
 void unmap_domain_page(const void *ptr)
 {
     unsigned int idx;
diff --git a/xen/common/domain.c b/xen/common/domain.c
index e44d403..735834f 100644
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -612,6 +612,10 @@ int rcu_lock_live_remote_domain_by_id(domid_t dom, struct domain **d)
     return 0;
 }
 
+/*
+ * called by:
+ *   - common/domctl.c|726| <<do_domctl(XEN_DOMCTL_destroydomain)>> ret = domain_kill(d);
+ */
 int domain_kill(struct domain *d)
 {
     int rc = 0;
diff --git a/xen/common/page_alloc.c b/xen/common/page_alloc.c
index 95db602..2fce20d 100644
--- a/xen/common/page_alloc.c
+++ b/xen/common/page_alloc.c
@@ -81,6 +81,13 @@ integer_param("dma_bits", dma_bitsize);
 #define round_pgdown(_p)  ((_p)&PAGE_MASK)
 #define round_pgup(_p)    (((_p)+(PAGE_SIZE-1))&PAGE_MASK)
 
+/*
+ * 在以下使用pglist_lock:
+ *   - common/page_alloc.c|877| <<reserve_offlined_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|883| <<reserve_offlined_page>> spin_unlock(&pglist_lock);
+ *   - common/page_alloc.c|1210| <<online_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|1241| <<online_page>> spin_unlock(&pglist_lock);
+ */
 static DEFINE_SPINLOCK(pglist_lock);
 /* Offlined page list, protected by pglist_lock. */
 static PAGE_LIST_HEAD(page_offlined_list);
@@ -88,9 +95,34 @@ static PAGE_LIST_HEAD(page_offlined_list);
 static PAGE_LIST_HEAD(page_broken_list);
 
 /* A rough flag to indicate whether a node have need_scrub pages */
+/*
+ * 在以下使用node_need_scrub:
+ *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+ *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+ *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+ *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+ *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+ *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+ */
 static unsigned long node_need_scrub[MAX_NUMNODES];
+/*
+ * 在以下使用is_scrubbing:
+ *   - common/page_alloc.c|1673| <<scrub_free_pages>> if ( test_and_set_bool(per_cpu(is_scrubbing, cpumask_first(per_cpu(cpu_sibling_mask, cpu)))) )
+ *   - common/page_alloc.c|1736| <<scrub_free_pages>> was_scrubbing = test_and_clear_bool(per_cpu(is_scrubbing,
+ */
 static DEFINE_PER_CPU(bool_t, is_scrubbing);
+/*
+ * 在以下使用scrub_list_cpu:
+ *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, scrub_list_cpu);
+/*
+ * 在以下使用free_list_cpu:
+ *   - common/page_alloc.c|1612| <<__scrub_free_pages>> struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, free_list_cpu);
 
 /*************************
@@ -602,6 +634,12 @@ static void check_low_mem_virq(unsigned long avail_pages)
 }
 
 /* Allocate 2^@order contiguous pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1812| <<alloc_xenheap_pages>> pg = alloc_heap_pages(MEMZONE_XEN, MEMZONE_XEN,
+ *   - common/page_alloc.c|1988| <<alloc_domheap_pages>> pg = alloc_heap_pages(dma_zone + 1, zone_hi, order, memflags, d);
+ *   - common/page_alloc.c|1992| <<alloc_domheap_pages>> ((pg = alloc_heap_pages(MEMZONE_XEN + 1, zone_hi, order,
+ */
 static struct page_info *alloc_heap_pages(
     unsigned int zone_lo, unsigned int zone_hi,
     unsigned int order, unsigned int memflags,
@@ -1391,6 +1429,15 @@ void __init end_boot_allocator(void)
     printk("\n");
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1561| <<scrub_heap_pages>> on_selected_cpus(&all_worker_cpus, smp_scrub_heap_pages, NULL, 1);
+ *   - common/page_alloc.c|1618| <<scrub_heap_pages>> on_selected_cpus(&node_cpus, smp_scrub_heap_pages, &region[i], 1);
+ *
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ */
 static void __init smp_scrub_heap_pages(void *data)
 {
     unsigned long mfn, start, end;
@@ -1464,6 +1511,10 @@ int __init find_non_smt(unsigned int node, cpumask_t *dest)
  * Scrub all unallocated pages in all heap zones. This function uses all
  * online cpu's to scrub the memory in parallel.
  */
+/*
+ * x86下的调用:
+ *   - arch/x86/setup.c|1545| <<__start_xen>> scrub_heap_pages();
+ */
 void __init scrub_heap_pages(void)
 {
     cpumask_t node_cpus, all_worker_cpus;
@@ -1599,6 +1650,10 @@ void __init scrub_heap_pages(void)
 
 #define SCRUB_BATCH_ORDER 12
 /* return 1 if should continue */
+/*
+ * called by:
+ *   - common/page_alloc.c|1726| <<scrub_free_pages>> } while ( __scrub_free_pages(node, cpu) );
+ */
 static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 {
     struct page_info *pg, *tmp;
@@ -1648,6 +1703,10 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 }
 
 /* return 1 if should continue */
+/*
+ * called by:
+ *   - arch/x86/domain.c|128| <<idle_loop>> if ( !scrub_free_pages() )
+ */
 int scrub_free_pages(void)
 {
     int order;
@@ -1963,6 +2022,16 @@ struct page_info *alloc_domheap_pages(
     return pg;
 }
 
+/*
+ * x86在以下使用:
+ *   - arch/x86/domain_build.c|288| <<alloc_chunk>> free_domheap_pages(page, free_order);
+ *   - arch/x86/domain_build.c|293| <<alloc_chunk>> free_domheap_pages(pg2, order);
+ *   - arch/x86/domain_build.c|661| <<construct_dom0>> free_domheap_pages(page, order);
+ *   - common/memory.c|642| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - common/memory.c|693| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - include/xen/mm.h|107| <<free_domheap_page>> #define free_domheap_page(p) (free_domheap_pages(p,0))
+ *   - include/xen/tmem_xen.h|140| <<__tmem_free_page_thispool>> free_domheap_pages(pi,0);
+ */
 void free_domheap_pages(struct page_info *pg, unsigned int order)
 {
     struct domain *d = page_get_owner(pg);
@@ -2094,6 +2163,16 @@ static __init int pagealloc_keyhandler_init(void)
 __initcall(pagealloc_keyhandler_init);
 
 
+/*
+ * called by:
+ *   - arch/x86/mm/p2m.c|1005| <<p2m_mem_paging_evict>> scrub_one_page(page);
+ *   - common/page_alloc.c|797| <<alloc_heap_pages>> scrub_one_page(&pg[i]);
+ *   - common/page_alloc.c|1442| <<smp_scrub_heap_pages>> scrub_one_page(pg);
+ *   - common/page_alloc.c|1622| <<__scrub_free_pages>> scrub_one_page(&pg[i]);
+ *   - common/tmem.c|1401| <<tmem_flush_npages>> scrub_one_page(pg);
+ *   - common/tmem.c|2795| <<tmem_relinquish_pages>> scrub_one_page(pfp);
+ *   - include/xen/tmem_xen.h|138| <<__tmem_free_page_thispool>> scrub_one_page(pi);
+ */
 void scrub_one_page(struct page_info *pg)
 {
     void *p;
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index 9e94eab..c7c81f4 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -322,6 +322,21 @@ struct arch_domain
         RELMEM_l2,
         RELMEM_done,
     } relmem;
+    /*
+     * 在以下修改relmem_list:
+     *   - arch/x86/domain.c|536| <<arch_domain_create>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/domain.c|2009| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2087| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2098| <<relinquish_memory>> page_list_move(list, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2173| <<domain_relinquish_resources>> page_list_splice(&d->arch.relmem_list, &d->page_list);
+     *   - arch/x86/domain.c|2174| <<domain_relinquish_resources>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/mm/p2m-pod.c|471| <<p2m_pod_offline_or_broken_hit>> page_list_add(p, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1980| <<free_domheap_pages>> page_list_del2(&pg[i], &d->xenpage_list, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1995| <<free_domheap_pages>> page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|404| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|419| <<iommu_populate_page_table>> page_list_move(&d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|424| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     */
     struct page_list_head relmem_list;
 
     cpuid_input_t *cpuids;
diff --git a/xen/include/asm-x86/mm.h b/xen/include/asm-x86/mm.h
index 56f2968..495a4f2 100644
--- a/xen/include/asm-x86/mm.h
+++ b/xen/include/asm-x86/mm.h
@@ -184,13 +184,24 @@ struct page_info
 #define PGT_l1_page_table PG_mask(1, 4)  /* using as an L1 page table?     */
 #define PGT_l2_page_table PG_mask(2, 4)  /* using as an L2 page table?     */
 #define PGT_l3_page_table PG_mask(3, 4)  /* using as an L3 page table?     */
+/* 0x4000000000000000 */
 #define PGT_l4_page_table PG_mask(4, 4)  /* using as an L4 page table?     */
 #define PGT_seg_desc_page PG_mask(5, 4)  /* using this page in a GDT/LDT?  */
+/* 0x7000000000000000 */
 #define PGT_writable_page PG_mask(7, 4)  /* has writable mappings?         */
 #define PGT_shared_page   PG_mask(8, 4)  /* CoW sharable page              */
 #define PGT_type_mask     PG_mask(15, 4) /* Bits 28-31 or 60-63.           */
 
  /* Owning guest has pinned this page to its current type? */
+/*
+ * 使用_PGT_pinned的地方:
+ *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+ *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+ */
 #define _PGT_pinned       PG_shift(5)
 #define PGT_pinned        PG_mask(1, 5)
  /* Has this page been validated for use as its current type? */
-- 
2.7.4

