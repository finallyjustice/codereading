From 336daa0840fb5d9a3a13bb2921e9f3177985a3ad Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 31 Jan 2020 15:20:41 -0800
Subject: [PATCH 1/1] xen-4.4.4-222-comment

xen-4.4.4-222

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 tools/libxc/xc_dom_x86.c       |   4 +
 tools/libxc/xc_domain.c        |  20 +++
 tools/libxc/xc_hvm_build_x86.c |  20 +++
 xen/arch/x86/domain.c          |  38 +++++
 xen/arch/x86/domain_page.c     |  17 +++
 xen/arch/x86/mm.c              |  16 ++
 xen/common/domain.c            |   4 +
 xen/common/memory.c            |  21 +++
 xen/common/page_alloc.c        | 269 +++++++++++++++++++++++++++++++++
 xen/include/asm-x86/domain.h   |  15 ++
 xen/include/asm-x86/guest_pt.h |   9 ++
 xen/include/asm-x86/mm.h       |  11 ++
 12 files changed, 444 insertions(+)

diff --git a/tools/libxc/xc_dom_x86.c b/tools/libxc/xc_dom_x86.c
index b2256f2c5e..6ea4408480 100644
--- a/tools/libxc/xc_dom_x86.c
+++ b/tools/libxc/xc_dom_x86.c
@@ -757,6 +757,10 @@ static int x86_shadow(xc_interface *xch, domid_t domid)
     return rc;
 }
 
+/*
+ * x86下调用的例子:
+ *   - libxc/xc_dom_boot.c|149| <<xc_dom_boot_mem_init>> rc = arch_setup_meminit(dom);
+ */
 int arch_setup_meminit(struct xc_dom_image *dom)
 {
     int rc;
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 251b49e728..d26b02b3f1 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -902,6 +902,12 @@ int xc_domain_claim_pages(xc_interface *xch,
     return err;
 }
 
+/*
+ * called by:
+ *   - libxc/xc_domain.c|943| <<xc_domain_populate_physmap_exact>> err = xc_domain_populate_physmap(xch, domid, nr_extents,
+ *   - libxc/xc_hvm_build_x86.c|484| <<setup_guest>> done = xc_domain_populate_physmap(xch, dom, nr_extents,
+ *   - libxc/xc_hvm_build_x86.c|525| <<setup_guest>> done = xc_domain_populate_physmap(xch, dom, nr_extents,
+ */
 int xc_domain_populate_physmap(xc_interface *xch,
                                uint32_t domid,
                                unsigned long nr_extents,
@@ -931,6 +937,20 @@ int xc_domain_populate_physmap(xc_interface *xch,
     return err;
 }
 
+/*
+ * x86下调用的例子:
+ *   - libxc/xc_dom_x86.c|796| <<arch_setup_meminit>> rc = xc_domain_populate_physmap_exact(dom->xch, dom->guest_domid,
+ *   - libxc/xc_dom_x86.c|901| <<arch_setup_meminit>> rc = xc_domain_populate_physmap_exact(dom->xch,
+ *   - libxc/xc_domain_restore.c|147| <<alloc_superpage_mfns>> if (xc_domain_populate_physmap_exact(xch, dom, max, SUPERPAGE_PFN_SHIFT,
+ *   - libxc/xc_domain_restore.c|211| <<uncanonicalize_pagetable>> rc = xc_domain_populate_physmap_exact(xch, dom, nr_mfns, 0, 0,
+ *   - libxc/xc_domain_restore.c|1149| <<apply_batch>> if ( xc_domain_populate_physmap_exact(xch, dom, 1,
+ *   - libxc/xc_domain_restore.c|1221| <<apply_batch>> rc = xc_domain_populate_physmap_exact(xch, dom, nr_mfns, 0, 0,
+ *   - libxc/xc_hvm_build_x86.c|418| <<setup_guest>> rc = xc_domain_populate_physmap_exact(
+ *   - libxc/xc_hvm_build_x86.c|543| <<setup_guest>> rc = xc_domain_populate_physmap_exact(
+ *   - libxc/xc_hvm_build_x86.c|583| <<setup_guest>> rc = xc_domain_populate_physmap_exact(xch, dom, 1, 0, 0, &pfn);
+ *   - tests/xen-access/xen-access.c|266| <<xenaccess_init>> rc = xc_domain_populate_physmap_exact(xenaccess->xc_handle,
+ *   - xenpaging/xenpaging.c|351| <<xenpaging_init>> rc = xc_domain_populate_physmap_exact(paging->xc_handle,
+ */
 int xc_domain_populate_physmap_exact(xc_interface *xch,
                                      uint32_t domid,
                                      unsigned long nr_extents,
diff --git a/tools/libxc/xc_hvm_build_x86.c b/tools/libxc/xc_hvm_build_x86.c
index 245f0d88fd..9426f22cce 100644
--- a/tools/libxc/xc_hvm_build_x86.c
+++ b/tools/libxc/xc_hvm_build_x86.c
@@ -33,8 +33,24 @@
 
 #include <xen/libelf/libelf.h>
 
+/*
+ * 在以下使用SUPERPAGE_2MB_SHIFT:
+ *   - libxc/xc_hvm_build_x86.c|37| <<SUPERPAGE_2MB_NR_PFNS>> #define SUPERPAGE_2MB_NR_PFNS (1UL << SUPERPAGE_2MB_SHIFT)
+ *   - libxc/xc_hvm_build_x86.c|522| <<setup_guest>> unsigned long nr_extents = count >> SUPERPAGE_2MB_SHIFT;
+ *   - libxc/xc_hvm_build_x86.c|527| <<setup_guest>> page_array[cur_pages+(i<<SUPERPAGE_2MB_SHIFT)];
+ *   - libxc/xc_hvm_build_x86.c|530| <<setup_guest>> SUPERPAGE_2MB_SHIFT,
+ *   - libxc/xc_hvm_build_x86.c|537| <<setup_guest>> done <<= SUPERPAGE_2MB_SHIFT;
+ */
 #define SUPERPAGE_2MB_SHIFT   9
 #define SUPERPAGE_2MB_NR_PFNS (1UL << SUPERPAGE_2MB_SHIFT)
+/*
+ * 在以下使用SUPERPAGE_1GB_SHIFT:
+ *   - libxc/xc_hvm_build_x86.c|39| <<SUPERPAGE_1GB_NR_PFNS>> #define SUPERPAGE_1GB_NR_PFNS (1UL << SUPERPAGE_1GB_SHIFT)
+ *   - libxc/xc_hvm_build_x86.c|481| <<setup_guest>> unsigned long nr_extents = count >> SUPERPAGE_1GB_SHIFT;
+ *   - libxc/xc_hvm_build_x86.c|486| <<setup_guest>> page_array[cur_pages+(i<<SUPERPAGE_1GB_SHIFT)];
+ *   - libxc/xc_hvm_build_x86.c|489| <<setup_guest>> SUPERPAGE_1GB_SHIFT,
+ *   - libxc/xc_hvm_build_x86.c|496| <<setup_guest>> done <<= SUPERPAGE_1GB_SHIFT;
+ */
 #define SUPERPAGE_1GB_SHIFT   18
 #define SUPERPAGE_1GB_NR_PFNS (1UL << SUPERPAGE_1GB_SHIFT)
 
@@ -229,6 +245,10 @@ static int check_mmio_hole(uint64_t start, uint64_t memsize,
         return 1;
 }
 
+/*
+ * called by:
+ *   - libxc/xc_hvm_build_x86.c|675| <<xc_hvm_build>> sts = setup_guest(xch, domid, &args, image, image_size);
+ */
 static int setup_guest(xc_interface *xch,
                        uint32_t dom, struct xc_hvm_build_args *args,
                        char *image, unsigned long image_size)
diff --git a/xen/arch/x86/domain.c b/xen/arch/x86/domain.c
index 8acc0c7d0c..6b30b10e59 100644
--- a/xen/arch/x86/domain.c
+++ b/xen/arch/x86/domain.c
@@ -119,6 +119,11 @@ static void play_dead(void)
     (*dead_idle)();
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|148| <<startup_cpu_idle_loop>> reset_stack_and_jump(idle_loop);
+ *   - arch/x86/domain.c|153| <<continue_idle_domain>> reset_stack_and_jump(idle_loop);
+ */
 static void idle_loop(void)
 {
     for ( ; ; )
@@ -1983,6 +1988,18 @@ int hypercall_xlat_continuation(unsigned int *id, unsigned int nr,
     return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/domain.c|2168| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->xenpage_list, ~0UL);
+ *   - arch/x86/domain.c|2175| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ *   - arch/x86/domain.c|2182| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l3_page_table);
+ *   - arch/x86/domain.c|2189| <<domain_relinquish_resources>> ret = relinquish_memory(d, &d->page_list, PGT_l2_page_table);
+ *
+ * page_list被很多地方使用,这里只是三个例子:
+ *   - common/page_alloc.c|2130| <<assign_pages>> page_list_add_tail(&pg[i], &d->page_list);
+ *   - arch/x86/domain.c|180| <<dump_pageframe_info>> page_list_for_each ( page, &d->page_list )
+ *   - arch/x86/numa.c|392| <<dump_numa>> page_list_for_each(page, &d->page_list)
+ */
 static int relinquish_memory(
     struct domain *d, struct page_list_head *list, unsigned long type)
 {
@@ -2003,6 +2020,17 @@ static int relinquish_memory(
             continue;
         }
 
+        /*
+	 * Clear a bit and return its old value
+	 *
+	 * 使用_PGT_pinned的地方:
+	 *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+	 *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+	 *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+	 *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+	 */
         if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
             ret = put_page_and_type_preemptible(page);
         switch ( ret )
@@ -2095,6 +2123,10 @@ static int relinquish_memory(
     return ret;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> rc = domain_relinquish_resources(d);
+ */
 int domain_relinquish_resources(struct domain *d)
 {
     int ret;
@@ -2172,6 +2204,12 @@ int domain_relinquish_resources(struct domain *d)
         /* fallthrough */
 
     case RELMEM_l4:
+        /*
+	 * page_list被很多地方使用,这里只是三个例子:
+	 *   - common/page_alloc.c|2130| <<assign_pages>> page_list_add_tail(&pg[i], &d->page_list);
+	 *   - arch/x86/domain.c|180| <<dump_pageframe_info>> page_list_for_each ( page, &d->page_list )
+	 *   - arch/x86/numa.c|392| <<dump_numa>> page_list_for_each(page, &d->page_list)
+	 */
         ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
         if ( ret )
             return ret;
diff --git a/xen/arch/x86/domain_page.c b/xen/arch/x86/domain_page.c
index 56f9bb1244..6ec449cbea 100644
--- a/xen/arch/x86/domain_page.c
+++ b/xen/arch/x86/domain_page.c
@@ -172,6 +172,23 @@ void *map_domain_page(unsigned long mfn)
     return (void *)MAPCACHE_VIRT_START + pfn_to_paddr(idx);
 }
 
+/*
+ * MAPCACHE_VCPU_ENTRIES = 0x00000010
+ * MAPCACHE_ENTRIES      = 0x0000000000020000
+ * MAPCACHE_VIRT_START   = 0xffff820040000000
+ * MAPCACHE_VIRT_END     = 0xffff820060000000
+ *
+ * DIRECTMAP_VIRT_START = 0xffff830000000000
+ * DIRECTMAP_SIZE       = 0x00007c8000000000
+ * DIRECTMAP_VIRT_END   = 0xffffff8000000000
+ *
+ * LINEAR_PT_VIRT_START = 0xffff810000000000
+ * LINEAR_PT_VIRT_END   = 0xffff818000000000
+ *
+ * HYPERVISOR_VIRT_START = 0xffff800000000000
+ * HYPERVISOR_VIRT_END   = 0xffff880000000000
+ */
+
 void unmap_domain_page(const void *ptr)
 {
     unsigned int idx;
diff --git a/xen/arch/x86/mm.c b/xen/arch/x86/mm.c
index 176893d7c5..ac35f743a7 100644
--- a/xen/arch/x86/mm.c
+++ b/xen/arch/x86/mm.c
@@ -158,6 +158,22 @@ struct rangeset *__read_mostly mmio_ro_ranges;
 
 bool_t __read_mostly opt_allow_superpage;
 bool_t __read_mostly opt_allow_hugepage;
+/*
+ * 在以下使用opt_allow_superpage:
+ *   - arch/x86/domain_build.c|1029| <<construct_dom0>> if ( opt_allow_superpage )
+ *   - arch/x86/mm.c|169| <<L2_DISALLOW_MASK>> #define L2_DISALLOW_MASK (unlikely(opt_allow_superpage) \
+ *   - arch/x86/mm.c|252| <<init_frametable>> opt_allow_superpage = 1;
+ *   - arch/x86/mm.c|273| <<init_frametable>> if (opt_allow_superpage)
+ *   - arch/x86/mm.c|1071| <<get_page_from_l2e>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|2679| <<mark_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2722| <<unmark_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2754| <<clear_superpage_mark>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|2769| <<get_superpage>> ASSERT(opt_allow_superpage);
+ *   - arch/x86/mm.c|2809| <<put_superpage>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm.c|3537| <<do_mmuext_op(MMUEXT_MARK_SUPER)>> if ( !opt_allow_superpage )
+ *   - arch/x86/mm/shadow/common.c|57| <<shadow_domain_init>> if ( !is_pv_domain(d) || opt_allow_superpage )
+ *   - include/asm-x86/guest_pt.h|191| <<guest_supports_superpages>> ? opt_allow_superpage
+ */
 boolean_param("allowsuperpage", opt_allow_superpage);
 boolean_param("allowhugepage", opt_allow_hugepage);
 
diff --git a/xen/common/domain.c b/xen/common/domain.c
index e44d403d96..735834ff6a 100644
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -612,6 +612,10 @@ int rcu_lock_live_remote_domain_by_id(domid_t dom, struct domain **d)
     return 0;
 }
 
+/*
+ * called by:
+ *   - common/domctl.c|726| <<do_domctl(XEN_DOMCTL_destroydomain)>> ret = domain_kill(d);
+ */
 int domain_kill(struct domain *d)
 {
     int rc = 0;
diff --git a/xen/common/memory.c b/xen/common/memory.c
index a07b18a9dc..bdc111d8ad 100644
--- a/xen/common/memory.c
+++ b/xen/common/memory.c
@@ -1100,6 +1100,27 @@ long do_memory_op(unsigned long cmd, XEN_GUEST_HANDLE_PARAM(void) arg)
     }
 
     case XENMEM_claim_pages:
+    /*
+     * Attempt to stake a claim for a domain on a quantity of pages
+     * of system RAM, but _not_ assign specific pageframes.  Only
+     * arithmetic is performed so the hypercall is very fast and need
+     * not be preemptible, thus sidestepping time-of-check-time-of-use
+     * races for memory allocation.  Returns 0 if the hypervisor page
+     * allocator has atomically and successfully claimed the requested
+     * number of pages, else non-zero.
+     *   
+     * Any domain may have only one active claim.  When sufficient memory
+     * has been allocated to resolve the claim, the claim silently expires.
+     * Claiming zero pages effectively resets any outstanding claim and
+     * is always successful.
+     *           
+     * Note that a valid claim may be staked even after memory has been
+     * allocated for a domain.  In this case, the claim is not incremental,
+     * i.e. if the domain's tot_pages is 3, and a claim is staked for 10,
+     * only 7 additional pages are claimed.
+     *       
+     * Caller must be privileged or the hypercall fails.
+     */
         if ( copy_from_guest(&reservation, arg, 1) )
             return -EFAULT;
 
diff --git a/xen/common/page_alloc.c b/xen/common/page_alloc.c
index 95db602e7c..c84bbfd804 100644
--- a/xen/common/page_alloc.c
+++ b/xen/common/page_alloc.c
@@ -51,6 +51,112 @@
 #define p2m_pod_offline_or_broken_replace(pg) BUG_ON(pg != NULL)
 #endif
 
+/*
+ * Here is how we expect memory pages to be reclaimed during VM destroy, that
+ * is, when we run "xm destroy" on dom0.
+ *
+ * "XendDomainInfo.destroy" is printed by below line 3842.
+ *
+ * 3830     def destroy(self):
+ * 3831         """Cleanup VM and destroy domain.  Nothrow guarantee."""
+ * 3832
+ * 3833         if self.domid is None:
+ * 3834             return
+ * 3835
+ * 3836         if self.destroying == False:
+ * 3837             self.destroying = True
+ * 3838         else:
+ * 3839             raise VmError("Domain (domid=%s) is destroying, please wait!", str(self.domid))
+ * 3840
+ * 3841         from xen.xend import XendDomain
+ * 3842         log.debug("XendDomainInfo.destroy: domid=%s", str(self.domid))
+ * 3843
+ * 3844         paths = self._prepare_phantom_paths()
+ * 3845
+ * 3846         if self.dompath is not None:
+ * 3847             try:
+ * 3848                 xc.domain_destroy_hook(self.domid)
+ * 3849                 xc.domain_pause(self.domid)
+ * 3850                 do_FLR(self.domid, self.info.is_hvm())
+ * 3851                 xc.domain_destroy(self.domid)
+ * 3852                 for state in DOM_STATES_OLD:
+ * 3853                     self.info[state] = 0
+ * 3854                 self._stateSet(DOM_STATE_HALTED)
+ *
+ * xend would run destroy() in python/xen/xend/XendDomainInfo.py, which would
+ * call xc_domain_destroy() in libxc, after printing "DEBUG
+ * (XendDomainInfo:3789) XendDomainInfo.destroy: domid=18".
+ *
+ * 83 int xc_domain_destroy(xc_interface *xch,
+ * 84                       uint32_t domid)
+ * 85 {
+ * 86     int ret;
+ * 87     DECLARE_DOMCTL;
+ * 88     domctl.cmd = XEN_DOMCTL_destroydomain;
+ * 89     domctl.domain = (domid_t)domid;
+ * 90     do {
+ * 91         ret = do_domctl(xch, &domctl);
+ * 92     } while ( ret && (errno == EAGAIN) );
+ * 93     return ret;
+ * 94 }
+ *
+ * xc_domain_destroy() traps to hypervisor via XEN_DOMCTL_destroydomain() and
+ * this would persists until the destroy is completed by hypervisor.
+ *
+ * This can be confirmed by oswatcher in sosreport that cpu usage of xend is
+ * always 100% in top command.
+ *
+ * In hypervisor side, XEN_DOMCTL_destroydomain is handled by
+ * do_domctl()->domain_kill():
+ *
+ * domain_kill()
+ *  -> domain_relinquish_resources()
+ *
+ * As shown in below code, if domain_relinquish_resources() cannot return 0 with
+ * all memory pages reclaimed, xend would trap to run again until all pages are
+ * reclaimed. I have verified with mini-os whose memory is only 16MB.
+ *
+ * 637     case DOMDYING_dying:
+ * 638         rc = domain_relinquish_resources(d);
+ * 639         if ( rc != 0 )
+ * 640         {
+ * 641             if ( rc == -ERESTART )
+ * 642                 rc = -EAGAIN;
+ * 643             break;
+ * 644         }
+ *
+ * Most memory pages (indeed regular pages) are reclaimed at relinquish_memory()
+ * at line 2175. Although the argument at 2175 is PGT_l4_page_table, regular
+ * pages are reclaimed as well.
+ *
+ * 2098 int domain_relinquish_resources(struct domain *d)
+ * 2099 {
+ * ... ...
+ * 2174     case RELMEM_l4:
+ * 2175         ret = relinquish_memory(d, &d->page_list, PGT_l4_page_table);
+ * 2176         if ( ret )
+ * 2177             return ret;
+ * 2178         d->arch.relmem = RELMEM_l3;
+ * 2179         // fallthrough
+ *
+ * According to my test, line 2040 should return true so that the page type does
+ * not need to be PGT_l4_page_table.
+ *
+ * 1986 static int relinquish_memory(
+ * 1987     struct domain *d, struct page_list_head *list, unsigned long type)
+ * 1988 {
+ * ... ...
+ * 2040             if ( likely((x & PGT_type_mask) != type) ||
+ * 2041                  likely(!(x & (PGT_validated|PGT_partial))) )
+ * 2042                 break;
+ * ... ...
+ * 2079         // Put the page on the list and /then/ potentially free it.
+ * 2080         page_list_add_tail(page, &d->arch.relmem_list);
+ * 2081         put_page(page);
+ *
+ * Until after line 2081, the d->tot_pages would not be decremented.
+ */
+
 /*
  * Comma-separated list of hexadecimal page numbers containing bad bytes.
  * e.g. 'badpage=0x3f45,0x8a321'.
@@ -81,6 +187,13 @@ integer_param("dma_bits", dma_bitsize);
 #define round_pgdown(_p)  ((_p)&PAGE_MASK)
 #define round_pgup(_p)    (((_p)+(PAGE_SIZE-1))&PAGE_MASK)
 
+/*
+ * 在以下使用pglist_lock:
+ *   - common/page_alloc.c|877| <<reserve_offlined_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|883| <<reserve_offlined_page>> spin_unlock(&pglist_lock);
+ *   - common/page_alloc.c|1210| <<online_page>> spin_lock(&pglist_lock);
+ *   - common/page_alloc.c|1241| <<online_page>> spin_unlock(&pglist_lock);
+ */
 static DEFINE_SPINLOCK(pglist_lock);
 /* Offlined page list, protected by pglist_lock. */
 static PAGE_LIST_HEAD(page_offlined_list);
@@ -88,9 +201,35 @@ static PAGE_LIST_HEAD(page_offlined_list);
 static PAGE_LIST_HEAD(page_broken_list);
 
 /* A rough flag to indicate whether a node have need_scrub pages */
+/*
+ * 修改node_need_scrub[]的地方:
+ *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+ *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+ *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+ * 在以下使用node_need_scrub:
+ *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+ *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+ *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+ *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+ */
 static unsigned long node_need_scrub[MAX_NUMNODES];
+/*
+ * 在以下使用is_scrubbing:
+ *   - common/page_alloc.c|1673| <<scrub_free_pages>> if ( test_and_set_bool(per_cpu(is_scrubbing, cpumask_first(per_cpu(cpu_sibling_mask, cpu)))) )
+ *   - common/page_alloc.c|1736| <<scrub_free_pages>> was_scrubbing = test_and_clear_bool(per_cpu(is_scrubbing,
+ */
 static DEFINE_PER_CPU(bool_t, is_scrubbing);
+/*
+ * 在以下使用scrub_list_cpu:
+ *   - common/page_alloc.c|1611| <<__scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ *   - common/page_alloc.c|1666| <<scrub_free_pages>> struct page_list_head *local_scrub_list = &this_cpu(scrub_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, scrub_list_cpu);
+/*
+ * 在以下使用free_list_cpu:
+ *   - common/page_alloc.c|1612| <<__scrub_free_pages>> struct page_list_head *local_free_list = &this_cpu(free_list_cpu);
+ */
 static DEFINE_PER_CPU(struct page_list_head, free_list_cpu);
 
 /*************************
@@ -120,6 +259,12 @@ static void __init boot_bug(int line)
 }
 #define BOOT_BUG_ON(p) if ( p ) boot_bug(__LINE__);
 
+/*
+ * called by:
+ *   - common/page_alloc.c|203| <<bootmem_region_zap>> bootmem_region_add(e, _e);
+ *   - common/page_alloc.c|224| <<init_boot_pages>> bootmem_region_add(ps >> PAGE_SHIFT, pe >> PAGE_SHIFT);
+ *   - common/page_alloc.c|302| <<alloc_boot_pages>> bootmem_region_add(pg + nr_pfns, _e);
+ */
 static void __init bootmem_region_add(unsigned long s, unsigned long e)
 {
     unsigned int i;
@@ -301,8 +446,31 @@ static long midsize_alloc_zone_pages;
 static DEFINE_SPINLOCK(heap_lock_globals);
 static spinlock_t heap_lock[MAX_NUMNODES];
 
+/*
+ * 在以下使用outstanding_claims:
+ *   - common/page_alloc.c|380| <<domain_adjust_tot_pages>> sys_before = outstanding_claims;
+ *   - common/page_alloc.c|383| <<domain_adjust_tot_pages>> outstanding_claims = sys_after;
+ *   - common/page_alloc.c|406| <<domain_set_outstanding_pages>> outstanding_claims -= d->outstanding_pages;
+ *   - common/page_alloc.c|439| <<domain_set_outstanding_pages>> avail_pages -= outstanding_claims;
+ *   - common/page_alloc.c|451| <<domain_set_outstanding_pages>> outstanding_claims += d->outstanding_pages;
+ *   - common/page_alloc.c|463| <<get_outstanding_claims>> *outstanding_pages = outstanding_claims;
+ *   - common/page_alloc.c|709| <<alloc_heap_pages>> if ( (outstanding_claims + request >
+ *   - common/page_alloc.c|737| <<alloc_heap_pages>> (opt_tmem ? tmem_freeable_pages() : 0) - outstanding_claims;
+ */
 static long outstanding_claims; /* total outstanding claims by all domains */
 
+/*
+ * called by:
+ *   - arch/x86/mm.c|4419| <<donate_page>> domain_adjust_tot_pages(d, 1);
+ *   - arch/x86/mm.c|4483| <<steal_page>> if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )
+ *   - arch/x86/mm/mem_sharing.c|659| <<page_make_sharable>> drop_dom_ref = !domain_adjust_tot_pages(d, -1);
+ *   - arch/x86/mm/mem_sharing.c|706| <<page_make_private>> if ( domain_adjust_tot_pages(d, 1) == 1 )
+ *   - common/grant_table.c|1995| <<gnttab_transfer>> if ( unlikely(domain_adjust_tot_pages(e, 1) == 1) )
+ *   - common/grant_table.c|2010| <<gnttab_transfer>> bool_t drop_dom_ref = !domain_adjust_tot_pages(e, -1);
+ *   - common/memory.c|636| <<memory_exchange>> !domain_adjust_tot_pages(d, -dec_count));
+ *   - common/page_alloc.c|1967| <<assign_pages>> domain_adjust_tot_pages(d, 1 << order);
+ *   - common/page_alloc.c|2067| <<free_domheap_pages>> drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
+ */
 unsigned long domain_adjust_tot_pages(struct domain *d, long pages)
 {
     long dom_before, dom_after, dom_claimed, sys_before, sys_after;
@@ -336,6 +504,11 @@ out:
     return d->tot_pages;
 }
 
+/*
+ * called by:
+ *   - common/domain.c|638| <<domain_kill>> domain_set_outstanding_pages(d, 0);
+ *   - common/memory.c|1122| <<do_mem_op(XENMEM_claim_pages)>> rc = domain_set_outstanding_pages(d, reservation.nr_extents);
+ */
 int domain_set_outstanding_pages(struct domain *d, unsigned long pages)
 {
     int ret = -ENOMEM;
@@ -602,6 +775,12 @@ static void check_low_mem_virq(unsigned long avail_pages)
 }
 
 /* Allocate 2^@order contiguous pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1812| <<alloc_xenheap_pages>> pg = alloc_heap_pages(MEMZONE_XEN, MEMZONE_XEN,
+ *   - common/page_alloc.c|1988| <<alloc_domheap_pages>> pg = alloc_heap_pages(dma_zone + 1, zone_hi, order, memflags, d);
+ *   - common/page_alloc.c|1992| <<alloc_domheap_pages>> ((pg = alloc_heap_pages(MEMZONE_XEN + 1, zone_hi, order,
+ */
 static struct page_info *alloc_heap_pages(
     unsigned int zone_lo, unsigned int zone_hi,
     unsigned int order, unsigned int memflags,
@@ -951,6 +1130,18 @@ static void merge_free_trunks(struct page_info *pg, unsigned int order,
 }
 
 /* Free 2^@order set of pages. */
+/*
+ * called by:
+ *   - common/page_alloc.c|1295| <<online_page>> free_heap_pages(pg, 0, 0);
+ *   - common/page_alloc.c|1366| <<init_heap_pages>> free_heap_pages(pg+i, 0, 0);
+ *   - common/page_alloc.c|1863| <<free_xenheap_pages>> free_heap_pages(virt_to_page(v), order, 0);
+ *   - common/page_alloc.c|1921| <<free_xenheap_pages>> free_heap_pages(pg, order, 1);
+ *   - common/page_alloc.c|2030| <<alloc_domheap_pages>> free_heap_pages(pg, order, 0);
+ *   - common/page_alloc.c|2089| <<free_domheap_pages>> free_heap_pages(pg, order, 1);
+ *   - common/page_alloc.c|2091| <<free_domheap_pages>> free_heap_pages(pg, order, 0);
+ *   - common/page_alloc.c|2096| <<free_domheap_pages>> free_heap_pages(pg, 0, 1);
+ *   - common/page_alloc.c|2102| <<free_domheap_pages>> free_heap_pages(pg, order, 1);
+ */
 static void free_heap_pages(
     struct page_info *pg, unsigned int order, bool_t need_scrub)
 {
@@ -1391,6 +1582,15 @@ void __init end_boot_allocator(void)
     printk("\n");
 }
 
+/*
+ * called by:
+ *   - common/page_alloc.c|1561| <<scrub_heap_pages>> on_selected_cpus(&all_worker_cpus, smp_scrub_heap_pages, NULL, 1);
+ *   - common/page_alloc.c|1618| <<scrub_heap_pages>> on_selected_cpus(&node_cpus, smp_scrub_heap_pages, &region[i], 1);
+ *
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ */
 static void __init smp_scrub_heap_pages(void *data)
 {
     unsigned long mfn, start, end;
@@ -1464,6 +1664,10 @@ int __init find_non_smt(unsigned int node, cpumask_t *dest)
  * Scrub all unallocated pages in all heap zones. This function uses all
  * online cpu's to scrub the memory in parallel.
  */
+/*
+ * x86下的调用:
+ *   - arch/x86/setup.c|1545| <<__start_xen>> scrub_heap_pages();
+ */
 void __init scrub_heap_pages(void)
 {
     cpumask_t node_cpus, all_worker_cpus;
@@ -1599,6 +1803,10 @@ void __init scrub_heap_pages(void)
 
 #define SCRUB_BATCH_ORDER 12
 /* return 1 if should continue */
+/*
+ * called by:
+ *   - common/page_alloc.c|1726| <<scrub_free_pages>> } while ( __scrub_free_pages(node, cpu) );
+ */
 static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 {
     struct page_info *pg, *tmp;
@@ -1648,6 +1856,10 @@ static int __scrub_free_pages(unsigned int node, unsigned int cpu)
 }
 
 /* return 1 if should continue */
+/*
+ * called by:
+ *   - arch/x86/domain.c|128| <<idle_loop>> if ( !scrub_free_pages() )
+ */
 int scrub_free_pages(void)
 {
     int order;
@@ -1874,6 +2086,13 @@ void init_domheap_pages(paddr_t ps, paddr_t pe)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/domain_build.c|674| <<construct_dom0>> if ( assign_pages(d, mfn_to_page(mfn++), 0, 0) )
+ *   - common/memory.c|617| <<memory_exchaeng>> if ( assign_pages(d, page, exch.out.extent_order,
+ *   - common/memory.c|682| <<memory_exchange>> if ( assign_pages(d, page, 0, MEMF_no_refcount) )
+ *   - common/page_alloc.c|2169| <<alloc_domheap_pages>> assign_pages(d, pg, order, memflags) )
+ */
 int assign_pages(
     struct domain *d,
     struct page_info *pg,
@@ -1963,6 +2182,16 @@ struct page_info *alloc_domheap_pages(
     return pg;
 }
 
+/*
+ * x86在以下使用:
+ *   - arch/x86/domain_build.c|288| <<alloc_chunk>> free_domheap_pages(page, free_order);
+ *   - arch/x86/domain_build.c|293| <<alloc_chunk>> free_domheap_pages(pg2, order);
+ *   - arch/x86/domain_build.c|661| <<construct_dom0>> free_domheap_pages(page, order);
+ *   - common/memory.c|642| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - common/memory.c|693| <<memory_exchange>> free_domheap_pages(page, exch.out.extent_order);
+ *   - include/xen/mm.h|107| <<free_domheap_page>> #define free_domheap_page(p) (free_domheap_pages(p,0))
+ *   - include/xen/tmem_xen.h|140| <<__tmem_free_page_thispool>> free_domheap_pages(pi,0);
+ */
 void free_domheap_pages(struct page_info *pg, unsigned int order)
 {
     struct domain *d = page_get_owner(pg);
@@ -1995,6 +2224,18 @@ void free_domheap_pages(struct page_info *pg, unsigned int order)
             page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
         }
 
+	/*
+	 * called by:
+	 *   - arch/x86/mm.c|4419| <<donate_page>> domain_adjust_tot_pages(d, 1);
+	 *   - arch/x86/mm.c|4483| <<steal_page>> if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )
+	 *   - arch/x86/mm/mem_sharing.c|659| <<page_make_sharable>> drop_dom_ref = !domain_adjust_tot_pages(d, -1);
+	 *   - arch/x86/mm/mem_sharing.c|706| <<page_make_private>> if ( domain_adjust_tot_pages(d, 1) == 1 )
+	 *   - common/grant_table.c|1995| <<gnttab_transfer>> if ( unlikely(domain_adjust_tot_pages(e, 1) == 1) )
+	 *   - common/grant_table.c|2010| <<gnttab_transfer>> bool_t drop_dom_ref = !domain_adjust_tot_pages(e, -1);
+	 *   - common/memory.c|636| <<memory_exchange>> !domain_adjust_tot_pages(d, -dec_count));
+	 *   - common/page_alloc.c|1967| <<assign_pages>> domain_adjust_tot_pages(d, 1 << order);
+	 *   - common/page_alloc.c|2067| <<free_domheap_pages>> drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
+	 */
         drop_dom_ref = !domain_adjust_tot_pages(d, -(1 << order));
 
         spin_unlock_recursive(&d->page_alloc_lock);
@@ -2094,6 +2335,22 @@ static __init int pagealloc_keyhandler_init(void)
 __initcall(pagealloc_keyhandler_init);
 
 
+/*
+ * called by:
+ *   - arch/x86/mm/p2m.c|1005| <<p2m_mem_paging_evict>> scrub_one_page(page);
+ *   - common/page_alloc.c|797| <<alloc_heap_pages>> scrub_one_page(&pg[i]);
+ *   - common/page_alloc.c|1442| <<smp_scrub_heap_pages>> scrub_one_page(pg);
+ *   - common/page_alloc.c|1622| <<__scrub_free_pages>> scrub_one_page(&pg[i]);
+ *   - common/tmem.c|1401| <<tmem_flush_npages>> scrub_one_page(pg);
+ *   - common/tmem.c|2795| <<tmem_relinquish_pages>> scrub_one_page(pfp);
+ *   - include/xen/tmem_xen.h|138| <<__tmem_free_page_thispool>> scrub_one_page(pi);
+ *
+ * 从smp_scrub_heap_pages()进来的话只能是初始化的时候:
+ * __start_xen()
+ *  -> scrub_heap_pages()
+ *      -> smp_scrub_heap_pages()
+ *          -> scrub_one_page()
+ */
 void scrub_one_page(struct page_info *pg)
 {
     void *p;
@@ -2140,6 +2397,18 @@ static void dump_heap(unsigned char key)
     }
     for ( i = 0; i < MAX_NUMNODES; i++ )
     {
+        /*
+	 * 修改node_need_scrub[]的地方:
+         *   - common/page_alloc.c|790| <<alloc_heap_pages>> node_need_scrub[node] -= (1 << order);
+         *   - common/page_alloc.c|1011| <<free_heap_pages>> node_need_scrub[node] += (1 << order);
+         *   - common/page_alloc.c|1647| <<__scrub_free_pages>> node_need_scrub[node] -= (1 << order);
+         * 在以下使用node_need_scrub:
+         *   - common/page_alloc.c|422| <<get_dirty_pages>> dirty_pages = node_need_scrub[node];
+         *   - common/page_alloc.c|1686| <<scrub_free_pages>> if ( node_need_scrub[node] && page_list_empty(local_scrub_list) )
+         *   - common/page_alloc.c|1741| <<scrub_free_pages>> if ( !node_need_scrub[node] && page_list_empty(local_scrub_list) )
+         *   - common/page_alloc.c|2171| <<dump_heap>> if ( !node_need_scrub[i] )
+         *   - common/page_alloc.c|2173| <<dump_heap>> printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
+	 */
         if ( !node_need_scrub[i] )
             continue;
         printk("Node %d has %lu unscrubbed pages\n", i, node_need_scrub[i]);
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index 9e94eabfb7..c7c81f487a 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -322,6 +322,21 @@ struct arch_domain
         RELMEM_l2,
         RELMEM_done,
     } relmem;
+    /*
+     * 在以下修改relmem_list:
+     *   - arch/x86/domain.c|536| <<arch_domain_create>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/domain.c|2009| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2087| <<relinquish_memory>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2098| <<relinquish_memory>> page_list_move(list, &d->arch.relmem_list);
+     *   - arch/x86/domain.c|2173| <<domain_relinquish_resources>> page_list_splice(&d->arch.relmem_list, &d->page_list);
+     *   - arch/x86/domain.c|2174| <<domain_relinquish_resources>> INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);
+     *   - arch/x86/mm/p2m-pod.c|471| <<p2m_pod_offline_or_broken_hit>> page_list_add(p, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1980| <<free_domheap_pages>> page_list_del2(&pg[i], &d->xenpage_list, &d->arch.relmem_list);
+     *   - common/page_alloc.c|1995| <<free_domheap_pages>> page_list_del2(&pg[i], &d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|404| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|419| <<iommu_populate_page_table>> page_list_move(&d->page_list, &d->arch.relmem_list);
+     *   - drivers/passthrough/iommu.c|424| <<iommu_populate_page_table>> page_list_add_tail(page, &d->arch.relmem_list);
+     */
     struct page_list_head relmem_list;
 
     cpuid_input_t *cpuids;
diff --git a/xen/include/asm-x86/guest_pt.h b/xen/include/asm-x86/guest_pt.h
index 6ce65892f8..d940924b83 100644
--- a/xen/include/asm-x86/guest_pt.h
+++ b/xen/include/asm-x86/guest_pt.h
@@ -181,6 +181,15 @@ static inline guest_l4e_t guest_l4e_from_gfn(gfn_t gfn, u32 flags)
 
 /* Which pagetable features are supported on this vcpu? */
 
+/*
+ * called by:
+ *   - arch/x86/mm/guest_walk.c|290| <<guest_walk_tables>> pse2M = (gflags & _PAGE_PSE) && guest_supports_superpages(v);
+ *   - arch/x86/mm/shadow/multi.c|243| <<shadow_check_gwalk>> if ( !(guest_supports_superpages(v) &&
+ *   - arch/x86/mm/shadow/multi.c|314| <<gw_remove_write_accesses>> if ( !(guest_supports_superpages(v) &&
+ *   - arch/x86/mm/shadow/multi.c|653| <<_sh_propagate>> guest_supports_superpages(v)))
+ *   - arch/x86/mm/shadow/multi.c|1860| <<shadow_get_and_create_l1e>> if ( guest_supports_superpages(v) && (flags & _PAGE_PSE) )
+ *   - arch/x86/mm/shadow/multi.c|2268| <<validate_gl2e>> if ( guest_supports_superpages(v) &&
+ */
 static inline int
 guest_supports_superpages(struct vcpu *v)
 {
diff --git a/xen/include/asm-x86/mm.h b/xen/include/asm-x86/mm.h
index 56f2968d39..495a4f2305 100644
--- a/xen/include/asm-x86/mm.h
+++ b/xen/include/asm-x86/mm.h
@@ -184,13 +184,24 @@ struct page_info
 #define PGT_l1_page_table PG_mask(1, 4)  /* using as an L1 page table?     */
 #define PGT_l2_page_table PG_mask(2, 4)  /* using as an L2 page table?     */
 #define PGT_l3_page_table PG_mask(3, 4)  /* using as an L3 page table?     */
+/* 0x4000000000000000 */
 #define PGT_l4_page_table PG_mask(4, 4)  /* using as an L4 page table?     */
 #define PGT_seg_desc_page PG_mask(5, 4)  /* using this page in a GDT/LDT?  */
+/* 0x7000000000000000 */
 #define PGT_writable_page PG_mask(7, 4)  /* has writable mappings?         */
 #define PGT_shared_page   PG_mask(8, 4)  /* CoW sharable page              */
 #define PGT_type_mask     PG_mask(15, 4) /* Bits 28-31 or 60-63.           */
 
  /* Owning guest has pinned this page to its current type? */
+/*
+ * 使用_PGT_pinned的地方:
+ *   - arch/x86/domain.c|2013| <<relinquish_memory>> if ( test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/domain.c|2023| <<relinquish_memory>> set_bit(_PGT_pinned, &page->u.inuse.type_info);
+ *   - arch/x86/mm.c|3223| <<do_mmuext_op>> else if ( unlikely(test_and_set_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3242| <<do_mmuext_op>> test_and_clear_bit(_PGT_pinned,
+ *   - arch/x86/mm.c|3275| <<do_mmuext_op>> if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )
+ *   - arch/x86/mm/p2m-pod.c|282| <<p2m_pod_set_cache_target>> if ( test_and_clear_bit(_PGT_pinned, &(page+i)->u.inuse.type_info) )
+ */
 #define _PGT_pinned       PG_shift(5)
 #define PGT_pinned        PG_mask(1, 5)
  /* Has this page been validated for use as its current type? */
-- 
2.17.1

