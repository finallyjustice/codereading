From e56a07801a012c4d3b3d875537663120ca747716 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 25 Mar 2024 15:26:17 -0700
Subject: [PATCH 1/1] linux v5.15.152

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/events/core.c          | 122 +++++++++++
 arch/x86/events/perf_event.h    |  61 ++++++
 arch/x86/include/asm/kvm_host.h |  19 ++
 arch/x86/kvm/pmu.c              |  27 +++
 include/linux/perf_event.h      |  33 +++
 include/linux/sched.h           |  35 +++
 kernel/events/core.c            | 375 ++++++++++++++++++++++++++++++++
 kernel/sched/core.c             |  25 +++
 8 files changed, 697 insertions(+)

diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 81d5e0a1f..a152f87b2 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -724,6 +724,8 @@ static void x86_pmu_disable(struct pmu *pmu)
 	cpuc->enabled = 0;
 	barrier();
 
+	/*
+	 */
 	static_call(x86_pmu_disable_all)();
 }
 
@@ -854,6 +856,10 @@ static bool perf_sched_restore_state(struct perf_sched *sched)
  * Select a counter for the current event to schedule. Return true on
  * success.
  */
+/*
+ * called by:
+ *   - arch/x86/events/core.c|914| <<perf_sched_find_counter>> while (!__perf_sched_find_counter(sched)) {
+ */
 static bool __perf_sched_find_counter(struct perf_sched *sched)
 {
 	struct event_constraint *c;
@@ -901,6 +907,16 @@ static bool __perf_sched_find_counter(struct perf_sched *sched)
 	return false;
 
 done:
+	/*
+	 * struct perf_sched *sched:
+	 * -> struct sched_state state;
+	 *    -> int     weight;
+	 *    -> int     event;          // event index
+	 *    -> int     counter;        // counter index
+	 *    -> int     unassigned;     // number of events to be assigned left
+	 *    -> int     nr_gp;          // number of GP counters used
+	 *    -> u64     used;
+	 */
 	sched->state.counter = idx;
 
 	if (c->overlap)
@@ -909,6 +925,10 @@ static bool __perf_sched_find_counter(struct perf_sched *sched)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|962| <<perf_assign_events>> if (!perf_sched_find_counter(&sched))
+ */
 static bool perf_sched_find_counter(struct perf_sched *sched)
 {
 	while (!__perf_sched_find_counter(sched)) {
@@ -951,6 +971,11 @@ static bool perf_sched_next_event(struct perf_sched *sched)
 /*
  * Assign a counter for each event.
  */
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1075| <<x86_schedule_events>> unsched = perf_assign_events(cpuc->event_constraint, n, wmin,
+ *   - arch/x86/events/intel/uncore.c|482| <<uncore_assign_events>> ret = perf_assign_events(box->event_constraint, n,
+ */
 int perf_assign_events(struct event_constraint **constraints, int n,
 			int wmin, int wmax, int gpmax, int *assign)
 {
@@ -969,6 +994,23 @@ int perf_assign_events(struct event_constraint **constraints, int n,
 }
 EXPORT_SYMBOL_GPL(perf_assign_events);
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|73| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_schedule_events, *x86_pmu.schedule_events);
+ *   - arch/x86/events/core.c|1514| <<x86_pmu_add>> ret = static_call(x86_pmu_schedule_events)(cpuc, n, assign);
+ *   - arch/x86/events/core.c|2055| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_schedule_events, x86_pmu.schedule_events);
+ *   - arch/x86/events/core.c|2338| <<x86_pmu_commit_txn>> ret = static_call(x86_pmu_schedule_events)(cpuc, n, assign);
+ *
+ * 在以下使用x86_schedule_events:
+ *   - arch/x86/events/amd/core.c|916| <<global>> struct x86_pmu amd_pmu.schedule_events = x86_schedule_events, --> "AMD"
+ *   - arch/x86/events/intel/core.c|4569| <<global>> struct x86_pmu core_pmu.schedule_events = x86_schedule_events, --> "core"
+ *   - arch/x86/events/intel/core.c|4620| <<global>> struct x86_pmu intel_pmu.schedule_events = x86_schedule_events, --> "Intel"
+ *   - arch/x86/events/intel/knc.c|298| <<global>> struct x86_pmu knc_pmu.schedule_events = x86_schedule_events, --> "knc"
+ *   - arch/x86/events/intel/p6.c|209| <<global>> struct x86_pmu p6_pmu.schedule_events = x86_schedule_events, --> "p6"
+ *   - arch/x86/events/zhaoxin/core.c|467| <<global>> struct x86_pmu zhaoxin_pmu.schedule_events = x86_schedule_events, --> "zhaoxin"
+ *
+ * 猜测是为了n分配??
+ */
 int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)
 {
 	int num_counters = hybrid(cpuc->pmu, num_counters);
@@ -1072,6 +1114,11 @@ int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)
 			WARN_ON(gpmax <= 0);
 		}
 
+		/*
+		 * called by:
+		 *   - arch/x86/events/core.c|1075| <<x86_schedule_events>> unsched = perf_assign_events(cpuc->event_constraint, n, wmin,
+		 *   - arch/x86/events/intel/uncore.c|482| <<uncore_assign_events>> ret = perf_assign_events(box->event_constraint, n,
+		 */
 		unsched = perf_assign_events(cpuc->event_constraint, n, wmin,
 					     wmax, gpmax, assign);
 	}
@@ -1107,6 +1154,10 @@ int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)
 	return unsched ? -EINVAL : 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1180| <<collect_event>> if (intel_cap.perf_metrics && add_nr_metric_event(cpuc, event))
+ */
 static int add_nr_metric_event(struct cpu_hw_events *cpuc,
 			       struct perf_event *event)
 {
@@ -1120,6 +1171,10 @@ static int add_nr_metric_event(struct cpu_hw_events *cpuc,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1749| <<x86_pmu_del>> del_nr_metric_event(cpuc, event);
+ */
 static void del_nr_metric_event(struct cpu_hw_events *cpuc,
 				struct perf_event *event)
 {
@@ -1127,6 +1182,11 @@ static void del_nr_metric_event(struct cpu_hw_events *cpuc,
 		cpuc->n_metric--;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1228| <<collect_events>> if (collect_event(cpuc, leader, max_count, n))
+ *   - arch/x86/events/core.c|1240| <<collect_events>> if (collect_event(cpuc, event, max_count, n))
+ */
 static int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,
 			 int max_count, int n)
 {
@@ -1135,9 +1195,21 @@ static int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,
 	if (intel_cap.perf_metrics && add_nr_metric_event(cpuc, event))
 		return -EINVAL;
 
+	/*
+	 * 在以下使用n_metric:
+	 *   - arch/x86/events/core.c|1154| <<add_nr_metric_event>> if (cpuc->n_metric == INTEL_TD_METRIC_NUM)
+	 *   - arch/x86/events/core.c|1156| <<add_nr_metric_event>> cpuc->n_metric++;
+	 *   - arch/x86/events/core.c|1167| <<del_nr_metric_event>> cpuc->n_metric--;
+	 *   - arch/x86/events/core.c|1183| <<collect_event>> if (n >= max_count + cpuc->n_metric)
+	 *   - arch/x86/events/core.c|2359| <<x86_pmu_cancel_txn>> __this_cpu_sub(cpu_hw_events.n_metric, __this_cpu_read(cpu_hw_events.n_txn_metric));
+	 */
 	if (n >= max_count + cpuc->n_metric)
 		return -EINVAL;
 
+	/*
+	 * struct cpu_hw_events *cpuc:
+	 * -> struct perf_event *event_list[X86_PMC_IDX_MAX]; // in enabled order
+	 */
 	cpuc->event_list[n] = event;
 	if (is_counter_pair(&event->hw)) {
 		cpuc->n_pair++;
@@ -1151,6 +1223,12 @@ static int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,
  * dogrp: true if must collect siblings events (group)
  * returns total number of events and error code
  */
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1528| <<x86_pmu_add>> ret = n = collect_events(cpuc, event, false);
+ *   - arch/x86/events/core.c|2504| <<validate_group>> n = collect_events(fake_cpuc, leader, true);
+ *   - arch/x86/events/core.c|2509| <<validate_group>> n = collect_events(fake_cpuc, event, false);
+ */
 static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader, bool dogrp)
 {
 	int num_counters = hybrid(cpuc->pmu, num_counters);
@@ -1205,6 +1283,10 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 	return n;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1330| <<x86_pmu_enable>> x86_assign_hw_event(event, cpuc, i);
+ */
 static inline void x86_assign_hw_event(struct perf_event *event,
 				struct cpu_hw_events *cpuc, int i)
 {
@@ -1274,6 +1356,13 @@ static inline int match_prev_assignment(struct hw_perf_event *hwc,
 
 static void x86_pmu_start(struct perf_event *event, int flags);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|1243| <<perf_pmu_enable>> pmu->pmu_enable(pmu);
+ *
+ * 在以下使用x86_pmu_enable():
+ *   - struct pmu pmu.pmu_enable = x86_pmu_enable,
+ */
 static void x86_pmu_enable(struct pmu *pmu)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -1439,6 +1528,31 @@ void x86_pmu_enable_event(struct perf_event *event)
  * The event is added to the group of enabled events
  * but only if it can be scheduled with existing events.
  */
+/*
+ * x86_pmu_add
+ * event_sched_in.part.0
+ * merge_sched_in
+ * visit_groups_merge.constprop.0.isra.0
+ * ctx_sched_in
+ * perf_event_sched_in
+ * __perf_event_task_sched_in
+ * finish_task_switch.isra.0
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用x86_pmu_add:
+ *   - arch/x86/events/core.c|69| <<global>> DEFINE_STATIC_CALL_NULL(x86_pmu_add, *x86_pmu.add);
+ *   - arch/x86/events/core.c|2736| <<global>> .add = x86_pmu_add,
+ *   - arch/x86/events/core.c|1536| <<x86_pmu_add>> static_call_cond(x86_pmu_add)(event);
+ *   - arch/x86/events/core.c|2051| <<x86_pmu_static_call_update>> static_call_update(x86_pmu_add, x86_pmu.add);
+ */
 static int x86_pmu_add(struct perf_event *event, int flags)
 {
 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
@@ -1449,6 +1563,9 @@ static int x86_pmu_add(struct perf_event *event, int flags)
 	hwc = &event->hw;
 
 	n0 = cpuc->n_events;
+	/*
+	 * 这里返回ffffea
+	 */
 	ret = n = collect_events(cpuc, event, false);
 	if (ret < 0)
 		goto out;
@@ -1468,6 +1585,11 @@ static int x86_pmu_add(struct perf_event *event, int flags)
 	if (cpuc->txn_flags & PERF_PMU_TXN_ADD)
 		goto done_collect;
 
+	/*
+	 * x86_schedule_events()
+	 *
+	 * int assign[X86_PMC_IDX_MAX];
+	 */
 	ret = static_call(x86_pmu_schedule_events)(cpuc, n, assign);
 	if (ret)
 		goto out;
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index 9b4d51c0e..71a2462d2 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -240,8 +240,25 @@ struct cpu_hw_events {
 	int			n_txn_pair;
 	int			n_txn_metric;
 	int			assign[X86_PMC_IDX_MAX]; /* event to counter assignment */
+	/*
+	 * 在以下使用cpu_hw_events->tags[X86_PMC_IDX_MAX];
+	 *   - arch/x86/events/core.c|1220| <<x86_assign_hw_event>> hwc->last_tag = ++cpuc->tags[i];
+	 *   - arch/x86/events/core.c|1276| <<match_prev_assignment>> return hwc->idx == cpuc->assign[i] && hwc->last_cpu == smp_processor_id() && hwc->last_tag == cpuc->tags[i];
+	 */
 	u64			tags[X86_PMC_IDX_MAX];
 
+	/*
+	 * 在以下使用cpu_hw_events->event_list[X86_PMC_IDX_MAX]:
+	 *   - arch/x86/events/core.c|1009| <<x86_schedule_events>> c = static_call(x86_pmu_get_event_constraints)(cpuc, i, cpuc->event_list[i]);
+	 *   - arch/x86/events/core.c|1023| <<x86_schedule_events>> hwc = &cpuc->event_list[i]->hw;
+	 *   - arch/x86/events/core.c|1094| <<x86_schedule_events>> e = cpuc->event_list[i];
+	 *   - arch/x86/events/core.c|1141| <<collect_event>> cpuc->event_list[n] = event;
+	 *   - arch/x86/events/core.c|1310| <<x86_pmu_enable>> event = cpuc->event_list[i];
+	 *   - arch/x86/events/core.c|1337| <<x86_pmu_enable>> event = cpuc->event_list[i];
+	 *   - arch/x86/events/core.c|1646| <<x86_pmu_del>> if (event == cpuc->event_list[i])
+	 *   - arch/x86/events/core.c|1661| <<x86_pmu_del>> cpuc->event_list[i-1] = cpuc->event_list[i];
+	 *   - arch/x86/events/intel/p4.c|1234| <<p4_pmu_schedule_events>> hwc = &cpuc->event_list[i]->hw;
+	 */
 	struct perf_event	*event_list[X86_PMC_IDX_MAX]; /* in enabled order */
 	struct event_constraint	*event_constraint[X86_PMC_IDX_MAX];
 
@@ -316,6 +333,14 @@ struct cpu_hw_events {
 	/*
 	 * Perf Metrics
 	 */
+	/*
+	 * 在以下使用n_metric:
+	 *   - arch/x86/events/core.c|1154| <<add_nr_metric_event>> if (cpuc->n_metric == INTEL_TD_METRIC_NUM)
+	 *   - arch/x86/events/core.c|1156| <<add_nr_metric_event>> cpuc->n_metric++;
+	 *   - arch/x86/events/core.c|1167| <<del_nr_metric_event>> cpuc->n_metric--;
+	 *   - arch/x86/events/core.c|1183| <<collect_event>> if (n >= max_count + cpuc->n_metric)
+	 *   - arch/x86/events/core.c|2359| <<x86_pmu_cancel_txn>> __this_cpu_sub(cpu_hw_events.n_metric, __this_cpu_read(cpu_hw_events.n_txn_metric));
+	 */
 	/* number of accepted metrics events */
 	int				n_metric;
 
@@ -1059,18 +1084,54 @@ extern u64 __read_mostly hw_cache_extra_regs
 
 u64 x86_perf_event_update(struct perf_event *event);
 
+/*
+ * #define MSR_F15H_PERF_CTL               0xc0010200
+ * #define MSR_F15H_PERF_CTL0              MSR_F15H_PERF_CTL
+ * #define MSR_F15H_PERF_CTL1              (MSR_F15H_PERF_CTL + 2)
+ * #define MSR_F15H_PERF_CTL2              (MSR_F15H_PERF_CTL + 4)
+ * #define MSR_F15H_PERF_CTL3              (MSR_F15H_PERF_CTL + 6)
+ * #define MSR_F15H_PERF_CTL4              (MSR_F15H_PERF_CTL + 8)
+ * #define MSR_F15H_PERF_CTL5              (MSR_F15H_PERF_CTL + 10)
+ *
+ * #define MSR_F15H_PERF_CTR               0xc0010201
+ * #define MSR_F15H_PERF_CTR0              MSR_F15H_PERF_CTR
+ * #define MSR_F15H_PERF_CTR1              (MSR_F15H_PERF_CTR + 2)
+ * #define MSR_F15H_PERF_CTR2              (MSR_F15H_PERF_CTR + 4)
+ * #define MSR_F15H_PERF_CTR3              (MSR_F15H_PERF_CTR + 6)
+ * #define MSR_F15H_PERF_CTR4              (MSR_F15H_PERF_CTR + 8)
+ * #define MSR_F15H_PERF_CTR5              (MSR_F15H_PERF_CTR + 10)
+ */
 static inline unsigned int x86_pmu_config_addr(int index)
 {
+	/*
+	 * amd的例子:
+	 * Legacy CPUs:
+	 *   4 counters starting at 0xc0010000 each offset by 1
+	 *
+	 * CPUs with core performance counter extensions:
+	 *   6 counters starting at 0xc0010200 each offset by 2
+	 */
 	return x86_pmu.eventsel + (x86_pmu.addr_offset ?
 				   x86_pmu.addr_offset(index, true) : index);
 }
 
 static inline unsigned int x86_pmu_event_addr(int index)
 {
+	/*
+	 * Legacy CPUs:
+	 *   4 counters starting at 0xc0010000 each offset by 1
+	 *
+	 * CPUs with core performance counter extensions:
+	 *   6 counters starting at 0xc0010200 each offset by 2
+	 */
 	return x86_pmu.perfctr + (x86_pmu.addr_offset ?
 				  x86_pmu.addr_offset(index, false) : index);
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1240| <<x86_assign_hw_event>> hwc->event_base_rdpmc = x86_pmu_rdpmc_index(hwc->idx);
+ */
 static inline int x86_pmu_rdpmc_index(int index)
 {
 	return x86_pmu.rdpmc_index ? x86_pmu.rdpmc_index(index) : index;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 08cfc26ee..83f97c1d4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -481,6 +481,25 @@ enum pmc_type {
 struct kvm_pmc {
 	enum pmc_type type;
 	u8 idx;
+	/*
+	 * 在以下使用kvm_pmc->counter:
+	 *   - arch/x86/kvm/pmu.c|120| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+	 *   - arch/x86/kvm/pmu.c|149| <<pmc_pause_counter>> u64 counter = pmc->counter;
+	 *   - arch/x86/kvm/pmu.c|156| <<pmc_pause_counter>> pmc->counter = counter & pmc_bitmask(pmc);
+	 *   - arch/x86/kvm/pmu.c|167| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+	 *   - arch/x86/kvm/pmu.h|56| <<pmc_read_counter>> counter = pmc->counter;
+	 *   - arch/x86/kvm/pmu.h|58| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.h|77| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+	 *   - arch/x86/kvm/pmu.h|150| <<pmc_update_sample_period>> get_sample_period(pmc, pmc->counter));
+	 *   - arch/x86/kvm/svm/pmu.c|280| <<amd_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+	 *   - arch/x86/kvm/svm/pmu.c|340| <<amd_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|444| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|448| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|595| <<intel_pmu_reset>> pmc->counter = pmc->eventsel = 0;
+	 *   arch/x86/kvm/vmx/pmu_intel.c|602| <<intel_pmu_reset>> pmc->counter = 0;
+	 *
+	 * 不是特别用
+	 */
 	u64 counter;
 	u64 eventsel;
 	struct perf_event *perf_event;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 62333f975..e03a1dc52 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -94,6 +94,13 @@ static void kvm_perf_overflow_intr(struct perf_event *perf_event,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|226| <<reprogram_gp_counter>> pmc_reprogram_counter(pmc, type, config, !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ *                                                                  !(eventsel & ARCH_PERFMON_EVENTSEL_OS), eventsel & ARCH_PERFMON_EVENTSEL_INT);
+ *   - arch/x86/kvm/pmu.c|261| <<reprogram_fixed_counter>> pmc_reprogram_counter(pmc, PERF_TYPE_HARDWARE, kvm_x86_ops.pmu_ops->find_fixed_event(idx),
+ *                                                                  !(en_field & 0x2), !(en_field & 0x1), pmi);
+ */
 static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
 				  u64 config, bool exclude_user,
 				  bool exclude_kernel, bool intr)
@@ -150,6 +157,11 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|233| <<reprogram_gp_counter>> if (pmc->current_config == eventsel && pmc_resume_counter(pmc))
+ *   - arch/x86/kvm/pmu.c|268| <<reprogram_fixed_counter>> if (pmc->current_config == (u64)ctrl && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -168,6 +180,12 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|277| <<reprogram_counter>> reprogram_gp_counter(pmc, pmc->eventsel);
+ *   - arch/x86/kvm/svm/pmu.c|289| <<amd_pmu_set_msr>> reprogram_gp_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|459| <<intel_pmu_set_msr>> reprogram_gp_counter(pmc, data);
+ */
 void reprogram_gp_counter(struct kvm_pmc *pmc, u64 eventsel)
 {
 	u64 config;
@@ -266,6 +284,11 @@ void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 }
 EXPORT_SYMBOL_GPL(reprogram_fixed_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|300| <<kvm_pmu_handle_event>> reprogram_counter(pmu, bit);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|68| <<global_ctrl_changed>> reprogram_counter(pmu, bit);
+ */
 void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 {
 	struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, pmc_idx);
@@ -284,6 +307,10 @@ void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 }
 EXPORT_SYMBOL_GPL(reprogram_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9794| <<vcpu_enter_guest>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 200995c52..d6cdd7e72 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -136,6 +136,18 @@ struct hw_perf_event {
 	union {
 		struct { /* hardware */
 			u64		config;
+			/*
+			 * 在以下使用hw_perf_event->last_tag:
+			 *   - arch/x86/events/core.c|664| <<__x86_pmu_event_init>> event->hw.last_tag = ~0ULL;
+			 *   - arch/x86/events/core.c|1220| <<x86_assign_hw_event>> hwc->last_tag = ++cpuc->tags[i];
+			 *   - arch/x86/events/core.c|1276| <<match_prev_assignment>> hwc->last_tag == cpuc->tags[i];
+			 *   - arch/x86/events/intel/uncore.c|243| <<uncore_assign_hw_event>> hwc->last_tag = ++box->tags[idx];
+			 *   - arch/x86/events/intel/uncore.c|604| <<uncore_pmu_event_add>> hwc->last_tag == box->tags[assign[i]])
+			 *   - arch/x86/events/intel/uncore.c|622| <<uncore_pmu_event_add>> hwc->last_tag != box->tags[assign[i]])
+			 *   - arch/x86/events/intel/uncore.c|665| <<uncore_pmu_event_del>> event->hw.last_tag = ~0ULL;
+			 *   - arch/x86/events/intel/uncore.c|751| <<uncore_pmu_event_init>> event->hw.last_tag = ~0ULL;
+			 *   - arch/x86/events/intel/uncore_snb.c|715| <<snb_uncore_imc_event_init>> event->hw.last_tag = ~0ULL;
+			 */
 			u64		last_tag;
 			unsigned long	config_base;
 			unsigned long	event_base;
@@ -291,6 +303,15 @@ struct pmu {
 	 */
 	int				capabilities;
 
+	/*
+	 * 在以下使用pmu->pmu_disable_count (percpu):
+	 *   - kernel/events/core.c|1242| <<perf_pmu_disable>> int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|1272| <<perf_pmu_enable>> int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|11464| <<perf_pmu_register>> pmu->pmu_disable_count = alloc_percpu(int );
+	 *   - kernel/events/core.c|11465| <<perf_pmu_register>> if (!pmu->pmu_disable_count)
+	 *   - kernel/events/core.c|11589| <<perf_pmu_register>> free_percpu(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|11606| <<perf_pmu_unregister>> free_percpu(pmu->pmu_disable_count);
+	 */
 	int __percpu			*pmu_disable_count;
 	struct perf_cpu_context __percpu *pmu_cpu_context;
 	atomic_t			exclusive_cnt; /* < 0: cpu; > 0: tsk */
@@ -1222,9 +1243,21 @@ static inline void perf_event_task_migrate(struct task_struct *task)
 		task->sched_migrated = 1;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|4899| <<finish_task_switch>> perf_event_task_sched_in(prev, current);
+ */
 static inline void perf_event_task_sched_in(struct task_struct *prev,
 					    struct task_struct *task)
 {
+	/*
+	 * 在以下使用perf_sched_events:
+	 *   - kernel/events/core.c|385| <<global>> DEFINE_STATIC_KEY_FALSE(perf_sched_events);
+	 *   - include/linux/perf_event.h|1244| <<perf_event_task_sched_in>> if (static_branch_unlikely(&perf_sched_events))
+	 *   - include/linux/perf_event.h|1267| <<perf_event_task_sched_out>> if (static_branch_unlikely(&perf_sched_events))
+	 *   - kernel/events/core.c|5136| <<perf_sched_delayed>> static_branch_disable(&perf_sched_events);
+	 *   - kernel/events/core.c|11838| <<account_event>> static_branch_enable(&perf_sched_events);
+	 */
 	if (static_branch_unlikely(&perf_sched_events))
 		__perf_event_task_sched_in(prev, task);
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 9b3cfe685..f37501577 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1216,9 +1216,44 @@ struct task_struct {
 	struct mutex			futex_exit_mutex;
 	unsigned int			futex_state;
 #endif
+	/*
+	 * 在以下使用task_struct->perf_event_ctxp[perf_nr_task_contexts]:
+	 *   - kernel/events/core.c|1536| <<perf_lock_task_context>> ctx = rcu_dereference(task->perf_event_ctxp[ctxn]);
+	 *   - kernel/events/core.c|1549| <<perf_lock_task_context>> if (ctx != rcu_dereference(task->perf_event_ctxp[ctxn])) {
+	 *   - kernel/events/core.c|3596| <<perf_event_context_sched_out>> struct perf_event_context *ctx = task->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|3612| <<perf_event_context_sched_out>> next_ctx = next->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|3677| <<perf_event_context_sched_out>> RCU_INIT_POINTER(task->perf_event_ctxp[ctxn], next_ctx);
+	 *   - kernel/events/core.c|3678| <<perf_event_context_sched_out>> RCU_INIT_POINTER(next->perf_event_ctxp[ctxn], ctx);
+	 *   - kernel/events/core.c|4135| <<__perf_event_task_sched_in>> ctx = task->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|4465| <<perf_event_enable_on_exec>> ctx = current->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|4945| <<find_get_context>> else if (task->perf_event_ctxp[ctxn])
+	 *   - kernel/events/core.c|4958| <<find_get_context>> rcu_assign_pointer(task->perf_event_ctxp[ctxn], ctx);
+	 *   - kernel/events/core.c|7983| <<perf_iterate_sb>> ctx = rcu_dereference(current->perf_event_ctxp[ctxn]);
+	 *   - kernel/events/core.c|8035| <<perf_event_exec>> ctx = rcu_dereference(current->perf_event_ctxp[ctxn]);
+	 *   - kernel/events/core.c|8882| <<perf_addr_filters_adjust>> ctx = rcu_dereference(current->perf_event_ctxp[ctxn]);
+	 *   - kernel/events/core.c|10091| <<perf_tp_event>> ctx = rcu_dereference(task->perf_event_ctxp[perf_sw_context]);
+	 *   - kernel/events/core.c|13133| <<perf_event_exit_task_context>> RCU_INIT_POINTER(child->perf_event_ctxp[ctxn], NULL);
+	 *   - kernel/events/core.c|13231| <<perf_event_free_task>> ctx = task->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|13243| <<perf_event_free_task>> RCU_INIT_POINTER(task->perf_event_ctxp[ctxn], NULL);
+	 *   - kernel/events/core.c|13277| <<perf_event_delayed_put>> WARN_ON_ONCE(task->perf_event_ctxp[ctxn]);
+	 *   - kernel/events/core.c|13496| <<inherit_task_group>> child_ctx = child->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|13508| <<inherit_task_group>> child->perf_event_ctxp[ctxn] = child_ctx;
+	 *   - kernel/events/core.c|13534| <<perf_event_init_context>> if (likely(!parent->perf_event_ctxp[ctxn]))
+	 *   - kernel/events/core.c|13590| <<perf_event_init_context>> child_ctx = child->perf_event_ctxp[ctxn];
+	 *   - kernel/events/core.c|13628| <<perf_event_init_task>> memset(child->perf_event_ctxp, 0, sizeof(child->perf_event_ctxp));
+	 */
 #ifdef CONFIG_PERF_EVENTS
 	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];
 	struct mutex			perf_event_mutex;
+	/*
+	 * 在以下使用perf_event_list:
+	 *   - init/init_task.c|143| <<global>> .perf_event_list = LIST_HEAD_INIT(init_task.perf_event_list),
+	 *   - kernel/events/core.c|6190| <<perf_event_task_enable>> list_for_each_entry(event, &current->perf_event_list, owner_entry) {
+	 *   - kernel/events/core.c|6206| <<perf_event_task_disable>> list_for_each_entry(event, &current->perf_event_list, owner_entry) {
+	 *   - kernel/events/core.c|12980| <<SYSCALL_DEFINE5>> list_add_tail(&event->owner_entry, &current->perf_event_list);
+	 *   - kernel/events/core.c|13345| <<perf_event_exit_task>> list_for_each_entry_safe(event, tmp, &child->perf_event_list,
+	 *   - kernel/events/core.c|13804| <<perf_event_init_task>> INIT_LIST_HEAD(&child->perf_event_list);
+	 */
 	struct list_head		perf_event_list;
 #endif
 #ifdef CONFIG_DEBUG_PREEMPT
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 4e5a73c7d..6f93f3c4a 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -382,6 +382,14 @@ enum event_type_t {
  */
 
 static void perf_sched_delayed(struct work_struct *work);
+/*
+ * 在以下使用perf_sched_events:
+ *   - kernel/events/core.c|385| <<global>> DEFINE_STATIC_KEY_FALSE(perf_sched_events);
+ *   - include/linux/perf_event.h|1244| <<perf_event_task_sched_in>> if (static_branch_unlikely(&perf_sched_events))
+ *   - include/linux/perf_event.h|1267| <<perf_event_task_sched_out>> if (static_branch_unlikely(&perf_sched_events))
+ *   - kernel/events/core.c|5136| <<perf_sched_delayed>> static_branch_disable(&perf_sched_events);
+ *   - kernel/events/core.c|11838| <<account_event>> static_branch_enable(&perf_sched_events);
+ */
 DEFINE_STATIC_KEY_FALSE(perf_sched_events);
 static DECLARE_DELAYED_WORK(perf_sched_work, perf_sched_delayed);
 static DEFINE_MUTEX(perf_sched_mutex);
@@ -396,6 +404,14 @@ static atomic_t nr_comm_events __read_mostly;
 static atomic_t nr_namespaces_events __read_mostly;
 static atomic_t nr_task_events __read_mostly;
 static atomic_t nr_freq_events __read_mostly;
+/*
+ * 在以下使用nr_switch_events:
+ *   - kernel/events/core.c|407| <<global>> static atomic_t nr_switch_events __read_mostly;
+ *   - kernel/events/core.c|3824| <<__perf_event_task_sched_out>> if (atomic_read(&nr_switch_events))
+ *   - kernel/events/core.c|4174| <<__perf_event_task_sched_in>> if (atomic_read(&nr_switch_events))
+ *   - kernel/events/core.c|5117| <<unaccount_event>> atomic_dec(&nr_switch_events);
+ *   - kernel/events/core.c|11825| <<account_event>> atomic_inc(&nr_switch_events);
+ */
 static atomic_t nr_switch_events __read_mostly;
 static atomic_t nr_ksymbol_events __read_mostly;
 static atomic_t nr_bpf_events __read_mostly;
@@ -1231,14 +1247,61 @@ static int perf_mux_hrtimer_restart_ipi(void *arg)
 
 void perf_pmu_disable(struct pmu *pmu)
 {
+	/*
+	 * 在以下使用pmu->pmu_disable_count (percpu):
+	 *   - kernel/events/core.c|1242| <<perf_pmu_disable>> int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|1272| <<perf_pmu_enable>> int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|11464| <<perf_pmu_register>> pmu->pmu_disable_count = alloc_percpu(int );
+	 *   - kernel/events/core.c|11465| <<perf_pmu_register>> if (!pmu->pmu_disable_count)
+	 *   - kernel/events/core.c|11589| <<perf_pmu_register>> free_percpu(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|11606| <<perf_pmu_unregister>> free_percpu(pmu->pmu_disable_count);
+	 */
 	int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	/*
+	 * x86_pmu_disable()
+	 */
 	if (!(*count)++)
 		pmu->pmu_disable(pmu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|2267| <<x86_pmu_cancel_txn>> perf_pmu_enable(pmu);
+ *   - arch/x86/events/core.c|2306| <<x86_pmu_commit_txn>> perf_pmu_enable(pmu);
+ *   - arch/x86/events/intel/core.c|2617| <<intel_pmu_read_topdown_event>> perf_pmu_enable(event->pmu);
+ *   - arch/x86/events/intel/ds.c|1812| <<intel_pmu_auto_reload_read>> perf_pmu_enable(event->pmu);
+ *   - kernel/events/core.c|882| <<perf_cgroup_switch>> perf_pmu_enable(cpuctx->ctx.pmu);
+ *   - kernel/events/core.c|2423| <<event_sched_out>> perf_pmu_enable(event->pmu);
+ *   - kernel/events/core.c|2446| <<group_sched_out>> perf_pmu_enable(ctx->pmu);
+ *   - kernel/events/core.c|2675| <<event_sched_in>> perf_pmu_enable(event->pmu);
+ *   - kernel/events/core.c|2848| <<ctx_resched>> perf_pmu_enable(cpuctx->ctx.pmu);
+ *   - kernel/events/core.c|3420| <<ctx_sched_out>> perf_pmu_enable(ctx->pmu);
+ *   - kernel/events/core.c|3600| <<perf_event_context_sched_out>> perf_pmu_enable(pmu);
+ *   - kernel/events/core.c|3631| <<perf_event_context_sched_out>> perf_pmu_enable(pmu);
+ *   - kernel/events/core.c|3681| <<__perf_pmu_sched_task>> perf_pmu_enable(pmu);
+ *   - kernel/events/core.c|4033| <<perf_event_context_sched_in>> perf_pmu_enable(pmu);
+ *   - kernel/events/core.c|4256| <<perf_adjust_freq_unthr_context>> perf_pmu_enable(event->pmu);
+ *   - kernel/events/core.c|4259| <<perf_adjust_freq_unthr_context>> perf_pmu_enable(ctx->pmu);
+ *   - kernel/events/core.c|4346| <<perf_rotate_context>> perf_pmu_enable(cpuctx->ctx.pmu);
+ *   - kernel/events/core.c|5733| <<__perf_event_period>> perf_pmu_enable(ctx->pmu);
+ *   - kernel/events/core.c|11117| <<perf_pmu_commit_txn>> perf_pmu_enable(pmu);
+ *   - kernel/events/core.c|11130| <<perf_pmu_cancel_txn>> perf_pmu_enable(pmu);
+ */
 void perf_pmu_enable(struct pmu *pmu)
 {
+	/*
+	 * 在以下使用pmu->pmu_disable_count (percpu):
+	 *   - kernel/events/core.c|1242| <<perf_pmu_disable>> int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|1272| <<perf_pmu_enable>> int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|11464| <<perf_pmu_register>> pmu->pmu_disable_count = alloc_percpu(int );
+	 *   - kernel/events/core.c|11465| <<perf_pmu_register>> if (!pmu->pmu_disable_count)
+	 *   - kernel/events/core.c|11589| <<perf_pmu_register>> free_percpu(pmu->pmu_disable_count);
+	 *   - kernel/events/core.c|11606| <<perf_pmu_unregister>> free_percpu(pmu->pmu_disable_count);
+	 */
 	int *count = this_cpu_ptr(pmu->pmu_disable_count);
+	/*
+	 * x86_pmu_enable()
+	 */
 	if (!--(*count))
 		pmu->pmu_enable(pmu);
 }
@@ -1479,6 +1542,11 @@ static u64 primary_event_id(struct perf_event *event)
  * This has to cope with the fact that until it is locked,
  * the context could get moved to another task.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|1567| <<perf_pin_task_context>> ctx = perf_lock_task_context(task, ctxn, &flags);
+ *   - kernel/events/core.c|4857| <<find_get_context>> ctx = perf_lock_task_context(task, ctxn, &flags);
+ */
 static struct perf_event_context *
 perf_lock_task_context(struct task_struct *task, int ctxn, unsigned long *flags)
 {
@@ -1496,6 +1564,15 @@ perf_lock_task_context(struct task_struct *task, int ctxn, unsigned long *flags)
 	 */
 	local_irq_save(*flags);
 	rcu_read_lock();
+	/*
+	 * perf_hw_context
+	 *
+	 * #ifdef CONFIG_PERF_EVENTS
+	 *     struct perf_event_context       *perf_event_ctxp[perf_nr_task_contexts];
+	 *     struct mutex                    perf_event_mutex;
+	 *     struct list_head                perf_event_list;
+	 * #endif
+	 */
 	ctx = rcu_dereference(task->perf_event_ctxp[ctxn]);
 	if (ctx) {
 		/*
@@ -1535,6 +1612,12 @@ perf_lock_task_context(struct task_struct *task, int ctxn, unsigned long *flags)
  * can't get swapped to another task.  This also increments its
  * reference count so that the context can't get freed.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|4470| <<perf_event_remove_on_exec>> ctx = perf_pin_task_context(current, ctxn);
+ *   - kernel/events/core.c|13027| <<perf_event_exit_task_context>> child_ctx = perf_pin_task_context(child, ctxn);
+ *   - kernel/events/core.c|13463| <<perf_event_init_context>> parent_ctx = perf_pin_task_context(parent, ctxn);
+ */
 static struct perf_event_context *
 perf_pin_task_context(struct task_struct *task, int ctxn)
 {
@@ -1859,6 +1942,10 @@ perf_event_groups_next(struct perf_event *event)
  * Add an event from the lists for its context.
  * Must be called with ctx->mutex and ctx->lock held.
  */
+/*
+ * called by:
+ *  -  kernel/events/core.c|2800| <<add_event_to_ctx>> list_add_event(event, ctx);
+ */
 static void
 list_add_event(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -2086,6 +2173,11 @@ static void perf_group_attach(struct perf_event *event)
  * Remove an event from the lists for its context.
  * Must be called with ctx->mutex and ctx->lock held.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2530| <<__perf_remove_from_context>> list_del_event(event, ctx);
+ *   - kernel/events/core.c|13212| <<perf_free_event>> list_del_event(event, ctx);
+ */
 static void
 list_del_event(struct perf_event *event, struct perf_event_context *ctx)
 {
@@ -2456,6 +2548,12 @@ group_sched_out(struct perf_event *group_event,
  * We disable the event on the hardware level first. After that we
  * remove it from the context list.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|2574| <<perf_remove_from_context>> __perf_remove_from_context(event, __get_cpu_context(ctx),
+ *   - kernel/events/core.c|2581| <<perf_remove_from_context>> event_function_call(event, __perf_remove_from_context, (void *)flags);
+ *   - kernel/events/core.c|13690| <<__perf_event_exit_context>> __perf_remove_from_context(event, cpuctx, ctx, (void *)DETACH_GROUP);
+ */
 static void
 __perf_remove_from_context(struct perf_event *event,
 			   struct perf_cpu_context *cpuctx,
@@ -2507,6 +2605,15 @@ __perf_remove_from_context(struct perf_event *event,
  * When called from perf_event_exit_task, it's OK because the
  * context has been detached from its task.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|5373| <<perf_event_release_kernel>> perf_remove_from_context(event, DETACH_GROUP|DETACH_DEAD);
+ *   - kernel/events/core.c|5413| <<perf_event_release_kernel>> perf_remove_from_context(child, DETACH_GROUP);
+ *   - kernel/events/core.c|12750| <<SYSCALL_DEFINE5(perf_event_open)>> perf_remove_from_context(group_leader, 0);
+ *   - kernel/events/core.c|12754| <<SYSCALL_DEFINE5(perf_event_open)>> perf_remove_from_context(sibling, 0);
+ *   - kernel/events/core.c|12976| <<perf_pmu_migrate_context>> perf_remove_from_context(event, 0);
+ *   - kernel/events/core.c|13071| <<perf_event_exit_event>> perf_remove_from_context(event, detach_flags);
+ */
 static void perf_remove_from_context(struct perf_event *event, unsigned long flags)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -2617,6 +2724,29 @@ void perf_event_disable_inatomic(struct perf_event *event)
 static void perf_log_throttle(struct perf_event *event, int enable);
 static void perf_log_itrace_start(struct perf_event *event);
 
+/*
+ * x86_pmu_add
+ * event_sched_in.part.0
+ * merge_sched_in
+ * visit_groups_merge.constprop.0.isra.0
+ * ctx_sched_in
+ * perf_event_sched_in
+ * __perf_event_task_sched_in
+ * finish_task_switch.isra.0
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - kernel/events/core.c|2804| <<group_sched_in>> if (event_sched_in(group_event, cpuctx, ctx))
+ *   - kernel/events/core.c|2811| <<group_sched_in>> if (event_sched_in(event, cpuctx, ctx)) {
+ */
 static int
 event_sched_in(struct perf_event *event,
 		 struct perf_cpu_context *cpuctx,
@@ -2654,6 +2784,9 @@ event_sched_in(struct perf_event *event,
 
 	perf_log_itrace_start(event);
 
+	/*
+	 * x86_pmu_add()
+	 */
 	if (event->pmu->add(event, PERF_EF_START)) {
 		perf_event_set_state(event, PERF_EVENT_STATE_INACTIVE);
 		event->oncpu = -1;
@@ -2677,6 +2810,10 @@ event_sched_in(struct perf_event *event,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3910| <<merge_sched_in>> if (!group_sched_in(event, cpuctx, ctx))
+ */
 static int
 group_sched_in(struct perf_event *group_event,
 	       struct perf_cpu_context *cpuctx,
@@ -2725,6 +2862,28 @@ group_sched_in(struct perf_event *group_event,
 	return -EAGAIN;
 }
 
+/*
+ * 104 static void pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type,
+ * 105                                   u64 config, bool exclude_user,
+ * 106                                   bool exclude_kernel, bool intr)
+ * 107 {
+ * 108         struct perf_event *event;
+ * 109         struct perf_event_attr attr = {
+ * 110                 .type = type,
+ * 111                 .size = sizeof(attr),
+ * 112                 .pinned = true,
+ * 113                 .exclude_idle = true,
+ * 114                 .exclude_host = 1,
+ * 115                 .exclude_user = exclude_user,
+ * 116                 .exclude_kernel = exclude_kernel,
+ * 117                 .config = config,
+ * 118         };
+ * 119
+ * 120         attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *
+ * called by:
+ *   - kernel/events/core.c|4044| <<merge_sched_in>> if (group_can_go_on(event, cpuctx, *can_add_hw)) {
+ */
 /*
  * Work out whether we can put this event group on the CPU now.
  */
@@ -2756,6 +2915,14 @@ static int group_can_go_on(struct perf_event *event,
 	return can_add_hw;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2952| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|2955| <<__perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|3009| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|3079| <<perf_install_in_context>> add_event_to_ctx(event, ctx);
+ *   - kernel/events/core.c|13363| <<inherit_event>> add_event_to_ctx(child_event, child_ctx);
+ */
 static void add_event_to_ctx(struct perf_event *event,
 			       struct perf_event_context *ctx)
 {
@@ -2785,6 +2952,12 @@ static void task_ctx_sched_out(struct perf_cpu_context *cpuctx,
 	ctx_sched_out(ctx, cpuctx, event_type);
 }
 
+/*
+ * called by:
+ *  - kernel/events/core.c|2934| <<ctx_resched>> perf_event_sched_in(cpuctx, task_ctx, current);
+ *  - kernel/events/core.c|4124| <<perf_event_context_sched_in>> perf_event_sched_in(cpuctx, ctx, task);
+ *  - kernel/events/core.c|4444| <<perf_rotate_context>> perf_event_sched_in(cpuctx, task_ctx, current);
+ */
 static void perf_event_sched_in(struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
 				struct task_struct *task)
@@ -2931,6 +3104,15 @@ static bool exclusive_event_installable(struct perf_event *event,
  *
  * Very similar to event_function_call, see comment there.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|12728| <<SYSCALL_DEFINE5(perf_event_open)>> perf_install_in_context(ctx, sibling, sibling->cpu);
+ *   - kernel/events/core.c|12738| <<SYSCALL_DEFINE5(perf_event_open)>> perf_install_in_context(ctx, group_leader, group_leader->cpu);
+ *   - kernel/events/core.c|12753| <<SYSCALL_DEFINE5(perf_event_open)>> perf_install_in_context(ctx, event, event->cpu);
+ *   - kernel/events/core.c|12894| <<perf_event_create_kernel_counter>> perf_install_in_context(ctx, event, event->cpu);
+ *   - kernel/events/core.c|12955| <<perf_pmu_migrate_context>> perf_install_in_context(dst_ctx, event, dst_cpu);
+ *   - kernel/events/core.c|12968| <<perf_pmu_migrate_context>> perf_install_in_context(dst_ctx, event, dst_cpu);
+ */
 static void
 perf_install_in_context(struct perf_event_context *ctx,
 			struct perf_event *event,
@@ -3871,12 +4053,27 @@ static inline void group_update_userpage(struct perf_event *group_event)
 		event_update_userpage(event);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4013| <<ctx_pinned_sched_in>> merge_sched_in, &can_add_hw);
+ *   - kernel/events/core.c|4027| <<ctx_flexible_sched_in>> merge_sched_in, &can_add_hw);
+ */
 static int merge_sched_in(struct perf_event *event, void *data)
 {
 	struct perf_event_context *ctx = event->ctx;
 	struct perf_cpu_context *cpuctx = __get_cpu_context(ctx);
 	int *can_add_hw = data;
 
+	/*
+	 * enum perf_event_state {
+	 *     PERF_EVENT_STATE_DEAD           = -4,
+	 *     PERF_EVENT_STATE_EXIT           = -3,
+	 *     PERF_EVENT_STATE_ERROR          = -2,
+	 *     PERF_EVENT_STATE_OFF            = -1,
+	 *     PERF_EVENT_STATE_INACTIVE       =  0,
+	 *     PERF_EVENT_STATE_ACTIVE         =  1,
+	 * };
+	 */
 	if (event->state <= PERF_EVENT_STATE_OFF)
 		return 0;
 
@@ -3884,6 +4081,9 @@ static int merge_sched_in(struct perf_event *event, void *data)
 		return 0;
 
 	if (group_can_go_on(event, cpuctx, *can_add_hw)) {
+		/*
+		 * 只在此处调用
+		 */
 		if (!group_sched_in(event, cpuctx, ctx))
 			list_add_tail(&event->active_list, get_event_list(event));
 	}
@@ -3892,6 +4092,17 @@ static int merge_sched_in(struct perf_event *event, void *data)
 		*can_add_hw = 0;
 		if (event->attr.pinned) {
 			perf_cgroup_event_disable(event, ctx);
+			/*
+			 * 在以下使用PERF_EVENT_STATE_ERROR:
+			 *   - include/linux/perf_event.h|591| <<global>> PERF_EVENT_STATE_ERROR = -2,
+			 *   - kernel/events/core.c|2269| <<perf_put_aux_event>> perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
+			 *   - kernel/events/core.c|2335| <<perf_remove_sibling_event>> perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
+			 *   - kernel/events/core.c|3191| <<__perf_event_enable>> event->state <= PERF_EVENT_STATE_ERROR)
+			 *   - kernel/events/core.c|3239| <<_perf_event_enable>> event->state < PERF_EVENT_STATE_ERROR) {
+			 *   - kernel/events/core.c|3252| <<_perf_event_enable>> if (event->state == PERF_EVENT_STATE_ERROR) {
+			 *   - kernel/events/core.c|4034| <<merge_sched_in>> perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
+			 *   - kernel/events/core.c|5800| <<__perf_read>> if (event->state == PERF_EVENT_STATE_ERROR)
+			 */
 			perf_event_set_state(event, PERF_EVENT_STATE_ERROR);
 		} else {
 			ctx->rotate_necessary = 1;
@@ -3931,6 +4142,15 @@ ctx_flexible_sched_in(struct perf_event_context *ctx,
 			   merge_sched_in, &can_add_hw);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|2881| <<perf_event_sched_in>> ctx_sched_in(ctx, cpuctx, EVENT_PINNED, task);
+ *   - kernel/events/core.c|2884| <<perf_event_sched_in>> ctx_sched_in(ctx, cpuctx, EVENT_FLEXIBLE, task);
+ *   - kernel/events/core.c|3166| <<__perf_event_enable>> ctx_sched_in(ctx, cpuctx, EVENT_TIME, current);
+ *   - kernel/events/core.c|3175| <<__perf_event_enable>> ctx_sched_in(ctx, cpuctx, EVENT_TIME, current);
+ *   - kernel/events/core.c|4082| <<cpu_ctx_sched_in>> ctx_sched_in(ctx, cpuctx, event_type, task);
+ *   - kernel/events/core.c|4516| <<perf_event_enable_on_exec>> ctx_sched_in(ctx, cpuctx, EVENT_TIME, current);
+ */
 static void
 ctx_sched_in(struct perf_event_context *ctx,
 	     struct perf_cpu_context *cpuctx,
@@ -3986,6 +4206,10 @@ static void cpu_ctx_sched_in(struct perf_cpu_context *cpuctx,
 	ctx_sched_in(ctx, cpuctx, event_type, task);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4171| <<__perf_event_task_sched_in>> perf_event_context_sched_in(ctx, task);
+ */
 static void perf_event_context_sched_in(struct perf_event_context *ctx,
 					struct task_struct *task)
 {
@@ -4014,6 +4238,9 @@ static void perf_event_context_sched_in(struct perf_event_context *ctx,
 	if (!ctx->nr_events)
 		goto unlock;
 
+	/*
+	 * 除了这里, 非常多调用
+	 */
 	perf_pmu_disable(pmu);
 	/*
 	 * We want to keep the following priority order:
@@ -4025,6 +4252,12 @@ static void perf_event_context_sched_in(struct perf_event_context *ctx,
 	 */
 	if (!RB_EMPTY_ROOT(&ctx->pinned_groups.tree))
 		cpu_ctx_sched_out(cpuctx, EVENT_FLEXIBLE);
+	/*
+	 * called by:
+	 *  - kernel/events/core.c|2934| <<ctx_resched>> perf_event_sched_in(cpuctx, task_ctx, current);
+	 *  - kernel/events/core.c|4124| <<perf_event_context_sched_in>> perf_event_sched_in(cpuctx, ctx, task);
+	 *  - kernel/events/core.c|4444| <<perf_rotate_context>> perf_event_sched_in(cpuctx, task_ctx, current);
+	 */
 	perf_event_sched_in(cpuctx, ctx, task);
 
 	if (cpuctx->sched_cb_usage && pmu->sched_task)
@@ -4047,6 +4280,10 @@ static void perf_event_context_sched_in(struct perf_event_context *ctx,
  * accessing the event control register. If a NMI hits, then it will
  * keep the event running.
  */
+/*
+ * called by:
+ *   - include/linux/perf_event.h|1241| <<perf_event_task_sched_in>> __perf_event_task_sched_in(prev, task);
+ */
 void __perf_event_task_sched_in(struct task_struct *prev,
 				struct task_struct *task)
 {
@@ -4063,14 +4300,37 @@ void __perf_event_task_sched_in(struct task_struct *prev,
 	if (atomic_read(this_cpu_ptr(&perf_cgroup_events)))
 		perf_cgroup_sched_in(prev, task);
 
+	/*
+	 * 对于下面的每一个: for ((ctxn) = 0; (ctxn) < perf_nr_task_contexts; (ctxn)++)
+	 * enum perf_event_task_context {
+	 *     perf_invalid_context = -1,
+	 *     perf_hw_context = 0,
+	 *     perf_sw_context,
+	 *     perf_nr_task_contexts,
+	 * };
+	 */
 	for_each_task_context_nr(ctxn) {
+		/*
+		 * struct perf_event_context *ctx;
+		 */
 		ctx = task->perf_event_ctxp[ctxn];
 		if (likely(!ctx))
 			continue;
 
+		/*
+		 * 只在此处调用
+		 */
 		perf_event_context_sched_in(ctx, task);
 	}
 
+	/*
+	 * 在以下使用nr_switch_events:
+	 *   - kernel/events/core.c|407| <<global>> static atomic_t nr_switch_events __read_mostly;
+	 *   - kernel/events/core.c|3824| <<__perf_event_task_sched_out>> if (atomic_read(&nr_switch_events))
+	 *   - kernel/events/core.c|4174| <<__perf_event_task_sched_in>> if (atomic_read(&nr_switch_events))
+	 *   - kernel/events/core.c|5117| <<unaccount_event>> atomic_dec(&nr_switch_events);
+	 *   - kernel/events/core.c|11825| <<account_event>> atomic_inc(&nr_switch_events);
+	 */
 	if (atomic_read(&nr_switch_events))
 		perf_event_switch(task, prev, true);
 
@@ -4154,6 +4414,11 @@ do {					\
 static DEFINE_PER_CPU(int, perf_throttled_count);
 static DEFINE_PER_CPU(u64, perf_throttled_seq);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4247| <<perf_adjust_freq_unthr_context>> perf_adjust_period(event, period, delta, false);
+ *   - kernel/events/core.c|9362| <<__perf_event_account_interrupt>> perf_adjust_period(event, delta, hwc->last_period, true);
+ */
 static void perf_adjust_period(struct perf_event *event, u64 nsec, u64 count, bool disable)
 {
 	struct hw_perf_event *hwc = &event->hw;
@@ -4773,6 +5038,11 @@ find_lively_task_by_vpid(pid_t vpid)
 	return task;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12444| <<SYSCALL_DEFINE5(perf_event_open,)>> ctx = find_get_context(pmu, task, event);
+ *   - kernel/events/core.c|12780| <<perf_event_create_kernel_counter>> ctx = find_get_context(event->pmu, task, event);
+ */
 /*
  * Returns a matching context with refcount and pincount.
  */
@@ -4804,6 +5074,9 @@ find_get_context(struct pmu *pmu, struct task_struct *task,
 	}
 
 	err = -EINVAL;
+	/*
+	 * 这个perf_hw_context
+	 */
 	ctxn = pmu->task_ctx_nr;
 	if (ctxn < 0)
 		goto errout;
@@ -4817,6 +5090,18 @@ find_get_context(struct pmu *pmu, struct task_struct *task,
 	}
 
 retry:
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|1567| <<perf_pin_task_context>> ctx = perf_lock_task_context(task, ctxn, &flags);
+	 *   - kernel/events/core.c|4857| <<find_get_context>> ctx = perf_lock_task_context(task, ctxn, &flags);
+	 *
+	 * struct task_struct:
+	 * #ifdef CONFIG_PERF_EVENTS
+	 *     struct perf_event_context       *perf_event_ctxp[perf_nr_task_contexts];
+	 *     struct mutex                    perf_event_mutex;
+	 *     struct list_head                perf_event_list;
+	 * #endif
+	 */
 	ctx = perf_lock_task_context(task, ctxn, &flags);
 	if (ctx) {
 		clone_ctx = unclone_ctx(ctx);
@@ -4854,6 +5139,14 @@ find_get_context(struct pmu *pmu, struct task_struct *task,
 		else {
 			get_ctx(ctx);
 			++ctx->pin_count;
+			/*
+			 * struct task_struct:
+			 * #ifdef CONFIG_PERF_EVENTS
+			 *     struct perf_event_context       *perf_event_ctxp[perf_nr_task_contexts];
+			 *     struct mutex                    perf_event_mutex;
+			 *     struct list_head                perf_event_list;
+			 * #endif
+			 */
 			rcu_assign_pointer(task->perf_event_ctxp[ctxn], ctx);
 		}
 		mutex_unlock(&task->perf_event_mutex);
@@ -5236,6 +5529,15 @@ static void put_event(struct perf_event *event)
  * object, it will not preserve its functionality. Once the last 'user'
  * gives up the object, we'll destroy the thing.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|207| <<kvm_pmu_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/arm64/kvm/pmu-emul.c|791| <<kvm_pmu_probe_pmuver>> perf_event_release_kernel(event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1043| <<measure_residency_fn>> perf_event_release_kernel(hit_event);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|1045| <<measure_residency_fn>> perf_event_release_kernel(miss_event);
+ *   - arch/x86/kvm/pmu.h|67| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|255| <<intel_pmu_release_guest_lbr_event>> perf_event_release_kernel(lbr_desc->event);
+ */
 int perf_event_release_kernel(struct perf_event *event)
 {
 	struct perf_event_context *ctx = event->ctx;
@@ -5683,6 +5985,10 @@ static void perf_event_for_each(struct perf_event *event,
 		perf_event_for_each_child(sibling, func);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5745| <<_perf_event_period>> event_function_call(event, __perf_event_period, &value);
+ */
 static void __perf_event_period(struct perf_event *event,
 				struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
@@ -5725,6 +6031,11 @@ static int perf_event_check_period(struct perf_event *event, u64 value)
 	return event->pmu->check_period(event, value);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|5756| <<perf_event_period>> ret = _perf_event_period(event, value);
+ *   - kernel/events/core.c|5811| <<_perf_ioctl>> return _perf_event_period(event, value);
+ */
 static int _perf_event_period(struct perf_event *event, u64 value)
 {
 	if (!is_sampling_event(event))
@@ -5747,6 +6058,11 @@ static int _perf_event_period(struct perf_event *event, u64 value)
 	return 0;
 }
 
+/*
+ * called by;
+ *   - arch/x86/kvm/pmu.c|166| <<pmc_resume_counter>> if (perf_event_period(pmc->perf_event,
+ *   - arch/x86/kvm/pmu.h|149| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+ */
 int perf_event_period(struct perf_event *event, u64 value)
 {
 	struct perf_event_context *ctx;
@@ -7795,6 +8111,15 @@ perf_event_read_event(struct perf_event *event,
 
 typedef void (perf_iterate_f)(struct perf_event *event, void *data);
 
+/*
+ * called by:
+ *   - kernel/events/core.c|8017| <<perf_iterate_sb>> perf_iterate_ctx(task_ctx, output, data, false);
+ *   - kernel/events/core.c|8026| <<perf_iterate_sb>> perf_iterate_ctx(ctx, output, data, false);
+ *   - kernel/events/core.c|8078| <<perf_event_exec>> perf_iterate_ctx(ctx, perf_event_addr_filters_exec,
+ *   - kernel/events/core.c|8129| <<__perf_pmu_output_stop>> perf_iterate_ctx(&cpuctx->ctx, __perf_event_output_stop, &ro, false);
+ *   - kernel/events/core.c|8131| <<__perf_pmu_output_stop>> perf_iterate_ctx(cpuctx->task_ctx, __perf_event_output_stop,
+ *   - kernel/events/core.c|8927| <<perf_addr_filters_adjust>> perf_iterate_ctx(ctx, __perf_addr_filters_adjust, vma, true);
+ */
 static void
 perf_iterate_ctx(struct perf_event_context *ctx,
 		   perf_iterate_f output,
@@ -7842,6 +8167,18 @@ static void perf_iterate_sb_cpu(perf_iterate_f output, void *data)
  * For new callers; ensure that account_pmu_sb_event() includes
  * your event, otherwise it might not get delivered.
  */
+/*
+ * called by:
+ *   - kernel/events/core.c|8267| <<perf_event_task>> perf_iterate_sb(perf_event_task_output,
+ *   - kernel/events/core.c|8347| <<perf_event_comm_event>> perf_iterate_sb(perf_event_comm_output,
+ *   - kernel/events/core.c|8502| <<perf_event_namespaces>> perf_iterate_sb(perf_event_namespaces_output,
+ *   - kernel/events/core.c|8597| <<perf_event_cgroup>> perf_iterate_sb(perf_event_cgroup_output,
+ *   - kernel/events/core.c|8823| <<perf_event_mmap_event>> perf_iterate_sb(perf_event_mmap_output,
+ *   - kernel/events/core.c|9119| <<perf_event_switch>> perf_iterate_sb(perf_event_switch_output, &switch_event, NULL);
+ *   - kernel/events/core.c|9248| <<perf_event_ksymbol>> perf_iterate_sb(perf_event_ksymbol_output, &ksymbol_event, NULL);
+ *   - kernel/events/core.c|9360| <<perf_event_bpf_event>> perf_iterate_sb(perf_event_bpf_output, &bpf_event, NULL);
+ *   - kernel/events/core.c|9444| <<perf_event_text_poke>> perf_iterate_sb(perf_event_text_poke_output, &text_poke_event, NULL);
+ */
 static void
 perf_iterate_sb(perf_iterate_f output, void *data,
 	       struct perf_event_context *task_ctx)
@@ -8037,6 +8374,10 @@ static int perf_event_task_match(struct perf_event *event)
 	       event->attr.task;
 }
 
+/*
+ * 在以下使用perf_event_task_output():
+ *   - kernel/events/core.c|8376| <<perf_event_task>> perf_iterate_sb(perf_event_task_output, &task_event, task_ctx);
+ */
 static void perf_event_task_output(struct perf_event *event,
 				   void *data)
 {
@@ -8934,6 +9275,11 @@ static void perf_event_switch_output(struct perf_event *event, void *data)
 	perf_output_end(&handle);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|3825| <<__perf_event_task_sched_out>> perf_event_switch(task, next, false);
+ *   - kernel/events/core.c|4175| <<__perf_event_task_sched_in>> perf_event_switch(task, prev, true);
+ */
 static void perf_event_switch(struct task_struct *task,
 			      struct task_struct *next_prev, bool sched_in)
 {
@@ -11638,6 +11984,10 @@ static void account_freq_event(void)
 }
 
 
+/*
+ * called by:
+ *   - kernel/events/core.c|12069| <<perf_event_alloc>> account_event(event);
+ */
 static void account_event(struct perf_event *event)
 {
 	bool inc = false;
@@ -11917,6 +12267,9 @@ perf_event_alloc(struct perf_event_attr *attr, int cpu,
 	if (err)
 		goto err_callchain_buffer;
 
+	/*
+	 * 只在此处调用
+	 */
 	/* symmetric to unaccount_event() in _free_event() */
 	account_event(event);
 
@@ -12747,6 +13100,20 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * 109         struct perf_event_attr attr = {
+ * 110                 .type = type,
+ * 111                 .size = sizeof(attr),
+ * 112                 .pinned = true,
+ * 113                 .exclude_idle = true,
+ * 114                 .exclude_host = 1,
+ * 115                 .exclude_user = exclude_user,
+ * 116                 .exclude_kernel = exclude_kernel,
+ * 117                 .config = config,
+ * 118         };
+ * 119
+ * 120         attr.sample_period = get_sample_period(pmc, pmc->counter);
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
@@ -12774,6 +13141,14 @@ perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 	/* Mark owner so we could distinguish it from user events. */
 	event->owner = TASK_TOMBSTONE;
 
+	/*
+	 * struct task_struct:
+	 * #ifdef CONFIG_PERF_EVENTS
+	 *     struct perf_event_context       *perf_event_ctxp[perf_nr_task_contexts];
+	 *     struct mutex                    perf_event_mutex;
+	 *     struct list_head                perf_event_list;
+	 * #endif
+	 */
 	/*
 	 * Get the target context (task or percpu):
 	 */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 25b8ea911..89e74f8fa 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4858,6 +4858,11 @@ prepare_task_switch(struct rq *rq, struct task_struct *prev,
  * past. prev == current is still correct but we need to recalculate this_rq
  * because prev may have moved to another CPU.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|4966| <<schedule_tail>> finish_task_switch(prev);
+ *   - kernel/sched/core.c|5033| <<context_switch>> return finish_task_switch(prev);
+ */
 static struct rq *finish_task_switch(struct task_struct *prev)
 	__releases(rq->lock)
 {
@@ -4951,6 +4956,12 @@ static struct rq *finish_task_switch(struct task_struct *prev)
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
  */
+/*
+ * called by:
+ *   - arch/s390/kernel/process.c|53| <<__ret_from_fork>> schedule_tail(prev);
+ *   - arch/um/kernel/process.c|124| <<new_thread_handler>> schedule_tail(current->thread.prev_sched);
+ *   - arch/um/kernel/process.c|142| <<fork_handler>> schedule_tail(current->thread.prev_sched);
+ */
 asmlinkage __visible void schedule_tail(struct task_struct *prev)
 	__releases(rq->lock)
 {
@@ -4975,6 +4986,10 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 /*
  * context_switch - switch to the new MM and the new thread's register state.
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|6376| <<__schedule>> rq = context_switch(rq, prev, next, &rf);
+ */
 static __always_inline struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next, struct rq_flags *rf)
@@ -6251,6 +6266,16 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
  *
  * WARNING: must be called with preemption disabled!
  */
+/*
+ * called by:
+ *   - kernel/sched/core.c|6394| <<do_task_dead>> __schedule(SM_NONE);
+ *   - kernel/sched/core.c|6459| <<schedule>> __schedule(SM_NONE);
+ *   - kernel/sched/core.c|6487| <<schedule_idle>> __schedule(SM_NONE);
+ *   - kernel/sched/core.c|6527| <<schedule_rtlock>> __schedule(SM_RTLOCK_WAIT);
+ *   - kernel/sched/core.c|6552| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+ *   - kernel/sched/core.c|6631| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+ *   - kernel/sched/core.c|6780| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+ */
 static void __sched notrace __schedule(unsigned int sched_mode)
 {
 	struct task_struct *prev, *next;
-- 
2.34.1

