From 6e3d6b3ec22a927298051d9edfb4a41a5c63c51b Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 22 Jul 2019 16:23:05 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.2

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-core.c                   |  61 +++
 block/blk-exec.c                   |  30 ++
 block/blk-flush.c                  |  79 ++++
 block/blk-lib.c                    |  28 ++
 block/blk-merge.c                  |  16 +
 block/blk-mq-cpumap.c              |  28 ++
 block/blk-mq-debugfs-zoned.c       |   4 +
 block/blk-mq-debugfs.c             |   9 +
 block/blk-mq-pci.c                 |  22 +
 block/blk-mq-rdma.c                |   5 +
 block/blk-mq-sched.c               | 269 +++++++++++++
 block/blk-mq-sched.h               |  50 +++
 block/blk-mq-sysfs.c               |  25 ++
 block/blk-mq-tag.c                 | 141 +++++++
 block/blk-mq-tag.h                 |  52 +++
 block/blk-mq-virtio.c              |  14 +
 block/blk-mq.c                     | 802 +++++++++++++++++++++++++++++++++++++
 block/blk-mq.h                     |  61 +++
 block/blk-settings.c               |  16 +
 block/blk-stat.c                   |   5 +
 block/blk-sysfs.c                  |   5 +
 block/bounce.c                     |   5 +
 block/genhd.c                      |  50 +++
 drivers/base/core.c                |   7 +
 drivers/block/null_blk_main.c      |  11 +
 drivers/block/virtio_blk.c         | 277 +++++++++++++
 drivers/nvme/host/core.c           |   4 +
 drivers/scsi/scsi.c                |   6 +
 drivers/scsi/scsi_lib.c            |  10 +
 drivers/virtio/virtio_pci_common.c |   5 +
 fs/direct-io.c                     |  13 +
 include/linux/blk-mq.h             | 205 ++++++++++
 include/linux/blk_types.h          |   5 +
 include/linux/blkdev.h             | 159 ++++++++
 include/linux/fs.h                 |  12 +
 include/linux/genhd.h              |  13 +
 lib/sbitmap.c                      |   5 +
 37 files changed, 2509 insertions(+)

diff --git a/block/blk-core.c b/block/blk-core.c
index 8340f69..3765839 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -66,6 +66,14 @@ struct kmem_cache *blk_requestq_cachep;
 /*
  * Controlling structure to kblockd
  */
+/*
+ * used by:
+ *   - block/blk-core.c|1627| <<kblockd_schedule_work>> return queue_work(kblockd_workqueue, work);
+ *   - block/blk-core.c|1633| <<kblockd_schedule_work_on>> return queue_work_on(cpu, kblockd_workqueue, work);
+ *   - block/blk-core.c|1640| <<kblockd_mod_delayed_work_on>> return mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);
+ *   - block/blk-core.c|1790| <<blk_dev_init>> kblockd_workqueue = alloc_workqueue("kblockd",
+ *   - block/blk-core.c|1792| <<blk_dev_init>> if (!kblockd_workqueue)
+ */
 static struct workqueue_struct *kblockd_workqueue;
 
 /**
@@ -528,6 +536,18 @@ EXPORT_SYMBOL(blk_get_queue);
  * @op: operation (REQ_OP_*) and REQ_* flags, e.g. REQ_SYNC.
  * @flags: BLK_MQ_REQ_* flags, e.g. BLK_MQ_REQ_NOWAIT.
  */
+/*
+ * 部分调用的例子:
+ *   - block/scsi_ioctl.c|310| <<sg_io>> rq = blk_get_request(q, writing ? REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, 0);
+ *   - block/scsi_ioctl.c|437| <<sg_scsi_ioctl>> rq = blk_get_request(q, in_len ? REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, 0);
+ *   - block/scsi_ioctl.c|525| <<__blk_send_generic>> rq = blk_get_request(q, REQ_OP_SCSI_OUT, 0);
+ *   - drivers/block/virtio_blk.c|463| <<virtblk_get_id>> req = blk_get_request(q, REQ_OP_DRV_IN, 0);
+ *   - drivers/scsi/osst.c|373| <<osst_execute>> req = blk_get_request(SRpnt->stp->device->request_queue,
+ *   - drivers/scsi/scsi_error.c|1952| <<scsi_eh_lock_door>> req = blk_get_request(sdev->request_queue, REQ_OP_SCSI_IN, 0);
+ *   - drivers/scsi/scsi_lib.c|246| <<__scsi_execute>> req = blk_get_request(sdev->request_queue,
+ *   - drivers/scsi/sg.c|1736| <<sg_start_req>> rq = blk_get_request(q, hp->dxfer_direction == SG_DXFER_TO_DEV ?
+ *   - drivers/scsi/st.c|549| <<st_scsi_execute>> req = blk_get_request(SRpnt->stp->device->request_queue,
+ */
 struct request *blk_get_request(struct request_queue *q, unsigned int op,
 				blk_mq_req_flags_t flags)
 {
@@ -1278,6 +1298,14 @@ void blk_account_io_done(struct request *req, u64 now)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|570| <<bio_attempt_back_merge>> blk_account_io_start(req, false);
+ *   - block/blk-core.c|593| <<bio_attempt_front_merge>> blk_account_io_start(req, false);
+ *   - block/blk-core.c|613| <<bio_attempt_discard_merge>> blk_account_io_start(req, false);
+ *   - block/blk-core.c|1191| <<blk_insert_cloned_request>> blk_account_io_start(rq, true);
+ *   - block/blk-mq.c|2038| <<blk_mq_bio_to_request>> blk_account_io_start(rq, true);
+ */
 void blk_account_io_start(struct request *rq, bool new_io)
 {
 	struct hd_struct *part;
@@ -1361,6 +1389,14 @@ EXPORT_SYMBOL_GPL(blk_steal_bios);
  *     %false - this request doesn't have any more data
  *     %true  - this request has more data
  **/
+/*
+ * called by:
+ *   - block/blk-mq.c|671| <<blk_mq_end_request>> if (blk_update_request(rq, error, blk_rq_bytes(rq)))
+ *   - drivers/block/loop.c|479| <<lo_complete_rq>> blk_update_request(rq, BLK_STS_OK, cmd->ret);
+ *   - drivers/ide/ide-io.c|70| <<ide_end_rq>> if (!blk_update_request(rq, error, nr_bytes)) {
+ *   - drivers/md/dm-rq.c|123| <<end_clone_bio>> blk_update_request(tio->orig, BLK_STS_OK, tio->completed);
+ *   - drivers/scsi/scsi_lib.c|571| <<scsi_end_request>> if (blk_update_request(req, error, bytes))
+ */
 bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
@@ -1659,6 +1695,10 @@ void blk_start_plug(struct blk_plug *plug)
 }
 EXPORT_SYMBOL(blk_start_plug);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1730| <<blk_flush_plug_list>> flush_plug_callbacks(plug, from_schedule);
+ */
 static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
 {
 	LIST_HEAD(callbacks);
@@ -1676,6 +1716,15 @@ static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_req.c|1302| <<drbd_check_plugged>> struct blk_plug_cb *cb = blk_check_plugged(drbd_unplug, resource, sizeof(*plug));
+ *   - drivers/block/umem.c|519| <<mm_check_plugged>> return !!blk_check_plugged(mm_unplug, card, sizeof(struct blk_plug_cb));
+ *   - drivers/md/raid1.c|1503| <<raid1_write_request>> cb = blk_check_plugged(raid1_unplug, mddev, sizeof(*plug));
+ *   - drivers/md/raid10.c|1286| <<raid10_write_one_disk>> cb = blk_check_plugged(raid10_unplug, mddev, sizeof(*plug));
+ *   - drivers/md/raid5.c|5458| <<release_stripe_plug>> struct blk_plug_cb *blk_cb = blk_check_plugged(
+ *   - fs/btrfs/raid56.c|1773| <<raid56_parity_write>> cb = blk_check_plugged(btrfs_raid_unplug, fs_info, sizeof(*plug));
+ */
 struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 				      int size)
 {
@@ -1701,6 +1750,14 @@ struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 }
 EXPORT_SYMBOL(blk_check_plugged);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1734| <<blk_finish_plug>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|2361| <<blk_mq_make_request>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|3947| <<blk_poll>> blk_flush_plug_list(current->plug, false);
+ *   - include/linux/blkdev.h|1232| <<blk_flush_plug>> blk_flush_plug_list(plug, false);
+ *   - include/linux/blkdev.h|1240| <<blk_schedule_flush_plug>> blk_flush_plug_list(plug, true);
+ */
 void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	flush_plug_callbacks(plug, from_schedule);
@@ -1729,6 +1786,10 @@ void blk_finish_plug(struct blk_plug *plug)
 }
 EXPORT_SYMBOL(blk_finish_plug);
 
+/*
+ * called by:
+ *   - block/genhd.c|1110| <<genhd_device_init>> blk_dev_init();
+ */
 int __init blk_dev_init(void)
 {
 	BUILD_BUG_ON(REQ_OP_LAST >= (1 << REQ_OP_BITS));
diff --git a/block/blk-exec.c b/block/blk-exec.c
index 1db44ca..4788ad9 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -17,6 +17,10 @@
  * @rq: request to complete
  * @error: end I/O status of the request
  */
+/*
+ * called by:
+ *   - block/blk-exec.c|84| <<blk_execute_rq>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+ */
 static void blk_end_sync_rq(struct request *rq, blk_status_t error)
 {
 	struct completion *waiting = rq->end_io_data;
@@ -45,6 +49,19 @@ static void blk_end_sync_rq(struct request *rq, blk_status_t error)
  * Note:
  *    This function will invoke @done directly if the queue is dead.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-exec.c|84| <<blk_execute_rq>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+ *   - drivers/nvme/host/core.c|758| <<nvme_execute_rq_polled>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+ *   - drivers/nvme/host/core.c|943| <<nvme_keep_alive>> blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
+ *   - drivers/nvme/host/lightnvm.c|688| <<nvme_nvm_submit_io>> blk_execute_rq_nowait(q, NULL, rq, 0, nvme_nvm_end_io);
+ *   - drivers/nvme/host/pci.c|1352| <<nvme_timeout>> blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+ *   - drivers/nvme/host/pci.c|2217| <<nvme_delete_queue>> blk_execute_rq_nowait(q, NULL, req, false,
+ *   - drivers/scsi/osst.c|422| <<osst_execute>> blk_execute_rq_nowait(req->q, NULL, req, 1, osst_end_async);
+ *   - drivers/scsi/scsi_error.c|1969| <<scsi_eh_lock_door>> blk_execute_rq_nowait(req->q, NULL, req, 1, eh_lock_door_done);
+ *   - drivers/scsi/sg.c|838| <<sg_common_write>> blk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,
+ *   - drivers/scsi/st.c|587| <<st_scsi_execute>> blk_execute_rq_nowait(req->q, NULL, req, 1, st_scsi_execute_end);
+ */
 void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   struct request *rq, int at_head,
 			   rq_end_io_fn *done)
@@ -74,6 +91,19 @@ EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);
  *    Insert a fully prepared request at the back of the I/O scheduler queue
  *    for execution and wait for completion.
  */
+/*
+ * 调用的部分例子:
+ *   - block/bsg.c|184| <<bsg_sg_io>> blk_execute_rq(q, NULL, rq, !(hdr.flags & BSG_FLAG_Q_AT_TAIL));
+ *   - block/scsi_ioctl.c|357| <<sg_io>> blk_execute_rq(q, bd_disk, rq, at_head); 
+ *   - block/scsi_ioctl.c|493| <<sg_scsi_ioctl>> blk_execute_rq(q, disk, rq, 0);
+ *   - block/scsi_ioctl.c|532| <<__blk_send_generic>> blk_execute_rq(q, bd_disk, rq, 0);
+ *   - drivers/block/virtio_blk.c|381| <<virtblk_get_id>> blk_execute_rq(vblk->disk->queue, vblk->disk, req, false);
+ *   - drivers/nvme/host/core.c|793| <<__nvme_submit_sync_cmd>> blk_execute_rq(req->q, NULL, req, at_head);
+ *   - drivers/nvme/host/core.c|886| <<nvme_submit_user_cmd>> blk_execute_rq(req->q, disk, req, 0);
+ *   - drivers/nvme/host/lightnvm.c|709| <<nvme_nvm_submit_io_sync>> blk_execute_rq(q, NULL, rq, 0);
+ *   - drivers/nvme/host/lightnvm.c|838| <<nvme_nvm_submit_user_cmd>> blk_execute_rq(q, NULL, rq, 0);
+ *   - drivers/scsi/scsi_lib.c|267| <<__scsi_execute>> blk_execute_rq(req->q, NULL, req, 1);
+ */
 void blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 		   struct request *rq, int at_head)
 {
diff --git a/block/blk-flush.c b/block/blk-flush.c
index aedd932..e28e094 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -75,6 +75,27 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
@@ -254,6 +275,10 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
  * spin_lock_irq(fq->mq_flush_lock)
  *
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|203| <<blk_flush_complete_seq>> blk_kick_flush(q, fq, cmd_flags);
+ */
 static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 			   unsigned int flags)
 {
@@ -313,6 +338,22 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|729| <<__blk_mq_end_request>> rq->end_io(rq, error); 
+ *   - drivers/lightnvm/core.c|785| <<nvm_end_io>> rqd->end_io(rqd);
+ *   - drivers/md/dm-bufio.c|526| <<dmio_complete>> b->end_io(b, unlikely(error != 0) ? BLK_STS_IOERR : 0);
+ *   - drivers/md/dm-bufio.c|556| <<use_dmio>> b->end_io(b, errno_to_blk_status(r));
+ *   - drivers/md/dm-bufio.c|564| <<bio_complete>> b->end_io(b, status);
+ *   - drivers/md/dm-mpath.c|559| <<multipath_release_clone>> pgpath->pg->ps.type->end_io(&pgpath->pg->ps,
+ *   - drivers/md/dm-mpath.c|1596| <<multipath_end_io>> ps->type->end_io(ps, &pgpath->path, mpio->nr_bytes);
+ *   - drivers/md/dm-mpath.c|1640| <<multipath_end_io_bio>> ps->type->end_io(ps, &pgpath->path, mpio->nr_bytes);
+ *   - fs/direct-io.c|292| <<dio_complete>> err = dio->end_io(dio->iocb, offset, ret, dio->private);
+ *   - fs/iomap.c|1507| <<iomap_dio_complete>> ret = dio->end_io(iocb,
+ *
+ * used by:
+ *   - block/blk-flush.c|434| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -346,6 +387,11 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|488| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2281| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -419,6 +465,31 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|2519| <<bitmap_flush_work>> blkdev_issue_flush(ic->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|647| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|681| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|745| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1040| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|182| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|687| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|157| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1427| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5142| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/fat/file.c|207| <<fat_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|242| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|417| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|770| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|874| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1024| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|197| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|168| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|671| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -461,6 +532,10 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2703| <<blk_mq_alloc_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -490,6 +565,10 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq-sysfs.c|43| <<blk_mq_hw_sysfs_release>> blk_free_flush_queue(hctx->fq);
+ */
 void blk_free_flush_queue(struct blk_flush_queue *fq)
 {
 	/* bio based request queue hasn't flush queue */
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 5f2c429..44c389d 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -22,6 +22,16 @@ struct bio *blk_next_bio(struct bio *bio, unsigned int nr_pages, gfp_t gfp)
 	return new;
 }
 
+/*
+ * called by:
+ *   - block/blk-lib.c|105| <<blkdev_issue_discard>> ret = __blkdev_issue_discard(bdev, sector, nr_sects, gfp_mask, flags,
+ *   - drivers/md/dm-thin.c|401| <<issue_discard>> return __blkdev_issue_discard(tc->pool_dev->bdev, s, len,
+ *   - drivers/md/raid0.c|532| <<raid0_handle_discard>> if (__blkdev_issue_discard(rdev->bdev,
+ *   - drivers/nvme/target/io-cmd-bdev.c|193| <<nvmet_bdev_discard_range>> ret = __blkdev_issue_discard(ns->bdev,
+ *   - fs/ext4/mballoc.c|2770| <<ext4_issue_discard>> return __blkdev_issue_discard(sb->s_bdev,
+ *   - fs/f2fs/segment.c|1148| <<__submit_discard_cmd>> err = __blkdev_issue_discard(bdev,
+ *   - fs/xfs/xfs_log_cil.c|548| <<xlog_discard_busy_extents>> error = __blkdev_issue_discard(mp->m_ddev_targp->bt_bdev,
+ */
 int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, int flags,
 		struct bio **biop)
@@ -94,6 +104,24 @@ EXPORT_SYMBOL(__blkdev_issue_discard);
  * Description:
  *    Issue a discard request for the sectors in question.
  */
+/*
+ * called by:
+ *   - block/blk-lib.c|97| <<blkdev_issue_discard>> int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
+ *   - block/ioctl.c|230| <<blk_ioctl_discard>> return blkdev_issue_discard(bdev, start >> 9, len >> 9,
+ *   - drivers/block/xen-blkback/blkback.c|1034| <<dispatch_discard_io>> err = blkdev_issue_discard(bdev, req->u.discard.sector_number,
+ *   - drivers/md/bcache/alloc.c|338| <<bch_allocator_thread>> blkdev_issue_discard(ca->bdev,
+ *   - drivers/md/raid5-cache.c|1347| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev,
+ *   - drivers/md/raid5-cache.c|1351| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev,
+ *   - drivers/md/raid5-cache.c|1355| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev, log->rdev->data_offset, end,
+ *   - fs/block_dev.c|2070| <<blkdev_fallocate>> error = blkdev_issue_discard(bdev, start >> 9, len >> 9,
+ *   - fs/btrfs/extent-tree.c|1957| <<btrfs_issue_discard>> ret = blkdev_issue_discard(bdev, start >> 9, size >> 9,
+ *   - fs/btrfs/extent-tree.c|1974| <<btrfs_issue_discard>> ret = blkdev_issue_discard(bdev, start >> 9, bytes_left >> 9,
+ *   - fs/xfs/xfs_discard.c|114| <<xfs_trim_extents>> error = blkdev_issue_discard(bdev, dbno, dlen, GFP_NOFS, 0);
+ *   - include/linux/blkdev.h|1276| <<sb_issue_discard>> return blkdev_issue_discard(sb->s_bdev,
+ *   - mm/swapfile.c|171| <<discard_swap>> err = blkdev_issue_discard(si->bdev, start_block,
+ *   - mm/swapfile.c|182| <<discard_swap>> err = blkdev_issue_discard(si->bdev, start_block,
+ *   - mm/swapfile.c|219| <<discard_swap_cluster>> if (blkdev_issue_discard(si->bdev, start_block,
+ */
 int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags)
 {
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 17713d7..b0df023 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -144,6 +144,11 @@ static inline unsigned get_max_io_size(struct request_queue *q,
 	return sectors;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|176| <<bvec_split_segs>> seg_size = get_max_segment_size(q, bv->bv_offset + total_len);
+ *   - block/blk-merge.c|389| <<blk_bvec_map_sg>> unsigned len = min(get_max_segment_size(q, offset), nbytes);
+ */
 static unsigned get_max_segment_size(struct request_queue *q,
 				     unsigned offset)
 {
@@ -194,6 +199,10 @@ static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 	return !!len;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|278| <<blk_queue_split>> split = blk_bio_segment_split(q, *bio, &q->bio_split, &nsegs);
+ */
 static struct bio *blk_bio_segment_split(struct request_queue *q,
 					 struct bio *bio,
 					 struct bio_set *bs,
@@ -258,6 +267,13 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return do_split ? new : NULL;
 }
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|2246| <<blk_mq_make_request>> blk_queue_split(q, &bio);
+ *   - drivers/md/dm.c|1765| <<dm_process_bio>> blk_queue_split(md->queue, &bio);
+ *   - drivers/md/md.c|307| <<md_make_request>> blk_queue_split(q, &bio);
+ *   - drivers/nvme/host/multipath.c|241| <<nvme_ns_head_make_request>> blk_queue_split(q, &bio);
+ */
 void blk_queue_split(struct request_queue *q, struct bio **bio)
 {
 	struct bio *split, *res;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index f945621..33df8dc 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,12 +15,21 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|49| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ *   - block/blk-mq-cpumap.c|53| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ */
 static int cpu_to_queue_index(struct blk_mq_queue_map *qmap,
 			      unsigned int nr_queues, const int cpu)
 {
 	return qmap->queue_offset + (cpu % nr_queues);
 }
 
+/*
+ * called by:
+ *    - block/blk-mq-cpumap.c|51| <<blk_mq_map_queues>> first_sibling = get_first_sibling(cpu);
+ */
 static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
@@ -32,6 +41,19 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3077| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3349| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|470| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1827| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2145| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7116| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1750| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -68,6 +90,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2142| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2198| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2843| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-debugfs-zoned.c b/block/blk-mq-debugfs-zoned.c
index 038cb62..ac46d52 100644
--- a/block/blk-mq-debugfs-zoned.c
+++ b/block/blk-mq-debugfs-zoned.c
@@ -6,6 +6,10 @@
 #include <linux/blkdev.h>
 #include "blk-mq-debugfs.h"
 
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|207| <<global>> { "zone_wlock", 0400, queue_zone_wlock_show, NULL },
+ */
 int queue_zone_wlock_show(void *data, struct seq_file *m)
 {
 	struct request_queue *q = data;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index 3afe327..ce471da 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -834,6 +834,10 @@ static void debugfs_create_files(struct dentry *parent, void *data,
 				    (void *)attr, &blk_mq_debugfs_fops);
 }
 
+/*
+ * called only by:
+ *   - block/blk-sysfs.c|992| <<blk_register_queue>> blk_mq_debugfs_register(q);
+ */
 void blk_mq_debugfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -985,6 +989,11 @@ void blk_mq_debugfs_unregister_queue_rqos(struct request_queue *q)
 	q->rqos_debugfs_dir = NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|860| <<blk_mq_debugfs_register>> blk_mq_debugfs_register_sched_hctx(q, hctx);
+ *   - block/blk-mq-sched.c|536| <<blk_mq_init_sched>> blk_mq_debugfs_register_sched_hctx(q, hctx);
+ */
 void blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 					struct blk_mq_hw_ctx *hctx)
 {
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94..818d826 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -23,6 +23,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|468| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7118| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5806| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
@@ -30,10 +36,26 @@ int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 	unsigned int queue, cpu;
 
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
+		/* return the affinity of a particular msi vector */
 		mask = pci_irq_get_affinity(pdev, queue + offset);
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * queue_offset设置的地方:
+		 *   - drivers/nvme/host/pci.c|466| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|1801| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|1804| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|1810| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|1813| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2130| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2133| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2139| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2142| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *
+		 * mq_map[cpu]记录着当前的sw queue对应的hw queue
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e..4643058 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1815| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|1817| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 2766066..96e37d5 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -17,6 +17,9 @@
 #include "blk-mq-tag.h"
 #include "blk-wbt.h"
 
+/*
+ * 目前没有调用
+ */
 void blk_mq_sched_free_hctx_data(struct request_queue *q,
 				 void (*exit)(struct blk_mq_hw_ctx *))
 {
@@ -32,6 +35,10 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_free_hctx_data);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|433| <<blk_mq_get_request>> blk_mq_sched_assign_ioc(rq);
+ */
 void blk_mq_sched_assign_ioc(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -62,8 +69,22 @@ void blk_mq_sched_assign_ioc(struct request *rq)
  * Mark a hardware queue as needing a restart. For shared queues, maintain
  * a count of how many hardware queues are marked for restart.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|213| <<blk_mq_sched_dispatch_requests>> blk_mq_sched_mark_restart_hctx(hctx);
+ *   - block/blk-mq.c|1197| <<blk_mq_mark_tag_wait>> blk_mq_sched_mark_restart_hctx(hctx);
+ *   - block/mq-deadline.c|397| <<dd_dispatch_request>> blk_mq_sched_mark_restart_hctx(hctx);
+ */
 void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 
@@ -71,8 +92,21 @@ void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|337| <<mq_flush_data_end_io>> blk_mq_sched_restart(hctx);
+ *   - block/blk-mq.c|524| <<__blk_mq_free_request>> blk_mq_sched_restart(hctx);
+ */
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 	clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
@@ -85,6 +119,16 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|229| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|234| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *
+ * 核心思想用scheduler的ops.dispatch_request(hctx)获取下一个request
+ * 使用queue_rq()下发
+ * 如果有下发不了的放入hctx->dispatch
+ * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -112,6 +156,13 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 		 * in blk_mq_dispatch_rq_list().
 		 */
 		list_add(&rq->queuelist, &rq_list);
+
+		/*
+		 * blk_mq_dispatch_rq_list():
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 }
 
@@ -131,6 +182,16 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|240| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *   - block/blk-mq-sched.c|246| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *
+ * 在budget允许的范围内
+ * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request, 然后调用queue_rq()
+ * 如果有下发不了的放入hctx->dispatch
+ * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -146,6 +207,9 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request
+		 */
 		rq = blk_mq_dequeue_from_ctx(hctx, ctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -162,11 +226,25 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		/* round robin for fair dispatch */
 		ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
 
+		/*
+		 * blk_mq_dispatch_rq_list():
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1478| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -178,12 +256,36 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
 		return;
 
+	/*
+	 * used only by:
+	 *   - block/blk-mq-debugfs.c|612| <<hctx_run_show>> seq_printf(m, "%lu\n", hctx->run);
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_run_write>> hctx->run = 0;
+	 *   - block/blk-mq-sched.c|229| <<blk_mq_sched_dispatch_requests>> hctx->run++;
+	 */
 	hctx->run++;
 
 	/*
 	 * If we have previous entries on our dispatch list, grab them first for
 	 * more fair dispatch.
 	 */
+	/*
+	 * 往hctx->dispatch添加新元素的地方:
+	 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * 从hctx->dispatch移除元素的地方(下发):
+	 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 *
+	 * 其他使用hctx->dispatch的地方:
+	 *   - block/blk-mq-debugfs.c|379| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+	 *   - block/blk-mq-debugfs.c|386| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+	 *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+	 *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+	 *   - block/blk-mq.c|72| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+	 *   - block/blk-mq.c|2474| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+	 */
 	if (!list_empty_careful(&hctx->dispatch)) {
 		spin_lock(&hctx->lock);
 		if (!list_empty(&hctx->dispatch))
@@ -206,23 +308,81 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	 */
 	if (!list_empty(&rq_list)) {
 		blk_mq_sched_mark_restart_hctx(hctx);
+		/*
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+			/*
+			 * 上面设置的has_sched_dispatch为true说明有io scheduler
+			 *
+			 *
+			 * blk_mq_do_dispatch_sched():
+			 * 核心思想用scheduler的ops.dispatch_request(hctx)获取下一个request
+			 * 使用queue_rq()下发
+			 * 如果有下发不了的放入hctx->dispatch
+			 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+			 *
+			 *
+			 * blk_mq_do_dispatch_ctx():
+			 * 在budget允许的范围内
+			 * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request, 然后调用queue_rq()
+			 * 如果有下发不了的放入hctx->dispatch
+			 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+			 */
 			if (has_sched_dispatch)
 				blk_mq_do_dispatch_sched(hctx);
 			else
 				blk_mq_do_dispatch_ctx(hctx);
 		}
 	} else if (has_sched_dispatch) {
+		/*
+		 * 核心思想用scheduler的ops.dispatch_request(hctx)获取下一个request
+		 * 使用queue_rq()下发
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		blk_mq_do_dispatch_sched(hctx);
 	} else if (hctx->dispatch_busy) {
+		/*
+		 * 设置dispatch_busy的地方:
+		 *   - block/blk-mq.c|1445| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+		 *
+		 * 使用dispatch_busy的地方:
+		 *   - block/blk-mq-debugfs.c|637| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+		 *   - block/blk-mq-sched.c|338| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+		 *   - block/blk-mq-sched.c|636| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+		 *   - block/blk-mq.c|1435| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+		 *   - block/blk-mq.c|2554| <<blk_mq_make_request>> !data.hctx->dispatch_busy)) {
+		 */
 		/* dequeue request one by one from sw queue if queue is busy */
+		/*
+		 * 在budget允许的范围内
+		 * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request, 然后调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		blk_mq_do_dispatch_ctx(hctx);
 	} else {
+		/*
+		 * 把hctx所属的ctx的ctx->rq_lists的request放入参数的list
+		 */
 		blk_mq_flush_busy_ctxs(hctx, &rq_list);
+		/*
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		blk_mq_dispatch_rq_list(q, &rq_list, false);
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|2053| <<bfq_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ *   - block/mq-deadline.c|480| <<dd_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ */
 bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 			    struct request **merged_request)
 {
@@ -259,6 +419,11 @@ EXPORT_SYMBOL_GPL(blk_mq_sched_try_merge);
  * Iterate list of requests and see if we can merge this bio with any
  * of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|340| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/kyber-iosched.c|575| <<kyber_bio_merge>> merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio);
+ */
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio)
 {
@@ -302,6 +467,12 @@ EXPORT_SYMBOL_GPL(blk_mq_bio_list_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|370| <<__blk_mq_sched_bio_merge>> ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
+ *
+ * 尝试把bio给merge到ctx->rq_lists
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_hw_ctx *hctx,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
@@ -318,6 +489,13 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.h|39| <<blk_mq_sched_bio_merge>> return __blk_mq_sched_bio_merge(q, bio);
+ *
+ * 如果支持scheduler就把bio用ops.bio_merge给merge了
+ * 否则尝试把bio给merge到ctx->rq_lists
+ */
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
@@ -336,6 +514,9 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 			!list_empty_careful(&ctx->rq_lists[type])) {
 		/* default per sw-queue merge */
 		spin_lock(&ctx->lock);
+		/*
+		 * 尝试把bio给merge到ctx->rq_lists
+		 */
 		ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
 		spin_unlock(&ctx->lock);
 	}
@@ -344,6 +525,11 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|5052| <<bfq_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq)) {
+ *   - block/mq-deadline.c|505| <<dd_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq))
+ */
 bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
 	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
@@ -356,11 +542,24 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|420| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ *
+ * 只有rq->rq_flags设置了RQF_FLUSH_SEQ才会添加到hctx->dispatch并且返回true,
+ * 否则返回false (不能bypass sched)
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
 {
 	/* dispatch flush rq directly */
+	/*
+	 * 设置和取消RQF_FLUSH_SEQ的地方:
+	 *   - block/blk-flush.c|130| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|309| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|401| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 */
 	if (rq->rq_flags & RQF_FLUSH_SEQ) {
 		spin_lock(&hctx->lock);
 		list_add(&rq->queuelist, &hctx->dispatch);
@@ -374,6 +573,22 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|62| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq-sched.h|21| <<blk_mq_sched_insert_request>> void blk_mq_sched_insert_request(struct request *rq, bool at_head,
+ *   - block/blk-mq.c|844| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|850| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2151| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *
+ * 1. 如果没有RQF_FLUSH_SEQ但是是flush则blk_insert_flush()
+ * 2. 试试能否blk_mq_sched_bypass_insert(): 比如是否有RQF_FLUSH_SEQ
+ * 3. 如果支持IO调度则用scheduler的.insert_requests()
+ * 4. 否则把request放入request->mq_ctx的rq_lists, 然后把ctx在hctx->ctx_map对应的bit设置
+ * 根据参数是否blk_mq_run_hw_queue()!
+ *                         
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -382,6 +597,13 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
 
+	/*
+	 * 设置和取消RQF_FLUSH_SEQ的地方:
+	 *   - block/blk-flush.c|130| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|309| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|401| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 */
+
 	/* flush rq in flush machinery need to be dispatched directly */
 	if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
 		blk_insert_flush(rq);
@@ -390,6 +612,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 
 	WARN_ON(e && (rq->tag != -1));
 
+	/*
+	 * 只有rq->rq_flags设置了RQF_FLUSH_SEQ才会添加到hctx->dispatch并且返回true,
+	 * 否则返回false (不能bypass sched)
+	 */
 	if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
 		goto run;
 
@@ -400,6 +626,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		e->type->ops.insert_requests(hctx, &list, at_head);
 	} else {
 		spin_lock(&ctx->lock);
+		/*
+		 * 把request放入ctx->rq_lists的头或者尾
+		 * 把ctx在hctx中对应的sbitmap设置上
+		 */
 		__blk_mq_insert_request(hctx, rq, at_head);
 		spin_unlock(&ctx->lock);
 	}
@@ -409,6 +639,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1854| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|1875| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -437,6 +672,10 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 			if (list_empty(list))
 				goto out;
 		}
+		/*
+		 * 把参数list中的request放入ctx->rq_lists
+		 * 并且标记ctx在hctx上pending(blk_mq_hctx_mark_pending)
+		 */
 		blk_mq_insert_requests(hctx, ctx, list);
 	}
 
@@ -445,6 +684,10 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 	percpu_ref_put(&q->q_usage_counter);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|509| <<blk_mq_sched_alloc_tags>> blk_mq_sched_free_tags(set, hctx, hctx_idx);
+ */
 static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -456,6 +699,10 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|536| <<blk_mq_init_sched>> ret = blk_mq_sched_alloc_tags(q, hctx, i);
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -476,6 +723,11 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 }
 
 /* called in queue's release handler, tagset has gone away */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|565| <<blk_mq_init_sched>> blk_mq_sched_tags_teardown(q);
+ *   - block/blk-mq-sched.c|602| <<blk_mq_exit_sched>> blk_mq_sched_tags_teardown(q);
+ */
 static void blk_mq_sched_tags_teardown(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -489,6 +741,11 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|576| <<elevator_switch_mq>> ret = blk_mq_init_sched(q, new_e);
+ *   - block/elevator.c|622| <<elevator_init_mq>> err = blk_mq_init_sched(q, e);
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -549,6 +806,13 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
  * called in either blk_queue_cleanup or elevator_switch, tagset
  * is required for freeing requests
  */
+/*
+ * called by:
+ *   - block/blk-core.c|333| <<blk_cleanup_queue>> blk_mq_sched_free_requests(q);
+ *   - block/blk-mq-sched.c|552| <<blk_mq_init_sched>> blk_mq_sched_free_requests(q);
+ *   - block/blk-mq-sched.c|564| <<blk_mq_init_sched>> blk_mq_sched_free_requests(q);
+ *   - block/blk.h|187| <<elevator_exit>> blk_mq_sched_free_requests(q);
+ */
 void blk_mq_sched_free_requests(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -562,6 +826,11 @@ void blk_mq_sched_free_requests(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|553| <<blk_mq_init_sched>> blk_mq_exit_sched(q, eq);
+ *   - block/elevator.c|185| <<__elevator_exit>> blk_mq_exit_sched(q, e);
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index 3cf92cb..26d0a54 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -30,15 +30,37 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e);
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e);
 void blk_mq_sched_free_requests(struct request_queue *q);
 
+/*
+ * called by only;
+ *   - block/blk-mq.c|2065| <<blk_mq_make_request>> if (blk_mq_sched_bio_merge(q, bio))
+ *
+ * 如果支持scheduler就把bio用ops.bio_merge给merge了
+ * 否则尝试把bio给merge到ctx->rq_lists
+ */
 static inline bool
 blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
+	/*
+	 * blk_queue_nomerges():
+	 * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+	 */
 	if (blk_queue_nomerges(q) || !bio_mergeable(bio))
 		return false;
 
+	/*
+	 * 如果支持scheduler就把bio用ops.bio_merge给merge了
+	 * 否则尝试把bio给merge到ctx->rq_lists
+	 */
 	return __blk_mq_sched_bio_merge(q, bio);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|238| <<blk_mq_sched_try_merge>> if (!blk_mq_sched_allow_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|247| <<blk_mq_sched_try_merge>> if (!blk_mq_sched_allow_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|284| <<blk_mq_bio_list_merge>> if (blk_mq_sched_allow_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|288| <<blk_mq_bio_list_merge>> if (blk_mq_sched_allow_merge(q, rq, bio))
+ */
 static inline bool
 blk_mq_sched_allow_merge(struct request_queue *q, struct request *rq,
 			 struct bio *bio)
@@ -51,6 +73,10 @@ blk_mq_sched_allow_merge(struct request_queue *q, struct request *rq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|576| <<__blk_mq_end_request>> blk_mq_sched_completed_request(rq, now);
+ */
 static inline void blk_mq_sched_completed_request(struct request *rq, u64 now)
 {
 	struct elevator_queue *e = rq->q->elevator;
@@ -59,6 +85,10 @@ static inline void blk_mq_sched_completed_request(struct request *rq, u64 now)
 		e->type->ops.completed_request(rq, now);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|702| <<blk_mq_start_request>> blk_mq_sched_started_request(rq);
+ */
 static inline void blk_mq_sched_started_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -68,6 +98,10 @@ static inline void blk_mq_sched_started_request(struct request *rq)
 		e->type->ops.started_request(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|753| <<blk_mq_requeue_request>> blk_mq_sched_requeue_request(rq);
+ */
 static inline void blk_mq_sched_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -77,6 +111,10 @@ static inline void blk_mq_sched_requeue_request(struct request *rq)
 		e->type->ops.requeue_request(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|70| <<blk_mq_hctx_has_pending>> blk_mq_sched_has_work(hctx);
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
@@ -87,8 +125,20 @@ static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1359| <<blk_mq_dispatch_rq_list>> needs_restart = blk_mq_sched_needs_restart(hctx);
+ */
 static inline bool blk_mq_sched_needs_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 }
 
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index d6e1a9b..7ade772 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -243,6 +243,11 @@ static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 	kobject_del(&hctx->kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|342| <<__blk_mq_register_dev>> ret = blk_mq_register_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|409| <<blk_mq_sysfs_register>> ret = blk_mq_register_hctx(hctx);
+ */
 static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -314,6 +319,15 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|357| <<blk_mq_register_dev>> ret = __blk_mq_register_dev(dev, q); --> 没人调用
+ *   - block/blk-sysfs.c|986| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ *
+ * blk_register_queue()
+ *  -> __blk_mq_register_dev()
+ *      -> blk_mq_register_hctx()
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -322,6 +336,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	WARN_ON_ONCE(!q->kobj.parent);
 	lockdep_assert_held(&q->sysfs_lock);
 
+	/* 第二个参数是parent */
 	ret = kobject_add(q->mq_kobj, kobject_get(&dev->kobj), "%s", "mq");
 	if (ret < 0)
 		goto out;
@@ -349,6 +364,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/* 没人调用 */
 int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	int ret;
@@ -376,12 +392,21 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3356| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
 	int i, ret = 0;
 
 	mutex_lock(&q->sysfs_lock);
+	/*
+	 * 在以下修改q->mq_sysfs_init_done:
+	 *   - block/blk-mq-sysfs.c|282| <<blk_mq_unregister_dev>> q->mq_sysfs_init_done = false;
+	 *   - block/blk-mq-sysfs.c|337| <<__blk_mq_register_dev>> q->mq_sysfs_init_done = true;
+	 */
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 7513c8e..30759ab 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -28,6 +28,12 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|61| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ *
+ * 应该就是表示当前hctx在被使用
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
@@ -40,6 +46,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|63| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|280| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -51,6 +62,12 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|111| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ *
+ * 应该就是表示当前hctx空闲
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -67,13 +84,33 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|110| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ *
+ * 我们应该是期待hctx_may_queue()返回true, 才能获得一个tag吧
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
 	unsigned int depth, users;
 
+	/*
+	 * 如果没设置BLK_MQ_F_TAG_SHARED, 一定返回true
+	 *
+	 * 说明scsi多个lun或者nvme多个namespace才可能返回false
+	 */
 	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return true;
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.c|37| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|38| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|104| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * 没设置BLK_MQ_S_TAG_ACTIVE反而返回true???
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		return true;
 
@@ -83,6 +120,13 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	if (bt->sb.depth == 1)
 		return true;
 
+	/*
+	 * 在以下使用active_queues:
+	 *   - block/blk-mq-debugfs.c|465| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|66| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|91| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	users = atomic_read(&hctx->tags->active_queues);
 	if (!users)
 		return true;
@@ -94,9 +138,22 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|163| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|186| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|192| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+	/*
+	 * 在以下使用BLK_MQ_REQ_INTERNAL:
+	 *   - block/blk-mq-tag.c|127| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|322| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.c|395| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -106,8 +163,17 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|393| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1068| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
@@ -172,6 +238,10 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		data->ctx = blk_mq_get_ctx(data->q);
 		data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
 						data->ctx);
+		/*
+		 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+		 * 否则返回data->hctx->tags
+		 */
 		tags = blk_mq_tags_from_data(data);
 		if (data->flags & BLK_MQ_REQ_RESERVED)
 			bt = &tags->breserved_tags;
@@ -198,6 +268,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|517| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|519| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|225| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
@@ -254,6 +330,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|492| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|493| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -332,6 +413,10 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|360| <<blk_mq_tagset_busy_iter>> blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
+ */
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -350,6 +435,26 @@ static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|430| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - drivers/block/mtip32xx/mtip32xx.c|2685| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags, mtip_queue_cmd, dd);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2690| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags,
+ *   - drivers/block/mtip32xx/mtip32xx.c|3808| <<mtip_block_remove>> blk_mq_tagset_busy_iter(&dd->tags, mtip_no_dev_cleanup, dd);
+ *   - drivers/block/nbd.c|762| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/block/skd_main.c|396| <<skd_in_flight>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_inc_in_flight, &count);
+ *   - drivers/block/skd_main.c|1917| <<skd_recover_requests>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_recover_request, skdev);
+ *   - drivers/nvme/host/fc.c|2763| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|2785| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|2412| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2413| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|905| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|918| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1745| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1759| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|408| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|417| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -376,6 +481,13 @@ EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|123| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|146| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|864| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|966| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -416,6 +528,14 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|547| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ *
+ * blk_mq_alloc_rq_map()
+ *  -> blk_mq_init_tags()
+ *      -> blk_mq_init_bitmap_tags()
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -436,6 +556,12 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2146| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ *
+ * 这里初始化blk_mq_tags的nr_tags(nr_reserved_tags)和sbitmap
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -464,6 +590,11 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3220| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3223| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -529,6 +660,16 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 调用的几个例子:
+ *   - drivers/nvme/host/nvme.h|129| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|693| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|859| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|3698| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5620| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5291| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|490| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..1ba657b 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -9,8 +9,30 @@
  */
 struct blk_mq_tags {
 	unsigned int nr_tags;
+	/*
+	 * 来源是set->reserved_tags, 设置set->reserved_tags的地方:
+	 *   - drivers/ide/ide-probe.c|783| <<ide_init_queue>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|2436| <<nvme_fc_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|3085| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|718| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|731| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/tcp.c|1447| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/tcp.c|1458| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/target/loop.c|340| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/target/loop.c|512| <<nvme_loop_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *
+	 * 只在以下修改:
+	 *   - block/blk-mq-tag.c|493| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用active_queues:
+	 *   - block/blk-mq-debugfs.c|465| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|66| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|91| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -36,11 +58,21 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|152| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|203| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1151| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/*
+	 * sbq_wait_ptr() - Get the next wait queue to use for a &struct
+	 * sbitmap_queue.
+	 */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -53,6 +85,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|407| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1084| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -61,6 +98,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|980| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2314| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,12 +117,22 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|219| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|302| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
 	hctx->tags->rqs[tag] = rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|218| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1081| <<blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 4883416..49a4de9 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,16 +21,30 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|701| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|661| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
+	/*
+	 * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+	 * struct virtio_config_ops virtio_pci_config_nodev_ops.get_vq_affinity = vp_get_vq_affinity()
+	 * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+	 */
 	if (!vdev->config->get_vq_affinity)
 		goto fallback;
 
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
+		/*
+		 * 对于virtblk, 参数first_vec是0
+		 * 对于virtscsi, 参数first_vec是2
+		 */
 		mask = vdev->config->get_vq_affinity(vdev, first_vec + queue);
 		if (!mask)
 			goto fallback;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index ce0f5f4..6dab8e7 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -39,6 +39,49 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 最核心的函数: blk_mq_sched_dispatch_requests()
+ */
+
+/*
+ * blktrace:
+ * Q  - trace_block_bio_queue()
+ * M  - trace_block_bio_backmerge()
+ * F  - trace_block_bio_frontmerge()
+ * G  - trace_block_getrq()
+ * S  - trace_block_sleeprq()
+ * R  - trace_block_rq_requeue()
+ * D  - trace_block_rq_issue()
+ * C  - trace_block_rq_complete()
+ * P  - trace_block_plug()
+ * U  - trace_block_unplug()
+ * UT - trace_block_unplug()
+ * I  - trace_block_rq_insert()
+ * X  - trace_block_split()
+ * B  - trace_block_bio_bounce()
+ * A  - trace_block_bio_remap() and trace_block_rq_remap()
+ *
+ *
+ * #define bio_op(bio) \
+ *         ((bio)->bi_opf & REQ_OP_MASK)
+ * #define req_op(req) \
+ *         ((req)->cmd_flags & REQ_OP_MASK)
+ *
+ * F  - if (op & REQ_PREFLUSH)
+ *
+ * W  - if (op & REQ_OP_MASK == REQ_OP_WRITE || op & REQ_OP_MASK == REQ_OP_WRITE_SAME)
+ * D  - if (op & REQ_OP_MASK == REQ_OP_DISCARD)
+ * DE - if (op & REQ_OP_MASK == REQ_OP_SECURE_ERASE)
+ * F  - if (op & REQ_OP_MASK == REQ_OP_FLUSH)
+ * R  - if (op & REQ_OP_MASK == REQ_OP_READ)
+ * N  - if (op & REQ_OP_MASK == else)
+ *
+ * F  - if (op & REQ_FUA)
+ * A  - if (op & REQ_RAHEAD)
+ * S  - if (op & REQ_SYNC)
+ * M  - if (op & REQ_META)
+ */
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
@@ -63,8 +106,48 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  * Check if any of the ctx, dispatch list or elevator
  * have pending work in this hardware queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1520| <<blk_mq_run_hw_queue>> blk_mq_hctx_has_pending(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 往hctx->dispatch添加新元素的地方:
+	 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 * 从hctx->dispatch移除元素的地方(下发):
+	 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 * 其他使用hctx->dispatch的地方:
+	 *   - block/blk-mq-debugfs.c|379| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+	 *   - block/blk-mq-debugfs.c|386| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+	 *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+	 *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+	 *   - block/blk-mq.c|72| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+	 *   - block/blk-mq.c|2474| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+	 *
+	 *
+	 * 似乎用来表示hctx的某一个ctx->rq_list是否有request
+	 *
+	 * 设置和取消hctx->ctx_map的地方:
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|110| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 * 遍历hctx->ctx_map的地方(都是从ctx->rq_list移除的操作):
+	 *   - block/blk-mq.c|1095| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1133| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off, ---> dispatch_rq_from_ctx
+	 * 测试hctx->ctx_map的地方:
+	 *   - block/blk-mq-sched.c|171| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|89| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|101| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 * 其他初始化hctx->ctx_map的地方:
+	 *   - block/blk-mq-debugfs.c|455| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sysfs.c|44| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2526| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2547| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2716| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
 			blk_mq_sched_has_work(hctx);
@@ -73,6 +156,11 @@ static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1798| <<__blk_mq_insert_request>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *   - block/blk-mq.c|1835| <<blk_mq_insert_requests>> blk_mq_hctx_mark_pending(hctx, ctx);
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
@@ -110,6 +198,10 @@ static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|75| <<part_in_flight>> return blk_mq_in_flight(q, part);
+ */
 unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	unsigned inflight[2];
@@ -142,6 +234,15 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|275| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|192| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3806| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|324| <<pmem_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3878| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	mutex_lock(&q->mq_freeze_lock);
@@ -175,6 +276,11 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);
  * Guarantee no request is in use, so we can change any data structure of
  * the queue afterward.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|307| <<blk_cleanup_queue>> blk_freeze_queue(q);
+ *   - block/blk-mq.c|197| <<blk_mq_freeze_queue>> blk_freeze_queue(q);
+ */
 void blk_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -215,6 +321,11 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  * FIXME: replace the scsi_internal_device_*block_nowait() calls in the
  * mpt3sas driver such that this function can be removed.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|341| <<blk_mq_quiesce_queue>> blk_mq_quiesce_queue_nowait(q);
+ *   - drivers/scsi/scsi_lib.c|2610| <<scsi_internal_device_block_nowait>> blk_mq_quiesce_queue_nowait(q);
+ */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
@@ -230,6 +341,22 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
  * sure no dispatch can happen until the queue is unquiesced via
  * blk_mq_unquiesce_queue().
  */
+/*
+ * 部分调用的地方:
+ *   - block/blk-mq.c|3691| <<blk_mq_update_nr_requests>> blk_mq_quiesce_queue(q);
+ *   - block/blk-sysfs.c|482| <<queue_wb_lat_store>> blk_mq_quiesce_queue(q);
+ *   - block/elevator.c|644| <<elevator_switch>> blk_mq_quiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|968| <<virtblk_freeze>> blk_mq_quiesce_queue(vblk->disk->queue);
+ *   - drivers/ide/ide-probe.c|1166| <<drive_rq_insert_work>> blk_mq_quiesce_queue(drive->queue);
+ *   - drivers/md/dm-rq.c|76| <<dm_stop_queue>> blk_mq_quiesce_queue(q);
+ *   - drivers/nvme/host/core.c|3889| <<nvme_stop_queues>> blk_mq_quiesce_queue(ns->queue);
+ *   - drivers/nvme/host/fc.c|2784| <<nvme_fc_delete_association>> blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1402| <<nvme_suspend_queue>> blk_mq_quiesce_queue(nvmeq->dev->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|902| <<nvme_rdma_teardown_admin_queue>> blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/tcp.c|1742| <<nvme_tcp_teardown_admin_queue>> blk_mq_quiesce_queue(ctrl->admin_q);
+ *   - drivers/nvme/target/loop.c|416| <<nvme_loop_shutdown_ctrl>> blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2642| <<scsi_internal_device_block>> blk_mq_quiesce_queue(q);
+ */
 void blk_mq_quiesce_queue(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -256,6 +383,28 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  * This function recovers queue into the state before quiescing
  * which is done by blk_mq_quiesce_queue.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3717| <<blk_mq_update_nr_requests>> blk_mq_unquiesce_queue(q);
+ *   - block/blk-sysfs.c|487| <<queue_wb_lat_store>> blk_mq_unquiesce_queue(q);
+ *   - block/elevator.c|648| <<elevator_switch>> blk_mq_unquiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|985| <<virtblk_restore>> blk_mq_unquiesce_queue(vblk->disk->queue);
+ *   - drivers/ide/ide-probe.c|1180| <<drive_rq_insert_work>> blk_mq_unquiesce_queue(drive->queue);
+ *   - drivers/md/dm-rq.c|67| <<dm_start_queue>> blk_mq_unquiesce_queue(q);
+ *   - drivers/nvme/host/core.c|108| <<nvme_set_queue_dying>> blk_mq_unquiesce_queue(ns->queue);
+ *   - drivers/nvme/host/core.c|3827| <<nvme_kill_queues>> blk_mq_unquiesce_queue(ctrl->admin_q);
+ *   - drivers/nvme/host/core.c|3900| <<nvme_start_queues>> blk_mq_unquiesce_queue(ns->queue);
+ *   - drivers/nvme/host/fc.c|2002| <<nvme_fc_ctrl_free>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|2624| <<nvme_fc_create_association>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|2817| <<nvme_fc_delete_association>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1613| <<nvme_dev_remove_admin>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1647| <<nvme_alloc_admin_tags>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|2423| <<nvme_dev_disable>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_teardown_admin_queue>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/tcp.c|1747| <<nvme_tcp_teardown_admin_queue>> blk_mq_unquiesce_queue(ctrl->admin_q);
+ *   - drivers/nvme/target/loop.c|419| <<nvme_loop_shutdown_ctrl>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2652| <<scsi_start_queue>> blk_mq_unquiesce_queue(q);
+ */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
 	blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
@@ -265,6 +414,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by only:
+ *   - block/blk-core.c|278| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -290,9 +443,17 @@ static inline bool blk_mq_need_time_stamp(struct request *rq)
 	return (rq->rq_flags & RQF_IO_STAT) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|469| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+	/*
+	 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
 	req_flags_t rq_flags = 0;
@@ -348,6 +509,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|495| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|546| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|2162| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -436,6 +603,10 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/core.c|437| <<nvme_alloc_request>> req = blk_mq_alloc_request_hctx(q, op, flags,
+ */
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
 {
@@ -482,6 +653,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|700| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|1141| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -499,6 +675,27 @@ static void __blk_mq_free_request(struct request *rq)
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|2056| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|549| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|731| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|484| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1002| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1048| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/ide/ide-atapi.c|218| <<ide_prep_sense>> blk_mq_free_request(sense_rq);
+ *   - drivers/ide/ide-cd.c|272| <<ide_cd_free_sense>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/ide/ide-probe.c|984| <<drive_release_dev>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/nvme/host/core.c|801| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|902| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|912| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|644| <<nvme_nvm_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|716| <<nvme_nvm_submit_io_sync>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|863| <<nvme_nvm_submit_user_cmd>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1206| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|2185| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2315| <<fnic_scsi_host_end_tag>> blk_mq_free_request(dummy);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -515,7 +712,23 @@ void blk_mq_free_request(struct request *rq)
 		}
 	}
 
+	/*
+	 * 在以下修改或者使用ctx->rq_completed:
+	 *   - block/blk-mq-debugfs.c|716| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|725| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|715| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	ctx->rq_completed[rq_is_sync(rq)]++;
+	/*
+	 * 在以下使用nr_active:
+	 *   - block/blk-mq.c|467| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1363| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|723| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|270| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|3018| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq-tag.c|138| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq-debugfs.c|629| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 */
 	if (rq->rq_flags & RQF_MQ_INFLIGHT)
 		atomic_dec(&hctx->nr_active);
 
@@ -530,6 +743,12 @@ void blk_mq_free_request(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_free_request);
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|593| <<blk_mq_end_request>> __blk_mq_end_request(rq, error);
+ *   - drivers/ide/ide-io.c|76| <<ide_end_rq>> __blk_mq_end_request(rq, error);
+ *   - drivers/scsi/scsi_lib.c|599| <<scsi_end_request>> __blk_mq_end_request(req, error);
+ */
 inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	u64 now = 0;
@@ -556,6 +775,25 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-flush.c|196| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|378| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1341| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|1956| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - block/blk-mq.c|1992| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+ *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/block/loop.c|493| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/null_blk_main.c|621| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+ *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+ *   - drivers/nvme/host/core.c|281| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *   - drivers/nvme/host/multipath.c|51| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+ *   - drivers/scsi/scsi_transport_fc.c|3584| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -669,6 +907,24 @@ int blk_mq_request_started(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
 
+/*
+ * 部分调用的例子:
+ *   - block/bsg-lib.c|271| <<bsg_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/loop.c|1893| <<loop_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/block/null_blk_main.c|1333| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|323| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/ide/ide-io.c|578| <<ide_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/md/dm-rq.c|449| <<dm_start_request>> blk_mq_start_request(orig);
+ *   - drivers/nvme/host/fabrics.c|556| <<nvmf_fail_nonready_command>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/fc.c|2255| <<nvme_fc_start_fcp_op>> blk_mq_start_request(op->rq);
+ *   - drivers/nvme/host/pci.c|925| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/nvme/host/rdma.c|1738| <<nvme_rdma_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/tcp.c|2114| <<nvme_tcp_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/target/loop.c|149| <<nvme_loop_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1574| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1650| <<scsi_queue_rq>> blk_mq_start_request(req);
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -702,6 +958,12 @@ void blk_mq_start_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_start_request);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|982| <<blk_mq_requeue_request>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1588| <<blk_mq_dispatch_rq_list>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|2321| <<__blk_mq_issue_directly>> __blk_mq_requeue_request(rq);
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -719,6 +981,18 @@ static void __blk_mq_requeue_request(struct request *rq)
 	}
 }
 
+/*
+ * 调用的部分例子:
+ *   - drivers/block/loop.c|481| <<lo_complete_rq>> blk_mq_requeue_request(rq, true);
+ *   - drivers/block/null_blk_main.c|1344| <<null_queue_rq>> blk_mq_requeue_request(bd->rq, true);
+ *   - drivers/block/xen-blkfront.c|2053| <<blkif_recover>> blk_mq_requeue_request(req, false);
+ *   - drivers/ide/ide-cd.c|261| <<ide_cd_breathe>> blk_mq_requeue_request(rq, false);
+ *   - drivers/ide/ide-io.c|450| <<ide_requeue_and_plug>> blk_mq_requeue_request(rq, false);
+ *   - drivers/md/dm-rq.c|191| <<dm_mq_delay_requeue_request>> blk_mq_requeue_request(rq, false);
+ *   - drivers/nvme/host/core.c|256| <<nvme_retry_req>> blk_mq_requeue_request(req, false);
+ *   - drivers/scsi/scsi_lib.c|151| <<scsi_mq_requeue_cmd>> blk_mq_requeue_request(cmd->request, true);
+ *   - drivers/scsi/scsi_lib.c|190| <<__scsi_queue_insert>> blk_mq_requeue_request(cmd->request, true);
+ */
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 {
 	__blk_mq_requeue_request(rq);
@@ -879,6 +1153,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1196| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -917,6 +1195,10 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|3598| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -984,6 +1266,12 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
  * Process software queues that have been marked busy, splicing them
  * to the for-dispatch
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|253| <<blk_mq_sched_dispatch_requests>> blk_mq_flush_busy_ctxs(hctx, &rq_list);
+ *
+ * 把hctx所属的ctx的ctx->rq_lists的request放入参数的list
+ */
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 {
 	struct flush_busy_ctx_data data = {
@@ -1000,6 +1288,10 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1163| <<blk_mq_dequeue_from_ctx>> dispatch_rq_from_ctx, &data);
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1020,6 +1312,12 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|193| <<blk_mq_do_dispatch_ctx>> rq = blk_mq_dequeue_from_ctx(hctx, ctx);
+ *
+ * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1043,6 +1341,14 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1256| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1282| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1362| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1394| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|2044| <<__blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
 	struct blk_mq_alloc_data data = {
@@ -1100,6 +1406,10 @@ static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
  * restart. For both cases, take care to check the condition again after
  * marking us as waiting.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1321| <<blk_mq_dispatch_rq_list>> if (!blk_mq_mark_tag_wait(hctx, rq)) {
+ */
 static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 				 struct request *rq)
 {
@@ -1173,6 +1483,14 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
  * - take 4 as factor for avoiding to get too small(0) result, and this
  *   factor doesn't matter because EWMA decreases exponentially
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1615| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1618| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|2211| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|2216| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|2220| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ */
 static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 {
 	unsigned int ewma;
@@ -1198,6 +1516,17 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|138| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|193| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|241| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|254| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 核心思想是为list的request调用queue_rq()
+ * 如果有下发不了的放入hctx->dispatch
+ * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1299,6 +1628,18 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 		 * the driver there was more coming, but that turned out to
 		 * be a lie.
 		 */
+		/*
+		 * 在以下设置commit_rqs:
+		 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+		 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+		 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+		 * 
+		 * 在以下调用commit_rqs:
+		 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+		 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+		 *
+		 * 应该都是kick了ring buffer
+		 */
 		if (q->mq_ops->commit_rqs)
 			q->mq_ops->commit_rqs(hctx);
 
@@ -1352,6 +1693,15 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1546| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1739| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1387,9 +1737,25 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
+	/*
+	 * 设置BLK_MQ_F_BLOCKING的地方:
+	 *   - block/bsg-lib.c|381| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/nbd.c|1586| <<nbd_dev_add>> BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1560| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|911| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|782| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|786| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|418| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|434| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * 核心思想优先下发hctx->dispatch
+	 * 如果有scheduler就下发ctx中scheduler中的
+	 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+	 */
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
 }
@@ -1449,6 +1815,17 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1614| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1653| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ *
+ * 根据参数async决定是同步还是异步执行下面的:
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1458,6 +1835,11 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		int cpu = get_cpu();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+			/*
+			 * 核心思想优先下发hctx->dispatch
+			 * 如果有scheduler就下发ctx中scheduler中的
+			 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+			 */
 			__blk_mq_run_hw_queue(hctx);
 			put_cpu();
 			return;
@@ -1466,16 +1848,55 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * run_work是blk_mq_run_work_fn()
+	 *
+	 * blk_mq_run_work_fn()
+	 *   -> __blk_mq_run_hw_queue()
+	 *
+	 * __blk_mq_run_hw_queue()的实现:
+	 * 核心思想优先下发hctx->dispatch
+	 * 如果有scheduler就下发ctx中scheduler中的
+	 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * 异步执行下面:
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|98| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|476| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|515| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|214| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1223| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1654| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1729| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1764| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1843| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2152| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2433| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ *
+ * 根据参数async决定是同步还是异步执行下面的:
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1495,6 +1916,13 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 	hctx_unlock(hctx, srcu_idx);
 
 	if (need_run) {
+		/*
+		 * 根据参数async决定是同步还是异步执行下面的:
+		 *
+		 * 核心思想优先下发hctx->dispatch
+		 * 如果有scheduler就下发ctx中scheduler中的
+		 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+		 */
 		__blk_mq_delay_run_hw_queue(hctx, async, 0);
 		return true;
 	}
@@ -1503,12 +1931,27 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL(blk_mq_run_hw_queue);
 
+/*
+ * 部分调用的例子:
+ *   - block/bfq-iosched.c|424| <<bfq_schedule_dispatch>> blk_mq_run_hw_queues(bfqd->queue, true);
+ *   - block/blk-mq-debugfs.c|164| <<queue_state_write>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|210| <<blk_freeze_queue_start>> blk_mq_run_hw_queues(q, false);
+ *   - block/blk-mq.c|327| <<blk_mq_unquiesce_queue>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|894| <<blk_mq_requeue_work>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/md/dm-table.c|2123| <<dm_table_run_md_queue_async>> blk_mq_run_hw_queues(queue, true);
+ *   - drivers/scsi/scsi_lib.c|350| <<scsi_kick_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|511| <<scsi_run_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|611| <<scsi_end_request>> blk_mq_run_hw_queues(q, true);
+ *   - drivers/scsi/scsi_sysfs.c|780| <<store_state_field>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_transport_fc.c|3678| <<fc_bsg_goose_queue>> blk_mq_run_hw_queues(q, true);
+ */
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/* 检测hctx->state的BLK_MQ_S_STOPPED是否设置了 */
 		if (blk_mq_hctx_stopped(hctx))
 			continue;
 
@@ -1550,6 +1993,16 @@ void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	cancel_delayed_work(&hctx->run_work);
 
+	/*
+	 * 用在hctx->state:
+	 *   - block/blk-mq.c|1553| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1578| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1599| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1623| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *
+	 * 在io completion的时候状态可能被清空
+	 */
 	set_bit(BLK_MQ_S_STOPPED, &hctx->state);
 }
 EXPORT_SYMBOL(blk_mq_stop_hw_queue);
@@ -1563,6 +2016,12 @@ EXPORT_SYMBOL(blk_mq_stop_hw_queue);
  * after blk_mq_stop_hw_queues() returns. Please use
  * blk_mq_quiesce_queue() for that requirement.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/null_blk_main.c|1124| <<null_stop_queue>> blk_mq_stop_hw_queues(q);
+ *   - drivers/block/xen-blkfront.c|1192| <<xlvbd_release_gendisk>> blk_mq_stop_hw_queues(info->rq);
+ *   - drivers/block/xen-blkfront.c|1351| <<blkif_free>> blk_mq_stop_hw_queues(info->rq);
+ */
 void blk_mq_stop_hw_queues(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1573,6 +2032,10 @@ void blk_mq_stop_hw_queues(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_mq_stop_hw_queues);
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|1590| <<blk_mq_start_hw_queues>> blk_mq_start_hw_queue(hctx);
+ */
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -1581,6 +2044,17 @@ void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL(blk_mq_start_hw_queue);
 
+/*
+ * called by:
+ *   - drivers/block/skd_main.c|712| <<skd_start_queue>> blk_mq_start_hw_queues(skdev->queue);
+ *   - drivers/block/sx8.c|692| <<carm_round_robin>> blk_mq_start_hw_queues(q);
+ *   - drivers/ide/ide-pm.c|259| <<ide_check_pm_state>> blk_mq_start_hw_queues(q);
+ *   - drivers/memstick/core/ms_block.c|2082| <<msb_start>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/ms_block.c|2212| <<msb_remove>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/mspro_block.c|822| <<mspro_block_start>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/mspro_block.c|1329| <<mspro_block_remove>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/mspro_block.c|1412| <<mspro_block_resume>> blk_mq_start_hw_queues(msb->queue);
+ */
 void blk_mq_start_hw_queues(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1591,6 +2065,10 @@ void blk_mq_start_hw_queues(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_mq_start_hw_queues);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1610| <<blk_mq_start_stopped_hw_queues>> blk_mq_start_stopped_hw_queue(hctx, async);
+ */
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	if (!blk_mq_hctx_stopped(hctx))
@@ -1601,6 +2079,15 @@ void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL_GPL(blk_mq_start_stopped_hw_queue);
 
+/*
+ * 调用的例子:
+ *   - block/blk-mq-debugfs.c|166| <<queue_state_write>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/null_blk_main.c|1132| <<null_restart_queue_async>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/virtio_blk.c|253| <<virtblk_done>> blk_mq_start_stopped_hw_queues(vblk->disk->queue, true);
+ *   - drivers/block/xen-blkfront.c|1222| <<kick_pending_request_queues_locked>> blk_mq_start_stopped_hw_queues(rinfo->dev_info->rq, true);
+ *   - drivers/block/xen-blkfront.c|2055| <<blkif_recover>> blk_mq_start_stopped_hw_queues(info->rq, true);
+ *   - drivers/ide/ide-pm.c|67| <<generic_ide_resume>> blk_mq_start_stopped_hw_queues(drive->queue, true);
+ */
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1611,6 +2098,11 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1623,9 +2115,17 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 	if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
 		return;
 
+	/*
+	 * 核心思想优先下发hctx->dispatch
+	 * 如果有scheduler就下发ctx中scheduler中的
+	 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+	 */
 	__blk_mq_run_hw_queue(hctx);
 }
 
+/*
+ * 把request放入ctx->rq_lists的头或者尾
+ */
 static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    bool at_head)
@@ -1643,6 +2143,13 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|504| <<blk_mq_sched_insert_request>> __blk_mq_insert_request(hctx, rq, at_head);
+ *
+ * 把request放入ctx->rq_lists的头或者尾
+ * 把ctx在hctx中对应的sbitmap设置上
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1650,6 +2157,7 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/* 把request放入ctx->rq_lists的头或者尾 */
 	__blk_mq_insert_req_list(hctx, rq, at_head);
 	blk_mq_hctx_mark_pending(hctx, ctx);
 }
@@ -1658,11 +2166,31 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|400| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|936| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|2196| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|2217| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|2260| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ *
+ * 把request放入hctx->dispatch, 如果参数run_queue则blk_mq_run_hw_queue(hctx, false)
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
 
 	spin_lock(&hctx->lock);
+	/*
+	 * 往hctx->dispatch添加新元素的地方:
+	 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * 从hctx->dispatch移除元素的地方(下发):
+	 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 */
 	list_add_tail(&rq->queuelist, &hctx->dispatch);
 	spin_unlock(&hctx->lock);
 
@@ -1670,6 +2198,13 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|546| <<blk_mq_sched_insert_requests>> blk_mq_insert_requests(hctx, ctx, list);
+ * 
+ * 把参数list中的request放入ctx->rq_lists
+ * 并且标记ctx在hctx上pending(blk_mq_hctx_mark_pending)
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1709,6 +2244,10 @@ static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
 	return blk_rq_pos(rqa) > blk_rq_pos(rqb);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1717| <<blk_flush_plug_list>> blk_mq_flush_plug_list(plug, from_schedule);
+ */
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct blk_mq_hw_ctx *this_hctx;
@@ -1771,6 +2310,12 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2101| <<__blk_mq_try_issue_directly>> return __blk_mq_issue_directly(hctx, rq, cookie, last);
+ *
+ * 通过queue_rq()把request下发
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1810,6 +2355,14 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2120| <<blk_mq_try_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+ *   - block/blk-mq.c|2137| <<blk_mq_request_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
+ *
+ * 核心思想是通过queue_rq()把request下发
+ * 如果没有tag还要用blk_mq_get_driver_tag()分配
+ */
 static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1842,6 +2395,9 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		goto insert;
 	}
 
+	/*
+	 * 通过queue_rq()把request下发
+	 */
 	return __blk_mq_issue_directly(hctx, rq, cookie, last);
 insert:
 	if (bypass_insert)
@@ -1851,6 +2407,16 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2312| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2319| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ *
+ * 核心思想是通过queue_rq()把request下发
+ * 如果没有tag还要用blk_mq_get_driver_tag()分配
+ * 如果失败要么把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+ * 要么blk_mq_end_request(rq, ret)
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1861,7 +2427,15 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 
 	hctx_lock(hctx, &srcu_idx);
 
+	/*
+	 * 核心思想是通过queue_rq()把request下发
+	 * 如果没有tag还要用blk_mq_get_driver_tag()分配
+	 */
 	ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+	/*
+	 * blk_mq_request_bypass_insert():
+	 * 把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+	 */
 	if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE)
 		blk_mq_request_bypass_insert(rq, true);
 	else if (ret != BLK_STS_OK)
@@ -1870,6 +2444,11 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1198| <<blk_insert_cloned_request>> return blk_mq_request_issue_directly(rq, true);
+ *   - block/blk-mq.c|2152| <<blk_mq_try_issue_list_directly>> ret = blk_mq_request_issue_directly(rq, list_empty(list));
+ */
 blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 {
 	blk_status_t ret;
@@ -1884,6 +2463,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|542| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1910,14 +2493,38 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 	 * the driver there was more coming, but that turned out to
 	 * be a lie.
 	 */
+	/*
+	 * 在以下设置commit_rqs:
+	 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+	 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+	 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+	 * 
+	 * 在以下调用commit_rqs:
+	 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+	 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+	 *
+	 * 应该都是kick了ring buffer
+	 */
 	if (!list_empty(list) && hctx->queue->mq_ops->commit_rqs)
 		hctx->queue->mq_ops->commit_rqs(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2287| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *   - block/blk-mq.c|2304| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *
+ * 核心思想是把request放入plug->mq_list
+ */
 static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 {
 	list_add_tail(&rq->queuelist, &plug->mq_list);
 	plug->rq_count++;
+	/*
+	 * 设置multiple_queues的地方:
+	 *   - block/blk-core.c|1668| <<blk_start_plug>> plug->multiple_queues = false;
+	 *   - block/blk-mq.c|2337| <<blk_add_rq_to_plug>> plug->multiple_queues = true;
+	 */
 	if (!plug->multiple_queues && !list_is_singular(&plug->mq_list)) {
 		struct request *tmp;
 
@@ -1945,10 +2552,25 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	if (!bio_integrity_prep(bio))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * blk_queue_nomerges():
+	 * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+	 *
+	 * blk_attempt_plug_merge():
+	 * try to merge with %current's plugged list
+	 * 成功返回true
+	 *
+	 * 这里的意思是, 如果request不是flush并且q->queue_flags没有设置QUEUE_FLAG_NOMERGES
+	 * 则试着放入current的plugged list
+	 */
 	if (!is_flush_fua && !blk_queue_nomerges(q) &&
 	    blk_attempt_plug_merge(q, bio, &same_queue_rq))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * 如果支持scheduler就把bio用ops.bio_merge给merge了
+	 * 否则尝试把bio给merge到ctx->rq_lists
+	 */
 	if (blk_mq_sched_bio_merge(q, bio))
 		return BLK_QC_T_NONE;
 
@@ -1979,6 +2601,20 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 		blk_mq_run_hw_queue(data.hctx, true);
 	} else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs)) {
 		/*
+		 * 为什么用q->mq_ops->commit_rqs???
+		 *
+		 * 在以下设置commit_rqs:
+		 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+		 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+		 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+		 * 
+		 * 在以下调用commit_rqs:
+		 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+		 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+		 *
+		 * 应该都是kick了ring buffer
+		 */
+		/*
 		 * Use plugging if we have a ->commit_rqs() hook as well, as
 		 * we know the driver uses bd->last in a smart fashion.
 		 */
@@ -1999,8 +2635,16 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			trace_block_plug(q);
 		}
 
+		/*
+		 * 核心思想是把request放入plug->mq_list
+		 */
 		blk_add_rq_to_plug(plug, rq);
 	} else if (plug && !blk_queue_nomerges(q)) {
+		/*
+		 * blk_queue_nomerges():
+		 * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+		 */
+
 		blk_mq_bio_to_request(rq, bio);
 
 		/*
@@ -2016,6 +2660,9 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			list_del_init(&same_queue_rq->queuelist);
 			plug->rq_count--;
 		}
+		/*
+		 * 核心思想是把request放入plug->mq_list
+		 */
 		blk_add_rq_to_plug(plug, rq);
 		trace_block_plug(q);
 
@@ -2024,6 +2671,12 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 		if (same_queue_rq) {
 			data.hctx = same_queue_rq->mq_hctx;
 			trace_block_unplug(q, 1, true);
+			/*
+			 * 核心思想是通过queue_rq()把same_queue_rq下发
+			 * 如果没有tag还要用blk_mq_get_driver_tag()分配
+			 * 如果失败要么把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+			 * 要么blk_mq_end_request(rq, ret)
+			 */
 			blk_mq_try_issue_directly(data.hctx, same_queue_rq,
 					&cookie);
 		}
@@ -2031,10 +2684,26 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			!data.hctx->dispatch_busy)) {
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 核心思想是通过queue_rq()把request下发
+		 * 如果没有tag还要用blk_mq_get_driver_tag()分配
+		 * 如果失败要么把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+		 * 要么blk_mq_end_request(rq, ret)
+		 */
 		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
 	} else {
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 1. 如果没有RQF_FLUSH_SEQ但是是flush则blk_insert_flush()
+		 * 2. 试试能否blk_mq_sched_bypass_insert(): 比如是否有RQF_FLUSH_SEQ
+		 * 3. 如果支持IO调度则用scheduler的.insert_requests()
+		 * 4. 否则把request放入request->mq_ctx的rq_lists, 然后把ctx在hctx->ctx_map对应的bit设置
+		 * 调用blk_mq_run_hw_queue()! 用async!!!
+		 *                         
+		 * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+		 * 调用blk_mq_run_hw_queue()! 用async!!!
+		 */
 		blk_mq_sched_insert_request(rq, false, true, true);
 	}
 
@@ -2081,6 +2750,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|466| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|533| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2486| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2123,6 +2798,11 @@ static size_t order_to_size(unsigned int order)
 	return (size_t)PAGE_SIZE << order;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2517| <<blk_mq_alloc_rqs>> if (blk_mq_init_request(set, rq, hctx_idx, node)) {
+ *   - block/blk-mq.c|2636| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+ */
 static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 			       unsigned int hctx_idx, int node)
 {
@@ -2138,6 +2818,12 @@ static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|551| <<blk_mq_sched_alloc_tags>> ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
+ *   - block/blk-mq-tag.c|630| <<blk_mq_tag_update_depth>> ret = blk_mq_alloc_rqs(set, new, hctx->queue_num, tdepth);
+ *   - block/blk-mq.c|2560| <<__blk_mq_alloc_rq_map>> ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
+ */
 int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx, unsigned int depth)
 {
@@ -2331,6 +3017,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3090| <<blk_mq_alloc_and_init_hctx>> hctx = blk_mq_alloc_hctx(q, set, node);
+ */
 static struct blk_mq_hw_ctx *
 blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		int node)
@@ -2571,6 +3261,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3139| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|3175| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2636,6 +3331,10 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|3424| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
 	struct blk_mq_ctxs *ctxs;
@@ -2694,6 +3393,27 @@ void blk_mq_release(struct request_queue *q)
 	blk_mq_sysfs_deinit(q);
 }
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3280| <<blk_mq_init_sq_queue>> q = blk_mq_init_queue(set);
+ *   - block/bsg-lib.c|385| <<bsg_setup_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/block/loop.c|2004| <<loop_add>> lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
+ *   - drivers/block/null_blk_main.c|1667| <<null_add_dev>> nullb->q = blk_mq_init_queue(nullb->tag_set);
+ *   - drivers/block/virtio_blk.c|806| <<virtblk_probe>> q = blk_mq_init_queue(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|986| <<xlvbd_init_blk_queue>> rq = blk_mq_init_queue(&info->tag_set);
+ *   - drivers/ide/ide-probe.c|790| <<ide_init_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/nvme/host/core.c|3253| <<nvme_alloc_ns>> ns->queue = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/fc.c|2452| <<nvme_fc_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3100| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1636| <<nvme_alloc_admin_tags>> dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|795| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|872| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/tcp.c|1644| <<nvme_tcp_configure_io_queues>> ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1696| <<nvme_tcp_configure_admin_queue>> ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|360| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|526| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1819| <<scsi_mq_alloc_queue>> sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	struct request_queue *uninit_q, *q;
@@ -2714,6 +3434,24 @@ EXPORT_SYMBOL(blk_mq_init_queue);
  * Helper for setting up a queue with mq ops, given queue depth, and
  * the passed in mq ops flags.
  */
+/*
+ * called by:
+ *   - drivers/block/amiflop.c|1783| <<fd_alloc_disk>> disk->queue = blk_mq_init_sq_queue(&unit[drive].tag_set, &amiflop_mq_ops,
+ *   - drivers/block/ataflop.c|1992| <<atari_floppy_init>> unit[i].disk->queue = blk_mq_init_sq_queue(&unit[i].tag_set,
+ *   - drivers/block/floppy.c|4530| <<do_floppy_init>> disks[drive]->queue = blk_mq_init_sq_queue(&tag_sets[drive],
+ *   - drivers/block/paride/pcd.c|314| <<pcd_init_units>> disk->queue = blk_mq_init_sq_queue(&cd->tag_set, &pcd_mq_ops,
+ *   - drivers/block/paride/pf.c|300| <<pf_init_units>> disk->queue = blk_mq_init_sq_queue(&pf->tag_set, &pf_mq_ops,
+ *   - drivers/block/ps3disk.c|444| <<ps3disk_probe>> queue = blk_mq_init_sq_queue(&priv->tag_set, &ps3disk_mq_ops, 1,
+ *   - drivers/block/sunvdc.c|794| <<init_queue>> q = blk_mq_init_sq_queue(&port->tag_set, &vdc_mq_ops, VDC_TX_RING_SIZE,
+ *   - drivers/block/swim.c|841| <<swim_floppy_init>> q = blk_mq_init_sq_queue(&swd->unit[drive].tag_set, &swim_mq_ops,
+ *   - drivers/block/swim3.c|1197| <<swim3_attach>> disk->queue = blk_mq_init_sq_queue(&fs->tag_set, &swim3_mq_ops, 2,
+ *   - drivers/block/xsysace.c|1010| <<ace_setup>> ace->queue = blk_mq_init_sq_queue(&ace->tag_set, &ace_mq_ops, 2,
+ *   - drivers/block/z2ram.c|363| <<z2_init>> z2_queue = blk_mq_init_sq_queue(&tag_set, &z2_mq_ops, 16,
+ *   - drivers/cdrom/gdrom.c|781| <<probe_gdrom>> gd.gdrom_rq = blk_mq_init_sq_queue(&gd.tag_set, &gdrom_mq_ops, 1,
+ *   - drivers/memstick/core/ms_block.c|2119| <<msb_init_disk>> msb->queue = blk_mq_init_sq_queue(&msb->tag_set, &msb_mq_ops, 2,
+ *   - drivers/memstick/core/mspro_block.c|1214| <<mspro_block_init_disk>> msb->queue = blk_mq_init_sq_queue(&msb->tag_set, &mspro_mq_ops, 2,
+ *   - drivers/mtd/mtd_blkdevs.c|433| <<add_mtd_blktrans_dev>> new->rq = blk_mq_init_sq_queue(new->tag_set, &mtd_mq_ops, 2,
+ */
 struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 					   const struct blk_mq_ops *ops,
 					   unsigned int queue_depth,
@@ -2744,6 +3482,10 @@ struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_sq_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2987| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2778,6 +3520,11 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3067| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3479| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
@@ -2852,6 +3599,11 @@ static unsigned int nr_hw_queues(struct blk_mq_tag_set *set)
 	return max(set->nr_hw_queues, nr_cpu_ids);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3020| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q);
+ *   - drivers/md/dm-rq.c|565| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q)
 {
@@ -3034,6 +3786,26 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * 调用的几个例子:
+ *   - block/blk-mq.c|3048| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+ *   - block/bsg-lib.c|382| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/block/loop.c|2000| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+ *   - drivers/block/null_blk_main.c|1562| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+ *   - drivers/block/virtio_blk.c|802| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+ *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/md/dm-rq.c|561| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+ *   - drivers/nvme/host/fc.c|2446| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3095| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1632| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+ *   - drivers/nvme/host/pci.c|2277| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+ *   - drivers/nvme/host/rdma.c|742| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/host/tcp.c|1468| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/target/loop.c|355| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|522| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1849| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -3137,6 +3909,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3315,6 +4091,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1166| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2525| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2285| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|878| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1650| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|468| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3482,6 +4268,17 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|761| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|301| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|463| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|522| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap.c|1483| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap.c|1981| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|414| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -3536,6 +4333,11 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * 只在以下使用:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
 	return rq->mq_ctx->cpu;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 633a5a7..9bf09f9 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -18,6 +18,29 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 把元素插入ctx->rq_lists的地方:
+		 *   - block/blk-mq-sched.c|359| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+		 *   - block/blk-mq.c|1814| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1816| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1863| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 *
+		 * 把元素从ctx->rq_lists移除的地方:
+		 *   - block/blk-mq.c|1103| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+		 *   - block/blk-mq.c|2421| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+		 *   - block/blk-mq.c|1143| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+		 *
+		 * 测试ctx->rq_lists是否为NULL的地方:
+		 *   - block/blk-mq-sched.c|386| <<__blk_mq_sched_bio_merge>> !list_empty_careful(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1142| <<dispatch_rq_from_ctx>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1145| <<dispatch_rq_from_ctx>> if (list_empty(&ctx->rq_lists[type]))
+		 *   - block/blk-mq.c|2420| <<blk_mq_hctx_notify_dead>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *
+		 * 其他使用ctx->rq_lists的地方:
+		 *   - block/blk-mq-debugfs.c|648| <<CTX_RQ_SEQ_OPS>> return seq_list_start(&ctx->rq_lists[type], *pos); \
+		 *   - block/blk-mq-debugfs.c|656| <<CTX_RQ_SEQ_OPS>> return seq_list_next(v, &ctx->rq_lists[type], pos); \
+		 *   - block/blk-mq.c|2601| <<blk_mq_init_cpu_queues>> INIT_LIST_HEAD(&__ctx->rq_lists[k]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
@@ -30,6 +53,12 @@ struct blk_mq_ctx {
 	unsigned long		rq_merged;
 
 	/* incremented at completion time */
+	/*
+	 * 在以下修改或者使用ctx->rq_completed:
+	 *   - block/blk-mq-debugfs.c|716| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|725| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|715| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
@@ -171,6 +200,15 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|173| <<blk_mq_get_tag>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq-tag.c|237| <<blk_mq_get_tag>> tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq.c|318| <<blk_mq_rq_ctx_init>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *
+ * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+ * 否则返回data->hctx->tags
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
@@ -179,6 +217,17 @@ static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data
 	return data->hctx->tags;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|178| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1460| <<__blk_mq_delay_run_hw_queue>> if (unlikely(blk_mq_hctx_stopped(hctx)))
+ *   - block/blk-mq.c|1517| <<blk_mq_run_hw_queues>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1538| <<blk_mq_queue_stopped>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1620| <<blk_mq_start_stopped_hw_queue>> if (!blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1861| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *
+ * 检测hctx->state的BLK_MQ_S_STOPPED是否设置了
+ */
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -222,6 +271,13 @@ static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|243| <<flush_end_io>> blk_mq_put_driver_tag(flush_rq);
+ *   - block/blk-flush.c|367| <<mq_flush_data_end_io>> blk_mq_put_driver_tag(rq);
+ *   - block/blk-mq.c|955| <<__blk_mq_requeue_request>> blk_mq_put_driver_tag(rq);
+ *   - block/blk-mq.c|1585| <<blk_mq_dispatch_rq_list>> blk_mq_put_driver_tag(nxt);
+ */
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	if (rq->tag == -1 || rq->internal_tag == -1)
@@ -230,6 +286,11 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|45| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3072| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 2ae348c..0dbd814 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -816,6 +816,22 @@ EXPORT_SYMBOL(blk_set_queue_depth);
  *
  * Tell the block layer about the write cache of @q.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/loop.c|1001| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/null_blk_main.c|1692| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - drivers/block/virtio_blk.c|732| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1911| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5331| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2001| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|323| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|154| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ *
+ * 如果wc(第二个参数)是true, 则为q->queue_flags设置QUEUE_FLAG_WC
+ * 如果fua(第三个参数)是true, 则为q->queue_flags设置QUEUE_FLAG_FUA
+ */
 void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
 {
 	if (wc)
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 940f15d..d0dc624 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -98,6 +98,11 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2911| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|829| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index 977c659..4df8b99 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -933,6 +933,11 @@ struct kobj_type blk_queue_ktype = {
  * blk_register_queue - register a block layer queue with sysfs
  * @disk: Disk of which the request queue should be registered with sysfs.
  */
+/*
+ * called by:
+ *   - block/genhd.c|738| <<__device_add_disk>> blk_register_queue(disk);
+ *   - drivers/md/dm.c|2302| <<dm_setup_md_queue>> blk_register_queue(md->disk);
+ */
 int blk_register_queue(struct gendisk *disk)
 {
 	int ret;
diff --git a/block/bounce.c b/block/bounce.c
index f8ed677..b5be4dc 100644
--- a/block/bounce.c
+++ b/block/bounce.c
@@ -357,6 +357,11 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	*bio_orig = bio;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|22| <<blk_rq_append_bio>> blk_queue_bounce(rq->q, bio);
+ *   - block/blk-mq.c|2244| <<blk_mq_make_request>> blk_queue_bounce(q, &bio);
+ */
 void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 {
 	mempool_t *pool;
diff --git a/block/genhd.c b/block/genhd.c
index 24654e1..16a7880 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -25,6 +25,11 @@
 
 #include "blk.h"
 
+/*
+ * 初始化的函数
+ * genhd_device_init()
+ */
+
 static DEFINE_MUTEX(block_class_lock);
 struct kobject *block_depr;
 
@@ -249,6 +254,22 @@ EXPORT_SYMBOL_GPL(disk_part_iter_next);
  * CONTEXT:
  * Don't care.
  */
+/*
+ * called by:
+ *   - block/genhd.c|669| <<register_disk>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|785| <<del_gendisk>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|980| <<printk_all_partitions>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|1063| <<show_partition>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|1400| <<diskstats_show>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|1582| <<set_disk_ro>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|61| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|66| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|137| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|145| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/partition-generic.c|457| <<drop_partitions>> disk_part_iter_exit(&piter);
+ *   - drivers/s390/block/dasd.c|437| <<dasd_state_ready_to_online>> disk_part_iter_exit(&piter);
+ *   - drivers/s390/block/dasd.c|464| <<dasd_state_online_to_ready>> disk_part_iter_exit(&piter);
+ */
 void disk_part_iter_exit(struct disk_part_iter *piter)
 {
 	disk_put_part(piter->part);
@@ -1082,10 +1103,20 @@ static struct kobject *base_probe(dev_t devt, int *partno, void *data)
 	return NULL;
 }
 
+/*
+ * 初始化的函数
+ */
 static int __init genhd_device_init(void)
 {
 	int error;
 
+	/*
+	 * used by:
+	 *   - block/genhd.c|1105| <<genhd_device_init>> block_class.dev_kobj = sysfs_dev_block_kobj;
+	 *   - drivers/base/core.c|2485| <<devices_init>> sysfs_dev_block_kobj = kobject_create_and_add("block", dev_kobj);
+	 *   - drivers/base/core.c|2486| <<devices_init>> if (!sysfs_dev_block_kobj)
+	 *   - drivers/base/core.c|2495| <<devices_init>> kobject_put(sysfs_dev_block_kobj);
+	 */
 	block_class.dev_kobj = sysfs_dev_block_kobj;
 	error = class_register(&block_class);
 	if (unlikely(error))
@@ -1325,6 +1356,25 @@ static void disk_release(struct device *dev)
 		blk_put_queue(disk->queue);
 	kfree(disk);
 }
+/*
+ * used by:
+ *   - block/genhd.c|941| <<printk_all_partitions>> class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|998| <<disk_seqf_start>> class_dev_iter_init(iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1089| <<genhd_device_init>> block_class.dev_kobj = sysfs_dev_block_kobj;
+ *   - block/genhd.c|1090| <<genhd_device_init>> error = class_register(&block_class); 
+ *   - block/genhd.c|1427| <<blk_lookup_devt>> class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1502| <<__alloc_disk_node>> disk_to_dev(disk)->class = &block_class;
+ *   - block/partition-generic.c|367| <<add_partition>> pdev->class = &block_class;
+ *   - drivers/base/class.c|178| <<__class_register>> if (!sysfs_deprecated || cls != &block_class)
+ *   - drivers/base/core.c|1752| <<get_device_parent>> if (sysfs_deprecated && dev->class == &block_class) {
+ *   - drivers/base/core.c|1753| <<get_device_parent>> if (parent && parent->class == &block_class)
+ *   - drivers/base/core.c|1755| <<get_device_parent>> return &block_class.p->subsys.kobj;
+ *   - drivers/base/core.c|1864| <<device_add_class_symlinks>> if (sysfs_deprecated && dev->class == &block_class)
+ *   - drivers/base/core.c|1898| <<device_remove_class_symlinks>> if (sysfs_deprecated && dev->class == &block_class)
+ *   - drivers/base/devtmpfs.c|78| <<is_blockdev>> return dev->class == &block_class;
+ *   - init/do_mounts.c|141| <<devt_from_partuuid>> dev = class_find_device(&block_class, NULL, &cmp,
+ *   - init/do_mounts.c|239| <<name_to_dev_t>> dev = class_find_device(&block_class, NULL, name + 10,
+ */
 struct class block_class = {
 	.name		= "block",
 };
diff --git a/drivers/base/core.c b/drivers/base/core.c
index fd7511e..94c418b 100644
--- a/drivers/base/core.c
+++ b/drivers/base/core.c
@@ -838,6 +838,13 @@ int (*platform_notify)(struct device *dev) = NULL;
 int (*platform_notify_remove)(struct device *dev) = NULL;
 static struct kobject *dev_kobj;
 struct kobject *sysfs_dev_char_kobj;
+/*
+ * used by:
+ *   - block/genhd.c|1105| <<genhd_device_init>> block_class.dev_kobj = sysfs_dev_block_kobj;
+ *   - drivers/base/core.c|2485| <<devices_init>> sysfs_dev_block_kobj = kobject_create_and_add("block", dev_kobj);
+ *   - drivers/base/core.c|2486| <<devices_init>> if (!sysfs_dev_block_kobj)
+ *   - drivers/base/core.c|2495| <<devices_init>> kobject_put(sysfs_dev_block_kobj);
+ */
 struct kobject *sysfs_dev_block_kobj;
 
 static DEFINE_MUTEX(device_hotplug_lock);
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index 447d635..34fd9d4 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -41,6 +41,12 @@ static inline u64 mb_per_tick(int mbps)
 enum nullb_device_flags {
 	NULLB_DEV_FL_CONFIGURED	= 0,
 	NULLB_DEV_FL_UP		= 1,
+	/*
+	 * 在以下使用:
+	 *   - drivers/block/null_blk_main.c|1141| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1386| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1675| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	NULLB_DEV_FL_THROTTLED	= 2,
 	NULLB_DEV_FL_CACHE	= 3,
 };
@@ -1132,6 +1138,11 @@ static void null_restart_queue_async(struct nullb *nullb)
 		blk_mq_start_stopped_hw_queues(q, true);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1289| <<null_queue_bio>> null_handle_cmd(cmd);
+ *   - drivers/block/null_blk_main.c|1351| <<null_queue_rq>> return null_handle_cmd(cmd);
+ */
 static blk_status_t null_handle_cmd(struct nullb_cmd *cmd)
 {
 	struct nullb_device *dev = cmd->nq->dev;
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 7ffd719..5262132 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -24,6 +24,11 @@
 static int major;
 static DEFINE_IDA(vd_index_ida);
 
+/*
+ * 在以下使用和分配:
+ *   - drivers/block/virtio_blk.c|501| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+ *   - drivers/block/virtio_blk.c|1033| <<init>> virtblk_wq = alloc_workqueue("virtio-blk", 0, 0);
+ */
 static struct workqueue_struct *virtblk_wq;
 
 struct virtio_blk_vq {
@@ -45,6 +50,19 @@ struct virtio_blk {
 	struct work_struct config_work;
 
 	/* What host tells us, plus 2 for header & tailer. */
+	/*
+	 * 在以下使用:
+	 *   - drivers/block/virtio_blk.c|358| <<virtio_queue_rq>> BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
+	 *   - drivers/block/virtio_blk.c|831| <<virtblk_init_request>> sg_init_table(vbr->sg, vblk->sg_elems);
+	 *   - drivers/block/virtio_blk.c|888| <<virtblk_probe>> u32 v, blk_size, max_size, sg_elems, opt_io_size;
+	 *   - drivers/block/virtio_blk.c|907| <<virtblk_probe>> &sg_elems);
+	 *   - drivers/block/virtio_blk.c|910| <<virtblk_probe>> if (err || !sg_elems)
+	 *   - drivers/block/virtio_blk.c|911| <<virtblk_probe>> sg_elems = 1;
+	 *   - drivers/block/virtio_blk.c|914| <<virtblk_probe>> sg_elems += 2;
+	 *   - drivers/block/virtio_blk.c|922| <<virtblk_probe>> vblk->sg_elems = sg_elems;
+	 *   - drivers/block/virtio_blk.c|952| <<virtblk_probe>> sizeof(struct scatterlist) * sg_elems;
+	 *   - drivers/block/virtio_blk.c|986| <<virtblk_probe>> blk_queue_max_segments(q, vblk->sg_elems-2);
+	 */
 	unsigned int sg_elems;
 
 	/* Ida index - used to track minor number allocations. */
@@ -66,6 +84,11 @@ struct virtblk_req {
 	struct scatterlist sg[];
 };
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/virtio_blk.c|382| <<virtblk_get_id>> err = blk_status_to_errno(virtblk_result(blk_mq_rq_to_pdu(req)));
+ */
 static inline blk_status_t virtblk_result(struct virtblk_req *vbr)
 {
 	switch (vbr->status) {
@@ -85,6 +108,10 @@ static inline blk_status_t virtblk_result(struct virtblk_req *vbr)
  * information.
  */
 #ifdef CONFIG_VIRTIO_BLK_SCSI
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|341| <<virtio_queue_rq>> err = virtblk_add_req_scsi(vblk->vqs[qid].vq, vbr, vbr->sg, num);
+ */
 static int virtblk_add_req_scsi(struct virtqueue *vq, struct virtblk_req *vbr,
 		struct scatterlist *data_sg, bool have_data)
 {
@@ -113,6 +140,11 @@ static int virtblk_add_req_scsi(struct virtqueue *vq, struct virtblk_req *vbr,
 	return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|166| <<virtblk_scsi_request_done>> static inline void virtblk_scsi_request_done(struct request *req)
+ *   - drivers/block/virtio_blk.c|257| <<virtblk_request_done>> virtblk_scsi_request_done(req);
+ */
 static inline void virtblk_scsi_request_done(struct request *req)
 {
 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
@@ -124,6 +156,9 @@ static inline void virtblk_scsi_request_done(struct request *req)
 	sreq->result = virtio32_to_cpu(vblk->vdev, vbr->in_hdr.errors);
 }
 
+/*
+ * struct block_device_operations virtblk_fops.ioctl = virtblk_ioctl()
+ */
 static int virtblk_ioctl(struct block_device *bdev, fmode_t mode,
 			     unsigned int cmd, unsigned long data)
 {
@@ -152,6 +187,10 @@ static inline void virtblk_scsi_request_done(struct request *req)
 #define virtblk_ioctl	NULL
 #endif /* CONFIG_VIRTIO_BLK_SCSI */
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|357| <<virtio_queue_rq>> err = virtblk_add_req(vblk->vqs[qid].vq, vbr, vbr->sg, num);
+ */
 static int virtblk_add_req(struct virtqueue *vq, struct virtblk_req *vbr,
 		struct scatterlist *data_sg, bool have_data)
 {
@@ -174,6 +213,10 @@ static int virtblk_add_req(struct virtqueue *vq, struct virtblk_req *vbr,
 	return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|340| <<virtio_queue_rq>> err = virtblk_setup_discard_write_zeroes(req, unmap);
+ */
 static int virtblk_setup_discard_write_zeroes(struct request *req, bool unmap)
 {
 	unsigned short segments = blk_rq_nr_discard_segments(req);
@@ -207,6 +250,28 @@ static int virtblk_setup_discard_write_zeroes(struct request *req, bool unmap)
 	return 0;
 }
 
+/*
+ * [0] virtblk_request_done
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - block/blk-mq.c|810| <<__blk_mq_complete_request_remote>> q->mq_ops->complete(rq);
+ *   - block/blk-mq.c|841| <<__blk_mq_complete_request>> q->mq_ops->complete(rq);
+ *   - block/blk-mq.c|855| <<__blk_mq_complete_request>> q->mq_ops->complete(rq);
+ *   - block/blk-mq.c|900| <<blk_mq_complete_request_sync>> rq->q->mq_ops->complete(rq);
+ *   - block/blk-softirq.c|37| <<blk_done_softirq>> rq->q->mq_ops->complete(rq);
+ *
+ * struct blk_mq_ops virtio_mq_ops.complete = virtblk_request_done()
+ */
 static inline void virtblk_request_done(struct request *req)
 {
 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
@@ -226,6 +291,20 @@ static inline void virtblk_request_done(struct request *req)
 	blk_mq_end_request(req, virtblk_result(vbr));
 }
 
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * used by:
+ *   - drivers/block/virtio_blk.c|532| <<init_vq>> callbacks[i] = virtblk_done;
+ */
 static void virtblk_done(struct virtqueue *vq)
 {
 	struct virtio_blk *vblk = vq->vdev->priv;
@@ -254,6 +333,13 @@ static void virtblk_done(struct virtqueue *vq)
 	spin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1644| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+ *   - block/blk-mq.c|2509| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+ *
+ * struct blk_mq_ops virtio_mq_ops.commit_rqs = virtio_commit_rqs()
+ */
 static void virtio_commit_rqs(struct blk_mq_hw_ctx *hctx)
 {
 	struct virtio_blk *vblk = hctx->queue->queuedata;
@@ -268,6 +354,13 @@ static void virtio_commit_rqs(struct blk_mq_hw_ctx *hctx)
 		virtqueue_notify(vq->vq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1592| <<blk_mq_dispatch_rq_list>> ret = q->mq_ops->queue_rq(hctx, &bd);
+ *   - block/blk-mq.c|2338| <<__blk_mq_issue_directly>> ret = q->mq_ops->queue_rq(hctx, &bd);
+ *
+ * struct blk_mq_ops virtio_mq_ops.queue_rq = virtio_queue_rq()
+ */
 static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 			   const struct blk_mq_queue_data *bd)
 {
@@ -282,6 +375,21 @@ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 	bool unmap = false;
 	u32 type;
 
+	/*
+	 * 修改nr_phys_segments的地方:
+	 *   - block/blk-core.c|611| <<bio_attempt_discard_merge>> req->nr_phys_segments = segments + 1;
+	 *   - block/blk-core.c|1462| <<blk_rq_bio_prep>> rq->nr_phys_segments = bio_phys_segments(q, bio);
+	 *   - block/blk-core.c|1464| <<blk_rq_bio_prep>> rq->nr_phys_segments = 1;
+	 *   - block/blk-core.c|1551| <<__blk_rq_prep_clone>> dst->nr_phys_segments = src->nr_phys_segments;
+	 *   - block/blk-merge.c|354| <<blk_recalc_rq_segments>> rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
+	 *   - block/blk-merge.c|554| <<ll_new_hw_segment>> req->nr_phys_segments += nr_phys_segs;
+	 *   - block/blk-merge.c|616| <<req_attempt_discard_merge>> req->nr_phys_segments = segments + blk_rq_nr_discard_segments(next);
+	 *   - block/blk-merge.c|638| <<ll_merge_requests_fn>> total_phys_segments = req->nr_phys_segments + next->nr_phys_segments;
+	 *   - block/blk-merge.c|646| <<ll_merge_requests_fn>> req->nr_phys_segments = total_phys_segments;
+	 *   - block/blk-mq.c|494| <<blk_mq_rq_ctx_init>> rq->nr_phys_segments = 0;
+	 *   - block/blk-mq.c|956| <<blk_mq_start_request>> rq->nr_phys_segments++;
+	 *   - block/blk-mq.c|980| <<__blk_mq_requeue_request>> rq->nr_phys_segments--;
+	 */
 	BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
 
 	switch (req_op(req)) {
@@ -324,6 +432,10 @@ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 			return BLK_STS_RESOURCE;
 	}
 
+	/*
+	 * map a request to scatterlist, return number of sg entries setup. Caller
+	 * must make sure sg can hold rq->nr_phys_segments entries
+	 */
 	num = blk_rq_map_sg(hctx->queue, req, vbr->sg);
 	if (num) {
 		if (rq_data_dir(req) == WRITE)
@@ -359,6 +471,10 @@ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 /* return id (s/n) string for *disk to *id_str
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|480| <<serial_show>> err = virtblk_get_id(disk, buf);
+ */
 static int virtblk_get_id(struct gendisk *disk, char *id_str)
 {
 	struct virtio_blk *vblk = disk->private_data;
@@ -382,6 +498,13 @@ static int virtblk_get_id(struct gendisk *disk, char *id_str)
 }
 
 /* We provide getgeo only to please some old bootloader/partitioning tools */
+/*
+ * called by:
+ *   - block/compat_ioctl.c|68| <<compat_hdio_getgeo>> ret = disk->fops->getgeo(bdev, &geo);
+ *   - block/ioctl.c|477| <<blkdev_getgeo>> ret = disk->fops->getgeo(bdev, &geo);
+ *
+ * struct block_device_operations virtblk_fops.getgeo = virtblk_getgeo()
+ */
 static int virtblk_getgeo(struct block_device *bd, struct hd_geometry *geo)
 {
 	struct virtio_blk *vblk = bd->bd_disk->private_data;
@@ -419,6 +542,10 @@ static int minor_to_index(int minor)
 	return minor >> PART_BITS;
 }
 
+/*
+ * # cat /sys/block/vda/serial 
+ * 0xabcdef
+ */
 static ssize_t serial_show(struct device *dev,
 			   struct device_attribute *attr, char *buf)
 {
@@ -442,6 +569,11 @@ static ssize_t serial_show(struct device *dev,
 static DEVICE_ATTR_RO(serial);
 
 /* The queue's logical block size must be set before calling this */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|536| <<virtblk_config_changed_work>> virtblk_update_capacity(vblk, true);
+ *   - drivers/block/virtio_blk.c|973| <<virtblk_probe>> virtblk_update_capacity(vblk, false);
+ */
 static void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)
 {
 	struct virtio_device *vdev = vblk->vdev;
@@ -467,6 +599,10 @@ static void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)
 	string_get_size(nblocks, queue_logical_block_size(q),
 			STRING_UNITS_10, cap_str_10, sizeof(cap_str_10));
 
+	/*
+	 * 启动的例子:
+	 * [    0.621051] virtio_blk virtio0: [vda] 12288000 512-byte logical blocks (6.29 GB/5.86 GiB)
+	 */
 	dev_notice(&vdev->dev,
 		   "[%s] %s%llu %d-byte logical blocks (%s/%s)\n",
 		   vblk->disk->disk_name,
@@ -479,6 +615,15 @@ static void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)
 	set_capacity(vblk->disk, capacity);
 }
 
+/*
+ * 在以下触发:
+ *   - drivers/block/virtio_blk.c|576| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+ *   - drivers/block/virtio_blk.c|1049| <<virtblk_remove>> flush_work(&vblk->config_work);
+ *   - drivers/block/virtio_blk.c|1082| <<virtblk_freeze>> flush_work(&vblk->config_work);
+ *
+ * 初始化为virtblk_config_changed_work():
+ *   - drivers/block/virtio_blk.c|880| <<virtblk_probe>> INIT_WORK(&vblk->config_work, virtblk_config_changed_work);
+ */
 static void virtblk_config_changed_work(struct work_struct *work)
 {
 	struct virtio_blk *vblk =
@@ -490,6 +635,12 @@ static void virtblk_config_changed_work(struct work_struct *work)
 	kobject_uevent_env(&disk_to_dev(vblk->disk)->kobj, KOBJ_CHANGE, envp);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|131| <<__virtio_config_changed>> drv->config_changed(dev);
+ *
+ * struct virtio_driver virtio_blk.config_changed = virtblk_config_changed()
+ */
 static void virtblk_config_changed(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -497,6 +648,11 @@ static void virtblk_config_changed(struct virtio_device *vdev)
 	queue_work(virtblk_wq, &vblk->config_work);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|804| <<virtblk_probe>> err = init_vq(vblk);
+ *   - drivers/block/virtio_blk.c|1011| <<virtblk_restore>> ret = init_vq(vdev->priv);
+ */
 static int init_vq(struct virtio_blk *vblk)
 {
 	int err;
@@ -558,6 +714,10 @@ static int init_vq(struct virtio_blk *vblk)
  * Legacy naming scheme used for virtio devices.  We are stuck with it for
  * virtio blk but don't ever use it for any new driver.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|879| <<virtblk_probe>> virtblk_name_format("vd", index, vblk->disk->disk_name, DISK_NAME_LEN);
+ */
 static int virtblk_name_format(char *prefix, int index, char *buf, int buflen)
 {
 	const int base = 'z' - 'a' + 1;
@@ -582,6 +742,11 @@ static int virtblk_name_format(char *prefix, int index, char *buf, int buflen)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|662| <<virtblk_update_cache_mode>> u8 writeback = virtblk_get_cache_mode(vdev);
+ *   - drivers/block/virtio_blk.c|697| <<cache_type_show>> u8 writeback = virtblk_get_cache_mode(vblk->vdev);
+ */
 static int virtblk_get_cache_mode(struct virtio_device *vdev)
 {
 	u8 writeback;
@@ -601,12 +766,30 @@ static int virtblk_get_cache_mode(struct virtio_device *vdev)
 	return writeback;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|688| <<cache_type_store>> virtblk_update_cache_mode(vdev);
+ *   - drivers/block/virtio_blk.c|889| <<virtblk_probe>> virtblk_update_cache_mode(vdev);
+ *
+ * 如果virtblk backend支持VIRTIO_BLK_F_CONFIG_WCE或者VIRTIO_BLK_F_FLUSH
+ * 则为q->queue_flags设置QUEUE_FLAG_WC
+ * 永远为q->queue_flags清空QUEUE_FLAG_FUA (virtblk代码就这样)
+ */
 static void virtblk_update_cache_mode(struct virtio_device *vdev)
 {
 	u8 writeback = virtblk_get_cache_mode(vdev);
 	struct virtio_blk *vblk = vdev->priv;
 
+	/*
+	 * writeback是true, 则为q->queue_flags设置QUEUE_FLAG_WC
+	 * 此外, 为q->queue_flags清空QUEUE_FLAG_FUA
+	 */
 	blk_queue_write_cache(vblk->disk->queue, writeback, false);
+	/*
+	 * This routine is a wrapper for lower-level driver's revalidate_disk
+	 * call-backs.  It is used to do common pre and post operations needed
+	 * for all revalidate_disk operations.
+	 */
 	revalidate_disk(vblk->disk);
 }
 
@@ -614,6 +797,11 @@ static const char *const virtblk_cache_types[] = {
 	"write through", "write back"
 };
 
+/*
+ * # echo "write through" > /sys/block/vda/cache_type
+ * # cat /sys/block/vda/cache_type 
+ * write through
+ */
 static ssize_t
 cache_type_store(struct device *dev, struct device_attribute *attr,
 		 const char *buf, size_t count)
@@ -633,11 +821,20 @@ cache_type_store(struct device *dev, struct device_attribute *attr,
 	return count;
 }
 
+/*
+ * # cat /sys/block/vda/cache_type 
+ * write back
+ */
 static ssize_t
 cache_type_show(struct device *dev, struct device_attribute *attr, char *buf)
 {
 	struct gendisk *disk = dev_to_disk(dev);
 	struct virtio_blk *vblk = disk->private_data;
+	/*
+	 * writeback返回0(false)或者1(true)
+	 * 0: virtblk_cache_types[0] = "write through"
+	 * 1: virtblk_cache_types[1] = "write back"
+	 */
 	u8 writeback = virtblk_get_cache_mode(vblk->vdev);
 
 	BUG_ON(writeback >= ARRAY_SIZE(virtblk_cache_types));
@@ -652,6 +849,12 @@ static struct attribute *virtblk_attrs[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - fs/sysfs/group.c|53| <<create_files>> mode = grp->is_visible(kobj, *attr, i);
+ *
+ * struct attribute_group virtblk_attr_group.is_visible = virtblk_attrs_are_visible()
+ */
 static umode_t virtblk_attrs_are_visible(struct kobject *kobj,
 		struct attribute *a, int n)
 {
@@ -672,11 +875,21 @@ static const struct attribute_group virtblk_attr_group = {
 	.is_visible = virtblk_attrs_are_visible,
 };
 
+/*
+ * used by:
+ *   - drivers/block/virtio_blk.c|976| <<virtblk_probe>> device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);
+ */
 static const struct attribute_group *virtblk_attr_groups[] = {
 	&virtblk_attr_group,
 	NULL,
 };
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2812| <<blk_mq_init_request>> ret = set->ops->init_request(set, rq, hctx_idx, node);
+ *
+ * struct blk_mq_ops virtio_mq_ops.init_request = virtblk_init_request()
+ */
 static int virtblk_init_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -690,6 +903,13 @@ static int virtblk_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by (不知道第二个是怎么回事):
+ *   - block/blk-mq.c|3776| <<blk_mq_update_queue_map>> return set->ops->map_queues(set);
+ *   - drivers/scsi/scsi_lib.c|1759| <<scsi_map_queues>> return shost->hostt->map_queues(shost);
+ *
+ * struct blk_mq_ops virtio_mq_ops.map_queues = virtblk_map_queues()
+ */
 static int virtblk_map_queues(struct blk_mq_tag_set *set)
 {
 	struct virtio_blk *vblk = set->driver_data;
@@ -699,6 +919,12 @@ static int virtblk_map_queues(struct blk_mq_tag_set *set)
 }
 
 #ifdef CONFIG_VIRTIO_BLK_SCSI
+/*
+ * called by:
+ *   - block/blk-core.c|541| <<blk_get_request>> q->mq_ops->initialize_rq_fn(req);
+ *
+ * struct blk_mq_ops virtio_mq_ops.initialize_rq_fn = virtblk_initialize_rq()
+ */
 static void virtblk_initialize_rq(struct request *req)
 {
 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
@@ -721,6 +947,24 @@ static const struct blk_mq_ops virtio_mq_ops = {
 static unsigned int virtblk_queue_depth;
 module_param_named(queue_depth, virtblk_queue_depth, uint, 0444);
 
+/*
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * struct virtio_driver virtio_blk.probe = virtblk_probe()
+ */
 static int virtblk_probe(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk;
@@ -761,8 +1005,19 @@ static int virtblk_probe(struct virtio_device *vdev)
 	}
 
 	vblk->vdev = vdev;
+	/*
+	 * 主要在下面使用sg_elems:
+	 *   - drivers/block/virtio_blk.c|358| <<virtio_queue_rq>> BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
+	 *   - drivers/block/virtio_blk.c|831| <<virtblk_init_request>> sg_init_table(vbr->sg, vblk->sg_elems);
+	 */
 	vblk->sg_elems = sg_elems;
 
+	/*
+	 * 在以下触发:
+	 *   - drivers/block/virtio_blk.c|576| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+	 *   - drivers/block/virtio_blk.c|1049| <<virtblk_remove>> flush_work(&vblk->config_work);
+	 *   - drivers/block/virtio_blk.c|1082| <<virtblk_freeze>> flush_work(&vblk->config_work);
+	 */
 	INIT_WORK(&vblk->config_work, virtblk_config_changed_work);
 
 	err = init_vq(vblk);
@@ -818,6 +1073,11 @@ static int virtblk_probe(struct virtio_device *vdev)
 	vblk->index = index;
 
 	/* configure queue flush support */
+	/*
+	 * 如果virtblk backend支持VIRTIO_BLK_F_CONFIG_WCE或者VIRTIO_BLK_F_FLUSH
+	 * 则为q->queue_flags设置QUEUE_FLAG_WC
+	 * 永远为q->queue_flags清空QUEUE_FLAG_FUA (virtblk代码就这样)
+	 */
 	virtblk_update_cache_mode(vdev);
 
 	/* If disk is read-only in the host, the guest should obey */
@@ -922,6 +1182,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	return err;
 }
 
+/*
+ * struct virtio_driver virtio_blk.remove = virtblk_remove()
+ */
 static void virtblk_remove(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -951,6 +1214,9 @@ static void virtblk_remove(struct virtio_device *vdev)
 }
 
 #ifdef CONFIG_PM_SLEEP
+/*
+ * struct virtio_driver virtio_blk.freeze = virtblk_freeze()
+ */
 static int virtblk_freeze(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -967,6 +1233,9 @@ static int virtblk_freeze(struct virtio_device *vdev)
 	return 0;
 }
 
+/*
+ * struct virtio_driver virtio_blk.restore = virtblk_restore()
+ */
 static int virtblk_restore(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -983,11 +1252,13 @@ static int virtblk_restore(struct virtio_device *vdev)
 }
 #endif
 
+/* struct virtio_driver virtio_blk.id_table = id_table */
 static const struct virtio_device_id id_table[] = {
 	{ VIRTIO_ID_BLOCK, VIRTIO_DEV_ANY_ID },
 	{ 0 },
 };
 
+/* struct virtio_driver virtio_blk.feature_table_legacy */
 static unsigned int features_legacy[] = {
 	VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,
 	VIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,
@@ -998,6 +1269,7 @@ static unsigned int features_legacy[] = {
 	VIRTIO_BLK_F_MQ, VIRTIO_BLK_F_DISCARD, VIRTIO_BLK_F_WRITE_ZEROES,
 }
 ;
+/* struct virtio_driver virtio_blk.feature_table = features */
 static unsigned int features[] = {
 	VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,
 	VIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,
@@ -1026,6 +1298,11 @@ static int __init init(void)
 {
 	int error;
 
+	/*
+	 * 在以下使用和分配:
+	 *   - drivers/block/virtio_blk.c|501| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+	 *   - drivers/block/virtio_blk.c|1033| <<init>> virtblk_wq = alloc_workqueue("virtio-blk", 0, 0);
+	 */
 	virtblk_wq = alloc_workqueue("virtio-blk", 0, 0);
 	if (!virtblk_wq)
 		return -ENOMEM;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 120fb59..e192cee 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -2459,6 +2459,10 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return 0;
 
 out_put_subsystem:
+	/*
+	 * 这里有一个bug, nvme_put_subsystem()已经调用了device_put()
+	 * 下面还调用一次
+	 */
 	nvme_put_subsystem(subsys);
 out_unlock:
 	mutex_unlock(&nvme_subsystems_lock);
diff --git a/drivers/scsi/scsi.c b/drivers/scsi/scsi.c
index 653d5ea..51a288e 100644
--- a/drivers/scsi/scsi.c
+++ b/drivers/scsi/scsi.c
@@ -183,6 +183,12 @@ void scsi_log_completion(struct scsi_cmnd *cmd, int disposition)
  *              request, waking processes that are waiting on results,
  *              etc.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|164| <<scmd_eh_abort_handler>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_error.c|2086| <<scsi_eh_flush_done_q>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_lib.c|1449| <<scsi_softirq_done>> scsi_finish_command(cmd);
+ */
 void scsi_finish_command(struct scsi_cmnd *cmd)
 {
 	struct scsi_device *sdev = cmd->device;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 65d0a10..1ab99ec 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -555,6 +555,12 @@ static void scsi_mq_uninit_cmd(struct scsi_cmnd *cmd)
 }
 
 /* Returns false when no more bytes to process, true if there are more */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|799| <<scsi_io_completion_action>> if (!scsi_end_request(req, blk_stat, blk_rq_err_bytes(req)))
+ *   - drivers/scsi/scsi_lib.c|949| <<scsi_io_completion>> if (likely(!scsi_end_request(req, blk_stat, good_bytes)))
+ *   - drivers/scsi/scsi_lib.c|955| <<scsi_io_completion>> if (scsi_end_request(req, blk_stat, blk_rq_bytes(req)))
+ */
 static bool scsi_end_request(struct request *req, blk_status_t error,
 		unsigned int bytes)
 {
@@ -915,6 +921,10 @@ static int scsi_io_completion_nz_result(struct scsi_cmnd *cmd, int result,
  *		c) We can call scsi_end_request() with blk_stat other than
  *		   BLK_STS_OK, to fail the remainder of the request.
  */
+/*
+ * called by only:
+ *   - drivers/scsi/scsi.c|233| <<scsi_finish_command>> scsi_io_completion(cmd, good_bytes);
+ */
 void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)
 {
 	int result = cmd->result;
diff --git a/drivers/virtio/virtio_pci_common.c b/drivers/virtio/virtio_pci_common.c
index f2862f6..857dd8b 100644
--- a/drivers/virtio/virtio_pci_common.c
+++ b/drivers/virtio/virtio_pci_common.c
@@ -444,6 +444,11 @@ int vp_set_vq_affinity(struct virtqueue *vq, const struct cpumask *cpu_mask)
 	return 0;
 }
 
+/*
+ * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+ * struct virtio_config_ops virtio_pci_config_nodev_ops.get_vq_affinity = vp_get_vq_affinity()
+ * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+ */
 const struct cpumask *vp_get_vq_affinity(struct virtio_device *vdev, int index)
 {
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
diff --git a/fs/direct-io.c b/fs/direct-io.c
index ac7fb19..f2d409a 100644
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@ -1170,6 +1170,10 @@ static inline int drop_refcount(struct dio *dio)
  * individual fields and will generate much worse code. This is important
  * for the whole file.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1422| <<__blockdev_direct_IO>> return do_blockdev_direct_IO(iocb, inode, bdev, iter, get_block,
+ */
 static inline ssize_t
 do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 		      struct block_device *bdev, struct iov_iter *iter,
@@ -1401,6 +1405,15 @@ do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 	return retval;
 }
 
+/*
+ * called by:
+ *   - fs/btrfs/inode.c|8616| <<btrfs_direct_IO>> ret = __blockdev_direct_IO(iocb, inode,
+ *   - fs/ext4/inode.c|3772| <<ext4_direct_IO_write>> ret = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ *   - fs/ext4/inode.c|3865| <<ext4_direct_IO_read>> ret = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/f2fs/data.c|2706| <<f2fs_direct_IO>> err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/ocfs2/aops.c|2443| <<ocfs2_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - include/linux/fs.h|3120| <<blockdev_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ */
 ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     struct block_device *bdev, struct iov_iter *iter,
 			     get_block_t get_block,
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 15d1aa5..e3a52ac 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,6 +15,24 @@ struct blk_flush_queue;
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 往hctx->dispatch添加新元素的地方:
+		 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 *
+		 * 从hctx->dispatch移除元素的地方(下发):
+		 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *
+		 * 其他使用hctx->dispatch的地方:
+		 *   - block/blk-mq-debugfs.c|379| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|386| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+		 *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|72| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq.c|2474| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
@@ -32,9 +50,43 @@ struct blk_mq_hw_ctx {
 
 	void			*driver_data;
 
+	/*
+	 * 似乎用来表示hctx的某一个ctx->rq_list是否有request
+	 *
+	 * 设置和取消hctx->ctx_map的地方:
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|110| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * 遍历hctx->ctx_map的地方(都是从ctx->rq_list移除的操作):
+	 *   - block/blk-mq.c|1095| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1133| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off, ---> dispatch_rq_from_ctx
+	 *
+	 * 测试hctx->ctx_map的地方:
+	 *   - block/blk-mq-sched.c|171| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|89| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|101| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *
+	 * 其他初始化hctx->ctx_map的地方:
+	 *   - block/blk-mq-debugfs.c|455| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sysfs.c|44| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2526| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2547| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2716| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	struct sbitmap		ctx_map;
 
 	struct blk_mq_ctx	*dispatch_from;
+	/*
+	 * 设置dispatch_busy的地方:
+	 *   - block/blk-mq.c|1445| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *
+	 * 使用dispatch_busy的地方:
+	 *   - block/blk-mq-debugfs.c|637| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|338| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|636| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1435| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|2554| <<blk_mq_make_request>> !data.hctx->dispatch_busy)) {
+	 */
 	unsigned int		dispatch_busy;
 
 	unsigned short		type;
@@ -56,6 +108,16 @@ struct blk_mq_hw_ctx {
 	unsigned int		numa_node;
 	unsigned int		queue_num;
 
+	/*
+	 * 在以下使用nr_active:
+	 *   - block/blk-mq.c|467| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1363| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|723| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|270| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|3018| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq-tag.c|138| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq-debugfs.c|629| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 */
 	atomic_t		nr_active;
 
 	struct hlist_node	cpuhp_dead;
@@ -77,8 +139,24 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_queue_map {
+	/*
+	 * mq_map[cpu]记录着当前的sw queue对应的hw queue
+	 */
 	unsigned int *mq_map;
 	unsigned int nr_queues;
+	/*
+	 * queue_offset设置的地方:
+	 *   - drivers/nvme/host/pci.c|466| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+	 *   - drivers/nvme/host/rdma.c|1801| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1804| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|1810| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1813| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2130| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2133| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2139| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2142| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 */
 	unsigned int queue_offset;
 };
 
@@ -102,6 +180,18 @@ struct blk_mq_tag_set {
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
+	/*
+	 * 设置set->reserved_tags的地方:
+	 *   - drivers/ide/ide-probe.c|783| <<ide_init_queue>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|2436| <<nvme_fc_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|3085| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|718| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|731| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/tcp.c|1447| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/tcp.c|1458| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/target/loop.c|340| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/target/loop.c|512| <<nvme_loop_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 */
 	unsigned int		reserved_tags;
 	unsigned int		cmd_size;	/* per-request extra data */
 	int			numa_node;
@@ -155,6 +245,16 @@ struct blk_mq_ops {
 	 * purpose of kicking the hardware (which the last request otherwise
 	 * would have done).
 	 */
+	/*
+	 * 在以下设置commit_rqs:
+	 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+	 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+	 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+	 * 
+	 * 在以下调用commit_rqs:
+	 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+	 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+	 */
 	commit_rqs_fn		*commit_rqs;
 
 	/*
@@ -218,14 +318,59 @@ struct blk_mq_ops {
 
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 设置和清除BLK_MQ_F_TAG_SHARED:
+	 *   - block/blk-mq.c|2637| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2683| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2413| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2639| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2665| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2682| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * 设置BLK_MQ_F_BLOCKING的地方:
+	 *   - block/bsg-lib.c|381| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/nbd.c|1586| <<nbd_dev_add>> BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1560| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|911| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|782| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|786| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|418| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|434| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
+	/*
+	 * 用在hctx->state:
+	 *   - block/blk-mq.c|1553| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1578| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1599| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1623| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *
+	 * 在io completion的时候状态可能被清空
+	 */
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.c|37| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|38| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|104| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
@@ -263,8 +408,24 @@ enum {
 	/* return when out of requests */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
+	/*
+	 * 可能会设置BLK_MQ_REQ_RESERVED的地方:
+	 *   - block/blk-mq.c|1082| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|203| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|935| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 在以下使用BLK_MQ_REQ_INTERNAL:
+	 *   - block/blk-mq-tag.c|127| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|322| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.c|395| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
@@ -343,6 +504,50 @@ static inline struct request *blk_mq_rq_from_pdu(void *pdu)
 {
 	return pdu - sizeof(struct request);
 }
+/*
+ * 几个调用的例子:
+ *   - drivers/block/loop.c|464| <<lo_complete_rq>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/loop.c|592| <<do_req_filebacked>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/loop.c|1890| <<loop_queue_rq>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/loop.c|1954| <<loop_init_request>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/null_blk_main.c|654| <<null_complete_rq>> end_cmd(blk_mq_rq_to_pdu(rq));
+ *   - drivers/block/null_blk_main.c|1332| <<null_queue_rq>> struct nullb_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);
+ *   - drivers/block/virtio_blk.c|118| <<virtblk_scsi_request_done>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/virtio_blk.c|212| <<virtblk_request_done>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/virtio_blk.c|280| <<virtio_queue_rq>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/virtio_blk.c|382| <<virtblk_get_id>> err = blk_status_to_errno(virtblk_result(blk_mq_rq_to_pdu(req)));
+ *   - drivers/block/virtio_blk.c|688| <<virtblk_init_request>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/virtio_blk.c|708| <<virtblk_initialize_rq>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/xen-blkfront.c|120| <<blkif_req>> return blk_mq_rq_to_pdu(rq);
+ *   - drivers/nvme/host/nvme.h|122| <<nvme_req>> return blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|427| <<nvme_init_request>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|527| <<nvme_pci_iod_list>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|533| <<nvme_pci_use_sgls>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|553| <<nvme_unmap_data>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|614| <<nvme_pci_setup_prps>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|728| <<nvme_pci_setup_sgls>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|788| <<nvme_setup_prp_simple>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|806| <<nvme_setup_sgl_simple>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|823| <<nvme_map_data>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|874| <<nvme_map_metadata>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|894| <<nvme_queue_rq>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|937| <<nvme_pci_complete_rq>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|1200| <<abort_endio>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|1255| <<nvme_timeout>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_error.c|286| <<scsi_times_out>> struct scsi_cmnd *scmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|567| <<scsi_end_request>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1078| <<scsi_initialize_rq>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/scsi/scsi_lib.c|1150| <<scsi_setup_scsi_cmnd>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1182| <<scsi_setup_fs_cmnd>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1198| <<scsi_setup_cmnd>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1436| <<scsi_softirq_done>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/scsi/scsi_lib.c|1563| <<scsi_mq_prep_fn>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1633| <<scsi_queue_rq>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1725| <<scsi_mq_init_request>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/scsi/scsi_lib.c|1748| <<scsi_mq_exit_request>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - include/scsi/scsi_request.h|22| <<scsi_req>> return blk_mq_rq_to_pdu(rq);
+ *   - include/scsi/scsi_tcq.h|39| <<scsi_host_find_tag>> return blk_mq_rq_to_pdu(req);
+ */
 static inline void *blk_mq_rq_to_pdu(struct request *rq)
 {
 	return rq + 1;
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 95202f8..780fccb 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -342,6 +342,11 @@ enum req_flag_bits {
 #define REQ_PREFLUSH		(1ULL << __REQ_PREFLUSH)
 #define REQ_RAHEAD		(1ULL << __REQ_RAHEAD)
 #define REQ_BACKGROUND		(1ULL << __REQ_BACKGROUND)
+/*
+ * 设置REQ_NOWAIT的地方:
+ *   - fs/direct-io.c|1267| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_NOWAIT;
+ *   - include/linux/bio.h|830| <<bio_set_polled>> bio->bi_opf |= REQ_NOWAIT;
+ */
 #define REQ_NOWAIT		(1ULL << __REQ_NOWAIT)
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 592669b..dfd3fcd 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -72,12 +72,38 @@ typedef __u32 __bitwise req_flags_t;
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
+/*
+ * 设置和取消RQF_FLUSH_SEQ的地方:
+ *   - block/blk-flush.c|130| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|309| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|401| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+ */
 #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
 /* track inflight for MQ */
 #define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
 /* don't call prep for this one */
+/*
+ * 在以下设置或者清除RQF_DONTPREP:
+ *   - drivers/block/skd_main.c|499| <<skd_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/ide/ide-io.c|464| <<ide_issue_rq>> rq->rq_flags |= RQF_DONTPREP;
+ *   - drivers/mmc/core/queue.c|297| <<mmc_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/nvme/host/core.c|424| <<nvme_clear_nvme_request>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/scsi/scsi_lib.c|1657| <<scsi_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/scsi/scsi_lib.c|146| <<scsi_mq_requeue_cmd>> cmd->request->rq_flags &= ~RQF_DONTPREP;
+ *
+ * 在以下使用RQF_DONTPREP:
+ *   - block/blk-mq.c|998| <<blk_mq_requeue_work>> if (!(rq->rq_flags & (RQF_SOFTBARRIER | RQF_DONTPREP)))
+ *   - block/blk-mq.c|1008| <<blk_mq_requeue_work>> if (rq->rq_flags & RQF_DONTPREP)
+ *   - drivers/block/skd_main.c|497| <<skd_mq_queue_rq>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/ide/ide-io.c|463| <<ide_issue_rq>> if (!blk_rq_is_passthrough(rq) && !(rq->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/mmc/core/queue.c|295| <<mmc_mq_queue_rq>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/nvme/host/core.c|421| <<nvme_clear_nvme_request>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/scsi/scsi_lib.c|145| <<scsi_mq_requeue_cmd>> if (cmd->request->rq_flags & RQF_DONTPREP) {
+ *   - drivers/scsi/scsi_lib.c|1653| <<scsi_queue_rq>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/scsi/scsi_lib.c|1705| <<scsi_queue_rq>> if (req->rq_flags & RQF_DONTPREP)
+ */
 #define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
 /* set for "ide_preempt" requests and also for requests for which the SCSI
    "quiesce" state must be ignored. */
@@ -108,9 +134,20 @@ typedef __u32 __bitwise req_flags_t;
 /* already slept for hybrid poll */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
+/*
+ * 在以下使用RQF_TIMED_OUT:
+ *   - block/blk-mq.c|1113| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+ *   - block/blk-mq.c|968| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-mq.c|1132| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+ */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
+/*
+ * used by only:
+ *   - include/linux/blkdev.h|779| <<rq_mergeable>> if (rq->rq_flags & RQF_NOMERGE_FLAGS)
+ */
 #define RQF_NOMERGE_FLAGS \
 	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
 
@@ -118,8 +155,30 @@ typedef __u32 __bitwise req_flags_t;
  * Request state for blk-mq.
  */
 enum mq_rq_state {
+	/*
+	 * 修改和使用MQ_RQ_IDLE的地方:
+	 *   - block/blk-mq.c|730| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|961| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2791| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|935| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|896| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *   - block/blk-mq-debugfs.c|328| <<global>> [MQ_RQ_IDLE] = "idle",
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 修改和使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq.c|938| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1086| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1124| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 *   - block/blk-mq-debugfs.c|329| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 修改和使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq.c|810| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|889| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|4193| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -210,6 +269,21 @@ struct request {
 	 * Number of scatter-gather DMA addr+len pairs after
 	 * physical address coalescing is performed.
 	 */
+	/*
+	 * 修改nr_phys_segments的地方:
+	 *   - block/blk-core.c|611| <<bio_attempt_discard_merge>> req->nr_phys_segments = segments + 1;
+	 *   - block/blk-core.c|1462| <<blk_rq_bio_prep>> rq->nr_phys_segments = bio_phys_segments(q, bio);
+	 *   - block/blk-core.c|1464| <<blk_rq_bio_prep>> rq->nr_phys_segments = 1;
+	 *   - block/blk-core.c|1551| <<__blk_rq_prep_clone>> dst->nr_phys_segments = src->nr_phys_segments;
+	 *   - block/blk-merge.c|354| <<blk_recalc_rq_segments>> rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
+	 *   - block/blk-merge.c|554| <<ll_new_hw_segment>> req->nr_phys_segments += nr_phys_segs;
+	 *   - block/blk-merge.c|616| <<req_attempt_discard_merge>> req->nr_phys_segments = segments + blk_rq_nr_discard_segments(next);
+	 *   - block/blk-merge.c|638| <<ll_merge_requests_fn>> total_phys_segments = req->nr_phys_segments + next->nr_phys_segments;
+	 *   - block/blk-merge.c|646| <<ll_merge_requests_fn>> req->nr_phys_segments = total_phys_segments;
+	 *   - block/blk-mq.c|494| <<blk_mq_rq_ctx_init>> rq->nr_phys_segments = 0;
+	 *   - block/blk-mq.c|956| <<blk_mq_start_request>> rq->nr_phys_segments++;
+	 *   - block/blk-mq.c|980| <<__blk_mq_requeue_request>> rq->nr_phys_segments--;
+	 */
 	unsigned short nr_phys_segments;
 
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
@@ -480,6 +554,15 @@ struct request_queue {
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
+	/*
+	 * 初始化q->timeout_work的地方:
+	 *   - block/blk-core.c|467| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 *   - block/blk-mq.c|3606| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 * 调用和取消q->timeout_work的地方:
+	 *   - block/blk-core.c|418| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work); 
+	 *   - block/blk-timeout.c|89| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 *   - block/blk-core.c|235| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 */
 	struct work_struct	timeout_work;
 
 	struct list_head	icq_list;
@@ -571,6 +654,11 @@ struct request_queue {
 	struct dentry		*rqos_debugfs_dir;
 #endif
 
+	/*
+	 * 在以下修改:
+	 *   - block/blk-mq-sysfs.c|282| <<blk_mq_unregister_dev>> q->mq_sysfs_init_done = false;
+	 *   - block/blk-mq-sysfs.c|337| <<__blk_mq_register_dev>> q->mq_sysfs_init_done = true;
+	 */
 	bool			mq_sysfs_init_done;
 
 	size_t			cmd_size;
@@ -583,6 +671,18 @@ struct request_queue {
 
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
+/*
+ * QUEUE_FLAG_NOMERGES使用的地方:
+ *   - block/blk-core.c|298| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|316| <<queue_nomerges_store>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|319| <<queue_nomerges_store>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - drivers/block/loop.c|220| <<__loop_update_dio>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|2019| <<loop_add>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1901| <<megasas_set_nvme_device_properties>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, sdev->request_queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2368| <<scsih_slave_configure>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES,
+ *   - include/linux/blkdev.h|632| <<blk_queue_nomerges>> #define blk_queue_nomerges(q) test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
@@ -600,10 +700,23 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下设置和使用:
+ *   - block/blk-stat.c|156| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|192| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|166| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-mq.c|926| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * used by:
+ *   - block/blk-mq.c|225| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|266| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ *   - include/linux/blkdev.h|639| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
@@ -618,6 +731,20 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
+/*
+ * QUEUE_FLAG_NOMERGES使用的地方:
+ *   - block/blk-core.c|298| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|316| <<queue_nomerges_store>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|319| <<queue_nomerges_store>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - drivers/block/loop.c|220| <<__loop_update_dio>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|2019| <<loop_add>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1901| <<megasas_set_nvme_device_properties>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, sdev->request_queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2368| <<scsih_slave_configure>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES,
+ *   - include/linux/blkdev.h|632| <<blk_queue_nomerges>> #define blk_queue_nomerges(q) test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
+ *
+ * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+ */
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
@@ -636,6 +763,13 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|178| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1498| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|1861| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *   - drivers/mmc/core/queue.c|469| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
@@ -719,6 +853,15 @@ static inline bool rq_is_sync(struct request *rq)
 	return op_is_sync(rq->cmd_flags);
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|5077| <<bfq_insert_request>> if (rq_mergeable(rq)) {
+ *   - block/blk-merge.c|712| <<attempt_merge>> if (!rq_mergeable(req) || !rq_mergeable(next))
+ *   - block/blk-merge.c|835| <<blk_rq_merge_ok>> if (!rq_mergeable(rq) || !bio_mergeable(bio))
+ *   - block/blk-mq-sched.c|438| <<blk_mq_sched_try_insert_merge>> return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
+ *   - block/elevator.c|229| <<elv_rqhash_find>> if (unlikely(!rq_mergeable(rq))) {
+ *   - block/mq-deadline.c|518| <<dd_insert_request>> if (rq_mergeable(rq)) {
+ */
 static inline bool rq_mergeable(struct request *rq)
 {
 	if (blk_rq_is_passthrough(rq))
@@ -1143,6 +1286,11 @@ struct blk_plug {
 	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */
 	unsigned short rq_count;
+	/*
+	 * 设置multiple_queues的地方:
+	 *   - block/blk-core.c|1668| <<blk_start_plug>> plug->multiple_queues = false;
+	 *   - block/blk-mq.c|2337| <<blk_add_rq_to_plug>> plug->multiple_queues = true;
+	 */
 	bool multiple_queues;
 };
 #define BLK_MAX_REQUEST_COUNT 16
@@ -1268,6 +1416,17 @@ static inline unsigned short queue_max_discard_segments(struct request_queue *q)
 	return q->limits.max_discard_segments;
 }
 
+/*
+ * called by:
+ *   - block/bio.c|671| <<can_add_page_to_seg>> if (bv->bv_len + len > queue_max_segment_size(q))
+ *   - block/blk-integrity.c|40| <<blk_rq_count_integrity_sg>> if (seg_size + iv.bv_len > queue_max_segment_size(q))
+ *   - block/blk-integrity.c|82| <<blk_rq_map_integrity_sg>> if (sg->length + iv.bv_len > queue_max_segment_size(q))
+ *   - block/blk-merge.c|154| <<get_max_segment_size>> return queue_max_segment_size(q);
+ *   - block/blk-merge.c|157| <<get_max_segment_size>> queue_max_segment_size(q));
+ *   - block/blk-merge.c|433| <<__blk_segment_map_sg_merge>> if ((*sg)->length + nbytes > queue_max_segment_size(q))
+ *   - block/blk-sysfs.c|135| <<queue_max_segment_size_show>> return queue_var_show(queue_max_segment_size(q), (page));
+ *   - drivers/mmc/core/queue.c|376| <<mmc_setup_queue>> dma_set_max_seg_size(mmc_dev(host), queue_max_segment_size(mq->queue));
+ */
 static inline unsigned int queue_max_segment_size(struct request_queue *q)
 {
 	return q->limits.max_segment_size;
diff --git a/include/linux/fs.h b/include/linux/fs.h
index f7fdfe9..b9f9f19 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -3112,6 +3112,18 @@ ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     dio_iodone_t end_io, dio_submit_t submit_io,
 			     int flags);
 
+/*
+ * called by:
+ *   - fs/affs/file.c|409| <<affs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, affs_get_block);
+ *   - fs/ext2/inode.c|945| <<ext2_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, ext2_get_block);
+ *   - fs/fat/inode.c|283| <<fat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, fat_get_block);
+ *   - fs/hfs/inode.c|137| <<hfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfs_get_block);
+ *   - fs/hfsplus/inode.c|134| <<hfsplus_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfsplus_get_block);
+ *   - fs/jfs/inode.c|342| <<jfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, jfs_get_block);
+ *   - fs/nilfs2/inode.c|303| <<nilfs_direct_IO>> return blockdev_direct_IO(iocb, inode, iter, nilfs_get_block);
+ *   - fs/reiserfs/inode.c|3262| <<reiserfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter,
+ *   - fs/udf/inode.c|217| <<udf_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, udf_get_block);
+ */
 static inline ssize_t blockdev_direct_IO(struct kiocb *iocb,
 					 struct inode *inode,
 					 struct iov_iter *iter,
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index 8b5330d..8a0f74e 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -139,6 +139,19 @@ struct hd_struct {
 #define GENHD_FL_CD				8
 #define GENHD_FL_UP				16
 #define GENHD_FL_SUPPRESS_PARTITION_INFO	32
+/*
+ * 部分使用的例子:
+ *   - block/genhd.c|720| <<__device_add_disk>> !(disk->flags & (GENHD_FL_EXT_DEVT | GENHD_FL_HIDDEN)));
+ *   - drivers/block/loop.c|2046| <<loop_add>> disk->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/block/null_blk_main.c|1537| <<null_gendisk_register>> disk->flags |= GENHD_FL_EXT_DEVT | GENHD_FL_SUPPRESS_PARTITION_INFO;
+ *   - drivers/block/virtio_blk.c|975| <<virtblk_probe>> vblk->disk->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/ide/ide-gd.c|417| <<ide_gd_probe>> g->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/md/md.c|5336| <<md_alloc>> disk->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/nvme/host/core.c|3251| <<nvme_alloc_ns>> int node = ctrl->numa_node, flags = GENHD_FL_EXT_DEVT, ret;
+ *   - drivers/nvme/host/multipath.c|331| <<nvme_mpath_alloc_disk>> head->disk->flags = GENHD_FL_EXT_DEVT;
+ *   - drivers/scsi/sd.c|3292| <<sd_probe_async>> gd->flags = GENHD_FL_EXT_DEVT;
+ *   - include/linux/genhd.h|237| <<disk_max_parts>> if (disk->flags & GENHD_FL_EXT_DEVT)
+ */
 #define GENHD_FL_EXT_DEVT			64 /* allow extended devt */
 #define GENHD_FL_NATIVE_CAPACITY		128
 #define GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE	256
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 54f57cd..c535670 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -607,6 +607,11 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->bitmap_tags);
+ *   - block/blk-mq-tag.c|58| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->breserved_tags);
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
-- 
2.7.4

