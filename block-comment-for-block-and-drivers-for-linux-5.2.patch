From 2b741c0fdb85c96310cfd29e71aa02220473ffd1 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 3 Sep 2019 00:26:52 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.2

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/bio.c                        | 164 +++++++
 block/blk-cgroup.c                 |   4 +
 block/blk-core.c                   | 137 ++++++
 block/blk-exec.c                   |  30 ++
 block/blk-flush.c                  |  79 +++
 block/blk-lib.c                    |  28 ++
 block/blk-map.c                    |   7 +
 block/blk-merge.c                  | 148 ++++++
 block/blk-mq-cpumap.c              |  28 ++
 block/blk-mq-debugfs-zoned.c       |   4 +
 block/blk-mq-debugfs.c             |   9 +
 block/blk-mq-pci.c                 |  22 +
 block/blk-mq-rdma.c                |   5 +
 block/blk-mq-sched.c               | 275 +++++++++++
 block/blk-mq-sched.h               |  50 ++
 block/blk-mq-sysfs.c               |  25 +
 block/blk-mq-tag.c                 | 143 ++++++
 block/blk-mq-tag.h                 |  52 ++
 block/blk-mq-virtio.c              |  14 +
 block/blk-mq.c                     | 975 +++++++++++++++++++++++++++++++++++++
 block/blk-mq.h                     |  72 +++
 block/blk-settings.c               |  33 ++
 block/blk-stat.c                   |   5 +
 block/blk-sysfs.c                  |   5 +
 block/blk.h                        |  23 +
 block/bounce.c                     |   5 +
 block/genhd.c                      | 493 +++++++++++++++++++
 block/partition-generic.c          |  25 +
 drivers/base/core.c                |  13 +
 drivers/base/map.c                 |  10 +
 drivers/block/null_blk_main.c      |  11 +
 drivers/block/virtio_blk.c         | 277 +++++++++++
 drivers/md/dm-linear.c             |  12 +
 drivers/md/dm.c                    |  27 +
 drivers/nvme/host/core.c           | 193 ++++++++
 drivers/nvme/host/fault_inject.c   |   4 +
 drivers/nvme/host/multipath.c      |  31 ++
 drivers/nvme/host/nvme.h           | 106 ++++
 drivers/nvme/host/pci.c            | 700 +++++++++++++++++++++++++-
 drivers/scsi/scsi.c                |   6 +
 drivers/scsi/scsi_lib.c            |  10 +
 drivers/virtio/virtio_pci_common.c |   5 +
 drivers/virtio/virtio_ring.c       |   8 +
 fs/block_dev.c                     | 104 ++++
 fs/direct-io.c                     |  13 +
 include/linux/bio.h                | 195 ++++++++
 include/linux/blk-cgroup.h         |  10 +
 include/linux/blk-mq.h             | 258 ++++++++++
 include/linux/blk_types.h          |  70 +++
 include/linux/blkdev.h             | 313 ++++++++++++
 include/linux/bvec.h               | 129 +++++
 include/linux/fs.h                 |  12 +
 include/linux/genhd.h              |  63 +++
 include/linux/nvme.h               |  26 +
 include/linux/percpu-refcount.h    |  20 +
 lib/percpu-refcount.c              |   6 +
 lib/sbitmap.c                      |   5 +
 57 files changed, 5496 insertions(+), 1 deletion(-)

diff --git a/block/bio.c b/block/bio.c
index ce797d7..d1fbca1 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -22,9 +22,21 @@
 #include "blk-rq-qos.h"
 
 /*
+ * 入口函数:
+ *   - bio_alloc()
+ *   - bio_kalloc()
+ */
+
+/*
  * Test patch to inline a certain number of bi_io_vec's inside the bio
  * itself, to shrink a bio data allocation from two mempool calls to one
  */
+/*
+ * used by:
+ *   - block/bio.c|484| <<bio_alloc_bioset>> inline_vecs = BIO_INLINE_VECS;
+ *   - block/bio.c|1988| <<bioset_init>> unsigned int back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);
+ *   - block/bio.c|2182| <<biovec_init_slabs>> if (bvs->nr_vecs <= BIO_INLINE_VECS) {
+ */
 #define BIO_INLINE_VECS		4
 
 /*
@@ -33,6 +45,15 @@
  * unsigned short
  */
 #define BV(x, n) { .nr_vecs = x, .name = "biovec-"#n }
+/*
+ * used by:
+ *   - block/bio.c|42| <<global>> static struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {
+ *   - block/bio.c|195| <<bvec_nr_vecs>> return bvec_slabs[--idx].nr_vecs;
+ *   - block/bio.c|209| <<bvec_free>> struct biovec_slab *bvs = bvec_slabs + idx;
+ *   - block/bio.c|260| <<bvec_alloc>> struct biovec_slab *bvs = bvec_slabs + *idx;
+ *   - block/bio.c|1988| <<biovec_init_pool>> struct biovec_slab *bp = bvec_slabs + BVEC_POOL_MAX;
+ *   - block/bio.c|2240| <<biovec_init_slabs>> struct biovec_slab *bvs = bvec_slabs + i;
+ */
 static struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {
 	BV(1, 1), BV(4, 4), BV(16, 16), BV(64, 64), BV(128, 128), BV(BIO_MAX_PAGES, max),
 };
@@ -42,6 +63,13 @@ static struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {
  * fs_bio_set is the bio_set containing bio and iovec memory pools used by
  * IO code that does not need private memory pools.
  */
+/*
+ * used by:
+ *   - block/bio.c|2228| <<init_bio>> if (bioset_init(&fs_bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS))
+ *   - block/bio.c|2231| <<init_bio>> if (bioset_integrity_create(&fs_bio_set, BIO_POOL_SIZE))
+ *   - block/blk-core.c|1617| <<blk_rq_prep_clone>> bs = &fs_bio_set;
+ *   - include/linux/bio.h|386| <<bio_alloc>> return bio_alloc_bioset(gfp_mask, nr_iovecs, &fs_bio_set);
+ */
 struct bio_set fs_bio_set;
 EXPORT_SYMBOL(fs_bio_set);
 
@@ -55,9 +83,39 @@ struct bio_slab {
 	char name[8];
 };
 static DEFINE_MUTEX(bio_slab_lock);
+/*
+ * used by:
+ *   - block/bio.c|83| <<bio_find_or_create_slab>> bslab = &bio_slabs[i];
+ *   - block/bio.c|100| <<bio_find_or_create_slab>> new_bio_slabs = krealloc(bio_slabs,
+ *   - block/bio.c|106| <<bio_find_or_create_slab>> bio_slabs = new_bio_slabs;
+ *   - block/bio.c|111| <<bio_find_or_create_slab>> bslab = &bio_slabs[entry];
+ *   - block/bio.c|135| <<bio_put_slab>> if (bs->bio_slab == bio_slabs[i].slab) {
+ *   - block/bio.c|136| <<bio_put_slab>> bslab = &bio_slabs[i];
+ *   - block/bio.c|2217| <<init_bio>> bio_slabs = kcalloc(bio_slab_max, sizeof(struct bio_slab),
+ *   - block/bio.c|2222| <<init_bio>> if (!bio_slabs)
+ */
 static struct bio_slab *bio_slabs;
+/*
+ * bio_slab_nr在以下使用:
+ *   - block/bio.c|82| <<bio_find_or_create_slab>> while (i < bio_slab_nr) {
+ *   - block/bio.c|98| <<bio_find_or_create_slab>> if (bio_slab_nr == bio_slab_max && entry == -1) {
+ *   - block/bio.c|109| <<bio_find_or_create_slab>> entry = bio_slab_nr++;
+ *   - block/bio.c|134| <<bio_put_slab>> for (i = 0; i < bio_slab_nr; i++) {
+ *   - block/bio.c|2216| <<init_bio>> bio_slab_nr = 0;
+ *
+ * bio_slab_max在以下使用:
+ *   - block/bio.c|98| <<bio_find_or_create_slab>> if (bio_slab_nr == bio_slab_max && entry == -1) {
+ *   - block/bio.c|99| <<bio_find_or_create_slab>> new_bio_slab_max = bio_slab_max << 1;
+ *   - block/bio.c|105| <<bio_find_or_create_slab>> bio_slab_max = new_bio_slab_max;
+ *   - block/bio.c|2215| <<init_bio>> bio_slab_max = 2;
+ *   - block/bio.c|2217| <<init_bio>> bio_slabs = kcalloc(bio_slab_max, sizeof(struct bio_slab),
+ */
 static unsigned int bio_slab_nr, bio_slab_max;
 
+/*
+ * called by:
+ *   - block/bio.c|1996| <<bioset_init>> bs->bio_slab = bio_find_or_create_slab(front_pad + back_pad);
+ */
 static struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)
 {
 	unsigned int sz = sizeof(struct bio) + extra_size;
@@ -114,6 +172,10 @@ static struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)
 	return slab;
 }
 
+/*
+ * called by only:
+ *   - block/bio.c|2051| <<bioset_exit>> bio_put_slab(bs);
+ */
 static void bio_put_slab(struct bio_set *bs)
 {
 	struct bio_slab *bslab = NULL;
@@ -143,11 +205,20 @@ static void bio_put_slab(struct bio_set *bs)
 	mutex_unlock(&bio_slab_lock);
 }
 
+/*
+ * called only by:
+ *   - block/bio-integrity.c|65| <<bio_integrity_alloc>> bip->bip_max_vcnt = bvec_nr_vecs(idx);
+ */
 unsigned int bvec_nr_vecs(unsigned short idx)
 {
 	return bvec_slabs[--idx].nr_vecs;
 }
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|100| <<bio_integrity_free>> bvec_free(&bs->bvec_integrity_pool, bip->bip_vec, bip->bip_slab);
+ *   - block/bio.c|308| <<bio_free>> bvec_free(&bs->bvec_pool, bio->bi_io_vec, BVEC_POOL_IDX(bio));
+ */
 void bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned int idx)
 {
 	if (!idx)
@@ -165,6 +236,12 @@ void bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned int idx)
 	}
 }
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|61| <<bio_integrity_alloc>> bip->bip_vec = bvec_alloc(gfp_mask, nr_vecs, &idx,
+ *   - block/bio.c|496| <<bio_alloc_bioset>> bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+ *   - block/bio.c|500| <<bio_alloc_bioset>> bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+ */
 struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,
 			   mempool_t *pool)
 {
@@ -235,6 +312,11 @@ void bio_uninit(struct bio *bio)
 }
 EXPORT_SYMBOL(bio_uninit);
 
+/*
+ * called by:
+ *   - block/bio.c|637| <<bio_put>> bio_free(bio);
+ *   - block/bio.c|645| <<bio_put>> bio_free(bio);
+ */
 static void bio_free(struct bio *bio)
 {
 	struct bio_set *bs = bio->bi_pool;
@@ -307,6 +389,11 @@ static struct bio *__bio_chain_endio(struct bio *bio)
 	return parent;
 }
 
+/*
+ * called by:
+ *   - block/bio.c|393| <<bio_chain>> bio->bi_end_io = bio_chain_endio;
+ *   - block/bio.c|1931| <<bio_endio>> if (bio->bi_end_io == bio_chain_endio) {
+ */
 static void bio_chain_endio(struct bio *bio)
 {
 	bio_endio(__bio_chain_endio(bio));
@@ -422,6 +509,33 @@ static void punt_bios_to_rescuer(struct bio_set *bs)
  *   RETURNS:
  *   Pointer to new bio on success, NULL on failure.
  */
+/*
+ * 部分调用的例子:
+ *   - lock/bio.c|678| <<bio_clone_fast>> b = bio_alloc_bioset(gfp_mask, 0, bs);
+ *   - block/bounce.c|246| <<bounce_clone_bio>> bio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);
+ *   - drivers/block/drbd/drbd_main.c|147| <<bio_alloc_drbd>> bio = bio_alloc_bioset(gfp_mask, 1, &drbd_md_io_bio_set);
+ *   - drivers/md/bcache/request.c|920| <<cached_dev_cache_miss>> cache_bio = bio_alloc_bioset(GFP_NOWAIT,
+ *   - drivers/md/bcache/request.c|1035| <<cached_dev_write>> flush = bio_alloc_bioset(GFP_NOIO, 0, 
+ *   - drivers/md/dm-crypt.c|1409| <<crypt_alloc_buffer>> clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, &cc->bs);
+ *   - drivers/md/dm-io.c|348| <<do_region>> bio = bio_alloc_bioset(GFP_NOIO, num_bvecs, &io->client->bios);
+ *   - drivers/md/dm-writecache.c|1476| <<__writecache_writeback_pmem>> bio = bio_alloc_bioset(GFP_NOIO, max_pages, &wc->bio_set);
+ *   - drivers/md/dm.c|573| <<alloc_io>> clone = bio_alloc_bioset(GFP_NOIO, 0, &md->io_bs);
+ *   - drivers/md/dm.c|608| <<alloc_tio>> struct bio *clone = bio_alloc_bioset(gfp_mask, 0, &ci->io->md->bs);
+ *   - drivers/md/md.c|187| <<bio_alloc_mddev>> return bio_alloc_bioset(gfp_mask, nr_iovecs, &mddev->bio_set);
+ *   - drivers/md/md.c|196| <<md_bio_alloc_sync>> return bio_alloc_bioset(GFP_NOIO, 1, &mddev->sync_set);
+ *   - drivers/md/raid5-cache.c|740| <<r5l_bio_alloc>> struct bio *bio = bio_alloc_bioset(GFP_NOIO, BIO_MAX_PAGES, &log->bs);
+ *   - drivers/md/raid5-cache.c|1639| <<r5l_recovery_allocate_ra_pool>> ctx->ra_bio = bio_alloc_bioset(GFP_KERNEL, BIO_MAX_PAGES, &log->bs);
+ *   - drivers/md/raid5-ppl.c|499| <<ppl_submit_iounit>> bio = bio_alloc_bioset(GFP_NOIO, BIO_MAX_PAGES,
+ *   - drivers/md/raid5-ppl.c|640| <<ppl_do_flush>> bio = bio_alloc_bioset(GFP_NOIO, 0, &ppl_conf->flush_bs);
+ *   - drivers/target/target_core_iblock.c|320| <<iblock_get_bio>> bio = bio_alloc_bioset(GFP_NOIO, sg_num, &ib_dev->ibd_bio_set);
+ *   - fs/block_dev.c|368| <<__blkdev_direct_IO>> bio = bio_alloc_bioset(GFP_KERNEL, nr_pages, &blkdev_dio_pool);
+ *   - fs/btrfs/extent_io.c|2807| <<btrfs_bio_alloc>> bio = bio_alloc_bioset(GFP_NOFS, BIO_MAX_PAGES, &btrfs_bioset);
+ *   - fs/btrfs/extent_io.c|2832| <<btrfs_io_bio_alloc>> bio = bio_alloc_bioset(GFP_NOFS, nr_iovecs, &btrfs_bioset);
+ *   - fs/btrfs/volumes.c|398| <<__alloc_device>> dev->flush_bio = bio_alloc_bioset(GFP_KERNEL, 0, NULL);
+ *   - fs/xfs/xfs_aops.c|699| <<xfs_alloc_ioend>> bio = bio_alloc_bioset(GFP_NOFS, BIO_MAX_PAGES, &xfs_ioend_bioset);
+ *   - include/linux/bio.h|386| <<bio_alloc>> return bio_alloc_bioset(gfp_mask, nr_iovecs, &fs_bio_set);
+ *   - include/linux/bio.h|391| <<bio_kmalloc>> return bio_alloc_bioset(gfp_mask, nr_iovecs, NULL);
+ */
 struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 			     struct bio_set *bs)
 {
@@ -558,6 +672,11 @@ void bio_put(struct bio *bio)
 }
 EXPORT_SYMBOL(bio_put);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1496| <<blk_rq_bio_prep>> rq->nr_phys_segments = bio_phys_segments(q, bio);
+ *   - block/blk-merge.c|542| <<ll_new_hw_segment>> int nr_phys_segs = bio_phys_segments(q, bio);
+ */
 int bio_phys_segments(struct request_queue *q, struct bio *bio)
 {
 	if (unlikely(!bio_flagged(bio, BIO_SEG_VALID)))
@@ -690,6 +809,11 @@ static bool can_add_page_to_seg(struct request_queue *q,
  *
  *	This should only be used by passthrough bios.
  */
+/*
+ * called by:
+ *   - block/bio.c|874| <<bio_add_pc_page>> return __bio_add_pc_page(q, bio, page, len, offset, false);
+ *   - block/bio.c|1528| <<bio_map_user_iov>> if (!__bio_add_pc_page(q, bio, page, n, offs,
+ */
 static int __bio_add_pc_page(struct request_queue *q, struct bio *bio,
 		struct page *page, unsigned int len, unsigned int offset,
 		bool put_same_page)
@@ -703,6 +827,7 @@ static int __bio_add_pc_page(struct request_queue *q, struct bio *bio,
 	if (unlikely(bio_flagged(bio, BIO_CLONED)))
 		return 0;
 
+	/* 计算bio加上新的page的size的sectors数目是否超过了最大支持的sector数目 */
 	if (((bio->bi_iter.bi_size + len) >> 9) > queue_max_hw_sectors(q))
 		return 0;
 
@@ -731,6 +856,10 @@ static int __bio_add_pc_page(struct request_queue *q, struct bio *bio,
 		}
 	}
 
+	/*
+	 * 上面尝试merge, 下面如果不能merge就要添加新的bio_vec
+	 * 但是bio_vec的数量满了就不能再添加bio_vec了
+	 */
 	if (bio_full(bio))
 		return 0;
 
@@ -1930,6 +2059,11 @@ EXPORT_SYMBOL_GPL(bio_trim);
  * create memory pools for biovec's in a bio_set.
  * use the global biovec slabs created for general use.
  */
+/*
+ * called by:
+ *   - block/bio-integrity.c|433| <<bioset_integrity_create>> if (biovec_init_pool(&bs->bvec_integrity_pool, pool_size)) {
+ *   - block/bio.c|2057| <<bioset_init>> biovec_init_pool(&bs->bvec_pool, pool_size))
+ */
 int biovec_init_pool(mempool_t *pool, int pool_entries)
 {
 	struct biovec_slab *bp = bvec_slabs + BVEC_POOL_MAX;
@@ -1980,6 +2114,21 @@ EXPORT_SYMBOL(bioset_exit);
  *    dispatch queued requests when the mempool runs out of space.
  *
  */
+/*
+ * 部分调用的例子:
+ *   - block/bio.c|2088| <<bioset_init_from_src>> return bioset_init(bs, src->bio_pool.min_nr, src->front_pad, flags);
+ *   - block/bio.c|2268| <<init_bio>> if (bioset_init(&fs_bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS))
+ *   - block/blk-core.c|455| <<blk_alloc_queue_node>> ret = bioset_init(&q->bio_split, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
+ *   - block/bounce.c|42| <<init_bounce_bioset>> ret = bioset_init(&bounce_bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
+ *   - block/bounce.c|47| <<init_bounce_bioset>> ret = bioset_init(&bounce_bio_split, BIO_POOL_SIZE, 0, 0);
+ *   - drivers/md/dm.c|3001| <<dm_alloc_md_mempools>> ret = bioset_init(&pools->io_bs, pool_size, io_front_pad, 0);
+ *   - drivers/md/dm.c|3016| <<dm_alloc_md_mempools>> ret = bioset_init(&pools->bs, pool_size, front_pad, 0);
+ *   - drivers/md/md.c|5494| <<md_run>> err = bioset_init(&mddev->bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
+ *   - drivers/md/md.c|5499| <<md_run>> err = bioset_init(&mddev->sync_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
+ *   - fs/block_dev.c|493| <<blkdev_init>> return bioset_init(&blkdev_dio_pool, 4, offsetof(struct blkdev_dio, bio), BIOSET_NEED_BVECS);
+ *   - fs/btrfs/extent_io.c|213| <<extent_io_init>> if (bioset_init(&btrfs_bioset, BIO_POOL_SIZE,
+ *   - fs/xfs/xfs_super.c|1873| <<xfs_init_zones>> if (bioset_init(&xfs_ioend_bioset, 4 * (PAGE_SIZE / SECTOR_SIZE),
+ */
 int bioset_init(struct bio_set *bs,
 		unsigned int pool_size,
 		unsigned int front_pad,
@@ -2177,6 +2326,21 @@ static void __init biovec_init_slabs(void)
 
 	for (i = 0; i < BVEC_POOL_NR; i++) {
 		int size;
+		/*
+		 * #define BV(x, n) { .nr_vecs = x, .name = "biovec-"#n }
+		 * static struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {
+		 *         BV(1, 1), BV(4, 4), BV(16, 16), BV(64, 64), BV(128, 128), BV(BIO_MAX_PAGES, max),
+		 * };     
+		 * #undef BV
+		 *
+		 * used by:
+		 *   - block/bio.c|42| <<global>> static struct biovec_slab bvec_slabs[BVEC_POOL_NR] __read_mostly = {
+		 *   - block/bio.c|195| <<bvec_nr_vecs>> return bvec_slabs[--idx].nr_vecs;
+		 *   - block/bio.c|209| <<bvec_free>> struct biovec_slab *bvs = bvec_slabs + idx;
+		 *   - block/bio.c|260| <<bvec_alloc>> struct biovec_slab *bvs = bvec_slabs + *idx;
+		 *   - block/bio.c|1988| <<biovec_init_pool>> struct biovec_slab *bp = bvec_slabs + BVEC_POOL_MAX;
+		 *   - block/bio.c|2240| <<biovec_init_slabs>> struct biovec_slab *bvs = bvec_slabs + i;
+		 */
 		struct biovec_slab *bvs = bvec_slabs + i;
 
 		if (bvs->nr_vecs <= BIO_INLINE_VECS) {
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index 1f7127b..668f511 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -164,6 +164,10 @@ static struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - include/linux/blk-cgroup.h|362| <<__blkg_lookup>> return blkg_lookup_slowpath(blkcg, q, update_hint);
+ */
 struct blkcg_gq *blkg_lookup_slowpath(struct blkcg *blkcg,
 				      struct request_queue *q, bool update_hint)
 {
diff --git a/block/blk-core.c b/block/blk-core.c
index 8340f69..f24904c3 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -66,6 +66,14 @@ struct kmem_cache *blk_requestq_cachep;
 /*
  * Controlling structure to kblockd
  */
+/*
+ * used by:
+ *   - block/blk-core.c|1627| <<kblockd_schedule_work>> return queue_work(kblockd_workqueue, work);
+ *   - block/blk-core.c|1633| <<kblockd_schedule_work_on>> return queue_work_on(cpu, kblockd_workqueue, work);
+ *   - block/blk-core.c|1640| <<kblockd_mod_delayed_work_on>> return mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);
+ *   - block/blk-core.c|1790| <<blk_dev_init>> kblockd_workqueue = alloc_workqueue("kblockd",
+ *   - block/blk-core.c|1792| <<blk_dev_init>> if (!kblockd_workqueue)
+ */
 static struct workqueue_struct *kblockd_workqueue;
 
 /**
@@ -104,6 +112,11 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_queue_flag_test_and_set);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|311| <<blk_kick_flush>> blk_rq_init(q, flush_rq);
+ *   - drivers/scsi/scsi_error.c|2326| <<scsi_ioctl_reset>> blk_rq_init(NULL, rq);
+ */
 void blk_rq_init(struct request_queue *q, struct request *rq)
 {
 	memset(rq, 0, sizeof(*rq));
@@ -167,6 +180,10 @@ int blk_status_to_errno(blk_status_t status)
 }
 EXPORT_SYMBOL_GPL(blk_status_to_errno);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1426| <<blk_update_request>> print_req_error(req, error);
+ */
 static void print_req_error(struct request *req, blk_status_t status)
 {
 	int idx = (__force int)status;
@@ -263,6 +280,15 @@ void blk_put_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_put_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|313| <<blk_cleanup_queue>> blk_set_queue_dying(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|4221| <<mtip_pci_remove>> blk_set_queue_dying(dd->queue);
+ *   - drivers/block/rbd.c|6214| <<do_rbd_remove>> blk_set_queue_dying(rbd_dev->disk->queue);
+ *   - drivers/md/dm.c|2405| <<__dm_destroy>> blk_set_queue_dying(md->queue);
+ *   - drivers/nvme/host/core.c|106| <<nvme_set_queue_dying>> blk_set_queue_dying(ns->queue);
+ *   - drivers/nvme/host/multipath.c|624| <<nvme_mpath_remove_disk>> blk_set_queue_dying(head->disk->queue);
+ */
 void blk_set_queue_dying(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_DYING, q);
@@ -351,6 +377,15 @@ EXPORT_SYMBOL(blk_alloc_queue);
  * @q: request queue pointer
  * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1057| <<generic_make_request>> if (likely(blk_queue_enter(q, flags) == 0)) {
+ *   - block/blk-core.c|1121| <<direct_make_request>> if (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {
+ *   - block/blk-mq.c|591| <<blk_mq_alloc_request>> ret = blk_queue_enter(q, flags);
+ *   - block/blk-mq.c|634| <<blk_mq_alloc_request_hctx>> ret = blk_queue_enter(q, flags);
+ *   - fs/block_dev.c|720| <<bdev_read_page>> result = blk_queue_enter(bdev->bd_queue, 0);
+ *   - fs/block_dev.c|757| <<bdev_write_page>> result = blk_queue_enter(bdev->bd_queue, 0);
+ */
 int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 {
 	const bool pm = flags & BLK_MQ_REQ_PREEMPT;
@@ -398,6 +433,21 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1083| <<generic_make_request>> blk_queue_exit(q);
+ *   - block/blk-core.c|1149| <<direct_make_request>> blk_queue_exit(q);
+ *   - block/blk-mq-tag.c|521| <<blk_mq_queue_tag_busy_iter>> blk_queue_exit(q);
+ *   - block/blk-mq.c|565| <<blk_mq_get_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|596| <<blk_mq_alloc_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|644| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|651| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|679| <<__blk_mq_free_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|1279| <<blk_mq_timeout_work>> blk_queue_exit(q);
+ *   - fs/block_dev.c|725| <<bdev_read_page>> blk_queue_exit(bdev->bd_queue);
+ *   - fs/block_dev.c|770| <<bdev_write_page>> blk_queue_exit(bdev->bd_queue);
+ *   - include/linux/blkdev.h|1060| <<bdev_write_page>> extern void blk_queue_exit(struct request_queue *q);
+ */
 void blk_queue_exit(struct request_queue *q)
 {
 	percpu_ref_put(&q->q_usage_counter);
@@ -528,6 +578,18 @@ EXPORT_SYMBOL(blk_get_queue);
  * @op: operation (REQ_OP_*) and REQ_* flags, e.g. REQ_SYNC.
  * @flags: BLK_MQ_REQ_* flags, e.g. BLK_MQ_REQ_NOWAIT.
  */
+/*
+ * 部分调用的例子:
+ *   - block/scsi_ioctl.c|310| <<sg_io>> rq = blk_get_request(q, writing ? REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, 0);
+ *   - block/scsi_ioctl.c|437| <<sg_scsi_ioctl>> rq = blk_get_request(q, in_len ? REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, 0);
+ *   - block/scsi_ioctl.c|525| <<__blk_send_generic>> rq = blk_get_request(q, REQ_OP_SCSI_OUT, 0);
+ *   - drivers/block/virtio_blk.c|463| <<virtblk_get_id>> req = blk_get_request(q, REQ_OP_DRV_IN, 0);
+ *   - drivers/scsi/osst.c|373| <<osst_execute>> req = blk_get_request(SRpnt->stp->device->request_queue,
+ *   - drivers/scsi/scsi_error.c|1952| <<scsi_eh_lock_door>> req = blk_get_request(sdev->request_queue, REQ_OP_SCSI_IN, 0);
+ *   - drivers/scsi/scsi_lib.c|246| <<__scsi_execute>> req = blk_get_request(sdev->request_queue,
+ *   - drivers/scsi/sg.c|1736| <<sg_start_req>> rq = blk_get_request(q, hp->dxfer_direction == SG_DXFER_TO_DEV ?
+ *   - drivers/scsi/st.c|549| <<st_scsi_execute>> req = blk_get_request(SRpnt->stp->device->request_queue,
+ */
 struct request *blk_get_request(struct request_queue *q, unsigned int op,
 				blk_mq_req_flags_t flags)
 {
@@ -571,6 +633,12 @@ bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|736| <<blk_attempt_plug_merge>> merged = bio_attempt_front_merge(q, rq, bio);
+ *   - block/blk-mq-sched.c|404| <<blk_mq_sched_try_merge>> if (!bio_attempt_front_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|449| <<blk_mq_bio_list_merge>> merged = bio_attempt_front_merge(q, rq, bio);
+ */
 bool bio_attempt_front_merge(struct request_queue *q, struct request *req,
 			     struct bio *bio)
 {
@@ -687,6 +755,11 @@ bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2344| <<blk_mq_bio_to_request>> blk_init_request_from_bio(rq, bio);
+ *   - drivers/nvme/host/lightnvm.c|663| <<nvme_nvm_alloc_request>> blk_init_request_from_bio(rq, rqd->bio);
+ */
 void blk_init_request_from_bio(struct request *req, struct bio *bio)
 {
 	if (bio->bi_opf & REQ_RAHEAD)
@@ -795,12 +868,20 @@ static inline int bio_check_eod(struct bio *bio, sector_t maxsector)
 /*
  * Remap block n of partition p to block n+start(p) of the disk.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|880| <<generic_make_request_checks>> if (unlikely(blk_partition_remap(bio)))
+ */
 static inline int blk_partition_remap(struct bio *bio)
 {
 	struct hd_struct *p;
 	int ret = -EIO;
 
 	rcu_read_lock();
+	/*
+	 * 返回gendisk下某个partition的hd_struct
+	 * gendisk->part_tbl->part[partno]
+	 */
 	p = __disk_get_part(bio->bi_disk, bio->bi_partno);
 	if (unlikely(!p))
 		goto out;
@@ -827,6 +908,11 @@ static inline int blk_partition_remap(struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|995| <<generic_make_request>> if (!generic_make_request_checks(bio))
+ *   - block/blk-core.c|1091| <<direct_make_request>> if (!generic_make_request_checks(bio))
+ */
 static noinline_for_stack bool
 generic_make_request_checks(struct bio *bio)
 {
@@ -1062,6 +1148,11 @@ EXPORT_SYMBOL(generic_make_request);
  * its make_request function.  (Calling direct_make_request again from
  * a workqueue is perfectly fine as that doesn't recurse).
  */
+/*
+ * called by:
+ *   - drivers/md/dm.c|1294| <<__map_bio>> ret = direct_make_request(clone);
+ *   - drivers/nvme/host/multipath.c|251| <<nvme_ns_head_make_request>> ret = direct_make_request(bio);
+ */
 blk_qc_t direct_make_request(struct bio *bio)
 {
 	struct request_queue *q = bio->bi_disk->queue;
@@ -1278,6 +1369,14 @@ void blk_account_io_done(struct request *req, u64 now)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|570| <<bio_attempt_back_merge>> blk_account_io_start(req, false);
+ *   - block/blk-core.c|593| <<bio_attempt_front_merge>> blk_account_io_start(req, false);
+ *   - block/blk-core.c|613| <<bio_attempt_discard_merge>> blk_account_io_start(req, false);
+ *   - block/blk-core.c|1191| <<blk_insert_cloned_request>> blk_account_io_start(rq, true);
+ *   - block/blk-mq.c|2038| <<blk_mq_bio_to_request>> blk_account_io_start(rq, true);
+ */
 void blk_account_io_start(struct request *rq, bool new_io)
 {
 	struct hd_struct *part;
@@ -1361,6 +1460,14 @@ EXPORT_SYMBOL_GPL(blk_steal_bios);
  *     %false - this request doesn't have any more data
  *     %true  - this request has more data
  **/
+/*
+ * called by:
+ *   - block/blk-mq.c|671| <<blk_mq_end_request>> if (blk_update_request(rq, error, blk_rq_bytes(rq)))
+ *   - drivers/block/loop.c|479| <<lo_complete_rq>> blk_update_request(rq, BLK_STS_OK, cmd->ret);
+ *   - drivers/ide/ide-io.c|70| <<ide_end_rq>> if (!blk_update_request(rq, error, nr_bytes)) {
+ *   - drivers/md/dm-rq.c|123| <<end_clone_bio>> blk_update_request(tio->orig, BLK_STS_OK, tio->completed);
+ *   - drivers/scsi/scsi_lib.c|571| <<scsi_end_request>> if (blk_update_request(req, error, bytes))
+ */
 bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
@@ -1439,6 +1546,11 @@ bool blk_update_request(struct request *req, blk_status_t error,
 }
 EXPORT_SYMBOL_GPL(blk_update_request);
 
+/*
+ * called by:
+ *   - block/blk-core.c|760| <<blk_init_request_from_bio>> blk_rq_bio_prep(req->q, req, bio);
+ *   - block/blk-map.c|25| <<blk_rq_append_bio>> blk_rq_bio_prep(rq->q, rq, *bio);
+ */
 void blk_rq_bio_prep(struct request_queue *q, struct request *rq,
 		     struct bio *bio)
 {
@@ -1659,6 +1771,10 @@ void blk_start_plug(struct blk_plug *plug)
 }
 EXPORT_SYMBOL(blk_start_plug);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1730| <<blk_flush_plug_list>> flush_plug_callbacks(plug, from_schedule);
+ */
 static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
 {
 	LIST_HEAD(callbacks);
@@ -1676,6 +1792,15 @@ static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_req.c|1302| <<drbd_check_plugged>> struct blk_plug_cb *cb = blk_check_plugged(drbd_unplug, resource, sizeof(*plug));
+ *   - drivers/block/umem.c|519| <<mm_check_plugged>> return !!blk_check_plugged(mm_unplug, card, sizeof(struct blk_plug_cb));
+ *   - drivers/md/raid1.c|1503| <<raid1_write_request>> cb = blk_check_plugged(raid1_unplug, mddev, sizeof(*plug));
+ *   - drivers/md/raid10.c|1286| <<raid10_write_one_disk>> cb = blk_check_plugged(raid10_unplug, mddev, sizeof(*plug));
+ *   - drivers/md/raid5.c|5458| <<release_stripe_plug>> struct blk_plug_cb *blk_cb = blk_check_plugged(
+ *   - fs/btrfs/raid56.c|1773| <<raid56_parity_write>> cb = blk_check_plugged(btrfs_raid_unplug, fs_info, sizeof(*plug));
+ */
 struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 				      int size)
 {
@@ -1701,6 +1826,14 @@ struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 }
 EXPORT_SYMBOL(blk_check_plugged);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1734| <<blk_finish_plug>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|2361| <<blk_mq_make_request>> blk_flush_plug_list(plug, false);
+ *   - block/blk-mq.c|3947| <<blk_poll>> blk_flush_plug_list(current->plug, false);
+ *   - include/linux/blkdev.h|1232| <<blk_flush_plug>> blk_flush_plug_list(plug, false);
+ *   - include/linux/blkdev.h|1240| <<blk_schedule_flush_plug>> blk_flush_plug_list(plug, true);
+ */
 void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	flush_plug_callbacks(plug, from_schedule);
@@ -1729,6 +1862,10 @@ void blk_finish_plug(struct blk_plug *plug)
 }
 EXPORT_SYMBOL(blk_finish_plug);
 
+/*
+ * called by:
+ *   - block/genhd.c|1110| <<genhd_device_init>> blk_dev_init();
+ */
 int __init blk_dev_init(void)
 {
 	BUILD_BUG_ON(REQ_OP_LAST >= (1 << REQ_OP_BITS));
diff --git a/block/blk-exec.c b/block/blk-exec.c
index 1db44ca..4788ad9 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -17,6 +17,10 @@
  * @rq: request to complete
  * @error: end I/O status of the request
  */
+/*
+ * called by:
+ *   - block/blk-exec.c|84| <<blk_execute_rq>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+ */
 static void blk_end_sync_rq(struct request *rq, blk_status_t error)
 {
 	struct completion *waiting = rq->end_io_data;
@@ -45,6 +49,19 @@ static void blk_end_sync_rq(struct request *rq, blk_status_t error)
  * Note:
  *    This function will invoke @done directly if the queue is dead.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-exec.c|84| <<blk_execute_rq>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, blk_end_sync_rq);
+ *   - drivers/nvme/host/core.c|758| <<nvme_execute_rq_polled>> blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+ *   - drivers/nvme/host/core.c|943| <<nvme_keep_alive>> blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
+ *   - drivers/nvme/host/lightnvm.c|688| <<nvme_nvm_submit_io>> blk_execute_rq_nowait(q, NULL, rq, 0, nvme_nvm_end_io);
+ *   - drivers/nvme/host/pci.c|1352| <<nvme_timeout>> blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+ *   - drivers/nvme/host/pci.c|2217| <<nvme_delete_queue>> blk_execute_rq_nowait(q, NULL, req, false,
+ *   - drivers/scsi/osst.c|422| <<osst_execute>> blk_execute_rq_nowait(req->q, NULL, req, 1, osst_end_async);
+ *   - drivers/scsi/scsi_error.c|1969| <<scsi_eh_lock_door>> blk_execute_rq_nowait(req->q, NULL, req, 1, eh_lock_door_done);
+ *   - drivers/scsi/sg.c|838| <<sg_common_write>> blk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,
+ *   - drivers/scsi/st.c|587| <<st_scsi_execute>> blk_execute_rq_nowait(req->q, NULL, req, 1, st_scsi_execute_end);
+ */
 void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 			   struct request *rq, int at_head,
 			   rq_end_io_fn *done)
@@ -74,6 +91,19 @@ EXPORT_SYMBOL_GPL(blk_execute_rq_nowait);
  *    Insert a fully prepared request at the back of the I/O scheduler queue
  *    for execution and wait for completion.
  */
+/*
+ * 调用的部分例子:
+ *   - block/bsg.c|184| <<bsg_sg_io>> blk_execute_rq(q, NULL, rq, !(hdr.flags & BSG_FLAG_Q_AT_TAIL));
+ *   - block/scsi_ioctl.c|357| <<sg_io>> blk_execute_rq(q, bd_disk, rq, at_head); 
+ *   - block/scsi_ioctl.c|493| <<sg_scsi_ioctl>> blk_execute_rq(q, disk, rq, 0);
+ *   - block/scsi_ioctl.c|532| <<__blk_send_generic>> blk_execute_rq(q, bd_disk, rq, 0);
+ *   - drivers/block/virtio_blk.c|381| <<virtblk_get_id>> blk_execute_rq(vblk->disk->queue, vblk->disk, req, false);
+ *   - drivers/nvme/host/core.c|793| <<__nvme_submit_sync_cmd>> blk_execute_rq(req->q, NULL, req, at_head);
+ *   - drivers/nvme/host/core.c|886| <<nvme_submit_user_cmd>> blk_execute_rq(req->q, disk, req, 0);
+ *   - drivers/nvme/host/lightnvm.c|709| <<nvme_nvm_submit_io_sync>> blk_execute_rq(q, NULL, rq, 0);
+ *   - drivers/nvme/host/lightnvm.c|838| <<nvme_nvm_submit_user_cmd>> blk_execute_rq(q, NULL, rq, 0);
+ *   - drivers/scsi/scsi_lib.c|267| <<__scsi_execute>> blk_execute_rq(req->q, NULL, req, 1);
+ */
 void blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
 		   struct request *rq, int at_head)
 {
diff --git a/block/blk-flush.c b/block/blk-flush.c
index aedd932..e28e094 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -75,6 +75,27 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
@@ -254,6 +275,10 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
  * spin_lock_irq(fq->mq_flush_lock)
  *
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|203| <<blk_flush_complete_seq>> blk_kick_flush(q, fq, cmd_flags);
+ */
 static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 			   unsigned int flags)
 {
@@ -313,6 +338,22 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|729| <<__blk_mq_end_request>> rq->end_io(rq, error); 
+ *   - drivers/lightnvm/core.c|785| <<nvm_end_io>> rqd->end_io(rqd);
+ *   - drivers/md/dm-bufio.c|526| <<dmio_complete>> b->end_io(b, unlikely(error != 0) ? BLK_STS_IOERR : 0);
+ *   - drivers/md/dm-bufio.c|556| <<use_dmio>> b->end_io(b, errno_to_blk_status(r));
+ *   - drivers/md/dm-bufio.c|564| <<bio_complete>> b->end_io(b, status);
+ *   - drivers/md/dm-mpath.c|559| <<multipath_release_clone>> pgpath->pg->ps.type->end_io(&pgpath->pg->ps,
+ *   - drivers/md/dm-mpath.c|1596| <<multipath_end_io>> ps->type->end_io(ps, &pgpath->path, mpio->nr_bytes);
+ *   - drivers/md/dm-mpath.c|1640| <<multipath_end_io_bio>> ps->type->end_io(ps, &pgpath->path, mpio->nr_bytes);
+ *   - fs/direct-io.c|292| <<dio_complete>> err = dio->end_io(dio->iocb, offset, ret, dio->private);
+ *   - fs/iomap.c|1507| <<iomap_dio_complete>> ret = dio->end_io(iocb,
+ *
+ * used by:
+ *   - block/blk-flush.c|434| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -346,6 +387,11 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|488| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2281| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -419,6 +465,31 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|2519| <<bitmap_flush_work>> blkdev_issue_flush(ic->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|647| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|681| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|745| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1040| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|182| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|687| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|157| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1427| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5142| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/fat/file.c|207| <<fat_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|242| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|417| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|770| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|874| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1024| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|197| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|168| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|671| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -461,6 +532,10 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2703| <<blk_mq_alloc_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -490,6 +565,10 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq-sysfs.c|43| <<blk_mq_hw_sysfs_release>> blk_free_flush_queue(hctx->fq);
+ */
 void blk_free_flush_queue(struct blk_flush_queue *fq)
 {
 	/* bio based request queue hasn't flush queue */
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 5f2c429..44c389d 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -22,6 +22,16 @@ struct bio *blk_next_bio(struct bio *bio, unsigned int nr_pages, gfp_t gfp)
 	return new;
 }
 
+/*
+ * called by:
+ *   - block/blk-lib.c|105| <<blkdev_issue_discard>> ret = __blkdev_issue_discard(bdev, sector, nr_sects, gfp_mask, flags,
+ *   - drivers/md/dm-thin.c|401| <<issue_discard>> return __blkdev_issue_discard(tc->pool_dev->bdev, s, len,
+ *   - drivers/md/raid0.c|532| <<raid0_handle_discard>> if (__blkdev_issue_discard(rdev->bdev,
+ *   - drivers/nvme/target/io-cmd-bdev.c|193| <<nvmet_bdev_discard_range>> ret = __blkdev_issue_discard(ns->bdev,
+ *   - fs/ext4/mballoc.c|2770| <<ext4_issue_discard>> return __blkdev_issue_discard(sb->s_bdev,
+ *   - fs/f2fs/segment.c|1148| <<__submit_discard_cmd>> err = __blkdev_issue_discard(bdev,
+ *   - fs/xfs/xfs_log_cil.c|548| <<xlog_discard_busy_extents>> error = __blkdev_issue_discard(mp->m_ddev_targp->bt_bdev,
+ */
 int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, int flags,
 		struct bio **biop)
@@ -94,6 +104,24 @@ EXPORT_SYMBOL(__blkdev_issue_discard);
  * Description:
  *    Issue a discard request for the sectors in question.
  */
+/*
+ * called by:
+ *   - block/blk-lib.c|97| <<blkdev_issue_discard>> int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
+ *   - block/ioctl.c|230| <<blk_ioctl_discard>> return blkdev_issue_discard(bdev, start >> 9, len >> 9,
+ *   - drivers/block/xen-blkback/blkback.c|1034| <<dispatch_discard_io>> err = blkdev_issue_discard(bdev, req->u.discard.sector_number,
+ *   - drivers/md/bcache/alloc.c|338| <<bch_allocator_thread>> blkdev_issue_discard(ca->bdev,
+ *   - drivers/md/raid5-cache.c|1347| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev,
+ *   - drivers/md/raid5-cache.c|1351| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev,
+ *   - drivers/md/raid5-cache.c|1355| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev, log->rdev->data_offset, end,
+ *   - fs/block_dev.c|2070| <<blkdev_fallocate>> error = blkdev_issue_discard(bdev, start >> 9, len >> 9,
+ *   - fs/btrfs/extent-tree.c|1957| <<btrfs_issue_discard>> ret = blkdev_issue_discard(bdev, start >> 9, size >> 9,
+ *   - fs/btrfs/extent-tree.c|1974| <<btrfs_issue_discard>> ret = blkdev_issue_discard(bdev, start >> 9, bytes_left >> 9,
+ *   - fs/xfs/xfs_discard.c|114| <<xfs_trim_extents>> error = blkdev_issue_discard(bdev, dbno, dlen, GFP_NOFS, 0);
+ *   - include/linux/blkdev.h|1276| <<sb_issue_discard>> return blkdev_issue_discard(sb->s_bdev,
+ *   - mm/swapfile.c|171| <<discard_swap>> err = blkdev_issue_discard(si->bdev, start_block,
+ *   - mm/swapfile.c|182| <<discard_swap>> err = blkdev_issue_discard(si->bdev, start_block,
+ *   - mm/swapfile.c|219| <<discard_swap_cluster>> if (blkdev_issue_discard(si->bdev, start_block,
+ */
 int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags)
 {
diff --git a/block/blk-map.c b/block/blk-map.c
index db9373b..7f0df5e 100644
--- a/block/blk-map.c
+++ b/block/blk-map.c
@@ -15,6 +15,13 @@
  * Append a bio to a passthrough request.  Only works if the bio can be merged
  * into the request based on the driver constraints.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|83| <<__blk_rq_map_user_iov>> ret = blk_rq_append_bio(rq, &bio);
+ *   - block/blk-map.c|245| <<blk_rq_map_kern>> ret = blk_rq_append_bio(rq, &bio);
+ *   - drivers/target/target_core_pscsi.c|914| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, &bio);
+ *   - drivers/target/target_core_pscsi.c|933| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, &bio);
+ */
 int blk_rq_append_bio(struct request *rq, struct bio **bio)
 {
 	struct bio *orig_bio = *bio;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 17713d7..838f306 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -12,11 +12,17 @@
 
 #include "blk.h"
 
+/*
+ * called by:
+ *   - block/blk-merge.c|53| <<req_gap_back_merge>> return bio_will_gap(req->q, req, req->biotail, bio);
+ *   - block/blk-merge.c|58| <<req_gap_front_merge>> return bio_will_gap(req->q, NULL, bio, req->bio);
+ */
 static inline bool bio_will_gap(struct request_queue *q,
 		struct request *prev_rq, struct bio *prev, struct bio *next)
 {
 	struct bio_vec pb, nb;
 
+	/* 如果没设置virt_boundary就一定返回false */
 	if (!bio_has_data(prev) || !queue_virt_boundary(q))
 		return false;
 
@@ -48,11 +54,20 @@ static inline bool bio_will_gap(struct request_queue *q,
 	return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|565| <<ll_back_merge_fn>> if (req_gap_back_merge(req, bio))
+ *   - block/blk-merge.c|628| <<ll_merge_requests_fn>> if (req_gap_back_merge(req, next->bio))
+ */
 static inline bool req_gap_back_merge(struct request *req, struct bio *bio)
 {
 	return bio_will_gap(req->q, req, req->biotail, bio);
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|642| <<ll_front_merge_fn>> if (req_gap_front_merge(req, bio))
+ */
 static inline bool req_gap_front_merge(struct request *req, struct bio *bio)
 {
 	return bio_will_gap(req->q, NULL, bio, req->bio);
@@ -132,6 +147,10 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
 	return bio_split(bio, q->limits.max_write_same_sectors, GFP_NOIO, bs);
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|232| <<blk_bio_segment_split>> const unsigned max_sectors = get_max_io_size(q, bio);
+ */
 static inline unsigned get_max_io_size(struct request_queue *q,
 				       struct bio *bio)
 {
@@ -144,12 +163,19 @@ static inline unsigned get_max_io_size(struct request_queue *q,
 	return sectors;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|176| <<bvec_split_segs>> seg_size = get_max_segment_size(q, bv->bv_offset + total_len);
+ *   - block/blk-merge.c|389| <<blk_bvec_map_sg>> unsigned len = min(get_max_segment_size(q, offset), nbytes);
+ */
 static unsigned get_max_segment_size(struct request_queue *q,
 				     unsigned offset)
 {
+	/* 返回q->limits.seg_boundary_mask */
 	unsigned long mask = queue_segment_boundary(q);
 
 	/* default segment boundary mask means no boundary limit */
+	/* 返回return q->limits.max_segment_size */
 	if (mask == BLK_SEG_BOUNDARY_MASK)
 		return queue_max_segment_size(q);
 
@@ -161,6 +187,12 @@ static unsigned get_max_segment_size(struct request_queue *q,
  * Split the bvec @bv into segments, and update all kinds of
  * variables.
  */
+/*
+ * called by:
+ *   - block/blk-merge.c|246| <<blk_bio_segment_split>> bvec_split_segs(q, &bv, &nsegs,
+ *   - block/blk-merge.c|261| <<blk_bio_segment_split>> } else if (bvec_split_segs(q, &bv, &nsegs, &sectors,
+ *   - block/blk-merge.c|356| <<__blk_recalc_rq_segments>> bvec_split_segs(q, &bv, &nr_phys_segs, NULL, UINT_MAX);
+ */
 static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 		unsigned *nsegs, unsigned *sectors, unsigned max_segs)
 {
@@ -194,6 +226,10 @@ static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 	return !!len;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|278| <<blk_queue_split>> split = blk_bio_segment_split(q, *bio, &q->bio_split, &nsegs);
+ */
 static struct bio *blk_bio_segment_split(struct request_queue *q,
 					 struct bio *bio,
 					 struct bio_set *bs,
@@ -207,14 +243,27 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	const unsigned max_sectors = get_max_io_size(q, bio);
 	const unsigned max_segs = queue_max_segments(q);
 
+	/*
+	 * 两个名字复杂的变量: bvprv和bvprvp
+	 */
+
+	/*
+	 * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+	 */
 	bio_for_each_bvec(bv, bio, iter) {
 		/*
 		 * If the queue doesn't support SG gaps and adding this
 		 * offset would create a gap, disallow it.
 		 */
+		/*
+		 * 第一次进来bvprvp是NULL
+		 */
 		if (bvprvp && bvec_gap_to_prev(q, bvprvp, bv.bv_offset))
 			goto split;
 
+		/*
+		 * sectors开始是0
+		 */
 		if (sectors + (bv.bv_len >> 9) > max_sectors) {
 			/*
 			 * Consider this a new segment if we're splitting in
@@ -230,6 +279,9 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 			goto split;
 		}
 
+		/*
+		 * nsegs开始也是0
+		 */
 		if (nsegs == max_segs)
 			goto split;
 
@@ -258,6 +310,13 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return do_split ? new : NULL;
 }
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|2246| <<blk_mq_make_request>> blk_queue_split(q, &bio);
+ *   - drivers/md/dm.c|1765| <<dm_process_bio>> blk_queue_split(md->queue, &bio);
+ *   - drivers/md/md.c|307| <<md_make_request>> blk_queue_split(q, &bio);
+ *   - drivers/nvme/host/multipath.c|241| <<nvme_ns_head_make_request>> blk_queue_split(q, &bio);
+ */
 void blk_queue_split(struct request_queue *q, struct bio **bio)
 {
 	struct bio *split, *res;
@@ -306,6 +365,11 @@ void blk_queue_split(struct request_queue *q, struct bio **bio)
 }
 EXPORT_SYMBOL(blk_queue_split);
 
+/*
+ * called by:
+ *   - block/blk-merge.c|397| <<blk_recalc_rq_segments>> rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
+ *   - block/blk-merge.c|405| <<blk_recount_segments>> bio->bi_phys_segments = __blk_recalc_rq_segments(q, bio);
+ */
 static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 					     struct bio *bio)
 {
@@ -326,6 +390,7 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 	}
 
 	for_each_bio(bio) {
+		/* 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page */
 		bio_for_each_bvec(bv, bio, iter)
 			bvec_split_segs(q, &bv, &nr_phys_segs, NULL, UINT_MAX);
 	}
@@ -333,6 +398,11 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 	return nr_phys_segs;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1257| <<blk_cloned_rq_check_limits>> blk_recalc_rq_segments(rq);
+ *   - block/blk-core.c|1542| <<blk_update_request>> blk_recalc_rq_segments(req);
+ */
 void blk_recalc_rq_segments(struct request *rq)
 {
 	rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
@@ -409,6 +479,10 @@ static inline int __blk_bvec_map_sg(struct bio_vec bv,
 }
 
 /* only try to merge bvecs into one sg if they are from two bios */
+/*
+ * called by only:
+ *   - block/blk-merge.c|513| <<__blk_bios_map_sg>> __blk_segment_map_sg_merge(q, &bvec, &bvprv, sg))
+ */
 static inline bool
 __blk_segment_map_sg_merge(struct request_queue *q, struct bio_vec *bvec,
 			   struct bio_vec *bvprv, struct scatterlist **sg)
@@ -430,6 +504,10 @@ __blk_segment_map_sg_merge(struct request_queue *q, struct bio_vec *bvec,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/blk-merge.c|534| <<blk_rq_map_sg>> nsegs = __blk_bios_map_sg(q, rq->bio, sglist, &sg);
+ */
 static int __blk_bios_map_sg(struct request_queue *q, struct bio *bio,
 			     struct scatterlist *sglist,
 			     struct scatterlist **sg)
@@ -440,16 +518,25 @@ static int __blk_bios_map_sg(struct request_queue *q, struct bio *bio,
 	bool new_bio = false;
 
 	for_each_bio(bio) {
+		/*
+		 * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+		 */
 		bio_for_each_bvec(bvec, bio, iter) {
 			/*
 			 * Only try to merge bvecs from two bios given we
 			 * have done bio internal merge when adding pages
 			 * to bio
 			 */
+			/*
+			 * 这里merge的时候没有考虑sg是否表示一个page
+			 */
 			if (new_bio &&
 			    __blk_segment_map_sg_merge(q, &bvec, &bvprv, sg))
 				goto next_bvec;
 
+			/*
+			 * 一个sg的一项只能最多表示一个page!!
+			 */
 			if (bvec.bv_offset + bvec.bv_len <= PAGE_SIZE)
 				nsegs += __blk_bvec_map_sg(bvec, sglist, sg);
 			else
@@ -470,6 +557,18 @@ static int __blk_bios_map_sg(struct request_queue *q, struct bio *bio,
  * map a request to scatterlist, return number of sg entries setup. Caller
  * must make sure sg can hold rq->nr_phys_segments entries
  */
+/*
+ * 部分调用的例子:
+ *   - block/bsg-lib.c|211| <<bsg_map_buffer>> buf->sg_cnt = blk_rq_map_sg(req->q, req, buf->sg_list);
+ *   - drivers/block/virtio_blk.c|439| <<virtio_queue_rq>> num = blk_rq_map_sg(hctx->queue, req, vbr->sg);
+ *   - drivers/block/xen-blkfront.c|741| <<blkif_queue_rw_req>> num_sg = blk_rq_map_sg(req->q, req, rinfo->shadow[id].sg);
+ *   - drivers/ide/ide-io.c|239| <<ide_map_sg>> cmd->sg_nents = blk_rq_map_sg(drive->queue, rq, sg);
+ *   - drivers/nvme/host/fc.c|2119| <<nvme_fc_map_data>> op->nents = blk_rq_map_sg(rq->q, rq, freq->sg_table.sgl);
+ *   - drivers/nvme/host/pci.c|847| <<nvme_map_data>> iod->nents = blk_rq_map_sg(req->q, req, iod->sg);
+ *   - drivers/nvme/host/rdma.c|1266| <<nvme_rdma_map_data>> req->nents = blk_rq_map_sg(rq->q, rq, req->sg_table.sgl);
+ *   - drivers/nvme/target/loop.c|164| <<nvme_loop_queue_rq>> iod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);
+ *   - drivers/scsi/scsi_lib.c|997| <<scsi_init_sgtable>> count = blk_rq_map_sg(req->q, req, sdb->table.sgl);
+ */
 int blk_rq_map_sg(struct request_queue *q, struct request *rq,
 		  struct scatterlist *sglist)
 {
@@ -492,6 +591,11 @@ int blk_rq_map_sg(struct request_queue *q, struct request *rq,
 		rq->extra_len += pad_len;
 	}
 
+	/*
+	 * 设置dma_drain_size的地方:
+	 *   - block/blk-settings.c|723| <<blk_queue_dma_drain>> q->dma_drain_size = size;
+	 *   - drivers/ata/libata-scsi.c|1375| <<ata_scsi_slave_destroy>> q->dma_drain_size = 0;
+	 */
 	if (q->dma_drain_size && q->dma_drain_needed(rq)) {
 		if (op_is_write(req_op(rq)))
 			memset(q->dma_drain_buffer, 0, q->dma_drain_size);
@@ -543,6 +647,11 @@ static inline int ll_new_hw_segment(struct request_queue *q,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|620| <<bio_attempt_back_merge>> if (!ll_back_merge_fn(q, req, bio))
+ *   - block/blk-map.c|27| <<blk_rq_append_bio>> if (!ll_back_merge_fn(rq->q, rq, *bio)) {
+ */
 int ll_back_merge_fn(struct request_queue *q, struct request *req,
 		     struct bio *bio)
 {
@@ -564,6 +673,10 @@ int ll_back_merge_fn(struct request_queue *q, struct request *req,
 	return ll_new_hw_segment(q, req, bio);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|641| <<bio_attempt_front_merge>> if (!ll_front_merge_fn(q, req, bio))
+ */
 int ll_front_merge_fn(struct request_queue *q, struct request *req,
 		      struct bio *bio)
 {
@@ -604,6 +717,10 @@ static bool req_attempt_discard_merge(struct request_queue *q, struct request *r
 	return false;
 }
 
+/*
+ * called by only:
+ *   - block/blk-merge.c|822| <<attempt_merge>> if (!ll_merge_requests_fn(q, req, next))
+ */
 static int ll_merge_requests_fn(struct request_queue *q, struct request *req,
 				struct request *next)
 {
@@ -706,6 +823,12 @@ static enum elv_merge blk_try_req_merge(struct request *req,
  * For non-mq, this has to be called with the request spinlock acquired.
  * For mq with scheduling, the appropriate queue wide lock should be held.
  */
+/*
+ * called by:
+ *   - block/blk-merge.c|854| <<attempt_back_merge>> return attempt_merge(q, rq, next);
+ *   - block/blk-merge.c|864| <<attempt_front_merge>> return attempt_merge(q, prev, rq);
+ *   - block/blk-merge.c|874| <<blk_attempt_req_merge>> free = attempt_merge(q, rq, next);
+ */
 static struct request *attempt_merge(struct request_queue *q,
 				     struct request *req, struct request *next)
 {
@@ -796,6 +919,10 @@ static struct request *attempt_merge(struct request_queue *q,
 	return next;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|397| <<blk_mq_sched_try_merge>> *merged_request = attempt_back_merge(q, rq);
+ */
 struct request *attempt_back_merge(struct request_queue *q, struct request *rq)
 {
 	struct request *next = elv_latter_request(q, rq);
@@ -806,6 +933,10 @@ struct request *attempt_back_merge(struct request_queue *q, struct request *rq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|406| <<blk_mq_sched_try_merge>> *merged_request = attempt_front_merge(q, rq);
+ */
 struct request *attempt_front_merge(struct request_queue *q, struct request *rq)
 {
 	struct request *prev = elv_former_request(q, rq);
@@ -816,6 +947,11 @@ struct request *attempt_front_merge(struct request_queue *q, struct request *rq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|357| <<elv_attempt_insert_merge>> if (q->last_merge && blk_attempt_req_merge(q, q->last_merge, rq))
+ *   - block/elevator.c|369| <<elv_attempt_insert_merge>> if (!__rq || !blk_attempt_req_merge(q, __rq, rq))
+ */
 int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 			  struct request *next)
 {
@@ -830,6 +966,12 @@ int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|728| <<blk_attempt_plug_merge>> if (rq->q != q || !blk_rq_merge_ok(rq, bio))
+ *   - block/blk-mq-sched.c|439| <<blk_mq_bio_list_merge>> if (!blk_rq_merge_ok(rq, bio))
+ *   - block/elevator.c|76| <<elv_bio_merge_ok>> if (!blk_rq_merge_ok(rq, bio))
+ */
 bool blk_rq_merge_ok(struct request *rq, struct bio *bio)
 {
 	if (!rq_mergeable(rq) || !bio_mergeable(bio))
@@ -868,6 +1010,12 @@ bool blk_rq_merge_ok(struct request *rq, struct bio *bio)
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|731| <<blk_attempt_plug_merge>> switch (blk_try_merge(rq, bio)) {
+ *   - block/blk-mq-sched.c|442| <<blk_mq_bio_list_merge>> switch (blk_try_merge(rq, bio)) {
+ *   - block/elevator.c|313| <<elv_merge>> enum elv_merge ret = blk_try_merge(q->last_merge, bio);
+ */
 enum elv_merge blk_try_merge(struct request *rq, struct bio *bio)
 {
 	if (blk_discard_mergable(rq))
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index f945621..33df8dc 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,12 +15,21 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|49| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ *   - block/blk-mq-cpumap.c|53| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ */
 static int cpu_to_queue_index(struct blk_mq_queue_map *qmap,
 			      unsigned int nr_queues, const int cpu)
 {
 	return qmap->queue_offset + (cpu % nr_queues);
 }
 
+/*
+ * called by:
+ *    - block/blk-mq-cpumap.c|51| <<blk_mq_map_queues>> first_sibling = get_first_sibling(cpu);
+ */
 static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
@@ -32,6 +41,19 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3077| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3349| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|470| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1827| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2145| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7116| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1750| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -68,6 +90,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2142| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2198| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2843| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-debugfs-zoned.c b/block/blk-mq-debugfs-zoned.c
index 038cb62..ac46d52 100644
--- a/block/blk-mq-debugfs-zoned.c
+++ b/block/blk-mq-debugfs-zoned.c
@@ -6,6 +6,10 @@
 #include <linux/blkdev.h>
 #include "blk-mq-debugfs.h"
 
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|207| <<global>> { "zone_wlock", 0400, queue_zone_wlock_show, NULL },
+ */
 int queue_zone_wlock_show(void *data, struct seq_file *m)
 {
 	struct request_queue *q = data;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index 3afe327..ce471da 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -834,6 +834,10 @@ static void debugfs_create_files(struct dentry *parent, void *data,
 				    (void *)attr, &blk_mq_debugfs_fops);
 }
 
+/*
+ * called only by:
+ *   - block/blk-sysfs.c|992| <<blk_register_queue>> blk_mq_debugfs_register(q);
+ */
 void blk_mq_debugfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -985,6 +989,11 @@ void blk_mq_debugfs_unregister_queue_rqos(struct request_queue *q)
 	q->rqos_debugfs_dir = NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|860| <<blk_mq_debugfs_register>> blk_mq_debugfs_register_sched_hctx(q, hctx);
+ *   - block/blk-mq-sched.c|536| <<blk_mq_init_sched>> blk_mq_debugfs_register_sched_hctx(q, hctx);
+ */
 void blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 					struct blk_mq_hw_ctx *hctx)
 {
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94..818d826 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -23,6 +23,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|468| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7118| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5806| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
@@ -30,10 +36,26 @@ int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 	unsigned int queue, cpu;
 
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
+		/* return the affinity of a particular msi vector */
 		mask = pci_irq_get_affinity(pdev, queue + offset);
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * queue_offset设置的地方:
+		 *   - drivers/nvme/host/pci.c|466| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|1801| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|1804| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|1810| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|1813| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2130| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2133| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2139| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2142| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *
+		 * mq_map[cpu]记录着当前的sw queue对应的hw queue
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e..4643058 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1815| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|1817| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 2766066..89a8452 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -17,6 +17,9 @@
 #include "blk-mq-tag.h"
 #include "blk-wbt.h"
 
+/*
+ * 目前没有调用
+ */
 void blk_mq_sched_free_hctx_data(struct request_queue *q,
 				 void (*exit)(struct blk_mq_hw_ctx *))
 {
@@ -32,6 +35,10 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_free_hctx_data);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|433| <<blk_mq_get_request>> blk_mq_sched_assign_ioc(rq);
+ */
 void blk_mq_sched_assign_ioc(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -62,8 +69,22 @@ void blk_mq_sched_assign_ioc(struct request *rq)
  * Mark a hardware queue as needing a restart. For shared queues, maintain
  * a count of how many hardware queues are marked for restart.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|213| <<blk_mq_sched_dispatch_requests>> blk_mq_sched_mark_restart_hctx(hctx);
+ *   - block/blk-mq.c|1197| <<blk_mq_mark_tag_wait>> blk_mq_sched_mark_restart_hctx(hctx);
+ *   - block/mq-deadline.c|397| <<dd_dispatch_request>> blk_mq_sched_mark_restart_hctx(hctx);
+ */
 void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 
@@ -71,8 +92,21 @@ void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|337| <<mq_flush_data_end_io>> blk_mq_sched_restart(hctx);
+ *   - block/blk-mq.c|524| <<__blk_mq_free_request>> blk_mq_sched_restart(hctx);
+ */
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 	clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
@@ -85,6 +119,16 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|229| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|234| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *
+ * 核心思想用scheduler的ops.dispatch_request(hctx)获取下一个request
+ * 使用queue_rq()下发
+ * 如果有下发不了的放入hctx->dispatch
+ * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -112,6 +156,13 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 		 * in blk_mq_dispatch_rq_list().
 		 */
 		list_add(&rq->queuelist, &rq_list);
+
+		/*
+		 * blk_mq_dispatch_rq_list():
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 }
 
@@ -131,6 +182,16 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|240| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *   - block/blk-mq-sched.c|246| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *
+ * 在budget允许的范围内
+ * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request, 然后调用queue_rq()
+ * 如果有下发不了的放入hctx->dispatch
+ * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -146,6 +207,9 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request
+		 */
 		rq = blk_mq_dequeue_from_ctx(hctx, ctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -162,11 +226,31 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		/* round robin for fair dispatch */
 		ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
 
+		/*
+		 * blk_mq_dispatch_rq_list():
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 
+	/*
+	 * 在以下使用dispatch_from:
+	 *   - block/blk-mq-sched.c|199| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+	 *   - block/blk-mq-sched.c|237| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+	 *   - block/blk-mq.c|3238| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+	 */
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1478| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -178,12 +262,36 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
 		return;
 
+	/*
+	 * used only by:
+	 *   - block/blk-mq-debugfs.c|612| <<hctx_run_show>> seq_printf(m, "%lu\n", hctx->run);
+	 *   - block/blk-mq-debugfs.c|621| <<hctx_run_write>> hctx->run = 0;
+	 *   - block/blk-mq-sched.c|229| <<blk_mq_sched_dispatch_requests>> hctx->run++;
+	 */
 	hctx->run++;
 
 	/*
 	 * If we have previous entries on our dispatch list, grab them first for
 	 * more fair dispatch.
 	 */
+	/*
+	 * 往hctx->dispatch添加新元素的地方:
+	 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * 从hctx->dispatch移除元素的地方(下发):
+	 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 *
+	 * 其他使用hctx->dispatch的地方:
+	 *   - block/blk-mq-debugfs.c|379| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+	 *   - block/blk-mq-debugfs.c|386| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+	 *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+	 *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+	 *   - block/blk-mq.c|72| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+	 *   - block/blk-mq.c|2474| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+	 */
 	if (!list_empty_careful(&hctx->dispatch)) {
 		spin_lock(&hctx->lock);
 		if (!list_empty(&hctx->dispatch))
@@ -206,23 +314,81 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	 */
 	if (!list_empty(&rq_list)) {
 		blk_mq_sched_mark_restart_hctx(hctx);
+		/*
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+			/*
+			 * 上面设置的has_sched_dispatch为true说明有io scheduler
+			 *
+			 *
+			 * blk_mq_do_dispatch_sched():
+			 * 核心思想用scheduler的ops.dispatch_request(hctx)获取下一个request
+			 * 使用queue_rq()下发
+			 * 如果有下发不了的放入hctx->dispatch
+			 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+			 *
+			 *
+			 * blk_mq_do_dispatch_ctx():
+			 * 在budget允许的范围内
+			 * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request, 然后调用queue_rq()
+			 * 如果有下发不了的放入hctx->dispatch
+			 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+			 */
 			if (has_sched_dispatch)
 				blk_mq_do_dispatch_sched(hctx);
 			else
 				blk_mq_do_dispatch_ctx(hctx);
 		}
 	} else if (has_sched_dispatch) {
+		/*
+		 * 核心思想用scheduler的ops.dispatch_request(hctx)获取下一个request
+		 * 使用queue_rq()下发
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		blk_mq_do_dispatch_sched(hctx);
 	} else if (hctx->dispatch_busy) {
+		/*
+		 * 设置dispatch_busy的地方:
+		 *   - block/blk-mq.c|1445| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+		 *
+		 * 使用dispatch_busy的地方:
+		 *   - block/blk-mq-debugfs.c|637| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+		 *   - block/blk-mq-sched.c|338| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+		 *   - block/blk-mq-sched.c|636| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+		 *   - block/blk-mq.c|1435| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+		 *   - block/blk-mq.c|2554| <<blk_mq_make_request>> !data.hctx->dispatch_busy)) {
+		 */
 		/* dequeue request one by one from sw queue if queue is busy */
+		/*
+		 * 在budget允许的范围内
+		 * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request, 然后调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		blk_mq_do_dispatch_ctx(hctx);
 	} else {
+		/*
+		 * 把hctx所属的ctx的ctx->rq_lists的request放入参数的list
+		 */
 		blk_mq_flush_busy_ctxs(hctx, &rq_list);
+		/*
+		 * 核心思想是为list的request调用queue_rq()
+		 * 如果有下发不了的放入hctx->dispatch
+		 * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+		 */
 		blk_mq_dispatch_rq_list(q, &rq_list, false);
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|2053| <<bfq_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ *   - block/mq-deadline.c|480| <<dd_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ */
 bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 			    struct request **merged_request)
 {
@@ -259,6 +425,11 @@ EXPORT_SYMBOL_GPL(blk_mq_sched_try_merge);
  * Iterate list of requests and see if we can merge this bio with any
  * of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|340| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/kyber-iosched.c|575| <<kyber_bio_merge>> merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio);
+ */
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio)
 {
@@ -302,6 +473,12 @@ EXPORT_SYMBOL_GPL(blk_mq_bio_list_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|370| <<__blk_mq_sched_bio_merge>> ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
+ *
+ * 尝试把bio给merge到ctx->rq_lists
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_hw_ctx *hctx,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
@@ -318,6 +495,13 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.h|39| <<blk_mq_sched_bio_merge>> return __blk_mq_sched_bio_merge(q, bio);
+ *
+ * 如果支持scheduler就把bio用ops.bio_merge给merge了
+ * 否则尝试把bio给merge到ctx->rq_lists
+ */
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
@@ -336,6 +520,9 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 			!list_empty_careful(&ctx->rq_lists[type])) {
 		/* default per sw-queue merge */
 		spin_lock(&ctx->lock);
+		/*
+		 * 尝试把bio给merge到ctx->rq_lists
+		 */
 		ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
 		spin_unlock(&ctx->lock);
 	}
@@ -344,6 +531,11 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|5052| <<bfq_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq)) {
+ *   - block/mq-deadline.c|505| <<dd_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq))
+ */
 bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
 	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
@@ -356,11 +548,24 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|420| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ *
+ * 只有rq->rq_flags设置了RQF_FLUSH_SEQ才会添加到hctx->dispatch并且返回true,
+ * 否则返回false (不能bypass sched)
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
 {
 	/* dispatch flush rq directly */
+	/*
+	 * 设置和取消RQF_FLUSH_SEQ的地方:
+	 *   - block/blk-flush.c|130| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|309| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|401| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 */
 	if (rq->rq_flags & RQF_FLUSH_SEQ) {
 		spin_lock(&hctx->lock);
 		list_add(&rq->queuelist, &hctx->dispatch);
@@ -374,6 +579,22 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|62| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq-sched.h|21| <<blk_mq_sched_insert_request>> void blk_mq_sched_insert_request(struct request *rq, bool at_head,
+ *   - block/blk-mq.c|844| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|850| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2151| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *
+ * 1. 如果没有RQF_FLUSH_SEQ但是是flush则blk_insert_flush()
+ * 2. 试试能否blk_mq_sched_bypass_insert(): 比如是否有RQF_FLUSH_SEQ
+ * 3. 如果支持IO调度则用scheduler的.insert_requests()
+ * 4. 否则把request放入request->mq_ctx的rq_lists, 然后把ctx在hctx->ctx_map对应的bit设置
+ * 根据参数是否blk_mq_run_hw_queue()!
+ *                         
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -382,6 +603,13 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
 
+	/*
+	 * 设置和取消RQF_FLUSH_SEQ的地方:
+	 *   - block/blk-flush.c|130| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|309| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+	 *   - block/blk-flush.c|401| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+	 */
+
 	/* flush rq in flush machinery need to be dispatched directly */
 	if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
 		blk_insert_flush(rq);
@@ -390,6 +618,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 
 	WARN_ON(e && (rq->tag != -1));
 
+	/*
+	 * 只有rq->rq_flags设置了RQF_FLUSH_SEQ才会添加到hctx->dispatch并且返回true,
+	 * 否则返回false (不能bypass sched)
+	 */
 	if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
 		goto run;
 
@@ -400,6 +632,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		e->type->ops.insert_requests(hctx, &list, at_head);
 	} else {
 		spin_lock(&ctx->lock);
+		/*
+		 * 把request放入ctx->rq_lists的头或者尾
+		 * 把ctx在hctx中对应的sbitmap设置上
+		 */
 		__blk_mq_insert_request(hctx, rq, at_head);
 		spin_unlock(&ctx->lock);
 	}
@@ -409,6 +645,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1854| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|1875| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -437,6 +678,10 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 			if (list_empty(list))
 				goto out;
 		}
+		/*
+		 * 把参数list中的request放入ctx->rq_lists
+		 * 并且标记ctx在hctx上pending(blk_mq_hctx_mark_pending)
+		 */
 		blk_mq_insert_requests(hctx, ctx, list);
 	}
 
@@ -445,6 +690,10 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 	percpu_ref_put(&q->q_usage_counter);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|509| <<blk_mq_sched_alloc_tags>> blk_mq_sched_free_tags(set, hctx, hctx_idx);
+ */
 static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -456,6 +705,10 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|536| <<blk_mq_init_sched>> ret = blk_mq_sched_alloc_tags(q, hctx, i);
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -476,6 +729,11 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 }
 
 /* called in queue's release handler, tagset has gone away */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|565| <<blk_mq_init_sched>> blk_mq_sched_tags_teardown(q);
+ *   - block/blk-mq-sched.c|602| <<blk_mq_exit_sched>> blk_mq_sched_tags_teardown(q);
+ */
 static void blk_mq_sched_tags_teardown(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -489,6 +747,11 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|576| <<elevator_switch_mq>> ret = blk_mq_init_sched(q, new_e);
+ *   - block/elevator.c|622| <<elevator_init_mq>> err = blk_mq_init_sched(q, e);
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -549,6 +812,13 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
  * called in either blk_queue_cleanup or elevator_switch, tagset
  * is required for freeing requests
  */
+/*
+ * called by:
+ *   - block/blk-core.c|333| <<blk_cleanup_queue>> blk_mq_sched_free_requests(q);
+ *   - block/blk-mq-sched.c|552| <<blk_mq_init_sched>> blk_mq_sched_free_requests(q);
+ *   - block/blk-mq-sched.c|564| <<blk_mq_init_sched>> blk_mq_sched_free_requests(q);
+ *   - block/blk.h|187| <<elevator_exit>> blk_mq_sched_free_requests(q);
+ */
 void blk_mq_sched_free_requests(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -562,6 +832,11 @@ void blk_mq_sched_free_requests(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|553| <<blk_mq_init_sched>> blk_mq_exit_sched(q, eq);
+ *   - block/elevator.c|185| <<__elevator_exit>> blk_mq_exit_sched(q, e);
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index 3cf92cb..26d0a54 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -30,15 +30,37 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e);
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e);
 void blk_mq_sched_free_requests(struct request_queue *q);
 
+/*
+ * called by only;
+ *   - block/blk-mq.c|2065| <<blk_mq_make_request>> if (blk_mq_sched_bio_merge(q, bio))
+ *
+ * 如果支持scheduler就把bio用ops.bio_merge给merge了
+ * 否则尝试把bio给merge到ctx->rq_lists
+ */
 static inline bool
 blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
+	/*
+	 * blk_queue_nomerges():
+	 * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+	 */
 	if (blk_queue_nomerges(q) || !bio_mergeable(bio))
 		return false;
 
+	/*
+	 * 如果支持scheduler就把bio用ops.bio_merge给merge了
+	 * 否则尝试把bio给merge到ctx->rq_lists
+	 */
 	return __blk_mq_sched_bio_merge(q, bio);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|238| <<blk_mq_sched_try_merge>> if (!blk_mq_sched_allow_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|247| <<blk_mq_sched_try_merge>> if (!blk_mq_sched_allow_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|284| <<blk_mq_bio_list_merge>> if (blk_mq_sched_allow_merge(q, rq, bio))
+ *   - block/blk-mq-sched.c|288| <<blk_mq_bio_list_merge>> if (blk_mq_sched_allow_merge(q, rq, bio))
+ */
 static inline bool
 blk_mq_sched_allow_merge(struct request_queue *q, struct request *rq,
 			 struct bio *bio)
@@ -51,6 +73,10 @@ blk_mq_sched_allow_merge(struct request_queue *q, struct request *rq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|576| <<__blk_mq_end_request>> blk_mq_sched_completed_request(rq, now);
+ */
 static inline void blk_mq_sched_completed_request(struct request *rq, u64 now)
 {
 	struct elevator_queue *e = rq->q->elevator;
@@ -59,6 +85,10 @@ static inline void blk_mq_sched_completed_request(struct request *rq, u64 now)
 		e->type->ops.completed_request(rq, now);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|702| <<blk_mq_start_request>> blk_mq_sched_started_request(rq);
+ */
 static inline void blk_mq_sched_started_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -68,6 +98,10 @@ static inline void blk_mq_sched_started_request(struct request *rq)
 		e->type->ops.started_request(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|753| <<blk_mq_requeue_request>> blk_mq_sched_requeue_request(rq);
+ */
 static inline void blk_mq_sched_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -77,6 +111,10 @@ static inline void blk_mq_sched_requeue_request(struct request *rq)
 		e->type->ops.requeue_request(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|70| <<blk_mq_hctx_has_pending>> blk_mq_sched_has_work(hctx);
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
@@ -87,8 +125,20 @@ static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1359| <<blk_mq_dispatch_rq_list>> needs_restart = blk_mq_sched_needs_restart(hctx);
+ */
 static inline bool blk_mq_sched_needs_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 }
 
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index d6e1a9b..7ade772 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -243,6 +243,11 @@ static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 	kobject_del(&hctx->kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|342| <<__blk_mq_register_dev>> ret = blk_mq_register_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|409| <<blk_mq_sysfs_register>> ret = blk_mq_register_hctx(hctx);
+ */
 static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -314,6 +319,15 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|357| <<blk_mq_register_dev>> ret = __blk_mq_register_dev(dev, q); --> 没人调用
+ *   - block/blk-sysfs.c|986| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ *
+ * blk_register_queue()
+ *  -> __blk_mq_register_dev()
+ *      -> blk_mq_register_hctx()
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -322,6 +336,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	WARN_ON_ONCE(!q->kobj.parent);
 	lockdep_assert_held(&q->sysfs_lock);
 
+	/* 第二个参数是parent */
 	ret = kobject_add(q->mq_kobj, kobject_get(&dev->kobj), "%s", "mq");
 	if (ret < 0)
 		goto out;
@@ -349,6 +364,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/* 没人调用 */
 int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	int ret;
@@ -376,12 +392,21 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3356| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
 	int i, ret = 0;
 
 	mutex_lock(&q->sysfs_lock);
+	/*
+	 * 在以下修改q->mq_sysfs_init_done:
+	 *   - block/blk-mq-sysfs.c|282| <<blk_mq_unregister_dev>> q->mq_sysfs_init_done = false;
+	 *   - block/blk-mq-sysfs.c|337| <<__blk_mq_register_dev>> q->mq_sysfs_init_done = true;
+	 */
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 7513c8e..3ed3cfc 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -28,6 +28,12 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|61| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ *
+ * 应该就是表示当前hctx在被使用
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
@@ -40,6 +46,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|63| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|280| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -51,6 +62,12 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|111| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ *
+ * 应该就是表示当前hctx空闲
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -67,13 +84,33 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|110| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ *
+ * 我们应该是期待hctx_may_queue()返回true, 才能获得一个tag吧
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
 	unsigned int depth, users;
 
+	/*
+	 * 如果没设置BLK_MQ_F_TAG_SHARED, 一定返回true
+	 *
+	 * 说明scsi多个lun或者nvme多个namespace才可能返回false
+	 */
 	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return true;
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.c|37| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|38| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|104| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * 没设置BLK_MQ_S_TAG_ACTIVE反而返回true???
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		return true;
 
@@ -83,6 +120,13 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	if (bt->sb.depth == 1)
 		return true;
 
+	/*
+	 * 在以下使用active_queues:
+	 *   - block/blk-mq-debugfs.c|465| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|66| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|91| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	users = atomic_read(&hctx->tags->active_queues);
 	if (!users)
 		return true;
@@ -94,9 +138,22 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|163| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|186| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|192| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+	/*
+	 * 在以下使用BLK_MQ_REQ_INTERNAL:
+	 *   - block/blk-mq-tag.c|127| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|322| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.c|395| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -106,8 +163,17 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|393| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1068| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
@@ -172,6 +238,10 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		data->ctx = blk_mq_get_ctx(data->q);
 		data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
 						data->ctx);
+		/*
+		 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+		 * 否则返回data->hctx->tags
+		 */
 		tags = blk_mq_tags_from_data(data);
 		if (data->flags & BLK_MQ_REQ_RESERVED)
 			bt = &tags->breserved_tags;
@@ -198,6 +268,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|517| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|519| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|225| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
@@ -254,6 +330,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|492| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|493| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -332,6 +413,10 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|360| <<blk_mq_tagset_busy_iter>> blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
+ */
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -350,6 +435,26 @@ static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|430| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - drivers/block/mtip32xx/mtip32xx.c|2685| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags, mtip_queue_cmd, dd);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2690| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags,
+ *   - drivers/block/mtip32xx/mtip32xx.c|3808| <<mtip_block_remove>> blk_mq_tagset_busy_iter(&dd->tags, mtip_no_dev_cleanup, dd);
+ *   - drivers/block/nbd.c|762| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/block/skd_main.c|396| <<skd_in_flight>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_inc_in_flight, &count);
+ *   - drivers/block/skd_main.c|1917| <<skd_recover_requests>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_recover_request, skdev);
+ *   - drivers/nvme/host/fc.c|2763| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|2785| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|2412| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2413| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|905| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|918| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1745| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1759| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|408| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|417| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -376,6 +481,15 @@ EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|123| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|146| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|864| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|966| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *
+ * 最终间接调用的bt_iter()会判断(rq->q == hctx->queue), 说明request属于这个queue
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -416,6 +530,14 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|547| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ *
+ * blk_mq_alloc_rq_map()
+ *  -> blk_mq_init_tags()
+ *      -> blk_mq_init_bitmap_tags()
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -436,6 +558,12 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2146| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ *
+ * 这里初始化blk_mq_tags的nr_tags(nr_reserved_tags)和sbitmap
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -464,6 +592,11 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3220| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3223| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -529,6 +662,16 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 调用的几个例子:
+ *   - drivers/nvme/host/nvme.h|129| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|693| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|859| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|3698| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5620| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5291| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|490| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..1ba657b 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -9,8 +9,30 @@
  */
 struct blk_mq_tags {
 	unsigned int nr_tags;
+	/*
+	 * 来源是set->reserved_tags, 设置set->reserved_tags的地方:
+	 *   - drivers/ide/ide-probe.c|783| <<ide_init_queue>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|2436| <<nvme_fc_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|3085| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|718| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|731| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/tcp.c|1447| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/tcp.c|1458| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/target/loop.c|340| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/target/loop.c|512| <<nvme_loop_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *
+	 * 只在以下修改:
+	 *   - block/blk-mq-tag.c|493| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用active_queues:
+	 *   - block/blk-mq-debugfs.c|465| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|66| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|91| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -36,11 +58,21 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|152| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|203| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1151| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/*
+	 * sbq_wait_ptr() - Get the next wait queue to use for a &struct
+	 * sbitmap_queue.
+	 */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -53,6 +85,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|407| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1084| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -61,6 +98,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|980| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2314| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,12 +117,22 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|219| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|302| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
 	hctx->tags->rqs[tag] = rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|218| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1081| <<blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 4883416..49a4de9 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,16 +21,30 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|701| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|661| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
+	/*
+	 * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+	 * struct virtio_config_ops virtio_pci_config_nodev_ops.get_vq_affinity = vp_get_vq_affinity()
+	 * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+	 */
 	if (!vdev->config->get_vq_affinity)
 		goto fallback;
 
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
+		/*
+		 * 对于virtblk, 参数first_vec是0
+		 * 对于virtscsi, 参数first_vec是2
+		 */
 		mask = vdev->config->get_vq_affinity(vdev, first_vec + queue);
 		if (!mask)
 			goto fallback;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index ce0f5f4..6b6d2c6 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -39,6 +39,49 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 最核心的函数: blk_mq_sched_dispatch_requests()
+ */
+
+/*
+ * blktrace:
+ * Q  - trace_block_bio_queue()
+ * M  - trace_block_bio_backmerge()
+ * F  - trace_block_bio_frontmerge()
+ * G  - trace_block_getrq()
+ * S  - trace_block_sleeprq()
+ * R  - trace_block_rq_requeue()
+ * D  - trace_block_rq_issue()
+ * C  - trace_block_rq_complete()
+ * P  - trace_block_plug()
+ * U  - trace_block_unplug()
+ * UT - trace_block_unplug()
+ * I  - trace_block_rq_insert()
+ * X  - trace_block_split()
+ * B  - trace_block_bio_bounce()
+ * A  - trace_block_bio_remap() and trace_block_rq_remap()
+ *
+ *
+ * #define bio_op(bio) \
+ *         ((bio)->bi_opf & REQ_OP_MASK)
+ * #define req_op(req) \
+ *         ((req)->cmd_flags & REQ_OP_MASK)
+ *
+ * F  - if (op & REQ_PREFLUSH)
+ *
+ * W  - if (op & REQ_OP_MASK == REQ_OP_WRITE || op & REQ_OP_MASK == REQ_OP_WRITE_SAME)
+ * D  - if (op & REQ_OP_MASK == REQ_OP_DISCARD)
+ * DE - if (op & REQ_OP_MASK == REQ_OP_SECURE_ERASE)
+ * F  - if (op & REQ_OP_MASK == REQ_OP_FLUSH)
+ * R  - if (op & REQ_OP_MASK == REQ_OP_READ)
+ * N  - if (op & REQ_OP_MASK == else)
+ *
+ * F  - if (op & REQ_FUA)
+ * A  - if (op & REQ_RAHEAD)
+ * S  - if (op & REQ_SYNC)
+ * M  - if (op & REQ_META)
+ */
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
@@ -63,8 +106,48 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  * Check if any of the ctx, dispatch list or elevator
  * have pending work in this hardware queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1520| <<blk_mq_run_hw_queue>> blk_mq_hctx_has_pending(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 往hctx->dispatch添加新元素的地方:
+	 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 * 从hctx->dispatch移除元素的地方(下发):
+	 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 * 其他使用hctx->dispatch的地方:
+	 *   - block/blk-mq-debugfs.c|379| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+	 *   - block/blk-mq-debugfs.c|386| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+	 *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+	 *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+	 *   - block/blk-mq.c|72| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+	 *   - block/blk-mq.c|2474| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+	 *
+	 *
+	 * 似乎用来表示hctx的某一个ctx->rq_list是否有request
+	 *
+	 * 设置和取消hctx->ctx_map的地方:
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|110| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 * 遍历hctx->ctx_map的地方(都是从ctx->rq_list移除的操作):
+	 *   - block/blk-mq.c|1095| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1133| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off, ---> dispatch_rq_from_ctx
+	 * 测试hctx->ctx_map的地方:
+	 *   - block/blk-mq-sched.c|171| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|89| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|101| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 * 其他初始化hctx->ctx_map的地方:
+	 *   - block/blk-mq-debugfs.c|455| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sysfs.c|44| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2526| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2547| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2716| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
 			blk_mq_sched_has_work(hctx);
@@ -73,15 +156,28 @@ static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1798| <<__blk_mq_insert_request>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *   - block/blk-mq.c|1835| <<blk_mq_insert_requests>> blk_mq_hctx_mark_pending(hctx, ctx);
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
+	/*
+	 * 设置index_hw的地方:
+	 *   - block/blk-mq.c|3326| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	const int bit = ctx->index_hw[hctx->type];
 
 	if (!sbitmap_test_bit(&hctx->ctx_map, bit))
 		sbitmap_set_bit(&hctx->ctx_map, bit);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2990| <<blk_mq_hctx_notify_dead>> blk_mq_hctx_clear_pending(hctx, ctx);
+ */
 static void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,
 				      struct blk_mq_ctx *ctx)
 {
@@ -95,6 +191,10 @@ struct mq_inflight {
 	unsigned int *inflight;
 };
 
+/*
+ * used by only:
+ *   - block/blk-mq.c|211| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ */
 static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 				  struct request *rq, void *priv,
 				  bool reserved)
@@ -110,6 +210,10 @@ static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|75| <<part_in_flight>> return blk_mq_in_flight(q, part);
+ */
 unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	unsigned inflight[2];
@@ -121,6 +225,10 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight[0];
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|238| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ */
 static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 				     struct request *rq, void *priv,
 				     bool reserved)
@@ -133,6 +241,10 @@ static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called only by:
+ *   - block/genhd.c|100| <<part_in_flight_rw>> blk_mq_in_flight_rw(q, part, inflight);
+ */
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2])
 {
@@ -142,9 +254,26 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|275| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|192| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3806| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|324| <<pmem_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3878| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	mutex_lock(&q->mq_freeze_lock);
+	/*
+	 * used by:
+	 *   - block/blk-core.c|427| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|253| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|314| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 *   - block/blk-mq.c|315| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|316| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	if (++q->mq_freeze_depth == 1) {
 		percpu_ref_kill(&q->q_usage_counter);
 		mutex_unlock(&q->mq_freeze_lock);
@@ -175,6 +304,11 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);
  * Guarantee no request is in use, so we can change any data structure of
  * the queue afterward.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|307| <<blk_cleanup_queue>> blk_freeze_queue(q);
+ *   - block/blk-mq.c|197| <<blk_mq_freeze_queue>> blk_freeze_queue(q);
+ */
 void blk_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -215,6 +349,11 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  * FIXME: replace the scsi_internal_device_*block_nowait() calls in the
  * mpt3sas driver such that this function can be removed.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|341| <<blk_mq_quiesce_queue>> blk_mq_quiesce_queue_nowait(q);
+ *   - drivers/scsi/scsi_lib.c|2610| <<scsi_internal_device_block_nowait>> blk_mq_quiesce_queue_nowait(q);
+ */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
@@ -230,6 +369,22 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
  * sure no dispatch can happen until the queue is unquiesced via
  * blk_mq_unquiesce_queue().
  */
+/*
+ * 部分调用的地方:
+ *   - block/blk-mq.c|3691| <<blk_mq_update_nr_requests>> blk_mq_quiesce_queue(q);
+ *   - block/blk-sysfs.c|482| <<queue_wb_lat_store>> blk_mq_quiesce_queue(q);
+ *   - block/elevator.c|644| <<elevator_switch>> blk_mq_quiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|968| <<virtblk_freeze>> blk_mq_quiesce_queue(vblk->disk->queue);
+ *   - drivers/ide/ide-probe.c|1166| <<drive_rq_insert_work>> blk_mq_quiesce_queue(drive->queue);
+ *   - drivers/md/dm-rq.c|76| <<dm_stop_queue>> blk_mq_quiesce_queue(q);
+ *   - drivers/nvme/host/core.c|3889| <<nvme_stop_queues>> blk_mq_quiesce_queue(ns->queue);
+ *   - drivers/nvme/host/fc.c|2784| <<nvme_fc_delete_association>> blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1402| <<nvme_suspend_queue>> blk_mq_quiesce_queue(nvmeq->dev->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|902| <<nvme_rdma_teardown_admin_queue>> blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/tcp.c|1742| <<nvme_tcp_teardown_admin_queue>> blk_mq_quiesce_queue(ctrl->admin_q);
+ *   - drivers/nvme/target/loop.c|416| <<nvme_loop_shutdown_ctrl>> blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2642| <<scsi_internal_device_block>> blk_mq_quiesce_queue(q);
+ */
 void blk_mq_quiesce_queue(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -256,6 +411,28 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  * This function recovers queue into the state before quiescing
  * which is done by blk_mq_quiesce_queue.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3717| <<blk_mq_update_nr_requests>> blk_mq_unquiesce_queue(q);
+ *   - block/blk-sysfs.c|487| <<queue_wb_lat_store>> blk_mq_unquiesce_queue(q);
+ *   - block/elevator.c|648| <<elevator_switch>> blk_mq_unquiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|985| <<virtblk_restore>> blk_mq_unquiesce_queue(vblk->disk->queue);
+ *   - drivers/ide/ide-probe.c|1180| <<drive_rq_insert_work>> blk_mq_unquiesce_queue(drive->queue);
+ *   - drivers/md/dm-rq.c|67| <<dm_start_queue>> blk_mq_unquiesce_queue(q);
+ *   - drivers/nvme/host/core.c|108| <<nvme_set_queue_dying>> blk_mq_unquiesce_queue(ns->queue);
+ *   - drivers/nvme/host/core.c|3827| <<nvme_kill_queues>> blk_mq_unquiesce_queue(ctrl->admin_q);
+ *   - drivers/nvme/host/core.c|3900| <<nvme_start_queues>> blk_mq_unquiesce_queue(ns->queue);
+ *   - drivers/nvme/host/fc.c|2002| <<nvme_fc_ctrl_free>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|2624| <<nvme_fc_create_association>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|2817| <<nvme_fc_delete_association>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1613| <<nvme_dev_remove_admin>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1647| <<nvme_alloc_admin_tags>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|2423| <<nvme_dev_disable>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_teardown_admin_queue>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/tcp.c|1747| <<nvme_tcp_teardown_admin_queue>> blk_mq_unquiesce_queue(ctrl->admin_q);
+ *   - drivers/nvme/target/loop.c|419| <<nvme_loop_shutdown_ctrl>> blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2652| <<scsi_start_queue>> blk_mq_unquiesce_queue(q);
+ */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
 	blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
@@ -265,6 +442,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by only:
+ *   - block/blk-core.c|278| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -285,14 +466,27 @@ EXPORT_SYMBOL(blk_mq_can_queue);
  * Only need start/end time stamping if we have stats enabled, or using
  * an IO scheduler.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|493| <<blk_mq_rq_ctx_init>> if (blk_mq_need_time_stamp(rq))
+ *   - block/blk-mq.c|760| <<__blk_mq_end_request>> if (blk_mq_need_time_stamp(rq))
+ */
 static inline bool blk_mq_need_time_stamp(struct request *rq)
 {
 	return (rq->rq_flags & RQF_IO_STAT) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|469| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+	/*
+	 * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
 	req_flags_t rq_flags = 0;
@@ -348,6 +542,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|495| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|546| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|2162| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -436,6 +636,10 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/core.c|437| <<nvme_alloc_request>> req = blk_mq_alloc_request_hctx(q, op, flags,
+ */
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
 {
@@ -482,6 +686,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|700| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|1141| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -499,6 +708,27 @@ static void __blk_mq_free_request(struct request *rq)
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|2056| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|549| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|731| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|484| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1002| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/block/mtip32xx/mtip32xx.c|1048| <<mtip_exec_internal_command>> blk_mq_free_request(rq);
+ *   - drivers/ide/ide-atapi.c|218| <<ide_prep_sense>> blk_mq_free_request(sense_rq);
+ *   - drivers/ide/ide-cd.c|272| <<ide_cd_free_sense>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/ide/ide-probe.c|984| <<drive_release_dev>> blk_mq_free_request(drive->sense_rq);
+ *   - drivers/nvme/host/core.c|801| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|902| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|912| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|644| <<nvme_nvm_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|716| <<nvme_nvm_submit_io_sync>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/lightnvm.c|863| <<nvme_nvm_submit_user_cmd>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1206| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|2185| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2315| <<fnic_scsi_host_end_tag>> blk_mq_free_request(dummy);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -515,7 +745,23 @@ void blk_mq_free_request(struct request *rq)
 		}
 	}
 
+	/*
+	 * 在以下修改或者使用ctx->rq_completed:
+	 *   - block/blk-mq-debugfs.c|716| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|725| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|715| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	ctx->rq_completed[rq_is_sync(rq)]++;
+	/*
+	 * 在以下使用nr_active:
+	 *   - block/blk-mq.c|467| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1363| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|723| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|270| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|3018| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq-tag.c|138| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq-debugfs.c|629| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 */
 	if (rq->rq_flags & RQF_MQ_INFLIGHT)
 		atomic_dec(&hctx->nr_active);
 
@@ -530,6 +776,12 @@ void blk_mq_free_request(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_free_request);
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|593| <<blk_mq_end_request>> __blk_mq_end_request(rq, error);
+ *   - drivers/ide/ide-io.c|76| <<ide_end_rq>> __blk_mq_end_request(rq, error);
+ *   - drivers/scsi/scsi_lib.c|599| <<scsi_end_request>> __blk_mq_end_request(req, error);
+ */
 inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	u64 now = 0;
@@ -556,6 +808,25 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-flush.c|196| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|378| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1341| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|1956| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - block/blk-mq.c|1992| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+ *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/block/loop.c|493| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/null_blk_main.c|621| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/md/dm-rq.c|174| <<dm_end_request>> blk_mq_end_request(rq, error);
+ *   - drivers/md/dm-rq.c|271| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+ *   - drivers/nvme/host/core.c|281| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *   - drivers/nvme/host/multipath.c|51| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+ *   - drivers/scsi/scsi_transport_fc.c|3584| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -572,6 +843,10 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|898| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -579,6 +854,12 @@ static void __blk_mq_complete_request(struct request *rq)
 	bool shared = false;
 	int cpu;
 
+	/*
+	 * 修改和使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq.c|810| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|889| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|4193| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
 	/*
 	 * Most of single queue controllers, there is only one irq vector
@@ -647,6 +928,18 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 部分调用的例子:
+ *   - drivers/block/loop.c|505| <<lo_rw_aio_do_completion>> blk_mq_complete_request(rq);
+ *   - drivers/block/loop.c|1939| <<loop_handle_cmd>> blk_mq_complete_request(rq);
+ *   - drivers/block/null_blk_main.c|1234| <<null_handle_cmd>> blk_mq_complete_request(cmd->rq);
+ *   - drivers/block/null_blk_main.c|1325| <<null_timeout_rq>> blk_mq_complete_request(rq);
+ *   - drivers/block/virtio_blk.c|323| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/md/dm-rq.c|291| <<dm_complete_request>> blk_mq_complete_request(rq);
+ *   - drivers/nvme/host/nvme.h|405| <<nvme_end_request>> blk_mq_complete_request(req);
+ *   - drivers/scsi/scsi_lib.c|1601| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -656,8 +949,18 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|291| <<nvme_cancel_request>> blk_mq_complete_request_sync(req);
+ */
 void blk_mq_complete_request_sync(struct request *rq)
 {
+	/*
+	 * 修改和使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq.c|810| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|889| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|4193| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
 	rq->q->mq_ops->complete(rq);
 }
@@ -669,6 +972,24 @@ int blk_mq_request_started(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
 
+/*
+ * 部分调用的例子:
+ *   - block/bsg-lib.c|271| <<bsg_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/loop.c|1893| <<loop_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/block/null_blk_main.c|1333| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|323| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/ide/ide-io.c|578| <<ide_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/md/dm-rq.c|449| <<dm_start_request>> blk_mq_start_request(orig);
+ *   - drivers/nvme/host/fabrics.c|556| <<nvmf_fail_nonready_command>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/fc.c|2255| <<nvme_fc_start_fcp_op>> blk_mq_start_request(op->rq);
+ *   - drivers/nvme/host/pci.c|925| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/nvme/host/rdma.c|1738| <<nvme_rdma_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/host/tcp.c|2114| <<nvme_tcp_queue_rq>> blk_mq_start_request(rq);
+ *   - drivers/nvme/target/loop.c|149| <<nvme_loop_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1574| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1650| <<scsi_queue_rq>> blk_mq_start_request(req);
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -702,6 +1023,12 @@ void blk_mq_start_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_start_request);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|982| <<blk_mq_requeue_request>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1588| <<blk_mq_dispatch_rq_list>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|2321| <<__blk_mq_issue_directly>> __blk_mq_requeue_request(rq);
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -719,6 +1046,18 @@ static void __blk_mq_requeue_request(struct request *rq)
 	}
 }
 
+/*
+ * 调用的部分例子:
+ *   - drivers/block/loop.c|481| <<lo_complete_rq>> blk_mq_requeue_request(rq, true);
+ *   - drivers/block/null_blk_main.c|1344| <<null_queue_rq>> blk_mq_requeue_request(bd->rq, true);
+ *   - drivers/block/xen-blkfront.c|2053| <<blkif_recover>> blk_mq_requeue_request(req, false);
+ *   - drivers/ide/ide-cd.c|261| <<ide_cd_breathe>> blk_mq_requeue_request(rq, false);
+ *   - drivers/ide/ide-io.c|450| <<ide_requeue_and_plug>> blk_mq_requeue_request(rq, false);
+ *   - drivers/md/dm-rq.c|191| <<dm_mq_delay_requeue_request>> blk_mq_requeue_request(rq, false);
+ *   - drivers/nvme/host/core.c|256| <<nvme_retry_req>> blk_mq_requeue_request(req, false);
+ *   - drivers/scsi/scsi_lib.c|151| <<scsi_mq_requeue_cmd>> blk_mq_requeue_request(cmd->request, true);
+ *   - drivers/scsi/scsi_lib.c|190| <<__scsi_queue_insert>> blk_mq_requeue_request(cmd->request, true);
+ */
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 {
 	__blk_mq_requeue_request(rq);
@@ -807,6 +1146,21 @@ void blk_mq_delay_kick_requeue_list(struct request_queue *q,
 }
 EXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|4307| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
+ *   - block/blk-mq.c|4309| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
+ *   - drivers/block/nbd.c|656| <<nbd_read_stat>> req = blk_mq_tag_to_rq(nbd->tag_set.tags[hwq],
+ *   - drivers/nvme/host/pci.c|1458| <<nvme_handle_cqe>> req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+ *   - drivers/nvme/host/rdma.c|1428| <<nvme_rdma_process_nvme_rsp>> rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|431| <<nvme_tcp_process_nvme_cqe>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), cqe->command_id);
+ *   - drivers/nvme/host/tcp.c|450| <<nvme_tcp_handle_c2h_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|554| <<nvme_tcp_handle_r2t>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|644| <<nvme_tcp_recv_data>> rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+ *   - drivers/nvme/host/tcp.c|741| <<nvme_tcp_recv_ddgst>> struct request *rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue),
+ *   - drivers/nvme/target/loop.c|112| <<nvme_loop_queue_response>> rq = blk_mq_tag_to_rq(nvme_loop_tagset(queue), cqe->command_id);
+ *   - include/scsi/scsi_tcq.h|33| <<scsi_host_find_tag>> req = blk_mq_tag_to_rq(shost->tag_set.tags[hwq],
+ */
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 {
 	if (tag < tags->nr_tags) {
@@ -879,6 +1233,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1196| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -917,6 +1275,10 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|3598| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -984,6 +1346,12 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
  * Process software queues that have been marked busy, splicing them
  * to the for-dispatch
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|253| <<blk_mq_sched_dispatch_requests>> blk_mq_flush_busy_ctxs(hctx, &rq_list);
+ *
+ * 把hctx所属的ctx的ctx->rq_lists的request放入参数的list
+ */
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 {
 	struct flush_busy_ctx_data data = {
@@ -1000,6 +1368,10 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1163| <<blk_mq_dequeue_from_ctx>> dispatch_rq_from_ctx, &data);
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1020,6 +1392,12 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|193| <<blk_mq_do_dispatch_ctx>> rq = blk_mq_dequeue_from_ctx(hctx, ctx);
+ *
+ * 从hctx所属的某一个ctx的ctx->rq_lists获得一个request
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1043,6 +1421,14 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1256| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1282| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1362| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1394| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|2044| <<__blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
 	struct blk_mq_alloc_data data = {
@@ -1100,6 +1486,10 @@ static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
  * restart. For both cases, take care to check the condition again after
  * marking us as waiting.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1321| <<blk_mq_dispatch_rq_list>> if (!blk_mq_mark_tag_wait(hctx, rq)) {
+ */
 static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 				 struct request *rq)
 {
@@ -1173,6 +1563,14 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
  * - take 4 as factor for avoiding to get too small(0) result, and this
  *   factor doesn't matter because EWMA decreases exponentially
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1615| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|1618| <<blk_mq_dispatch_rq_list>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|2211| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ *   - block/blk-mq.c|2216| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, true);
+ *   - block/blk-mq.c|2220| <<__blk_mq_issue_directly>> blk_mq_update_dispatch_busy(hctx, false);
+ */
 static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 {
 	unsigned int ewma;
@@ -1198,6 +1596,17 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|138| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|193| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|241| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|254| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 核心思想是为list的request调用queue_rq()
+ * 如果有下发不了的放入hctx->dispatch
+ * 根据情况也许会直接或者间接blk_mq_run_hw_queue()
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1299,6 +1708,18 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 		 * the driver there was more coming, but that turned out to
 		 * be a lie.
 		 */
+		/*
+		 * 在以下设置commit_rqs:
+		 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+		 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+		 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+		 * 
+		 * 在以下调用commit_rqs:
+		 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+		 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+		 *
+		 * 应该都是kick了ring buffer
+		 */
 		if (q->mq_ops->commit_rqs)
 			q->mq_ops->commit_rqs(hctx);
 
@@ -1352,6 +1773,15 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1546| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1739| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1387,9 +1817,25 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
+	/*
+	 * 设置BLK_MQ_F_BLOCKING的地方:
+	 *   - block/bsg-lib.c|381| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/nbd.c|1586| <<nbd_dev_add>> BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1560| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|911| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|782| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|786| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|418| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|434| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * 核心思想优先下发hctx->dispatch
+	 * 如果有scheduler就下发ctx中scheduler中的
+	 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+	 */
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
 }
@@ -1449,6 +1895,17 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1614| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1653| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ *
+ * 根据参数async决定是同步还是异步执行下面的:
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1458,6 +1915,11 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		int cpu = get_cpu();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+			/*
+			 * 核心思想优先下发hctx->dispatch
+			 * 如果有scheduler就下发ctx中scheduler中的
+			 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+			 */
 			__blk_mq_run_hw_queue(hctx);
 			put_cpu();
 			return;
@@ -1466,16 +1928,55 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * run_work是blk_mq_run_work_fn()
+	 *
+	 * blk_mq_run_work_fn()
+	 *   -> __blk_mq_run_hw_queue()
+	 *
+	 * __blk_mq_run_hw_queue()的实现:
+	 * 核心思想优先下发hctx->dispatch
+	 * 如果有scheduler就下发ctx中scheduler中的
+	 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * 异步执行下面:
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|98| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|476| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|515| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|214| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1223| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1654| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1729| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1764| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1843| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2152| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2433| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ *
+ * 根据参数async决定是同步还是异步执行下面的:
+ *
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1495,6 +1996,13 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 	hctx_unlock(hctx, srcu_idx);
 
 	if (need_run) {
+		/*
+		 * 根据参数async决定是同步还是异步执行下面的:
+		 *
+		 * 核心思想优先下发hctx->dispatch
+		 * 如果有scheduler就下发ctx中scheduler中的
+		 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+		 */
 		__blk_mq_delay_run_hw_queue(hctx, async, 0);
 		return true;
 	}
@@ -1503,12 +2011,27 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL(blk_mq_run_hw_queue);
 
+/*
+ * 部分调用的例子:
+ *   - block/bfq-iosched.c|424| <<bfq_schedule_dispatch>> blk_mq_run_hw_queues(bfqd->queue, true);
+ *   - block/blk-mq-debugfs.c|164| <<queue_state_write>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|210| <<blk_freeze_queue_start>> blk_mq_run_hw_queues(q, false);
+ *   - block/blk-mq.c|327| <<blk_mq_unquiesce_queue>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|894| <<blk_mq_requeue_work>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/md/dm-table.c|2123| <<dm_table_run_md_queue_async>> blk_mq_run_hw_queues(queue, true);
+ *   - drivers/scsi/scsi_lib.c|350| <<scsi_kick_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|511| <<scsi_run_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|611| <<scsi_end_request>> blk_mq_run_hw_queues(q, true);
+ *   - drivers/scsi/scsi_sysfs.c|780| <<store_state_field>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_transport_fc.c|3678| <<fc_bsg_goose_queue>> blk_mq_run_hw_queues(q, true);
+ */
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/* 检测hctx->state的BLK_MQ_S_STOPPED是否设置了 */
 		if (blk_mq_hctx_stopped(hctx))
 			continue;
 
@@ -1550,6 +2073,16 @@ void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	cancel_delayed_work(&hctx->run_work);
 
+	/*
+	 * 用在hctx->state:
+	 *   - block/blk-mq.c|1553| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1578| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1599| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1623| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *
+	 * 在io completion的时候状态可能被清空
+	 */
 	set_bit(BLK_MQ_S_STOPPED, &hctx->state);
 }
 EXPORT_SYMBOL(blk_mq_stop_hw_queue);
@@ -1563,6 +2096,12 @@ EXPORT_SYMBOL(blk_mq_stop_hw_queue);
  * after blk_mq_stop_hw_queues() returns. Please use
  * blk_mq_quiesce_queue() for that requirement.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/null_blk_main.c|1124| <<null_stop_queue>> blk_mq_stop_hw_queues(q);
+ *   - drivers/block/xen-blkfront.c|1192| <<xlvbd_release_gendisk>> blk_mq_stop_hw_queues(info->rq);
+ *   - drivers/block/xen-blkfront.c|1351| <<blkif_free>> blk_mq_stop_hw_queues(info->rq);
+ */
 void blk_mq_stop_hw_queues(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1573,6 +2112,10 @@ void blk_mq_stop_hw_queues(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_mq_stop_hw_queues);
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|1590| <<blk_mq_start_hw_queues>> blk_mq_start_hw_queue(hctx);
+ */
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -1581,6 +2124,17 @@ void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL(blk_mq_start_hw_queue);
 
+/*
+ * called by:
+ *   - drivers/block/skd_main.c|712| <<skd_start_queue>> blk_mq_start_hw_queues(skdev->queue);
+ *   - drivers/block/sx8.c|692| <<carm_round_robin>> blk_mq_start_hw_queues(q);
+ *   - drivers/ide/ide-pm.c|259| <<ide_check_pm_state>> blk_mq_start_hw_queues(q);
+ *   - drivers/memstick/core/ms_block.c|2082| <<msb_start>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/ms_block.c|2212| <<msb_remove>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/mspro_block.c|822| <<mspro_block_start>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/mspro_block.c|1329| <<mspro_block_remove>> blk_mq_start_hw_queues(msb->queue);
+ *   - drivers/memstick/core/mspro_block.c|1412| <<mspro_block_resume>> blk_mq_start_hw_queues(msb->queue);
+ */
 void blk_mq_start_hw_queues(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1591,6 +2145,10 @@ void blk_mq_start_hw_queues(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_mq_start_hw_queues);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1610| <<blk_mq_start_stopped_hw_queues>> blk_mq_start_stopped_hw_queue(hctx, async);
+ */
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	if (!blk_mq_hctx_stopped(hctx))
@@ -1601,6 +2159,15 @@ void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL_GPL(blk_mq_start_stopped_hw_queue);
 
+/*
+ * 调用的例子:
+ *   - block/blk-mq-debugfs.c|166| <<queue_state_write>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/null_blk_main.c|1132| <<null_restart_queue_async>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/virtio_blk.c|253| <<virtblk_done>> blk_mq_start_stopped_hw_queues(vblk->disk->queue, true);
+ *   - drivers/block/xen-blkfront.c|1222| <<kick_pending_request_queues_locked>> blk_mq_start_stopped_hw_queues(rinfo->dev_info->rq, true);
+ *   - drivers/block/xen-blkfront.c|2055| <<blkif_recover>> blk_mq_start_stopped_hw_queues(info->rq, true);
+ *   - drivers/ide/ide-pm.c|67| <<generic_ide_resume>> blk_mq_start_stopped_hw_queues(drive->queue, true);
+ */
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1611,6 +2178,11 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * 核心思想优先下发hctx->dispatch
+ * 如果有scheduler就下发ctx中scheduler中的
+ * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1623,9 +2195,17 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 	if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
 		return;
 
+	/*
+	 * 核心思想优先下发hctx->dispatch
+	 * 如果有scheduler就下发ctx中scheduler中的
+	 * 否则根据情况下发部分(budget)或全部hctx所属ctx的request
+	 */
 	__blk_mq_run_hw_queue(hctx);
 }
 
+/*
+ * 把request放入ctx->rq_lists的头或者尾
+ */
 static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    bool at_head)
@@ -1643,6 +2223,13 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|504| <<blk_mq_sched_insert_request>> __blk_mq_insert_request(hctx, rq, at_head);
+ *
+ * 把request放入ctx->rq_lists的头或者尾
+ * 把ctx在hctx中对应的sbitmap设置上
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1650,6 +2237,7 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/* 把request放入ctx->rq_lists的头或者尾 */
 	__blk_mq_insert_req_list(hctx, rq, at_head);
 	blk_mq_hctx_mark_pending(hctx, ctx);
 }
@@ -1658,11 +2246,31 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|400| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|936| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|2196| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|2217| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|2260| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ *
+ * 把request放入hctx->dispatch, 如果参数run_queue则blk_mq_run_hw_queue(hctx, false)
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
 
 	spin_lock(&hctx->lock);
+	/*
+	 * 往hctx->dispatch添加新元素的地方:
+	 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * 从hctx->dispatch移除元素的地方(下发):
+	 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 */
 	list_add_tail(&rq->queuelist, &hctx->dispatch);
 	spin_unlock(&hctx->lock);
 
@@ -1670,6 +2278,13 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|546| <<blk_mq_sched_insert_requests>> blk_mq_insert_requests(hctx, ctx, list);
+ * 
+ * 把参数list中的request放入ctx->rq_lists
+ * 并且标记ctx在hctx上pending(blk_mq_hctx_mark_pending)
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1709,6 +2324,10 @@ static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
 	return blk_rq_pos(rqa) > blk_rq_pos(rqb);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1717| <<blk_flush_plug_list>> blk_mq_flush_plug_list(plug, from_schedule);
+ */
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct blk_mq_hw_ctx *this_hctx;
@@ -1764,6 +2383,14 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2641| <<blk_mq_make_request>> blk_mq_bio_to_request(rq, bio);
+ *   - block/blk-mq.c|2669| <<blk_mq_make_request>> blk_mq_bio_to_request(rq, bio);
+ *   - block/blk-mq.c|2692| <<blk_mq_make_request>> blk_mq_bio_to_request(rq, bio);
+ *   - block/blk-mq.c|2730| <<blk_mq_make_request>> blk_mq_bio_to_request(rq, bio);
+ *   - block/blk-mq.c|2740| <<blk_mq_make_request>> blk_mq_bio_to_request(rq, bio);
+ */
 static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 {
 	blk_init_request_from_bio(rq, bio);
@@ -1771,6 +2398,12 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2101| <<__blk_mq_try_issue_directly>> return __blk_mq_issue_directly(hctx, rq, cookie, last);
+ *
+ * 通过queue_rq()把request下发
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1810,6 +2443,14 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2120| <<blk_mq_try_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+ *   - block/blk-mq.c|2137| <<blk_mq_request_issue_directly>> ret = __blk_mq_try_issue_directly(hctx, rq, &unused_cookie, true, last);
+ *
+ * 核心思想是通过queue_rq()把request下发
+ * 如果没有tag还要用blk_mq_get_driver_tag()分配
+ */
 static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1842,6 +2483,9 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		goto insert;
 	}
 
+	/*
+	 * 通过queue_rq()把request下发
+	 */
 	return __blk_mq_issue_directly(hctx, rq, cookie, last);
 insert:
 	if (bypass_insert)
@@ -1851,6 +2495,16 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2312| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2319| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ *
+ * 核心思想是通过queue_rq()把request下发
+ * 如果没有tag还要用blk_mq_get_driver_tag()分配
+ * 如果失败要么把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+ * 要么blk_mq_end_request(rq, ret)
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1861,7 +2515,15 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 
 	hctx_lock(hctx, &srcu_idx);
 
+	/*
+	 * 核心思想是通过queue_rq()把request下发
+	 * 如果没有tag还要用blk_mq_get_driver_tag()分配
+	 */
 	ret = __blk_mq_try_issue_directly(hctx, rq, cookie, false, true);
+	/*
+	 * blk_mq_request_bypass_insert():
+	 * 把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+	 */
 	if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE)
 		blk_mq_request_bypass_insert(rq, true);
 	else if (ret != BLK_STS_OK)
@@ -1870,6 +2532,11 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1198| <<blk_insert_cloned_request>> return blk_mq_request_issue_directly(rq, true);
+ *   - block/blk-mq.c|2152| <<blk_mq_try_issue_list_directly>> ret = blk_mq_request_issue_directly(rq, list_empty(list));
+ */
 blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 {
 	blk_status_t ret;
@@ -1884,6 +2551,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|542| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1910,14 +2581,38 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 	 * the driver there was more coming, but that turned out to
 	 * be a lie.
 	 */
+	/*
+	 * 在以下设置commit_rqs:
+	 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+	 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+	 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+	 * 
+	 * 在以下调用commit_rqs:
+	 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+	 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+	 *
+	 * 应该都是kick了ring buffer
+	 */
 	if (!list_empty(list) && hctx->queue->mq_ops->commit_rqs)
 		hctx->queue->mq_ops->commit_rqs(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2287| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *   - block/blk-mq.c|2304| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *
+ * 核心思想是把request放入plug->mq_list
+ */
 static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 {
 	list_add_tail(&rq->queuelist, &plug->mq_list);
 	plug->rq_count++;
+	/*
+	 * 设置multiple_queues的地方:
+	 *   - block/blk-core.c|1668| <<blk_start_plug>> plug->multiple_queues = false;
+	 *   - block/blk-mq.c|2337| <<blk_add_rq_to_plug>> plug->multiple_queues = true;
+	 */
 	if (!plug->multiple_queues && !list_is_singular(&plug->mq_list)) {
 		struct request *tmp;
 
@@ -1928,6 +2623,11 @@ static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1081| <<generic_make_request>> ret = q->make_request_fn(q, bio);
+ *   - block/blk-core.c|1148| <<direct_make_request>> ret = q->make_request_fn(q, bio);
+ */
 static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 {
 	const int is_sync = op_is_sync(bio->bi_opf);
@@ -1940,15 +2640,33 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	blk_queue_bounce(q, &bio);
 
+	/*
+	 * 是否超过了预设最大处理大小,若是则进行拆分,拆分后会进行generic_make_request函数调用
+	 */
 	blk_queue_split(q, &bio);
 
 	if (!bio_integrity_prep(bio))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * blk_queue_nomerges():
+	 * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+	 *
+	 * blk_attempt_plug_merge():
+	 * try to merge with %current's plugged list
+	 * 成功返回true
+	 *
+	 * 这里的意思是, 如果request不是flush并且q->queue_flags没有设置QUEUE_FLAG_NOMERGES
+	 * 则试着放入current的plugged list
+	 */
 	if (!is_flush_fua && !blk_queue_nomerges(q) &&
 	    blk_attempt_plug_merge(q, bio, &same_queue_rq))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * 如果支持scheduler就把bio用ops.bio_merge给merge了
+	 * 否则尝试把bio给merge到ctx->rq_lists
+	 */
 	if (blk_mq_sched_bio_merge(q, bio))
 		return BLK_QC_T_NONE;
 
@@ -1979,6 +2697,20 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 		blk_mq_run_hw_queue(data.hctx, true);
 	} else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs)) {
 		/*
+		 * 为什么用q->mq_ops->commit_rqs???
+		 *
+		 * 在以下设置commit_rqs:
+		 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+		 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+		 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+		 * 
+		 * 在以下调用commit_rqs:
+		 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+		 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+		 *
+		 * 应该都是kick了ring buffer
+		 */
+		/*
 		 * Use plugging if we have a ->commit_rqs() hook as well, as
 		 * we know the driver uses bd->last in a smart fashion.
 		 */
@@ -1999,8 +2731,16 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			trace_block_plug(q);
 		}
 
+		/*
+		 * 核心思想是把request放入plug->mq_list
+		 */
 		blk_add_rq_to_plug(plug, rq);
 	} else if (plug && !blk_queue_nomerges(q)) {
+		/*
+		 * blk_queue_nomerges():
+		 * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+		 */
+
 		blk_mq_bio_to_request(rq, bio);
 
 		/*
@@ -2016,6 +2756,9 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			list_del_init(&same_queue_rq->queuelist);
 			plug->rq_count--;
 		}
+		/*
+		 * 核心思想是把request放入plug->mq_list
+		 */
 		blk_add_rq_to_plug(plug, rq);
 		trace_block_plug(q);
 
@@ -2024,6 +2767,12 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 		if (same_queue_rq) {
 			data.hctx = same_queue_rq->mq_hctx;
 			trace_block_unplug(q, 1, true);
+			/*
+			 * 核心思想是通过queue_rq()把same_queue_rq下发
+			 * 如果没有tag还要用blk_mq_get_driver_tag()分配
+			 * 如果失败要么把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+			 * 要么blk_mq_end_request(rq, ret)
+			 */
 			blk_mq_try_issue_directly(data.hctx, same_queue_rq,
 					&cookie);
 		}
@@ -2031,10 +2780,26 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			!data.hctx->dispatch_busy)) {
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 核心思想是通过queue_rq()把request下发
+		 * 如果没有tag还要用blk_mq_get_driver_tag()分配
+		 * 如果失败要么把request放入hctx->dispatch, 调用blk_mq_run_hw_queue(hctx, false)
+		 * 要么blk_mq_end_request(rq, ret)
+		 */
 		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
 	} else {
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 1. 如果没有RQF_FLUSH_SEQ但是是flush则blk_insert_flush()
+		 * 2. 试试能否blk_mq_sched_bypass_insert(): 比如是否有RQF_FLUSH_SEQ
+		 * 3. 如果支持IO调度则用scheduler的.insert_requests()
+		 * 4. 否则把request放入request->mq_ctx的rq_lists, 然后把ctx在hctx->ctx_map对应的bit设置
+		 * 调用blk_mq_run_hw_queue()! 用async!!!
+		 *                         
+		 * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+		 * 调用blk_mq_run_hw_queue()! 用async!!!
+		 */
 		blk_mq_sched_insert_request(rq, false, true, true);
 	}
 
@@ -2081,6 +2846,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|466| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|533| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2486| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2123,6 +2894,11 @@ static size_t order_to_size(unsigned int order)
 	return (size_t)PAGE_SIZE << order;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2517| <<blk_mq_alloc_rqs>> if (blk_mq_init_request(set, rq, hctx_idx, node)) {
+ *   - block/blk-mq.c|2636| <<blk_mq_init_hctx>> if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,
+ */
 static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 			       unsigned int hctx_idx, int node)
 {
@@ -2138,6 +2914,12 @@ static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|551| <<blk_mq_sched_alloc_tags>> ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
+ *   - block/blk-mq-tag.c|630| <<blk_mq_tag_update_depth>> ret = blk_mq_alloc_rqs(set, new, hctx->queue_num, tdepth);
+ *   - block/blk-mq.c|2560| <<__blk_mq_alloc_rq_map>> ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
+ */
 int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx, unsigned int depth)
 {
@@ -2331,6 +3113,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3090| <<blk_mq_alloc_and_init_hctx>> hctx = blk_mq_alloc_hctx(q, set, node);
+ */
 static struct blk_mq_hw_ctx *
 blk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,
 		int node)
@@ -2458,6 +3244,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3743| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|4157| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2471,8 +3262,37 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	mutex_lock(&q->sysfs_lock);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * cpumask使用的地方:
+		 *   - block/blk-mq-sysfs.c|45| <<blk_mq_hw_sysfs_release>> free_cpumask_var(hctx->cpumask);
+		 *   - block/blk-mq-sysfs.c|172| <<blk_mq_hw_sysfs_cpus_show>> for_each_cpu(i, hctx->cpumask) {
+		 *   - block/blk-mq.c|647| <<blk_mq_alloc_request_hctx>> cpu = cpumask_first_and(alloc_data.hctx->cpumask, cpu_online_mask);
+		 *   - block/blk-mq.c|1777| <<__blk_mq_run_hw_queue>> if (!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask) &&
+		 *   - block/blk-mq.c|1781| <<__blk_mq_run_hw_queue>> cpumask_empty(hctx->cpumask) ? "inactive": "active");
+		 *   - block/blk-mq.c|1816| <<blk_mq_first_mapped_cpu>> int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
+		 *   - block/blk-mq.c|1819| <<blk_mq_first_mapped_cpu>> cpu = cpumask_first(hctx->cpumask);
+		 *   - block/blk-mq.c|1839| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+		 *   - block/blk-mq.c|1888| <<__blk_mq_delay_run_hw_queue>> if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+		 *   - block/blk-mq.c|3102| <<blk_mq_alloc_hctx>> if (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))
+		 *   - block/blk-mq.c|3152| <<blk_mq_alloc_hctx>> free_cpumask_var(hctx->cpumask);
+		 *   - block/blk-mq.c|3231| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+		 *   - block/blk-mq.c|3270| <<blk_mq_map_swqueue>> if (cpumask_test_cpu(i, hctx->cpumask))
+		 *   - block/blk-mq.c|3273| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+		 */
 		cpumask_clear(hctx->cpumask);
+		/*
+		 * 在以下修改:
+		 *   - block/blk-mq.c|3130| <<blk_mq_alloc_hctx>> hctx->nr_ctx = 0;
+		 *   - block/blk-mq.c|3237| <<blk_mq_map_swqueue>> hctx->nr_ctx = 0;
+		 *   - block/blk-mq.c|3298| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+		 */
 		hctx->nr_ctx = 0;
+		/*
+		 * 在以下使用dispatch_from:
+		 *   - block/blk-mq-sched.c|199| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+		 *   - block/blk-mq-sched.c|237| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+		 *   - block/blk-mq.c|3238| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+		 */
 		hctx->dispatch_from = NULL;
 	}
 
@@ -2513,6 +3333,23 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			if (cpumask_test_cpu(i, hctx->cpumask))
 				continue;
 
+			/*
+			 * cpumask使用的地方:
+			 *   - block/blk-mq-sysfs.c|45| <<blk_mq_hw_sysfs_release>> free_cpumask_var(hctx->cpumask);
+			 *   - block/blk-mq-sysfs.c|172| <<blk_mq_hw_sysfs_cpus_show>> for_each_cpu(i, hctx->cpumask) {
+			 *   - block/blk-mq.c|647| <<blk_mq_alloc_request_hctx>> cpu = cpumask_first_and(alloc_data.hctx->cpumask, cpu_online_mask);
+			 *   - block/blk-mq.c|1777| <<__blk_mq_run_hw_queue>> if (!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask) &&
+			 *   - block/blk-mq.c|1781| <<__blk_mq_run_hw_queue>> cpumask_empty(hctx->cpumask) ? "inactive": "active");
+			 *   - block/blk-mq.c|1816| <<blk_mq_first_mapped_cpu>> int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
+			 *   - block/blk-mq.c|1819| <<blk_mq_first_mapped_cpu>> cpu = cpumask_first(hctx->cpumask);
+			 *   - block/blk-mq.c|1839| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+			 *   - block/blk-mq.c|1888| <<__blk_mq_delay_run_hw_queue>> if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+			 *   - block/blk-mq.c|3102| <<blk_mq_alloc_hctx>> if (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))
+			 *   - block/blk-mq.c|3152| <<blk_mq_alloc_hctx>> free_cpumask_var(hctx->cpumask);
+			 *   - block/blk-mq.c|3231| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+			 *   - block/blk-mq.c|3270| <<blk_mq_map_swqueue>> if (cpumask_test_cpu(i, hctx->cpumask))
+			 *   - block/blk-mq.c|3273| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+			 */
 			cpumask_set_cpu(i, hctx->cpumask);
 			hctx->type = j;
 			ctx->index_hw[hctx->type] = hctx->nr_ctx;
@@ -2571,6 +3408,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3139| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|3175| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2598,6 +3440,10 @@ static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3734| <<blk_mq_exit_queue>> blk_mq_del_queue_tag_set(q);
+ */
 static void blk_mq_del_queue_tag_set(struct request_queue *q)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -2614,6 +3460,10 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3703| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2636,6 +3486,10 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|3424| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
 	struct blk_mq_ctxs *ctxs;
@@ -2694,6 +3548,27 @@ void blk_mq_release(struct request_queue *q)
 	blk_mq_sysfs_deinit(q);
 }
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3280| <<blk_mq_init_sq_queue>> q = blk_mq_init_queue(set);
+ *   - block/bsg-lib.c|385| <<bsg_setup_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/block/loop.c|2004| <<loop_add>> lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
+ *   - drivers/block/null_blk_main.c|1667| <<null_add_dev>> nullb->q = blk_mq_init_queue(nullb->tag_set);
+ *   - drivers/block/virtio_blk.c|806| <<virtblk_probe>> q = blk_mq_init_queue(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|986| <<xlvbd_init_blk_queue>> rq = blk_mq_init_queue(&info->tag_set);
+ *   - drivers/ide/ide-probe.c|790| <<ide_init_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/nvme/host/core.c|3253| <<nvme_alloc_ns>> ns->queue = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/fc.c|2452| <<nvme_fc_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3100| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1636| <<nvme_alloc_admin_tags>> dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|795| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|872| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/tcp.c|1644| <<nvme_tcp_configure_io_queues>> ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1696| <<nvme_tcp_configure_admin_queue>> ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|360| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|526| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1819| <<scsi_mq_alloc_queue>> sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	struct request_queue *uninit_q, *q;
@@ -2714,6 +3589,24 @@ EXPORT_SYMBOL(blk_mq_init_queue);
  * Helper for setting up a queue with mq ops, given queue depth, and
  * the passed in mq ops flags.
  */
+/*
+ * called by:
+ *   - drivers/block/amiflop.c|1783| <<fd_alloc_disk>> disk->queue = blk_mq_init_sq_queue(&unit[drive].tag_set, &amiflop_mq_ops,
+ *   - drivers/block/ataflop.c|1992| <<atari_floppy_init>> unit[i].disk->queue = blk_mq_init_sq_queue(&unit[i].tag_set,
+ *   - drivers/block/floppy.c|4530| <<do_floppy_init>> disks[drive]->queue = blk_mq_init_sq_queue(&tag_sets[drive],
+ *   - drivers/block/paride/pcd.c|314| <<pcd_init_units>> disk->queue = blk_mq_init_sq_queue(&cd->tag_set, &pcd_mq_ops,
+ *   - drivers/block/paride/pf.c|300| <<pf_init_units>> disk->queue = blk_mq_init_sq_queue(&pf->tag_set, &pf_mq_ops,
+ *   - drivers/block/ps3disk.c|444| <<ps3disk_probe>> queue = blk_mq_init_sq_queue(&priv->tag_set, &ps3disk_mq_ops, 1,
+ *   - drivers/block/sunvdc.c|794| <<init_queue>> q = blk_mq_init_sq_queue(&port->tag_set, &vdc_mq_ops, VDC_TX_RING_SIZE,
+ *   - drivers/block/swim.c|841| <<swim_floppy_init>> q = blk_mq_init_sq_queue(&swd->unit[drive].tag_set, &swim_mq_ops,
+ *   - drivers/block/swim3.c|1197| <<swim3_attach>> disk->queue = blk_mq_init_sq_queue(&fs->tag_set, &swim3_mq_ops, 2,
+ *   - drivers/block/xsysace.c|1010| <<ace_setup>> ace->queue = blk_mq_init_sq_queue(&ace->tag_set, &ace_mq_ops, 2,
+ *   - drivers/block/z2ram.c|363| <<z2_init>> z2_queue = blk_mq_init_sq_queue(&tag_set, &z2_mq_ops, 16,
+ *   - drivers/cdrom/gdrom.c|781| <<probe_gdrom>> gd.gdrom_rq = blk_mq_init_sq_queue(&gd.tag_set, &gdrom_mq_ops, 1,
+ *   - drivers/memstick/core/ms_block.c|2119| <<msb_init_disk>> msb->queue = blk_mq_init_sq_queue(&msb->tag_set, &msb_mq_ops, 2,
+ *   - drivers/memstick/core/mspro_block.c|1214| <<mspro_block_init_disk>> msb->queue = blk_mq_init_sq_queue(&msb->tag_set, &mspro_mq_ops, 2,
+ *   - drivers/mtd/mtd_blkdevs.c|433| <<add_mtd_blktrans_dev>> new->rq = blk_mq_init_sq_queue(new->tag_set, &mtd_mq_ops, 2,
+ */
 struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 					   const struct blk_mq_ops *ops,
 					   unsigned int queue_depth,
@@ -2744,6 +3637,10 @@ struct request_queue *blk_mq_init_sq_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_sq_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2987| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2778,6 +3675,11 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3067| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3479| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
@@ -2852,6 +3754,11 @@ static unsigned int nr_hw_queues(struct blk_mq_tag_set *set)
 	return max(set->nr_hw_queues, nr_cpu_ids);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3020| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q);
+ *   - drivers/md/dm-rq.c|565| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q)
 {
@@ -3034,6 +3941,26 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * 调用的几个例子:
+ *   - block/blk-mq.c|3048| <<blk_mq_init_sq_queue>> ret = blk_mq_alloc_tag_set(set);
+ *   - block/bsg-lib.c|382| <<bsg_setup_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/block/loop.c|2000| <<loop_add>> err = blk_mq_alloc_tag_set(&lo->tag_set);
+ *   - drivers/block/null_blk_main.c|1562| <<null_init_tag_set>> return blk_mq_alloc_tag_set(set);
+ *   - drivers/block/virtio_blk.c|802| <<virtblk_probe>> err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|984| <<xlvbd_init_blk_queue>> if (blk_mq_alloc_tag_set(&info->tag_set))
+ *   - drivers/ide/ide-probe.c|787| <<ide_init_queue>> if (blk_mq_alloc_tag_set(set))
+ *   - drivers/md/dm-rq.c|561| <<dm_mq_init_request_queue>> err = blk_mq_alloc_tag_set(md->tag_set);
+ *   - drivers/nvme/host/fc.c|2446| <<nvme_fc_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3095| <<nvme_fc_init_ctrl>> ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1632| <<nvme_alloc_admin_tags>> if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+ *   - drivers/nvme/host/pci.c|2277| <<nvme_dev_add>> ret = blk_mq_alloc_tag_set(&dev->tagset);
+ *   - drivers/nvme/host/rdma.c|742| <<nvme_rdma_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/host/tcp.c|1468| <<nvme_tcp_alloc_tagset>> ret = blk_mq_alloc_tag_set(set);
+ *   - drivers/nvme/target/loop.c|355| <<nvme_loop_configure_admin_queue>> error = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|522| <<nvme_loop_create_io_queues>> ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1849| <<scsi_mq_setup_tags>> return blk_mq_alloc_tag_set(&shost->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -3137,6 +4064,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3252,6 +4183,10 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|4151| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3315,6 +4250,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1166| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2525| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2285| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|878| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1650| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|468| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3333,6 +4278,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|764| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3482,11 +4431,32 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|761| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|301| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|463| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|522| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap.c|1483| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap.c|1981| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|414| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
 	long state;
 
+	/*
+	 * 使用QUEUE_FLAG_POLL的地方:
+	 *   - block/blk-core.c|912| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3690| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-mq.c|4336| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - drivers/nvme/host/core.c|781| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 */
 	if (!blk_qc_t_valid(cookie) ||
 	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
 		return 0;
@@ -3536,6 +4506,11 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * 只在以下使用:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
 	return rq->mq_ctx->cpu;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 633a5a7..22d905c 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -18,10 +18,37 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 把元素插入ctx->rq_lists的地方:
+		 *   - block/blk-mq-sched.c|359| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+		 *   - block/blk-mq.c|1814| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1816| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1863| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 *
+		 * 把元素从ctx->rq_lists移除的地方:
+		 *   - block/blk-mq.c|1103| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+		 *   - block/blk-mq.c|2421| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+		 *   - block/blk-mq.c|1143| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+		 *
+		 * 测试ctx->rq_lists是否为NULL的地方:
+		 *   - block/blk-mq-sched.c|386| <<__blk_mq_sched_bio_merge>> !list_empty_careful(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1142| <<dispatch_rq_from_ctx>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *   - block/blk-mq.c|1145| <<dispatch_rq_from_ctx>> if (list_empty(&ctx->rq_lists[type]))
+		 *   - block/blk-mq.c|2420| <<blk_mq_hctx_notify_dead>> if (!list_empty(&ctx->rq_lists[type])) {
+		 *
+		 * 其他使用ctx->rq_lists的地方:
+		 *   - block/blk-mq-debugfs.c|648| <<CTX_RQ_SEQ_OPS>> return seq_list_start(&ctx->rq_lists[type], *pos); \
+		 *   - block/blk-mq-debugfs.c|656| <<CTX_RQ_SEQ_OPS>> return seq_list_next(v, &ctx->rq_lists[type], pos); \
+		 *   - block/blk-mq.c|2601| <<blk_mq_init_cpu_queues>> INIT_LIST_HEAD(&__ctx->rq_lists[k]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * 设置index_hw的地方:
+	 *   - block/blk-mq.c|3326| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -30,6 +57,12 @@ struct blk_mq_ctx {
 	unsigned long		rq_merged;
 
 	/* incremented at completion time */
+	/*
+	 * 在以下修改或者使用ctx->rq_completed:
+	 *   - block/blk-mq-debugfs.c|716| <<ctx_completed_show>> seq_printf(m, "%lu %lu\n", ctx->rq_completed[1], ctx->rq_completed[0]);
+	 *   - block/blk-mq-debugfs.c|725| <<ctx_completed_write>> ctx->rq_completed[0] = ctx->rq_completed[1] = 0;
+	 *   - block/blk-mq.c|715| <<blk_mq_free_request>> ctx->rq_completed[rq_is_sync(rq)]++;
+	 */
 	unsigned long		____cacheline_aligned_in_smp rq_completed[2];
 
 	struct request_queue	*queue;
@@ -99,6 +132,13 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *
  * @flags: request command flags
  * @cpu: cpu ctx
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|503| <<__blk_mq_sched_bio_merge>> struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, bio->bi_opf, ctx);
+ *   - block/blk-mq-tag.c|239| <<blk_mq_get_tag>> data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
+ *   - block/blk-mq.c|538| <<blk_mq_get_request>> data->hctx = blk_mq_map_queue(q, data->cmd_flags,
+ *   - block/blk.h|42| <<blk_get_flush_queue>> return blk_mq_map_queue(q, REQ_OP_FLUSH, ctx)->fq;
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 						     unsigned int flags,
 						     struct blk_mq_ctx *ctx)
@@ -171,6 +211,15 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|173| <<blk_mq_get_tag>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq-tag.c|237| <<blk_mq_get_tag>> tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq.c|318| <<blk_mq_rq_ctx_init>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *
+ * 如果data->flags设置了BLK_MQ_REQ_INTERNAL, 返回data->hctx->sched_tags
+ * 否则返回data->hctx->tags
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
@@ -179,6 +228,17 @@ static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data
 	return data->hctx->tags;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|178| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1460| <<__blk_mq_delay_run_hw_queue>> if (unlikely(blk_mq_hctx_stopped(hctx)))
+ *   - block/blk-mq.c|1517| <<blk_mq_run_hw_queues>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1538| <<blk_mq_queue_stopped>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1620| <<blk_mq_start_stopped_hw_queue>> if (!blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1861| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *
+ * 检测hctx->state的BLK_MQ_S_STOPPED是否设置了
+ */
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -222,6 +282,13 @@ static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|243| <<flush_end_io>> blk_mq_put_driver_tag(flush_rq);
+ *   - block/blk-flush.c|367| <<mq_flush_data_end_io>> blk_mq_put_driver_tag(rq);
+ *   - block/blk-mq.c|955| <<__blk_mq_requeue_request>> blk_mq_put_driver_tag(rq);
+ *   - block/blk-mq.c|1585| <<blk_mq_dispatch_rq_list>> blk_mq_put_driver_tag(nxt);
+ */
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	if (rq->tag == -1 || rq->internal_tag == -1)
@@ -230,6 +297,11 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|45| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3072| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 2ae348c..0749bf7 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -302,6 +302,12 @@ EXPORT_SYMBOL_GPL(blk_queue_max_discard_segments);
  *    Enables a low level driver to set an upper limit on the size of a
  *    coalesced segment
  **/
+/*
+ * 部分调用的例子:
+ *   - drivers/block/virtio_blk.c|1102| <<virtblk_probe>> blk_queue_max_segment_size(q, max_size);
+ *   - drivers/block/xen-blkfront.c|951| <<blkif_set_queue_limits>> blk_queue_max_segment_size(rq, PAGE_SIZE);
+ *   - drivers/scsi/scsi_lib.c|1787| <<__scsi_init_queue>> blk_queue_max_segment_size(q, shost->max_segment_size);
+ */
 void blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)
 {
 	if (max_size < PAGE_SIZE) {
@@ -742,6 +748,17 @@ EXPORT_SYMBOL(blk_queue_segment_boundary);
  * @q:  the request queue for the device
  * @mask:  the memory boundary mask
  **/
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iscsi_iser.c|978| <<iscsi_iser_slave_alloc>> blk_queue_virt_boundary(sdev->request_queue, ~MASK_4K);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|3074| <<srp_slave_alloc>> blk_queue_virt_boundary(sdev->request_queue,
+ *   - drivers/nvme/host/core.c|2029| <<nvme_set_queue_limits>> blk_queue_virt_boundary(q, ctrl->page_size - 1);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1902| <<megasas_set_nvme_device_properties>> blk_queue_virt_boundary(sdev->request_queue, mr_nvme_pg_size - 1);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2370| <<scsih_slave_configure>> blk_queue_virt_boundary(sdev->request_queue,
+ *   - drivers/scsi/storvsc_drv.c|1426| <<storvsc_device_configure>> blk_queue_virt_boundary(sdevice->request_queue, PAGE_SIZE - 1);
+ *   - drivers/usb/storage/scsiglue.c|84| <<slave_alloc>> blk_queue_virt_boundary(sdev->request_queue, maxp - 1);
+ *   - drivers/usb/storage/uas.c|813| <<uas_slave_alloc>> blk_queue_virt_boundary(sdev->request_queue, maxp - 1);
+ */
 void blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)
 {
 	q->limits.virt_boundary_mask = mask;
@@ -816,6 +833,22 @@ EXPORT_SYMBOL(blk_set_queue_depth);
  *
  * Tell the block layer about the write cache of @q.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/loop.c|1001| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/null_blk_main.c|1692| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - drivers/block/virtio_blk.c|732| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/md/dm-table.c|1911| <<dm_table_set_restrictions>> blk_queue_write_cache(q, wc, fua);
+ *   - drivers/md/md.c|5331| <<md_alloc>> blk_queue_write_cache(mddev->queue, true, true);
+ *   - drivers/nvme/host/core.c|2001| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|323| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/scsi/sd.c|154| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ *
+ * 如果wc(第二个参数)是true, 则为q->queue_flags设置QUEUE_FLAG_WC
+ * 如果fua(第三个参数)是true, 则为q->queue_flags设置QUEUE_FLAG_FUA
+ */
 void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
 {
 	if (wc)
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 940f15d..d0dc624 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -98,6 +98,11 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2911| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|829| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index 977c659..4df8b99 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -933,6 +933,11 @@ struct kobj_type blk_queue_ktype = {
  * blk_register_queue - register a block layer queue with sysfs
  * @disk: Disk of which the request queue should be registered with sysfs.
  */
+/*
+ * called by:
+ *   - block/genhd.c|738| <<__device_add_disk>> blk_register_queue(disk);
+ *   - drivers/md/dm.c|2302| <<dm_setup_md_queue>> blk_register_queue(md->disk);
+ */
 int blk_register_queue(struct gendisk *disk)
 {
 	int ret;
diff --git a/block/blk.h b/block/blk.h
index 7814aa2..1a10f14 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -66,6 +66,13 @@ static inline void blk_queue_enter_live(struct request_queue *q)
 	percpu_ref_get(&q->q_usage_counter);
 }
 
+/*
+ * called by:
+ *   - block/blk-integrity.c|38| <<blk_rq_count_integrity_sg>> if (!biovec_phys_mergeable(q, &ivprv, &iv))
+ *   - block/blk-integrity.c|80| <<blk_rq_map_integrity_sg>> if (!biovec_phys_mergeable(q, &ivprv, &iv))
+ *   - block/blk-merge.c|51| <<bio_will_gap>> if (biovec_phys_mergeable(q, &pb, &nb))
+ *   - block/blk-merge.c|483| <<__blk_segment_map_sg_merge>> if (!biovec_phys_mergeable(q, bvprv, bvec))
+ */
 static inline bool biovec_phys_mergeable(struct request_queue *q,
 		struct bio_vec *vec1, struct bio_vec *vec2)
 {
@@ -82,9 +89,17 @@ static inline bool biovec_phys_mergeable(struct request_queue *q,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|53| <<bio_will_gap>> return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
+ *   - block/blk.h|109| <<bvec_gap_to_prev>> return __bvec_gap_to_prev(q, bprv, offset);
+ */
 static inline bool __bvec_gap_to_prev(struct request_queue *q,
 		struct bio_vec *bprv, unsigned int offset)
 {
+	/*
+	 * 比如offset是下一个bvec的offset, bprv是上一个的bvec
+	 */
 	return (offset & queue_virt_boundary(q)) ||
 		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
 }
@@ -93,6 +108,14 @@ static inline bool __bvec_gap_to_prev(struct request_queue *q,
  * Check if adding a bio_vec after bprv with offset would create a gap in
  * the SG list. Most drivers don't care about this, but some do.
  */
+/*
+ * called by:
+ *   - block/bio-integrity.c|134| <<bio_integrity_add_page>> bvec_gap_to_prev(bio->bi_disk->queue,
+ *   - block/bio.c|843| <<__bio_add_pc_page>> if (bvec_gap_to_prev(q, bvec, offset))
+ *   - block/blk-merge.c|224| <<blk_bio_segment_split>> if (bvprvp && bvec_gap_to_prev(q, bvprvp, bv.bv_offset))
+ *   - block/blk.h|120| <<integrity_req_gap_back_merge>> return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+ *   - block/blk.h|130| <<integrity_req_gap_front_merge>> return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+ */
 static inline bool bvec_gap_to_prev(struct request_queue *q,
 		struct bio_vec *bprv, unsigned int offset)
 {
diff --git a/block/bounce.c b/block/bounce.c
index f8ed677..b5be4dc 100644
--- a/block/bounce.c
+++ b/block/bounce.c
@@ -357,6 +357,11 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	*bio_orig = bio;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|22| <<blk_rq_append_bio>> blk_queue_bounce(rq->q, bio);
+ *   - block/blk-mq.c|2244| <<blk_mq_make_request>> blk_queue_bounce(q, &bio);
+ */
 void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 {
 	mempool_t *pool;
diff --git a/block/genhd.c b/block/genhd.c
index 24654e1..ac36f5e 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -25,7 +25,43 @@
 
 #include "blk.h"
 
+/*
+ * struct gendisk
+ *  -> struct disk_part_tbl __rcu *part_tbl
+ *      -> struct hd_struct part0
+ *          -> sector_t start_sect;
+ *          -> sector_t nr_sects;
+ *          -> struct device __dev;
+ *              -> struct device *parent;
+ *          -> int partno;
+ *          -> struct disk_stats __percpu *dkstats;
+ *
+ * struct disk_part_tbl
+ *  -> int len;
+ *  -> struct hd_struct __rcu *part[];
+ *
+ * struct hd_struct
+ *  -> sector_t start_sect;
+ *  -> sector_t nr_sects;
+ *  -> struct device __dev;
+ *      -> struct device *parent; (vda1的parent指向gendisk.part0.__dev)
+ *  -> int partno;
+ *  -> struct disk_stats __percpu *dkstats;
+ *
+ *
+ * 初始化的函数
+ * genhd_device_init()
+ */
+
 static DEFINE_MUTEX(block_class_lock);
+/*
+ * used by:
+ *   - block/genhd.c|752| <<register_disk>> err = sysfs_create_link(block_depr, &ddev->kobj,
+ *   - block/genhd.c|996| <<del_gendisk>> sysfs_remove_link(block_depr, dev_name(disk_to_dev(disk)));
+ *   - block/genhd.c|1308| <<genhd_device_init>> block_depr = kobject_create_and_add("block", NULL);
+ *
+ * 似乎指/sys/block ???
+ */
 struct kobject *block_depr;
 
 /* for extended dynamic devt allocation, currently only one major is used */
@@ -35,8 +71,23 @@ struct kobject *block_depr;
  * results from going away underneath its user.
  */
 static DEFINE_SPINLOCK(ext_devt_lock);
+/*
+ * used by:
+ *   - block/genhd.c|608| <<blk_alloc_devt>> idx = idr_alloc(&ext_devt_idr, part, 0, NR_EXT_DEVT, GFP_NOWAIT);
+ *   - block/genhd.c|635| <<blk_free_devt>> idr_remove(&ext_devt_idr, blk_mangle_minor(MINOR(devt)));
+ *   - block/genhd.c|647| <<blk_invalidate_devt>> idr_replace(&ext_devt_idr, NULL, blk_mangle_minor(MINOR(devt)));
+ *   - block/genhd.c|1042| <<get_gendisk>> part = idr_find(&ext_devt_idr, blk_mangle_minor(MINOR(devt)));
+ */
 static DEFINE_IDR(ext_devt_idr);
 
+/*
+ * 这里只是声明, 下面是具体的实现:
+ *   - block/genhd.c|1600| <<global>> static const struct device_type disk_type = {
+ *   - block/genhd.c|1139| <<printk_all_partitions>> class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1196| <<disk_seqf_start>> class_dev_iter_init(iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1685| <<blk_lookup_devt>> class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1771| <<__alloc_disk_node>> disk_to_dev(disk)->type = &disk_type;
+ */
 static const struct device_type disk_type;
 
 static void disk_check_events(struct disk_events *ev,
@@ -46,16 +97,40 @@ static void disk_add_events(struct gendisk *disk);
 static void disk_del_events(struct gendisk *disk);
 static void disk_release_events(struct gendisk *disk);
 
+/*
+ * called by:
+ *   - block/bio.c|1754| <<generic_start_io_acct>> part_inc_in_flight(q, part, op_is_write(op));
+ *   - block/blk-core.c|1350| <<blk_account_io_start>> part_inc_in_flight(rq->q, part, rw);
+ *
+ * 不适用mq
+ * 增加parition的hd_struct->dkstats[cpu]->in_flight[rw]
+ * 如果不是partition 0, 还要增加partition 0的
+ */
 void part_inc_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
 {
 	if (queue_is_mq(q))
 		return;
 
+	/*
+	 * struct hd_struct
+	 *   struct disk_stats __percpu *dkstats;
+	 *     local_t in_flight[2];
+	 */
 	part_stat_local_inc(part, in_flight[rw]);
 	if (part->partno)
 		part_stat_local_inc(&part_to_disk(part)->part0, in_flight[rw]);
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1772| <<generic_end_io_acct>> part_dec_in_flight(q, part, op_is_write(req_op));
+ *   - block/blk-core.c|1308| <<blk_account_io_done>> part_dec_in_flight(req->q, part, rq_data_dir(req));
+ *   - block/blk-merge.c|688| <<blk_account_io_merge>> part_dec_in_flight(req->q, part, rq_data_dir(req));
+ *
+ * 不适用mq
+ * 减少parition的hd_struct->dkstats[cpu]->in_flight[rw]
+ * 如果不是partition 0, 还要减少partition 0的
+ */
 void part_dec_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
 {
 	if (queue_is_mq(q))
@@ -66,6 +141,10 @@ void part_dec_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
 		part_stat_local_dec(&part_to_disk(part)->part0, in_flight[rw]);
 }
 
+/*
+ * 计算当前inflight的request数量,
+ * 对mq的特殊计算还不清楚, 似乎用遍历tag的方式
+ */
 unsigned int part_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	int cpu;
@@ -86,6 +165,11 @@ unsigned int part_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight;
 }
 
+/*
+ * 计算当前inflight的request数量,
+ * 对mq的特殊计算还不清楚, 似乎用遍历tag的方式
+ * 分开在参数返回read和write
+ */
 void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		       unsigned int inflight[2])
 {
@@ -108,6 +192,15 @@ void part_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 		inflight[1] = 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|824| <<blk_partition_remap>> p = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *   - block/genhd.c|144| <<disk_get_part>> part = __disk_get_part(disk, partno);
+ *   - fs/buffer.c|3006| <<guard_bio_eod>> part = __disk_get_part(bio->bi_disk, bio->bi_partno);
+ *
+ * 返回gendisk下某个partition的hd_struct
+ * gendisk->part_tbl->part[partno]
+ */
 struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *ptbl = rcu_dereference(disk->part_tbl);
@@ -131,6 +224,20 @@ struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
  * RETURNS:
  * Pointer to the found partition on success, NULL if not found.
  */
+/*
+ * called by:
+ *   - block/genhd.c|985| <<bdget_disk>> part = disk_get_part(disk, partno);
+ *   - block/genhd.c|1561| <<blk_lookup_devt>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|74| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - block/ioctl.c|112| <<blkpg_ioctl>> part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1520| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - fs/block_dev.c|1574| <<__blkdev_get>> bdev->bd_part = disk_get_part(disk, partno);
+ *   - init/do_mounts.c|154| <<devt_from_partuuid>> part = disk_get_part(disk, dev_to_part(dev)->partno + offset);
+ *
+ * 返回gendisk下某个partition的hd_struct
+ * gendisk->part_tbl->part[partno]
+ * 要增加part_to_dev(part)的reference count
+ */
 struct hd_struct *disk_get_part(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
@@ -249,6 +356,22 @@ EXPORT_SYMBOL_GPL(disk_part_iter_next);
  * CONTEXT:
  * Don't care.
  */
+/*
+ * called by:
+ *   - block/genhd.c|669| <<register_disk>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|785| <<del_gendisk>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|980| <<printk_all_partitions>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|1063| <<show_partition>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|1400| <<diskstats_show>> disk_part_iter_exit(&piter);
+ *   - block/genhd.c|1582| <<set_disk_ro>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|61| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|66| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|137| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/ioctl.c|145| <<blkpg_ioctl>> disk_part_iter_exit(&piter);
+ *   - block/partition-generic.c|457| <<drop_partitions>> disk_part_iter_exit(&piter);
+ *   - drivers/s390/block/dasd.c|437| <<dasd_state_ready_to_online>> disk_part_iter_exit(&piter);
+ *   - drivers/s390/block/dasd.c|464| <<dasd_state_online_to_ready>> disk_part_iter_exit(&piter);
+ */
 void disk_part_iter_exit(struct disk_part_iter *piter)
 {
 	disk_put_part(piter->part);
@@ -256,6 +379,13 @@ void disk_part_iter_exit(struct disk_part_iter *piter)
 }
 EXPORT_SYMBOL_GPL(disk_part_iter_exit);
 
+/*
+ * called by:
+ *   - block/genhd.c|363| <<disk_map_sector_rcu>> if (part && sector_in_part(part, sector))
+ *   - block/genhd.c|369| <<disk_map_sector_rcu>> if (part && sector_in_part(part, sector)) {
+ *
+ * 判断参数的sector是否属于某个partition
+ */
 static inline int sector_in_part(struct hd_struct *part, sector_t sector)
 {
 	return part->start_sect <= sector &&
@@ -277,6 +407,12 @@ static inline int sector_in_part(struct hd_struct *part, sector_t sector)
  * RETURNS:
  * Found partition on success, part0 is returned if no partition matches
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1337| <<blk_account_io_start>> part = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));
+ *
+ * 根据sector在gendisk找到对应的partition
+ */
 struct hd_struct *disk_map_sector_rcu(struct gendisk *disk, sector_t sector)
 {
 	struct disk_part_tbl *ptbl;
@@ -305,6 +441,14 @@ EXPORT_SYMBOL_GPL(disk_map_sector_rcu);
  * Can be deleted altogether. Later.
  *
  */
+/*
+ * 使用major_names[]的地方:
+ *   - block/genhd.c|410| <<blkdev_show>> for (dp = major_names[major_to_index(offset)]; dp; dp = dp->next)
+ *   - block/genhd.c|446| <<register_blkdev>> for (index = ARRAY_SIZE(major_names)-1; index > 0; index--) {
+ *   - block/genhd.c|447| <<register_blkdev>> if (major_names[index] == NULL)
+ *   - block/genhd.c|480| <<register_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next) {
+ *   - block/genhd.c|508| <<unregister_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next)
+ */
 #define BLKDEV_MAJOR_HASH_SIZE 255
 static struct blk_major_name {
 	struct blk_major_name *next;
@@ -313,6 +457,12 @@ static struct blk_major_name {
 } *major_names[BLKDEV_MAJOR_HASH_SIZE];
 
 /* index in the above - for now: assume no multimajor ranges */
+/*
+ * called by:
+ *   - block/genhd.c|446| <<blkdev_show>> for (dp = major_names[major_to_index(offset)]; dp; dp = dp->next)
+ *   - block/genhd.c|536| <<register_blkdev>> index = major_to_index(major);
+ *   - block/genhd.c|563| <<unregister_blkdev>> int index = major_to_index(major);
+ */
 static inline int major_to_index(unsigned major)
 {
 	return major % BLKDEV_MAJOR_HASH_SIZE;
@@ -351,6 +501,22 @@ void blkdev_show(struct seq_file *seqf, off_t offset)
  * See Documentation/admin-guide/devices.txt for the list of allocated
  * major numbers.
  */
+/*
+ * 调用的例子:
+ *   - block/genhd.c|1304| <<genhd_device_init>> register_blkdev(BLOCK_EXT_MAJOR, "blkext");
+ *   - drivers/block/virtio_blk.c|1310| <<init>> major = register_blkdev(0, "virtblk");
+ *   - drivers/block/xen-blkfront.c|2718| <<xlblk_init>> if (register_blkdev(XENVBD_MAJOR, DEV_NAME)) {
+ *   - drivers/block/loop.c|2256| <<loop_init>> if (register_blkdev(LOOP_MAJOR, "loop")) {
+ *   - drivers/block/null_blk_main.c|1798| <<null_init>> null_major = register_blkdev(0, "nullb");
+ *   - drivers/md/bcache/super.c|2508| <<bcache_init>> bcache_major = register_blkdev(0, "bcache");
+ *   - drivers/md/dm.c|235| <<local_init>> r = register_blkdev(_major, _name);
+ *   - drivers/md/md.c|9123| <<md_init>> if ((ret = register_blkdev(MD_MAJOR, "md")) < 0)
+ *   - drivers/md/md.c|9126| <<md_init>> if ((ret = register_blkdev(0, "mdp")) < 0)
+ *   - drivers/scsi/sd.c|3641| <<init_sd>> if (register_blkdev(sd_major(i), "sd") != 0)
+ *
+ * 为参数major和name生成一个struct blk_major_name
+ * 插入major_names数组
+ */
 int register_blkdev(unsigned int major, const char *name)
 {
 	struct blk_major_name **n, *p;
@@ -358,6 +524,15 @@ int register_blkdev(unsigned int major, const char *name)
 
 	mutex_lock(&block_class_lock);
 
+	/*
+	 * 使用major_names[]的地方:
+	 *   - block/genhd.c|410| <<blkdev_show>> for (dp = major_names[major_to_index(offset)]; dp; dp = dp->next)
+	 *   - block/genhd.c|446| <<register_blkdev>> for (index = ARRAY_SIZE(major_names)-1; index > 0; index--) {
+	 *   - block/genhd.c|447| <<register_blkdev>> if (major_names[index] == NULL)
+	 *   - block/genhd.c|480| <<register_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next) {
+	 *   - block/genhd.c|508| <<unregister_blkdev>> for (n = &major_names[index]; *n; n = &(*n)->next)
+	 */
+
 	/* temporary */
 	if (major == 0) {
 		for (index = ARRAY_SIZE(major_names)-1; index > 0; index--) {
@@ -437,6 +612,18 @@ void unregister_blkdev(unsigned int major, const char *name)
 
 EXPORT_SYMBOL(unregister_blkdev);
 
+/*
+ * used by:
+ *   - block/genhd.c|791| <<blk_register_region>> kobj_map(bdev_map, devt, range, module, probe, lock, data);
+ *   - block/genhd.c|798| <<blk_unregister_region>> kobj_unmap(bdev_map, devt, range);
+ *   - block/genhd.c|1155| <<get_gendisk>> kobj = kobj_lookup(bdev_map, devt, partno);
+ *   - block/genhd.c|1430| <<genhd_device_init>> bdev_map = kobj_map_init(base_probe, &block_class_lock);
+ *
+ * 关于kobj_map(), 内核中所有都字符设备都会记录在一个kobj_map结构的bdev_map变量中.
+ * 这个结构的变量中包含一个散列表用来快速存取所有的对象.kobj_map()函数就是用来把
+ * 块设备编号和相关数据结构变量一起保存到bdev_map这个散列表里.当后续要打开一个
+ * 块设备文件时,通过调用 kobj_lookup()函数,根据设备编号就可以找到相关结构变量.
+ */
 static struct kobj_map *bdev_map;
 
 /**
@@ -452,6 +639,13 @@ static struct kobj_map *bdev_map;
  * CONTEXT:
  * Don't care.
  */
+/*
+ * called by:
+ *   - block/genhd.c|656| <<blk_alloc_devt>> *devt = MKDEV(BLOCK_EXT_MAJOR, blk_mangle_minor(idx));
+ *   - block/genhd.c|676| <<blk_free_devt>> idr_remove(&ext_devt_idr, blk_mangle_minor(MINOR(devt)));
+ *   - block/genhd.c|688| <<blk_invalidate_devt>> idr_replace(&ext_devt_idr, NULL, blk_mangle_minor(MINOR(devt)));
+ *   - block/genhd.c|1083| <<get_gendisk>> part = idr_find(&ext_devt_idr, blk_mangle_minor(MINOR(devt)));
+ */
 static int blk_mangle_minor(int minor)
 {
 #ifdef CONFIG_DEBUG_BLOCK_EXT_DEVT
@@ -485,6 +679,13 @@ static int blk_mangle_minor(int minor)
  * CONTEXT:
  * Might sleep.
  */
+/*
+ * called by:
+ *   - block/genhd.c|812| <<__device_add_disk>> retval = blk_alloc_devt(&disk->part0, &devt);
+ *   - block/partition-generic.c|376| <<add_partition>> err = blk_alloc_devt(p, &devt);
+ *
+ * 通过参数的devt返回
+ */
 int blk_alloc_devt(struct hd_struct *part, dev_t *devt)
 {
 	struct gendisk *disk = part_to_disk(part);
@@ -500,6 +701,14 @@ int blk_alloc_devt(struct hd_struct *part, dev_t *devt)
 	idr_preload(GFP_KERNEL);
 
 	spin_lock_bh(&ext_devt_lock);
+	/*
+	 * 在以下使用ext_devt_idr:
+	 *   - block/genhd.c|81| <<global>> static DEFINE_IDR(ext_devt_idr);
+	 *   - block/genhd.c|690| <<blk_alloc_devt>> idx = idr_alloc(&ext_devt_idr, part, 0, NR_EXT_DEVT, GFP_NOWAIT);
+	 *   - block/genhd.c|717| <<blk_free_devt>> idr_remove(&ext_devt_idr, blk_mangle_minor(MINOR(devt)));
+	 *   - block/genhd.c|729| <<blk_invalidate_devt>> idr_replace(&ext_devt_idr, NULL, blk_mangle_minor(MINOR(devt)));
+	 *   - block/genhd.c|1124| <<get_gendisk>> part = idr_find(&ext_devt_idr, blk_mangle_minor(MINOR(devt)));
+	 */
 	idx = idr_alloc(&ext_devt_idr, part, 0, NR_EXT_DEVT, GFP_NOWAIT);
 	spin_unlock_bh(&ext_devt_lock);
 
@@ -535,6 +744,11 @@ void blk_free_devt(dev_t devt)
 /*
  * We invalidate devt by assigning NULL pointer for devt in idr.
  */
+/*
+ * called by:
+ *   - block/genhd.c|1073| <<del_gendisk>> blk_invalidate_devt(disk_devt(disk));
+ *   - block/partition-generic.c|294| <<delete_partition>> blk_invalidate_devt(part_devt(part));
+ */
 void blk_invalidate_devt(dev_t devt)
 {
 	if (MAJOR(devt) == BLOCK_EXT_MAJOR) {
@@ -544,6 +758,10 @@ void blk_invalidate_devt(dev_t devt)
 	}
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|1250| <<printk_all_partitions>> bdevt_str(part_devt(part), devt_buf),
+ */
 static char *bdevt_str(dev_t devt, char *buf)
 {
 	if (MAJOR(devt) <= 0xff && MINOR(devt) <= 0xff) {
@@ -561,10 +779,33 @@ static char *bdevt_str(dev_t devt, char *buf)
  * range must be nonzero
  * The hash chain is sorted on range, so that subranges can override.
  */
+/*
+ * called by:
+ *   - block/genhd.c|859| <<__device_add_disk>> blk_register_region(disk_devt(disk), disk->minors, NULL,
+ *   - drivers/block/amiflop.c|1887| <<amiga_floppy_probe>> blk_register_region(MKDEV(FLOPPY_MAJOR, 0), 256, THIS_MODULE,
+ *   - drivers/block/ataflop.c|2038| <<atari_floppy_init>> blk_register_region(MKDEV(FLOPPY_MAJOR, 0), 256, THIS_MODULE,
+ *   - drivers/block/brd.c|517| <<brd_init>> blk_register_region(MKDEV(RAMDISK_MAJOR, 0), 1UL << MINORBITS,
+ *   - drivers/block/floppy.c|4558| <<do_floppy_init>> blk_register_region(MKDEV(FLOPPY_MAJOR, 0), 256, THIS_MODULE,
+ *   - drivers/block/loop.c|2261| <<loop_init>> blk_register_region(MKDEV(LOOP_MAJOR, 0), range,
+ *   - drivers/block/swim.c|867| <<swim_floppy_init>> blk_register_region(MKDEV(FLOPPY_MAJOR, 0), 256, THIS_MODULE,
+ *   - drivers/block/z2ram.c|378| <<z2_init>> blk_register_region(MKDEV(Z2RAM_MAJOR, 0), Z2MINOR_COUNT, THIS_MODULE,
+ *   - drivers/ide/ide-probe.c|950| <<ide_register_region>> blk_register_region(MKDEV(disk->major, disk->first_minor),
+ *   - drivers/ide/ide-probe.c|1024| <<hwif_init>> blk_register_region(MKDEV(hwif->major, 0), MAX_DRIVES << PARTN_BITS,
+ *   - drivers/md/md.c|9130| <<md_init>> blk_register_region(MKDEV(MD_MAJOR, 0), 512, THIS_MODULE,
+ *   - drivers/md/md.c|9132| <<md_init>> blk_register_region(MKDEV(mdp_major, 0), 1UL<<MINORBITS, THIS_MODULE,
+ *   - drivers/scsi/sd.c|3451| <<sd_remove>> blk_register_region(devt, SD_MINORS, NULL,
+ *   - drivers/scsi/sd.c|3644| <<init_sd>> blk_register_region(sd_major(i), SD_MINORS, NULL,
+ */
 void blk_register_region(dev_t devt, unsigned long range, struct module *module,
 			 struct kobject *(*probe)(dev_t, int *, void *),
 			 int (*lock)(dev_t, void *), void *data)
 {
+	/*
+	 * 关于kobj_map(), 内核中所有都字符设备都会记录在一个kobj_map结构的bdev_map变量中.
+	 * 这个结构的变量中包含一个散列表用来快速存取所有的对象.kobj_map()函数就是用来把
+	 * 块设备编号和相关数据结构变量一起保存到bdev_map这个散列表里.当后续要打开一个
+	 * 块设备文件时,通过调用 kobj_lookup()函数,根据设备编号就可以找到相关结构变量.
+	 */
 	kobj_map(bdev_map, devt, range, module, probe, lock, data);
 }
 
@@ -577,6 +818,10 @@ void blk_unregister_region(dev_t devt, unsigned long range)
 
 EXPORT_SYMBOL(blk_unregister_region);
 
+/*
+ * used by:
+ *   - block/genhd.c|982| <<__device_add_disk>> exact_match, exact_lock, disk);
+ */
 static struct kobject *exact_match(dev_t devt, int *partno, void *data)
 {
 	struct gendisk *p = data;
@@ -584,6 +829,10 @@ static struct kobject *exact_match(dev_t devt, int *partno, void *data)
 	return &disk_to_dev(p)->kobj;
 }
 
+/*
+ * used by:
+ *   - block/genhd.c|982| <<__device_add_disk>> exact_match, exact_lock, disk);
+ */
 static int exact_lock(dev_t devt, void *data)
 {
 	struct gendisk *p = data;
@@ -593,6 +842,10 @@ static int exact_lock(dev_t devt, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|862| <<__device_add_disk>> register_disk(parent, disk, groups);
+ */
 static void register_disk(struct device *parent, struct gendisk *disk,
 			  const struct attribute_group **groups)
 {
@@ -647,6 +900,11 @@ static void register_disk(struct device *parent, struct gendisk *disk,
 	if (!get_capacity(disk))
 		goto exit;
 
+	/*
+	 * 先通过partno在gendisk中获取hd_struct (gendisk->part_tbl->part[partno])
+	 * 要增加part_to_dev(part)的reference count
+	 * 然后获取partition(hd_struct)对应的block_device
+	 */
 	bdev = bdget_disk(disk, 0);
 	if (!bdev)
 		goto exit;
@@ -688,6 +946,27 @@ static void register_disk(struct device *parent, struct gendisk *disk,
  *
  * FIXME: error handling
  */
+/*
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/genhd.c|781| <<device_add_disk>> __device_add_disk(parent, disk, groups, true);
+ *   - block/genhd.c|787| <<device_add_disk_no_queue_reg>> __device_add_disk(parent, disk, NULL, false);
+ */
 static void __device_add_disk(struct device *parent, struct gendisk *disk,
 			      const struct attribute_group **groups,
 			      bool register_queue)
@@ -726,6 +1005,11 @@ static void __device_add_disk(struct device *parent, struct gendisk *disk,
 		int ret;
 
 		/* Register BDI before referencing it from bdev */
+		/*
+		 * struct gendisk 
+		 *  -> struct hd_struct part0
+		 *      -> struct device __dev
+		 */
 		disk_to_dev(disk)->devt = devt;
 		ret = bdi_register_owner(disk->queue->backing_dev_info,
 						disk_to_dev(disk));
@@ -747,6 +1031,21 @@ static void __device_add_disk(struct device *parent, struct gendisk *disk,
 	blk_integrity_add(disk);
 }
 
+/*
+ * 部分调用的例子:
+ *   - drivers/block/virtio_blk.c|1168| <<virtblk_probe>> device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);
+ *   - drivers/block/xen-blkfront.c|2424| <<blkfront_connect>> device_add_disk(&info->xbdev->dev, info->gd, NULL);
+ *   - drivers/ide/ide-cd.c|1801| <<ide_cd_probe>> device_add_disk(&drive->gendev, g, NULL);
+ *   - drivers/ide/ide-gd.c|422| <<ide_gd_probe>> device_add_disk(&drive->gendev, g, NULL);
+ *   - drivers/nvdimm/blk.c|285| <<nsblk_attach_disk>> device_add_disk(dev, disk, NULL);
+ *   - drivers/nvdimm/btt.c|1560| <<btt_blk_init>> device_add_disk(&btt->nd_btt->dev, btt->btt_disk, NULL);
+ *   - drivers/nvdimm/pmem.c|484| <<pmem_attach_disk>> device_add_disk(dev, disk, NULL);
+ *   - drivers/nvme/host/core.c|3349| <<nvme_alloc_ns>> device_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);
+ *   - drivers/nvme/host/multipath.c|352| <<nvme_mpath_set_live>> device_add_disk(&head->subsys->dev, head->disk,
+ *   - drivers/scsi/sd.c|3300| <<sd_probe_async>> device_add_disk(dev, gd, NULL);
+ *   - drivers/scsi/sr.c|763| <<sr_probe>> device_add_disk(&sdev->sdev_gendev, disk, NULL);
+ *   - include/linux/genhd.h|456| <<add_disk>> device_add_disk(NULL, disk, NULL);
+ */
 void device_add_disk(struct device *parent, struct gendisk *disk,
 		     const struct attribute_group **groups)
 
@@ -755,6 +1054,10 @@ void device_add_disk(struct device *parent, struct gendisk *disk,
 }
 EXPORT_SYMBOL(device_add_disk);
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|474| <<add_disk_no_queue_reg>> device_add_disk_no_queue_reg(NULL, disk);
+ */
 void device_add_disk_no_queue_reg(struct device *parent, struct gendisk *disk)
 {
 	__device_add_disk(parent, disk, NULL, false);
@@ -859,6 +1162,12 @@ static ssize_t disk_badblocks_store(struct device *dev,
  * This function gets the structure containing partitioning
  * information for the given device @devt.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|824| <<__acquires>> disk = get_gendisk(MKDEV(major, minor), &part);
+ *   - fs/block_dev.c|1204| <<bdev_get_gendisk>> struct gendisk *disk = get_gendisk(bdev->bd_dev, partno);
+ *   - kernel/power/hibernate.c|848| <<software_resume>> while (!get_gendisk(swsusp_resume_device, &partno))
+ */
 struct gendisk *get_gendisk(dev_t devt, int *partno)
 {
 	struct gendisk *disk = NULL;
@@ -866,6 +1175,12 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 	if (MAJOR(devt) != BLOCK_EXT_MAJOR) {
 		struct kobject *kobj;
 
+		/*
+		 * 关于kobj_map(), 内核中所有都字符设备都会记录在一个kobj_map结构的bdev_map变量中.
+		 * 这个结构的变量中包含一个散列表用来快速存取所有的对象.kobj_map()函数就是用来把
+		 * 块设备编号和相关数据结构变量一起保存到bdev_map这个散列表里.当后续要打开一个
+		 * 块设备文件时,通过调用 kobj_lookup()函数,根据设备编号就可以找到相关结构变量.
+		 */
 		kobj = kobj_lookup(bdev_map, devt, partno);
 		if (kobj)
 			disk = dev_to_disk(kobj_to_dev(kobj));
@@ -914,12 +1229,44 @@ EXPORT_SYMBOL(get_gendisk);
  * RETURNS:
  * Resulting block_device on success, NULL on failure.
  */
+/*
+ * called by:
+ *   - block/genhd.c|776| <<register_disk>> bdev = bdget_disk(disk, 0);
+ *   - block/genhd.c|1830| <<invalidate_partition>> struct block_device *bdev = bdget_disk(disk, partno);
+ *   - drivers/block/aoe/aoecmd.c|901| <<aoecmd_sleepwork>> bd = bdget_disk(d->gd, 0);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3694| <<mtip_block_initialize>> dd->bdev = bdget_disk(dd->disk, 0);
+ *   - drivers/block/nbd.c|282| <<nbd_size_update>> struct block_device *bdev = bdget_disk(nbd->disk, 0);
+ *   - drivers/block/nbd.c|1385| <<nbd_release>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2145| <<blkfront_closing>> bdev = bdget_disk(info->gd, 0);
+ *   - drivers/block/xen-blkfront.c|2506| <<blkfront_remove>> bdev = bdget_disk(disk, 0);
+ *   - drivers/block/xen-blkfront.c|2588| <<blkif_release>> bdev = bdget_disk(disk, 0);
+ *   - drivers/block/zram/zram_drv.c|1770| <<reset_store>> bdev = bdget_disk(zram->disk, 0);
+ *   - drivers/block/zram/zram_drv.c|1974| <<zram_remove>> bdev = bdget_disk(zram->disk, 0);
+ *   - drivers/md/dm.c|2006| <<alloc_dev>> md->bdev = bdget_disk(md->disk, 0);
+ *   - drivers/s390/block/dasd_genhd.c|104| <<dasd_scan_partitions>> bdev = bdget_disk(block->gdp, 0);
+ *   - drivers/s390/block/dasd_ioctl.c|229| <<dasd_format>> struct block_device *bdev = bdget_disk(block->gdp, 0);
+ *   - fs/block_dev.c|1183| <<bd_start_claiming>> whole = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1422| <<revalidate_disk>> struct block_device *bdev = bdget_disk(disk, 0);
+ *   - fs/block_dev.c|1568| <<__blkdev_get>> whole = bdget_disk(disk, 0);
+ *
+ * 先通过partno在gendisk中获取hd_struct (gendisk->part_tbl->part[partno])
+ * 要增加part_to_dev(part)的reference count
+ * 然后获取partition(hd_struct)对应的block_device
+ */
 struct block_device *bdget_disk(struct gendisk *disk, int partno)
 {
 	struct hd_struct *part;
 	struct block_device *bdev = NULL;
 
+	/*
+	 * 返回gendisk下某个partition的hd_struct
+	 * gendisk->part_tbl->part[partno]
+	 * 要增加part_to_dev(part)的reference count
+	 */
 	part = disk_get_part(disk, partno);
+	/*
+	 * 获取partition对应的block_device
+	 */
 	if (part)
 		bdev = bdget(part_devt(part));
 	disk_put_part(part);
@@ -933,13 +1280,29 @@ EXPORT_SYMBOL(bdget_disk);
  * filesystem can't be mounted and thus to give the victim some idea of what
  * went wrong
  */
+/*
+ * called by:
+ *   - init/do_mounts.c|438| <<mount_block_root>> printk_all_partitions();
+ *   - init/do_mounts.c|451| <<mount_block_root>> printk_all_partitions();
+ */
 void __init printk_all_partitions(void)
 {
+	/*
+	 * struct class_dev_iter {
+	 *     struct klist_iter ki;
+	 *     const struct device_type *type;
+	 * };
+	 */
 	struct class_dev_iter iter;
 	struct device *dev;
 
 	class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
 	while ((dev = class_dev_iter_next(&iter))) {
+		/*
+		 * struct gendisk 
+		 *  -> struct hd_struct part0
+		 *      -> struct device __dev
+		 */
 		struct gendisk *disk = dev_to_disk(dev);
 		struct disk_part_iter piter;
 		struct hd_struct *part;
@@ -1065,6 +1428,10 @@ static int show_partition(struct seq_file *seqf, void *v)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - block/genhd.c|1798| <<proc_genhd_init>> proc_create_seq("partitions", 0, NULL, &partitions_op);
+ */
 static const struct seq_operations partitions_op = {
 	.start	= show_partition_start,
 	.next	= disk_seqf_next,
@@ -1082,20 +1449,41 @@ static struct kobject *base_probe(dev_t devt, int *partno, void *data)
 	return NULL;
 }
 
+/*
+ * 初始化的函数
+ */
 static int __init genhd_device_init(void)
 {
 	int error;
 
+	/*
+	 * used by:
+	 *   - block/genhd.c|1105| <<genhd_device_init>> block_class.dev_kobj = sysfs_dev_block_kobj;
+	 *   - drivers/base/core.c|2485| <<devices_init>> sysfs_dev_block_kobj = kobject_create_and_add("block", dev_kobj);
+	 *   - drivers/base/core.c|2486| <<devices_init>> if (!sysfs_dev_block_kobj)
+	 *   - drivers/base/core.c|2495| <<devices_init>> kobject_put(sysfs_dev_block_kobj);
+	 */
 	block_class.dev_kobj = sysfs_dev_block_kobj;
 	error = class_register(&block_class);
 	if (unlikely(error))
 		return error;
+	/*
+	 * 关于kobj_map(), 内核中所有都字符设备都会记录在一个kobj_map结构的bdev_map变量中.
+	 * 这个结构的变量中包含一个散列表用来快速存取所有的对象.kobj_map()函数就是用来把
+	 * 块设备编号和相关数据结构变量一起保存到bdev_map这个散列表里.当后续要打开一个
+	 * 块设备文件时,通过调用 kobj_lookup()函数,根据设备编号就可以找到相关结构变量.
+	 */
 	bdev_map = kobj_map_init(base_probe, &block_class_lock);
 	blk_dev_init();
 
 	register_blkdev(BLOCK_EXT_MAJOR, "blkext");
 
 	/* create top-level block dir */
+	/*
+	 * used by:
+	 *   - block/genhd.c|854| <<register_disk>> err = sysfs_create_link(block_depr, &ddev->kobj,
+	 *   - block/genhd.c|1103| <<del_gendisk>> sysfs_remove_link(block_depr, dev_name(disk_to_dev(disk)));
+	 */
 	if (!sysfs_deprecated)
 		block_depr = kobject_create_and_add("block", NULL);
 	return 0;
@@ -1245,6 +1633,11 @@ static const struct attribute_group *disk_attr_groups[] = {
  * LOCKING:
  * Matching bd_mutex locked or the caller is the only user of @disk.
  */
+/*
+ * called by:
+ *   - block/genhd.c|1533| <<disk_expand_part_tbl>> disk_replace_part_tbl(disk, new_ptbl);
+ *   - block/genhd.c|1544| <<disk_release>> disk_replace_part_tbl(disk, NULL);
+ */
 static void disk_replace_part_tbl(struct gendisk *disk,
 				  struct disk_part_tbl *new_ptbl)
 {
@@ -1274,6 +1667,32 @@ static void disk_replace_part_tbl(struct gendisk *disk,
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * [0] disk_expand_part_tbl
+ * [0] rescan_partitions
+ * [0] __blkdev_get
+ * [0] blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0]ret_from_fork
+ *
+ * called by:
+ *   - block/genhd.c|1575| <<__alloc_disk_node>> if (disk_expand_part_tbl(disk, 0)) {
+ *   - block/partition-generic.c|326| <<add_partition>> err = disk_expand_part_tbl(disk, partno);
+ *   - block/partition-generic.c|576| <<rescan_partitions>> disk_expand_part_tbl(disk, highest);
+ */
 int disk_expand_part_tbl(struct gendisk *disk, int partno)
 {
 	struct disk_part_tbl *old_ptbl =
@@ -1325,6 +1744,25 @@ static void disk_release(struct device *dev)
 		blk_put_queue(disk->queue);
 	kfree(disk);
 }
+/*
+ * used by:
+ *   - block/genhd.c|941| <<printk_all_partitions>> class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|998| <<disk_seqf_start>> class_dev_iter_init(iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1089| <<genhd_device_init>> block_class.dev_kobj = sysfs_dev_block_kobj;
+ *   - block/genhd.c|1090| <<genhd_device_init>> error = class_register(&block_class); 
+ *   - block/genhd.c|1427| <<blk_lookup_devt>> class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+ *   - block/genhd.c|1502| <<__alloc_disk_node>> disk_to_dev(disk)->class = &block_class;
+ *   - block/partition-generic.c|367| <<add_partition>> pdev->class = &block_class;
+ *   - drivers/base/class.c|178| <<__class_register>> if (!sysfs_deprecated || cls != &block_class)
+ *   - drivers/base/core.c|1752| <<get_device_parent>> if (sysfs_deprecated && dev->class == &block_class) {
+ *   - drivers/base/core.c|1753| <<get_device_parent>> if (parent && parent->class == &block_class)
+ *   - drivers/base/core.c|1755| <<get_device_parent>> return &block_class.p->subsys.kobj;
+ *   - drivers/base/core.c|1864| <<device_add_class_symlinks>> if (sysfs_deprecated && dev->class == &block_class)
+ *   - drivers/base/core.c|1898| <<device_remove_class_symlinks>> if (sysfs_deprecated && dev->class == &block_class)
+ *   - drivers/base/devtmpfs.c|78| <<is_blockdev>> return dev->class == &block_class;
+ *   - init/do_mounts.c|141| <<devt_from_partuuid>> dev = class_find_device(&block_class, NULL, &cmp,
+ *   - init/do_mounts.c|239| <<name_to_dev_t>> dev = class_find_device(&block_class, NULL, name + 10,
+ */
 struct class block_class = {
 	.name		= "block",
 };
@@ -1409,6 +1847,7 @@ static const struct seq_operations diskstats_op = {
 	.show	= diskstats_show
 };
 
+/* 初始化函数 */
 static int __init proc_genhd_init(void)
 {
 	proc_create_seq("diskstats", 0, NULL, &diskstats_op);
@@ -1418,6 +1857,12 @@ static int __init proc_genhd_init(void)
 module_init(proc_genhd_init);
 #endif /* CONFIG_PROC_FS */
 
+/*
+ * called by:
+ *   - init/do_mounts.c|281| <<name_to_dev_t>> res = blk_lookup_devt(s, 0);
+ *   - init/do_mounts.c|297| <<name_to_dev_t>> res = blk_lookup_devt(s, part);
+ *   - init/do_mounts.c|305| <<name_to_dev_t>> res = blk_lookup_devt(s, part);
+ */
 dev_t blk_lookup_devt(const char *name, int partno)
 {
 	dev_t devt = MKDEV(0, 0);
@@ -1453,6 +1898,10 @@ dev_t blk_lookup_devt(const char *name, int partno)
 }
 EXPORT_SYMBOL(blk_lookup_devt);
 
+/*
+ * called by:
+ *   - include/linux/genhd.h|682| <<alloc_disk_node>> __disk = __alloc_disk_node(minors, node_id); \
+ */
 struct gendisk *__alloc_disk_node(int minors, int node_id)
 {
 	struct gendisk *disk;
@@ -1465,8 +1914,12 @@ struct gendisk *__alloc_disk_node(int minors, int node_id)
 		minors = DISK_MAX_PARTS;
 	}
 
+	/* 分配gendisk */
 	disk = kzalloc_node(sizeof(struct gendisk), GFP_KERNEL, node_id);
 	if (disk) {
+		/*
+		 * struct hd_struct part0;
+		 */
 		if (!init_part_stats(&disk->part0)) {
 			kfree(disk);
 			return NULL;
@@ -1478,6 +1931,11 @@ struct gendisk *__alloc_disk_node(int minors, int node_id)
 			kfree(disk);
 			return NULL;
 		}
+		/*
+		 * 参数1: The conditions under which the dereference will take place
+		 *
+		 * struct disk_part_tbl __rcu *part_tbl;
+		 */
 		ptbl = rcu_dereference_protected(disk->part_tbl, 1);
 		rcu_assign_pointer(ptbl->part[0], &disk->part0);
 
@@ -1507,6 +1965,19 @@ struct gendisk *__alloc_disk_node(int minors, int node_id)
 }
 EXPORT_SYMBOL(__alloc_disk_node);
 
+/*
+ * called by:
+ *   - block/genhd.c|822| <<exact_lock>> if (!get_disk_and_module(p))
+ *   - block/genhd.c|1163| <<get_gendisk>> if (part && get_disk_and_module(part_to_disk(part))) {
+ *   - drivers/block/amiflop.c|1855| <<floppy_find>> return get_disk_and_module(unit[drive].gendisk);
+ *   - drivers/block/ataflop.c|1970| <<floppy_find>> return get_disk_and_module(unit[drive].disk);
+ *   - drivers/block/brd.c|464| <<brd_probe>> kobj = brd ? get_disk_and_module(brd->brd_disk) : NULL;
+ *   - drivers/block/floppy.c|4502| <<floppy_find>> return get_disk_and_module(disks[drive]);
+ *   - drivers/block/loop.c|2132| <<loop_probe>> kobj = get_disk_and_module(lo->lo_disk);
+ *   - drivers/block/swim.c|775| <<floppy_find>> return get_disk_and_module(swd->unit[drive].disk);
+ *   - drivers/block/z2ram.c|336| <<z2_find>> return get_disk_and_module(z2ram_gendisk);
+ *   - drivers/ide/ide-probe.c|943| <<exact_lock>> if (!get_disk_and_module(p))
+ */
 struct kobject *get_disk_and_module(struct gendisk *disk)
 {
 	struct module *owner;
@@ -1549,6 +2020,10 @@ void put_disk_and_module(struct gendisk *disk)
 }
 EXPORT_SYMBOL(put_disk_and_module);
 
+/*
+ * called by only:
+ *   - block/genhd.c|1968| <<set_disk_ro>> set_disk_ro_uevent(disk, flag);
+ */
 static void set_disk_ro_uevent(struct gendisk *gd, int ro)
 {
 	char event[] = "DISK_RO=1";
@@ -1593,6 +2068,13 @@ int bdev_read_only(struct block_device *bdev)
 
 EXPORT_SYMBOL(bdev_read_only);
 
+/*
+ * called by:
+ *   - block/genhd.c|1061| <<del_gendisk>> invalidate_partition(disk, part->partno);
+ *   - block/genhd.c|1067| <<del_gendisk>> invalidate_partition(disk, 0);
+ *   - block/partition-generic.c|475| <<drop_partitions>> res = invalidate_partition(disk, 0);
+ *   - drivers/s390/block/dasd_genhd.c|168| <<dasd_destroy_partitions>> invalidate_partition(block->gdp, 0);
+ */
 int invalidate_partition(struct gendisk *disk, int partno)
 {
 	int res = 0;
@@ -1636,6 +2118,12 @@ static char *disk_uevents[] = {
 
 /* list of all disk_events */
 static DEFINE_MUTEX(disk_events_mutex);
+/*
+ * used by:
+ *   - block/genhd.c|2423| <<disk_events_set_dfl_poll_msecs>> list_for_each_entry(ev, &disk_events, node)
+ *   - block/genhd.c|2447| <<disk_alloc_events>> struct disk_events *ev;
+ *   - block/genhd.c|2480| <<disk_add_events>> list_add_tail(&disk->ev->node, &disk_events);
+ */
 static LIST_HEAD(disk_events);
 
 /* disable in-kernel polling by default */
@@ -1833,6 +2321,11 @@ static void disk_events_workfn(struct work_struct *work)
 	disk_check_events(ev, &ev->clearing);
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|1935| <<disk_clear_events>> disk_check_events(ev, &clearing);
+ *   - block/genhd.c|1961| <<disk_events_workfn>> disk_check_events(ev, &ev->clearing);
+ */
 static void disk_check_events(struct disk_events *ev,
 			      unsigned int *clearing_ptr)
 {
diff --git a/block/partition-generic.c b/block/partition-generic.c
index aee643c..13ddd14a 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -306,6 +306,31 @@ static DEVICE_ATTR(whole_disk, 0444, whole_disk_show, NULL);
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * [0] add_partition
+ * [0] rescan_partitions
+ * [0] __blkdev_get
+ * [0] blkdev_get
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/ioctl.c|69| <<blkpg_ioctl>> part = add_partition(disk, partno, start, length,
+ *   - block/partition-generic.c|625| <<rescan_partitions>> part = add_partition(disk, p, from, size,
+ */
 struct hd_struct *add_partition(struct gendisk *disk, int partno,
 				sector_t start, sector_t len, int flags,
 				struct partition_meta_info *info)
diff --git a/drivers/base/core.c b/drivers/base/core.c
index fd7511e..9f9c86c 100644
--- a/drivers/base/core.c
+++ b/drivers/base/core.c
@@ -838,6 +838,15 @@ int (*platform_notify)(struct device *dev) = NULL;
 int (*platform_notify_remove)(struct device *dev) = NULL;
 static struct kobject *dev_kobj;
 struct kobject *sysfs_dev_char_kobj;
+/*
+ * used by:
+ *   - block/genhd.c|1105| <<genhd_device_init>> block_class.dev_kobj = sysfs_dev_block_kobj;
+ *   - drivers/base/core.c|2485| <<devices_init>> sysfs_dev_block_kobj = kobject_create_and_add("block", dev_kobj);
+ *   - drivers/base/core.c|2486| <<devices_init>> if (!sysfs_dev_block_kobj)
+ *   - drivers/base/core.c|2495| <<devices_init>> kobject_put(sysfs_dev_block_kobj);
+ *
+ * 在devices_init()创建sysfs_dev_block_kobj为/sys/dev/block
+ */
 struct kobject *sysfs_dev_block_kobj;
 
 static DEFINE_MUTEX(device_hotplug_lock);
@@ -2476,15 +2485,19 @@ EXPORT_SYMBOL_GPL(device_find_child);
 
 int __init devices_init(void)
 {
+	/* 创建/sys/devices */
 	devices_kset = kset_create_and_add("devices", &device_uevent_ops, NULL);
 	if (!devices_kset)
 		return -ENOMEM;
+	/* 创建/sys/dev */
 	dev_kobj = kobject_create_and_add("dev", NULL);
 	if (!dev_kobj)
 		goto dev_kobj_err;
+	/* 创建/sys/dev/block */
 	sysfs_dev_block_kobj = kobject_create_and_add("block", dev_kobj);
 	if (!sysfs_dev_block_kobj)
 		goto block_kobj_err;
+	/* 创建/sys/dev/char */
 	sysfs_dev_char_kobj = kobject_create_and_add("char", dev_kobj);
 	if (!sysfs_dev_char_kobj)
 		goto char_kobj_err;
diff --git a/drivers/base/map.c b/drivers/base/map.c
index 5650ab2..0e2fa1a 100644
--- a/drivers/base/map.c
+++ b/drivers/base/map.c
@@ -92,6 +92,11 @@ void kobj_unmap(struct kobj_map *domain, dev_t dev, unsigned long range)
 	kfree(found);
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|1179| <<get_gendisk>> kobj = kobj_lookup(bdev_map, devt, partno);
+ *   - fs/char_dev.c|385| <<chrdev_open>> kobj = kobj_lookup(cdev_map, inode->i_rdev, &idx);
+ */
 struct kobject *kobj_lookup(struct kobj_map *domain, dev_t dev, int *index)
 {
 	struct kobject *kobj;
@@ -132,6 +137,11 @@ struct kobject *kobj_lookup(struct kobj_map *domain, dev_t dev, int *index)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|1430| <<genhd_device_init>> bdev_map = kobj_map_init(base_probe, &block_class_lock);
+ *   - fs/char_dev.c|665| <<chrdev_init>> cdev_map = kobj_map_init(base_probe, &chrdevs_lock);
+ */
 struct kobj_map *kobj_map_init(kobj_probe_t *base_probe, struct mutex *lock)
 {
 	struct kobj_map *p = kmalloc(sizeof(struct kobj_map), GFP_KERNEL);
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index 447d635..34fd9d4 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -41,6 +41,12 @@ static inline u64 mb_per_tick(int mbps)
 enum nullb_device_flags {
 	NULLB_DEV_FL_CONFIGURED	= 0,
 	NULLB_DEV_FL_UP		= 1,
+	/*
+	 * 在以下使用:
+	 *   - drivers/block/null_blk_main.c|1141| <<null_handle_cmd>> if (test_bit(NULLB_DEV_FL_THROTTLED, &dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1386| <<null_del_dev>> if (test_bit(NULLB_DEV_FL_THROTTLED, &nullb->dev->flags)) {
+	 *   - drivers/block/null_blk_main.c|1675| <<null_add_dev>> set_bit(NULLB_DEV_FL_THROTTLED, &dev->flags);
+	 */
 	NULLB_DEV_FL_THROTTLED	= 2,
 	NULLB_DEV_FL_CACHE	= 3,
 };
@@ -1132,6 +1138,11 @@ static void null_restart_queue_async(struct nullb *nullb)
 		blk_mq_start_stopped_hw_queues(q, true);
 }
 
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1289| <<null_queue_bio>> null_handle_cmd(cmd);
+ *   - drivers/block/null_blk_main.c|1351| <<null_queue_rq>> return null_handle_cmd(cmd);
+ */
 static blk_status_t null_handle_cmd(struct nullb_cmd *cmd)
 {
 	struct nullb_device *dev = cmd->nq->dev;
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 7ffd719..5262132 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -24,6 +24,11 @@
 static int major;
 static DEFINE_IDA(vd_index_ida);
 
+/*
+ * 在以下使用和分配:
+ *   - drivers/block/virtio_blk.c|501| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+ *   - drivers/block/virtio_blk.c|1033| <<init>> virtblk_wq = alloc_workqueue("virtio-blk", 0, 0);
+ */
 static struct workqueue_struct *virtblk_wq;
 
 struct virtio_blk_vq {
@@ -45,6 +50,19 @@ struct virtio_blk {
 	struct work_struct config_work;
 
 	/* What host tells us, plus 2 for header & tailer. */
+	/*
+	 * 在以下使用:
+	 *   - drivers/block/virtio_blk.c|358| <<virtio_queue_rq>> BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
+	 *   - drivers/block/virtio_blk.c|831| <<virtblk_init_request>> sg_init_table(vbr->sg, vblk->sg_elems);
+	 *   - drivers/block/virtio_blk.c|888| <<virtblk_probe>> u32 v, blk_size, max_size, sg_elems, opt_io_size;
+	 *   - drivers/block/virtio_blk.c|907| <<virtblk_probe>> &sg_elems);
+	 *   - drivers/block/virtio_blk.c|910| <<virtblk_probe>> if (err || !sg_elems)
+	 *   - drivers/block/virtio_blk.c|911| <<virtblk_probe>> sg_elems = 1;
+	 *   - drivers/block/virtio_blk.c|914| <<virtblk_probe>> sg_elems += 2;
+	 *   - drivers/block/virtio_blk.c|922| <<virtblk_probe>> vblk->sg_elems = sg_elems;
+	 *   - drivers/block/virtio_blk.c|952| <<virtblk_probe>> sizeof(struct scatterlist) * sg_elems;
+	 *   - drivers/block/virtio_blk.c|986| <<virtblk_probe>> blk_queue_max_segments(q, vblk->sg_elems-2);
+	 */
 	unsigned int sg_elems;
 
 	/* Ida index - used to track minor number allocations. */
@@ -66,6 +84,11 @@ struct virtblk_req {
 	struct scatterlist sg[];
 };
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|226| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/virtio_blk.c|382| <<virtblk_get_id>> err = blk_status_to_errno(virtblk_result(blk_mq_rq_to_pdu(req)));
+ */
 static inline blk_status_t virtblk_result(struct virtblk_req *vbr)
 {
 	switch (vbr->status) {
@@ -85,6 +108,10 @@ static inline blk_status_t virtblk_result(struct virtblk_req *vbr)
  * information.
  */
 #ifdef CONFIG_VIRTIO_BLK_SCSI
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|341| <<virtio_queue_rq>> err = virtblk_add_req_scsi(vblk->vqs[qid].vq, vbr, vbr->sg, num);
+ */
 static int virtblk_add_req_scsi(struct virtqueue *vq, struct virtblk_req *vbr,
 		struct scatterlist *data_sg, bool have_data)
 {
@@ -113,6 +140,11 @@ static int virtblk_add_req_scsi(struct virtqueue *vq, struct virtblk_req *vbr,
 	return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|166| <<virtblk_scsi_request_done>> static inline void virtblk_scsi_request_done(struct request *req)
+ *   - drivers/block/virtio_blk.c|257| <<virtblk_request_done>> virtblk_scsi_request_done(req);
+ */
 static inline void virtblk_scsi_request_done(struct request *req)
 {
 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
@@ -124,6 +156,9 @@ static inline void virtblk_scsi_request_done(struct request *req)
 	sreq->result = virtio32_to_cpu(vblk->vdev, vbr->in_hdr.errors);
 }
 
+/*
+ * struct block_device_operations virtblk_fops.ioctl = virtblk_ioctl()
+ */
 static int virtblk_ioctl(struct block_device *bdev, fmode_t mode,
 			     unsigned int cmd, unsigned long data)
 {
@@ -152,6 +187,10 @@ static inline void virtblk_scsi_request_done(struct request *req)
 #define virtblk_ioctl	NULL
 #endif /* CONFIG_VIRTIO_BLK_SCSI */
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|357| <<virtio_queue_rq>> err = virtblk_add_req(vblk->vqs[qid].vq, vbr, vbr->sg, num);
+ */
 static int virtblk_add_req(struct virtqueue *vq, struct virtblk_req *vbr,
 		struct scatterlist *data_sg, bool have_data)
 {
@@ -174,6 +213,10 @@ static int virtblk_add_req(struct virtqueue *vq, struct virtblk_req *vbr,
 	return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|340| <<virtio_queue_rq>> err = virtblk_setup_discard_write_zeroes(req, unmap);
+ */
 static int virtblk_setup_discard_write_zeroes(struct request *req, bool unmap)
 {
 	unsigned short segments = blk_rq_nr_discard_segments(req);
@@ -207,6 +250,28 @@ static int virtblk_setup_discard_write_zeroes(struct request *req, bool unmap)
 	return 0;
 }
 
+/*
+ * [0] virtblk_request_done
+ * [0] blk_mq_complete_request
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - block/blk-mq.c|810| <<__blk_mq_complete_request_remote>> q->mq_ops->complete(rq);
+ *   - block/blk-mq.c|841| <<__blk_mq_complete_request>> q->mq_ops->complete(rq);
+ *   - block/blk-mq.c|855| <<__blk_mq_complete_request>> q->mq_ops->complete(rq);
+ *   - block/blk-mq.c|900| <<blk_mq_complete_request_sync>> rq->q->mq_ops->complete(rq);
+ *   - block/blk-softirq.c|37| <<blk_done_softirq>> rq->q->mq_ops->complete(rq);
+ *
+ * struct blk_mq_ops virtio_mq_ops.complete = virtblk_request_done()
+ */
 static inline void virtblk_request_done(struct request *req)
 {
 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
@@ -226,6 +291,20 @@ static inline void virtblk_request_done(struct request *req)
 	blk_mq_end_request(req, virtblk_result(vbr));
 }
 
+/*
+ * [0] virtblk_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ *
+ * used by:
+ *   - drivers/block/virtio_blk.c|532| <<init_vq>> callbacks[i] = virtblk_done;
+ */
 static void virtblk_done(struct virtqueue *vq)
 {
 	struct virtio_blk *vblk = vq->vdev->priv;
@@ -254,6 +333,13 @@ static void virtblk_done(struct virtqueue *vq)
 	spin_unlock_irqrestore(&vblk->vqs[qid].lock, flags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1644| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+ *   - block/blk-mq.c|2509| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+ *
+ * struct blk_mq_ops virtio_mq_ops.commit_rqs = virtio_commit_rqs()
+ */
 static void virtio_commit_rqs(struct blk_mq_hw_ctx *hctx)
 {
 	struct virtio_blk *vblk = hctx->queue->queuedata;
@@ -268,6 +354,13 @@ static void virtio_commit_rqs(struct blk_mq_hw_ctx *hctx)
 		virtqueue_notify(vq->vq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1592| <<blk_mq_dispatch_rq_list>> ret = q->mq_ops->queue_rq(hctx, &bd);
+ *   - block/blk-mq.c|2338| <<__blk_mq_issue_directly>> ret = q->mq_ops->queue_rq(hctx, &bd);
+ *
+ * struct blk_mq_ops virtio_mq_ops.queue_rq = virtio_queue_rq()
+ */
 static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 			   const struct blk_mq_queue_data *bd)
 {
@@ -282,6 +375,21 @@ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 	bool unmap = false;
 	u32 type;
 
+	/*
+	 * 修改nr_phys_segments的地方:
+	 *   - block/blk-core.c|611| <<bio_attempt_discard_merge>> req->nr_phys_segments = segments + 1;
+	 *   - block/blk-core.c|1462| <<blk_rq_bio_prep>> rq->nr_phys_segments = bio_phys_segments(q, bio);
+	 *   - block/blk-core.c|1464| <<blk_rq_bio_prep>> rq->nr_phys_segments = 1;
+	 *   - block/blk-core.c|1551| <<__blk_rq_prep_clone>> dst->nr_phys_segments = src->nr_phys_segments;
+	 *   - block/blk-merge.c|354| <<blk_recalc_rq_segments>> rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
+	 *   - block/blk-merge.c|554| <<ll_new_hw_segment>> req->nr_phys_segments += nr_phys_segs;
+	 *   - block/blk-merge.c|616| <<req_attempt_discard_merge>> req->nr_phys_segments = segments + blk_rq_nr_discard_segments(next);
+	 *   - block/blk-merge.c|638| <<ll_merge_requests_fn>> total_phys_segments = req->nr_phys_segments + next->nr_phys_segments;
+	 *   - block/blk-merge.c|646| <<ll_merge_requests_fn>> req->nr_phys_segments = total_phys_segments;
+	 *   - block/blk-mq.c|494| <<blk_mq_rq_ctx_init>> rq->nr_phys_segments = 0;
+	 *   - block/blk-mq.c|956| <<blk_mq_start_request>> rq->nr_phys_segments++;
+	 *   - block/blk-mq.c|980| <<__blk_mq_requeue_request>> rq->nr_phys_segments--;
+	 */
 	BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
 
 	switch (req_op(req)) {
@@ -324,6 +432,10 @@ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 			return BLK_STS_RESOURCE;
 	}
 
+	/*
+	 * map a request to scatterlist, return number of sg entries setup. Caller
+	 * must make sure sg can hold rq->nr_phys_segments entries
+	 */
 	num = blk_rq_map_sg(hctx->queue, req, vbr->sg);
 	if (num) {
 		if (rq_data_dir(req) == WRITE)
@@ -359,6 +471,10 @@ static blk_status_t virtio_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 /* return id (s/n) string for *disk to *id_str
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|480| <<serial_show>> err = virtblk_get_id(disk, buf);
+ */
 static int virtblk_get_id(struct gendisk *disk, char *id_str)
 {
 	struct virtio_blk *vblk = disk->private_data;
@@ -382,6 +498,13 @@ static int virtblk_get_id(struct gendisk *disk, char *id_str)
 }
 
 /* We provide getgeo only to please some old bootloader/partitioning tools */
+/*
+ * called by:
+ *   - block/compat_ioctl.c|68| <<compat_hdio_getgeo>> ret = disk->fops->getgeo(bdev, &geo);
+ *   - block/ioctl.c|477| <<blkdev_getgeo>> ret = disk->fops->getgeo(bdev, &geo);
+ *
+ * struct block_device_operations virtblk_fops.getgeo = virtblk_getgeo()
+ */
 static int virtblk_getgeo(struct block_device *bd, struct hd_geometry *geo)
 {
 	struct virtio_blk *vblk = bd->bd_disk->private_data;
@@ -419,6 +542,10 @@ static int minor_to_index(int minor)
 	return minor >> PART_BITS;
 }
 
+/*
+ * # cat /sys/block/vda/serial 
+ * 0xabcdef
+ */
 static ssize_t serial_show(struct device *dev,
 			   struct device_attribute *attr, char *buf)
 {
@@ -442,6 +569,11 @@ static ssize_t serial_show(struct device *dev,
 static DEVICE_ATTR_RO(serial);
 
 /* The queue's logical block size must be set before calling this */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|536| <<virtblk_config_changed_work>> virtblk_update_capacity(vblk, true);
+ *   - drivers/block/virtio_blk.c|973| <<virtblk_probe>> virtblk_update_capacity(vblk, false);
+ */
 static void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)
 {
 	struct virtio_device *vdev = vblk->vdev;
@@ -467,6 +599,10 @@ static void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)
 	string_get_size(nblocks, queue_logical_block_size(q),
 			STRING_UNITS_10, cap_str_10, sizeof(cap_str_10));
 
+	/*
+	 * 启动的例子:
+	 * [    0.621051] virtio_blk virtio0: [vda] 12288000 512-byte logical blocks (6.29 GB/5.86 GiB)
+	 */
 	dev_notice(&vdev->dev,
 		   "[%s] %s%llu %d-byte logical blocks (%s/%s)\n",
 		   vblk->disk->disk_name,
@@ -479,6 +615,15 @@ static void virtblk_update_capacity(struct virtio_blk *vblk, bool resize)
 	set_capacity(vblk->disk, capacity);
 }
 
+/*
+ * 在以下触发:
+ *   - drivers/block/virtio_blk.c|576| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+ *   - drivers/block/virtio_blk.c|1049| <<virtblk_remove>> flush_work(&vblk->config_work);
+ *   - drivers/block/virtio_blk.c|1082| <<virtblk_freeze>> flush_work(&vblk->config_work);
+ *
+ * 初始化为virtblk_config_changed_work():
+ *   - drivers/block/virtio_blk.c|880| <<virtblk_probe>> INIT_WORK(&vblk->config_work, virtblk_config_changed_work);
+ */
 static void virtblk_config_changed_work(struct work_struct *work)
 {
 	struct virtio_blk *vblk =
@@ -490,6 +635,12 @@ static void virtblk_config_changed_work(struct work_struct *work)
 	kobject_uevent_env(&disk_to_dev(vblk->disk)->kobj, KOBJ_CHANGE, envp);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|131| <<__virtio_config_changed>> drv->config_changed(dev);
+ *
+ * struct virtio_driver virtio_blk.config_changed = virtblk_config_changed()
+ */
 static void virtblk_config_changed(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -497,6 +648,11 @@ static void virtblk_config_changed(struct virtio_device *vdev)
 	queue_work(virtblk_wq, &vblk->config_work);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|804| <<virtblk_probe>> err = init_vq(vblk);
+ *   - drivers/block/virtio_blk.c|1011| <<virtblk_restore>> ret = init_vq(vdev->priv);
+ */
 static int init_vq(struct virtio_blk *vblk)
 {
 	int err;
@@ -558,6 +714,10 @@ static int init_vq(struct virtio_blk *vblk)
  * Legacy naming scheme used for virtio devices.  We are stuck with it for
  * virtio blk but don't ever use it for any new driver.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|879| <<virtblk_probe>> virtblk_name_format("vd", index, vblk->disk->disk_name, DISK_NAME_LEN);
+ */
 static int virtblk_name_format(char *prefix, int index, char *buf, int buflen)
 {
 	const int base = 'z' - 'a' + 1;
@@ -582,6 +742,11 @@ static int virtblk_name_format(char *prefix, int index, char *buf, int buflen)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|662| <<virtblk_update_cache_mode>> u8 writeback = virtblk_get_cache_mode(vdev);
+ *   - drivers/block/virtio_blk.c|697| <<cache_type_show>> u8 writeback = virtblk_get_cache_mode(vblk->vdev);
+ */
 static int virtblk_get_cache_mode(struct virtio_device *vdev)
 {
 	u8 writeback;
@@ -601,12 +766,30 @@ static int virtblk_get_cache_mode(struct virtio_device *vdev)
 	return writeback;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|688| <<cache_type_store>> virtblk_update_cache_mode(vdev);
+ *   - drivers/block/virtio_blk.c|889| <<virtblk_probe>> virtblk_update_cache_mode(vdev);
+ *
+ * 如果virtblk backend支持VIRTIO_BLK_F_CONFIG_WCE或者VIRTIO_BLK_F_FLUSH
+ * 则为q->queue_flags设置QUEUE_FLAG_WC
+ * 永远为q->queue_flags清空QUEUE_FLAG_FUA (virtblk代码就这样)
+ */
 static void virtblk_update_cache_mode(struct virtio_device *vdev)
 {
 	u8 writeback = virtblk_get_cache_mode(vdev);
 	struct virtio_blk *vblk = vdev->priv;
 
+	/*
+	 * writeback是true, 则为q->queue_flags设置QUEUE_FLAG_WC
+	 * 此外, 为q->queue_flags清空QUEUE_FLAG_FUA
+	 */
 	blk_queue_write_cache(vblk->disk->queue, writeback, false);
+	/*
+	 * This routine is a wrapper for lower-level driver's revalidate_disk
+	 * call-backs.  It is used to do common pre and post operations needed
+	 * for all revalidate_disk operations.
+	 */
 	revalidate_disk(vblk->disk);
 }
 
@@ -614,6 +797,11 @@ static const char *const virtblk_cache_types[] = {
 	"write through", "write back"
 };
 
+/*
+ * # echo "write through" > /sys/block/vda/cache_type
+ * # cat /sys/block/vda/cache_type 
+ * write through
+ */
 static ssize_t
 cache_type_store(struct device *dev, struct device_attribute *attr,
 		 const char *buf, size_t count)
@@ -633,11 +821,20 @@ cache_type_store(struct device *dev, struct device_attribute *attr,
 	return count;
 }
 
+/*
+ * # cat /sys/block/vda/cache_type 
+ * write back
+ */
 static ssize_t
 cache_type_show(struct device *dev, struct device_attribute *attr, char *buf)
 {
 	struct gendisk *disk = dev_to_disk(dev);
 	struct virtio_blk *vblk = disk->private_data;
+	/*
+	 * writeback返回0(false)或者1(true)
+	 * 0: virtblk_cache_types[0] = "write through"
+	 * 1: virtblk_cache_types[1] = "write back"
+	 */
 	u8 writeback = virtblk_get_cache_mode(vblk->vdev);
 
 	BUG_ON(writeback >= ARRAY_SIZE(virtblk_cache_types));
@@ -652,6 +849,12 @@ static struct attribute *virtblk_attrs[] = {
 	NULL,
 };
 
+/*
+ * called by:
+ *   - fs/sysfs/group.c|53| <<create_files>> mode = grp->is_visible(kobj, *attr, i);
+ *
+ * struct attribute_group virtblk_attr_group.is_visible = virtblk_attrs_are_visible()
+ */
 static umode_t virtblk_attrs_are_visible(struct kobject *kobj,
 		struct attribute *a, int n)
 {
@@ -672,11 +875,21 @@ static const struct attribute_group virtblk_attr_group = {
 	.is_visible = virtblk_attrs_are_visible,
 };
 
+/*
+ * used by:
+ *   - drivers/block/virtio_blk.c|976| <<virtblk_probe>> device_add_disk(&vdev->dev, vblk->disk, virtblk_attr_groups);
+ */
 static const struct attribute_group *virtblk_attr_groups[] = {
 	&virtblk_attr_group,
 	NULL,
 };
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2812| <<blk_mq_init_request>> ret = set->ops->init_request(set, rq, hctx_idx, node);
+ *
+ * struct blk_mq_ops virtio_mq_ops.init_request = virtblk_init_request()
+ */
 static int virtblk_init_request(struct blk_mq_tag_set *set, struct request *rq,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -690,6 +903,13 @@ static int virtblk_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by (不知道第二个是怎么回事):
+ *   - block/blk-mq.c|3776| <<blk_mq_update_queue_map>> return set->ops->map_queues(set);
+ *   - drivers/scsi/scsi_lib.c|1759| <<scsi_map_queues>> return shost->hostt->map_queues(shost);
+ *
+ * struct blk_mq_ops virtio_mq_ops.map_queues = virtblk_map_queues()
+ */
 static int virtblk_map_queues(struct blk_mq_tag_set *set)
 {
 	struct virtio_blk *vblk = set->driver_data;
@@ -699,6 +919,12 @@ static int virtblk_map_queues(struct blk_mq_tag_set *set)
 }
 
 #ifdef CONFIG_VIRTIO_BLK_SCSI
+/*
+ * called by:
+ *   - block/blk-core.c|541| <<blk_get_request>> q->mq_ops->initialize_rq_fn(req);
+ *
+ * struct blk_mq_ops virtio_mq_ops.initialize_rq_fn = virtblk_initialize_rq()
+ */
 static void virtblk_initialize_rq(struct request *req)
 {
 	struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
@@ -721,6 +947,24 @@ static const struct blk_mq_ops virtio_mq_ops = {
 static unsigned int virtblk_queue_depth;
 module_param_named(queue_depth, virtblk_queue_depth, uint, 0444);
 
+/*
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * struct virtio_driver virtio_blk.probe = virtblk_probe()
+ */
 static int virtblk_probe(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk;
@@ -761,8 +1005,19 @@ static int virtblk_probe(struct virtio_device *vdev)
 	}
 
 	vblk->vdev = vdev;
+	/*
+	 * 主要在下面使用sg_elems:
+	 *   - drivers/block/virtio_blk.c|358| <<virtio_queue_rq>> BUG_ON(req->nr_phys_segments + 2 > vblk->sg_elems);
+	 *   - drivers/block/virtio_blk.c|831| <<virtblk_init_request>> sg_init_table(vbr->sg, vblk->sg_elems);
+	 */
 	vblk->sg_elems = sg_elems;
 
+	/*
+	 * 在以下触发:
+	 *   - drivers/block/virtio_blk.c|576| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+	 *   - drivers/block/virtio_blk.c|1049| <<virtblk_remove>> flush_work(&vblk->config_work);
+	 *   - drivers/block/virtio_blk.c|1082| <<virtblk_freeze>> flush_work(&vblk->config_work);
+	 */
 	INIT_WORK(&vblk->config_work, virtblk_config_changed_work);
 
 	err = init_vq(vblk);
@@ -818,6 +1073,11 @@ static int virtblk_probe(struct virtio_device *vdev)
 	vblk->index = index;
 
 	/* configure queue flush support */
+	/*
+	 * 如果virtblk backend支持VIRTIO_BLK_F_CONFIG_WCE或者VIRTIO_BLK_F_FLUSH
+	 * 则为q->queue_flags设置QUEUE_FLAG_WC
+	 * 永远为q->queue_flags清空QUEUE_FLAG_FUA (virtblk代码就这样)
+	 */
 	virtblk_update_cache_mode(vdev);
 
 	/* If disk is read-only in the host, the guest should obey */
@@ -922,6 +1182,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	return err;
 }
 
+/*
+ * struct virtio_driver virtio_blk.remove = virtblk_remove()
+ */
 static void virtblk_remove(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -951,6 +1214,9 @@ static void virtblk_remove(struct virtio_device *vdev)
 }
 
 #ifdef CONFIG_PM_SLEEP
+/*
+ * struct virtio_driver virtio_blk.freeze = virtblk_freeze()
+ */
 static int virtblk_freeze(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -967,6 +1233,9 @@ static int virtblk_freeze(struct virtio_device *vdev)
 	return 0;
 }
 
+/*
+ * struct virtio_driver virtio_blk.restore = virtblk_restore()
+ */
 static int virtblk_restore(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
@@ -983,11 +1252,13 @@ static int virtblk_restore(struct virtio_device *vdev)
 }
 #endif
 
+/* struct virtio_driver virtio_blk.id_table = id_table */
 static const struct virtio_device_id id_table[] = {
 	{ VIRTIO_ID_BLOCK, VIRTIO_DEV_ANY_ID },
 	{ 0 },
 };
 
+/* struct virtio_driver virtio_blk.feature_table_legacy */
 static unsigned int features_legacy[] = {
 	VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,
 	VIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,
@@ -998,6 +1269,7 @@ static unsigned int features_legacy[] = {
 	VIRTIO_BLK_F_MQ, VIRTIO_BLK_F_DISCARD, VIRTIO_BLK_F_WRITE_ZEROES,
 }
 ;
+/* struct virtio_driver virtio_blk.feature_table = features */
 static unsigned int features[] = {
 	VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,
 	VIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,
@@ -1026,6 +1298,11 @@ static int __init init(void)
 {
 	int error;
 
+	/*
+	 * 在以下使用和分配:
+	 *   - drivers/block/virtio_blk.c|501| <<virtblk_config_changed>> queue_work(virtblk_wq, &vblk->config_work);
+	 *   - drivers/block/virtio_blk.c|1033| <<init>> virtblk_wq = alloc_workqueue("virtio-blk", 0, 0);
+	 */
 	virtblk_wq = alloc_workqueue("virtio-blk", 0, 0);
 	if (!virtblk_wq)
 		return -ENOMEM;
diff --git a/drivers/md/dm-linear.c b/drivers/md/dm-linear.c
index ad980a3..a93f5ef 100644
--- a/drivers/md/dm-linear.c
+++ b/drivers/md/dm-linear.c
@@ -26,6 +26,9 @@ struct linear_c {
 /*
  * Construct a linear mapping: <dev_path> <offset>
  */
+/*
+ * struct target_type linear_target.ctrl = linear_ctrl()
+ */
 static int linear_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 {
 	struct linear_c *lc;
@@ -70,6 +73,9 @@ static int linear_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 	return ret;
 }
 
+/*
+ * struct target_type linear_target.dtr = linear_dtr()
+ */
 static void linear_dtr(struct dm_target *ti)
 {
 	struct linear_c *lc = (struct linear_c *) ti->private;
@@ -95,6 +101,12 @@ static void linear_map_bio(struct dm_target *ti, struct bio *bio)
 			linear_map_sector(ti, bio->bi_iter.bi_sector);
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|1285| <<__map_bio>> r = ti->type->map(ti, clone);
+ *
+ * struct target_type linear_target.map = linear_map()
+ */
 static int linear_map(struct dm_target *ti, struct bio *bio)
 {
 	linear_map_bio(ti, bio);
diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index 5475081..5bfdb23 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -1262,6 +1262,11 @@ void dm_remap_zone_report(struct dm_target *ti, sector_t start,
 }
 EXPORT_SYMBOL_GPL(dm_remap_zone_report);
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|1406| <<__clone_and_map_simple_bio>> return __map_bio(tio);
+ *   - drivers/md/dm.c|1461| <<__clone_and_map_data_bio>> (void ) __map_bio(tio);
+ */
 static blk_qc_t __map_bio(struct dm_target_io *tio)
 {
 	int r;
@@ -1444,6 +1449,10 @@ static int __send_empty_flush(struct clone_info *ci)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|1585| <<__split_and_process_non_flush>> r = __clone_and_map_data_bio(ci, ti, ci->sector, &len);
+ */
 static int __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti,
 				    sector_t sector, unsigned *len)
 {
@@ -1567,6 +1576,11 @@ static bool __process_abnormal_io(struct clone_info *ci, struct dm_target *ti,
 /*
  * Select the correct strategy for processing a non-flush bio.
  */
+/*
+ * called by:
+ *   - drivers/md/dm.c|1635| <<__split_and_process_bio>> error = __split_and_process_non_flush(&ci);
+ *   - drivers/md/dm.c|1640| <<__split_and_process_bio>> error = __split_and_process_non_flush(&ci);
+ */
 static int __split_and_process_non_flush(struct clone_info *ci)
 {
 	struct dm_target *ti;
@@ -1592,6 +1606,10 @@ static int __split_and_process_non_flush(struct clone_info *ci)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|1773| <<dm_process_bio>> return __split_and_process_bio(md, map, bio);
+ */
 static void init_clone_info(struct clone_info *ci, struct mapped_device *md,
 			    struct dm_table *map, struct bio *bio)
 {
@@ -1737,6 +1755,11 @@ static void dm_queue_split(struct mapped_device *md, struct dm_target *ti, struc
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|1796| <<dm_make_request>> ret = dm_process_bio(md, map, bio);
+ *   - drivers/md/dm.c|2475| <<dm_wq_work>> (void ) dm_process_bio(md, map, c);
+ */
 static blk_qc_t dm_process_bio(struct mapped_device *md,
 			       struct dm_table *map, struct bio *bio)
 {
@@ -1773,6 +1796,10 @@ static blk_qc_t dm_process_bio(struct mapped_device *md,
 		return __split_and_process_bio(md, map, bio);
 }
 
+/*
+ * used by:
+ *   - drivers/md/dm.c|2289| <<dm_setup_md_queue>> blk_queue_make_request(md->queue, dm_make_request);
+ */
 static blk_qc_t dm_make_request(struct request_queue *q, struct bio *bio)
 {
 	struct mapped_device *md = q->queuedata;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 120fb59..6f4241a 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -108,11 +108,26 @@ static void nvme_set_queue_dying(struct nvme_ns *ns)
 	blk_mq_unquiesce_queue(ns->queue);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1356| <<nvme_passthru_end>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|2833| <<nvme_dev_ioctl>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|2867| <<nvme_sysfs_rescan>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|3732| <<nvme_handle_aen_notice>> nvme_queue_scan(ctrl);
+ *   - drivers/nvme/host/core.c|3800| <<nvme_start_ctrl>> nvme_queue_scan(ctrl);
+ */
 static void nvme_queue_scan(struct nvme_ctrl *ctrl)
 {
 	/*
 	 * Only new queue scan work when admin and IO queues are both alive
 	 */
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/core.c|117| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3578| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3808| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|2808| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	if (ctrl->state == NVME_CTRL_LIVE)
 		queue_work(nvme_wq, &ctrl->scan_work);
 }
@@ -127,12 +142,29 @@ int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_reset_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2793| <<nvme_dev_ioctl>> return nvme_reset_ctrl_sync(ctrl);
+ *   - drivers/nvme/host/core.c|2818| <<nvme_sysfs_reset>> ret = nvme_reset_ctrl_sync(ctrl);
+ *   - drivers/nvme/host/pci.c|2807| <<nvme_async_probe>> nvme_reset_ctrl_sync(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2923| <<nvme_reset_done>> nvme_reset_ctrl_sync(&dev->ctrl);
+ *
+ * pci上调用nvme_reset_work()
+ */
 int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
 {
 	int ret;
 
 	ret = nvme_reset_ctrl(ctrl);
 	if (!ret) {
+		/*
+		 * 在以下设置reset_work:
+		 *   - drivers/nvme/host/pci.c|2861| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+		 *   - drivers/nvme/host/fc.c|3059| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+		 *   - drivers/nvme/host/rdma.c|1992| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+		 *   - drivers/nvme/host/tcp.c|2223| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+		 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+		 */
 		flush_work(&ctrl->reset_work);
 		if (ctrl->state != NVME_CTRL_LIVE &&
 		    ctrl->state != NVME_CTRL_ADMIN_ONLY)
@@ -257,6 +289,17 @@ static void nvme_retry_req(struct request *req)
 	blk_mq_delay_kick_requeue_list(req->q, delay);
 }
 
+/*
+ * 在以下使用:
+ *   - struct blk_mq_ops nvme_tcp_mq_ops.complete = 
+ *   - struct blk_mq_ops nvme_tcp_admin_mq_ops.complete =
+ *   - drivers/nvme/host/fabrics.c|557| <<nvmf_fail_nonready_command>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/fc.c|2373| <<nvme_fc_complete_rq>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/pci.c|946| <<nvme_pci_complete_rq>> nvme_complete_rq(req);
+ *   - drivers/nvme/host/rdma.c|1789| <<nvme_rdma_complete_rq>> nvme_complete_rq(rq);
+ *   - drivers/nvme/host/trace.h|121| <<__field>> TRACE_EVENT(nvme_complete_rq,
+ *   - drivers/nvme/target/loop.c|81| <<nvme_loop_complete_rq>> nvme_complete_rq(req);
+ */
 void nvme_complete_rq(struct request *req)
 {
 	blk_status_t status = nvme_error_status(req);
@@ -282,11 +325,27 @@ void nvme_complete_rq(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_complete_rq);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2412| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2413| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|906| <<nvme_rdma_teardown_admin_queue>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/host/rdma.c|919| <<nvme_rdma_teardown_io_queues>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1746| <<nvme_tcp_teardown_admin_queue>> nvme_cancel_request, ctrl);
+ *   - drivers/nvme/host/tcp.c|1760| <<nvme_tcp_teardown_io_queues>> nvme_cancel_request, ctrl);
+ *   - drivers/nvme/target/loop.c|409| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|418| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ */
 bool nvme_cancel_request(struct request *req, void *data, bool reserved)
 {
 	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
 				"Cancelling I/O %d", req->tag);
 
+	/*
+	 * 这里cancel的时候似乎注意?
+	 * 不要cancel已经complete(中断执行了)但是没有调用complete()的地方??
+	 */
+
 	nvme_req(req)->status = NVME_SC_ABORT_REQ;
 	blk_mq_complete_request_sync(req);
 	return true;
@@ -416,6 +475,11 @@ static void nvme_put_ns(struct nvme_ns *ns)
 	kref_put(&ns->kref, nvme_free_ns);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|503| <<nvme_alloc_request>> nvme_clear_nvme_request(req);
+ *   - drivers/nvme/host/core.c|772| <<nvme_setup_cmd>> nvme_clear_nvme_request(req);
+ */
 static inline void nvme_clear_nvme_request(struct request *req)
 {
 	if (!(req->rq_flags & RQF_DONTPREP)) {
@@ -624,6 +688,13 @@ static inline blk_status_t nvme_setup_write_zeroes(struct nvme_ns *ns,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|755| <<nvme_setup_cmd>> ret = nvme_setup_rw(ns, req, cmd);
+ *
+ * 对于rw, nvme command的length是以512-byte (sector的大小) 为单位的
+ * 所以, 对于所指向的prp们, 从第一个page的offset开始, 后面必须是连续的就可以了
+ */
 static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 		struct request *req, struct nvme_command *cmnd)
 {
@@ -642,11 +713,26 @@ static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
 	cmnd->rw.nsid = cpu_to_le32(ns->head->ns_id);
 	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+	/*
+	 * 修改lba_shift的地方:
+	 *   - drivers/nvme/host/core.c|1745| <<__nvme_revalidate_disk>> ns->lba_shift = id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ds;
+	 *   - drivers/nvme/host/core.c|1747| <<__nvme_revalidate_disk>> ns->lba_shift = 9;
+	 *   - drivers/nvme/host/core.c|3397| <<nvme_alloc_ns>> ns->lba_shift = 9;
+	 *
+	 * qemu下nvme的lba_shift是9, 也就是512
+	 *
+	 * number of logical blocks
+	 * 从0开始
+	 */
 	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
 
 	if (req_op(req) == REQ_OP_WRITE && ctrl->nr_streams)
 		nvme_assign_write_stream(ctrl, req, &control, &dsmgmt);
 
+	/*
+	 * 设置ms的地方:
+	 *   - drivers/nvme/host/core.c|1763| <<__nvme_revalidate_disk>> ns->ms = le16_to_cpu(id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ms);
+	 */
 	if (ns->ms) {
 		/*
 		 * If formated with metadata, the block layer always provides a
@@ -701,6 +787,14 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2313| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|1009| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|1734| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2056| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|145| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
@@ -746,6 +840,10 @@ static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
 	complete(waiting);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|818| <<__nvme_submit_sync_cmd>> nvme_execute_rq_polled(req->q, NULL, req, at_head);
+ */
 static void nvme_execute_rq_polled(struct request_queue *q,
 		struct gendisk *bd_disk, struct request *rq, int at_head)
 {
@@ -984,6 +1082,11 @@ void nvme_stop_keep_alive(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_keep_alive);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2635| <<nvme_init_identify>> ret = nvme_identify_ctrl(ctrl, &id);
+ *   - drivers/nvme/host/core.c|3613| <<nvme_scan_work>> if (nvme_identify_ctrl(ctrl, &id))
+ */
 static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
 {
 	struct nvme_command c = { };
@@ -1132,6 +1235,15 @@ static int nvme_set_features(struct nvme_ctrl *dev, unsigned fid, unsigned dword
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2420| <<nvme_fc_create_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/fc.c|2494| <<nvme_fc_recreate_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2230| <<nvme_setup_io_queues>> result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/rdma.c|651| <<nvme_rdma_alloc_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ *   - drivers/nvme/host/tcp.c|1603| <<nvme_tcp_alloc_io_queues>> ret = nvme_set_queue_count(ctrl, &nr_io_queues);
+ *   - drivers/nvme/target/loop.c|297| <<nvme_loop_init_io_queues>> ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+ */
 int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count)
 {
 	u32 q_count = (*count - 1) | ((*count - 1) << 16);
@@ -1898,6 +2010,13 @@ static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
  * bits', but doing so may cause the device to complete commands to the
  * admin queue ... and we don't know what memory that might be pointing at!
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1493| <<nvme_disable_admin_queue>> nvme_disable_ctrl(&dev->ctrl, dev->ctrl.cap);
+ *   - drivers/nvme/host/pci.c|1771| <<nvme_pci_configure_admin_queue>> result = nvme_disable_ctrl(&dev->ctrl, dev->ctrl.cap);
+ *   - drivers/nvme/host/rdma.c|1868| <<nvme_rdma_shutdown_ctrl>> nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ *   - drivers/nvme/host/tcp.c|1891| <<nvme_tcp_teardown_ctrl>> nvme_disable_ctrl(ctrl, ctrl->cap);
+ */
 int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
 {
 	int ret;
@@ -1916,6 +2035,14 @@ int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
 }
 EXPORT_SYMBOL_GPL(nvme_disable_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2649| <<nvme_fc_create_association>> ret = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ *   - drivers/nvme/host/pci.c|1803| <<nvme_pci_configure_admin_queue>> result = nvme_enable_ctrl(&dev->ctrl, dev->ctrl.cap);
+ *   - drivers/nvme/host/rdma.c|817| <<nvme_rdma_configure_admin_queue>> error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ *   - drivers/nvme/host/tcp.c|1716| <<nvme_tcp_configure_admin_queue>> error = nvme_enable_ctrl(ctrl, ctrl->cap);
+ *   - drivers/nvme/target/loop.c|382| <<nvme_loop_configure_admin_queue>> error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+ */
 int nvme_enable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
 {
 	/*
@@ -1979,6 +2106,11 @@ int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_shutdown_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2651| <<nvme_init_identify>> nvme_set_queue_limits(ctrl, ctrl->admin_q);
+ *   - drivers/nvme/host/core.c|3343| <<nvme_alloc_ns>> nvme_set_queue_limits(ctrl, ns->queue);
+ */
 static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 		struct request_queue *q)
 {
@@ -1995,6 +2127,10 @@ static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
 	if ((ctrl->quirks & NVME_QUIRK_STRIPE_SIZE) &&
 	    is_power_of_2(ctrl->max_hw_sectors))
 		blk_queue_chunk_sectors(q, ctrl->max_hw_sectors);
+	/*
+	 * 设置ctrl->page_size的地方:
+	 *   - drivers/nvme/host/core.c|2059| <<nvme_enable_ctrl>> ctrl->page_size = 1 << page_shift;
+	 */
 	blk_queue_virt_boundary(q, ctrl->page_size - 1);
 	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)
 		vwc = true;
@@ -2459,6 +2595,10 @@ static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return 0;
 
 out_put_subsystem:
+	/*
+	 * 这里有一个bug, nvme_put_subsystem()已经调用了device_put()
+	 * 下面还调用一次
+	 */
 	nvme_put_subsystem(subsys);
 out_unlock:
 	mutex_unlock(&nvme_subsystems_lock);
@@ -3093,6 +3233,10 @@ static int __nvme_check_ids(struct nvme_subsystem *subsys,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3193| <<nvme_init_ns_head>> head = nvme_alloc_ns_head(ctrl, nsid, id);
+ */
 static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 		unsigned nsid, struct nvme_id_ns *id)
 {
@@ -3147,6 +3291,10 @@ static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3314| <<nvme_alloc_ns>> ret = nvme_init_ns_head(ns, nsid, id);
+ */
 static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
 		struct nvme_id_ns *id)
 {
@@ -3238,6 +3386,10 @@ static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3409| <<nvme_validate_ns>> nvme_alloc_ns(ctrl, nsid);
+ */
 static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
@@ -3365,6 +3517,11 @@ static void nvme_ns_remove(struct nvme_ns *ns)
 	nvme_put_ns(ns);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3452| <<nvme_scan_ns_list>> nvme_validate_ns(ctrl, nsid);
+ *   - drivers/nvme/host/core.c|3476| <<nvme_scan_ns_sequential>> nvme_validate_ns(ctrl, i);
+ */
 static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 {
 	struct nvme_ns *ns;
@@ -3396,6 +3553,10 @@ static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|3620| <<nvme_scan_work>> if (!nvme_scan_ns_list(ctrl, nn))
+ */
 static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 {
 	struct nvme_ns *ns;
@@ -3437,6 +3598,10 @@ static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3565| <<nvme_scan_work>> nvme_scan_ns_sequential(ctrl, nn);
+ */
 static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl, unsigned nn)
 {
 	unsigned i;
@@ -3472,6 +3637,13 @@ static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
 	kfree(log);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/core.c|117| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|3578| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+ *   - drivers/nvme/host/core.c|3808| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+ *   - drivers/nvme/host/pci.c|2808| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+ */
 static void nvme_scan_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl =
@@ -3494,6 +3666,9 @@ static void nvme_scan_work(struct work_struct *work)
 
 	mutex_lock(&ctrl->scan_lock);
 	nn = le32_to_cpu(id->nn);
+	/*
+	 * 似乎qemu的controller是设置NVME_QUIRK_IDENTIFY_CNS
+	 */
 	if (ctrl->vs >= NVME_VS(1, 1, 0) &&
 	    !(ctrl->quirks & NVME_QUIRK_IDENTIFY_CNS)) {
 		if (!nvme_scan_ns_list(ctrl, nn))
@@ -3686,6 +3861,15 @@ void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2710| <<nvme_fc_create_association>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2709| <<nvme_reset_work>> nvme_start_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1012| <<nvme_rdma_setup_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|1825| <<nvme_tcp_setup_ctrl>> nvme_start_ctrl(ctrl);
+ *   - drivers/nvme/target/loop.c|474| <<nvme_loop_reset_ctrl_work>> nvme_start_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|627| <<nvme_loop_create_ctrl>> nvme_start_ctrl(&ctrl->ctrl);
+ */
 void nvme_start_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (ctrl->kato)
@@ -3736,6 +3920,14 @@ static void nvme_free_ctrl(struct device *dev)
  * earliest initialization so that we have the initialized structured around
  * during probing.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3113| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+ *   - drivers/nvme/host/pci.c|2882| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+ *   - drivers/nvme/host/tcp.c|2265| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+ *   - drivers/nvme/target/loop.c|579| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+ */
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks)
 {
@@ -3908,6 +4100,7 @@ void nvme_sync_queues(struct nvme_ctrl *ctrl)
 	struct nvme_ns *ns;
 
 	down_read(&ctrl->namespaces_rwsem);
+	/* cancel any pending callbacks on a queue */
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		blk_sync_queue(ns->queue);
 	up_read(&ctrl->namespaces_rwsem);
diff --git a/drivers/nvme/host/fault_inject.c b/drivers/nvme/host/fault_inject.c
index 4cfd2c9..876385d 100644
--- a/drivers/nvme/host/fault_inject.c
+++ b/drivers/nvme/host/fault_inject.c
@@ -15,6 +15,10 @@ static DECLARE_FAULT_ATTR(fail_default_attr);
 static char *fail_request;
 module_param(fail_request, charp, 0000);
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|3438| <<nvme_alloc_ns>> nvme_fault_inject_init(ns);
+ */
 void nvme_fault_inject_init(struct nvme_ns *ns)
 {
 	struct dentry *dir, *parent;
diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 499acf0..a065fbf 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -289,6 +289,10 @@ static void nvme_requeue_work(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3162| <<nvme_alloc_ns_head>> ret = nvme_mpath_alloc_disk(ctrl, head);
+ */
 int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 {
 	struct request_queue *q;
@@ -581,8 +585,23 @@ static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
 	return 0;
 }
 
+/*
+ * [0] nvme_mpath_add_disk
+ * [0] nvme_validate_ns
+ * [0] nvme_scan_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|3363| <<nvme_alloc_ns>> nvme_mpath_add_disk(ns, id);
+ */
 void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 {
+	/*
+	 * 在qemu上测的nvme_ctrl_use_ana(ns->ctrl)是0
+	 */
 	if (nvme_ctrl_use_ana(ns->ctrl)) {
 		mutex_lock(&ns->ctrl->ana_lock);
 		ns->ana_grpid = le32_to_cpu(id->anagrpid);
@@ -610,6 +629,18 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	put_disk(head->disk);
 }
 
+/*
+ * [0] nvme_mpath_init
+ * [0] nvme_init_identify
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|2685| <<nvme_init_identify>> ret = nvme_mpath_init(ctrl, id);
+ */
 int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	int error;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 55553d2..3037a9f 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -165,6 +165,22 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/pci.c|2861| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/fc.c|3059| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/rdma.c|1992| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2223| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 *
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|124| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|158| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3150| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2974| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|3059| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 
@@ -176,11 +192,33 @@ struct nvme_ctrl {
 	char name[12];
 	u16 cntlid;
 
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/core.c|1947| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	 *   - drivers/nvme/host/core.c|1948| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|1950| <<nvme_disable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|1980| <<nvme_enable_ctrl>> ctrl->ctrl_config = NVME_CC_CSS_NVM;
+	 *   - drivers/nvme/host/core.c|1981| <<nvme_enable_ctrl>> ctrl->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
+	 *   - drivers/nvme/host/core.c|1982| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_AMS_RR | NVME_CC_SHN_NONE;
+	 *   - drivers/nvme/host/core.c|1983| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
+	 *   - drivers/nvme/host/core.c|1984| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|1986| <<nvme_enable_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|1999| <<nvme_shutdown_ctrl>> ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	 *   - drivers/nvme/host/core.c|2000| <<nvme_shutdown_ctrl>> ctrl->ctrl_config |= NVME_CC_SHN_NORMAL;
+	 *   - drivers/nvme/host/core.c|2002| <<nvme_shutdown_ctrl>> ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	 *   - drivers/nvme/host/core.c|3641| <<nvme_ctrl_pp_status>> return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
+	 *   - drivers/nvme/host/pci.c|2591| <<nvme_reset_work>> bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
+	 *   - drivers/nvme/host/pci.c|2602| <<nvme_reset_work>> if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
+	 */
 	u32 ctrl_config;
 	u16 mtfa;
 	u32 queue_count;
 
 	u64 cap;
+	/*
+	 * 设置ctrl->page_size的地方:
+	 *   - drivers/nvme/host/core.c|2059| <<nvme_enable_ctrl>> ctrl->page_size = 1 << page_shift;
+	 */
 	u32 page_size;
 	u32 max_hw_sectors;
 	u32 max_segments;
@@ -193,6 +231,10 @@ struct nvme_ctrl {
 	atomic_t abort_limit;
 	u8 vwc;
 	u32 vs;
+	/*
+	 * 更新sgls的地方:
+	 *   - drivers/nvme/host/core.c|2709| <<nvme_init_identify>> ctrl->sgls = le32_to_cpu(id->sgls);
+	 */
 	u32 sgls;
 	u16 kas;
 	u8 npss;
@@ -206,6 +248,13 @@ struct nvme_ctrl {
 	unsigned long quirks;
 	struct nvme_id_power_state psd[32];
 	struct nvme_effects_log *effects;
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/core.c|117| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3578| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3808| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|2808| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	struct work_struct scan_work;
 	struct work_struct async_event_work;
 	struct delayed_work ka_work;
@@ -337,7 +386,17 @@ struct nvme_ns {
 	struct kref kref;
 	struct nvme_ns_head *head;
 
+	/*
+	 * 修改lba_shift的地方:
+	 *   - drivers/nvme/host/core.c|1745| <<__nvme_revalidate_disk>> ns->lba_shift = id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ds;
+	 *   - drivers/nvme/host/core.c|1747| <<__nvme_revalidate_disk>> ns->lba_shift = 9;
+	 *   - drivers/nvme/host/core.c|3397| <<nvme_alloc_ns>> ns->lba_shift = 9;
+	 */
 	int lba_shift;
+	/*
+	 * 设置ms的地方:
+	 *   - drivers/nvme/host/core.c|1763| <<__nvme_revalidate_disk>> ns->ms = le16_to_cpu(id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ms);
+	 */
 	u16 ms;
 	u16 sgs;
 	u32 sws;
@@ -388,11 +447,28 @@ static inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)
 	return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|606| <<nvme_setup_discard>> u64 slba = nvme_block_nr(ns, bio->bi_iter.bi_sector);
+ *   - drivers/nvme/host/core.c|647| <<nvme_setup_write_zeroes>> cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+ *   - drivers/nvme/host/core.c|671| <<nvme_setup_rw>> cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+ */
 static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
 {
 	return (sector >> (ns->lba_shift - 9));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|1678| <<nvme_fc_fcpio_done>> nvme_end_request(rq, status, result);
+ *   - drivers/nvme/host/pci.c|992| <<nvme_handle_cqe>> nvme_end_request(req, cqe->status, cqe->result);
+ *   - drivers/nvme/host/rdma.c|1106| <<nvme_rdma_inv_rkey_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1320| <<nvme_rdma_send_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1463| <<nvme_rdma_process_nvme_rsp>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/tcp.c|440| <<nvme_tcp_process_nvme_cqe>> nvme_end_request(rq, cqe->status, cqe->result);
+ *   - drivers/nvme/host/tcp.c|634| <<nvme_tcp_end_request>> nvme_end_request(rq, cpu_to_le16(status << 1), res);
+ *   - drivers/nvme/target/loop.c|120| <<nvme_loop_queue_response>> nvme_end_request(rq, cqe->status, cqe->result);
+ */
 static inline void nvme_end_request(struct request *req, __le16 status,
 		union nvme_result result)
 {
@@ -405,11 +481,41 @@ static inline void nvme_end_request(struct request *req, __le16 status,
 	blk_mq_complete_request(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|185| <<nvme_delete_ctrl_sync>> nvme_get_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|1440| <<nvme_ioctl>> nvme_get_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/core.c|3367| <<nvme_alloc_ns>> nvme_get_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|3130| <<nvme_fc_init_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2559| <<nvme_remove_dead_ctrl>> nvme_get_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2864| <<nvme_probe>> nvme_get_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|2020| <<nvme_rdma_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2282| <<nvme_tcp_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|618| <<nvme_loop_create_ctrl>> nvme_get_ctrl(&ctrl->ctrl);
+ */
 static inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)
 {
 	get_device(ctrl->device);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|156| <<nvme_do_delete_ctrl>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|190| <<nvme_delete_ctrl_sync>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|437| <<nvme_free_ns>> nvme_put_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/core.c|1448| <<nvme_ioctl>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/fabrics.c|1126| <<nvmf_dev_release>> nvme_put_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|3133| <<nvme_fc_init_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/fc.c|3160| <<nvme_fc_init_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/pci.c|2563| <<nvme_remove_dead_ctrl>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2698| <<nvme_remove_dead_ctrl_work>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2796| <<nvme_async_probe>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2942| <<nvme_remove>> nvme_put_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|2030| <<nvme_rdma_create_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/tcp.c|2292| <<nvme_tcp_create_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|485| <<nvme_loop_reset_ctrl_work>> nvme_put_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|638| <<nvme_loop_create_ctrl>> nvme_put_ctrl(&ctrl->ctrl);
+ */
 static inline void nvme_put_ctrl(struct nvme_ctrl *ctrl)
 {
 	put_device(ctrl->device);
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 524d6bd..9e025d6 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -36,7 +36,17 @@
  * These can be higher, but we need to ensure that any command doesn't
  * require an sg allocation that needs more than a page of data.
  */
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|3017| <<nvme_reset_work>> dev->ctrl.max_hw_sectors = NVME_MAX_KB_SZ << 1;
+ *   - drivers/nvme/host/pci.c|3284| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 #define NVME_MAX_KB_SZ	4096
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|2968| <<nvme_reset_work>> dev->ctrl.max_segments = NVME_MAX_SEGS;
+ *   - drivers/nvme/host/pci.c|3235| <<nvme_probe>> NVME_MAX_SEGS, true);
+ */
 #define NVME_MAX_SEGS	127
 
 static int use_threaded_interrupts;
@@ -98,25 +108,86 @@ struct nvme_dev {
 	struct blk_mq_tag_set admin_tagset;
 	u32 __iomem *dbs;
 	struct device *dev;
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|596| <<nvme_unmap_data>> dma_pool_free(dev->prp_page_pool, addr, dma_addr);
+	 *   - drivers/nvme/host/pci.c|658| <<nvme_pci_setup_prps>> pool = dev->prp_page_pool;
+	 *   - drivers/nvme/host/pci.c|753| <<nvme_pci_setup_sgls>> pool = dev->prp_page_pool;
+	 *   - drivers/nvme/host/pci.c|2463| <<nvme_setup_prp_pools>> dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
+	 *   - drivers/nvme/host/pci.c|2465| <<nvme_setup_prp_pools>> if (!dev->prp_page_pool)
+	 *   - drivers/nvme/host/pci.c|2472| <<nvme_setup_prp_pools>> dma_pool_destroy(dev->prp_page_pool);
+	 *   - drivers/nvme/host/pci.c|2480| <<nvme_release_prp_pools>> dma_pool_destroy(dev->prp_page_pool);
+	 *
+	 * 大小是PAGE_SIZE (4096)
+	 */
 	struct dma_pool *prp_page_pool;
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|579| <<nvme_unmap_data>> dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
+	 *   - drivers/nvme/host/pci.c|655| <<nvme_pci_setup_prps>> pool = dev->prp_small_pool;
+	 *   - drivers/nvme/host/pci.c|750| <<nvme_pci_setup_sgls>> pool = dev->prp_small_pool;
+	 *   - drivers/nvme/host/pci.c|2469| <<nvme_setup_prp_pools>> dev->prp_small_pool = dma_pool_create("prp list 256", dev->dev,
+	 *   - drivers/nvme/host/pci.c|2471| <<nvme_setup_prp_pools>> if (!dev->prp_small_pool) {
+	 *   - drivers/nvme/host/pci.c|2481| <<nvme_release_prp_pools>> dma_pool_destroy(dev->prp_small_pool);
+	 *
+	 * 大小是256
+	 */
 	struct dma_pool *prp_small_pool;
+	/*
+	 * 修改online_queues的地方:
+	 *   - drivers/nvme/host/pci.c|1574| <<nvme_suspend_queue>> nvmeq->dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|1721| <<nvme_init_queue>> dev->online_queues++;
+	 *   - drivers/nvme/host/pci.c|1781| <<nvme_create_queue>> dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|1935| <<nvme_pci_configure_admin_queue>> dev->online_queues--;
+	 */
 	unsigned online_queues;
+	/*
+	 * 修改的地方:
+	 *   - drivers/nvme/host/pci.c|2507| <<nvme_setup_io_queues>> dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
+	 */
 	unsigned max_qid;
 	unsigned io_queues[HCTX_MAX_TYPES];
 	unsigned int num_vecs;
 	int q_depth;
+	/*
+	 * 获取db_stride的地方:
+	 *   - drivers/nvme/host/pci.c|2619| <<nvme_pci_enable>> dev->db_stride = 1 << NVME_CAP_STRIDE(dev->ctrl.cap);
+	 */
 	u32 db_stride;
 	void __iomem *bar;
 	unsigned long bar_mapped_size;
 	struct work_struct remove_work;
 	struct mutex shutdown_lock;
 	bool subsystem;
+	/*
+	 * 设置cmb_size的地方:
+	 *   - drivers/nvme/host/pci.c|2048| <<nvme_map_cmb>> dev->cmb_size = size;
+	 *   - drivers/nvme/host/pci.c|2063| <<nvme_release_cmb>> if (dev->cmb_size) {
+	 *   - drivers/nvme/host/pci.c|2066| <<nvme_release_cmb>> dev->cmb_size = 0;
+	 */
 	u64 cmb_size;
+	/*
+	 * 设置cmb_use_sqes的地方:
+	 *   - drivers/nvme/host/pci.c|1905| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2202| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	bool cmb_use_sqes;
 	u32 cmbsz;
 	u32 cmbloc;
 	struct nvme_ctrl ctrl;
 
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|626| <<nvme_unmap_data>> mempool_free(iod->sg, dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|879| <<nvme_map_data>> iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
+	 *   - drivers/nvme/host/pci.c|2551| <<nvme_pci_free_ctrl>> mempool_destroy(dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|2848| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 *   - drivers/nvme/host/pci.c|2852| <<nvme_probe>> if (!dev->iod_mempool) {
+	 *   - drivers/nvme/host/pci.c|2870| <<nvme_probe>> mempool_destroy(dev->iod_mempool);
+	 *
+	 * 因为nvme_pci_iod_alloc_size()只有一处调用, size=4096, nseg=127, use_sgl=true,
+	 * 所以返回127个struct nvme_sgl_desc加上127个struct scatterlist
+	 */
 	mempool_t *iod_mempool;
 
 	/* shadow doorbell buffer support: */
@@ -126,6 +197,10 @@ struct nvme_dev {
 	dma_addr_t dbbuf_eis_dma_addr;
 
 	/* host memory buffer support: */
+	/*
+	 * host_mem_size设置的唯一地方:
+	 *   - drivers/nvme/host/pci.c|2161| <<__nvme_alloc_host_mem>> dev->host_mem_size = size;
+	 */
 	u64 host_mem_size;
 	u32 nr_host_mem_descs;
 	dma_addr_t host_mem_descs_dma;
@@ -157,11 +232,21 @@ static int queue_count_set(const char *val, const struct kernel_param *kp)
 	return param_set_int(val, kp);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|367| <<nvme_dbbuf_init>> nvmeq->dbbuf_sq_db = &dev->dbbuf_dbs[sq_idx(qid, dev->db_stride)];
+ *   - drivers/nvme/host/pci.c|369| <<nvme_dbbuf_init>> nvmeq->dbbuf_sq_ei = &dev->dbbuf_eis[sq_idx(qid, dev->db_stride)];
+ */
 static inline unsigned int sq_idx(unsigned int qid, u32 stride)
 {
 	return qid * 2 * stride;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|368| <<nvme_dbbuf_init>> nvmeq->dbbuf_cq_db = &dev->dbbuf_dbs[cq_idx(qid, dev->db_stride)];
+ *   - drivers/nvme/host/pci.c|370| <<nvme_dbbuf_init>> nvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];
+ */
 static inline unsigned int cq_idx(unsigned int qid, u32 stride)
 {
 	return (qid * 2 + 1) * stride;
@@ -184,7 +269,28 @@ struct nvme_queue {
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
 	volatile struct nvme_completion *cqes;
 	struct blk_mq_tags **tags;
+	/*
+	 * 在以下修改sq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1534| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+	 *   - drivers/nvme/host/pci.c|1543| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+	 *
+	 * 在以下使用sq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1254| <<adapter_alloc_sq>> c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1449| <<nvme_free_queue>> nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1536| <<nvme_alloc_sq_cmds>> if (nvmeq->sq_dma_addr) {
+	 *   - drivers/nvme/host/pci.c|1789| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+	 */
 	dma_addr_t sq_dma_addr;
+	/*
+	 * 在以下修改cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1568| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 *
+	 * 在以下使用cq_dma_addr:
+	 *   - drivers/nvme/host/pci.c|1235| <<adapter_alloc_cq>> c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1451| <<nvme_free_queue>> (void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1589| <<nvme_alloc_queue>> nvmeq->cq_dma_addr);
+	 *   - drivers/nvme/host/pci.c|1801| <<nvme_pci_configure_admin_queue>> lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+	 */
 	dma_addr_t cq_dma_addr;
 	u32 __iomem *q_db;
 	u16 q_depth;
@@ -196,9 +302,22 @@ struct nvme_queue {
 	u16 qid;
 	u8 cq_phase;
 	unsigned long flags;
+/*
+ * 修改NVMEQ_ENABLED的地方:
+ *   - drivers/nvme/host/pci.c|1490| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1680| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|1842| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|2237| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+ *   - drivers/nvme/host/pci.c|2285| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+ */
 #define NVMEQ_ENABLED		0
 #define NVMEQ_SQ_CMB		1
 #define NVMEQ_DELETE_ERROR	2
+/*
+ * 修改NVMEQ_POLLED的地方:
+ *   - drivers/nvme/host/pci.c|1577| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1744| <<nvme_create_queue>> set_bit(NVMEQ_POLLED, &nvmeq->flags);
+ */
 #define NVMEQ_POLLED		3
 	u32 *dbbuf_sq_db;
 	u32 *dbbuf_cq_db;
@@ -216,8 +335,28 @@ struct nvme_queue {
 struct nvme_iod {
 	struct nvme_request req;
 	struct nvme_queue *nvmeq;
+	/*
+	 * 使用的地方:
+	 *   - drivers/nvme/host/pci.c|579| <<nvme_unmap_data>> if (iod->use_sgl) {
+	 *   - drivers/nvme/host/pci.c|860| <<nvme_map_data>> iod->use_sgl = nvme_pci_use_sgls(dev, req);
+	 *   - drivers/nvme/host/pci.c|861| <<nvme_map_data>> if (iod->use_sgl)
+	 */
 	bool use_sgl;
 	int aborted;
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|800| <<nvme_unmap_data>> if (iod->npages == 0)
+	 *   - drivers/nvme/host/pci.c|804| <<nvme_unmap_data>> for (i = 0; i < iod->npages; i++) {  
+	 *   - drivers/nvme/host/pci.c|901| <<nvme_pci_setup_prps>> iod->npages = 0;
+	 *   - drivers/nvme/host/pci.c|907| <<nvme_pci_setup_prps>> iod->npages = 1;
+	 *   - drivers/nvme/host/pci.c|913| <<nvme_pci_setup_prps>> iod->npages = -1;
+	 *   - drivers/nvme/host/pci.c|925| <<nvme_pci_setup_prps>> list[iod->npages++] = prp_list;
+	 *   - drivers/nvme/host/pci.c|1022| <<nvme_pci_setup_sgls>> iod->npages = 0;
+	 *   - drivers/nvme/host/pci.c|1026| <<nvme_pci_setup_sgls>> iod->npages = 1;
+	 *   - drivers/nvme/host/pci.c|1031| <<nvme_pci_setup_sgls>> iod->npages = -1;
+	 *   - drivers/nvme/host/pci.c|1050| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[iod->npages++] = sg_list;
+	 *   - drivers/nvme/host/pci.c|1237| <<nvme_queue_rq>> iod->npages = -1;
+	 */
 	int npages;		/* In the PRP list. 0 means small pool in use */
 	int nents;		/* Used in scatterlist */
 	dma_addr_t first_dma;
@@ -231,17 +370,35 @@ static unsigned int max_io_queues(void)
 	return num_possible_cpus() + write_queues + poll_queues;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|248| <<nvme_dbbuf_size>> return (max_queue_count() * 8 * stride);
+ *   - drivers/nvme/host/pci.c|2758| <<nvme_probe>> dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
+ *
+ * 返回num_possible_cpus() + write_queues + poll_queues + 1 (admin queue)
+ */
 static unsigned int max_queue_count(void)
 {
 	/* IO queues + admin queue */
 	return 1 + max_io_queues();
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|322| <<nvme_dbbuf_dma_alloc>> unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
+ *   - drivers/nvme/host/pci.c|347| <<nvme_dbbuf_dma_free>> unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
+ */
 static inline unsigned int nvme_dbbuf_size(u32 stride)
 {
 	return (max_queue_count() * 8 * stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2892| <<nvme_reset_work>> result = nvme_dbbuf_dma_alloc(dev);
+ *
+ * shadow doorbell buffer support
+ */
 static int nvme_dbbuf_dma_alloc(struct nvme_dev *dev)
 {
 	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
@@ -295,6 +452,10 @@ static void nvme_dbbuf_init(struct nvme_dev *dev,
 	nvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2602| <<nvme_dev_add>> nvme_dbbuf_set(dev);
+ */
 static void nvme_dbbuf_set(struct nvme_dev *dev)
 {
 	struct nvme_command c;
@@ -314,12 +475,21 @@ static void nvme_dbbuf_set(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|461| <<nvme_dbbuf_update_and_check_event>> if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
+ */
 static inline int nvme_dbbuf_need_event(u16 event_idx, u16 new_idx, u16 old)
 {
 	return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
 }
 
 /* Update dbbuf and return true if an MMIO is required */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|628| <<nvme_write_sq_db>> if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
+ *   - drivers/nvme/host/pci.c|1174| <<nvme_ring_cq_doorbell>> if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
+ */
 static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
 					      volatile u32 *dbbuf_ei)
 {
@@ -355,6 +525,10 @@ static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
  * as it only leads to a small amount of wasted memory for the lifetime of
  * the I/O.
  */
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|423| <<nvme_pci_iod_alloc_size>> alloc_size = sizeof(__le64 *) * nvme_npages(size, dev);
+ */
 static int nvme_npages(unsigned size, struct nvme_dev *dev)
 {
 	unsigned nprps = DIV_ROUND_UP(size + dev->ctrl.page_size,
@@ -366,11 +540,23 @@ static int nvme_npages(unsigned size, struct nvme_dev *dev)
  * Calculates the number of pages needed for the SGL segments. For example a 4k
  * page can accommodate 256 SGL descriptors.
  */
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|421| <<nvme_pci_iod_alloc_size>> alloc_size = sizeof(__le64 *) * nvme_pci_npages_sgl(nseg);
+ */
 static int nvme_pci_npages_sgl(unsigned int num_seg)
 {
 	return DIV_ROUND_UP(num_seg * sizeof(struct nvme_sgl_desc), PAGE_SIZE);
 }
 
+/*
+ * 只有一处调用:
+ *   - drivers/nvme/host/pci.c|3064| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ *                                                                                 NVME_MAX_SEGS, true);
+ *
+ * 因为nvme_pci_iod_alloc_size()只有一处调用, size=4096, nseg=127, use_sgl=true,
+ * 所以返回127个struct nvme_sgl_desc加上127个struct scatterlist
+ */
 static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 		unsigned int size, unsigned int nseg, bool use_sgl)
 {
@@ -384,6 +570,9 @@ static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 	return alloc_size + sizeof(struct scatterlist) * nseg;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.init_hctx = nvme_admin_init_hctx()
+ */
 static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 				unsigned int hctx_idx)
 {
@@ -399,6 +588,9 @@ static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.exit_hctx = nvme_admin_exit_hctx()
+ */
 static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -406,6 +598,9 @@ static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_i
 	nvmeq->tags = NULL;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.init_hctx = nvme_init_hctx()
+ */
 static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 			  unsigned int hctx_idx)
 {
@@ -420,6 +615,10 @@ static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.init_request = nvme_init_request()
+ * struct blk_mq_ops nvme_mq_ops.init_request = nvme_init_request()
+ */
 static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
 		unsigned int hctx_idx, unsigned int numa_node)
 {
@@ -435,6 +634,10 @@ static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
 	return 0;
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|602| <<nvme_pci_map_queues>> offset = queue_irq_offset(dev);
+ */
 static int queue_irq_offset(struct nvme_dev *dev)
 {
 	/* if we have more than 1 vec, admin queue offsets us by 1 */
@@ -444,6 +647,9 @@ static int queue_irq_offset(struct nvme_dev *dev)
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.map_queues = nvme_pci_map_queues()
+ */
 static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_dev *dev = set->driver_data;
@@ -478,6 +684,11 @@ static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 /*
  * Write sq tail if we are asked to, or if the next command would wrap.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|594| <<nvme_submit_cmd>> nvme_write_sq_db(nvmeq, write_sq);
+ *   - drivers/nvme/host/pci.c|604| <<nvme_commit_rqs>> nvme_write_sq_db(nvmeq, true);
+ */
 static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 {
 	if (!write_sq) {
@@ -501,17 +712,28 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
  * @cmd: The command to send
  * @write_sq: whether to write to the SQ doorbell
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1026| <<nvme_queue_rq>> nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+ *   - drivers/nvme/host/pci.c|1224| <<nvme_pci_submit_async_event>> nvme_submit_cmd(nvmeq, &c, true);
+ */
 static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
 	spin_lock(&nvmeq->sq_lock);
 	memcpy(&nvmeq->sq_cmds[nvmeq->sq_tail], cmd, sizeof(*cmd));
+	/*
+	 * 增加sq_tail
+	 */
 	if (++nvmeq->sq_tail == nvmeq->q_depth)
 		nvmeq->sq_tail = 0;
 	nvme_write_sq_db(nvmeq, write_sq);
 	spin_unlock(&nvmeq->sq_lock);
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.commit_rqs = nvme_commit_rqs()
+ */
 static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -522,12 +744,33 @@ static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
 	spin_unlock(&nvmeq->sq_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|670| <<nvme_unmap_data>> dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
+ *   - drivers/nvme/host/pci.c|674| <<nvme_unmap_data>> void *addr = nvme_pci_iod_list(req)[i];
+ *   - drivers/nvme/host/pci.c|725| <<nvme_pci_setup_prps>> void **list = nvme_pci_iod_list(req);
+ *   - drivers/nvme/host/pci.c|863| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[0] = sg_list;
+ *   - drivers/nvme/host/pci.c|878| <<nvme_pci_setup_sgls>> nvme_pci_iod_list(req)[iod->npages++] = sg_list;
+ */
 static void **nvme_pci_iod_list(struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	/*
+	 * nvme_iod的sg是struct scatterlist *sg;
+	 */
 	return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|975| <<nvme_map_data>> iod->use_sgl = nvme_pci_use_sgls(dev, req);
+ *
+ * 什么时候不使用sgl?
+ * 1. dev->ctrl.sgls不支持
+ * 2. qid是0的时候 (admin queue)
+ * 3. 内核参数sgl_threshold没有设置的时候
+ * 4. 平均segment的大小小于sgl_threshold的时候 (avg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);)
+ */
 static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -539,6 +782,10 @@ static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 
 	avg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);
 
+	/*
+	 * 更新sgls的地方:
+	 *   - drivers/nvme/host/core.c|2709| <<nvme_init_identify>> ctrl->sgls = le32_to_cpu(id->sgls);
+	 */
 	if (!(dev->ctrl.sgls & ((1 << 0) | (1 << 1))))
 		return false;
 	if (!iod->nvmeq->qid)
@@ -548,6 +795,12 @@ static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|994| <<nvme_map_data>> nvme_unmap_data(dev, req);
+ *   - drivers/nvme/host/pci.c|1069| <<nvme_queue_rq>> nvme_unmap_data(dev, req);
+ *   - drivers/nvme/host/pci.c|1089| <<nvme_pci_complete_rq>> nvme_unmap_data(dev, req);
+ */
 static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -594,6 +847,10 @@ static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
 	mempool_free(iod->sg, dev->iod_mempool);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|916| <<nvme_pci_setup_prps>> WARN(DO_ONCE(nvme_print_sgl, iod->sg, iod->nents),
+ */
 static void nvme_print_sgl(struct scatterlist *sgl, int nents)
 {
 	int i;
@@ -608,15 +865,27 @@ static void nvme_print_sgl(struct scatterlist *sgl, int nents)
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|900| <<nvme_map_data>> ret = nvme_pci_setup_prps(dev, req, &cmnd->rw);
+ *
+ * 对于rw, nvme command的length是以512-byte (sector的大小) 为单位的
+ * 所以, 对于所指向的prp们, 从第一个page的offset开始, 后面必须是连续的就可以了
+ */
 static blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 	struct dma_pool *pool;
 	int length = blk_rq_payload_bytes(req);
+	/* iod->sg类型是struct scatterlist *sg */
 	struct scatterlist *sg = iod->sg;
 	int dma_len = sg_dma_len(sg);
 	u64 dma_addr = sg_dma_address(sg);
+	/*
+	 * 设置ctrl->page_size的地方:
+	 *   - drivers/nvme/host/core.c|2059| <<nvme_enable_ctrl>> ctrl->page_size = 1 << page_shift;
+	 */
 	u32 page_size = dev->ctrl.page_size;
 	int offset = dma_addr & (page_size - 1);
 	__le64 *prp_list;
@@ -624,51 +893,99 @@ static blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,
 	dma_addr_t prp_dma;
 	int nprps, i;
 
+	/*
+	 * offset是第一个sg的
+	 *
+	 * 这里的length减少在第一个page中的length
+	 */
 	length -= (page_size - offset);
 	if (length <= 0) {
 		iod->first_dma = 0;
 		goto done;
 	}
 
+	/*
+	 * dma_len是第一个sg的len, 减少在第一个page中的length
+	 */
 	dma_len -= (page_size - offset);
 	if (dma_len) {
+		/*
+		 * 如果第一个sg用了多个page, 就进入下一个page
+		 */
 		dma_addr += (page_size - offset);
 	} else {
+		/*
+		 * 如果第一个sg没用多个page, 就进入下一个sg
+		 */
 		sg = sg_next(sg);
 		dma_addr = sg_dma_address(sg);
 		dma_len = sg_dma_len(sg);
 	}
 
+	/*
+	 * 如果这个条件满足, 说明不管几个sg, 只有两个page
+	 * goto done可以把第一个放入prp1, 第二个放入prp2
+	 */
 	if (length <= page_size) {
 		iod->first_dma = dma_addr;
 		goto done;
 	}
 
+	/*
+	 * 到这里的时候length是不包含第一个page的那些部分的
+	 *
+	 * 不包含似乎因为要把第一个sg(的第一个page)放入prp1
+	 * 剩下的page们放入prp2
+	 */
 	nprps = DIV_ROUND_UP(length, page_size);
+	/* 确认用哪个pool可以装下nprps个entry */
 	if (nprps <= (256 / 8)) {
+		/*
+		 * 大小是256
+		 */
 		pool = dev->prp_small_pool;
 		iod->npages = 0;
 	} else {
+		/*
+		 * 大小是PAGE_SIZE (4096)
+		 */
 		pool = dev->prp_page_pool;
 		iod->npages = 1;
 	}
 
+	/*
+	 * prp_list的类型是__le64 *
+	 * 用于存放除第一个sg(的第一个page)之外的page们
+	 */
 	prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
 	if (!prp_list) {
 		iod->first_dma = dma_addr;
 		iod->npages = -1;
 		return BLK_STS_RESOURCE;
 	}
+	/*
+	 * 上面void **list = nvme_pci_iod_list(req);
+	 */
 	list[0] = prp_list;
 	iod->first_dma = prp_dma;
 	i = 0;
 	for (;;) {
+		/*
+		 * 如果一个page不够用了
+		 */
 		if (i == page_size >> 3) {
 			__le64 *old_prp_list = prp_list;
 			prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
 			if (!prp_list)
 				return BLK_STS_RESOURCE;
 			list[iod->npages++] = prp_list;
+			/*
+			 * 假设page_size是4096, 此时i是512 (最大index是511)
+			 * 下面2行代码中
+			 *
+			 * 第一行让新的prp page的第一个entry复制上一个prp page的最后(第511)个entry
+			 * 因为第二行代码会让上一个的prp page的最后(第511)个entry指向新的prp page
+			 */
 			prp_list[0] = old_prp_list[i - 1];
 			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
 			i = 1;
@@ -679,6 +996,7 @@ static blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,
 		length -= page_size;
 		if (length <= 0)
 			break;
+		/* dma_len > 0 说明一个sg包含多个page */
 		if (dma_len > 0)
 			continue;
 		if (unlikely(dma_len < 0))
@@ -701,17 +1019,41 @@ static blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,
 	return BLK_STS_IOERR;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|845| <<nvme_pci_setup_sgls>> nvme_pci_sgl_set_data(&cmd->dptr.sgl, sg);
+ *   - drivers/nvme/host/pci.c|883| <<nvme_pci_setup_sgls>> nvme_pci_sgl_set_data(&sg_list[i++], sg);
+ *
+ * 根据参数的sg把struct nvme_sgl_desc *sge设置成NVME_SGL_FMT_DATA_DESC (data block descriptor)
+ */
 static void nvme_pci_sgl_set_data(struct nvme_sgl_desc *sge,
 		struct scatterlist *sg)
 {
 	sge->addr = cpu_to_le64(sg_dma_address(sg));
 	sge->length = cpu_to_le32(sg_dma_len(sg));
+	/*
+	 * For struct nvme_sgl_desc:
+	 *   @NVME_SGL_FMT_DATA_DESC:           data block descriptor
+	 *   @NVME_SGL_FMT_SEG_DESC:            sgl segment descriptor
+	 *   @NVME_SGL_FMT_LAST_SEG_DESC:       last sgl segment descriptor
+	 */
 	sge->type = NVME_SGL_FMT_DATA_DESC << 4;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1023| <<nvme_pci_setup_sgls>> nvme_pci_sgl_set_seg(&cmd->dptr.sgl, sgl_dma, entries);
+ *   - drivers/nvme/host/pci.c|1037| <<nvme_pci_setup_sgls>> nvme_pci_sgl_set_seg(link, sgl_dma, entries);
+ */
 static void nvme_pci_sgl_set_seg(struct nvme_sgl_desc *sge,
 		dma_addr_t dma_addr, int entries)
 {
+	/*
+	 * For struct nvme_sgl_desc:
+	 *   @NVME_SGL_FMT_DATA_DESC:           data block descriptor
+	 *   @NVME_SGL_FMT_SEG_DESC:            sgl segment descriptor
+	 *   @NVME_SGL_FMT_LAST_SEG_DESC:       last sgl segment descriptor
+	 */
 	sge->addr = cpu_to_le64(dma_addr);
 	if (entries < SGES_PER_PAGE) {
 		sge->length = cpu_to_le32(entries * sizeof(*sge));
@@ -722,6 +1064,10 @@ static void nvme_pci_sgl_set_seg(struct nvme_sgl_desc *sge,
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|868| <<nvme_map_data>> ret = nvme_pci_setup_sgls(dev, req, &cmnd->rw, nr_mapped);
+ */
 static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmd, int entries)
 {
@@ -736,14 +1082,26 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 	cmd->flags = NVME_CMD_SGL_METABUF;
 
 	if (entries == 1) {
+		/*
+		 * struct nvme_sgl_desc {
+		 *     __le64  addr;
+		 *     __le32  length;
+		 *     __u8    rsvd[3];
+		 *     __u8    type;
+		 * };
+		 *
+		 * 根据参数的sg把struct nvme_sgl_desc *sge设置成NVME_SGL_FMT_DATA_DESC (data block descriptor)
+		 */
 		nvme_pci_sgl_set_data(&cmd->dptr.sgl, sg);
 		return BLK_STS_OK;
 	}
 
 	if (entries <= (256 / sizeof(struct nvme_sgl_desc))) {
+		/* 大小是256 */
 		pool = dev->prp_small_pool;
 		iod->npages = 0;
 	} else {
+		/* 大小是PAGE_SIZE (4096) */
 		pool = dev->prp_page_pool;
 		iod->npages = 1;
 	}
@@ -754,12 +1112,19 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 		return BLK_STS_RESOURCE;
 	}
 
+	/*
+	 * sg_list是分配的dma sg entry (相当于xen的indirect) 的虚拟地址
+	 * sgl_dma是dma地址
+	 */
 	nvme_pci_iod_list(req)[0] = sg_list;
 	iod->first_dma = sgl_dma;
 
 	nvme_pci_sgl_set_seg(&cmd->dptr.sgl, sgl_dma, entries);
 
 	do {
+		/*
+		 * "i == SGES_PER_PAGE"是一个list放不下的情况
+		 */
 		if (i == SGES_PER_PAGE) {
 			struct nvme_sgl_desc *old_sg_desc = sg_list;
 			struct nvme_sgl_desc *link = &old_sg_desc[i - 1];
@@ -774,6 +1139,7 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 			nvme_pci_sgl_set_seg(link, sgl_dma, entries);
 		}
 
+		/* 根据参数的sg把struct nvme_sgl_desc *sge设置成NVME_SGL_FMT_DATA_DESC (data block descriptor) */
 		nvme_pci_sgl_set_data(&sg_list[i++], sg);
 		sg = sg_next(sg);
 	} while (--entries > 0);
@@ -781,13 +1147,27 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1081| <<nvme_map_data>> return nvme_setup_prp_simple(dev, req,
+ *
+ * 最多支持2个page,
+ * 一个放在prp1, 一个放在prp2
+ */
 static blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd,
 		struct bio_vec *bv)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	/*
+	 * 设置ctrl->page_size的地方:
+	 *   - drivers/nvme/host/core.c|2059| <<nvme_enable_ctrl>> ctrl->page_size = 1 << page_shift;
+	 */
 	unsigned int first_prp_len = dev->ctrl.page_size - bv->bv_offset;
 
+	/*
+	 * first_dma是dma map (比如swiotlb)后的返回地址
+	 */
 	iod->first_dma = dma_map_bvec(dev->dev, bv, rq_dma_dir(req), 0);
 	if (dma_mapping_error(dev->dev, iod->first_dma))
 		return BLK_STS_RESOURCE;
@@ -799,6 +1179,10 @@ static blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1086| <<nvme_map_data>> return nvme_setup_sgl_simple(dev, req,
+ */
 static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd,
 		struct bio_vec *bv)
@@ -810,6 +1194,13 @@ static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 		return BLK_STS_RESOURCE;
 	iod->dma_len = bv->bv_len;
 
+	/*
+	 * For struct nvme_sgl_desc:
+	 *   @NVME_SGL_FMT_DATA_DESC:           data block descriptor
+	 *   @NVME_SGL_FMT_SEG_DESC:            sgl segment descriptor
+	 *   @NVME_SGL_FMT_LAST_SEG_DESC:       last sgl segment descriptor
+	 */
+
 	cmnd->flags = NVME_CMD_SGL_METABUF;
 	cmnd->dptr.sgl.addr = cpu_to_le64(iod->first_dma);
 	cmnd->dptr.sgl.length = cpu_to_le32(iod->dma_len);
@@ -817,6 +1208,13 @@ static blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,
 	return 0;
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1014| <<nvme_queue_rq>> ret = nvme_map_data(dev, req, &cmnd);
+ *
+ * 对于rw, nvme command的length是以512-byte (sector的大小) 为单位的
+ * 所以, 对于所指向的prp们, 从第一个page的offset开始, 后面必须是连续的就可以了
+ */
 static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -825,13 +1223,24 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	int nr_mapped;
 
 	if (blk_rq_nr_phys_segments(req) == 1) {
+		/*
+		 * Return the first full biovec in the request.  The caller needs to check that
+		 * there are any bvecs before calling this helper.
+		 */
 		struct bio_vec bv = req_bvec(req);
 
 		if (!is_pci_p2pdma_page(bv.bv_page)) {
+			/* 虽然一个segment, 但是包含2个page以下 */
 			if (bv.bv_offset + bv.bv_len <= dev->ctrl.page_size * 2)
 				return nvme_setup_prp_simple(dev, req,
 							     &cmnd->rw, &bv);
 
+			/*
+			 * 更新sgls的地方:
+			 *   - drivers/nvme/host/core.c|2709| <<nvme_init_identify>> ctrl->sgls = le32_to_cpu(id->sgls);
+			 *
+			 * admin queue (0) 必须是prp
+			 */
 			if (iod->nvmeq->qid &&
 			    dev->ctrl.sgls & ((1 << 0) | (1 << 1)))
 				return nvme_setup_sgl_simple(dev, req,
@@ -840,10 +1249,30 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	}
 
 	iod->dma_len = 0;
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|626| <<nvme_unmap_data>> mempool_free(iod->sg, dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|879| <<nvme_map_data>> iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
+	 *   - drivers/nvme/host/pci.c|2551| <<nvme_pci_free_ctrl>> mempool_destroy(dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|2848| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 *   - drivers/nvme/host/pci.c|2852| <<nvme_probe>> if (!dev->iod_mempool) {
+	 *   - drivers/nvme/host/pci.c|2870| <<nvme_probe>> mempool_destroy(dev->iod_mempool);
+	 *
+	 * alloc_size是:
+	 * alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+	 *                                      NVME_MAX_SEGS, true);
+	 *
+	 * 因为nvme_pci_iod_alloc_size()只有一处调用, size=4096, nseg=127, use_sgl=true,
+	 * 所以返回127个struct nvme_sgl_desc加上127个struct scatterlist
+	 */
 	iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
 	if (!iod->sg)
 		return BLK_STS_RESOURCE;
 	sg_init_table(iod->sg, blk_rq_nr_phys_segments(req));
+	/*
+	 * map a request to scatterlist, return number of sg entries setup. Caller
+	 * must make sure sg can hold rq->nr_phys_segments entries
+	 */
 	iod->nents = blk_rq_map_sg(req->q, req, iod->sg);
 	if (!iod->nents)
 		goto out;
@@ -857,6 +1286,13 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	if (!nr_mapped)
 		goto out;
 
+	/*
+	 * 什么时候不使用sgl (nvme_pci_use_sgls()返回false)?
+	 * 1. dev->ctrl.sgls不支持
+	 * 2. qid是0的时候 (admin queue)
+	 * 3. 内核参数sgl_threshold没有设置的时候
+	 * 4. 平均segment的大小小于sgl_threshold的时候 (avg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);)
+	 */
 	iod->use_sgl = nvme_pci_use_sgls(dev, req);
 	if (iod->use_sgl)
 		ret = nvme_pci_setup_sgls(dev, req, &cmnd->rw, nr_mapped);
@@ -868,6 +1304,10 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1217| <<nvme_queue_rq>> ret = nvme_map_metadata(dev, req, &cmnd);
+ */
 static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -884,6 +1324,13 @@ static blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,
 /*
  * NOTE: ns is NULL when called on the admin queue.
  */
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.queue_rq = nvme_queue_rq()
+ * struct blk_mq_ops nvme_mq_ops.queue_rq = nvme_queue_rq()
+ *
+ * 对于rw, nvme command的length是以512-byte (sector的大小) 为单位的
+ * 所以, 对于所指向的prp们, 从第一个page的offset开始, 后面必须是连续的就可以了
+ */
 static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -891,6 +1338,10 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct nvme_queue *nvmeq = hctx->driver_data;
 	struct nvme_dev *dev = nvmeq->dev;
 	struct request *req = bd->rq;
+	/*
+	 * admin和io都是:
+	 *   dev->tagset.cmd_size = sizeof(struct nvme_iod)
+	 */
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
 	struct nvme_command cmnd;
 	blk_status_t ret;
@@ -903,6 +1354,15 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	 * We should not need to do this, but we're still using this to
 	 * ensure we can drain requests on a dying queue.
 	 */
+	/*
+	 * NVMEQ_ENABLED使用的地方:
+	 *   - drivers/nvme/host/pci.c|906| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 *   - drivers/nvme/host/pci.c|1398| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1578| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1727| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2118| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2166| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 */
 	if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
 		return BLK_STS_IOERR;
 
@@ -932,6 +1392,10 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.complete = nvme_pci_complete_rq()
+ * struct blk_mq_ops nvme_mq_ops.complete = nvme_pci_complete_rq()
+ */
 static void nvme_pci_complete_rq(struct request *req)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -962,6 +1426,10 @@ static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
 		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1231| <<nvme_complete_cqes>> nvme_handle_cqe(nvmeq, start);
+ */
 static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 {
 	volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
@@ -987,11 +1455,26 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 		return;
 	}
 
+	/*
+	 * 下发的时候设置command_id的地方:
+	 *   - drivers/nvme/host/core.c|829| <<nvme_setup_cmd>> cmd->common.command_id = req->tag;
+	 *   - drivers/nvme/host/fc.c|1784| <<nvme_fc_init_aen_ops>> sqe->common.command_id = NVME_AQ_BLK_MQ_DEPTH + i;
+	 *   - drivers/nvme/host/rdma.c|1409| <<nvme_rdma_submit_async_event>> cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+	 *   - drivers/nvme/host/rdma.c|1428| <<nvme_rdma_process_nvme_rsp>> rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
+	 *   - drivers/nvme/host/tcp.c|541| <<nvme_tcp_setup_h2c_data_pdu>> data->command_id = rq->tag;
+	 *   - drivers/nvme/host/tcp.c|1988| <<nvme_tcp_submit_async_event>> cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+	 */
 	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
 	trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
 	nvme_end_request(req, cqe->status, cqe->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1534| <<nvme_irq>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1574| <<nvme_poll_irqdisable>> nvme_complete_cqes(nvmeq, start, end);
+ *   - drivers/nvme/host/pci.c|1594| <<nvme_poll>> nvme_complete_cqes(nvmeq, start, end);
+ */
 static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
 {
 	while (start != end) {
@@ -1011,6 +1494,13 @@ static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1313| <<nvme_irq>> nvme_process_cq(nvmeq, &start, &end, -1);
+ *   - drivers/nvme/host/pci.c|1350| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1354| <<nvme_poll_irqdisable>> found = nvme_process_cq(nvmeq, &start, &end, tag);
+ *   - drivers/nvme/host/pci.c|1375| <<nvme_poll>> found = nvme_process_cq(nvmeq, &start, &end, -1);
+ */
 static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
 				  u16 *end, unsigned int tag)
 {
@@ -1091,6 +1581,9 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	return found;
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_ops.poll = nvme_poll()
+ */
 static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
@@ -1108,6 +1601,9 @@ static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 	return found;
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.submit_async_event = nvme_pci_submit_async_event()
+ */
 static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)
 {
 	struct nvme_dev *dev = to_nvme_dev(ctrl);
@@ -1250,6 +1746,10 @@ static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
 			 csts, result);
 }
 
+/*
+ * struct blk_mq_ops nvme_mq_admin_ops.timeout = nvme_timeout()
+ * struct blk_mq_ops nvme_mq_ops.timeout = nvme_timeout()
+ */
 static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
 {
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
@@ -1425,6 +1925,10 @@ static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
 	nvme_poll_irqdisable(nvmeq, -1);
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2288| <<nvme_setup_io_queues>> result = nvme_cmb_qdepth(dev, nr_io_queues,
+ */
 static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 				int entry_size)
 {
@@ -1454,6 +1958,11 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 
+	/*
+	 * 设置cmb_use_sqes的地方:
+	 *   - drivers/nvme/host/pci.c|1905| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2202| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
 		nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(depth));
 		nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
@@ -1471,6 +1980,11 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1791| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|1825| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+ */
 static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 {
 	struct nvme_queue *nvmeq = &dev->queues[qid];
@@ -1534,6 +2048,10 @@ static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 	wmb(); /* ensure the first interrupt sees the initialization */
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1869| <<nvme_create_io_queues>> ret = nvme_create_queue(&dev->queues[i], i, polled);
+ */
 static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1571,6 +2089,17 @@ static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 			goto release_sq;
 	}
 
+	/*
+	 * 修改NVMEQ_ENABLED的地方:
+	 *   - drivers/nvme/host/pci.c|1490| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1680| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1842| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2237| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2285| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *
+	 * 使用NVMEQ_ENABLED的地方:
+	 *   - drivers/nvme/host/pci.c|1006| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 */
 	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
 	return result;
 
@@ -1616,6 +2145,10 @@ static void nvme_dev_remove_admin(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2652| <<nvme_reset_work>> result = nvme_alloc_admin_tags(dev);
+ */
 static int nvme_alloc_admin_tags(struct nvme_dev *dev)
 {
 	if (!dev->ctrl.admin_q) {
@@ -1654,6 +2187,12 @@ static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
 	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1703| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2146| <<nvme_setup_io_queues>> result = nvme_remap_bar(dev, size);
+ *   - drivers/nvme/host/pci.c|2691| <<nvme_dev_map>> if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+ */
 static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -1708,6 +2247,9 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
 	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
 
+	/*
+	 * 关于pci, 只在这里调用!!!
+	 */
 	result = nvme_enable_ctrl(&dev->ctrl, dev->ctrl.cap);
 	if (result)
 		return result;
@@ -1724,6 +2266,10 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	return result;
 }
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|2287| <<nvme_setup_io_queues>> result = nvme_create_io_queues(dev);
+ */
 static int nvme_create_io_queues(struct nvme_dev *dev)
 {
 	unsigned i, max, rw_queues;
@@ -1784,6 +2330,10 @@ static u32 nvme_cmb_size(struct nvme_dev *dev)
 	return (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2423| <<nvme_pci_enable>> nvme_map_cmb(dev);
+ */
 static void nvme_map_cmb(struct nvme_dev *dev)
 {
 	u64 size, offset;
@@ -2096,6 +2646,10 @@ static void nvme_disable_io_queues(struct nvme_dev *dev)
 		__nvme_disable_io_queues(dev, nvme_admin_delete_cq);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|2735| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+ */
 static int nvme_setup_io_queues(struct nvme_dev *dev)
 {
 	struct nvme_queue *adminq = &dev->queues[0];
@@ -2103,6 +2657,7 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 	int result, nr_io_queues;
 	unsigned long size;
 
+	/* 返回num_possible_cpus() + write_queues + poll_queues, 不包括admin?? */
 	nr_io_queues = max_io_queues();
 	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
 	if (result < 0)
@@ -2110,9 +2665,25 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 
 	if (nr_io_queues == 0)
 		return 0;
-	
+
+	/*
+	 * 修改NVMEQ_ENABLED的地方:
+	 *   - drivers/nvme/host/pci.c|1490| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+	 *   - drivers/nvme/host/pci.c|1680| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|1842| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	 *   - drivers/nvme/host/pci.c|2237| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *   - drivers/nvme/host/pci.c|2285| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+	 *
+	 * 使用NVMEQ_ENABLED的地方:
+	 *   - drivers/nvme/host/pci.c|1006| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+	 */
 	clear_bit(NVMEQ_ENABLED, &adminq->flags);
 
+	/*
+	 * 设置cmb_use_sqes的地方:
+	 *   - drivers/nvme/host/pci.c|1905| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2202| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	if (dev->cmb_use_sqes) {
 		result = nvme_cmb_qdepth(dev, nr_io_queues,
 				sizeof(struct nvme_command));
@@ -2161,6 +2732,9 @@ static int nvme_setup_io_queues(struct nvme_dev *dev)
 		return result;
 	set_bit(NVMEQ_ENABLED, &adminq->flags);
 
+	/*
+	 * 只在这里被调用
+	 */
 	result = nvme_create_io_queues(dev);
 	if (result || dev->online_queues < 2)
 		return result;
@@ -2256,6 +2830,10 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 /*
  * return error value only when tagset allocation failed
  */
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2807| <<nvme_reset_work>> if (nvme_dev_add(dev))
+ */
 static int nvme_dev_add(struct nvme_dev *dev)
 {
 	int ret;
@@ -2374,6 +2952,20 @@ static void nvme_pci_disable(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1278| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1307| <<nvme_timeout>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|1325| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2476| <<nvme_remove_dead_ctrl>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2498| <<nvme_reset_work>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2794| <<nvme_reset_prepare>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2806| <<nvme_shutdown>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2823| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2830| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2847| <<nvme_suspend>> nvme_dev_disable(ndev, true);
+ *   - drivers/nvme/host/pci.c|2879| <<nvme_error_detected>> nvme_dev_disable(dev, false);
+ */
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 {
 	bool dead = true, freeze = false;
@@ -2427,12 +3019,31 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 
 static int nvme_setup_prp_pools(struct nvme_dev *dev)
 {
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|596| <<nvme_unmap_data>> dma_pool_free(dev->prp_page_pool, addr, dma_addr);
+	 *   - drivers/nvme/host/pci.c|658| <<nvme_pci_setup_prps>> pool = dev->prp_page_pool;
+	 *   - drivers/nvme/host/pci.c|753| <<nvme_pci_setup_sgls>> pool = dev->prp_page_pool;
+	 *   - drivers/nvme/host/pci.c|2463| <<nvme_setup_prp_pools>> dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
+	 *   - drivers/nvme/host/pci.c|2465| <<nvme_setup_prp_pools>> if (!dev->prp_page_pool)
+	 *   - drivers/nvme/host/pci.c|2472| <<nvme_setup_prp_pools>> dma_pool_destroy(dev->prp_page_pool);
+	 *   - drivers/nvme/host/pci.c|2480| <<nvme_release_prp_pools>> dma_pool_destroy(dev->prp_page_pool);
+	 */
 	dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
 						PAGE_SIZE, PAGE_SIZE, 0);
 	if (!dev->prp_page_pool)
 		return -ENOMEM;
 
 	/* Optimisation for I/Os between 4k and 128k */
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|579| <<nvme_unmap_data>> dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
+	 *   - drivers/nvme/host/pci.c|655| <<nvme_pci_setup_prps>> pool = dev->prp_small_pool;
+	 *   - drivers/nvme/host/pci.c|750| <<nvme_pci_setup_sgls>> pool = dev->prp_small_pool;
+	 *   - drivers/nvme/host/pci.c|2469| <<nvme_setup_prp_pools>> dev->prp_small_pool = dma_pool_create("prp list 256", dev->dev,
+	 *   - drivers/nvme/host/pci.c|2471| <<nvme_setup_prp_pools>> if (!dev->prp_small_pool) {
+	 *   - drivers/nvme/host/pci.c|2481| <<nvme_release_prp_pools>> dma_pool_destroy(dev->prp_small_pool);
+	 */
 	dev->prp_small_pool = dma_pool_create("prp list 256", dev->dev,
 						256, 256, 0);
 	if (!dev->prp_small_pool) {
@@ -2448,6 +3059,9 @@ static void nvme_release_prp_pools(struct nvme_dev *dev)
 	dma_pool_destroy(dev->prp_small_pool);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_pci_ctrl_ops.free_ctrl = nvme_pci_free_ctrl()
+ */
 static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
 {
 	struct nvme_dev *dev = to_nvme_dev(ctrl);
@@ -2475,6 +3089,17 @@ static void nvme_remove_dead_ctrl(struct nvme_dev *dev, int status)
 		nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * 关于PCI, 在以下调用reset_work:
+ *   - drivers/nvme/host/core.c|124| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|143| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|158| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/pci.c|2974| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3059| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ *
+ * used by:
+ *   - drivers/nvme/host/pci.c|2861| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ */
 static void nvme_reset_work(struct work_struct *work)
 {
 	struct nvme_dev *dev =
@@ -2490,6 +3115,12 @@ static void nvme_reset_work(struct work_struct *work)
 	 * If we're called to reset a live controller first shut it down before
 	 * moving on.
 	 */
+	/*
+	 * 添加或者删除NVME_CC_ENABLE的地方:
+	 *   - drivers/nvme/host/core.c|1948| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|1984| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_ENABLE;
+	 *   - drivers/pci/quirks.c|3771| <<nvme_disable_and_flr>> cfg &= ~(NVME_CC_SHN_MASK | NVME_CC_ENABLE);
+	 */
 	if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
 		nvme_dev_disable(dev, false);
 	nvme_sync_queues(&dev->ctrl);
@@ -2610,24 +3241,28 @@ static void nvme_remove_dead_ctrl_work(struct work_struct *work)
 	nvme_put_ctrl(&dev->ctrl);
 }
 
+/* struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_read32 = nvme_pci_reg_read32() */
 static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
 {
 	*val = readl(to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/* struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_write32 = nvme_pci_reg_write32() */
 static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
 {
 	writel(val, to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/* struct nvme_ctrl_ops nvme_pci_ctrl_ops.reg_read64 = nvme_pci_reg_read64() */
 static int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
 {
 	*val = readq(to_nvme_dev(ctrl)->bar + off);
 	return 0;
 }
 
+/* struct nvme_ctrl_ops nvme_pci_ctrl_ops.get_address = nvme_pci_get_address() */
 static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 {
 	struct pci_dev *pdev = to_pci_dev(to_nvme_dev(ctrl)->dev);
@@ -2635,6 +3270,10 @@ static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
 	return snprintf(buf, size, "%s", dev_name(&pdev->dev));
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|3198| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ */
 static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.name			= "pcie",
 	.module			= THIS_MODULE,
@@ -2648,6 +3287,10 @@ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.get_address		= nvme_pci_get_address,
 };
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2766| <<nvme_probe>> result = nvme_dev_map(dev);
+ */
 static int nvme_dev_map(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2695,15 +3338,32 @@ static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|2890| <<nvme_probe>> async_schedule(nvme_async_probe, dev);
+ */
 static void nvme_async_probe(void *data, async_cookie_t cookie)
 {
 	struct nvme_dev *dev = data;
 
+	/*
+	 * pci上调用nvme_reset_work()
+	 */
 	nvme_reset_ctrl_sync(&dev->ctrl);
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/core.c|117| <<nvme_queue_scan>> queue_work(nvme_wq, &ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3578| <<nvme_remove_namespaces>> flush_work(&ctrl->scan_work);
+	 *   - drivers/nvme/host/core.c|3808| <<nvme_init_ctrl>> INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	 *   - drivers/nvme/host/pci.c|2808| <<nvme_async_probe>> flush_work(&dev->ctrl.scan_work);
+	 */
 	flush_work(&dev->ctrl.scan_work);
 	nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * struct pci_driver nvme_driver.probe = nvme_probe()
+ */
 static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 {
 	int node, result = -ENOMEM;
@@ -2719,6 +3379,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (!dev)
 		return -ENOMEM;
 
+	/* max_queue_count()返回num_possible_cpus() + write_queues + poll_queues + 1 (admin queue) */
 	dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
 					GFP_KERNEL, node);
 	if (!dev->queues)
@@ -2735,6 +3396,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
 	mutex_init(&dev->shutdown_lock);
 
+	/*
+	 * 只有这里调用nvme_setup_prp_pools()
+	 */
 	result = nvme_setup_prp_pools(dev);
 	if (result)
 		goto unmap;
@@ -2749,6 +3413,15 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 						NVME_MAX_SEGS, true);
 	WARN_ON_ONCE(alloc_size > PAGE_SIZE);
 
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|626| <<nvme_unmap_data>> mempool_free(iod->sg, dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|879| <<nvme_map_data>> iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
+	 *   - drivers/nvme/host/pci.c|2551| <<nvme_pci_free_ctrl>> mempool_destroy(dev->iod_mempool);
+	 *   - drivers/nvme/host/pci.c|2848| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 *   - drivers/nvme/host/pci.c|2852| <<nvme_probe>> if (!dev->iod_mempool) {
+	 *   - drivers/nvme/host/pci.c|2870| <<nvme_probe>> mempool_destroy(dev->iod_mempool);
+	 */
 	dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
 						mempool_kfree,
 						(void *) alloc_size,
@@ -2784,18 +3457,27 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	return result;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_prepare = nvme_reset_prepare()
+ */
 static void nvme_reset_prepare(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
 	nvme_dev_disable(dev, false);
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_done = nvme_reset_done()
+ */
 static void nvme_reset_done(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
 	nvme_reset_ctrl_sync(&dev->ctrl);
 }
 
+/*
+ * struct pci_driver nvme_driver.shutdown = nvme_shutdown()
+ */
 static void nvme_shutdown(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2807,6 +3489,9 @@ static void nvme_shutdown(struct pci_dev *pdev)
  * state. This function must not have any dependencies on the device state in
  * order to proceed.
  */
+/*
+ * struct pci_driver nvme_driver.remove = nvme_remove()
+ */
 static void nvme_remove(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2856,6 +3541,9 @@ static int nvme_resume(struct device *dev)
 
 static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
 
+/*
+ * struct pci_error_handlers nvme_err_handler.err_detected = nvme_error_detected()
+ */
 static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 						pci_channel_state_t state)
 {
@@ -2882,6 +3570,9 @@ static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 	return PCI_ERS_RESULT_NEED_RESET;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.slot_reset = nvme_slot_reset()
+ */
 static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2892,6 +3583,9 @@ static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 	return PCI_ERS_RESULT_RECOVERED;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.resume = nvme_error_resume()
+ */
 static void nvme_error_resume(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2899,6 +3593,9 @@ static void nvme_error_resume(struct pci_dev *pdev)
 	flush_work(&dev->ctrl.reset_work);
 }
 
+/*
+ * struct pci_driver nvme_driver.err_handle = nvme_err_handler
+ */
 static const struct pci_error_handlers nvme_err_handler = {
 	.error_detected	= nvme_error_detected,
 	.slot_reset	= nvme_slot_reset,
@@ -2907,6 +3604,7 @@ static const struct pci_error_handlers nvme_err_handler = {
 	.reset_done	= nvme_reset_done,
 };
 
+/* struct pci_driver nvme_driver.id_table = nvme_id_table */
 static const struct pci_device_id nvme_id_table[] = {
 	{ PCI_VDEVICE(INTEL, 0x0953),
 		.driver_data = NVME_QUIRK_STRIPE_SIZE |
diff --git a/drivers/scsi/scsi.c b/drivers/scsi/scsi.c
index 653d5ea..51a288e 100644
--- a/drivers/scsi/scsi.c
+++ b/drivers/scsi/scsi.c
@@ -183,6 +183,12 @@ void scsi_log_completion(struct scsi_cmnd *cmd, int disposition)
  *              request, waking processes that are waiting on results,
  *              etc.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|164| <<scmd_eh_abort_handler>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_error.c|2086| <<scsi_eh_flush_done_q>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_lib.c|1449| <<scsi_softirq_done>> scsi_finish_command(cmd);
+ */
 void scsi_finish_command(struct scsi_cmnd *cmd)
 {
 	struct scsi_device *sdev = cmd->device;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 65d0a10..1ab99ec 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -555,6 +555,12 @@ static void scsi_mq_uninit_cmd(struct scsi_cmnd *cmd)
 }
 
 /* Returns false when no more bytes to process, true if there are more */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|799| <<scsi_io_completion_action>> if (!scsi_end_request(req, blk_stat, blk_rq_err_bytes(req)))
+ *   - drivers/scsi/scsi_lib.c|949| <<scsi_io_completion>> if (likely(!scsi_end_request(req, blk_stat, good_bytes)))
+ *   - drivers/scsi/scsi_lib.c|955| <<scsi_io_completion>> if (scsi_end_request(req, blk_stat, blk_rq_bytes(req)))
+ */
 static bool scsi_end_request(struct request *req, blk_status_t error,
 		unsigned int bytes)
 {
@@ -915,6 +921,10 @@ static int scsi_io_completion_nz_result(struct scsi_cmnd *cmd, int result,
  *		c) We can call scsi_end_request() with blk_stat other than
  *		   BLK_STS_OK, to fail the remainder of the request.
  */
+/*
+ * called by only:
+ *   - drivers/scsi/scsi.c|233| <<scsi_finish_command>> scsi_io_completion(cmd, good_bytes);
+ */
 void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)
 {
 	int result = cmd->result;
diff --git a/drivers/virtio/virtio_pci_common.c b/drivers/virtio/virtio_pci_common.c
index f2862f6..857dd8b 100644
--- a/drivers/virtio/virtio_pci_common.c
+++ b/drivers/virtio/virtio_pci_common.c
@@ -444,6 +444,11 @@ int vp_set_vq_affinity(struct virtqueue *vq, const struct cpumask *cpu_mask)
 	return 0;
 }
 
+/*
+ * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+ * struct virtio_config_ops virtio_pci_config_nodev_ops.get_vq_affinity = vp_get_vq_affinity()
+ * struct virtio_config_ops virtio_pci_config_ops.get_vq_affinity = vp_get_vq_affinity()
+ */
 const struct cpumask *vp_get_vq_affinity(struct virtio_device *vdev, int index)
 {
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index c8be1c4..c8cafe2 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -238,6 +238,14 @@ static inline bool virtqueue_use_indirect(struct virtqueue *_vq,
  * unconditionally on data path.
  */
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|265| <<virtio_max_dma_size>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|275| <<vring_alloc_queue>> if (vring_use_dma_api(vdev)) {
+ *   - drivers/virtio/virtio_ring.c|308| <<vring_free_queue>> if (vring_use_dma_api(vdev))
+ *   - drivers/virtio/virtio_ring.c|1607| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ *   - drivers/virtio/virtio_ring.c|2081| <<bool>> vq->use_dma_api = vring_use_dma_api(vdev);
+ */
 static bool vring_use_dma_api(struct virtio_device *vdev)
 {
 	if (!virtio_has_iommu_quirk(vdev))
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 749f598..635453ab 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -850,6 +850,18 @@ static struct file_system_type bd_type = {
 	.kill_sb	= kill_anon_super,
 };
 
+/*
+ * used by:
+ *   - fs/block_dev.c|871| <<bdev_cache_init>> blockdev_superblock = bd_mnt->mnt_sb;
+ *   - fs/block_dev.c|910| <<bdev_unhash_inode>> inode = ilookup5(blockdev_superblock, hash(dev), bdev_test, &dev);
+ *   - fs/block_dev.c|1009| <<bdget>> inode = iget5_locked(blockdev_superblock, hash(dev),
+ *   - fs/block_dev.c|2278| <<iterate_bdevs>> spin_lock(&blockdev_superblock->s_inode_list_lock);
+ *   - fs/block_dev.c|2279| <<iterate_bdevs>> list_for_each_entry(inode, &blockdev_superblock->s_inodes, i_sb_list) {
+ *   - fs/block_dev.c|2291| <<iterate_bdevs>> spin_unlock(&blockdev_superblock->s_inode_list_lock);
+ *   - fs/block_dev.c|2309| <<iterate_bdevs>> spin_lock(&blockdev_superblock->s_inode_list_lock);
+ *   - fs/block_dev.c|2311| <<iterate_bdevs>> spin_unlock(&blockdev_superblock->s_inode_list_lock);
+ *   - include/linux/fs.h|2574| <<sb_is_blkdev_sb>> return sb == blockdev_superblock;
+ */
 struct super_block *blockdev_superblock __read_mostly;
 EXPORT_SYMBOL_GPL(blockdev_superblock);
 
@@ -892,6 +904,11 @@ static int bdev_set(struct inode *inode, void *data)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - fs/block_dev.c|941| <<bdget>> list_add(&bdev->bd_list, &all_bdevs);
+ *   - fs/block_dev.c|966| <<nr_blockdev_pages>> list_for_each_entry(bdev, &all_bdevs, bd_list) {
+ */
 static LIST_HEAD(all_bdevs);
 
 /*
@@ -909,11 +926,98 @@ void bdev_unhash_inode(dev_t dev)
 	}
 }
 
+/*
+ * [0] bdget
+ * [0] bdget_disk
+ * [0] revalidate_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] bdget
+ * [0] bdget_disk
+ * [0] __device_add_disk
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] bdget
+ * [0] bd_acquire
+ * [0] lookup_bdev.part
+ * [0] blkdev_get_by_path
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] ksys_mount
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] bdget
+ * [0] bdget_disk
+ * [0] blkdev_get
+ * [0] blkdev_get_by_path
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] do_mount
+ * [0] ksys_mount
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/genhd.c|1090| <<bdget_disk>> bdev = bdget(part_devt(part));
+ *   - block/ioctl.c|78| <<blkpg_ioctl>> bdevp = bdget(part_devt(part));
+ *   - block/ioctl.c|115| <<blkpg_ioctl>> bdevp = bdget(part_devt(part));
+ *   - drivers/block/drbd/drbd_main.c|2823| <<drbd_create_device>> device->this_bdev = bdget(MKDEV(DRBD_MAJOR, minor));
+ *   - drivers/block/pktcdvd.c|1106| <<pkt_start_recovery>> pkt_bdev = bdget(kdev_t_to_nr(pd->pkt_dev));
+ *   - drivers/block/pktcdvd.c|2182| <<pkt_open_dev>> bdget(pd->bdev->bd_dev);
+ *   - drivers/block/pktcdvd.c|2590| <<pkt_new_dev>> bdev = bdget(dev);
+ *   - drivers/char/raw.c|173| <<bind_set>> rawdev->binding = bdget(dev);
+ *   - fs/block_dev.c|1002| <<bd_acquire>> bdev = bdget(inode->i_rdev);
+ *   - fs/block_dev.c|1785| <<blkdev_get_by_dev>> bdev = bdget(dev);
+ *   - kernel/trace/blktrace.c|1761| <<sysfs_blk_trace_attr_show>> bdev = bdget(part_devt(p));
+ *   - kernel/trace/blktrace.c|1822| <<sysfs_blk_trace_attr_store>> bdev = bdget(part_devt(p));
+ *   - mm/swapfile.c|1670| <<swap_type_of>> bdev = bdget(device);
+ */
 struct block_device *bdget(dev_t dev)
 {
 	struct block_device *bdev;
 	struct inode *inode;
 
+	/*
+	 * obtain an inode from a mounted file system
+	 */
 	inode = iget5_locked(blockdev_superblock, hash(dev),
 			bdev_test, bdev_set, &dev);
 
diff --git a/fs/direct-io.c b/fs/direct-io.c
index ac7fb19..f2d409a 100644
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@ -1170,6 +1170,10 @@ static inline int drop_refcount(struct dio *dio)
  * individual fields and will generate much worse code. This is important
  * for the whole file.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1422| <<__blockdev_direct_IO>> return do_blockdev_direct_IO(iocb, inode, bdev, iter, get_block,
+ */
 static inline ssize_t
 do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 		      struct block_device *bdev, struct iov_iter *iter,
@@ -1401,6 +1405,15 @@ do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 	return retval;
 }
 
+/*
+ * called by:
+ *   - fs/btrfs/inode.c|8616| <<btrfs_direct_IO>> ret = __blockdev_direct_IO(iocb, inode,
+ *   - fs/ext4/inode.c|3772| <<ext4_direct_IO_write>> ret = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ *   - fs/ext4/inode.c|3865| <<ext4_direct_IO_read>> ret = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/f2fs/data.c|2706| <<f2fs_direct_IO>> err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/ocfs2/aops.c|2443| <<ocfs2_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - include/linux/fs.h|3120| <<blockdev_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ */
 ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     struct block_device *bdev, struct iov_iter *iter,
 			     get_block_t get_block,
diff --git a/include/linux/bio.h b/include/linux/bio.h
index f87abaa..e3ae719 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -23,30 +23,92 @@
 
 #define BIO_MAX_PAGES		256
 
+/*
+ * called by:
+ *   - block/blk-core.c|725| <<blk_init_request_from_bio>> req->ioprio = bio_prio(bio);
+ *   - block/blk-merge.c|904| <<blk_rq_merge_ok>> if (rq->ioprio != bio_prio(bio))
+ */
 #define bio_prio(bio)			(bio)->bi_ioprio
+/*
+ * called by:
+ *   - drivers/md/bcache/movinggc.c|85| <<moving_init>> bio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));
+ *   - drivers/md/bcache/writeback.c|253| <<dirty_init>> bio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));
+ */
 #define bio_set_prio(bio, prio)		((bio)->bi_ioprio = prio)
 
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bio_iter_iovec(bio, iter)				\
 	bvec_iter_bvec((bio)->bi_io_vec, (iter))
 
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bio_iter_page(bio, iter)				\
 	bvec_iter_page((bio)->bi_io_vec, (iter))
+/*
+ * 二个进行比较:
+ * 1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * 2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ */
 #define bio_iter_len(bio, iter)					\
 	bvec_iter_len((bio)->bi_io_vec, (iter))
+/*
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
 #define bio_iter_offset(bio, iter)				\
 	bvec_iter_offset((bio)->bi_io_vec, (iter))
 
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bio_page(bio)		bio_iter_page((bio), (bio)->bi_iter)
+/*
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
 #define bio_offset(bio)		bio_iter_offset((bio), (bio)->bi_iter)
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bio_iovec(bio)		bio_iter_iovec((bio), (bio)->bi_iter)
 
+/*
+ * 判断是否有多个segment
+ */
 #define bio_multiple_segments(bio)				\
 	((bio)->bi_iter.bi_size != bio_iovec(bio).bv_len)
 
+/*
+ * bio的iter所表示的sector的数量
+ */
 #define bvec_iter_sectors(iter)	((iter).bi_size >> 9)
+/*
+ * bio的iter表示的最后一个vector
+ */
 #define bvec_iter_end_sector(iter) ((iter).bi_sector + bvec_iter_sectors((iter)))
 
+/*
+ * bio的iter所表示的sector的数量
+ */
 #define bio_sectors(bio)	bvec_iter_sectors((bio)->bi_iter)
+/*
+ * bio的iter表示的最后一个vector
+ */
 #define bio_end_sector(bio)	bvec_iter_end_sector((bio)->bi_iter)
 
 /*
@@ -70,6 +132,10 @@ static inline bool bio_has_data(struct bio *bio)
 	return false;
 }
 
+/*
+ * called by only:
+ *   - include/linux/bio.h|132| <<bio_advance_iter>> if (bio_no_advance_iter(bio))
+ */
 static inline bool bio_no_advance_iter(struct bio *bio)
 {
 	return bio_op(bio) == REQ_OP_DISCARD ||
@@ -78,6 +144,9 @@ static inline bool bio_no_advance_iter(struct bio *bio)
 	       bio_op(bio) == REQ_OP_WRITE_ZEROES;
 }
 
+/*
+ * 测试bio->bi_opf是否设置了REQ_NOMERGE_FLAGS
+ */
 static inline bool bio_mergeable(struct bio *bio)
 {
 	if (bio->bi_opf & REQ_NOMERGE_FLAGS)
@@ -102,11 +171,18 @@ static inline void *bio_data(struct bio *bio)
 	return NULL;
 }
 
+/*
+ * 不能再添加bio_vec了
+ */
 static inline bool bio_full(struct bio *bio)
 {
 	return bio->bi_vcnt >= bio->bi_max_vecs;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|179| <<bio_for_each_segment_all>> for (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )
+ */
 static inline bool bio_next_segment(const struct bio *bio,
 				    struct bvec_iter_all *iter)
 {
@@ -136,6 +212,31 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 		/* TODO: It is reasonable to complete bio with error here. */
 }
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|170| <<bio_integrity_process>> __bio_for_each_segment(bv, bio, bviter, *proc_iter) {
+ *   - block/bio.c|642| <<zero_fill_bio_iter>> __bio_for_each_segment(bv, bio, iter, start) {
+ *   - drivers/block/aoe/aoecmd.c|302| <<skb_fillup>> __bio_for_each_segment(bv, bio, iter, iter)
+ *   - drivers/block/aoe/aoecmd.c|1030| <<bvcpy>> __bio_for_each_segment(bv, bio, iter, iter) {
+ *   - drivers/md/dm-integrity.c|1527| <<integrity_metadata>> __bio_for_each_segment(bv, bio, iter, dio->orig_bi_iter) {
+ *   - include/linux/bio.h|222| <<bio_for_each_segment>> __bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
+ *   - include/linux/ceph/messenger.h|125| <<ceph_bio_iter_advance_step>> __bio_for_each_segment(bv, (it)->bio, __cur_iter, __cur_iter) \
+ *
+ *
+ * bio_advance_iter():
+ *
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ *
+ *
+ * 从start开始, 遍历每一个最大1个page的bvec
+ */
 #define __bio_for_each_segment(bvl, bio, iter, start)			\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -145,6 +246,19 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 #define bio_for_each_segment(bvl, bio, iter)				\
 	__bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|163| <<bio_for_each_bvec>> __bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)
+ *
+ * mp_bvec_iter_bvec():
+ * 构造一个bvec:
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * bv_offset : 所以返回的是当前bvec"完成到"的offset
+ *
+ *
+ * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+ */
 #define __bio_for_each_bvec(bvl, bio, iter, start)		\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -152,9 +266,19 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 	     bio_advance_iter((bio), &(iter), (bvl).bv_len))
 
 /* iterate over multi-page bvec */
+/*
+ * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+ */
 #define bio_for_each_bvec(bvl, bio, iter)			\
 	__bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_main.c|1598| <<_drbd_send_bio>> bio_iter_last(bvec, iter)
+ *   - drivers/block/drbd/drbd_main.c|1620| <<_drbd_send_zc_bio>> bio_iter_last(bvec, iter) ? 0 : MSG_MORE);
+ *   - drivers/block/nbd.c|572| <<nbd_send_cmd>> bool is_last = !next && bio_iter_last(bvec, iter);
+ *   - include/linux/blkdev.h|1003| <<rq_iter_last>> bio_iter_last(bvec, _iter.iter))
+ */
 #define bio_iter_last(bvec, iter) ((iter).bi_size == (bvec).bv_len)
 
 static inline unsigned bio_segments(struct bio *bio)
@@ -381,11 +505,55 @@ extern struct bio *bio_clone_fast(struct bio *, gfp_t, struct bio_set *);
 
 extern struct bio_set fs_bio_set;
 
+/*
+ * 部分调用的例子:
+ *   - block/blk-flush.c|516| <<blkdev_issue_flush>> bio = bio_alloc(gfp_mask, 0);
+ *   - block/blk-lib.c|15| <<blk_next_bio>> struct bio *new = bio_alloc(gfp, nr_pages);
+ *   - drivers/block/xen-blkback/blkback.c|1368| <<dispatch_rw_block_io>> bio = bio_alloc(GFP_KERNEL, nr_iovecs);
+ *   - drivers/block/xen-blkback/blkback.c|1387| <<dispatch_rw_block_io>> bio = bio_alloc(GFP_KERNEL, 0);
+ *   - drivers/md/dm-log-writes.c|220| <<write_metadata>> bio = bio_alloc(GFP_KERNEL, 1);
+ *   - drivers/md/dm-log-writes.c|279| <<write_inline_data>> bio = bio_alloc(GFP_KERNEL, bio_pages);
+ *   - drivers/md/dm-log-writes.c|367| <<log_one_block>> bio = bio_alloc(GFP_KERNEL, min(block->vec_cnt, BIO_MAX_PAGES));
+ *   - drivers/md/dm-log-writes.c|389| <<log_one_block>> bio = bio_alloc(GFP_KERNEL, min(block->vec_cnt - i, BIO_MAX_PAGES));
+ *   - drivers/md/dm-thin.c|1182| <<process_prepared_discard_passdown_pt1>> discard_parent = bio_alloc(GFP_NOIO, 1);
+ *   - drivers/md/dm-zoned-metadata.c|409| <<dmz_get_mblock_slow>> bio = bio_alloc(GFP_NOIO, 1);
+ *   - drivers/md/dm-zoned-metadata.c|578| <<dmz_write_mblock>> bio = bio_alloc(GFP_NOIO, 1);
+ *   - drivers/md/dm-zoned-metadata.c|604| <<dmz_rdwr_block>> bio = bio_alloc(GFP_NOIO, 1);
+ *   - drivers/md/md.c|185| <<bio_alloc_mddev>> return bio_alloc(gfp_mask, nr_iovecs);
+ *   - drivers/md/md.c|194| <<md_bio_alloc_sync>> return bio_alloc(GFP_NOIO, 1);
+ *   - fs/block_dev.c|448| <<__blkdev_direct_IO>> bio = bio_alloc(GFP_KERNEL, nr_pages);
+ *   - fs/buffer.c|3073| <<submit_bh_wbc>> bio = bio_alloc(GFP_NOIO, 1);
+ *   - fs/direct-io.c|439| <<dio_bio_alloc>> bio = bio_alloc(GFP_KERNEL, nr_vecs);
+ *   - fs/ext4/page-io.c|374| <<io_submit_init_bio>> bio = bio_alloc(GFP_NOIO, BIO_MAX_PAGES);
+ *   - fs/ext4/readpage.c|251| <<ext4_mpage_readpages>> bio = bio_alloc(GFP_KERNEL,
+ *   - fs/mpage.c|79| <<mpage_alloc>> bio = bio_alloc(gfp_flags, nr_vecs);
+ *   - fs/mpage.c|83| <<mpage_alloc>> bio = bio_alloc(gfp_flags, nr_vecs);
+ *   - fs/xfs/xfs_aops.c|731| <<xfs_chain_bio>> new = bio_alloc(GFP_NOFS, BIO_MAX_PAGES);
+ *   - fs/xfs/xfs_buf.c|1378| <<xfs_buf_ioapply_map>> bio = bio_alloc(GFP_NOIO, nr_pages);
+ *   - kernel/power/swap.c|270| <<hib_submit_io>> bio = bio_alloc(GFP_NOIO | __GFP_HIGH, 1);
+ *   - mm/page_io.c|34| <<get_swap_bio>> bio = bio_alloc(gfp_flags, 1);
+ */
 static inline struct bio *bio_alloc(gfp_t gfp_mask, unsigned int nr_iovecs)
 {
 	return bio_alloc_bioset(gfp_mask, nr_iovecs, &fs_bio_set);
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1368| <<bio_copy_user_iov>> bio = bio_kmalloc(gfp_mask, nr_pages);
+ *   - block/bio.c|1469| <<bio_map_user_iov>> bio = bio_kmalloc(gfp_mask, iov_iter_npages(iter, BIO_MAX_PAGES));
+ *   - block/bio.c|1596| <<bio_map_kern>> bio = bio_kmalloc(gfp_mask, nr_pages);
+ *   - block/bio.c|1675| <<bio_copy_kern>> bio = bio_kmalloc(gfp_mask, nr_pages);
+ *   - drivers/block/pktcdvd.c|532| <<pkt_alloc_packet_data>> pkt->w_bio = bio_kmalloc(GFP_KERNEL, frames);
+ *   - drivers/block/pktcdvd.c|546| <<pkt_alloc_packet_data>> struct bio *bio = bio_kmalloc(GFP_KERNEL, 1);
+ *   - drivers/lightnvm/pblk-core.c|593| <<pblk_bio_map_addr>> bio = bio_kmalloc(gfp_mask, nr_secs);
+ *   - drivers/md/bcache/debug.c|114| <<bch_data_verify>> check = bio_kmalloc(GFP_NOIO, bio_segments(bio));
+ *   - drivers/md/dm-bufio.c|578| <<use_bio>> bio = bio_kmalloc(GFP_NOWAIT | __GFP_NORETRY | __GFP_NOWARN, vec_size);
+ *   - drivers/md/raid1.c|130| <<r1buf_pool_alloc>> bio = bio_kmalloc(gfp_flags, RESYNC_PAGES);
+ *   - drivers/md/raid10.c|178| <<r10buf_pool_alloc>> bio = bio_kmalloc(gfp_flags, RESYNC_PAGES);
+ *   - drivers/md/raid10.c|184| <<r10buf_pool_alloc>> bio = bio_kmalloc(gfp_flags, RESYNC_PAGES);
+ *   - drivers/target/target_core_pscsi.c|835| <<pscsi_get_bio>> bio = bio_kmalloc(GFP_KERNEL, nr_vecs);
+ */
 static inline struct bio *bio_kmalloc(gfp_t gfp_mask, unsigned int nr_iovecs)
 {
 	return bio_alloc_bioset(gfp_mask, nr_iovecs, NULL);
@@ -700,7 +868,27 @@ struct bio_set {
 	struct kmem_cache *bio_slab;
 	unsigned int front_pad;
 
+	/*
+	 * used by:
+	 *   - block/bio.c|316| <<bio_free>> mempool_free(p, &bs->bio_pool);
+	 *   - block/bio.c|538| <<bio_alloc_bioset>> p = mempool_alloc(&bs->bio_pool, gfp_mask);
+	 *   - block/bio.c|542| <<bio_alloc_bioset>> p = mempool_alloc(&bs->bio_pool, gfp_mask);
+	 *   - block/bio.c|579| <<bio_alloc_bioset>> mempool_free(p, &bs->bio_pool);
+	 *   - block/bio.c|2019| <<bioset_exit>> mempool_exit(&bs->bio_pool);
+	 *   - block/bio.c|2082| <<bioset_init>> if (mempool_init_slab_pool(&bs->bio_pool, pool_size, bs->bio_slab))
+	 *   - block/bio.c|2117| <<bioset_init_from_src>> return bioset_init(bs, src->bio_pool.min_nr, src->front_pad, flags);
+	 */
 	mempool_t bio_pool;
+	/*
+	 * used by:
+	 *   - block/bio.c|308| <<bio_free>> bvec_free(&bs->bvec_pool, bio->bi_io_vec, BVEC_POOL_IDX(bio));
+	 *   - block/bio.c|535| <<bio_alloc_bioset>> if (WARN_ON_ONCE(!mempool_initialized(&bs->bvec_pool) &&
+	 *   - block/bio.c|585| <<bio_alloc_bioset>> bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+	 *   - block/bio.c|589| <<bio_alloc_bioset>> bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+	 *   - block/bio.c|2047| <<bioset_exit>> mempool_exit(&bs->bvec_pool);
+	 *   - block/bio.c|2113| <<bioset_init>> biovec_init_pool(&bs->bvec_pool, pool_size))
+	 *   - block/bio.c|2139| <<bioset_init_from_src>> if (src->bvec_pool.min_nr)
+	 */
 	mempool_t bvec_pool;
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 	mempool_t bio_integrity_pool;
@@ -736,6 +924,13 @@ static inline bool bioset_initialized(struct bio_set *bs)
 
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 
+/*
+ * called by:
+ *   - block/t10-pi.c|203| <<t10_pi_prepare>> bip_for_each_vec(iv, bip, iter) {
+ *   - block/t10-pi.c|258| <<t10_pi_complete>> bip_for_each_vec(iv, bip, iter) {
+ *   - drivers/md/dm-integrity.c|1580| <<integrity_metadata>> bip_for_each_vec(biv, bip, iter) {
+ *   - include/linux/bio.h|819| <<bio_for_each_integrity_vec>> bip_for_each_vec(_bvl, _bio->bi_integrity, _iter)
+ */
 #define bip_for_each_vec(bvl, bip, iter)				\
 	for_each_bvec(bvl, (bip)->bip_vec, iter, (bip)->bip_iter)
 
diff --git a/include/linux/blk-cgroup.h b/include/linux/blk-cgroup.h
index 76c6131..e446c37 100644
--- a/include/linux/blk-cgroup.h
+++ b/include/linux/blk-cgroup.h
@@ -370,6 +370,12 @@ static inline struct blkcg_gq *__blkg_lookup(struct blkcg *blkcg,
  * Lookup blkg for the @blkcg - @q pair.  This function should be called
  * under RCU read loc.
  */
+/*
+ * called by:
+ *   - block/bfq-cgroup.c|495| <<bfq_lookup_bfqg>> blkg = blkg_lookup(blkcg, bfqd->queue);
+ *   - block/blk-cgroup.c|360| <<blkg_lookup_create>> struct blkcg_gq *blkg = blkg_lookup(blkcg, q);
+ *   - block/blk-cgroup.c|1722| <<blkcg_maybe_throttle_current>> blkg = blkg_lookup(blkcg, q);
+ */
 static inline struct blkcg_gq *blkg_lookup(struct blkcg *blkcg,
 					   struct request_queue *q)
 {
@@ -769,6 +775,10 @@ static inline void blkcg_bio_issue_init(struct bio *bio)
 	bio_issue_init(&bio->bi_issue, bio_sectors(bio));
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|994| <<generic_make_request_checks>> if (!blkcg_bio_issue_check(q, bio))
+ */
 static inline bool blkcg_bio_issue_check(struct request_queue *q,
 					 struct bio *bio)
 {
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 15d1aa5..98c6fb4 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,11 +15,54 @@ struct blk_flush_queue;
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 往hctx->dispatch添加新元素的地方:
+		 *   - block/blk-mq-sched.c|425| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1391| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1779| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2364| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 *
+		 * 从hctx->dispatch移除元素的地方(下发):
+		 *   - block/blk-mq-sched.c|222| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *
+		 * 其他使用hctx->dispatch的地方:
+		 *   - block/blk-mq-debugfs.c|379| <<hctx_dispatch_start>> return seq_list_start(&hctx->dispatch, *pos);
+		 *   - block/blk-mq-debugfs.c|386| <<hctx_dispatch_next>> return seq_list_next(v, &hctx->dispatch, pos);
+		 *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> if (!list_empty_careful(&hctx->dispatch)) {
+		 *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> if (!list_empty(&hctx->dispatch))
+		 *   - block/blk-mq.c|72| <<blk_mq_hctx_has_pending>> return !list_empty_careful(&hctx->dispatch) ||
+		 *   - block/blk-mq.c|2474| <<blk_mq_alloc_hctx>> INIT_LIST_HEAD(&hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
 
+	/*
+	 * 在以下使用run_work:
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1913| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|2045| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq.c|2161| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 *   - block/blk-mq.c|3110| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 */
 	struct delayed_work	run_work;
+	/*
+	 * cpumask使用的地方:
+	 *   - block/blk-mq-sysfs.c|45| <<blk_mq_hw_sysfs_release>> free_cpumask_var(hctx->cpumask);
+	 *   - block/blk-mq-sysfs.c|172| <<blk_mq_hw_sysfs_cpus_show>> for_each_cpu(i, hctx->cpumask) {
+	 *   - block/blk-mq.c|647| <<blk_mq_alloc_request_hctx>> cpu = cpumask_first_and(alloc_data.hctx->cpumask, cpu_online_mask);
+	 *   - block/blk-mq.c|1777| <<__blk_mq_run_hw_queue>> if (!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask) &&
+	 *   - block/blk-mq.c|1781| <<__blk_mq_run_hw_queue>> cpumask_empty(hctx->cpumask) ? "inactive": "active");
+	 *   - block/blk-mq.c|1816| <<blk_mq_first_mapped_cpu>> int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
+	 *   - block/blk-mq.c|1819| <<blk_mq_first_mapped_cpu>> cpu = cpumask_first(hctx->cpumask);
+	 *   - block/blk-mq.c|1839| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1888| <<__blk_mq_delay_run_hw_queue>> if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+	 *   - block/blk-mq.c|3102| <<blk_mq_alloc_hctx>> if (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))
+	 *   - block/blk-mq.c|3152| <<blk_mq_alloc_hctx>> free_cpumask_var(hctx->cpumask);
+	 *   - block/blk-mq.c|3231| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *   - block/blk-mq.c|3270| <<blk_mq_map_swqueue>> if (cpumask_test_cpu(i, hctx->cpumask))
+	 *   - block/blk-mq.c|3273| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 */
 	cpumask_var_t		cpumask;
 	int			next_cpu;
 	int			next_cpu_batch;
@@ -32,13 +75,71 @@ struct blk_mq_hw_ctx {
 
 	void			*driver_data;
 
+	/*
+	 * 似乎用来表示hctx的某一个ctx->rq_list是否有request
+	 *
+	 * 设置和取消hctx->ctx_map的地方:
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|110| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * 遍历hctx->ctx_map的地方(都是从ctx->rq_list移除的操作):
+	 *   - block/blk-mq.c|1095| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1133| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off, ---> dispatch_rq_from_ctx
+	 *
+	 * 测试hctx->ctx_map的地方:
+	 *   - block/blk-mq-sched.c|171| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|89| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|101| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *
+	 * 其他初始化hctx->ctx_map的地方:
+	 *   - block/blk-mq-debugfs.c|455| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sysfs.c|44| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2526| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2547| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2716| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	struct sbitmap		ctx_map;
 
+	/*
+	 * 在以下使用dispatch_from:
+	 *   - block/blk-mq-sched.c|199| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+	 *   - block/blk-mq-sched.c|237| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+	 *   - block/blk-mq.c|3238| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+	 */
 	struct blk_mq_ctx	*dispatch_from;
+	/*
+	 * 设置dispatch_busy的地方:
+	 *   - block/blk-mq.c|1445| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 *
+	 * 使用dispatch_busy的地方:
+	 *   - block/blk-mq-debugfs.c|637| <<hctx_dispatch_busy_show>> seq_printf(m, "%u\n", hctx->dispatch_busy);
+	 *   - block/blk-mq-sched.c|338| <<blk_mq_sched_dispatch_requests>> } else if (hctx->dispatch_busy) {
+	 *   - block/blk-mq-sched.c|636| <<blk_mq_sched_insert_requests>> if (!hctx->dispatch_busy && !e && !run_queue_async) {
+	 *   - block/blk-mq.c|1435| <<blk_mq_update_dispatch_busy>> ewma = hctx->dispatch_busy;
+	 *   - block/blk-mq.c|2554| <<blk_mq_make_request>> !data.hctx->dispatch_busy)) {
+	 */
 	unsigned int		dispatch_busy;
 
 	unsigned short		type;
+	/*
+	 * 在以下修改nr_ctx:
+	 *   - block/blk-mq.c|3130| <<blk_mq_alloc_hctx>> hctx->nr_ctx = 0;
+	 *   - block/blk-mq.c|3237| <<blk_mq_map_swqueue>> hctx->nr_ctx = 0;
+	 *   - block/blk-mq.c|3298| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	unsigned short		nr_ctx;
+	/*
+	 * 在以下使用ctxs:
+	 *   - block/blk-mq-sched.c|177| <<blk_mq_next_ctx>> return hctx->ctxs[idx];
+	 *   - block/blk-mq-sysfs.c|46| <<blk_mq_hw_sysfs_release>> kfree(hctx->ctxs);
+	 *   - block/blk-mq.c|1306| <<flush_busy_ctx>> struct blk_mq_ctx *ctx = hctx->ctxs[bitnr];
+	 *   - block/blk-mq.c|1351| <<dispatch_rq_from_ctx>> struct blk_mq_ctx *ctx = hctx->ctxs[bitnr];
+	 *   - block/blk-mq.c|3122| <<blk_mq_alloc_hctx>> hctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),
+	 *   - block/blk-mq.c|3124| <<blk_mq_alloc_hctx>> if (!hctx->ctxs)
+	 *   - block/blk-mq.c|3150| <<blk_mq_alloc_hctx>> kfree(hctx->ctxs);
+	 *   - block/blk-mq.c|3298| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx; 
+	 *   - include/linux/blk-mq.h|591| <<hctx_for_each_ctx>> ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
+	 */
 	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;
@@ -56,6 +157,16 @@ struct blk_mq_hw_ctx {
 	unsigned int		numa_node;
 	unsigned int		queue_num;
 
+	/*
+	 * 在以下使用nr_active:
+	 *   - block/blk-mq.c|467| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|1363| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|723| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.h|270| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|3018| <<blk_mq_alloc_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq-tag.c|138| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq-debugfs.c|629| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 */
 	atomic_t		nr_active;
 
 	struct hlist_node	cpuhp_dead;
@@ -77,13 +188,33 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_queue_map {
+	/*
+	 * mq_map[cpu]记录着当前的sw queue对应的hw queue
+	 */
 	unsigned int *mq_map;
 	unsigned int nr_queues;
+	/*
+	 * queue_offset设置的地方:
+	 *   - drivers/nvme/host/pci.c|466| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+	 *   - drivers/nvme/host/rdma.c|1801| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1804| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|1810| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1813| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|1824| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2130| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2133| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2139| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2142| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 */
 	unsigned int queue_offset;
 };
 
 enum hctx_type {
 	HCTX_TYPE_DEFAULT,	/* all I/O not otherwise accounted for */
+	/*
+	 * 就一个地方用来map:
+	 *   - block/blk-mq.h|143| <<blk_mq_map_queue>> type = HCTX_TYPE_READ;
+	 */
 	HCTX_TYPE_READ,		/* just for READ I/O */
 	HCTX_TYPE_POLL,		/* polled I/O of any kind */
 
@@ -102,6 +233,18 @@ struct blk_mq_tag_set {
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
+	/*
+	 * 设置set->reserved_tags的地方:
+	 *   - drivers/ide/ide-probe.c|783| <<ide_init_queue>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|2436| <<nvme_fc_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 *   - drivers/nvme/host/fc.c|3085| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|718| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/rdma.c|731| <<nvme_rdma_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/host/tcp.c|1447| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 2;
+	 *   - drivers/nvme/host/tcp.c|1458| <<nvme_tcp_alloc_tagset>> set->reserved_tags = 1;
+	 *   - drivers/nvme/target/loop.c|340| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.reserved_tags = 2;
+	 *   - drivers/nvme/target/loop.c|512| <<nvme_loop_create_io_queues>> ctrl->tag_set.reserved_tags = 1;
+	 */
 	unsigned int		reserved_tags;
 	unsigned int		cmd_size;	/* per-request extra data */
 	int			numa_node;
@@ -155,6 +298,16 @@ struct blk_mq_ops {
 	 * purpose of kicking the hardware (which the last request otherwise
 	 * would have done).
 	 */
+	/*
+	 * 在以下设置commit_rqs:
+	 *   - drivers/block/ataflop.c|1960| <<global>> .commit_rqs = ataflop_commit_rqs,
+	 *   - drivers/block/virtio_blk.c|716| <<global>> .commit_rqs = virtio_commit_rqs,
+	 *   - drivers/nvme/host/pci.c|1597| <<global>> .commit_rqs = nvme_commit_rqs,
+	 * 
+	 * 在以下调用commit_rqs:
+	 *   - block/blk-mq.c|1470| <<blk_mq_dispatch_rq_list>> q->mq_ops->commit_rqs(hctx);
+	 *   - block/blk-mq.c|2219| <<blk_mq_try_issue_list_directly>> hctx->queue->mq_ops->commit_rqs(hctx);
+	 */
 	commit_rqs_fn		*commit_rqs;
 
 	/*
@@ -218,14 +371,59 @@ struct blk_mq_ops {
 
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 设置和清除BLK_MQ_F_TAG_SHARED:
+	 *   - block/blk-mq.c|2637| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2683| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2413| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2639| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2665| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2682| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * 设置BLK_MQ_F_BLOCKING的地方:
+	 *   - block/bsg-lib.c|381| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/nbd.c|1586| <<nbd_dev_add>> BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1560| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|911| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|782| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|786| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|418| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|434| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
+	/*
+	 * 用在hctx->state:
+	 *   - block/blk-mq.c|1553| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1578| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1599| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1623| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *
+	 * 在io completion的时候状态可能被清空
+	 */
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 使用的地方:
+	 *   - block/blk-mq-tag.c|37| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|38| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|104| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 使用BLK_MQ_S_SCHED_RESTART的地方:
+	 *   - block/blk-mq-sched.c|67| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|70| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|76| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|92| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
@@ -263,8 +461,24 @@ enum {
 	/* return when out of requests */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
+	/*
+	 * 可能会设置BLK_MQ_REQ_RESERVED的地方:
+	 *   - block/blk-mq.c|1082| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|203| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|935| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 在以下使用BLK_MQ_REQ_INTERNAL:
+	 *   - block/blk-mq-tag.c|127| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|322| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.c|395| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
@@ -343,6 +557,50 @@ static inline struct request *blk_mq_rq_from_pdu(void *pdu)
 {
 	return pdu - sizeof(struct request);
 }
+/*
+ * 几个调用的例子:
+ *   - drivers/block/loop.c|464| <<lo_complete_rq>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/loop.c|592| <<do_req_filebacked>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/loop.c|1890| <<loop_queue_rq>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/loop.c|1954| <<loop_init_request>> struct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/null_blk_main.c|654| <<null_complete_rq>> end_cmd(blk_mq_rq_to_pdu(rq));
+ *   - drivers/block/null_blk_main.c|1332| <<null_queue_rq>> struct nullb_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);
+ *   - drivers/block/virtio_blk.c|118| <<virtblk_scsi_request_done>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/virtio_blk.c|212| <<virtblk_request_done>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/virtio_blk.c|280| <<virtio_queue_rq>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/virtio_blk.c|382| <<virtblk_get_id>> err = blk_status_to_errno(virtblk_result(blk_mq_rq_to_pdu(req)));
+ *   - drivers/block/virtio_blk.c|688| <<virtblk_init_request>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(rq);
+ *   - drivers/block/virtio_blk.c|708| <<virtblk_initialize_rq>> struct virtblk_req *vbr = blk_mq_rq_to_pdu(req);
+ *   - drivers/block/xen-blkfront.c|120| <<blkif_req>> return blk_mq_rq_to_pdu(rq);
+ *   - drivers/nvme/host/nvme.h|122| <<nvme_req>> return blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|427| <<nvme_init_request>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|527| <<nvme_pci_iod_list>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|533| <<nvme_pci_use_sgls>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|553| <<nvme_unmap_data>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|614| <<nvme_pci_setup_prps>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|728| <<nvme_pci_setup_sgls>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|788| <<nvme_setup_prp_simple>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|806| <<nvme_setup_sgl_simple>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|823| <<nvme_map_data>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|874| <<nvme_map_metadata>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|894| <<nvme_queue_rq>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|937| <<nvme_pci_complete_rq>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|1200| <<abort_endio>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/nvme/host/pci.c|1255| <<nvme_timeout>> struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_error.c|286| <<scsi_times_out>> struct scsi_cmnd *scmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|567| <<scsi_end_request>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1078| <<scsi_initialize_rq>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/scsi/scsi_lib.c|1150| <<scsi_setup_scsi_cmnd>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1182| <<scsi_setup_fs_cmnd>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1198| <<scsi_setup_cmnd>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1436| <<scsi_softirq_done>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/scsi/scsi_lib.c|1563| <<scsi_mq_prep_fn>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1633| <<scsi_queue_rq>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
+ *   - drivers/scsi/scsi_lib.c|1725| <<scsi_mq_init_request>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - drivers/scsi/scsi_lib.c|1748| <<scsi_mq_exit_request>> struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);
+ *   - include/scsi/scsi_request.h|22| <<scsi_req>> return blk_mq_rq_to_pdu(rq);
+ *   - include/scsi/scsi_tcq.h|39| <<scsi_host_find_tag>> return blk_mq_rq_to_pdu(req);
+ */
 static inline void *blk_mq_rq_to_pdu(struct request *rq)
 {
 	return rq + 1;
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 95202f8..08b25c5 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -157,11 +157,26 @@ struct bio {
 	/* Number of segments in this BIO after
 	 * physical address coalescing is performed.
 	 */
+	/*
+	 * used by:
+	 *   - block/bio.c|685| <<bio_phys_segments>> return bio->bi_phys_segments;
+	 *   - block/bio.c|856| <<__bio_add_pc_page>> if (bio->bi_phys_segments >= queue_max_segments(q))
+	 *   - block/bio.c|866| <<__bio_add_pc_page>> bio->bi_phys_segments = bio->bi_vcnt;
+	 *   - block/blk-merge.c|300| <<blk_queue_split>> res->bi_phys_segments = nsegs;
+	 *   - block/blk-merge.c|362| <<blk_recount_segments>> bio->bi_phys_segments = __blk_recalc_rq_segments(q, bio);
+	 */
 	unsigned int		bi_phys_segments;
 
 	struct bvec_iter	bi_iter;
 
 	atomic_t		__bi_remaining;
+	/*
+	 * called by:
+	 *   - block/bio.c|1976| <<bio_endio>> bio->bi_end_io(bio);
+	 *   - drivers/md/bcache/request.c|1101| <<detached_dev_end_io>> bio->bi_end_io(bio);
+	 *   - drivers/md/bcache/request.c|1124| <<detached_dev_do_request>> bio->bi_end_io(bio);
+	 *   - fs/btrfs/check-integrity.c|2144| <<btrfsic_bio_end_io>> bp->bi_end_io(bp);
+	 */
 	bio_end_io_t		*bi_end_io;
 
 	void			*bi_private;
@@ -210,7 +225,42 @@ struct bio {
  */
 enum {
 	BIO_NO_PAGE_REF,	/* don't put release vec pages */
+	/*
+	 * 似乎如果没设置, 就需要重新计算bio的segment的数量
+	 *
+	 * 设置和清除BIO_SEG_VALID的地方:
+	 *   - block/bio.c|867| <<__bio_add_pc_page>> bio_set_flag(bio, BIO_SEG_VALID);
+	 *   - block/blk-merge.c|301| <<blk_queue_split>> bio_set_flag(res, BIO_SEG_VALID);
+	 *   - block/blk-merge.c|365| <<blk_recount_segments>> bio_set_flag(bio, BIO_SEG_VALID);
+	 *   - block/bio.c|2036| <<bio_trim>> bio_clear_flag(bio, BIO_SEG_VALID);
+	 *   - drivers/md/raid5.c|5254| <<raid5_read_one_chunk>> bio_clear_flag(align_bi, BIO_SEG_VALID);
+	 * 使用BIO_SEG_VALID的地方:
+	 *   - block/bio.c|682| <<bio_phys_segments>> if (unlikely(!bio_flagged(bio, BIO_SEG_VALID)))
+	 *   - block/blk-merge.c|575| <<ll_back_merge_fn>> if (!bio_flagged(req->biotail, BIO_SEG_VALID))
+	 *   - block/blk-merge.c|577| <<ll_back_merge_fn>> if (!bio_flagged(bio, BIO_SEG_VALID))
+	 *   - block/blk-merge.c|597| <<ll_front_merge_fn>> if (!bio_flagged(bio, BIO_SEG_VALID))
+	 *   - block/blk-merge.c|599| <<ll_front_merge_fn>> if (!bio_flagged(req->bio, BIO_SEG_VALID))
+	 */
 	BIO_SEG_VALID,		/* bi_phys_segments valid */
+	/*
+	 * 设置BIO_CLONED的地方:
+	 *   - block/bio.c|709| <<__bio_clone_fast>> bio_set_flag(bio, BIO_CLONED);
+	 * 使用BIO_CLONED的地方:
+	 *   - block/bio.c|822| <<__bio_add_pc_page>> if (unlikely(bio_flagged(bio, BIO_CLONED)))
+	 *   - block/bio.c|897| <<__bio_try_merge_page>> if (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))
+	 *   - block/bio.c|928| <<__bio_add_page>> WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/compression.c|170| <<end_compressed_bio_read>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/disk-io.c|855| <<btree_csum_one_bio>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/extent_io.c|2587| <<end_bio_extent_writepage>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/extent_io.c|2658| <<end_bio_extent_readpage>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/extent_io.c|3759| <<end_bio_extent_buffer_writepage>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/inode.c|7896| <<btrfs_retry_endio_nocsum>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/inode.c|7989| <<btrfs_retry_endio>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/raid56.c|1159| <<index_rbio_pages>> if (bio_flagged(bio, BIO_CLONED))
+	 *   - fs/btrfs/raid56.c|1447| <<set_bio_pages_uptodate>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - include/linux/bio.h|271| <<bio_first_bvec_all>> WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
+	 *   - include/linux/bio.h|282| <<bio_last_bvec_all>> WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
+	 */
 	BIO_CLONED,		/* doesn't own data */
 	BIO_BOUNCED,		/* bio is a bounce bio */
 	BIO_USER_MAPPED,	/* contains user pages */
@@ -295,6 +345,11 @@ enum req_opf {
 	REQ_OP_SCSI_IN		= 32,
 	REQ_OP_SCSI_OUT		= 33,
 	/* Driver private requests */
+	/*
+	 * 部分使用的例子:
+	 *   - drivers/block/virtio_blk.c|485| <<virtblk_get_id>> req = blk_get_request(q, REQ_OP_DRV_IN, 0);
+	 *   - drivers/nvme/host/core.c|495| <<nvme_alloc_request>> unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	 */
 	REQ_OP_DRV_IN		= 34,
 	REQ_OP_DRV_OUT		= 35,
 
@@ -337,11 +392,26 @@ enum req_flag_bits {
 #define REQ_PRIO		(1ULL << __REQ_PRIO)
 #define REQ_NOMERGE		(1ULL << __REQ_NOMERGE)
 #define REQ_IDLE		(1ULL << __REQ_IDLE)
+/*
+ * used by:
+ *   - block/bio-integrity.c|74| <<bio_integrity_alloc>> bio->bi_opf |= REQ_INTEGRITY;
+ *   - block/bio-integrity.c|108| <<bio_integrity_free>> bio->bi_opf &= ~REQ_INTEGRITY;
+ *   - drivers/md/dm-integrity.c|1440| <<integrity_end_io>> bio->bi_opf |= REQ_INTEGRITY;
+ *   - drivers/md/dm-integrity.c|1983| <<dm_integrity_map_continue>> bio->bi_opf &= ~REQ_INTEGRITY;
+ *   - drivers/nvme/host/core.c|980| <<nvme_submit_user_cmd>> req->cmd_flags |= REQ_INTEGRITY;
+ *   - include/linux/bio.h|439| <<bio_integrity>> if (bio->bi_opf & REQ_INTEGRITY)
+ *   - include/linux/blkdev.h|1843| <<blk_integrity_rq>> return rq->cmd_flags & REQ_INTEGRITY;
+ */
 #define REQ_INTEGRITY		(1ULL << __REQ_INTEGRITY)
 #define REQ_FUA			(1ULL << __REQ_FUA)
 #define REQ_PREFLUSH		(1ULL << __REQ_PREFLUSH)
 #define REQ_RAHEAD		(1ULL << __REQ_RAHEAD)
 #define REQ_BACKGROUND		(1ULL << __REQ_BACKGROUND)
+/*
+ * 设置REQ_NOWAIT的地方:
+ *   - fs/direct-io.c|1267| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_NOWAIT;
+ *   - include/linux/bio.h|830| <<bio_set_polled>> bio->bi_opf |= REQ_NOWAIT;
+ */
 #define REQ_NOWAIT		(1ULL << __REQ_NOWAIT)
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 592669b..53baecb 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -72,12 +72,38 @@ typedef __u32 __bitwise req_flags_t;
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
+/*
+ * 设置和取消RQF_FLUSH_SEQ的地方:
+ *   - block/blk-flush.c|130| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|309| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|401| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+ */
 #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
 /* track inflight for MQ */
 #define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
 /* don't call prep for this one */
+/*
+ * 在以下设置或者清除RQF_DONTPREP:
+ *   - drivers/block/skd_main.c|499| <<skd_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/ide/ide-io.c|464| <<ide_issue_rq>> rq->rq_flags |= RQF_DONTPREP;
+ *   - drivers/mmc/core/queue.c|297| <<mmc_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/nvme/host/core.c|424| <<nvme_clear_nvme_request>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/scsi/scsi_lib.c|1657| <<scsi_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/scsi/scsi_lib.c|146| <<scsi_mq_requeue_cmd>> cmd->request->rq_flags &= ~RQF_DONTPREP;
+ *
+ * 在以下使用RQF_DONTPREP:
+ *   - block/blk-mq.c|998| <<blk_mq_requeue_work>> if (!(rq->rq_flags & (RQF_SOFTBARRIER | RQF_DONTPREP)))
+ *   - block/blk-mq.c|1008| <<blk_mq_requeue_work>> if (rq->rq_flags & RQF_DONTPREP)
+ *   - drivers/block/skd_main.c|497| <<skd_mq_queue_rq>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/ide/ide-io.c|463| <<ide_issue_rq>> if (!blk_rq_is_passthrough(rq) && !(rq->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/mmc/core/queue.c|295| <<mmc_mq_queue_rq>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/nvme/host/core.c|421| <<nvme_clear_nvme_request>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/scsi/scsi_lib.c|145| <<scsi_mq_requeue_cmd>> if (cmd->request->rq_flags & RQF_DONTPREP) {
+ *   - drivers/scsi/scsi_lib.c|1653| <<scsi_queue_rq>> if (!(req->rq_flags & RQF_DONTPREP)) {
+ *   - drivers/scsi/scsi_lib.c|1705| <<scsi_queue_rq>> if (req->rq_flags & RQF_DONTPREP)
+ */
 #define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
 /* set for "ide_preempt" requests and also for requests for which the SCSI
    "quiesce" state must be ignored. */
@@ -102,15 +128,36 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
+/*
+ * 设置RQF_SPECIAL_PAYLOAD的地方:
+ *   - block/blk-core.c|1644| <<__blk_rq_prep_clone>> dst->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ *   - drivers/block/virtio_blk.c|248| <<virtblk_setup_discard_write_zeroes>> req->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ *   - drivers/nvme/host/core.c|665| <<nvme_setup_discard>> req->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ *   - drivers/scsi/sd.c|838| <<sd_setup_unmap_cmnd>> rq->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ *   - drivers/scsi/sd.c|872| <<sd_setup_write_same16_cmnd>> rq->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ *   - drivers/scsi/sd.c|903| <<sd_setup_write_same10_cmnd>> rq->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ *   - include/linux/blkdev.h|152| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+ */
 #define RQF_SPECIAL_PAYLOAD	((__force req_flags_t)(1 << 18))
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
+/*
+ * 在以下使用RQF_TIMED_OUT:
+ *   - block/blk-mq.c|1113| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+ *   - block/blk-mq.c|968| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-mq.c|1132| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+ */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
+/*
+ * used by only:
+ *   - include/linux/blkdev.h|779| <<rq_mergeable>> if (rq->rq_flags & RQF_NOMERGE_FLAGS)
+ */
 #define RQF_NOMERGE_FLAGS \
 	(RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
 
@@ -118,8 +165,30 @@ typedef __u32 __bitwise req_flags_t;
  * Request state for blk-mq.
  */
 enum mq_rq_state {
+	/*
+	 * 修改和使用MQ_RQ_IDLE的地方:
+	 *   - block/blk-mq.c|730| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|961| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2791| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|935| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|896| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *   - block/blk-mq-debugfs.c|328| <<global>> [MQ_RQ_IDLE] = "idle",
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 修改和使用MQ_RQ_IN_FLIGHT的地方:
+	 *   - block/blk-mq.c|938| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|1086| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|1124| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 *   - block/blk-mq-debugfs.c|329| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 修改和使用MQ_RQ_COMPLETE的地方:
+	 *   - block/blk-mq.c|810| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|889| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|4193| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -210,6 +279,21 @@ struct request {
 	 * Number of scatter-gather DMA addr+len pairs after
 	 * physical address coalescing is performed.
 	 */
+	/*
+	 * 修改nr_phys_segments的地方:
+	 *   - block/blk-core.c|611| <<bio_attempt_discard_merge>> req->nr_phys_segments = segments + 1;
+	 *   - block/blk-core.c|1462| <<blk_rq_bio_prep>> rq->nr_phys_segments = bio_phys_segments(q, bio);
+	 *   - block/blk-core.c|1464| <<blk_rq_bio_prep>> rq->nr_phys_segments = 1;
+	 *   - block/blk-core.c|1551| <<__blk_rq_prep_clone>> dst->nr_phys_segments = src->nr_phys_segments;
+	 *   - block/blk-merge.c|354| <<blk_recalc_rq_segments>> rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
+	 *   - block/blk-merge.c|554| <<ll_new_hw_segment>> req->nr_phys_segments += nr_phys_segs;
+	 *   - block/blk-merge.c|616| <<req_attempt_discard_merge>> req->nr_phys_segments = segments + blk_rq_nr_discard_segments(next);
+	 *   - block/blk-merge.c|638| <<ll_merge_requests_fn>> total_phys_segments = req->nr_phys_segments + next->nr_phys_segments;
+	 *   - block/blk-merge.c|646| <<ll_merge_requests_fn>> req->nr_phys_segments = total_phys_segments;
+	 *   - block/blk-mq.c|494| <<blk_mq_rq_ctx_init>> rq->nr_phys_segments = 0;
+	 *   - block/blk-mq.c|956| <<blk_mq_start_request>> rq->nr_phys_segments++;
+	 *   - block/blk-mq.c|980| <<__blk_mq_requeue_request>> rq->nr_phys_segments--;
+	 */
 	unsigned short nr_phys_segments;
 
 #if defined(CONFIG_BLK_DEV_INTEGRITY)
@@ -313,6 +397,15 @@ enum blk_zoned_model {
 struct queue_limits {
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
+	/*
+	 * used by:
+	 *   - block/blk-settings.c|43| <<blk_set_default_limits>> lim->virt_boundary_mask = 0;
+	 *   - block/blk-settings.c|314| <<blk_queue_max_segment_size>> WARN_ON_ONCE(q->limits.virt_boundary_mask);
+	 *   - block/blk-settings.c|512| <<blk_stack_limits>> t->virt_boundary_mask = min_not_zero(t->virt_boundary_mask,
+	 *   - block/blk-settings.c|513| <<blk_stack_limits>> b->virt_boundary_mask);
+	 *   - block/blk-settings.c|747| <<blk_queue_virt_boundary>> q->limits.virt_boundary_mask = mask;
+	 *   - include/linux/blkdev.h|1424| <<queue_virt_boundary>> return q->limits.virt_boundary_mask;
+	 */
 	unsigned long		virt_boundary_mask;
 
 	unsigned int		max_hw_sectors;
@@ -409,6 +502,10 @@ struct request_queue {
 
 	/* hw dispatch queues */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 只在下面一处修改:
+	 *   - block/blk-mq.c|3609| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -428,6 +525,15 @@ struct request_queue {
 	 * counter is above zero then only RQF_PM and RQF_PREEMPT requests are
 	 * processed.
 	 */
+	/*
+	 * used by:
+	 *   - block/blk-core.c|262| <<blk_set_pm_only>> atomic_inc(&q->pm_only);
+	 *   - block/blk-core.c|270| <<blk_clear_pm_only>> pm_only = atomic_dec_return(&q->pm_only);
+	 *   - include/linux/blkdev.h|827| <<blk_queue_pm_only>> #define blk_queue_pm_only(q) atomic_read(&(q)->pm_only)
+	 *   - block/blk-core.c|271| <<blk_clear_pm_only>> WARN_ON_ONCE(pm_only < 0);
+	 *   - block/blk-core.c|272| <<blk_clear_pm_only>> if (pm_only == 0)
+	 *   - block/blk-mq-debugfs.c|99| <<queue_pm_only_show>> seq_printf(m, "%d\n", atomic_read(&q->pm_only));
+	 */
 	atomic_t		pm_only;
 
 	/*
@@ -468,6 +574,11 @@ struct request_queue {
 	 */
 	unsigned long		nr_requests;	/* Max # of requests */
 
+	/*
+	 * 设置dma_drain_size的地方:
+	 *   - block/blk-settings.c|723| <<blk_queue_dma_drain>> q->dma_drain_size = size;
+	 *   - drivers/ata/libata-scsi.c|1375| <<ata_scsi_slave_destroy>> q->dma_drain_size = 0;
+	 */
 	unsigned int		dma_drain_size;
 	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
@@ -476,10 +587,31 @@ struct request_queue {
 	unsigned int		rq_timeout;
 	int			poll_nsec;
 
+	/*
+	 * poll_cb在以下使用:
+	 *   - block/blk-mq.c|3739| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+	 *   - block/blk-mq.c|3742| <<blk_mq_init_allocated_queue>> if (!q->poll_cb)
+	 *   - block/blk-mq.c|3811| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+	 *   - block/blk-mq.c|3812| <<blk_mq_init_allocated_queue>> q->poll_cb = NULL;
+	 *   - block/blk-mq.c|4248| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+	 *   - block/blk-mq.c|4259| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+	 *   - block/blk-mq.c|4262| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+	 *   - block/blk-sysfs.c|890| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+	 *   - block/blk-sysfs.c|891| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+	 */
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
 	struct timer_list	timeout;
+	/*
+	 * 初始化q->timeout_work的地方:
+	 *   - block/blk-core.c|467| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 *   - block/blk-mq.c|3606| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 * 调用和取消q->timeout_work的地方:
+	 *   - block/blk-core.c|418| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work); 
+	 *   - block/blk-timeout.c|89| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 *   - block/blk-core.c|235| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 */
 	struct work_struct	timeout_work;
 
 	struct list_head	icq_list;
@@ -542,6 +674,14 @@ struct request_queue {
 	struct list_head	unused_hctx_list;
 	spinlock_t		unused_hctx_lock;
 
+	/*
+	 * used by:
+	 *   - block/blk-core.c|427| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|253| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|314| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 *   - block/blk-mq.c|315| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|316| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	int			mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -562,7 +702,31 @@ struct request_queue {
 	struct percpu_ref	q_usage_counter;
 
 	struct blk_mq_tag_set	*tag_set;
+	/*
+	 * used by:
+	 *   - block/blk-mq.c|3325| <<blk_mq_update_tag_set_depth>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3337| <<blk_mq_del_queue_tag_set>> list_del_rcu(&q->tag_set_list);
+	 *   - block/blk-mq.c|3345| <<blk_mq_del_queue_tag_set>> INIT_LIST_HEAD(&q->tag_set_list);
+	 *   - block/blk-mq.c|3364| <<blk_mq_add_queue_tag_set>> list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
+	 *   - block/blk-mq.c|4081| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|4092| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|4096| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|4105| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|4117| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|4123| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|4126| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 */
 	struct list_head	tag_set_list;
+	/*
+	 * used by:
+	 *   - block/blk-core.c|464| <<blk_alloc_queue_node>> ret = bioset_init(&q->bio_split, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
+	 *   - block/blk-core.c|522| <<blk_alloc_queue_node>> bioset_exit(&q->bio_split);
+	 *   - block/blk-merge.c|301| <<blk_queue_split>> split = blk_bio_discard_split(q, *bio, &q->bio_split, &nsegs);
+	 *   - block/blk-merge.c|304| <<blk_queue_split>> split = blk_bio_write_zeroes_split(q, *bio, &q->bio_split, &nsegs);
+	 *   - block/blk-merge.c|307| <<blk_queue_split>> split = blk_bio_write_same_split(q, *bio, &q->bio_split, &nsegs);
+	 *   - block/blk-merge.c|310| <<blk_queue_split>> split = blk_bio_segment_split(q, *bio, &q->bio_split, &nsegs);
+	 *   - block/blk-sysfs.c|907| <<__blk_release_queue>> bioset_exit(&q->bio_split);
+	 */
 	struct bio_set		bio_split;
 
 #ifdef CONFIG_BLK_DEBUG_FS
@@ -571,6 +735,11 @@ struct request_queue {
 	struct dentry		*rqos_debugfs_dir;
 #endif
 
+	/*
+	 * 在以下修改:
+	 *   - block/blk-mq-sysfs.c|282| <<blk_mq_unregister_dev>> q->mq_sysfs_init_done = false;
+	 *   - block/blk-mq-sysfs.c|337| <<__blk_mq_register_dev>> q->mq_sysfs_init_done = true;
+	 */
 	bool			mq_sysfs_init_done;
 
 	size_t			cmd_size;
@@ -582,7 +751,25 @@ struct request_queue {
 };
 
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
+/*
+ * used by:
+ *   - block/blk-core.c|285| <<blk_set_queue_dying>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+ *   - block/blk-core.c|317| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+ *   - include/linux/blkdev.h|778| <<blk_queue_dying>> #define blk_queue_dying(q) test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
+/*
+ * QUEUE_FLAG_NOMERGES使用的地方:
+ *   - block/blk-core.c|298| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|316| <<queue_nomerges_store>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|319| <<queue_nomerges_store>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - drivers/block/loop.c|220| <<__loop_update_dio>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|2019| <<loop_add>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1901| <<megasas_set_nvme_device_properties>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, sdev->request_queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2368| <<scsih_slave_configure>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES,
+ *   - include/linux/blkdev.h|632| <<blk_queue_nomerges>> #define blk_queue_nomerges(q) test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
@@ -596,14 +783,37 @@ struct request_queue {
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+/*
+ * used by:
+ *   - block/blk-core.c|912| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-mq.c|3690| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-mq.c|4336| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+ *   - drivers/nvme/host/core.c|781| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+ */
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下设置和使用:
+ *   - block/blk-stat.c|156| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|192| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|166| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-mq.c|926| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * used by:
+ *   - block/blk-mq.c|225| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|266| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ *   - include/linux/blkdev.h|639| <<blk_queue_quiesced>> #define blk_queue_quiesced(q) test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
@@ -618,6 +828,20 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
+/*
+ * QUEUE_FLAG_NOMERGES使用的地方:
+ *   - block/blk-core.c|298| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|316| <<queue_nomerges_store>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
+ *   - block/blk-sysfs.c|319| <<queue_nomerges_store>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);
+ *   - drivers/block/loop.c|220| <<__loop_update_dio>> blk_queue_flag_clear(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|223| <<__loop_update_dio>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/block/loop.c|2019| <<loop_add>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, lo->lo_queue);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1901| <<megasas_set_nvme_device_properties>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES, sdev->request_queue);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2368| <<scsih_slave_configure>> blk_queue_flag_set(QUEUE_FLAG_NOMERGES,
+ *   - include/linux/blkdev.h|632| <<blk_queue_nomerges>> #define blk_queue_nomerges(q) test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
+ *
+ * 如果q->queue_flags设置了QUEUE_FLAG_NOMERGES
+ */
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
@@ -636,6 +860,13 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|178| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1498| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|1861| <<__blk_mq_try_issue_directly>> if (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)) {
+ *   - drivers/mmc/core/queue.c|469| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_pm_only(q)	atomic_read(&(q)->pm_only)
 #define blk_queue_fua(q)	test_bit(QUEUE_FLAG_FUA, &(q)->queue_flags)
@@ -719,6 +950,15 @@ static inline bool rq_is_sync(struct request *rq)
 	return op_is_sync(rq->cmd_flags);
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|5077| <<bfq_insert_request>> if (rq_mergeable(rq)) {
+ *   - block/blk-merge.c|712| <<attempt_merge>> if (!rq_mergeable(req) || !rq_mergeable(next))
+ *   - block/blk-merge.c|835| <<blk_rq_merge_ok>> if (!rq_mergeable(rq) || !bio_mergeable(bio))
+ *   - block/blk-mq-sched.c|438| <<blk_mq_sched_try_insert_merge>> return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
+ *   - block/elevator.c|229| <<elv_rqhash_find>> if (unlikely(!rq_mergeable(rq))) {
+ *   - block/mq-deadline.c|518| <<dd_insert_request>> if (rq_mergeable(rq)) {
+ */
 static inline bool rq_mergeable(struct request *rq)
 {
 	if (blk_rq_is_passthrough(rq))
@@ -958,6 +1198,12 @@ static inline struct bio_vec req_bvec(struct request *rq)
 {
 	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
 		return rq->special_vec;
+	/*
+	 * 构造一个bvec:
+	 * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+	 * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+	 * bv_offset : 所以返回的是当前bvec"完成到"的offset
+	 */
 	return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
 }
 
@@ -1105,6 +1351,21 @@ static inline unsigned short blk_rq_nr_phys_segments(struct request *rq)
 {
 	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
 		return 1;
+	/*
+	 * 修改nr_phys_segments的地方:
+	 *   - block/blk-core.c|611| <<bio_attempt_discard_merge>> req->nr_phys_segments = segments + 1;
+	 *   - block/blk-core.c|1462| <<blk_rq_bio_prep>> rq->nr_phys_segments = bio_phys_segments(q, bio);
+	 *   - block/blk-core.c|1464| <<blk_rq_bio_prep>> rq->nr_phys_segments = 1;
+	 *   - block/blk-core.c|1551| <<__blk_rq_prep_clone>> dst->nr_phys_segments = src->nr_phys_segments;
+	 *   - block/blk-merge.c|354| <<blk_recalc_rq_segments>> rq->nr_phys_segments = __blk_recalc_rq_segments(rq->q, rq->bio);
+	 *   - block/blk-merge.c|554| <<ll_new_hw_segment>> req->nr_phys_segments += nr_phys_segs;
+	 *   - block/blk-merge.c|616| <<req_attempt_discard_merge>> req->nr_phys_segments = segments + blk_rq_nr_discard_segments(next);
+	 *   - block/blk-merge.c|638| <<ll_merge_requests_fn>> total_phys_segments = req->nr_phys_segments + next->nr_phys_segments;
+	 *   - block/blk-merge.c|646| <<ll_merge_requests_fn>> req->nr_phys_segments = total_phys_segments;
+	 *   - block/blk-mq.c|494| <<blk_mq_rq_ctx_init>> rq->nr_phys_segments = 0;
+	 *   - block/blk-mq.c|956| <<blk_mq_start_request>> rq->nr_phys_segments++;
+	 *   - block/blk-mq.c|980| <<__blk_mq_requeue_request>> rq->nr_phys_segments--;
+	 */
 	return rq->nr_phys_segments;
 }
 
@@ -1143,6 +1404,11 @@ struct blk_plug {
 	struct list_head mq_list; /* blk-mq requests */
 	struct list_head cb_list; /* md requires an unplug callback */
 	unsigned short rq_count;
+	/*
+	 * 设置multiple_queues的地方:
+	 *   - block/blk-core.c|1668| <<blk_start_plug>> plug->multiple_queues = false;
+	 *   - block/blk-mq.c|2337| <<blk_add_rq_to_plug>> plug->multiple_queues = true;
+	 */
 	bool multiple_queues;
 };
 #define BLK_MAX_REQUEST_COUNT 16
@@ -1243,13 +1509,49 @@ static inline unsigned long queue_segment_boundary(struct request_queue *q)
 	return q->limits.seg_boundary_mask;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|131| <<blk_rq_map_user_iov>> else if (queue_virt_boundary(q))
+ *   - block/blk-map.c|132| <<blk_rq_map_user_iov>> copy = queue_virt_boundary(q) & iov_iter_gap_alignment(iter);
+ *   - block/blk-merge.c|20| <<bio_will_gap>> if (!bio_has_data(prev) || !queue_virt_boundary(q))
+ *   - block/blk-merge.c|32| <<bio_will_gap>> if (pb.bv_offset & queue_virt_boundary(q))
+ *   - block/blk-merge.c|188| <<bvec_split_segs>> if ((bv->bv_offset + total_len) & queue_virt_boundary(q))
+ *   - block/blk.h|88| <<__bvec_gap_to_prev>> return (offset & queue_virt_boundary(q)) ||
+ *   - block/blk.h|89| <<__bvec_gap_to_prev>> ((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
+ *   - block/blk.h|99| <<bvec_gap_to_prev>> if (!queue_virt_boundary(q))
+ */
 static inline unsigned long queue_virt_boundary(struct request_queue *q)
 {
+	/*
+	 * used by:
+	 *   - block/blk-settings.c|43| <<blk_set_default_limits>> lim->virt_boundary_mask = 0;
+	 *   - block/blk-settings.c|314| <<blk_queue_max_segment_size>> WARN_ON_ONCE(q->limits.virt_boundary_mask);
+	 *   - block/blk-settings.c|512| <<blk_stack_limits>> t->virt_boundary_mask = min_not_zero(t->virt_boundary_mask,
+	 *   - block/blk-settings.c|513| <<blk_stack_limits>> b->virt_boundary_mask);
+	 *   - block/blk-settings.c|747| <<blk_queue_virt_boundary>> q->limits.virt_boundary_mask = mask;
+	 *   - include/linux/blkdev.h|1424| <<queue_virt_boundary>> return q->limits.virt_boundary_mask;
+	 */
 	return q->limits.virt_boundary_mask;
+
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/iser/iscsi_iser.c|978| <<iscsi_iser_slave_alloc>> blk_queue_virt_boundary(sdev->request_queue, ~MASK_4K);
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|3074| <<srp_slave_alloc>> blk_queue_virt_boundary(sdev->request_queue,
+	 *   - drivers/nvme/host/core.c|2029| <<nvme_set_queue_limits>> blk_queue_virt_boundary(q, ctrl->page_size - 1);
+	 *   - drivers/scsi/megaraid/megaraid_sas_base.c|1902| <<megasas_set_nvme_device_properties>> blk_queue_virt_boundary(sdev->request_queue, mr_nvme_pg_size - 1);
+	 *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|2370| <<scsih_slave_configure>> blk_queue_virt_boundary(sdev->request_queue,
+	 *   - drivers/scsi/storvsc_drv.c|1426| <<storvsc_device_configure>> blk_queue_virt_boundary(sdevice->request_queue, PAGE_SIZE - 1);
+	 *   - drivers/usb/storage/scsiglue.c|84| <<slave_alloc>> blk_queue_virt_boundary(sdev->request_queue, maxp - 1);
+	 *   - drivers/usb/storage/uas.c|813| <<uas_slave_alloc>> blk_queue_virt_boundary(sdev->request_queue, maxp - 1);
+	 */
 }
 
 static inline unsigned int queue_max_sectors(struct request_queue *q)
 {
+	/*
+	 * blk_queue_max_sectors() can be used to set the maximum size of
+	 * any request in (512-byte) sectors.
+	 */
 	return q->limits.max_sectors;
 }
 
@@ -1268,6 +1570,17 @@ static inline unsigned short queue_max_discard_segments(struct request_queue *q)
 	return q->limits.max_discard_segments;
 }
 
+/*
+ * called by:
+ *   - block/bio.c|671| <<can_add_page_to_seg>> if (bv->bv_len + len > queue_max_segment_size(q))
+ *   - block/blk-integrity.c|40| <<blk_rq_count_integrity_sg>> if (seg_size + iv.bv_len > queue_max_segment_size(q))
+ *   - block/blk-integrity.c|82| <<blk_rq_map_integrity_sg>> if (sg->length + iv.bv_len > queue_max_segment_size(q))
+ *   - block/blk-merge.c|154| <<get_max_segment_size>> return queue_max_segment_size(q);
+ *   - block/blk-merge.c|157| <<get_max_segment_size>> queue_max_segment_size(q));
+ *   - block/blk-merge.c|433| <<__blk_segment_map_sg_merge>> if ((*sg)->length + nbytes > queue_max_segment_size(q))
+ *   - block/blk-sysfs.c|135| <<queue_max_segment_size_show>> return queue_var_show(queue_max_segment_size(q), (page));
+ *   - drivers/mmc/core/queue.c|376| <<mmc_setup_queue>> dma_set_max_seg_size(mmc_dev(host), queue_max_segment_size(mq->queue));
+ */
 static inline unsigned int queue_max_segment_size(struct request_queue *q)
 {
 	return q->limits.max_segment_size;
diff --git a/include/linux/bvec.h b/include/linux/bvec.h
index a032f01..3d6527b 100644
--- a/include/linux/bvec.h
+++ b/include/linux/bvec.h
@@ -42,22 +42,58 @@ struct bvec_iter_all {
  * various member access, note that bio_data should of course not be used
  * on highmem page vectors
  */
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec
+ */
 #define __bvec_iter_bvec(bvec, iter)	(&(bvec)[(iter).bi_idx])
 
 /* multi-page (mp_bvec) helpers */
+/*
+ * called by:
+ *   - include/linux/bvec.h|63| <<mp_bvec_iter_bvec>> .bv_page = mp_bvec_iter_page((bvec), (iter)), \
+ *   - include/linux/bvec.h|77| <<bvec_iter_page>> (mp_bvec_iter_page((bvec), (iter)) + \
+ *
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ */
 #define mp_bvec_iter_page(bvec, iter)				\
 	(__bvec_iter_bvec((bvec), (iter))->bv_page)
 
+/*
+ * 这里二个选最小的:
+ * 1. (iter).bi_size: 所有bvec剩下的
+ * 2. __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done: 当前bvec上下的
+ *
+ * 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ */
 #define mp_bvec_iter_len(bvec, iter)				\
 	min((iter).bi_size,					\
 	    __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done)
 
+/*
+ * __bvec_iter_bvec((bvec), (iter))->bv_offset: 当前bvec起始的offset
+ * (iter).bi_bvec_done: 当前bvec已经完成的
+ *
+ * 所以返回的是当前bvec"完成到"的offset
+ */
 #define mp_bvec_iter_offset(bvec, iter)				\
 	(__bvec_iter_bvec((bvec), (iter))->bv_offset + (iter).bi_bvec_done)
 
+/*
+ * 返回当前bvec中已经完成到的offset所在的以PAGE_SIZE为单位的index
+ */
 #define mp_bvec_iter_page_idx(bvec, iter)			\
 	(mp_bvec_iter_offset((bvec), (iter)) / PAGE_SIZE)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|158| <<__bio_for_each_bvec>> ((bvl = mp_bvec_iter_bvec((bio)->bi_io_vec, (iter))), 1); \
+ *   - include/linux/blkdev.h|1151| <<req_bvec>> return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
+ *
+ * 构造一个bvec:
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * bv_offset : 所以返回的是当前bvec"完成到"的offset
+ */
 #define mp_bvec_iter_bvec(bvec, iter)				\
 ((struct bio_vec) {						\
 	.bv_page	= mp_bvec_iter_page((bvec), (iter)),	\
@@ -66,17 +102,55 @@ struct bvec_iter_all {
 })
 
 /* For building single-page bvec in flight */
+/*
+ * called by:
+ *   - include/linux/bio.h|37| <<bio_iter_offset>> bvec_iter_offset((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|110| <<bvec_iter_len>> PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
+ *   - include/linux/bvec.h|120| <<bvec_iter_bvec>> .bv_offset = bvec_iter_offset((bvec), (iter)), \
+ *
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
  #define bvec_iter_offset(bvec, iter)				\
 	(mp_bvec_iter_offset((bvec), (iter)) % PAGE_SIZE)
 
+/*
+ * 二个进行比较:
+ * 1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * 2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ */
 #define bvec_iter_len(bvec, iter)				\
 	min_t(unsigned, mp_bvec_iter_len((bvec), (iter)),		\
 	      PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
 
+/*
+ * mp_bvec_iter_page((bvec), (iter)): 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * mp_bvec_iter_page_idx((bvec), (iter)): 返回当前bvec中已经完成到的offset所在的以PAGE_SIZE为单位的index
+ *
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bvec_iter_page(bvec, iter)				\
 	(mp_bvec_iter_page((bvec), (iter)) +			\
 	 mp_bvec_iter_page_idx((bvec), (iter)))
 
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|1755| <<__journal_read_write>> struct bio_vec biv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - drivers/md/dm-io.c|211| <<bio_get_page>> struct bio_vec bvec = bvec_iter_bvec((struct bio_vec *)dp->context_ptr,
+ *   - drivers/nvdimm/blk.c|84| <<nd_blk_rw_integrity>> bv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - drivers/nvdimm/btt.c|1158| <<btt_rw_integrity>> bv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - include/linux/bio.h|30| <<bio_iter_iovec>> bvec_iter_bvec((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|163| <<for_each_bvec>> ((bvl = bvec_iter_bvec((bio_vec), (iter))), 1); \
+ *   - net/ceph/messenger.c|892| <<ceph_msg_data_bvecs_next>> struct bio_vec bv = bvec_iter_bvec(cursor->data->bvec_pos.bvecs,
+ *
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bvec_iter_bvec(bvec, iter)				\
 ((struct bio_vec) {						\
 	.bv_page	= bvec_iter_page((bvec), (iter)),	\
@@ -84,6 +158,19 @@ struct bvec_iter_all {
 	.bv_offset	= bvec_iter_offset((bvec), (iter)),	\
 })
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|376| <<bio_integrity_advance>> bvec_iter_advance(bip->bip_vec, &bip->bip_iter, bytes);
+ *   - drivers/md/dm-integrity.c|1764| <<__journal_read_write>> bvec_iter_advance(bip->bip_vec, &bip->bip_iter, tag_now);
+ *   - drivers/md/dm-io.c|226| <<bio_next_page>> bvec_iter_advance((struct bio_vec *)dp->context_ptr,
+ *   - drivers/nvdimm/blk.c|101| <<nd_blk_rw_integrity>> if (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))
+ *   - drivers/nvdimm/btt.c|1182| <<btt_rw_integrity>> if (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))
+ *   - include/linux/bio.h|142| <<bio_advance_iter>> bvec_iter_advance(bio->bi_io_vec, iter, bytes);
+ *   - include/linux/bvec.h|193| <<for_each_bvec>> bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
+ *   - include/linux/ceph/messenger.h|139| <<__ceph_bvec_iter_advance_step>> bvec_iter_advance((it)->bvecs, &(it)->iter, (n)); \
+ *   - net/ceph/messenger.c|909| <<ceph_msg_data_bvecs_advance>> bvec_iter_advance(bvecs, &cursor->bvec_iter, bytes);
+ *   - net/sunrpc/xprtsock.c|390| <<xs_flush_bvec>> bvec_iter_advance(bvec, &bi, seek & PAGE_MASK);
+ */
 static inline bool bvec_iter_advance(const struct bio_vec *bv,
 		struct bvec_iter *iter, unsigned bytes)
 {
@@ -95,6 +182,12 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 
 	while (bytes) {
 		const struct bio_vec *cur = bv + iter->bi_idx;
+		/*
+		 * 三个里面选择最小值:
+		 * 1. bytes                           : 想要advance的
+		 * 2. iter->bi_size                   : 所有bvec剩下的
+		 * 3. cur->bv_len - iter->bi_bvec_done: 当前bvec剩下的
+		 */
 		unsigned len = min3(bytes, iter->bi_size,
 				    cur->bv_len - iter->bi_bvec_done);
 
@@ -102,6 +195,11 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 		iter->bi_size -= len;
 		iter->bi_bvec_done += len;
 
+		/*
+		 * 如果当前bio_vec已经用完了, 则要进入下一个bio_vec (iter->bi_idx++)
+		 * 清空iter->bi_bvec_done因为是number of bytes completed in current bvec
+		 * 现在要进入下一个了
+		 */
 		if (iter->bi_bvec_done == cur->bv_len) {
 			iter->bi_bvec_done = 0;
 			iter->bi_idx++;
@@ -110,6 +208,15 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	return true;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|815| <<bip_for_each_vec>> for_each_bvec(bvl, (bip)->bip_vec, iter, (bip)->bip_iter)
+ *   - include/linux/ceph/messenger.h|158| <<ceph_bvec_iter_advance_step>> for_each_bvec(bv, (it)->bvecs, __cur_iter, __cur_iter) \
+ *   - lib/iov_iter.c|70| <<iterate_bvec>> for_each_bvec(__v, i->bvec, __bi, __start) { \
+ *   - net/sunrpc/xprtsock.c|391| <<xs_flush_bvec>> for_each_bvec(bv, bvec, bi, bi)
+ *
+ * 这里第一个参数bvl是基于一个page的bvec
+ */
 #define for_each_bvec(bvl, bio_vec, iter, start)			\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -117,6 +224,10 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	     bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
 
 /* for iterating one bio from start to end */
+/*
+ * used by:
+ *   - block/bounce.c|142| <<copy_to_high_bio_irq>> struct bvec_iter from_iter = BVEC_ITER_ALL_INIT;
+ */
 #define BVEC_ITER_ALL_INIT (struct bvec_iter)				\
 {									\
 	.bi_sector	= 0,						\
@@ -125,6 +236,10 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	.bi_bvec_done	= 0,						\
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|132| <<bio_for_each_segment_all>> for (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )
+ */
 static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
 {
 	iter_all->done = 0;
@@ -133,6 +248,10 @@ static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
 	return &iter_all->bv;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|123| <<bio_next_segment>> bvec_advance(&bio->bi_io_vec[iter->idx], iter);
+ */
 static inline void bvec_advance(const struct bio_vec *bvec,
 				struct bvec_iter_all *iter_all)
 {
@@ -149,6 +268,12 @@ static inline void bvec_advance(const struct bio_vec *bvec,
 			   bvec->bv_len - iter_all->done);
 	iter_all->done += bv->bv_len;
 
+	/*
+	 * 似乎iter_all->done是当前bvec完成的len?
+	 *
+	 * 所以这里当前bvec如果都完成了
+	 * 就要进入下一个bvec?? ---> iter_all->idx++
+	 */
 	if (iter_all->done == bvec->bv_len) {
 		iter_all->idx++;
 		iter_all->done = 0;
@@ -159,6 +284,10 @@ static inline void bvec_advance(const struct bio_vec *bvec,
  * Get the last single-page segment from the multi-page bvec and store it
  * in @seg
  */
+/*
+ * called by:
+ *   - fs/buffer.c|3046| <<guard_bio_eod>> mp_bvec_last_segment(bvec, &bv);
+ */
 static inline void mp_bvec_last_segment(const struct bio_vec *bvec,
 					struct bio_vec *seg)
 {
diff --git a/include/linux/fs.h b/include/linux/fs.h
index f7fdfe9..b9f9f19 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -3112,6 +3112,18 @@ ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     dio_iodone_t end_io, dio_submit_t submit_io,
 			     int flags);
 
+/*
+ * called by:
+ *   - fs/affs/file.c|409| <<affs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, affs_get_block);
+ *   - fs/ext2/inode.c|945| <<ext2_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, ext2_get_block);
+ *   - fs/fat/inode.c|283| <<fat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, fat_get_block);
+ *   - fs/hfs/inode.c|137| <<hfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfs_get_block);
+ *   - fs/hfsplus/inode.c|134| <<hfsplus_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfsplus_get_block);
+ *   - fs/jfs/inode.c|342| <<jfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, jfs_get_block);
+ *   - fs/nilfs2/inode.c|303| <<nilfs_direct_IO>> return blockdev_direct_IO(iocb, inode, iter, nilfs_get_block);
+ *   - fs/reiserfs/inode.c|3262| <<reiserfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter,
+ *   - fs/udf/inode.c|217| <<udf_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, udf_get_block);
+ */
 static inline ssize_t blockdev_direct_IO(struct kiocb *iocb,
 					 struct inode *inode,
 					 struct iov_iter *iter,
diff --git a/include/linux/genhd.h b/include/linux/genhd.h
index 8b5330d..931b8c3 100644
--- a/include/linux/genhd.h
+++ b/include/linux/genhd.h
@@ -23,6 +23,11 @@
 
 #define dev_to_disk(device)	container_of((device), struct gendisk, part0.__dev)
 #define dev_to_part(device)	container_of((device), struct hd_struct, __dev)
+/*
+ * struct gendisk 
+ *  -> struct hd_struct part0
+ *      -> struct device __dev
+ */
 #define disk_to_dev(disk)	(&(disk)->part0.__dev)
 #define part_to_dev(part)	(&((part)->__dev))
 
@@ -139,6 +144,19 @@ struct hd_struct {
 #define GENHD_FL_CD				8
 #define GENHD_FL_UP				16
 #define GENHD_FL_SUPPRESS_PARTITION_INFO	32
+/*
+ * 部分使用的例子:
+ *   - block/genhd.c|720| <<__device_add_disk>> !(disk->flags & (GENHD_FL_EXT_DEVT | GENHD_FL_HIDDEN)));
+ *   - drivers/block/loop.c|2046| <<loop_add>> disk->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/block/null_blk_main.c|1537| <<null_gendisk_register>> disk->flags |= GENHD_FL_EXT_DEVT | GENHD_FL_SUPPRESS_PARTITION_INFO;
+ *   - drivers/block/virtio_blk.c|975| <<virtblk_probe>> vblk->disk->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/ide/ide-gd.c|417| <<ide_gd_probe>> g->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/md/md.c|5336| <<md_alloc>> disk->flags |= GENHD_FL_EXT_DEVT;
+ *   - drivers/nvme/host/core.c|3251| <<nvme_alloc_ns>> int node = ctrl->numa_node, flags = GENHD_FL_EXT_DEVT, ret;
+ *   - drivers/nvme/host/multipath.c|331| <<nvme_mpath_alloc_disk>> head->disk->flags = GENHD_FL_EXT_DEVT;
+ *   - drivers/scsi/sd.c|3292| <<sd_probe_async>> gd->flags = GENHD_FL_EXT_DEVT;
+ *   - include/linux/genhd.h|237| <<disk_max_parts>> if (disk->flags & GENHD_FL_EXT_DEVT)
+ */
 #define GENHD_FL_EXT_DEVT			64 /* allow extended devt */
 #define GENHD_FL_NATIVE_CAPACITY		128
 #define GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE	256
@@ -199,6 +217,20 @@ struct gendisk {
 	 * non-critical accesses use RCU.  Always access through
 	 * helpers.
 	 */
+	/*
+	 * used by:
+	 *   - block/genhd.c|118| <<__disk_get_part>> struct disk_part_tbl *ptbl = rcu_dereference(disk->part_tbl);
+	 *   - block/genhd.c|170| <<disk_part_iter_init>> ptbl = rcu_dereference(disk->part_tbl);
+	 *   - block/genhd.c|208| <<disk_part_iter_next>> ptbl = rcu_dereference(piter->disk->part_tbl);
+	 *   - block/genhd.c|307| <<disk_map_sector_rcu>> ptbl = rcu_dereference(disk->part_tbl);
+	 *   - block/genhd.c|1283| <<disk_replace_part_tbl>> rcu_dereference_protected(disk->part_tbl, 1);
+	 *   - block/genhd.c|1285| <<disk_replace_part_tbl>> rcu_assign_pointer(disk->part_tbl, new_ptbl);
+	 *   - block/genhd.c|1311| <<disk_expand_part_tbl>> rcu_dereference_protected(disk->part_tbl, 1);
+	 *   - block/genhd.c|1531| <<__alloc_disk_node>> ptbl = rcu_dereference_protected(disk->part_tbl, 1);
+	 *   - block/partition-generic.c|273| <<delete_partition>> rcu_dereference_protected(disk->part_tbl, 1);
+	 *   - block/partition-generic.c|324| <<add_partition>> ptbl = rcu_dereference_protected(disk->part_tbl, 1);
+	 *   - fs/direct-io.c|1431| <<__blockdev_direct_IO>> prefetch(&bdev->bd_disk->part_tbl);
+	 */
 	struct disk_part_tbl __rcu *part_tbl;
 	struct hd_struct part0;
 
@@ -221,6 +253,19 @@ struct gendisk {
 	struct lockdep_map lockdep_map;
 };
 
+/*
+ * called by:
+ *   - block/bio.c|1739| <<update_io_ticks>> part = &part_to_disk(part)->part0;
+ *   - block/genhd.c|61| <<part_inc_in_flight>> part_stat_local_inc(&part_to_disk(part)->part0, in_flight[rw]);
+ *   - block/genhd.c|71| <<part_dec_in_flight>> part_stat_local_dec(&part_to_disk(part)->part0, in_flight[rw]);
+ *   - block/genhd.c|517| <<blk_alloc_devt>> struct gendisk *disk = part_to_disk(part);
+ *   - block/genhd.c|940| <<get_gendisk>> if (part && get_disk_and_module(part_to_disk(part))) {
+ *   - block/genhd.c|942| <<get_gendisk>> disk = part_to_disk(part);
+ *   - block/partition-generic.c|122| <<part_stat_show>> struct request_queue *q = part_to_disk(p)->queue;
+ *   - block/partition-generic.c|153| <<part_inflight_show>> struct request_queue *q = part_to_disk(p)->queue;
+ *   - include/linux/genhd.h|408| <<part_stat_add>> __part_stat_add(&part_to_disk((part))->part0, \
+ *   - init/do_mounts.c|153| <<devt_from_partuuid>> disk = part_to_disk(dev_to_part(dev));
+ */
 static inline struct gendisk *part_to_disk(struct hd_struct *part)
 {
 	if (likely(part)) {
@@ -303,9 +348,16 @@ extern struct hd_struct *disk_map_sector_rcu(struct gendisk *disk,
 #define part_stat_lock()	({ rcu_read_lock(); get_cpu(); })
 #define part_stat_unlock()	do { put_cpu(); rcu_read_unlock(); } while (0)
 
+/*
+ * part是hd_struct
+ * 里面有struct disk_stats __percpu *dkstats;
+ */
 #define part_stat_get_cpu(part, field, cpu)					\
 	(per_cpu_ptr((part)->dkstats, (cpu))->field)
 
+/*
+ * part是struct hd_struct
+ */
 #define part_stat_get(part, field)					\
 	part_stat_get_cpu(part, field, smp_processor_id())
 
@@ -674,6 +726,17 @@ extern ssize_t part_fail_store(struct device *dev,
 	__disk;								\
 })
 
+/*
+ * minors (支持的最多非ext parition)
+ *   - drivers/block/floppy.c|4524| <<do_floppy_init>> disks[drive] = alloc_disk(1);
+ *   - drivers/block/loop.c|2022| <<loop_add>> disk = lo->lo_disk = alloc_disk(1 << part_shift);
+ *   - drivers/block/virtio_blk.c|1028| <<virtblk_probe>> vblk->disk = alloc_disk(1 << PART_BITS);
+ *   - drivers/block/xen-blkfront.c|1141| <<xlvbd_alloc_gendisk>> gd = alloc_disk(nr_minors);
+ *   - drivers/md/md.c|5314| <<md_alloc>> disk = alloc_disk(1 << shift);
+ *   - drivers/nvdimm/blk.c|264| <<nsblk_attach_disk>> disk = alloc_disk(0);
+ *   - drivers/nvdimm/btt.c|1528| <<btt_blk_init>> btt->btt_disk = alloc_disk(0);
+ *   - drivers/scsi/sd.c|3364| <<sd_probe>> gd = alloc_disk(SD_MINORS);
+ */
 #define alloc_disk(minors) alloc_disk_node(minors, NUMA_NO_NODE)
 
 static inline int hd_ref_init(struct hd_struct *part)
diff --git a/include/linux/nvme.h b/include/linux/nvme.h
index 8028ada..d90c136 100644
--- a/include/linux/nvme.h
+++ b/include/linux/nvme.h
@@ -144,6 +144,12 @@ enum {
 #define NVME_NVM_IOCQES		4
 
 enum {
+	/*
+	 * 添加或者删除NVME_CC_ENABLE的地方:
+	 *   - drivers/nvme/host/core.c|1948| <<nvme_disable_ctrl>> ctrl->ctrl_config &= ~NVME_CC_ENABLE;
+	 *   - drivers/nvme/host/core.c|1984| <<nvme_enable_ctrl>> ctrl->ctrl_config |= NVME_CC_ENABLE;
+	 *   - drivers/pci/quirks.c|3771| <<nvme_disable_and_flr>> cfg &= ~(NVME_CC_SHN_MASK | NVME_CC_ENABLE);
+	 */
 	NVME_CC_ENABLE		= 1 << 0,
 	NVME_CC_CSS_NVM		= 0 << 4,
 	NVME_CC_EN_SHIFT	= 0,
@@ -593,8 +599,25 @@ enum {
  *   @NVME_TRANSPORT_SGL_DATA_DESC:	Transport SGL data dlock descriptor
  */
 enum {
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|968| <<nvme_pci_sgl_set_data>> sge->type = NVME_SGL_FMT_DATA_DESC << 4;
+	 *   - drivers/nvme/host/pci.c|1105| <<nvme_setup_sgl_simple>> cmnd->dptr.sgl.type = NVME_SGL_FMT_DATA_DESC << 4;
+	 *   - drivers/nvme/host/rdma.c|1180| <<nvme_rdma_map_sg_inline>> sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
+	 *   - drivers/nvme/host/tcp.c|1958| <<nvme_tcp_set_sg_inline>> sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
+	 *   - drivers/nvme/target/rdma.c|702| <<nvmet_rdma_map_sgl>> case NVME_SGL_FMT_DATA_DESC:
+	 *   - drivers/nvme/target/tcp.c|326| <<nvmet_tcp_map_data>> if (sgl->type == ((NVME_SGL_FMT_DATA_DESC << 4) |
+	 */
 	NVME_SGL_FMT_DATA_DESC		= 0x00,
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|985| <<nvme_pci_sgl_set_seg>> sge->type = NVME_SGL_FMT_SEG_DESC << 4;
+	 */
 	NVME_SGL_FMT_SEG_DESC		= 0x02,
+	/*
+	 * used by:
+	 *   - drivers/nvme/host/pci.c|982| <<nvme_pci_sgl_set_seg>> sge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;
+	 */
 	NVME_SGL_FMT_LAST_SEG_DESC	= 0x03,
 	NVME_KEY_SGL_FMT_DATA_DESC	= 0x04,
 	NVME_TRANSPORT_SGL_DATA_DESC	= 0x05,
@@ -671,6 +694,9 @@ struct nvme_rw_command {
 	__le64			metadata;
 	union nvme_data_ptr	dptr;
 	__le64			slba;
+	/*
+	 * number of logical blocks
+	 */
 	__le16			length;
 	__le16			control;
 	__le32			dsmgmt;
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index b297cd1..cc80fd7e 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -123,6 +123,26 @@ void percpu_ref_reinit(struct percpu_ref *ref);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|416| <<blkg_destroy>> percpu_ref_kill(&blkg->refcnt);
+ *   - block/blk-mq.c|254| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+ *   - drivers/dax/device.c|45| <<dev_dax_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/infiniband/sw/rdmavt/mr.c|277| <<rvt_free_lkey>> percpu_ref_kill(&mr->refcount);
+ *   - drivers/nvme/target/core.c|589| <<nvmet_ns_disable>> percpu_ref_kill(&ns->ref);
+ *   - drivers/pci/p2pdma.c|96| <<pci_p2pdma_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/target/target_core_transport.c|2902| <<target_sess_cmd_list_set_waiting>> percpu_ref_kill(&se_sess->cmd_count);
+ *   - drivers/target/target_core_transport.c|2934| <<transport_clear_lun_ref>> percpu_ref_kill(&lun->lun_ref);
+ *   - fs/aio.c|631| <<free_ioctx_users>> percpu_ref_kill(&ctx->reqs);
+ *   - fs/aio.c|850| <<kill_ioctx>> percpu_ref_kill(&ctx->users);
+ *   - fs/io_uring.c|2828| <<io_ring_ctx_wait_and_kill>> percpu_ref_kill(&ctx->refs);
+ *   - fs/io_uring.c|3161| <<__io_uring_register>> percpu_ref_kill(&ctx->refs);
+ *   - include/linux/genhd.h|767| <<hd_struct_kill>> percpu_ref_kill(&part->ref);
+ *   - kernel/cgroup/cgroup.c|2225| <<cgroup_kill_sb>> percpu_ref_kill(&root->cgrp.self.refcnt);
+ *   - kernel/cgroup/cgroup.c|5536| <<__acquires>> percpu_ref_kill(&cgrp->self.refcnt);
+ *   - mm/backing-dev.c|527| <<cgwb_kill>> percpu_ref_kill(&wb->refcnt);
+ *   - mm/hmm.c|1368| <<hmm_devmem_ref_kill>> percpu_ref_kill(ref);
+ */
 static inline void percpu_ref_kill(struct percpu_ref *ref)
 {
 	percpu_ref_kill_and_confirm(ref, NULL);
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 071a76c..485be6f 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -326,6 +326,12 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_percpu);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|738| <<nvmet_sq_destroy>> percpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);
+ *   - include/linux/percpu-refcount.h|128| <<percpu_ref_kill>> percpu_ref_kill_and_confirm(ref, NULL);
+ *   - kernel/cgroup/cgroup.c|5444| <<kill_css>> percpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);
+ */
 void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_kill)
 {
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 54f57cd..c535670 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -607,6 +607,11 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->bitmap_tags);
+ *   - block/blk-mq-tag.c|58| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->breserved_tags);
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
-- 
2.7.4

