From b27faf6f276826468cf2e14b0db88f3c18d7ad71 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 29 Aug 2021 12:14:29 -0700
Subject: [PATCH 1/1] linux v5.13

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h      |  395 ++++++++
 arch/x86/include/asm/pvclock-abi.h   |   51 ++
 arch/x86/include/asm/pvclock.h       |   20 +
 arch/x86/include/uapi/asm/kvm_para.h |    7 +
 arch/x86/kernel/apic/io_apic.c       |    4 +
 arch/x86/kernel/crash.c              |    6 +
 arch/x86/kernel/kvm.c                |   27 +
 arch/x86/kernel/kvmclock.c           |   26 +
 arch/x86/kernel/pvclock.c            |   37 +
 arch/x86/kernel/tsc.c                |    6 +
 arch/x86/kvm/cpuid.c                 |    9 +
 arch/x86/kvm/emulate.c               |    4 +
 arch/x86/kvm/hyperv.c                |    4 +
 arch/x86/kvm/lapic.c                 |   43 +
 arch/x86/kvm/lapic.h                 |   15 +
 arch/x86/kvm/mmu/mmu.c               |   22 +
 arch/x86/kvm/svm/nested.c            |   15 +
 arch/x86/kvm/vmx/posted_intr.c       |   14 +
 arch/x86/kvm/vmx/vmcs.h              |   12 +
 arch/x86/kvm/vmx/vmx.c               |  230 +++++
 arch/x86/kvm/vmx/vmx.h               |   29 +
 arch/x86/kvm/x86.c                   | 1240 ++++++++++++++++++++++++++
 drivers/acpi/acpica/nseval.c         |   12 +
 drivers/acpi/acpica/psparse.c        |   93 ++
 drivers/acpi/acpica/psxface.c        |    4 +
 drivers/acpi/scan.c                  |   33 +
 drivers/acpi/utils.c                 |   14 +
 drivers/cpuidle/poll_state.c         |    7 +
 drivers/iommu/intel/dmar.c           |   30 +
 drivers/iommu/intel/irq_remapping.c  |   43 +
 drivers/iommu/iommu.c                |   78 ++
 drivers/misc/pvpanic/pvpanic.c       |   26 +
 drivers/net/virtio_net.c             |   43 +
 drivers/net/xen-netback/rx.c         |   21 +
 drivers/net/xen-netfront.c           |   19 +
 include/linux/intel-iommu.h          |    5 +
 include/linux/kvm_host.h             |   18 +
 include/linux/netdevice.h            |    7 +
 include/linux/timekeeping.h          |    6 +
 include/uapi/linux/kvm.h             |   39 +
 kernel/locking/qspinlock.c           |   64 ++
 kernel/locking/qspinlock_paravirt.h  |  152 ++++
 kernel/time/timekeeping.c            |   11 +
 mm/memory-failure.c                  |   25 +
 net/core/dev.c                       |    8 +
 virt/kvm/kvm_main.c                  |   34 +
 46 files changed, 3008 insertions(+)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9c7ced0e3171..08498e6dc529 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -58,6 +58,26 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2831| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2993| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3084| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3093| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4315| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4952| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6096| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|8118| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8179| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9360| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|9645| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10762| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|333| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|350| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * kvm_guest_time_update()
+ * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
@@ -67,11 +87,50 @@
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1270| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2158| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2336| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8243| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9288| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|10748| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 调用kvm_gen_update_masterclock()
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
+/*
+ * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+ *   - arch/x86/kvm/x86.c|2742| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|2767| <<kvm_gen_update_masterclock>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *   - arch/x86/kvm/x86.c|8053| <<kvm_hyperv_tsc_notifier>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *
+ * 给所有vCPU发送KVM_REQ_MCLOCK_INPROGRESS,将它们踢出Guest模式,该请求没有Handler,因此vCPU无法再进入Guest
+ */
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2179| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4339| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9358| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * kvm_gen_kvmclock_update()
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+ * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -686,6 +745,28 @@ struct kvm_vcpu_arch {
 
 	/* emulate context */
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->emulate_ctxt:
+	 *   - arch/x86/kvm/svm/svm.c|2150| <<svm_instr_opcode>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/trace.h|778| <<static_call>> __entry->len = vcpu->arch.emulate_ctxt->fetch.ptr
+	 *   - arch/x86/kvm/trace.h|779| <<static_call>> - vcpu->arch.emulate_ctxt->fetch.data;
+	 *   - arch/x86/kvm/trace.h|780| <<static_call>> __entry->rip = vcpu->arch.emulate_ctxt->_eip - __entry->len;
+	 *   - arch/x86/kvm/trace.h|782| <<static_call>> vcpu->arch.emulate_ctxt->fetch.data,
+	 *   - arch/x86/kvm/trace.h|784| <<static_call>> __entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt->mode);
+	 *   - arch/x86/kvm/x86.c|6512| <<emulator_read_write_onepage>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7219| <<inject_emulated_exception>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7243| <<alloc_emulate_ctxt>> vcpu->arch.emulate_ctxt = ctxt;
+	 *   - arch/x86/kvm/x86.c|7250| <<init_emulate_ctxt>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7280| <<kvm_inject_realmode_interrupt>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7601| <<x86_decode_emulated_instruction>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7627| <<x86_emulate_instruction>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|9789| <<__get_regs>> emulator_writeback_register_cache(vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|9981| <<kvm_task_switch>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|10417| <<kvm_arch_vcpu_create>> kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|10462| <<kvm_arch_vcpu_destroy>> kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|11939| <<kvm_sev_es_outs>> ret = emulator_pio_out_emulated(vcpu->arch.emulate_ctxt, size, port,
+	 *   - arch/x86/kvm/x86.c|11954| <<kvm_sev_es_ins>> ret = emulator_pio_in_emulated(vcpu->arch.emulate_ctxt, size, port,
+	 */
 	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
@@ -693,10 +774,29 @@ struct kvm_vcpu_arch {
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3506| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3510| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	unsigned int hw_tsc_khz;
 	struct gfn_to_hva_cache pv_time;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	bool pv_time_enabled;
 	/* set guest stopped flag in pvclock flags field */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|2765| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|4767| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	bool pvclock_set_guest_stopped_request;
 
 	struct {
@@ -706,21 +806,133 @@ struct kvm_vcpu_arch {
 		struct gfn_to_pfn_cache cache;
 	} st;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|526| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2357| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2363| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2482| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3289| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3591| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	u64 l1_tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3358| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset += vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4467| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|23| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|791| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|792| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2535| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4503| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/vmx.c|1991| <<vmx_write_l1_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.tsc_offset - g_tsc_offset, offset);
+	 *   - arch/x86/kvm/x86.c|4478| <<kvm_get_msr_common(MSR_IA32_TSC)>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset : vcpu->arch.tsc_offset;
+	 */
 	u64 tsc_offset;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_offset_adjustment:
+	 *   - arch/x86/kvm/x86.c|5199| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+	 *   - arch/x86/kvm/x86.c|5200| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+	 *   - arch/x86/kvm/x86.c|5201| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_offset_adjustment = 0;
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_hardware_enable>> vcpu->arch.tsc_offset_adjustment += delta_cyc;
+	 */
 	u64 tsc_offset_adjustment;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2319| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2517| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2454| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_shift,
+	 *   - arch/x86/kvm/x86.c|2531| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|361| <<nsec_to_cycles>> vcpu->arch.virtual_tsc_shift);
+	 */
 	s8 virtual_tsc_shift;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2530| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_mult,
+	 *   - arch/x86/kvm/x86.h|360| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+	 */
 	u32 virtual_tsc_mult;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1552| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1570| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1590| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1595| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1697| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2113| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2118| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|3903| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2434| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2445| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2463| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2496| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_ioctl>> r = vcpu->arch.virtual_tsc_khz;
+	 *
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	u64 tsc_scaling_ratio;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
@@ -731,6 +943,22 @@ struct kvm_vcpu_arch {
 	struct kvm_mtrr mtrr_state;
 	u64 pat;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->switch_db_regs:
+	 *   - arch/x86/kvm/svm/svm.c|1868| <<svm_sync_dirty_debug_regs>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/svm/svm.c|2563| <<dr_interception>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/svm/svm.c|3769| <<svm_vcpu_run>> if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))
+	 *   - arch/x86/kvm/vmx/vmx.c|4400| <<vmx_exec_control>> if (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)
+	 *   - arch/x86/kvm/vmx/vmx.c|5381| <<handle_dr>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|5409| <<vmx_sync_dirty_debug_regs>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/x86.c|1154| <<kvm_update_dr0123>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_RELOAD;
+	 *   - arch/x86/kvm/x86.c|1167| <<kvm_update_dr7>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_BP_ENABLED;
+	 *   - arch/x86/kvm/x86.c|1169| <<kvm_update_dr7>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_BP_ENABLED;
+	 *   - arch/x86/kvm/x86.c|9362| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.switch_db_regs)) {
+	 *   - arch/x86/kvm/x86.c|9369| <<vcpu_enter_guest>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
+	 *   - arch/x86/kvm/x86.c|9392| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)) {
+	 *   - arch/x86/kvm/x86.c|9397| <<vcpu_enter_guest>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
+	 */
 	unsigned switch_db_regs;
 	unsigned long db[KVM_NR_DB_REGS];
 	unsigned long dr6;
@@ -1005,6 +1233,13 @@ struct kvm_arch {
 	bool apic_access_page_done;
 	unsigned long apicv_inhibit_reasons;
 
+	/*
+	 * 似乎在以下使用kvm_arch->wall_clock (gpa_t):
+	 *   - arch/x86/kvm/x86.c|3690| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+	 *   - arch/x86/kvm/x86.c|3697| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+	 *   - arch/x86/kvm/x86.c|4026| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+	 *   - arch/x86/kvm/x86.c|4032| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+	 */
 	gpa_t wall_clock;
 
 	bool mwait_in_guest;
@@ -1013,22 +1248,159 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2542| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|2599| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_write = data;
+	 *   - arch/x86/kvm/x86.c|11075| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2562| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2600| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 */
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2889| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2918| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2890| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = data;
+	 *   - arch/x86/kvm/x86.c|2919| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 cur_tsc_write;
+	/*
+	 * cur_tsc_offset = 18443709364331608385,
+	 *
+	 * crash> eval 18443709364331608385
+	 * hexadecimal: fff537f2a987f941
+	 *     decimal: 18443709364331608385  (-3034709377943231)
+	 *       octal: 1777651577125141774501
+	 *      binary: 1111111111110101001101111111001010101001100001111111100101000001
+	 *
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu0/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu1/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu2/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu3/tsc-offset
+	 * -3034709377943231
+	 *
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 */
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 *
+	 * 测试的例子是cur_tsc_generation = 1
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_synchronize_tsc>> spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2516| <<kvm_synchronize_tsc>> spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2745| <<kvm_gen_update_masterclock>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2747| <<kvm_gen_update_masterclock>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2765| <<get_kvmclock_ns>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2767| <<get_kvmclock_ns>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2773| <<get_kvmclock_ns>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2877| <<kvm_guest_time_update>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2883| <<kvm_guest_time_update>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5985| <<kvm_arch_vm_ioctl>> spin_lock_irq(&ka->pvclock_gtod_sync_lock);
+	 *   - arch/x86/kvm/x86.c|5991| <<kvm_arch_vm_ioctl>> spin_unlock_irq(&ka->pvclock_gtod_sync_lock);
+	 *   - arch/x86/kvm/x86.c|8010| <<kvm_hyperv_tsc_notifier>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|8012| <<kvm_hyperv_tsc_notifier>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|10850| <<kvm_arch_init_vm>> spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
+	 */
 	spinlock_t pvclock_gtod_sync_lock;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched 
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|2780| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|2853| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2984| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6090| <<kvm_arch_vm_ioctl>> now_ns = ka->master_kernel_ns;
+	 *
+	 * 猜测是内核启动了的ns
+	 */
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|2945| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3079| <<get_kvmclock_ns>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3216| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *
+	 * 猜测是内核启动了的cycle
+	 */
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_fn:
+	 *   - arch/x86/kvm/x86.c|3703| <<kvmclock_update_fn>> kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3737| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3764| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11787| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11835| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3754| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3765| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11435| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11788| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
@@ -1043,7 +1415,22 @@ struct kvm_arch {
 	int audit_point;
 	#endif
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2950| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|11175| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 */
 	bool backwards_tsc_observed;
+	/*
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 *
+	 * 如果写入的是MSR_KVM_SYSTEM_TIME,表明Guest使用的是旧版kvmclock,不支持Master Clock模式,
+	 * 此时要设置kvm->arch.boot_vcpu_runs_old_kvmclock = 1,并对当前vCPU(即vCPU0)发送一个KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 这最终会导致kvm->arch.use_master_clock = 0
+	 */
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
 
@@ -1550,6 +1937,14 @@ extern u64 kvm_mce_cap_supported;
  */
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
+/*
+ * 在以下使用EMULTYPE_SKIP:
+ *   - arch/x86/kvm/svm/svm.c|347| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/vmx/vmx.c|1816| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/x86.c|7315| <<handle_emulation_failure>> if (emulation_type & EMULTYPE_SKIP) {
+ *   - arch/x86/kvm/x86.c|7610| <<x86_decode_emulated_instruction>> if (!(emulation_type & EMULTYPE_SKIP) &&
+ *   - arch/x86/kvm/x86.c|7683| <<x86_emulate_instruction>> if (emulation_type & EMULTYPE_SKIP) {
+ */
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_ALLOW_RETRY_PF	    (1 << 3)
 #define EMULTYPE_TRAP_UD_FORCED	    (1 << 4)
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226efe3e..63f1da735cbe 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -23,12 +23,54 @@
  * and after reading them.
  */
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ *
+ *
+ * 在kvm vm上测试的时候, 下面的都不变
+ * system_time=23867275
+ * tsc_timestamp=206742471, version=4, tsc_to_system_mul=2532092704, tsc_shift=255
+ * kvm_clock_read()一直在变
+ */
 struct pvclock_vcpu_time_info {
+	/*
+	 * 奇数表示Host正在修改,偶数表示已修改完毕
+	 */
 	u32   version;
 	u32   pad0;
+	/*
+	 * 更新pvti时,vCPU的vTSC 
+	 */
 	u64   tsc_timestamp;
+	/*
+	 * 当dmesg是14.903088的时候,
+	 * kvm_clock_read()返回15169858774 (15.169858774)
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	u64   system_time;
+	/*
+	 * 用于将tsc转换为nsec
+	 */
 	u32   tsc_to_system_mul;
+	/*
+	 * 用于将tsc转换为nsec
+	 */
 	s8    tsc_shift;
 	u8    flags;
 	u8    pad[2];
@@ -41,6 +83,15 @@ struct pvclock_wall_clock {
 } __attribute__((__packed__));
 
 #define PVCLOCK_TSC_STABLE_BIT	(1 << 0)
+/*
+ * 在以下使用PVCLOCK_GUEST_STOPPED:
+ *   - arch/x86/kernel/kvmclock.c|152| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+ *   - arch/x86/kernel/kvmclock.c|153| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kernel/pvclock.c|80| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+ *   - arch/x86/kernel/pvclock.c|81| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kvm/x86.c|2769| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
+ *   - arch/x86/kvm/x86.c|2772| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+ */
 #define PVCLOCK_GUEST_STOPPED	(1 << 1)
 /* PVCLOCK_COUNTS_FROM_ZERO broke ABI and can't be used anymore. */
 #define PVCLOCK_COUNTS_FROM_ZERO (1 << 2)
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2c68..269fb08ac2be 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -39,6 +39,12 @@ bool pvclock_read_retry(const struct pvclock_vcpu_time_info *src,
  * Scale a 64-bit delta by scaling and multiplying by a 32-bit fraction,
  * yielding a 64-bit result.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/pvclock.h|99| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
+ *   - arch/x86/kvm/x86.c|2378| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.h|360| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+ */
 static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 {
 	u64 product;
@@ -78,10 +84,24 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 	return product;
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+ *   - arch/x86/kernel/pvclock.c|76| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+ *   - arch/x86/kvm/x86.c|3072| <<get_kvmclock_ns>> ret = __pvclock_read_cycles(&hv_clock, rdtsc());
+ *   - drivers/ptp/ptp_kvm_x86.c|91| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair.tsc);
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
+	/*
+	 * 更新pvti时,vCPU的vTSC
+	 */
 	u64 delta = tsc - src->tsc_timestamp;
+	/*
+	 * src->tsc_to_system_mul和src->tsc_shift:
+	 * 用于将tsc转换为nsec
+	 */
 	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
 	return src->system_time + offset;
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 950afebfba88..ec41631d2082 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -34,6 +34,13 @@
 #define KVM_FEATURE_ASYNC_PF_INT	14
 #define KVM_FEATURE_MSI_EXT_DEST_ID	15
 
+/*
+ * 在以下使用KVM_HINTS_REALTIME:
+ *   - arch/x86/kernel/kvm.c|459| <<pv_tlb_flush_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|471| <<pv_sched_yield_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|946| <<kvm_spinlock_init>> if (kvm_para_has_hint(KVM_HINTS_REALTIME)) {
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|99| <<haltpoll_want>> return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
+ */
 #define KVM_HINTS_REALTIME      0
 
 /* The last 8 bits are used to indicate how to interpret the flags field
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index d5c691a3208b..f3becec4bfdf 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -869,6 +869,10 @@ void ioapic_set_alloc_attr(struct irq_alloc_info *info, int node,
 	info->ioapic.valid = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|1055| <<mp_map_pin_to_irq>> ioapic_copy_alloc_attr(&tmp, info, gsi, ioapic, pin);
+ */
 static void ioapic_copy_alloc_attr(struct irq_alloc_info *dst,
 				   struct irq_alloc_info *src,
 				   u32 gsi, int ioapic_idx, int pin)
diff --git a/arch/x86/kernel/crash.c b/arch/x86/kernel/crash.c
index 54ce999ed321..2598591a9769 100644
--- a/arch/x86/kernel/crash.c
+++ b/arch/x86/kernel/crash.c
@@ -56,6 +56,12 @@ struct crash_memmap_data {
  *
  * protected by rcu.
  */
+/*
+ * 在以下使用crash_vmclear_loaded_vmcss:
+ *   - arch/x86/kernel/crash.c|67| <<cpu_crash_vmclear_loaded_vmcss>> do_vmclear_operation = rcu_dereference(crash_vmclear_loaded_vmcss);
+ *   - arch/x86/kvm/vmx/vmx.c|8159| <<vmx_exit>> RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+ *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_init>> rcu_assign_pointer(crash_vmclear_loaded_vmcss, crash_vmclear_local_loaded_vmcss);
+ */
 crash_vmclear_fn __rcu *crash_vmclear_loaded_vmcss = NULL;
 EXPORT_SYMBOL_GPL(crash_vmclear_loaded_vmcss);
 
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index a26643dc6bd6..30497a5e2ccb 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -453,6 +453,11 @@ static int kvm_cpu_online(unsigned int cpu)
 
 static DEFINE_PER_CPU(cpumask_var_t, __pv_cpu_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|613| <<kvm_alloc_cpumask>> if (pv_tlb_flush_supported() || pv_ipi_supported())
+ *   - arch/x86/kernel/kvm.c|719| <<kvm_guest_init>> if (pv_tlb_flush_supported()) {
+ */
 static bool pv_tlb_flush_supported(void)
 {
 	return (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &&
@@ -868,6 +873,10 @@ static void kvm_kick_cpu(int cpu)
 
 #include <asm/qspinlock.h>
 
+/*
+ * 在以下使用kvm_wait():
+ *   - arch/x86/kernel/kvm.c|987| <<kvm_spinlock_init>> pv_ops.lock.wait = kvm_wait;
+ */
 static void kvm_wait(u8 *ptr, u8 val)
 {
 	if (in_nmi())
@@ -884,6 +893,9 @@ static void kvm_wait(u8 *ptr, u8 val)
 	} else {
 		local_irq_disable();
 
+		/*
+		 * 缺少的注释: safe_halt() will enable IRQ
+		 */
 		if (READ_ONCE(*ptr) == val)
 			safe_halt();
 
@@ -960,6 +972,21 @@ void __init kvm_spinlock_init(void)
 
 	pr_info("PV spinlocks enabled\n");
 
+
+	/*
+	 * Implement paravirt qspinlocks; the general idea is to halt the vcpus instead
+	 * of spinning them.
+	 *
+	 * This relies on the architecture to provide two paravirt hypercalls:
+	 *
+	 *   pv_wait(u8 *ptr, u8 val) -- suspends the vcpu if *ptr == val
+	 *   pv_kick(cpu)             -- wakes a suspended vcpu
+	 *
+	 * Using these we implement __pv_queued_spin_lock_slowpath() and
+	 * __pv_queued_spin_unlock() to replace native_queued_spin_lock_slowpath() and
+	 * native_queued_spin_unlock().
+	 */
+
 	__pv_init_lock_hash();
 	pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_ops.lock.queued_spin_unlock =
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ad273e5861c1..05b9a7d18e1b 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -67,6 +67,10 @@ static inline struct pvclock_vsyscall_time_info *this_cpu_hvclock(void)
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
+/*
+ * 在以下使用kvm_get_wallclock():
+ *   - arch/x86/kernel/kvmclock.c|349| <<kvmclock_init>> x86_platform.get_wallclock = kvm_get_wallclock;
+ */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
@@ -80,6 +84,24 @@ static int kvm_set_wallclock(const struct timespec64 *now)
 	return -ENODEV;
 }
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ */
 static u64 kvm_clock_read(void)
 {
 	u64 ret;
@@ -163,6 +185,10 @@ static int kvm_cs_enable(struct clocksource *cs)
 	return 0;
 }
 
+/*
+ * kvm_clock 作为一个clocksource,其频率为1GHz,实际上其read回调返回的值
+ * 是就是System Time(即Host Boot Time + Kvmclock Offset)的读数,单位为纳秒
+ */
 struct clocksource kvm_clock = {
 	.name	= "kvm-clock",
 	.read	= kvm_clock_get_cycles,
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index eda37df016f0..aa98e9042e7b 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -36,6 +36,11 @@ unsigned long pvclock_tsc_khz(struct pvclock_vcpu_time_info *src)
 	return pv_tsc_khz;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|154| <<kvm_check_and_clear_guest_paused>> pvclock_touch_watchdogs();
+ *   - arch/x86/kernel/pvclock.c|82| <<pvclock_clocksource_read>> pvclock_touch_watchdogs();
+ */
 void pvclock_touch_watchdogs(void)
 {
 	touch_softlockup_watchdog_sync();
@@ -44,6 +49,12 @@ void pvclock_touch_watchdogs(void)
 	reset_hung_task_detector();
 }
 
+/*
+ * 在以下使用atomic的last_value:
+ *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+ *   - arch/x86/kernel/pvclock.c|128| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+ *   - arch/x86/kernel/pvclock.c|132| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+ */
 static atomic64_t last_value = ATOMIC64_INIT(0);
 
 void pvclock_resume(void)
@@ -64,6 +75,26 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ *
+ * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time
+ */
 u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
@@ -100,6 +131,12 @@ u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 	 * updating at the same time, and one of them could be slightly behind,
 	 * making the assumption that last_value always go forward fail to hold.
 	 */
+	/*
+	 * 在以下使用atomic的last_value:
+	 *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+	 *   - arch/x86/kernel/pvclock.c|128| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+	 *   - arch/x86/kernel/pvclock.c|132| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+	 */
 	last = atomic64_read(&last_value);
 	do {
 		if (ret < last)
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57ec01192180..d2bb06ca9b1e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -30,9 +30,15 @@
 #include <asm/i8259.h>
 #include <asm/uv/uv.h>
 
+/*
+ * cpu频率
+ */
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
 
+/*
+ * tsc频率
+ */
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index b4da665bb892..a7210d9f985b 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -399,6 +399,11 @@ static __always_inline void kvm_cpu_cap_mask(enum cpuid_leafs leaf, u32 mask)
 	__kvm_cpu_cap_mask(leaf);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|907| <<svm_set_cpu_caps>> kvm_set_cpu_caps();
+ *   - arch/x86/kvm/vmx/vmx.c|7454| <<vmx_set_cpu_caps>> kvm_set_cpu_caps();
+ */
 void kvm_set_cpu_caps(void)
 {
 	unsigned int f_nx = is_efer_nx() ? F(NX) : 0;
@@ -663,6 +668,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1011| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 5e5de05a8fbf..fc629ed18159 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -5481,6 +5481,10 @@ void init_decode_cache(struct x86_emulate_ctxt *ctxt)
 	ctxt->mem_read.end = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7715| <<x86_emulate_instruction>> r = x86_emulate_insn(ctxt);
+ */
 int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index f00830e5202f..9214d1abb66c 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1168,6 +1168,10 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 	mutex_unlock(&hv->hv_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2753| <<kvm_gen_update_masterclock>> kvm_hv_invalidate_tsc_page(kvm);
+ */
 void kvm_hv_invalidate_tsc_page(struct kvm *kvm)
 {
 	struct kvm_hv *hv = to_kvm_hv(kvm);
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 17fa4ab1b834..0878217a8dbf 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1233,6 +1233,11 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 	kvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2049| <<kvm_lapic_reg_write>> apic_set_eoi(apic);
+ *   - arch/x86/kvm/lapic.c|2736| <<apic_sync_pv_eoi_from_guest>> vector = apic_set_eoi(apic);
+ */
 static int apic_set_eoi(struct kvm_lapic *apic)
 {
 	int vector = apic_find_highest_isr(apic);
@@ -1845,6 +1850,21 @@ static void cancel_hv_timer(struct kvm_lapic *apic)
 	WARN_ON(preemptible());
 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
 	static_call(kvm_x86_cancel_hv_timer)(apic->vcpu);
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	apic->lapic_timer.hv_timer_in_use = false;
 }
 
@@ -1864,6 +1884,21 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	ktimer->hv_timer_in_use = true;
 	hrtimer_cancel(&ktimer->timer);
 
@@ -2174,6 +2209,10 @@ static int apic_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5286| <<handle_apic_access>> kvm_lapic_set_eoi(vcpu);
+ */
 void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 {
 	kvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);
@@ -2181,6 +2220,10 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
 /* emulate APIC access in a trap manner */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5287| <<handle_apic_write>> kvm_apic_write_nodecode(vcpu, offset);
+ */
 void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 997c45a5963a..14a229cde4a1 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -40,6 +40,21 @@ struct kvm_timer {
 	u32 timer_advance_ns;
 	s64 advance_expire_delta;
 	atomic_t pending;			/* accumulated triggered timers */
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	bool hv_timer_in_use;
 };
 
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 8d5876dfc6b7..fa9ac7aee862 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4850,6 +4850,20 @@ kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu)
 	return role.base;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|208| <<kvm_vcpu_after_set_cpuid>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|1230| <<init_vmcb>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4362| <<nested_vmx_restore_host_state>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2942| <<enter_rmode>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|863| <<kvm_post_set_cr0>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1046| <<kvm_post_set_cr4>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1556| <<set_efer>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|7118| <<emulator_set_hflags>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|7451| <<kvm_smm_changed>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|8970| <<enter_smm>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|10057| <<__set_sregs>> kvm_mmu_reset_context(vcpu);
+ */
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -4882,6 +4896,14 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4855| <<kvm_mmu_reset_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5943| <<kvm_mmu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|3090| <<kvm_vcpu_flush_tlb_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|9156| <<vcpu_enter_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|10781| <<kvm_unload_vcpu_mmu>> kvm_mmu_unload(vcpu);
+ */
 void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_free_roots(vcpu, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index 5e8d8443154e..2dc8b636c99a 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -114,6 +114,16 @@ static void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|533| <<nested_vmcb02_prepare_control>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|302| <<set_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|317| <<clr_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|327| <<set_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|337| <<clr_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|346| <<svm_set_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|355| <<svm_clr_intercept>> recalc_intercepts(svm);
+ */
 void recalc_intercepts(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *c, *h, *g;
@@ -478,6 +488,11 @@ static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|575| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm);
+ *   - arch/x86/kvm/svm/nested.c|1343| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm);
+ */
 static void nested_vmcb02_prepare_control(struct vcpu_svm *svm)
 {
 	const u32 mask = V_INTR_MASKING_MASK | V_GIF_ENABLE_MASK | V_GIF_MASK;
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index 5f81ef092bd4..f628f22d0eb6 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -13,6 +13,13 @@
  * We maintain a per-CPU linked-list of vCPU, so in wakeup_handler() we
  * can find which vCPU should be waken up.
  */
+/*
+ * 在以下使用percpu的blocked_vcpu_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|16| <<global>> static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|215| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/posted_intr.c|227| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
 static DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);
 
@@ -151,6 +158,13 @@ int pi_pre_block(struct kvm_vcpu *vcpu)
 	if (!WARN_ON_ONCE(vcpu->pre_pcpu != -1)) {
 		vcpu->pre_pcpu = vcpu->cpu;
 		spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));
+		/*
+		 * 在以下使用percpu的blocked_vcpu_on_cpu:
+		 *   - arch/x86/kvm/vmx/posted_intr.c|16| <<global>> static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
+		 *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+		 *   - arch/x86/kvm/vmx/posted_intr.c|215| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/posted_intr.c|227| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+		 */
 		list_add_tail(&vcpu->blocked_vcpu_list,
 			      &per_cpu(blocked_vcpu_on_cpu,
 				       vcpu->pre_pcpu));
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index 1472c6c376f7..c10002683851 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -64,7 +64,19 @@ struct loaded_vmcs {
 	bool hv_timer_soft_disabled;
 	/* Support for vnmi-less CPUs */
 	int soft_vnmi_blocked;
+	/*
+	 * 在以下使用loaded_vmcs->entry_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|6521| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->entry_time));
+	 *   - arch/x86/kvm/vmx/vmx.c|6689| <<vmx_vcpu_run>> vmx->loaded_vmcs->entry_time = ktime_get();
+	 */
 	ktime_t entry_time;
+	/*
+	 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|4632| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|4670| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6065| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
+	 *   - arch/x86/kvm/vmx/vmx.c|6519| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time +=
+	 */
 	s64 vnmi_blocked_time;
 	unsigned long *msr_bitmap;
 	struct list_head loaded_vmcss_on_cpu_link;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c2a779b688e6..ccb5e4a2dabe 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -76,31 +76,95 @@ static const struct x86_cpu_id vmx_cpu_id[] = {
 MODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);
 #endif
 
+/*
+ * 在以下使用enable_vpid:
+ *   - arch/x86/kvm/vmx/vmx.c|80| <<global>> module_param_named(vpid, enable_vpid, bool, 0444);
+ *   - arch/x86/kvm/vmx/nested.c|1177| <<nested_vmx_transition_tlb_flush>> if (!enable_vpid)
+ *   - arch/x86/kvm/vmx/nested.c|2198| <<prepare_vmcs02_early_rare>> if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/nested.c|6526| <<nested_vmx_setup_ctls_msrs>> if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/vmx.c|3146| <<vmx_flush_tlb_all>> } else if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/vmx.c|3879| <<allocate_vpid>> if (!enable_vpid)
+ *   - arch/x86/kvm/vmx/vmx.c|3893| <<free_vpid>> if (!enable_vpid || vpid == 0)
+ *   - arch/x86/kvm/vmx/vmx.c|7955| <<hardware_setup>> enable_vpid = 0;
+ *
+ * The original architecture for VMX operation required VMX transitions to flush the TLBs and paging-structure caches.
+ * This ensured that translations cached for the old linear-address space would not be used after the transition.
+ * Virtual-processor identifiers (VPIDs) introduce to VMX operation a facility by which a logical processor may cache
+ * information for multiple linear-address spaces. When VPIDs are used, VMX transitions may retain cached informa-
+ * tion and the logical processor switches to a different linear-address space.
+ */
 bool __read_mostly enable_vpid = 1;
 module_param_named(vpid, enable_vpid, bool, 0444);
 
 static bool __read_mostly enable_vnmi = 1;
 module_param_named(vnmi, enable_vnmi, bool, S_IRUGO);
 
+/*
+ * 在以下使用flexpriority_enabled:
+ *   - arch/x86/kvm/vmx/vmx.c|86| <<global>> module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6537| <<nested_vmx_setup_ctls_msrs>> if (flexpriority_enabled)
+ *   - arch/x86/kvm/vmx/vmx.c|731| <<cpu_need_virtualize_apic_accesses>> return flexpriority_enabled && lapic_in_kernel(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|736| <<report_flexpriority>> return flexpriority_enabled;
+ *   - arch/x86/kvm/vmx/vmx.c|6346| <<vmx_set_virtual_apic_mode>> if (!flexpriority_enabled &&
+ *   - arch/x86/kvm/vmx/vmx.c|6367| <<vmx_set_virtual_apic_mode>> if (flexpriority_enabled) {
+ *   - arch/x86/kvm/vmx/vmx.c|7936| <<hardware_setup>> flexpriority_enabled = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7946| <<hardware_setup>> if (!flexpriority_enabled)
+ *
+ * 参考cpu_has_vmx_flexpriority()
+ */
 bool __read_mostly flexpriority_enabled = 1;
 module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
 
 bool __read_mostly enable_ept = 1;
 module_param_named(ept, enable_ept, bool, S_IRUGO);
 
+/*
+ * This control determines whether guest software may run
+ * in unpaged protected mode or in real-address mode.
+ */
 bool __read_mostly enable_unrestricted_guest = 1;
 module_param_named(unrestricted_guest,
 			enable_unrestricted_guest, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_ept_ad_bits:
+ *   - arch/x86/kvm/vmx/vmx.c|96| <<global>> module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6501| <<nested_vmx_setup_ctls_msrs>> if (enable_ept_ad_bits) {
+ *   - arch/x86/kvm/vmx/vmx.c|3269| <<construct_eptp>> if (enable_ept_ad_bits &&
+ *   - arch/x86/kvm/vmx/vmx.c|7930| <<hardware_setup>> enable_ept_ad_bits = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7985| <<hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits,
+ *   - arch/x86/kvm/vmx/vmx.c|8002| <<hardware_setup>> if (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())
+ */
 bool __read_mostly enable_ept_ad_bits = 1;
 module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
 
+/*
+ * 在以下使用emulate_invalid_guest_state:
+ *   - arch/x86/kvm/vmx/vmx.c|99| <<global>> module_param(emulate_invalid_guest_state, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|1521| <<emulation_required>> return emulate_invalid_guest_state && !vmx_guest_state_valid(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2914| <<fix_pmode_seg>> if (!emulate_invalid_guest_state) {
+ *   - arch/x86/kvm/vmx/vmx.c|2977| <<fix_rmode_seg>> if (!emulate_invalid_guest_state) {
+ *   - arch/x86/kvm/vmx/vmx.c|6586| <<vmx_has_emulated_msr>> return enable_unrestricted_guest || emulate_invalid_guest_state;
+ */
 static bool __read_mostly emulate_invalid_guest_state = true;
 module_param(emulate_invalid_guest_state, bool, S_IRUGO);
 
+/*
+ * 在以下使用fasteoi:
+ *   - arch/x86/kvm/vmx/vmx.c|102| <<global>> module_param(fasteoi, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|5273| <<handle_apic_access>> if (likely(fasteoi)) {
+ */
 static bool __read_mostly fasteoi = 1;
 module_param(fasteoi, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_apicv:
+ *   - arch/x86/kvm/vmx/vmx.c|105| <<global>> module_param(enable_apicv, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6378| <<nested_vmx_setup_ctls_msrs>> (enable_apicv ? PIN_BASED_POSTED_INTR : 0);
+ *   - arch/x86/kvm/vmx/vmx.c|3901| <<vmx_msr_bitmap_mode>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7026| <<vmx_vm_init>> kvm_apicv_init(kvm, enable_apicv);
+ *   - arch/x86/kvm/vmx/vmx.c|7879| <<hardware_setup>> enable_apicv = 0;
+ */
 bool __read_mostly enable_apicv = 1;
 module_param(enable_apicv, bool, S_IRUGO);
 
@@ -112,9 +176,25 @@ module_param(enable_apicv, bool, S_IRUGO);
 static bool __read_mostly nested = 1;
 module_param(nested, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_pml (Page Modification Logging):
+ *   - arch/x86/kvm/vmx/vmx.c|116| <<global>> module_param_named(pml, enable_pml, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|2171| <<prepare_vmcs02_constant_state>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|4478| <<init_vmcs>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|5996| <<__vmx_handle_exit>> if (enable_pml && !is_guest_mode(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|6866| <<vmx_free_vcpu>> if (enable_pml)
+ *   - arch/x86/kvm/vmx/vmx.c|6892| <<vmx_create_vcpu>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|7912| <<hardware_setup>> enable_pml = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7914| <<hardware_setup>> if (!enable_pml)
+ */
 bool __read_mostly enable_pml = 1;
 module_param_named(pml, enable_pml, bool, S_IRUGO);
 
+/*
+ * 在以下使用dump_invalid_vmcs:
+ *   - arch/x86/kvm/vmx/vmx.c|119| <<global>> module_param(dump_invalid_vmcs, bool, 0644);
+ *   - arch/x86/kvm/vmx/vmx.c|5821| <<dump_vmcs>> if (!dump_invalid_vmcs) {
+ */
 static bool __read_mostly dump_invalid_vmcs = 0;
 module_param(dump_invalid_vmcs, bool, 0644);
 
@@ -124,12 +204,40 @@ module_param(dump_invalid_vmcs, bool, 0644);
 #define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL
 
 /* Guest_tsc -> host_tsc conversion requires 64-bit division.  */
+/*
+ * 在以下使用cpu_preemption_timer_multi:
+ *   - arch/x86/kvm/vmx/vmx.c|6649| <<vmx_update_hv_timer>> cpu_preemption_timer_multi);
+ *   - arch/x86/kvm/vmx/vmx.c|7490| <<vmx_set_hv_timer>> if (delta_tsc >> (cpu_preemption_timer_multi + 32))
+ *   - arch/x86/kvm/vmx/vmx.c|7925| <<hardware_setup>> cpu_preemption_timer_multi =
+ *   - arch/x86/kvm/vmx/vmx.c|7930| <<hardware_setup>> use_timer_freq >>= cpu_preemption_timer_multi;
+ */
 static int __read_mostly cpu_preemption_timer_multi;
+/*
+ * 在以下使用enable_preemption_timer:
+ *   - arch/x86/kvm/vmx/vmx.c|130| <<global>> module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|4199| <<vmx_pin_based_exec_ctrl>> if (!enable_preemption_timer)
+ *   - arch/x86/kvm/vmx/vmx.c|6764| <<vmx_vcpu_run>> if (enable_preemption_timer)
+ *   - arch/x86/kvm/vmx/vmx.c|7918| <<hardware_setup>> enable_preemption_timer = false;
+ *   - arch/x86/kvm/vmx/vmx.c|7920| <<hardware_setup>> if (enable_preemption_timer) {
+ *   - arch/x86/kvm/vmx/vmx.c|7938| <<hardware_setup>> enable_preemption_timer = false;
+ *   - arch/x86/kvm/vmx/vmx.c|7941| <<hardware_setup>> if (!enable_preemption_timer) {
+ */
 static bool __read_mostly enable_preemption_timer = 1;
 #ifdef CONFIG_X86_64
 module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
 #endif
 
+/*
+ * 在以下使用allow_smaller_maxphyaddr:
+ *   - arch/x86/kvm/vmx/vmx.c|134| <<global>> module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|209| <<global>> bool __read_mostly allow_smaller_maxphyaddr = 0;
+ *   - arch/x86/kvm/svm/svm.c|1057| <<svm_hardware_setup>> allow_smaller_maxphyaddr = !npt_enabled;
+ *   - arch/x86/kvm/vmx/vmx.c|4923| <<handle_exception_nmi>> WARN_ON_ONCE(!allow_smaller_maxphyaddr);
+ *   - arch/x86/kvm/vmx/vmx.c|5420| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/vmx/vmx.c|8104| <<vmx_init>> allow_smaller_maxphyaddr = true;
+ *   - arch/x86/kvm/vmx/vmx.h|578| <<vmx_need_pf_intercept>> return allow_smaller_maxphyaddr && cpuid_maxphyaddr(vcpu) < boot_cpu_data.x86_phys_bits;
+ *   - arch/x86/kvm/x86.c|3992| <<kvm_vm_ioctl_check_extension>> r = (int ) allow_smaller_maxphyaddr;
+ */
 extern bool __read_mostly allow_smaller_maxphyaddr;
 module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
 
@@ -190,26 +298,77 @@ static unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;
 module_param(ple_window, uint, 0444);
 
 /* Default doubles per-vcpu window every exit. */
+/*
+ * 在以下使用ple_window_grow:
+ *   - arch/x86/kvm/vmx/vmx.c|194| <<global>> module_param(ple_window_grow, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5508| <<grow_ple_window>> ple_window_grow,
+ *   - arch/x86/kvm/vmx/vmx.c|7873| <<hardware_setup>> ple_window_grow = 0;
+ */
 static unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;
 module_param(ple_window_grow, uint, 0444);
 
 /* Default resets per-vcpu window every exit to ple_window. */
+/*
+ * 在以下使用ple_window_shrink:
+ *   - arch/x86/kvm/vmx/vmx.c|198| <<global>> module_param(ple_window_shrink, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5524| <<shrink_ple_window>> ple_window_shrink,
+ *   - arch/x86/kvm/vmx/vmx.c|7875| <<hardware_setup>> ple_window_shrink = 0;
+ */
 static unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;
 module_param(ple_window_shrink, uint, 0444);
 
 /* Default is to compute the maximum so we can never overflow. */
+/*
+ * 在以下使用ple_window_max:
+ *   - arch/x86/kvm/vmx/vmx.c|202| <<global>> module_param(ple_window_max, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5509| <<grow_ple_window>> ple_window_max);
+ *   - arch/x86/kvm/vmx/vmx.c|7874| <<hardware_setup>> ple_window_max = 0;
+ */
 static unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
 module_param(ple_window_max, uint, 0444);
 
 /* Default is SYSTEM mode, 1 for host-guest mode */
+/*
+ * Processor Trace virtualization can be work in one of 3 possible
+ * modes by set new option "pt_mode". Default value is system mode.
+ * a. system-wide: trace both host/guest and output to host buffer;
+ * b. host-only: only trace host and output to host buffer;
+ * c. host-guest: trace host/guest simultaneous and output to their respective buffer.
+ *
+ * KVM currently only supports (a) and (c).
+ *
+ * 在以下使用pt_mode:
+ *   - arch/x86/kvm/vmx/vmx.c|206| <<global>> module_param(pt_mode, int , S_IRUGO);
+ *   - arch/x86/kvm/vmx/capabilities.h|373| <<vmx_pt_mode_is_system>> return pt_mode == PT_MODE_SYSTEM;
+ *   - arch/x86/kvm/vmx/capabilities.h|377| <<vmx_pt_mode_is_host_guest>> return pt_mode == PT_MODE_HOST_GUEST;
+ *   - arch/x86/kvm/vmx/vmx.c|7951| <<hardware_setup>> if (pt_mode != PT_MODE_SYSTEM && pt_mode != PT_MODE_HOST_GUEST)
+ *   - arch/x86/kvm/vmx/vmx.c|7954| <<hardware_setup>> pt_mode = PT_MODE_SYSTEM;
+ */
 int __read_mostly pt_mode = PT_MODE_SYSTEM;
 module_param(pt_mode, int, S_IRUGO);
 
+/*
+ * 在以下使用vmx_l1d_should_flush:
+ *   - arch/x86/kvm/vmx/vmx.c|300| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|6665| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+/*
+ * 在以下使用vmx_l1d_flush_cond:
+ *   - arch/x86/kvm/vmx/vmx.c|453| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|455| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|6321| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
 static DEFINE_MUTEX(vmx_l1d_flush_mutex);
 
 /* Storage for pre module init parameter parsing */
+/*
+ * 在以下使用vmentry_l1d_flush_param:
+ *   - arch/x86/kvm/vmx/vmx.c|507| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|8242| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
 
 static const struct {
@@ -225,6 +384,16 @@ static const struct {
 };
 
 #define L1D_CACHE_ORDER 4
+/*
+ * 在以下使用vmx_l1d_flush_pages:
+ *   - arch/x86/kvm/vmx/vmx.c|423| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ *   - arch/x86/kvm/vmx/vmx.c|432| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+ *   - arch/x86/kvm/vmx/vmx.c|440| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1,
+ *   - arch/x86/kvm/vmx/vmx.c|6368| <<vmx_l1d_flush>> :: [flush_pages] "r" (vmx_l1d_flush_pages),
+ *   - arch/x86/kvm/vmx/vmx.c|8128| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+ *   - arch/x86/kvm/vmx/vmx.c|8129| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ *   - arch/x86/kvm/vmx/vmx.c|8130| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+ */
 static void *vmx_l1d_flush_pages;
 
 static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
@@ -411,6 +580,14 @@ noinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)
 }
 
 static DEFINE_PER_CPU(struct vmcs *, vmxarea);
+/*
+ * 在以下使用loaded_vmcss_on_cpu:
+ *   - arch/x86/kvm/vmx/vmx.c|583| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|883| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|1516| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+ *   - arch/x86/kvm/vmx/vmx.c|2607| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|8249| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+ */
 DEFINE_PER_CPU(struct vmcs *, current_vmcs);
 /*
  * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed
@@ -1680,6 +1857,14 @@ static int vmx_skip_emulated_instruction(struct kvm_vcpu *vcpu)
 	return skip_emulated_instruction(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1730| <<vmx_queue_exception>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4571| <<vmx_vcpu_reset>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4615| <<vmx_inject_irq>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4646| <<vmx_inject_nmi>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_enter_smm>> vmx_clear_hlt(vcpu);
+ */
 static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -4643,6 +4828,14 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/vmx.c|1730| <<vmx_queue_exception>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4571| <<vmx_vcpu_reset>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4615| <<vmx_inject_irq>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4646| <<vmx_inject_nmi>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_enter_smm>> vmx_clear_hlt(vcpu);
+	 */
 	vmx_clear_hlt(vcpu);
 }
 
@@ -5432,6 +5625,10 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * calld by:
+ *   - arch/x86/kvm/vmx/vmx.c|6180| <<__vmx_handle_exit>> return handle_invalid_guest_state(vcpu);
+ */
 static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6146,6 +6343,10 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
  * information but as all relevant affected CPUs have 32KiB L1D cache size
  * there is no point in doing so.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6830| <<vmx_vcpu_enter_exit>> vmx_l1d_flush(vcpu);
+ */
 static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 {
 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
@@ -7771,6 +7972,12 @@ static __init void vmx_setup_user_return_msrs(void)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10652| <<kvm_arch_hardware_setup>> r = ops->hardware_setup();
+ *
+ * struct kvm_x86_init_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
@@ -7999,6 +8206,10 @@ static void vmx_exit(void)
 }
 module_exit(vmx_exit);
 
+/*
+ * 在以下使用vmx_init():
+ *   - arch/x86/kvm/vmx/vmx.c|8270| <<global>> module_init(vmx_init);
+ */
 static int __init vmx_init(void)
 {
 	int r, cpu;
@@ -8049,6 +8260,11 @@ static int __init vmx_init(void)
 	 * contain 'auto' which will be turned into the default 'cond'
 	 * mitigation mode.
 	 */
+	/*
+	 * 在以下使用vmentry_l1d_flush_param:
+	 *   - arch/x86/kvm/vmx/vmx.c|507| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|8242| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+	 */
 	r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
 	if (r) {
 		vmx_exit();
@@ -8056,12 +8272,26 @@ static int __init vmx_init(void)
 	}
 
 	for_each_possible_cpu(cpu) {
+		/*
+		 * 在以下使用loaded_vmcss_on_cpu:
+		 *   - arch/x86/kvm/vmx/vmx.c|583| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|883| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/vmx.c|1516| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+		 *   - arch/x86/kvm/vmx/vmx.c|2607| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/vmx.c|8249| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+		 */
 		INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
 
 		pi_init_cpu(cpu);
 	}
 
 #ifdef CONFIG_KEXEC_CORE
+	/*
+	 * 在以下使用crash_vmclear_loaded_vmcss:
+	 *   - arch/x86/kernel/crash.c|67| <<cpu_crash_vmclear_loaded_vmcss>> do_vmclear_operation = rcu_dereference(crash_vmclear_loaded_vmcss);
+	 *   - arch/x86/kvm/vmx/vmx.c|8159| <<vmx_exit>> RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_init>> rcu_assign_pointer(crash_vmclear_loaded_vmcss, crash_vmclear_local_loaded_vmcss);
+	 */
 	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
 			   crash_vmclear_local_loaded_vmcss);
 #endif
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 16e4e457ba23..d55c9febb9b2 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -242,6 +242,27 @@ struct vcpu_vmx {
 
 	unsigned long         exit_qualification;
 	u32                   exit_intr_info;
+	/*
+	 * 在以下使用idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|4871| <<handle_exception_nmi>> vect_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<handle_task_switch>> idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5323| <<handle_task_switch>> idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5324| <<handle_task_switch>> type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5340| <<handle_task_switch>> if (vmx->idt_vectoring_info &
+	 *   - arch/x86/kvm/vmx/vmx.c|5384| <<handle_ept_violation>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5604| <<handle_pml_full>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5735| <<vmx_get_exit_info>> *info2 = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5985| <<__vmx_handle_exit>> u32 vectoring_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6513| <<vmx_recover_nmi_blocking>> idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6547| <<__vmx_complete_interrupts>> u32 idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|6555| <<__vmx_complete_interrupts>> idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6566| <<__vmx_complete_interrupts>> vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6567| <<__vmx_complete_interrupts>> type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6583| <<__vmx_complete_interrupts>> if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6602| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|6832| <<vmx_vcpu_run>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6844| <<vmx_vcpu_run>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -311,6 +332,14 @@ struct vcpu_vmx {
 
 	/* Dynamic PLE window. */
 	unsigned int ple_window;
+	/*
+	 * 在以下使用vcpu_vmx->ple_window_dirty:
+	 *   - arch/x86/kvm/vmx/vmx.c|4423| <<init_vmcs>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|5490| <<grow_ple_window>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|5506| <<shrink_ple_window>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6698| <<vmx_vcpu_run>> if (vmx->ple_window_dirty) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6699| <<vmx_vcpu_run>> vmx->ple_window_dirty = false;
+	 */
 	bool ple_window_dirty;
 
 	bool req_immediate_exit;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e0f4a46649d7..f0cc227a286d 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -135,13 +135,39 @@ EXPORT_SYMBOL_GPL(report_ignored_msrs);
 unsigned int min_timer_period_us = 200;
 module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用kvmclock_periodic_sync:
+ *   - arch/x86/kvm/x86.c|139| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|3206| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+ *   - arch/x86/kvm/x86.c|10792| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+ */
 static bool __read_mostly kvmclock_periodic_sync = true;
 module_param(kvmclock_periodic_sync, bool, S_IRUGO);
 
+/*
+ * 在以下设置kvm_has_tsc_control:
+ *   - arch/x86/kvm/svm/svm.c|962| <<svm_hardware_setup>> kvm_has_tsc_control = true;
+ *   - arch/x86/kvm/vmx/vmx.c|8063| <<hardware_setup>> kvm_has_tsc_control = true;
+ * 在以下使用kvm_has_tsc_control:
+ *   - arch/x86/kvm/debugfs.c|56| <<kvm_arch_create_vcpu_debugfs>> if (kvm_has_tsc_control) {
+ *   - arch/x86/kvm/vmx/nested.c|2537| <<prepare_vmcs02>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/vmx/nested.c|4507| <<nested_vmx_vmexit>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/vmx/vmx.c|1572| <<vmx_vcpu_load_vmcs>> if (kvm_has_tsc_control &&
+ *   - arch/x86/kvm/x86.c|2244| <<set_tsc_khz>> if (!kvm_has_tsc_control) {
+ *   - arch/x86/kvm/x86.c|2975| <<kvm_guest_time_update>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/x86.c|4099| <<kvm_vm_ioctl_check_extension>> r = kvm_has_tsc_control;
+ *   - arch/x86/kvm/x86.c|5218| <<kvm_arch_vcpu_ioctl>> if (kvm_has_tsc_control &&
+ *   - arch/x86/kvm/x86.c|10813| <<kvm_arch_hardware_setup>> if (kvm_has_tsc_control) {
+ */
 bool __read_mostly kvm_has_tsc_control;
 EXPORT_SYMBOL_GPL(kvm_has_tsc_control);
 u32  __read_mostly kvm_max_guest_tsc_khz;
 EXPORT_SYMBOL_GPL(kvm_max_guest_tsc_khz);
+/*
+ * 4.14简单测试
+ * crash> kvm_tsc_scaling_ratio_frac_bits
+ * kvm_tsc_scaling_ratio_frac_bits = $6 = 0 '\000'
+ */
 u8   __read_mostly kvm_tsc_scaling_ratio_frac_bits;
 EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
 u64  __read_mostly kvm_max_tsc_scaling_ratio;
@@ -1300,6 +1326,11 @@ static const u32 msrs_to_save_all[] = {
 	MSR_ARCH_PERFMON_EVENTSEL0 + 16, MSR_ARCH_PERFMON_EVENTSEL0 + 17,
 };
 
+/*
+ * 在以下使用msrs_to_save:
+ *   - arch/x86/kvm/x86.c|4034| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6062| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];
 static unsigned num_msrs_to_save;
 
@@ -2005,6 +2036,13 @@ struct pvclock_clock {
 struct pvclock_gtod_data {
 	seqcount_t	seq;
 
+	/*
+	 * CLOCK_MONOTONIC和CLOCK_MONOTONIC_RAW
+	 *
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 */
 	struct pvclock_clock clock; /* extract of a clocksource struct */
 	struct pvclock_clock raw_clock; /* extract of a clocksource struct */
 
@@ -2012,15 +2050,115 @@ struct pvclock_gtod_data {
 	u64		wall_time_sec;
 };
 
+/*
+ * 这是4.14的例子
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $1 = {
+ *   seq = {
+ *     sequence = 507970306
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 10566905113766286,
+ *     mask = 18446744073709551615,
+ *     mult = 4945956,
+ *     shift = 24
+ *   },
+ *   boot_ns = 2220701952811493,
+ *   nsec_base = 334196164124419,
+ *   wall_time_sec = 1630097708
+ * }
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $4 = {
+ *   seq = {
+ *     sequence = 508028248
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 10567925239710761,
+ *     mask = 18446744073709551615,
+ *     mult = 4945954,
+ *     shift = 24
+ *   },
+ *   boot_ns = 2221001952811493,
+ *   nsec_base = 12666403808125002,
+ *   wall_time_sec = 1630098008
+ * }
+ *
+ * 在以下使用pvclock_gtod_data:
+ *   - arch/x86/kvm/x86.c|2024| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2055| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - arch/x86/kvm/x86.c|2275| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2353| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+ *   - arch/x86/kvm/x86.c|2478| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+ *   - arch/x86/kvm/x86.c|2533| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2552| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2575| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2587| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2660| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+ *   - arch/x86/kvm/x86.c|8172| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ */
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * 4.14上的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8175| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
 
+	/*
+	 * start a seqcount_t write side critical section
+	 */
 	write_seqcount_begin(&vdata->seq);
 
 	/* copy pvclock gtod data */
+	/*
+	 * struct tk_read_base - base structure for timekeeping readout
+	 * @clock:      Current clocksource used for timekeeping.
+	 * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+	 * @cycle_last: @clock cycle value at last update
+	 * @mult:       (NTP adjusted) multiplier for scaled math conversion
+	 * @shift:      Shift value for scaled math conversion
+	 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+	 * @base:       ktime_t (nanoseconds) base time for readout
+	 * @base_real:  Nanoseconds base value for clock REALTIME readout
+	 *
+	 * struct timekeeper *tk:
+	 * -> struct tk_read_base tkr_mono; --> The readout base structure for CLOCK_MONOTONIC
+	 *    -> struct clocksource *clock;
+	 *       -> u64     mask;
+	 *       -> u64     cycle_last;
+	 *       -> u32     mult;
+	 *       -> u32     shift;
+	 *       -> u64     xtime_nsec;
+	 *       -> ktime_t base;
+	 *       -> u64     base_real;
+	 * -> struct tk_read_base tkr_raw; --> The readout base structure for CLOCK_MONOTONIC_RAW
+	 */
 	vdata->clock.vclock_mode	= tk->tkr_mono.clock->vdso_clock_mode;
 	vdata->clock.cycle_last		= tk->tkr_mono.cycle_last;
 	vdata->clock.mask		= tk->tkr_mono.mask;
@@ -2044,6 +2182,18 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	write_seqcount_end(&vdata->seq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2079| <<get_kvmclock_base_ns>> static s64 get_kvmclock_base_ns(void )
+ *   - arch/x86/kvm/x86.c|2391| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|2728| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2744| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2855| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|5949| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|10812| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+ *
+ * 应该是返回启动以后的ns
+ */
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Count up from boot time, but with the frequency of the raw clock.  */
@@ -2053,10 +2203,21 @@ static s64 get_kvmclock_base_ns(void)
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Master clock not used, so we can just use CLOCK_BOOTTIME.  */
+	/*
+	 * monotonic time since boot in ns format
+	 */
 	return ktime_get_boottime_ns();
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3317| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|3324| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_write_wall_clock(kvm, gpa + wc_ofs, sec_hi_ofs - wc_ofs);
+ *
+ * 根据分析, 这里分享给VM的wall clock是不变的 (就是启动的时间)
+ */
 void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2085,6 +2246,12 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * ktime_get_real_ns(): get the real (wall-) time in ns
+	 *
+	 * 这里的wall_nsec指的应该是VM启动的时候的时间
+	 * 所以可以用当前的KVM的ns减去VM已经运行了的ns
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -2103,26 +2270,77 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3473| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3479| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ *
+ * MSR_KVM_SYSTEM_TIME_NEW: old_msr是false
+ * MSR_KVM_SYSTEM_TIME    : old_msr是true
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->boot_vcpu_runs_old_kvmclock:
+		 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+		 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+		 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+		 *
+		 * 如果运行的是vcpu0,且是否使用旧的kvmclock msr与当前的
+		 * boot_vcpu_runs_old_kvmclock标志不一致,那么一定是出了
+		 * 一些什么问题,需要校准MASTERCLOCK,发出KVM_REQ_MASTERCLOCK_UPDATE
+		 * 请求
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
 		ka->boot_vcpu_runs_old_kvmclock = old_msr;
 	}
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> gpa_t time;
+	 *    -> struct gfn_to_hva_cache pv_time;
+	 */
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2179| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4339| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9358| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * kvm_gen_kvmclock_update()
+	 * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+	 * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	vcpu->arch.pv_time_enabled = false;
+	/*
+	 * 在guest的kvm_register_clock()会在最后一位设置1
+	 */
 	if (!(system_time & 1))
 		return;
 
+	/*
+	 * 会把传过来的地址参数data记录到pv_time中.
+	 * 这样子就可以通过pv_time来直接修改Guest中的pv time
+	 */
 	if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
 				       &vcpu->arch.pv_time, system_time & ~1ULL,
 				       sizeof(struct pvclock_vcpu_time_info)))
@@ -2131,12 +2349,24 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2351| <<kvm_get_time_scale>> *pmultiplier = div_frac(scaled64, tps32);
+ */
 static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 {
 	do_shl32_div32(dividend, divisor);
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2415| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3515| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ *
+ * 这里应该是要计算pshift和pmultiplier来让base_hz能转换成scaled_hz
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2166,12 +2396,41 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|3308| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+ *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3453| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|8696| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|8711| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|8730| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ *
+ * 在4.14上是3392425, 对应dmesg
+ * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+/*
+ * 在以下使用max_tsc_khz:
+ *   - arch/x86/kvm/x86.c|8927| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|8938| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+ *   - arch/x86/kvm/x86.c|11328| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static unsigned long max_tsc_khz;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2447| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 {
 	u64 v = (u64)khz * (1000000 + ppm);
@@ -2179,12 +2438,34 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 	return v;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2452| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
 
 	/* Guest TSC same frequency as host TSC? */
 	if (!scale) {
+		/*
+		 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+		 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+		 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+		 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+		 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+		 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+		 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+		 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+		 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+		 *
+		 * 在普通测试上是0 (可能没有migration吧)
+		 */
 		vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
 		return 0;
 	}
@@ -2192,7 +2473,17 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	/* TSC scaling supported? */
 	if (!kvm_has_tsc_control) {
 		if (user_tsc_khz > tsc_khz) {
+			/*
+			 * 如果在KVM_SET_TSC_KHZ时设置的vTSCfreq > pTSCfreq,且Host不支持TSC Scaling,
+			 * 便会设置vcpu->arch.always_catchup = 1,每次VMExit就会发送KVM_REQ_CLOCK_UPDATE,
+			 * 从而保证TSC Offset不断增加,令Guest TSC不断catchup设定的vTSCfreq下的理论值.
+			 */
 			vcpu->arch.tsc_catchup = 1;
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+			 *   - arch/x86/kvm/x86.c|2454| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+			 */
 			vcpu->arch.tsc_always_catchup = 1;
 			return 0;
 		} else {
@@ -2211,10 +2502,33 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 		return -1;
 	}
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	vcpu->arch.tsc_scaling_ratio = ratio;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6044| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|11412| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2228,9 +2542,26 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	}
 
 	/* Compute a scale to convert nanoseconds in TSC cycles */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2415| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3515| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+	 *
+	 * 这里应该是要计算pshift和pmultiplier来让base_hz能转换成scaled_hz
+	 */
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2248,8 +2579,21 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3625| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ *
+ * 这个函数只在vcpu->tsc_catchup的情况下调用
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2319| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2517| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 注意: 这里是kernel_ns - vcpu->arch.this_tsc_nsec (中间有减号)
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
@@ -2257,11 +2601,35 @@ static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 	return tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2382| <<kvm_track_tsc_matching>> (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
+ *   - arch/x86/kvm/x86.c|2727| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2730| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ *   - arch/x86/kvm/x86.c|2739| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2742| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ *   - arch/x86/kvm/x86.c|8556| <<pvclock_gtod_notify>> if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
+ */
 static inline int gtod_is_based_on_tsc(int mode)
 {
+	/*
+	 * 在以下使用VDSO_CLOCKMODE_HVCLOCK:
+	 *   - drivers/clocksource/hyperv_timer.c|440| <<global>> .vdso_clock_mode = VDSO_CLOCKMODE_HVCLOCK,
+	 *   - arch/x86/entry/vdso/vma.c|215| <<vvar_fault>> if (tsc_pg && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))
+	 *   - arch/x86/include/asm/vdso/clocksource.h|8| <<VDSO_ARCH_CLOCKMODES>> VDSO_CLOCKMODE_HVCLOCK
+	 *   - arch/x86/include/asm/vdso/gettimeofday.h|263| <<__arch_get_hw_counter>> if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
+	 *   - arch/x86/kvm/x86.c|2396| <<gtod_is_based_on_tsc>> return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
+	 *   - arch/x86/kvm/x86.c|2878| <<vgettsc>> case VDSO_CLOCKMODE_HVCLOCK:
+	 *   - arch/x86/kvm/x86.c|2883| <<vgettsc>> *mode = VDSO_CLOCKMODE_HVCLOCK;
+	 *   - drivers/clocksource/hyperv_timer.c|425| <<hv_cs_enable>> vclocks_set_used(VDSO_CLOCKMODE_HVCLOCK);
+	 */
 	return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2552| <<kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2269,6 +2637,16 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *
+	 * 在4.14上8个vcpu的VM nr_vcpus_matched_tsc是7
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			 atomic_read(&vcpu->kvm->online_vcpus));
 
@@ -2280,6 +2658,27 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 调用kvm_gen_update_masterclock()
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+	 * 似乎用来填充"struct pvclock_vcpu_time_info"
+	 *
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 这里其实是两个条件 ('or'不是'and'):
+	 * - ka->use_master_clock: 是不是要再evaluate一次???
+	 * - (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched)
+	 *   其实这里有隐藏条件就是!ka->use_master_clock
+	 *   也就是说没用master clock但是符合用master clock的条件
+	 *   这个时候可以再次evaluate一次!
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2300,14 +2699,40 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
  *
  * N equals to kvm_tsc_scaling_ratio_frac_bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2661| <<kvm_scale_tsc>> _tsc = __scale_tsc(ratio, tsc);
+ *   - arch/x86/kvm/x86.c|11773| <<kvm_arch_hardware_setup>> __scale_tsc(kvm_max_tsc_scaling_ratio, tsc_khz));
+ */
 static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 {
+	/*
+	 * 4.14简单测试
+	 * crash> kvm_tsc_scaling_ratio_frac_bits
+	 * kvm_tsc_scaling_ratio_frac_bits = $6 = 0 '\000'
+	 */
 	return mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2484| <<kvm_compute_tsc_offset>> tsc = kvm_scale_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/x86.c|2506| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+ *   - arch/x86/kvm/x86.c|2718| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc(vcpu, (u64) adjustment);
+ *   - arch/x86/kvm/x86.c|3281| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
+ *   - arch/x86/kvm/x86.c|4067| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(vcpu, rdtsc()) + tsc_offset;
+ */
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 {
 	u64 _tsc = tsc;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	u64 ratio = vcpu->arch.tsc_scaling_ratio;
 
 	if (ratio != kvm_default_tsc_scaling_ratio)
@@ -2317,6 +2742,13 @@ u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2570| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2649| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3755| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4681| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu,
+ */
 static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2326,18 +2758,95 @@ static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|530| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1611| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1622| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1707| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1791| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|1815| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|943| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2088| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|7646| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|2858| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|8386| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|9451| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|526| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2357| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2363| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2482| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3289| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3591| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
+/*
+ * cur_tsc_offset = 18443709364331608385,
+ *
+ * crash> eval 18443709364331608385
+ * hexadecimal: fff537f2a987f941
+ *     decimal: 18443709364331608385  (-3034709377943231)
+ *       octal: 1777651577125141774501
+ *      binary: 1111111111110101001101111111001010101001100001111111100101000001
+ *
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu0/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu1/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu2/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu3/tsc-offset
+ * -3034709377943231
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	vcpu->arch.l1_tsc_offset = offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3358| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset += vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4467| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|23| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|791| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|792| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2535| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4503| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/vmx.c|1991| <<vmx_write_l1_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.tsc_offset - g_tsc_offset, offset);
+	 *   - arch/x86/kvm/x86.c|4478| <<kvm_get_msr_common(MSR_IA32_TSC)>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset : vcpu->arch.tsc_offset;
+	 *
+	 * vmx_write_l1_tsc_offset()
+	 */
 	vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2923| <<kvm_synchronize_tsc>> if (!kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5087| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5093| <<kvm_arch_vcpu_load>> if (kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|11395| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+ *   - arch/x86/kvm/x86.c|11660| <<kvm_arch_hardware_enable>> stable = !kvm_check_tsc_unstable();
+ *
+ * 返回true说明unstable
+ * 返回false说明stable
+ */
 static inline bool kvm_check_tsc_unstable(void)
 {
 #ifdef CONFIG_X86_64
@@ -2345,12 +2854,68 @@ static inline bool kvm_check_tsc_unstable(void)
 	 * TSC is marked unstable when we're running on Hyper-V,
 	 * 'TSC page' clocksource is good.
 	 */
+	/*
+	 * 在以下使用VDSO_CLOCKMODE_HVCLOCK:
+	 *   - drivers/clocksource/hyperv_timer.c|440| <<global>> .vdso_clock_mode = VDSO_CLOCKMODE_HVCLOCK,
+	 *   - arch/x86/entry/vdso/vma.c|215| <<vvar_fault>> if (tsc_pg && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))
+	 *   - arch/x86/include/asm/vdso/clocksource.h|8| <<VDSO_ARCH_CLOCKMODES>> VDSO_CLOCKMODE_HVCLOCK
+	 *   - arch/x86/include/asm/vdso/gettimeofday.h|263| <<__arch_get_hw_counter>> if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
+	 *   - arch/x86/kvm/x86.c|2396| <<gtod_is_based_on_tsc>> return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
+	 *   - arch/x86/kvm/x86.c|2878| <<vgettsc>> case VDSO_CLOCKMODE_HVCLOCK:
+	 *   - arch/x86/kvm/x86.c|2883| <<vgettsc>> *mode = VDSO_CLOCKMODE_HVCLOCK;
+	 *   - drivers/clocksource/hyperv_timer.c|425| <<hv_cs_enable>> vclocks_set_used(VDSO_CLOCKMODE_HVCLOCK);
+	 */
 	if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
 		return false;
 #endif
 	return check_tsc_unstable();
 }
 
+/*
+ * 在4.14上, 2个vcpu调用6次, 4个vcpu调用12次
+ * /usr/share/bcc/tools/trace -t -C 'kvm_write_tsc'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 3.001404 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.002039 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.002591 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.003144 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.018026 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.018118 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.018204 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.018262 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.031890 1   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.031936 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.031979 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.032021 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ *
+ * 根据分析, 应该是QEMU来的 (不是VM kernel写的MSR TSC)
+ *
+ *
+ * KVM希望各个vCPU的TSC处在同步状态,称之为Master Clock模式,每当vTSC被修改,就有两种可能:
+ *
+ * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+ * 此时TSC进入下一代,体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+ * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offset,
+ * 其中offset即上述L1 TSC Offset.重新同步后，可以以此为基点导出任意vCPU的TSC值.
+ *
+ * vCPU在尝试重新同步TSC，此时不能将L1 offset设置为offset
+ * 此时会设置kvm->arch.nr_vcpus_matched_tsc++,一旦所有vCPU都处于matched状态,就可以重新回到Master Clock模式
+ * 在满足vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz的前提下(vTSCfreq相同是TSC能同步的前提),以下情形视为尝试同步TSC:
+ *
+ * Host Initiated且写入值为0,视为正在初始化
+ * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+ * 按照Host TSC是否同步(即是否stable),会为L1 TSC offset设置不同的值
+ *
+ * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+ * 对于Unstable Host,则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+ * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差值
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|3395| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|10581| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ *
+ * 4.14测试kvm_write_tsc的时候data=0
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2360,11 +2925,53 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool already_matched;
 	bool synchronizing = false;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * offset = vTSC - (pTSC * scale)
+	 * 这里是根据tsc计算的
+	 *
+	 * 根据写入的guest tsc和host的tsc计算scaling的offset
+	 */
 	offset = kvm_compute_tsc_offset(vcpu, data);
+	/*
+	 * monotonic time since boot in ns format
+	 *
+	 * 应该是返回启动以后的ns
+	 */
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 在以下尝试同步TSC:
+	 *   - Host Initiated且写入值为0,视为正在初始化
+	 *   - 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+	 *
+	 * vcpu->arch.virtual_tsc_khz为vCPU的vTSCfreq
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	if (vcpu->arch.virtual_tsc_khz) {
 		if (data == 0) {
 			/*
@@ -2374,6 +2981,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 */
 			synchronizing = true;
 		} else {
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2542| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|2599| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_write = data;
+			 *   - arch/x86/kvm/x86.c|11075| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2382,6 +2995,9 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 * of virtual cycle time against real time is
 			 * interpreted as an attempt to synchronize the CPU.
 			 */
+			/*
+			 * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+			 */
 			synchronizing = data < tsc_exp + tsc_hz &&
 					data + tsc_hz > tsc_exp;
 		}
@@ -2393,9 +3009,30 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	 * compensation code attempt to catch up if we fall behind, but
 	 * it's better to try to match offsets from the beginning.
          */
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2562| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2600| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 */
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+		/*
+		 * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+		 * 对于Unstable Host, 则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+		 * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差
+		 */
+
+		/*
+		 * kvm_check_tsc_unstable():
+		 * 返回true说明unstable
+		 * 返回false说明stable
+		 */
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
@@ -2405,6 +3042,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		matched = true;
 		already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
 	} else {
+		/*
+		 * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+		 * 此时TSC进入下一代m体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+		 * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offsetm
+		 * 其中offset即上述L1 TSC Offset.重新同步后, 可以以此为基点导出任意vCPU的TSC值.
+		 */
+
 		/*
 		 * We split periods of matched TSC writes into generations.
 		 * For each generation, we track the original measured
@@ -2414,9 +3058,22 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+		 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 *
+		 * 测试的例子是cur_tsc_generation = 1
+		 */
 		kvm->arch.cur_tsc_generation++;
 		kvm->arch.cur_tsc_nsec = ns;
 		kvm->arch.cur_tsc_write = data;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+		 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+		 */
 		kvm->arch.cur_tsc_offset = offset;
 		matched = false;
 	}
@@ -2432,15 +3089,35 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	vcpu->arch.last_guest_tsc = data;
 
 	/* Keep track of which generation this VCPU has synchronized to */
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
 	spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
 	if (!matched) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 */
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (!already_matched) {
 		kvm->arch.nr_vcpus_matched_tsc++;
@@ -2450,6 +3127,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3035| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3627| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|4140| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|4169| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
@@ -2457,6 +3141,10 @@ static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5082| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+ */
 static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 {
 	if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
@@ -2467,6 +3155,10 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2578| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2487,6 +3179,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2602| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2622| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2523,6 +3220,10 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2962| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2532,6 +3233,13 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 
 	do {
 		seq = read_seqcount_begin(&gtod->seq);
+		/*
+		 * CLOCK_MONOTONIC和CLOCK_MONOTONIC_RAW
+		 *
+		 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+		 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+		 * jiffies一定是单调递增的.
+		 */
 		ns = gtod->raw_clock.base_cycles;
 		ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
 		ns >>= gtod->raw_clock.shift;
@@ -2542,6 +3250,10 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2651| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2564,6 +3276,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2932| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
@@ -2627,6 +3343,20 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2759| <<kvm_gen_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|8046| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|10888| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ *
+ * GTOD: Generic time of Day
+ *
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2634,6 +3364,14 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	int vclock_mode;
 	bool host_tsc_clocksource, vcpus_matched;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
@@ -2641,14 +3379,46 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
 	 */
+	/*
+	 * 若Host Clocksource为TSC, 则读取当前时刻的TSC值, 记为pTSC1,
+	 * pvclock中记录的TSC值记为pTSC0, 我们据此设置Master Clock:
+	 *
+	 * kvm->arch.master_cycle_now设置为pTSC1, 即该时刻的Host TSC值
+	 * kvm->arch.master_kernel_ns设置为nsec_base + tsc_to_nsec(pTSC1 - pTSC0) + boot_ns 即该时刻的Host Boot Time
+	 *
+	 * kvm_get_time_and_clockread()的返回值: returns true if host is using TSC based clocksource
+	 */
 	host_tsc_clocksource = kvm_get_time_and_clockread(
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2950| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|11175| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 *
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 *
+	 * 如果写入的是MSR_KVM_SYSTEM_TIME,表明Guest使用的是旧版kvmclock,不支持Master Clock模式,
+	 * 此时要设置kvm->arch.boot_vcpu_runs_old_kvmclock = 1,并对当前vCPU(即vCPU0)发送一个KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 这最终会导致kvm->arch.use_master_clock = 0
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2663,6 +3433,38 @@ void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * 调用的次数非常少, 就是qemu创建和VM kernel启动的时候, 下面是4.14的例子
+ * kvm_gen_update_masterclock
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1270| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2158| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2336| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8243| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9288| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|10748| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|6011| <<kvm_arch_vm_ioctl>> kvm_gen_update_masterclock(kvm);
+ *   - arch/x86/kvm/x86.c|9289| <<vcpu_enter_guest>> kvm_gen_update_masterclock(vcpu->kvm);
+ *
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvm_gen_update_masterclock(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2671,15 +3473,35 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 	struct kvm_arch *ka = &kvm->arch;
 	unsigned long flags;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2753| <<kvm_gen_update_masterclock>> kvm_hv_invalidate_tsc_page(kvm);
+	 */
 	kvm_hv_invalidate_tsc_page(kvm);
 
+	/*
+	 * 申请kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *
+	 * 给所有vCPU发送KVM_REQ_MCLOCK_INPROGRESS,将它们踢出Guest模式
+	 * 该请求没有Handler,因此vCPU无法再进入Guest
+	 */
 	kvm_make_mclock_inprogress_request(kvm);
 
 	/* no guest entries from this point */
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	/*
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 	spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
 
+	/*
+	 * KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -2689,6 +3511,15 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|527| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2164| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/x86.c|6578| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|69| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|405| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|446| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2709,6 +3540,10 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
 	get_cpu();
 
+	/*
+	 * 在4.14上是3392425, 对应dmesg
+	 * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+	 */
 	if (__this_cpu_read(cpu_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
 				   &hv_clock.tsc_shift,
@@ -2722,6 +3557,16 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2868| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|2870| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|2873| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ *
+ * vcpu_enter_guest()
+ * -> kvm_guest_time_update()
+ *    -> kvm_setup_pvclock_page()
+ */
 static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				   struct gfn_to_hva_cache *cache,
 				   unsigned int offset)
@@ -2752,6 +3597,9 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	if (guest_hv_clock.version & 1)
 		++guest_hv_clock.version;  /* first time write, random junk */
 
+	/* struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.version = guest_hv_clock.version + 1;
 	kvm_write_guest_offset_cached(v->kvm, cache,
 				      &vcpu->hv_clock, offset,
@@ -2760,8 +3608,23 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	smp_wmb();
 
 	/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
+	/*
+	 * 在以下使用PVCLOCK_GUEST_STOPPED:
+	 *   - arch/x86/kernel/kvmclock.c|152| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+	 *   - arch/x86/kernel/kvmclock.c|153| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+	 *   - arch/x86/kernel/pvclock.c|80| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+	 *   - arch/x86/kernel/pvclock.c|81| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+	 *   - arch/x86/kvm/x86.c|2769| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+	 */
 	vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|2765| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|4767| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	if (vcpu->pvclock_set_guest_stopped_request) {
 		vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
 		vcpu->pvclock_set_guest_stopped_request = false;
@@ -2781,6 +3644,12 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				     sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9174| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2799,8 +3668,20 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 * to the guest.
 	 */
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 */
 	use_master_clock = ka->use_master_clock;
 	if (use_master_clock) {
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|2945| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3079| <<get_kvmclock_ns>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3216| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *
+		 * live crash了好多次, 这两个都不变
+		 */
 		host_tsc = ka->master_cycle_now;
 		kernel_ns = ka->master_kernel_ns;
 	}
@@ -2808,14 +3689,33 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* Keep irq disabled to prevent changes to the clock */
 	local_irq_save(flags);
+	/*
+	 * 在以下使用cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3308| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3453| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|8696| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|8711| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|8730| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 *
+	 * 在4.14上是3392425, 对应dmesg
+	 * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+	 */
 	tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
 	if (unlikely(tgt_tsc_khz == 0)) {
 		local_irq_restore(flags);
+		/*
+		 * kvm_guest_time_update()
+		 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 		return 1;
 	}
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
+		/*
+		 * kernel启动后的时间ns
+		 */
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
@@ -2831,6 +3731,14 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 *
+	 * 测试的一个例子是vcpu->tsc_catchup=false
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -2843,9 +3751,22 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* With all the info we got, fill in the values */
 
+	/*
+	 * 在以下设置kvm_has_tsc_control:
+	 *   - arch/x86/kvm/svm/svm.c|962| <<svm_hardware_setup>> kvm_has_tsc_control = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|8063| <<hardware_setup>> kvm_has_tsc_control = true;
+	 */
 	if (kvm_has_tsc_control)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
 
+	/*
+	 * 将1KHZ TSC转换成guest TSC
+	 * 如果当前guest时钟(kvmclock)的频率不同，则更新转换比例
+	 *
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3506| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3510| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
@@ -2853,8 +3774,46 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * 在kvm vm上测试的时候, 下面的都不变
+	 * system_time=23867275
+	 * tsc_timestamp=206742471, version=4, tsc_to_system_mul=2532092704, tsc_shift=255
+	 * kvm_clock_read()一直在变
+	 *
+	 * 把hypervisor clock换成hpet就开始增长了!!!!!
+	 *
+	 *
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 *    -> u64 tsc_timestamp;
+	 *    -> u64 system_time;
+	 *
+	 * 更新pvti时,vCPU的vTSC
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 *
+	 * 当dmesg是14.903088的时候,
+	 * kvm_clock_read()返回15169858774 (15.169858774)
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	vcpu->last_guest_tsc = tsc_timestamp;
 
 	/* If the host uses TSC clocksource, then it is stable */
@@ -2864,6 +3823,35 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	vcpu->hv_clock.flags = pvclock_flags;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *
+	 * pv_time = {
+         *   generation = 182, 
+	 *   gpa = 15401488384, 
+	 *   hva = 140509217558528, 
+	 *   len = 32, 
+	 *   memslot = 0xffff8c237bd80008
+	 * }, 
+	 * pv_time_enabled = true, 
+	 * pvclock_set_guest_stopped_request = false, 
+	 * st = {
+	 *   preempted = 1 '\001', 
+	 *   msr_val = 17979068481, 
+	 *   last_steal = 81611316161, 
+	 *   cache = {
+	 *     generation = 182, 
+	 *     gfn = 4389421, 
+	 *     pfn = 28505645, 
+	 *     dirty = true
+	 *   }
+	 * },
+	 */
 	if (vcpu->pv_time_enabled)
 		kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
 	if (vcpu->xen.vcpu_info_set)
@@ -2892,6 +3880,13 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvmclock_update_fn():
+ *   - arch/x86/kvm/x86.c|11121| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *
+ * 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	int i;
@@ -2902,20 +3897,46 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9359| <<vcpu_enter_guest>> kvm_gen_kvmclock_update(vcpu);
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+ * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 调用kvm_guest_time_update()
+	 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 调用kvmclock_update_fn(), 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
+/*
+ * 在以下使用KVMCLOCK_SYNC_PERIOD:
+ *   - arch/x86/kvm/x86.c|3281| <<kvmclock_sync_fn>> KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|10875| <<kvm_arch_vcpu_postcreate>> KVMCLOCK_SYNC_PERIOD);
+ *
+ * 300秒
+ */
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
 static void kvmclock_sync_fn(struct work_struct *work)
@@ -2928,7 +3949,19 @@ static void kvmclock_sync_fn(struct work_struct *work)
 	if (!kvmclock_periodic_sync)
 		return;
 
+	/*
+	 * 调用kvmclock_update_fn(), 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3754| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3765| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11435| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11788| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -3284,6 +4317,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))
 			return 1;
 
+		/*
+		 * 似乎在以下使用kvm_vcpu_arch->wall_clock (gpa_t):
+		 *   - arch/x86/kvm/x86.c|3690| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+		 *   - arch/x86/kvm/x86.c|3697| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+		 *   - arch/x86/kvm/x86.c|4026| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+		 *   - arch/x86/kvm/x86.c|4032| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+		 */
 		vcpu->kvm->arch.wall_clock = data;
 		kvm_write_wall_clock(vcpu->kvm, data, 0);
 		break;
@@ -4018,6 +5058,11 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		if (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))
 			goto out;
 		n = msr_list.nmsrs;
+		/*
+		 * 在以下使用msrs_to_save:
+		 *   - arch/x86/kvm/x86.c|4034| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+		 *   - arch/x86/kvm/x86.c|6062| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+		 */
 		msr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;
 		if (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))
 			goto out;
@@ -4108,6 +5153,32 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * 4.14的例子.
+ * kvm_arch_vcpu_load
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_arch_vcpu_load
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|211| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5058| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -5906,6 +6977,16 @@ long kvm_arch_vm_ioctl(struct file *filp,
 			now_ns = ka->master_kernel_ns;
 		else
 			now_ns = get_kvmclock_base_ns();
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+		 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		ka->kvmclock_offset = user_ns.clock - now_ns;
 		spin_unlock_irq(&ka->pvclock_gtod_sync_lock);
 
@@ -7277,6 +8358,11 @@ void kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip)
 }
 EXPORT_SYMBOL_GPL(kvm_inject_realmode_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7663| <<x86_emulate_instruction>> return handle_emulation_failure(vcpu, emulation_type);
+ *   - arch/x86/kvm/x86.c|7720| <<x86_emulate_instruction>> return handle_emulation_failure(vcpu, emulation_type);
+ */
 static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 {
 	++vcpu->stat.insn_emulation_fail;
@@ -7287,6 +8373,14 @@ static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用EMULTYPE_SKIP:
+	 *   - arch/x86/kvm/svm/svm.c|347| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+	 *   - arch/x86/kvm/vmx/vmx.c|1816| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+	 *   - arch/x86/kvm/x86.c|7315| <<handle_emulation_failure>> if (emulation_type & EMULTYPE_SKIP) {
+	 *   - arch/x86/kvm/x86.c|7610| <<x86_decode_emulated_instruction>> if (!(emulation_type & EMULTYPE_SKIP) &&
+	 *   - arch/x86/kvm/x86.c|7683| <<x86_emulate_instruction>> if (emulation_type & EMULTYPE_SKIP) {
+	 */
 	if (emulation_type & EMULTYPE_SKIP) {
 		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
@@ -7595,6 +8689,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5149| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|7783| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|7790| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -8095,6 +9195,15 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 };
 
 #ifdef CONFIG_X86_64
+/*
+ * 在一下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|8628| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|8637| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|8829| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+ *
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|8385| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -8103,9 +9212,25 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	int i;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+	 * 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
@@ -8119,20 +9244,88 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
+	/*
+	 * 在一下使用pvclock_gtod_work:
+	 *   - arch/x86/kvm/x86.c|8628| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+	 *   - arch/x86/kvm/x86.c|8637| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+	 *   - arch/x86/kvm/x86.c|8829| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+	 *
+	 * 在以下使用pvclock_gtod_update_fn():
+	 *   - arch/x86/kvm/x86.c|8385| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+	 */
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|8640| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|8710| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|8828| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 4.14上的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * kvm通过pvclock_gtod_register_notifier向timekeeper层注册了一个回调pvclock_gtod_notify,
+ * 每当Host Kernel时钟更新时(即timekeeping_update被调用时),就会调用pvclock_gtod_notify.
+ *
+ * 参数unused不使用
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
+	/*
+	 * struct pvclock_gtod_data {
+	 *     seqcount_t      seq;
+	 *
+	 *     struct pvclock_clock clock; // extract of a clocksource struct
+	 *     struct pvclock_clock raw_clock; // extract of a clocksource struct
+	 *
+	 *     ktime_t         offs_boot;
+	 *     u64             wall_time_sec;
+	 * };
+	 *
+	 * 在以下使用pvclock_gtod_data:
+	 *   - arch/x86/kvm/x86.c|2024| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2055| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+	 *   - arch/x86/kvm/x86.c|2275| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+	 *   - arch/x86/kvm/x86.c|2478| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2533| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2552| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2575| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+	 *   - arch/x86/kvm/x86.c|2587| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+	 *   - arch/x86/kvm/x86.c|2660| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+	 *   - arch/x86/kvm/x86.c|8172| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 */
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	struct timekeeper *tk = priv;
 
+	/*
+	 * 只在此处调用
+	 */
 	update_pvclock_gtod(tk);
 
 	/*
@@ -8140,12 +9333,36 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|8640| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|8710| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|8828| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+	 *
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 *
+	 * 根据注释,若clocksource不是TSC,但全局变量kvm_guest_has_master_clock非零,
+	 * 说明clocksource从TSC变为了非TSC,此时向所有vCPU发送KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 然后令kvm_guest_has_master_clock = 0.
+	 *
+	 * 也就是说要同时满足两个条件
+	 * 1. !gtod_is_based_on_tsc(gtod->clock.vclock_mode) --> host的clock不是tsc
+	 * 2. atomic_read(&kvm_guest_has_master_clock) != 0  --> kvm_guest_has_master_clock不是0
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
 	return 0;
 }
 
+/*
+ * 在以下使用pvclock_gtod_notifier:
+ *   - arch/x86/kvm/x86.c|8518| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|8550| <<kvm_arch_exit>> pvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);
+ */
 static struct notifier_block pvclock_gtod_notifier = {
 	.notifier_call = pvclock_gtod_notify,
 };
@@ -10403,6 +11620,12 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3414| <<kvm_vm_ioctl_create_vcpu>> kvm_arch_vcpu_postcreate(vcpu);
+ *
+ * 大概是创建vcpu到最后的时候调用
+ */
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -10749,7 +11972,24 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	mutex_init(&kvm->arch.apic_map_lock);
 	spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	/*
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 
 	kvm->arch.guest_can_read_msr_platform_info = true;
diff --git a/drivers/acpi/acpica/nseval.c b/drivers/acpi/acpica/nseval.c
index 63748ac699f7..fe26edf54aab 100644
--- a/drivers/acpi/acpica/nseval.c
+++ b/drivers/acpi/acpica/nseval.c
@@ -39,6 +39,18 @@ ACPI_MODULE_NAME("nseval")
  * MUTEX:       Locks interpreter
  *
  ******************************************************************************/
+/*
+ * called by:
+ *   - drivers/acpi/acpica/evgpe.c|506| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/evregion.c|630| <<acpi_ev_execute_reg_method>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/hwxface.c|362| <<acpi_get_sleep_type_data>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/nsinit.c|156| <<acpi_ns_initialize_devices>> status = acpi_ns_evaluate(info.evaluate_info);
+ *   - drivers/acpi/acpica/nsinit.c|176| <<acpi_ns_initialize_devices>> status = acpi_ns_evaluate(info.evaluate_info);
+ *   - drivers/acpi/acpica/nsinit.c|645| <<acpi_ns_init_one_device>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/nsxfeval.c|354| <<acpi_evaluate_object>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/rsutils.c|746| <<acpi_rs_set_srs_method_data>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/uteval.c|60| <<acpi_ut_evaluate_object>> status = acpi_ns_evaluate(info);
+ */
 acpi_status acpi_ns_evaluate(struct acpi_evaluate_info *info)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/psparse.c b/drivers/acpi/acpica/psparse.c
index 7eb7a81619a3..ef8e5d37ae0d 100644
--- a/drivers/acpi/acpica/psparse.c
+++ b/drivers/acpi/acpica/psparse.c
@@ -405,6 +405,99 @@ acpi_ps_next_parse_state(struct acpi_walk_state *walk_state,
  *
  ******************************************************************************/
 
+/*
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ut_evaluate_object
+ * [0] acpi_rs_get_prt_method_data
+ * [0] acpi_get_irq_routing_table
+ * [0] acpi_pci_irq_find_prt_entry
+ * [0] acpi_pci_irq_lookup
+ * [0] acpi_pci_irq_enable
+ * [0] do_pci_enable_device
+ * [0] pci_enable_device_flags
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_bus_get_status
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_bus_get_status
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_processor_add
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_ost
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/acpica/dbmethod.c|325| <<acpi_db_disassemble_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dbutils.c|366| <<acpi_db_second_pass_parse>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsargs.c|86| <<acpi_ds_execute_arguments>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsargs.c|125| <<acpi_ds_execute_arguments>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsmethod.c|100| <<acpi_ds_auto_serialize_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/nsparse.c|228| <<acpi_ns_one_complete_parse>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/psxface.c|190| <<acpi_ps_execute_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/psxface.c|295| <<acpi_ps_execute_table>> status = acpi_ps_parse_aml(walk_state);
+ */
 acpi_status acpi_ps_parse_aml(struct acpi_walk_state *walk_state)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/psxface.c b/drivers/acpi/acpica/psxface.c
index fd0f28c7af1e..3cc77b619493 100644
--- a/drivers/acpi/acpica/psxface.c
+++ b/drivers/acpi/acpica/psxface.c
@@ -81,6 +81,10 @@ acpi_debug_trace(const char *name, u32 debug_level, u32 debug_layer, u32 flags)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/nseval.c|205| <<acpi_ns_evaluate>> status = acpi_ps_execute_method(info);
+ */
 acpi_status acpi_ps_execute_method(struct acpi_evaluate_info *info)
 {
 	acpi_status status;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index e10d38ac7cf2..7fb53881fa41 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -321,6 +321,12 @@ static int acpi_scan_device_check(struct acpi_device *adev)
 			dev_warn(&adev->dev, "Already enumerated\n");
 			return -EALREADY;
 		}
+		/*
+		 * cpu hotplug会跑到这里
+		 * 在这里加一个while(1)就不会触发下面的event了
+		 * {"timestamp": {"seconds": 1628873592, "microseconds": 941033}, "event": "ACPI_DEVICE_OST",
+		 * "data": {"info": {"device": "core1", "source": 1, "status": 0, "slot": "2", "slot-type": "CPU"}}}
+		 */
 		error = acpi_bus_scan(adev->handle);
 		if (error) {
 			dev_warn(&adev->dev, "Namespace scan failure\n");
@@ -363,6 +369,10 @@ static int acpi_scan_bus_check(struct acpi_device *adev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|419| <<acpi_device_hotplug>> error = acpi_generic_hotplug_event(adev, src);
+ */
 static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 {
 	switch (type) {
@@ -383,6 +393,29 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * (qemu) device_add host-x86_64-cpu,id=core1,socket-id=1,core-id=0,thread-id=0
+ *
+ * Comm: kworker/u8:0
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn0
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * (qemu) device_del core1
+ *
+ * Comm: kworker/u8:3
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
diff --git a/drivers/acpi/utils.c b/drivers/acpi/utils.c
index 3b54b8fd7396..64ef25517fb3 100644
--- a/drivers/acpi/utils.c
+++ b/drivers/acpi/utils.c
@@ -404,6 +404,20 @@ EXPORT_SYMBOL(acpi_get_physical_device_location);
  * must call this function when evaluating _OST for hotplug operations.
  * When the platform does not support _OST, this function has no effect.
  */
+/*
+ * https://zhuanlan.zhihu.com/p/113296734
+ *
+ * called by:
+ *   - drivers/acpi/acpi_pad.c|404| <<acpi_pad_handle_notify>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_AGGREGATOR_NOTIFY, 0, &param);
+ *   - drivers/acpi/bus.c|496| <<acpi_bus_notify>> acpi_evaluate_ost(handle, type, ost_code, NULL);
+ *   - drivers/acpi/bus.c|577| <<sb_notify_work>> acpi_evaluate_ost(sb_handle, ACPI_OST_EC_OSPM_SHUTDOWN,
+ *   - drivers/acpi/device_sysfs.c|386| <<eject_store>> acpi_evaluate_ost(acpi_device->handle, ACPI_OST_EC_OSPM_EJECT,
+ *   - drivers/acpi/processor_perflib.c|105| <<acpi_processor_ppc_ost>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_NOTIFY_PERFORMANCE,
+ *   - drivers/acpi/scan.c|389| <<acpi_generic_hotplug_event>> acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_EJECT_REQUEST,
+ *   - drivers/acpi/scan.c|455| <<acpi_device_hotplug>> acpi_evaluate_ost(adev->handle, src, ost_code, NULL);
+ *   - drivers/pci/pcie/edr.c|137| <<acpi_send_edr_status>> status = acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_DISCONNECT_RECOVER,
+ *   - drivers/xen/xen-acpi-pad.c|92| <<acpi_pad_handle_notify>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_AGGREGATOR_NOTIFY,
+ */
 acpi_status
 acpi_evaluate_ost(acpi_handle handle, u32 source_event, u32 status_code,
 		  struct acpi_buffer *status_buf)
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index f7e83613ae94..57637c9de6cb 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -10,6 +10,13 @@
 
 #define POLL_IDLE_RELAX_COUNT	200
 
+/*
+ * called by:
+ *   - drivers/cpuidle/cpuidle.c|237| <<cpuidle_enter_state>> entered_state = target_state->enter(dev, drv, index);
+ *
+ * 在以下使用poll_idle():
+ *   - drivers/cpuidle/poll_state.c|55| <<cpuidle_poll_state_init>> state->enter = poll_idle;
+ */
 static int __cpuidle poll_idle(struct cpuidle_device *dev,
 			       struct cpuidle_driver *drv, int index)
 {
diff --git a/drivers/iommu/intel/dmar.c b/drivers/iommu/intel/dmar.c
index 84057cb9596c..eed94b655907 100644
--- a/drivers/iommu/intel/dmar.c
+++ b/drivers/iommu/intel/dmar.c
@@ -56,6 +56,21 @@ struct dmar_res_callback {
  * 2) Use RCU in interrupt context
  */
 DECLARE_RWSEM(dmar_global_lock);
+/*
+ * 在以下使用dmar_drhd_units:
+ *   - drivers/iommu/intel/cap_audit.c|149| <<cap_audit_static>> if (list_empty(&dmar_drhd_units))
+ *   - drivers/iommu/intel/dmar.c|77| <<dmar_register_drhd_unit>> list_add_tail_rcu(&drhd->list, &dmar_drhd_units);
+ *   - drivers/iommu/intel/dmar.c|79| <<dmar_register_drhd_unit>> list_add_rcu(&drhd->list, &dmar_drhd_units);
+ *   - drivers/iommu/intel/dmar.c|397| <<dmar_find_dmaru>> list_for_each_entry_rcu(dmaru, &dmar_drhd_units, list,
+ *   - drivers/iommu/intel/dmar.c|812| <<dmar_dev_scope_init>> if (list_empty(&dmar_drhd_units)) {
+ *   - drivers/iommu/intel/dmar.c|853| <<dmar_table_init>> } else if (list_empty(&dmar_drhd_units)) {
+ *   - drivers/iommu/intel/dmar.c|2127| <<dmar_free_unused_resources>> if (dmar_dev_scope_status != 1 && !list_empty(&dmar_drhd_units))
+ *   - drivers/iommu/intel/dmar.c|2131| <<dmar_free_unused_resources>> list_for_each_entry_safe(dmaru, dmaru_n, &dmar_drhd_units, list) {
+ *   - include/linux/dmar.h|74| <<for_each_drhd_unit>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|78| <<for_each_active_drhd_unit>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|83| <<for_each_active_iommu>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|88| <<for_each_iommu>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ */
 LIST_HEAD(dmar_drhd_units);
 
 struct acpi_table_header * __initdata dmar_tbl;
@@ -1819,6 +1834,10 @@ static const char *irq_remap_fault_reasons[] =
 	"Blocked an interrupt request due to source-id verification failure",
 };
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/dmar.c|1913| <<dmar_fault_do_one>> reason = dmar_get_fault_reason(fault_reason, &fault_type);
+ */
 static const char *dmar_get_fault_reason(u8 fault_reason, int *fault_type)
 {
 	if (fault_reason >= 0x20 && (fault_reason - 0x20 <
@@ -1903,6 +1922,10 @@ void dmar_msi_read(int irq, struct msi_msg *msg)
 	raw_spin_unlock_irqrestore(&iommu->register_lock, flag);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/dmar.c|1991| <<dmar_fault>> dmar_fault_do_one(iommu, type, fault_reason,
+ */
 static int dmar_fault_do_one(struct intel_iommu *iommu, int type,
 		u8 fault_reason, u32 pasid, u16 source_id,
 		unsigned long long addr)
@@ -1927,6 +1950,13 @@ static int dmar_fault_do_one(struct intel_iommu *iommu, int type,
 }
 
 #define PRIMARY_FAULT_REG_LEN (16)
+/*
+ * 在以下使用dmar_fault():
+ *   - drivers/iommu/intel/dmar.c|2027| <<dmar_set_interrupt>> ret = request_irq(irq, dmar_fault, IRQF_NO_THREAD, iommu->name, iommu);
+ *   - drivers/iommu/intel/dmar.c|2054| <<enable_drhd_fault_handling>> dmar_fault(iommu->irq, iommu);
+ *   - drivers/iommu/intel/iommu.c|2905| <<intel_iommu_init_qi>> dmar_fault(-1, iommu);
+ *   - drivers/iommu/intel/irq_remapping.c|593| <<intel_setup_irq_remapping>> dmar_fault(-1, iommu);
+ */
 irqreturn_t dmar_fault(int irq, void *dev_id)
 {
 	struct intel_iommu *iommu = dev_id;
diff --git a/drivers/iommu/intel/irq_remapping.c b/drivers/iommu/intel/irq_remapping.c
index f912fe45bea2..8c37bc33d00e 100644
--- a/drivers/iommu/intel/irq_remapping.c
+++ b/drivers/iommu/intel/irq_remapping.c
@@ -470,6 +470,11 @@ static int iommu_load_old_irte(struct intel_iommu *iommu)
 }
 
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|618| <<intel_setup_irq_remapping>> iommu_set_irq_remapping(iommu, eim_mode);
+ *   - drivers/iommu/intel/irq_remapping.c|1072| <<reenable_irq_remapping>> iommu_set_irq_remapping(iommu, eim);
+ */
 static void iommu_set_irq_remapping(struct intel_iommu *iommu, int mode)
 {
 	unsigned long flags;
@@ -1104,6 +1109,10 @@ void intel_irq_remap_add_device(struct dmar_pci_notify_info *info)
 	dev_set_msi_domain(&info->dev->dev, map_dev_to_ir(info->dev));
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1285| <<intel_irq_remapping_prepare_irte>> prepare_irte(irte, irq_cfg->vector, irq_cfg->dest_apicid);
+ */
 static void prepare_irte(struct irte *irte, int vector, unsigned int dest)
 {
 	memset(irte, 0, sizeof(*irte));
@@ -1187,6 +1196,23 @@ intel_ir_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	return IRQ_SET_MASK_OK_DONE;
 }
 
+/*
+ * [0] intel_ir_compose_msi_msg
+ * [0] irq_chip_compose_msi_msg
+ * [0] msi_domain_activate
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void intel_ir_compose_msi_msg(struct irq_data *irq_data,
 				     struct msi_msg *msg)
 {
@@ -1254,6 +1280,23 @@ static void fill_msi_msg(struct msi_msg *msg, u32 index, u32 subhandle)
 	msg->arch_data.dmar_subhandle = subhandle;
 }
 
+/*
+ * [0] intel_irq_remapping_alloc
+ * [0] intel_irq_remapping_alloc
+ * [0] msi_domain_alloc
+ * [0] __irq_domain_alloc_irqs
+ * [0] __msi_domain_alloc_irqs
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1381| <<intel_irq_remapping_alloc>> intel_irq_remapping_prepare_irte(ird, irq_cfg, info, index, i);
+ */
 static void intel_irq_remapping_prepare_irte(struct intel_ir_data *data,
 					     struct irq_cfg *irq_cfg,
 					     struct irq_alloc_info *info,
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index 808ab70d5df5..96a182d0556a 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -26,6 +26,12 @@
 #include <trace/events/iommu.h>
 
 static struct kset *iommu_group_kset;
+/*
+ * 在以下使用iommu_group_ida:
+ *   - drivers/iommu/iommu.c|597| <<iommu_group_release>> ida_simple_remove(&iommu_group_ida, group->id);
+ *   - drivers/iommu/iommu.c|653| <<iommu_group_alloc>> ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
+ *   - drivers/iommu/iommu.c|663| <<iommu_group_alloc>> ida_simple_remove(&iommu_group_ida, group->id);
+ */
 static DEFINE_IDA(iommu_group_ida);
 
 static unsigned int iommu_def_domain_type __read_mostly;
@@ -150,6 +156,29 @@ subsys_initcall(iommu_subsys_init);
  *
  * Return: 0 on success, or an error.
  */
+/*
+ * called by:
+ *   - drivers/iommu/amd/init.c|1891| <<iommu_init_pci>> iommu_device_register(&iommu->iommu, &amd_iommu_ops, NULL);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c|3669| <<arm_smmu_device_probe>> ret = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);
+ *   - drivers/iommu/arm/arm-smmu/arm-smmu.c|2164| <<arm_smmu_device_probe>> err = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);
+ *   - drivers/iommu/arm/arm-smmu/qcom_iommu.c|850| <<qcom_iommu_device_probe>> ret = iommu_device_register(&qcom_iommu->iommu, &qcom_iommu_ops, dev);
+ *   - drivers/iommu/exynos-iommu.c|633| <<exynos_sysmmu_probe>> ret = iommu_device_register(&data->iommu, &exynos_iommu_ops, dev);
+ *   - drivers/iommu/fsl_pamu_domain.c|477| <<pamu_domain_init>> ret = iommu_device_register(&pamu_iommu, &fsl_pamu_ops, NULL);
+ *   - drivers/iommu/intel/dmar.c|1158| <<alloc_iommu>> err = iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);
+ *   - drivers/iommu/intel/iommu.c|4402| <<intel_iommu_init>> iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);
+ *   - drivers/iommu/ipmmu-vmsa.c|1079| <<ipmmu_probe>> ret = iommu_device_register(&mmu->iommu, &ipmmu_ops, &pdev->dev);
+ *   - drivers/iommu/msm_iommu.c|795| <<msm_iommu_probe>> ret = iommu_device_register(&iommu->iommu, &msm_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/mtk_iommu.c|895| <<mtk_iommu_probe>> ret = iommu_device_register(&data->iommu, &mtk_iommu_ops, dev);
+ *   - drivers/iommu/mtk_iommu_v1.c|619| <<mtk_iommu_probe>> ret = iommu_device_register(&data->iommu, &mtk_iommu_ops, dev);
+ *   - drivers/iommu/omap-iommu.c|1238| <<omap_iommu_probe>> err = iommu_device_register(&obj->iommu, &omap_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/rockchip-iommu.c|1199| <<rk_iommu_probe>> err = iommu_device_register(&iommu->iommu, &rk_iommu_ops, dev);
+ *   - drivers/iommu/s390-iommu.c|336| <<zpci_init_iommu>> rc = iommu_device_register(&zdev->iommu_dev, &s390_iommu_ops, NULL);
+ *   - drivers/iommu/sprd-iommu.c|511| <<sprd_iommu_probe>> ret = iommu_device_register(&sdev->iommu, &sprd_iommu_ops, dev);
+ *   - drivers/iommu/sun50i-iommu.c|971| <<sun50i_iommu_probe>> ret = iommu_device_register(&iommu->iommu, &sun50i_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/tegra-gart.c|356| <<tegra_gart_probe>> err = iommu_device_register(&gart->iommu, &gart_iommu_ops, dev);
+ *   - drivers/iommu/tegra-smmu.c|1148| <<tegra_smmu_probe>> err = iommu_device_register(&smmu->iommu, &tegra_smmu_ops, dev);
+ *   - drivers/iommu/virtio-iommu.c|1069| <<viommu_probe>> iommu_device_register(&viommu->iommu, &viommu_ops, parent_dev);
+ */
 int iommu_device_register(struct iommu_device *iommu,
 			  const struct iommu_ops *ops, struct device *hwdev)
 {
@@ -596,6 +625,22 @@ static struct kobj_type iommu_group_ktype = {
  * group to be automatically reclaimed once it has no devices or external
  * references.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|985| <<iommu_register_group>> grp = iommu_group_alloc();
+ *   - drivers/iommu/fsl_pamu_domain.c|342| <<get_device_iommu_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1462| <<generic_device_group>> return iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1540| <<pci_device_group>> return iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1552| <<fsl_mc_device_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/ipmmu-vmsa.c|879| <<ipmmu_find_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/mtk_iommu.c|609| <<mtk_iommu_device_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/omap-iommu.c|1229| <<omap_iommu_probe>> obj->group = iommu_group_alloc();
+ *   - drivers/iommu/rockchip-iommu.c|1189| <<rk_iommu_probe>> iommu->group = iommu_group_alloc();
+ *   - drivers/iommu/sprd-iommu.c|501| <<sprd_iommu_probe>> sdev->group = iommu_group_alloc();
+ *   - drivers/iommu/sun50i-iommu.c|934| <<sun50i_iommu_probe>> iommu->group = iommu_group_alloc();
+ *   - drivers/vfio/mdev/mdev_driver.c|21| <<mdev_attach_iommu>> group = iommu_group_alloc();
+ *   - drivers/vfio/vfio.c|124| <<vfio_iommu_group_get>> group = iommu_group_alloc();
+ */
 struct iommu_group *iommu_group_alloc(void)
 {
 	struct iommu_group *group;
@@ -611,6 +656,12 @@ struct iommu_group *iommu_group_alloc(void)
 	INIT_LIST_HEAD(&group->entry);
 	BLOCKING_INIT_NOTIFIER_HEAD(&group->notifier);
 
+	/*
+	 * 在以下使用iommu_group_ida:
+	 *   - drivers/iommu/iommu.c|597| <<iommu_group_release>> ida_simple_remove(&iommu_group_ida, group->id);
+	 *   - drivers/iommu/iommu.c|653| <<iommu_group_alloc>> ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
+	 *   - drivers/iommu/iommu.c|663| <<iommu_group_alloc>> ida_simple_remove(&iommu_group_ida, group->id);
+	 */
 	ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
 	if (ret < 0) {
 		kfree(group);
@@ -834,6 +885,13 @@ static bool iommu_is_attach_deferred(struct iommu_domain *domain,
  * This function is called by an iommu driver to add a device into a
  * group.  Adding a device increments the group reference count.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|1161| <<iommu_add_device>> return iommu_group_add_device(table_group->group, dev);
+ *   - drivers/iommu/iommu.c|1609| <<iommu_group_get_for_dev>> ret = iommu_group_add_device(group, dev);
+ *   - drivers/vfio/mdev/mdev_driver.c|25| <<mdev_attach_iommu>> ret = iommu_group_add_device(group, &mdev->dev);
+ *   - drivers/vfio/vfio.c|130| <<vfio_iommu_group_get>> ret = iommu_group_add_device(group, dev);
+ */
 int iommu_group_add_device(struct iommu_group *group, struct device *dev)
 {
 	int ret, i = 0;
@@ -890,6 +948,16 @@ int iommu_group_add_device(struct iommu_group *group, struct device *dev)
 
 	trace_add_device_to_group(group->id, dev);
 
+	/*
+	 * [    0.316738] iommu: Default domain type: Translated
+	 * [    0.498945] pci 0000:00:00.0: Adding to iommu group 0
+	 * [    0.499739] pci 0000:00:01.0: Adding to iommu group 1
+	 * [    0.500488] pci 0000:00:02.0: Adding to iommu group 2
+	 * [    0.501261] pci 0000:00:03.0: Adding to iommu group 3
+	 * [    0.502012] pci 0000:00:1f.0: Adding to iommu group 4
+	 * [    0.502787] pci 0000:00:1f.2: Adding to iommu group 4
+	 * [    0.503506] pci 0000:00:1f.3: Adding to iommu group 4
+	 */
 	dev_info(dev, "Adding to iommu group %d\n", group->id);
 
 	return 0;
@@ -1427,6 +1495,16 @@ EXPORT_SYMBOL_GPL(generic_device_group);
  * Use standard PCI bus topology, isolation features, and DMA alias quirks
  * to find or create an IOMMU group for a device.
  */
+/*
+ * called by:
+ *   - drivers/iommu/amd/iommu.c|1738| <<amd_iommu_device_group>> return pci_device_group(dev);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c|2546| <<arm_smmu_device_group>> group = pci_device_group(dev);
+ *   - drivers/iommu/arm/arm-smmu/arm-smmu.c|1473| <<arm_smmu_device_group>> group = pci_device_group(dev);
+ *   - drivers/iommu/fsl_pamu_domain.c|394| <<get_pci_device_group>> group = pci_device_group(&pdev->dev);
+ *   - drivers/iommu/intel/iommu.c|5290| <<intel_iommu_device_group>> return pci_device_group(dev);
+ *   - drivers/iommu/tegra-smmu.c|929| <<tegra_smmu_device_group>> group->group = pci_device_group(dev);
+ *   - drivers/iommu/virtio-iommu.c|924| <<viommu_device_group>> return pci_device_group(dev);
+ */
 struct iommu_group *pci_device_group(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
diff --git a/drivers/misc/pvpanic/pvpanic.c b/drivers/misc/pvpanic/pvpanic.c
index 65f70a4da8c0..3e299e3f2945 100644
--- a/drivers/misc/pvpanic/pvpanic.c
+++ b/drivers/misc/pvpanic/pvpanic.c
@@ -25,6 +25,13 @@ MODULE_AUTHOR("Mihai Carabas <mihai.carabas@oracle.com>");
 MODULE_DESCRIPTION("pvpanic device driver ");
 MODULE_LICENSE("GPL");
 
+/*
+ * 在以下使用pvpanic_list:
+ *   - drivers/misc/pvpanic/pvpanic.c|37| <<pvpanic_send_event>> list_for_each_entry(pi_cur, &pvpanic_list, list) {
+ *   - drivers/misc/pvpanic/pvpanic.c|69| <<pvpanic_probe>> list_add(&pi->list, &pvpanic_list);
+ *   - drivers/misc/pvpanic/pvpanic.c|84| <<pvpanic_remove>> list_for_each_entry_safe(pi_cur, pi_next, &pvpanic_list, list) {
+ *   - drivers/misc/pvpanic/pvpanic.c|96| <<pvpanic_init>> INIT_LIST_HEAD(&pvpanic_list);
+ */
 static struct list_head pvpanic_list;
 static spinlock_t pvpanic_lock;
 
@@ -60,6 +67,25 @@ static struct notifier_block pvpanic_panic_nb = {
 	.priority = 1, /* let this called before broken drm_fb_helper */
 };
 
+/*
+ * [0] pvpanic_probe
+ * [0] platform_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/misc/pvpanic/pvpanic-mmio.c|109| <<pvpanic_mmio_probe>> return pvpanic_probe(pi);
+ *   - drivers/misc/pvpanic/pvpanic-pci.c|102| <<pvpanic_pci_probe>> return pvpanic_probe(pi);
+ */
 int pvpanic_probe(struct pvpanic_instance *pi)
 {
 	if (!pi || !pi->base)
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 78a01c71a17c..7a4ffd356bb5 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -214,6 +214,19 @@ struct virtnet_info {
 	u8 hdr_len;
 
 	/* Work struct for refilling if we run low on memory. */
+	/*
+	 * 在以下使用virtnet_info->refill:
+	 *   - drivers/net/virtio_net.c|1383| <<refill_work>> container_of(work, struct virtnet_info, refill.work);
+	 *   - drivers/net/virtio_net.c|1398| <<refill_work>> schedule_delayed_work(&vi->refill, HZ/2);
+	 *   - drivers/net/virtio_net.c|1429| <<virtnet_receive>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1552| <<virtnet_open>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1877| <<_virtnet_set_queues>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1899| <<virtnet_close>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2375| <<virtnet_freeze_down>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2401| <<virtnet_restore_up>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|2883| <<virtnet_alloc_queues>> INIT_DELAYED_WORK(&vi->refill, refill_work);
+	 *   - drivers/net/virtio_net.c|3234| <<virtnet_probe>> cancel_delayed_work_sync(&vi->refill);
+	 */
 	struct delayed_work refill;
 
 	/* Work struct for config space updates */
@@ -319,6 +332,13 @@ static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 	return p;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|339| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|354| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|1340| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|1352| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
@@ -1340,6 +1360,15 @@ static void skb_recv_done(struct virtqueue *rvq)
 	virtqueue_napi_schedule(&rq->napi, rvq);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1371| <<virtnet_napi_tx_enable>> return virtnet_napi_enable(vq, napi);
+ *   - drivers/net/virtio_net.c|1392| <<refill_work>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|1565| <<virtnet_open>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2404| <<virtnet_restore_up>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2538| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2555| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ */
 static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 {
 	napi_enable(napi);
@@ -1854,6 +1883,12 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1888| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2161| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2517| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -2690,6 +2725,10 @@ static void virtnet_free_queues(struct virtnet_info *vi)
 	kfree(vi->ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2712| <<free_receive_bufs>> _free_receive_bufs(vi);
+ */
 static void _free_receive_bufs(struct virtnet_info *vi)
 {
 	struct bpf_prog *old_prog;
@@ -2706,6 +2745,10 @@ static void _free_receive_bufs(struct virtnet_info *vi)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3249| <<remove_vq_common>> free_receive_bufs(vi);
+ */
 static void free_receive_bufs(struct virtnet_info *vi)
 {
 	rtnl_lock();
diff --git a/drivers/net/xen-netback/rx.c b/drivers/net/xen-netback/rx.c
index accc991d153f..02a4c84c60f1 100644
--- a/drivers/net/xen-netback/rx.c
+++ b/drivers/net/xen-netback/rx.c
@@ -137,6 +137,11 @@ static void xenvif_rx_queue_drop_expired(struct xenvif_queue *queue)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|183| <<xenvif_rx_copy_add>> xenvif_rx_copy_flush(queue);
+ *   - drivers/net/xen-netback/rx.c|487| <<xenvif_rx_action>> xenvif_rx_copy_flush(queue);
+ */
 static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 {
 	unsigned int i;
@@ -171,6 +176,10 @@ static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 	__skb_queue_purge(queue->rx_copy.completed);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|387| <<xenvif_rx_data_slot>> xenvif_rx_copy_add(queue, req, offset, data, len);
+ */
 static void xenvif_rx_copy_add(struct xenvif_queue *queue,
 			       struct xen_netif_rx_request *req,
 			       unsigned int offset, void *data, size_t len)
@@ -371,6 +380,10 @@ static void xenvif_rx_next_chunk(struct xenvif_queue *queue,
 	*len = chunk_len;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|461| <<xenvif_rx_skb>> xenvif_rx_data_slot(queue, &pkt, req, rsp);
+ */
 static void xenvif_rx_data_slot(struct xenvif_queue *queue,
 				struct xenvif_pkt_state *pkt,
 				struct xen_netif_rx_request *req,
@@ -439,6 +452,10 @@ static void xenvif_rx_extra_slot(struct xenvif_queue *queue,
 	BUG();
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|482| <<xenvif_rx_action>> xenvif_rx_skb(queue);
+ */
 static void xenvif_rx_skb(struct xenvif_queue *queue)
 {
 	struct xenvif_pkt_state pkt;
@@ -469,6 +486,10 @@ static void xenvif_rx_skb(struct xenvif_queue *queue)
 
 #define RX_BATCH_SIZE 64
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|646| <<xenvif_kthread_guest_rx>> xenvif_rx_action(queue);
+ */
 void xenvif_rx_action(struct xenvif_queue *queue)
 {
 	struct sk_buff_head completed_skbs;
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 44275908d61a..de188737d9c2 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -356,6 +356,25 @@ static void xennet_alloc_rx_buffers(struct netfront_queue *queue)
 		notify_remote_via_irq(queue->rx_irq);
 }
 
+/*
+ * [0] xennet_open
+ * [0] __dev_open
+ * [0] __dev_change_flags
+ * [0] dev_change_flags
+ * [0] do_setlink
+ * [0] __rtnl_newlink
+ * [0] rtnl_newlink
+ * [0] rtnetlink_rcv_msg
+ * [0] netlink_rcv_skb
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int xennet_open(struct net_device *dev)
 {
 	struct netfront_info *np = netdev_priv(dev);
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index 03faf20a6817..0a0c23d021f8 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -82,6 +82,11 @@
 #define DMAR_IQA_REG	0x90	/* Invalidation queue addr register */
 #define DMAR_ICS_REG	0x9c	/* Invalidation complete status register */
 #define DMAR_IQER_REG	0xb0	/* Invalidation queue error record register */
+/*
+ * 在以下使用DMAR_IRTA_REG:
+ *   - drivers/iommu/intel/irq_remapping.c|440| <<iommu_load_old_irte>> irta = dmar_readq(iommu->reg + DMAR_IRTA_REG);
+ *   - drivers/iommu/intel/irq_remapping.c|483| <<iommu_set_irq_remapping>> dmar_writeq(iommu->reg + DMAR_IRTA_REG,
+ */
 #define DMAR_IRTA_REG	0xb8    /* Interrupt remapping table addr register */
 #define DMAR_PQH_REG	0xc0	/* Page request queue head register */
 #define DMAR_PQT_REG	0xc8	/* Page request queue tail register */
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8583ed3ff344..0d509c5f4388 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1176,6 +1176,24 @@ search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1256| <<resize_hpt_rehash_hpte>> __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|120| <<kvmppc_set_dirty_from_hpte>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|136| <<revmap_for_hpte>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|221| <<kvmppc_do_h_enter>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|881| <<kvmppc_get_hpa>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|758| <<account_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|788| <<unaccount_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|946| <<gfn_to_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|139| <<inspect_spte_has_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|202| <<audit_write_protection>> slot = __gfn_to_memslot(slots, sp->gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<handle_changed_spte_dirty_log>> slot = __gfn_to_memslot(__kvm_memslots(kvm, as_id), gfn);
+ *   - virt/kvm/kvm_main.c|1843| <<gfn_to_memslot>> return __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - virt/kvm/kvm_main.c|1849| <<kvm_vcpu_gfn_to_memslot>> return __gfn_to_memslot(kvm_vcpu_memslots(vcpu), gfn);
+ *   - virt/kvm/kvm_main.c|2336| <<__kvm_map_gfn>> struct kvm_memory_slot *slot = __gfn_to_memslot(slots, gfn);
+ *   - virt/kvm/kvm_main.c|2721| <<__kvm_gfn_to_hva_cache_init>> ghc->memslot = __gfn_to_memslot(slots, start_gfn);
+ */
 static inline struct kvm_memory_slot *
 __gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
 {
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 5cbc950b34df..ed90c68355c0 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -2156,6 +2156,13 @@ struct net_device {
 #ifdef CONFIG_PCPU_DEV_REFCNT
 	int __percpu		*pcpu_refcnt;
 #else
+	/*
+	 * 在以下使用net_device->dev_refcnt:
+	 *   - include/linux/netdevice.h|4142| <<dev_put>> refcount_dec(&dev->dev_refcnt);
+	 *   - include/linux/netdevice.h|4157| <<dev_hold>> refcount_inc(&dev->dev_refcnt);
+	 *   - net/core/dev.c|10420| <<netdev_refcnt_read>> return refcount_read(&dev->dev_refcnt);
+	 *   - net/core/dev.c|10789| <<alloc_netdev_mqs>> refcount_set(&dev->dev_refcnt, 1);
+	 */
 	refcount_t		dev_refcnt;
 #endif
 
diff --git a/include/linux/timekeeping.h b/include/linux/timekeeping.h
index 78a98bdff76d..b6e384cd9e36 100644
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@ -161,6 +161,12 @@ static inline u64 ktime_get_real_ns(void)
 
 static inline u64 ktime_get_boottime_ns(void)
 {
+	/*
+	 * ktime_get_boottime - Returns monotonic time since boot in ktime_t format
+	 *
+	 * This is similar to CLOCK_MONTONIC/ktime_get, but also includes the
+	 * time spent in suspend.
+	 */
 	return ktime_to_ns(ktime_get_boottime());
 }
 
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 79d9c44d1ad7..25850504ea8f 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -251,6 +251,25 @@ struct kvm_xen_exit {
 #define KVM_EXIT_S390_RESET       14
 #define KVM_EXIT_DCR              15 /* deprecated */
 #define KVM_EXIT_NMI              16
+/*
+ * x86在以下使用KVM_EXIT_INTERNAL_ERROR:
+ *   - arch/x86/kvm/svm/nested.c|1366| <<svm_get_nested_state_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/svm/sev.c|2199| <<sev_es_validate_vmgexit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/svm/svm.c|3226| <<svm_handle_invalid_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/nested.c|3135| <<nested_get_vmcs12_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/nested.c|3193| <<vmx_get_nested_state_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|56| <<sgx_handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|115| <<sgx_inject_fault>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|158| <<__handle_encls_ecreate>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|5077| <<handle_exception_nmi>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|5649| <<handle_invalid_guest_state>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|6240| <<__vmx_handle_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|6304| <<__vmx_handle_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|7311| <<handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|7320| <<handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|9984| <<kvm_task_switch>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|11712| <<kvm_handle_memory_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ */
 #define KVM_EXIT_INTERNAL_ERROR   17
 #define KVM_EXIT_OSI              18
 #define KVM_EXIT_PAPR_HCALL	  19
@@ -272,12 +291,32 @@ struct kvm_xen_exit {
 
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
+/*
+ * x86在以下使用KVM_INTERNAL_ERROR_EMULATION:
+ *   - arch/x86/kvm/svm/nested.c|1368| <<svm_get_nested_state_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/nested.c|3137| <<nested_get_vmcs12_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/nested.c|3195| <<vmx_get_nested_state_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|57| <<sgx_handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|116| <<sgx_inject_fault>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|159| <<__handle_encls_ecreate>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/vmx.c|5651| <<handle_invalid_guest_state>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|7312| <<handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|7321| <<handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|9985| <<kvm_task_switch>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|11713| <<kvm_handle_memory_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ */
 #define KVM_INTERNAL_ERROR_EMULATION	1
 /* Encounter unexpected simultaneous exceptions. */
 #define KVM_INTERNAL_ERROR_SIMUL_EX	2
 /* Encounter unexpected vm-exit due to delivery event. */
 #define KVM_INTERNAL_ERROR_DELIVERY_EV	3
 /* Encounter unexpected vm-exit reason */
+/*
+ * 在以下使用KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON:
+ *   - arch/x86/kvm/svm/sev.c|2200| <<sev_es_validate_vmgexit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ *   - arch/x86/kvm/svm/svm.c|3227| <<svm_handle_invalid_exit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ *   - arch/x86/kvm/vmx/vmx.c|6306| <<__vmx_handle_exit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ */
 #define KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON	4
 
 /* for KVM_RUN, returned by mmap(vcpu_fd, offset=0) */
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index cbff6ba53d56..816ec8e6b420 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -105,6 +105,13 @@ struct qnode {
  *
  * PV doubles the storage and uses the second cacheline for PV state.
  */
+/*
+ * 在以下使用percpu的qnodes[]:
+ *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+ *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+ *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+ *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+ */
 static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
 
 /*
@@ -122,11 +129,22 @@ static inline __pure u32 encode_tail(int cpu, int idx)
 	return tail;
 }
 
+/*
+ * called by:
+ *   - kernel/locking/qspinlock.c|508| <<queued_spin_lock_slowpath>> prev = decode_tail(old);
+ */
 static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 {
 	int cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;
 	int idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
 
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 */
 	return per_cpu_ptr(&qnodes[idx].mcs, cpu);
 }
 
@@ -312,6 +330,22 @@ static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,
  * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
  *   queue               :         ^--'                             :
  */
+/*
+ * 在以下使用queued_spin_lock_slowpath():
+ *   - arch/x86/kernel/paravirt.c|362| <<global>> .lock.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
+ *   - include/asm-generic/qspinlock.h|71| <<global>> extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);
+ *   - kernel/locking/qspinlock.c|289| <<global>> #define queued_spin_lock_slowpath native_queued_spin_lock_slowpath
+ *   - kernel/locking/qspinlock.c|595| <<global>> #undef queued_spin_lock_slowpath
+ *   - kernel/locking/qspinlock.c|596| <<global>> #define queued_spin_lock_slowpath __pv_queued_spin_lock_slowpath
+ *   - arch/powerpc/include/asm/qspinlock.h|15| <<queued_spin_lock_slowpath>> static __always_inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+ *   - arch/powerpc/include/asm/qspinlock.h|43| <<queued_spin_lock>> queued_spin_lock_slowpath(lock, val);
+ *   - arch/x86/hyperv/hv_spinlock.c|80| <<hv_init_spinlocks>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - arch/x86/include/asm/paravirt.h|585| <<pv_queued_spin_lock_slowpath>> PVOP_VCALL2(lock.queued_spin_lock_slowpath, lock, val);
+ *   - arch/x86/include/asm/qspinlock.h|49| <<queued_spin_lock_slowpath>> static inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+ *   - arch/x86/kernel/kvm.c|979| <<kvm_spinlock_init>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - arch/x86/xen/spinlock.c|139| <<xen_init_spinlocks>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - include/asm-generic/qspinlock.h|85| <<queued_spin_lock>> queued_spin_lock_slowpath(lock, val);
+ */
 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 {
 	struct mcs_spinlock *prev, *next, *node;
@@ -397,6 +431,15 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 queue:
 	lockevent_inc(lock_slowpath);
 pv_queue:
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 *
+	 * struct mcs_spinlock *prev, *next, *node;
+	 */
 	node = this_cpu_ptr(&qnodes[0].mcs);
 	idx = node->count++;
 	tail = encode_tail(smp_processor_id(), idx);
@@ -412,6 +455,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 */
 	if (unlikely(idx >= MAX_NODES)) {
 		lockevent_inc(lock_no_node);
+		/*
+		 * called by:
+		 *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+		 *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+		 * 重新定义的queued_spin_trylock():
+		 * #define queued_spin_trylock(l)       pv_hybrid_queued_unfair_trylock(l)
+		 */
 		while (!queued_spin_trylock(lock))
 			cpu_relax();
 		goto release;
@@ -440,6 +490,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 * attempt the trylock once more in the hope someone let go while we
 	 * weren't watching.
 	 */
+	/*
+	 * called by:
+	 *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+	 *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+	 * 重新定义的queued_spin_trylock():
+	 * #define queued_spin_trylock(l)       pv_hybrid_queued_unfair_trylock(l)
+	 */
 	if (queued_spin_trylock(lock))
 		goto release;
 
@@ -557,6 +614,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	/*
 	 * release the node
 	 */
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 */
 	__this_cpu_dec(qnodes[0].mcs.count);
 }
 EXPORT_SYMBOL(queued_spin_lock_slowpath);
diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h
index e84d21aa0722..12bbb17628de 100644
--- a/kernel/locking/qspinlock_paravirt.h
+++ b/kernel/locking/qspinlock_paravirt.h
@@ -21,6 +21,13 @@
  * native_queued_spin_unlock().
  */
 
+/*
+ * 在以下使用_Q_SLOW_VAL:
+ *   - kernel/locking/qspinlock_paravirt.h|391| <<pv_kick_node>> WRITE_ONCE(lock->locked, _Q_SLOW_VAL);
+ *   - kernel/locking/qspinlock_paravirt.h|456| <<pv_wait_head_or_lock>> if (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {
+ *   - kernel/locking/qspinlock_paravirt.h|470| <<pv_wait_head_or_lock>> pv_wait(&lock->locked, _Q_SLOW_VAL);
+ *   - kernel/locking/qspinlock_paravirt.h|501| <<__pv_queued_spin_unlock_slowpath>> if (unlikely(locked != _Q_SLOW_VAL)) {
+ */
 #define _Q_SLOW_VAL	(3U << _Q_LOCKED_OFFSET)
 
 /*
@@ -35,12 +42,24 @@
  * controlled by PV_PREV_CHECK_MASK. This is to ensure that we won't
  * pound on the cacheline of the previous node too heavily.
  */
+/*
+ * 只在以下使用PV_PREV_CHECK_MASK:
+ *   - kernel/locking/qspinlock_paravirt.h|269| <<pv_wait_early>> if ((loop & PV_PREV_CHECK_MASK) != 0)
+ */
 #define PV_PREV_CHECK_MASK	0xff
 
 /*
  * Queue node uses: vcpu_running & vcpu_halted.
  * Queue head uses: vcpu_running & vcpu_hashed.
  */
+/*
+ * 在以下使用vcpu_halted:
+ *   - kernel/locking/qspinlock_paravirt.h|320| <<pv_wait_node>> smp_store_mb(pn->state, vcpu_halted);
+ *   - kernel/locking/qspinlock_paravirt.h|325| <<pv_wait_node>> pv_wait(&pn->state, vcpu_halted);
+ *   - kernel/locking/qspinlock_paravirt.h|333| <<pv_wait_node>> cmpxchg(&pn->state, vcpu_halted, vcpu_running);
+ *   - kernel/locking/qspinlock_paravirt.h|380| <<pv_kick_node>> if (cmpxchg_relaxed(&pn->state, vcpu_halted, vcpu_hashed)
+ *   - kernel/locking/qspinlock_paravirt.h|381| <<pv_kick_node>> != vcpu_halted)
+ */
 enum vcpu_state {
 	vcpu_running = 0,
 	vcpu_halted,		/* Used only in pv_wait_node */
@@ -77,6 +96,13 @@ struct pv_node {
  * queued lock (no lock starvation) and an unfair lock (good performance
  * on not heavily contended locks).
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+ *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+ * 重新定义的queued_spin_trylock():
+ * #define queued_spin_trylock(l)	pv_hybrid_queued_unfair_trylock(l)
+ */
 #define queued_spin_trylock(l)	pv_hybrid_queued_unfair_trylock(l)
 static inline bool pv_hybrid_queued_unfair_trylock(struct qspinlock *lock)
 {
@@ -173,10 +199,34 @@ struct pv_hash_entry {
 	struct pv_node   *node;
 };
 
+/*
+ * 在以下使用PV_HE_PER_LINE:
+ *   - kernel/locking/qspinlock_paravirt.h|230| <<__pv_init_lock_hash>> int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);
+ *   - kernel/locking/qspinlock_paravirt.h|248| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+ */
 #define PV_HE_PER_LINE	(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))
+/*
+ * 在以下使用PV_HE_MIN:
+ *   - kernel/locking/qspinlock_paravirt.h|232| <<__pv_init_lock_hash>> if (pv_hash_size < PV_HE_MIN)
+ *   - kernel/locking/qspinlock_paravirt.h|233| <<__pv_init_lock_hash>> pv_hash_size = PV_HE_MIN;
+ */
 #define PV_HE_MIN	(PAGE_SIZE / sizeof(struct pv_hash_entry))
 
+/*
+ * 在以下使用pv_lock_hash:
+ *   - kernel/locking/qspinlock_paravirt.h|199| <<__pv_init_lock_hash>> pv_lock_hash = alloc_large_system_hash("PV qspinlock",
+ *   - kernel/locking/qspinlock_paravirt.h|208| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+ *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+ */
 static struct pv_hash_entry *pv_lock_hash;
+/*
+ * 在以下使用pv_lock_hash_bits:
+ *   - kernel/locking/qspinlock_paravirt.h|203| <<__pv_init_lock_hash>> &pv_lock_hash_bits, NULL,
+ *   - kernel/locking/qspinlock_paravirt.h|209| <<for_each_hash_entry>> offset < (1 << pv_lock_hash_bits); \
+ *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+ *   - kernel/locking/qspinlock_paravirt.h|214| <<pv_hash>> unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
+ *   - kernel/locking/qspinlock_paravirt.h|241| <<pv_unhash>> unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
+ */
 static unsigned int pv_lock_hash_bits __read_mostly;
 
 /*
@@ -185,6 +235,13 @@ static unsigned int pv_lock_hash_bits __read_mostly;
  * This function should be called from the paravirt spinlock initialization
  * routine.
  */
+/*
+ * called by:
+ *   - arch/powerpc/include/asm/qspinlock.h|70| <<pv_spinlocks_init>> __pv_init_lock_hash();
+ *   - arch/x86/hyperv/hv_spinlock.c|79| <<hv_init_spinlocks>> __pv_init_lock_hash();
+ *   - arch/x86/kernel/kvm.c|978| <<kvm_spinlock_init>> __pv_init_lock_hash();
+ *   - arch/x86/xen/spinlock.c|138| <<xen_init_spinlocks>> __pv_init_lock_hash();
+ */
 void __init __pv_init_lock_hash(void)
 {
 	int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);
@@ -196,6 +253,12 @@ void __init __pv_init_lock_hash(void)
 	 * Allocate space from bootmem which should be page-size aligned
 	 * and hence cacheline aligned.
 	 */
+	/*
+	 * 在以下使用pv_lock_hash:
+	 *   - kernel/locking/qspinlock_paravirt.h|199| <<__pv_init_lock_hash>> pv_lock_hash = alloc_large_system_hash("PV qspinlock",
+	 *   - kernel/locking/qspinlock_paravirt.h|208| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+	 *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+	 */
 	pv_lock_hash = alloc_large_system_hash("PV qspinlock",
 					       sizeof(struct pv_hash_entry),
 					       pv_hash_size, 0,
@@ -204,11 +267,21 @@ void __init __pv_init_lock_hash(void)
 					       pv_hash_size, pv_hash_size);
 }
 
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|268| <<pv_hash>> for_each_hash_entry(he, offset, hash) {
+ *   - kernel/locking/qspinlock_paravirt.h|295| <<pv_unhash>> for_each_hash_entry(he, offset, hash) {
+ */
 #define for_each_hash_entry(he, offset, hash)						\
 	for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0;	\
 	     offset < (1 << pv_lock_hash_bits);						\
 	     offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
 
+/*
+ * 在以下使用pv_hash():
+ *   - kernel/locking/qspinlock_paravirt.h|480| <<pv_kick_node>> (void )pv_hash(lock, pn);
+ *   - kernel/locking/qspinlock_paravirt.h|531| <<pv_wait_head_or_lock>> lp = pv_hash(lock, pn);
+ */
 static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)
 {
 	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
@@ -236,6 +309,10 @@ static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)
 	BUG();
 }
 
+/*
+ * 在以下使用pv_unhash():
+ *   - kernel/locking/qspinlock_paravirt.h|609| <<__pv_queued_spin_unlock_slowpath>> node = pv_unhash(lock);
+ */
 static struct pv_node *pv_unhash(struct qspinlock *lock)
 {
 	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
@@ -245,6 +322,12 @@ static struct pv_node *pv_unhash(struct qspinlock *lock)
 	for_each_hash_entry(he, offset, hash) {
 		if (READ_ONCE(he->lock) == lock) {
 			node = READ_ONCE(he->node);
+			/*
+			 * truct pv_hash_entry {
+			 *     struct qspinlock *lock;
+			 *     struct pv_node   *node;
+			 * };
+			 */
 			WRITE_ONCE(he->lock, NULL);
 			return node;
 		}
@@ -263,9 +346,29 @@ static struct pv_node *pv_unhash(struct qspinlock *lock)
  * Return true if when it is time to check the previous node which is not
  * in a running state.
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|304| <<pv_wait_node>> if (pv_wait_early(pp, loop)) {
+ *   - kernel/locking/qspinlock_paravirt.h|324| <<pv_wait_node>> lockevent_cond_inc(pv_wait_early, wait_early);
+ */
 static inline bool
 pv_wait_early(struct pv_node *prev, int loop)
 {
+	/*
+	 * Queue Node Adaptive Spinning
+	 *
+	 * A queue node vCPU will stop spinning if the vCPU in the previous node is
+	 * not running. The one lock stealing attempt allowed at slowpath entry
+	 * mitigates the slight slowdown for non-overcommitted guest with this
+	 * aggressive wait-early mechanism.
+	 *      
+	 * The status of the previous node will be checked at fixed interval
+	 * controlled by PV_PREV_CHECK_MASK. This is to ensure that we won't
+	 * pound on the cacheline of the previous node too heavily.
+	 *
+	 * 只在以下使用PV_PREV_CHECK_MASK:
+	 *   - kernel/locking/qspinlock_paravirt.h|269| <<pv_wait_early>> if ((loop & PV_PREV_CHECK_MASK) != 0)
+	 */
 	if ((loop & PV_PREV_CHECK_MASK) != 0)
 		return false;
 
@@ -275,6 +378,12 @@ pv_wait_early(struct pv_node *prev, int loop)
 /*
  * Initialize the PV part of the mcs_spinlock node.
  */
+/*
+ * 在以下使用pv_init_node():
+ *   - kernel/locking/qspinlock.c|283| <<global>> #define pv_init_node __pv_init_node
+ *   - kernel/locking/qspinlock.c|590| <<global>> #undef pv_init_node
+ *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> pv_init_node(node);
+ */
 static void pv_init_node(struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -290,6 +399,14 @@ static void pv_init_node(struct mcs_spinlock *node)
  * pv_kick_node() is used to set _Q_SLOW_VAL and fill in hash table on its
  * behalf.
  */
+/*
+ * 在以下使用pv_wait_node():
+ *   - kernel/locking/qspinlock.c|284| <<global>> #define pv_wait_node __pv_wait_node
+ *   - kernel/locking/qspinlock.c|591| <<global>> #undef pv_wait_node
+ *   - kernel/locking/lock_events_list.h|35| <<LOCK_EVENT>> LOCK_EVENT(pv_wait_node)
+ *   - kernel/locking/qspinlock.c|490| <<queued_spin_lock_slowpath>> pv_wait_node(node, prev);
+ *   - kernel/locking/qspinlock_paravirt.h|411| <<pv_wait_node>> lockevent_inc(pv_wait_node);
+ */
 static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -357,6 +474,12 @@ static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
  * such that they're waiting in pv_wait_head_or_lock(), this avoids a
  * wake/sleep cycle.
  */
+/*
+ * 在以下使用pv_kick_node():
+ *   - kernel/locking/qspinlock.c|285| <<global>> #define pv_kick_node __pv_kick_node
+ *   - kernel/locking/qspinlock.c|592| <<global>> #undef pv_kick_node
+ *   - kernel/locking/qspinlock.c|571| <<queued_spin_lock_slowpath>> pv_kick_node(lock, next);
+ */
 static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -399,6 +522,12 @@ static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
  *
  * The current value of the lock will be returned for additional processing.
  */
+/*
+ * 在以下使用pv_wait_head_or_lock():
+ *   - kernel/locking/qspinlock.c|286| <<global>> #define pv_wait_head_or_lock __pv_wait_head_or_lock
+ *   - kernel/locking/qspinlock.c|593| <<global>> #undef pv_wait_head_or_lock
+ *   - kernel/locking/qspinlock.c|525| <<queued_spin_lock_slowpath>> if ((val = pv_wait_head_or_lock(lock, node)))
+ */
 static u32
 pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
 {
@@ -489,6 +618,10 @@ pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
  * PV versions of the unlock fastpath and slowpath functions to be used
  * instead of queued_spin_unlock().
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|560| <<__pv_queued_spin_unlock>> __pv_queued_spin_unlock_slowpath(lock, locked);
+ */
 __visible void
 __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 {
@@ -544,6 +677,14 @@ __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 #include <asm/qspinlock_paravirt.h>
 
 #ifndef __pv_queued_spin_unlock
+/*
+ * 在以下使用__pv_queued_spin_unlock():
+ *   - arch/x86/include/asm/qspinlock_paravirt.h|66| <<global>> PV_CALLEE_SAVE_REGS_THUNK(__pv_queued_spin_unlock);
+ *   - arch/powerpc/include/asm/qspinlock.h|29| <<queued_spin_unlock>> __pv_queued_spin_unlock(lock);
+ *   - arch/x86/hyperv/hv_spinlock.c|81| <<hv_init_spinlocks>> pv_ops.lock.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ *   - arch/x86/kernel/kvm.c|981| <<kvm_spinlock_init>> PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ *   - arch/x86/xen/spinlock.c|141| <<xen_init_spinlocks>> PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ */
 __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 {
 	u8 locked;
@@ -553,7 +694,18 @@ __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 	 * unhash. Otherwise it would be possible to have multiple @lock
 	 * entries, which would be BAD.
 	 */
+	/*
+	 * cmpxchg(void *ptr, unsigned long old, unsigned long new);
+	 * 函数完成的功能是: 将old和ptr指向的内容比较,如果相等,
+	 * 则将new写入到ptr中,返回old,如果不相等,则返回ptr指向的内容.
+	 *
+	 * struct qspinlock *lock:
+	 * -> u8 locked;
+	 */
 	locked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);
+	/*
+	 * 如果slow的话最后两位可能都是11
+	 */
 	if (likely(locked == _Q_LOCKED_VAL))
 		return;
 
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 8a364aa9881a..7d359593b30a 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -657,6 +657,12 @@ static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
  * pvclock_gtod_register_notifier - register a pvclock timedata update listener
  * @nb: Pointer to the notifier block to register
  */
+/*
+ * called by:
+ *   - arch/arm/xen/enlighten.c|393| <<xen_guest_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|8528| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/xen/time.c|520| <<xen_time_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ */
 int pvclock_gtod_register_notifier(struct notifier_block *nb)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1754,6 +1760,11 @@ void timekeeping_inject_sleeptime64(const struct timespec64 *delta)
 /**
  * timekeeping_resume - Resumes the generic timekeeping subsystem.
  */
+/*
+ * 在以下使用timekeeping_resume():
+ *   - struct syscore_ops timekeeping_syscore_ops.resume = timekeeping_resume()
+ *   - kernel/time/tick-common.c|549| <<tick_unfreeze>> timekeeping_resume();
+ */
 void timekeeping_resume(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index 6f5f78885ab4..cc364cb43033 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -59,8 +59,20 @@
 #include "internal.h"
 #include "ras/ras_event.h"
 
+/*
+ * 在以下使用sysctl_memory_failure_early_kill:
+ *   - kernel/sysctl.c|3072| <<global>> .data = &sysctl_memory_failure_early_kill,
+ *   - kernel/sysctl.c|3073| <<global>> .maxlen = sizeof(sysctl_memory_failure_early_kill),
+ *   - mm/memory-failure.c|436| <<find_early_kill_thread>> if (sysctl_memory_failure_early_kill)
+ */
 int sysctl_memory_failure_early_kill __read_mostly = 0;
 
+/*
+ * 在以下使用sysctl_memory_failure_recovery:
+ *   - kernel/sysctl.c|3081| <<global>> .data = &sysctl_memory_failure_recovery,
+ *   - kernel/sysctl.c|3082| <<global>> .maxlen = sizeof(sysctl_memory_failure_recovery),
+ *   - mm/memory-failure.c|1451| <<memory_failure>> if (!sysctl_memory_failure_recovery)
+ */
 int sysctl_memory_failure_recovery __read_mostly = 1;
 
 atomic_long_t num_poisoned_pages __read_mostly = ATOMIC_LONG_INIT(0);
@@ -1437,6 +1449,19 @@ static int memory_failure_dev_pagemap(unsigned long pfn, int flags,
  * Must run in process context (e.g. a work queue) with interrupts
  * enabled and no spinlocks hold.
  */
+/*
+ * called by:
+ *   - arch/parisc/kernel/pdt.c|331| <<pdt_mainloop>> memory_failure(pde >> PAGE_SHIFT, 0);
+ *   - arch/powerpc/kernel/mce.c|313| <<machine_process_ue_event>> memory_failure(pfn, 0);
+ *   - arch/powerpc/platforms/powernv/opal-memory-errors.c|50| <<handle_memory_error_event>> memory_failure(paddr_start >> PAGE_SHIFT, 0);
+ *   - arch/x86/kernel/cpu/mce/core.c|649| <<uc_decode_notifier>> if (!memory_failure(pfn, 0)) {
+ *   - arch/x86/kernel/cpu/mce/core.c|1266| <<kill_me_maybe>> if (!memory_failure(p->mce_addr >> PAGE_SHIFT, flags) &&
+ *   - arch/x86/kernel/cpu/mce/core.c|1455| <<memory_failure>> int memory_failure(unsigned long pfn, int flags)
+ *   - drivers/base/memory.c|549| <<hard_offline_page_store>> ret = memory_failure(pfn, 0);
+ *   - mm/hwpoison-inject.c|51| <<hwpoison_inject>> return memory_failure(pfn, 0);
+ *   - mm/madvise.c|911| <<madvise_inject_error>> ret = memory_failure(pfn, MF_COUNT_INCREASED);
+ *   - mm/memory-failure.c|1694| <<memory_failure_work_func>> memory_failure(entry.pfn, entry.flags);
+ */
 int memory_failure(unsigned long pfn, int flags)
 {
 	struct page *p;
diff --git a/net/core/dev.c b/net/core/dev.c
index ef8cf7619baf..08e6abc40bb3 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -10438,6 +10438,10 @@ int netdev_unregister_timeout_secs __read_mostly = 10;
  * We can get stuck here if buggy protocols don't correctly
  * call dev_put.
  */
+/*
+ * called by:
+ *   - net/core/dev.c|10563| <<netdev_run_todo>> netdev_wait_allrefs(dev);
+ */
 static void netdev_wait_allrefs(struct net_device *dev)
 {
 	unsigned long rebroadcast_time, warning_time;
@@ -10519,6 +10523,10 @@ static void netdev_wait_allrefs(struct net_device *dev)
  * We must not return until all unregister events added during
  * the interval the lock was held have been completed.
  */
+/*
+ * called by:
+ *   - net/core/rtnetlink.c|112| <<rtnl_unlock>> netdev_run_todo();
+ */
 void netdev_run_todo(void)
 {
 	struct list_head list;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 46fb042837d2..be991d97d91b 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -1638,6 +1638,10 @@ EXPORT_SYMBOL_GPL(kvm_get_dirty_log);
  * exiting to userspace will be logged for the next call.
  *
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1739| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log_protect(kvm, log);
+ */
 static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 {
 	struct kvm_memslots *slots;
@@ -2990,6 +2994,18 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 					++vcpu->stat.halt_poll_invalid;
 				goto out;
 			}
+			/*
+			 * 这里少了一个cpu_relax()
+			 *
+			 * Sean Christopherson suggested as below:
+			 *
+			 * "Rather than disallowing halt-polling entirely, on x86 it should be
+			 * sufficient to simply have the hardware thread yield to its sibling(s)
+			 * via PAUSE.  It probably won't get back all performance, but I would
+			 * expect it to be close.
+			 * This compiles on all KVM architectures, and AFAICT the intended usage
+			 * of cpu_relax() is identical for all architectures."
+			 */
 			poll_end = cur = ktime_get();
 		} while (kvm_vcpu_can_poll(cur, stop));
 	}
@@ -4319,6 +4335,13 @@ static struct file_operations kvm_chardev_ops = {
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * 在以下使用kvm_dev:
+ *   - virt/kvm/kvm_main.c|4951| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|4992| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ *   - virt/kvm/kvm_main.c|5182| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|5223| <<kvm_exit>> misc_deregister(&kvm_dev);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
@@ -5089,6 +5112,17 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2156| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1615| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1037| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5060| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4580| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8230| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
-- 
2.17.1

