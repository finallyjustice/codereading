From 04aad0460049eb3278c44f32a75cfc3e93c86e03 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 27 Feb 2022 22:28:14 -0800
Subject: [PATCH 1/1] linux v5.13

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/s390/kvm/kvm-s390.c                      |   12 +
 arch/x86/include/asm/irq_remapping.h          |    9 +
 arch/x86/include/asm/kvm_host.h               |  611 +++++
 arch/x86/include/asm/msr-index.h              |    8 +
 arch/x86/include/asm/processor.h              |   16 +
 arch/x86/include/asm/pvclock-abi.h            |   51 +
 arch/x86/include/asm/pvclock.h                |   20 +
 arch/x86/include/asm/xen/interface.h          |    7 +
 arch/x86/include/uapi/asm/kvm_para.h          |   13 +
 arch/x86/kernel/apic/io_apic.c                |    4 +
 arch/x86/kernel/apic/probe_64.c               |    5 +
 arch/x86/kernel/apic/vector.c                 |   13 +
 arch/x86/kernel/cpu/common.c                  |   27 +
 arch/x86/kernel/crash.c                       |    6 +
 arch/x86/kernel/irq.c                         |    6 +
 arch/x86/kernel/kvm.c                         |   74 +
 arch/x86/kernel/kvmclock.c                    |   26 +
 arch/x86/kernel/pvclock.c                     |   47 +
 arch/x86/kernel/smpboot.c                     |   51 +
 arch/x86/kernel/tsc.c                         |    6 +
 arch/x86/kvm/cpuid.c                          |  104 +
 arch/x86/kvm/emulate.c                        |   12 +
 arch/x86/kvm/hyperv.c                         |   10 +
 arch/x86/kvm/i8254.c                          |    8 +
 arch/x86/kvm/i8259.c                          |    4 +
 arch/x86/kvm/irq.c                            |   36 +
 arch/x86/kvm/irq.h                            |   23 +
 arch/x86/kvm/irq_comm.c                       |  152 ++
 arch/x86/kvm/kvm_cache_regs.h                 |    5 +
 arch/x86/kvm/lapic.c                          |  755 ++++++
 arch/x86/kvm/lapic.h                          |  237 ++
 arch/x86/kvm/mmu.h                            |   10 +
 arch/x86/kvm/mmu/mmu.c                        |   95 +
 arch/x86/kvm/mmu/page_track.c                 |   27 +
 arch/x86/kvm/pmu.c                            |   11 +
 arch/x86/kvm/svm/nested.c                     |   15 +
 arch/x86/kvm/vmx/capabilities.h               |   13 +
 arch/x86/kvm/vmx/nested.c                     |   16 +
 arch/x86/kvm/vmx/pmu_intel.c                  |    4 +
 arch/x86/kvm/vmx/posted_intr.c                |   51 +
 arch/x86/kvm/vmx/vmcs.h                       |   12 +
 arch/x86/kvm/vmx/vmx.c                        |  514 ++++
 arch/x86/kvm/vmx/vmx.h                        |   63 +
 arch/x86/kvm/vmx/vmx_ops.h                    |   22 +
 arch/x86/kvm/x86.c                            | 2369 +++++++++++++++++
 arch/x86/kvm/x86.h                            |   13 +
 arch/x86/xen/enlighten.c                      |   44 +
 arch/x86/xen/time.c                           |   39 +
 block/blk-exec.c                              |    4 +
 block/blk-mq.c                                |    3 +
 block/genhd.c                                 |   17 +
 block/scsi_ioctl.c                            |    4 +
 drivers/acpi/acpica/nseval.c                  |   12 +
 drivers/acpi/acpica/psparse.c                 |   93 +
 drivers/acpi/acpica/psxface.c                 |    4 +
 drivers/acpi/osl.c                            |   30 +
 drivers/acpi/scan.c                           |   36 +
 drivers/acpi/utils.c                          |   14 +
 drivers/base/base.h                           |   24 +
 drivers/block/virtio_blk.c                    |    9 +
 drivers/block/xen-blkfront.c                  |  279 ++
 drivers/cpuidle/poll_state.c                  |    7 +
 drivers/crypto/virtio/virtio_crypto_algs.c    |    4 +
 drivers/iommu/intel/dmar.c                    |   30 +
 drivers/iommu/intel/irq_remapping.c           |   43 +
 drivers/iommu/iommu.c                         |   78 +
 drivers/misc/pvpanic/pvpanic.c                |   26 +
 drivers/net/tap.c                             |   20 +
 drivers/net/tun.c                             |   10 +
 drivers/net/virtio_net.c                      |   78 +
 drivers/net/xen-netback/rx.c                  |   21 +
 drivers/net/xen-netfront.c                    |   70 +
 drivers/ptp/ptp_clock.c                       |   46 +
 drivers/ptp/ptp_kvm_common.c                  |    7 +
 drivers/scsi/scsi.c                           |    6 +
 drivers/scsi/scsi_error.c                     |   12 +
 drivers/scsi/scsi_lib.c                       |   69 +
 drivers/scsi/scsi_logging.c                   |   10 +
 drivers/scsi/scsi_scan.c                      |   12 +
 drivers/scsi/sd.c                             |    3 +
 drivers/scsi/sym53c8xx_2/sym_hipd.c           |    5 +
 drivers/scsi/virtio_scsi.c                    |  188 ++
 drivers/target/target_core_configfs.c         |   14 +
 drivers/target/target_core_sbc.c              |    6 +
 drivers/target/target_core_spc.c              |    8 +
 drivers/target/target_core_transport.c        |  184 ++
 drivers/vdpa/ifcvf/ifcvf_main.c               |    7 +
 drivers/vdpa/mlx5/net/mlx5_vnet.c             |   13 +
 drivers/vdpa/vdpa.c                           |  116 +
 drivers/vdpa/vdpa_sim/vdpa_sim.c              |   86 +
 drivers/vdpa/vdpa_sim/vdpa_sim_net.c          |   56 +
 drivers/vdpa/virtio_pci/vp_vdpa.c             |    7 +
 drivers/vfio/pci/vfio_pci_intrs.c             |   16 +
 drivers/vhost/iotlb.c                         |   65 +
 drivers/vhost/scsi.c                          |  283 ++
 drivers/vhost/vdpa.c                          |   94 +
 drivers/vhost/vhost.c                         |  154 ++
 drivers/vhost/vhost.h                         |   25 +
 drivers/virtio/virtio.c                       |   76 +
 drivers/virtio/virtio_pci_common.c            |    4 +
 drivers/virtio/virtio_pci_modern.c            |    4 +
 drivers/virtio/virtio_pci_modern_dev.c        |   17 +
 drivers/virtio/virtio_ring.c                  |   45 +
 drivers/virtio/virtio_vdpa.c                  |   22 +
 drivers/xen/xenbus/xenbus_client.c            |   23 +
 drivers/xen/xenbus/xenbus_probe_frontend.c    |   15 +
 fs/configfs/configfs_internal.h               |    8 +
 fs/eventfd.c                                  |   33 +
 include/kvm/iodev.h                           |    6 +
 include/linux/blk_types.h                     |   18 +
 include/linux/intel-iommu.h                   |    5 +
 include/linux/kvm_host.h                      |  138 +
 include/linux/netdevice.h                     |   76 +
 include/linux/timekeeping.h                   |    6 +
 include/linux/vdpa.h                          |   16 +
 include/scsi/scsi.h                           |   10 +
 include/scsi/scsi_cmnd.h                      |   17 +
 include/scsi/scsi_device.h                    |    7 +
 include/target/target_core_base.h             |   16 +
 include/uapi/linux/kvm.h                      |   44 +
 include/uapi/linux/virtio_ids.h               |   27 +
 include/xen/interface/vcpu.h                  |    5 +
 include/xen/xen-ops.h                         |   25 +
 include/xen/xenbus.h                          |   15 +
 kernel/cpu.c                                  |   11 +
 kernel/irq/matrix.c                           |   15 +
 kernel/locking/qspinlock.c                    |   64 +
 kernel/locking/qspinlock_paravirt.h           |  152 ++
 kernel/sched/clock.c                          |   15 +
 kernel/sched/core.c                           |    4 +
 kernel/stop_machine.c                         |  260 ++
 kernel/time/clocksource.c                     |   29 +
 kernel/time/timekeeping.c                     |   11 +
 mm/memory-failure.c                           |   25 +
 net/core/dev.c                                |  104 +
 net/sched/sch_generic.c                       |    4 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |    9 +
 .../testing/selftests/kvm/lib/x86_64/ucall.c  |   35 +
 tools/testing/selftests/kvm/steal_time.c      |  149 ++
 .../selftests/kvm/x86_64/tsc_msrs_test.c      |   25 +
 .../kvm/x86_64/vmx_tsc_adjust_test.c          |   54 +
 virt/kvm/async_pf.c                           |   36 +
 virt/kvm/coalesced_mmio.c                     |    4 +
 virt/kvm/eventfd.c                            |  136 +
 virt/kvm/irqchip.c                            |   93 +
 virt/kvm/kvm_main.c                           |  916 +++++++
 virt/kvm/vfio.c                               |   16 +
 virt/lib/irqbypass.c                          |   19 +
 148 files changed, 10948 insertions(+)

diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 1296fc10f80c..991e3d709e41 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -177,6 +177,13 @@ module_param(hpage, int, 0444);
 MODULE_PARM_DESC(hpage, "1m huge page backing support");
 
 /* maximum percentage of steal time for polling.  >100 is treated like 100 */
+/*
+ * 在以下使用halt_poll_max_steal:
+ *   - arch/s390/kvm/kvm-s390.c|180| <<global>> static u8 halt_poll_max_steal = 10;
+ *   - arch/s390/kvm/kvm-s390.c|181| <<global>> module_param(halt_poll_max_steal, byte, 0644);
+ *   - arch/s390/kvm/kvm-s390.c|182| <<global>> MODULE_PARM_DESC(halt_poll_max_steal, "Maximum percentage of steal time to allow polling");
+ *   - arch/s390/kvm/kvm-s390.c|3403| <<kvm_arch_no_poll>> halt_poll_max_steal) {
+ */
 static u8 halt_poll_max_steal = 10;
 module_param(halt_poll_max_steal, byte, 0644);
 MODULE_PARM_DESC(halt_poll_max_steal, "Maximum percentage of steal time to allow polling");
@@ -3396,6 +3403,11 @@ static void kvm_gmap_notifier(struct gmap *gmap, unsigned long start,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3092| <<kvm_vcpu_block>> if (vcpu->halt_poll_ns && !kvm_arch_no_poll(vcpu)) {
+ *   - virt/kvm/kvm_main.c|3142| <<kvm_vcpu_block>> if (!kvm_arch_no_poll(vcpu)) {
+ */
 bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 {
 	/* do not poll with more than halt_poll_max_steal percent of steal time */
diff --git a/arch/x86/include/asm/irq_remapping.h b/arch/x86/include/asm/irq_remapping.h
index 7cc49432187f..34add2ef9c8c 100644
--- a/arch/x86/include/asm/irq_remapping.h
+++ b/arch/x86/include/asm/irq_remapping.h
@@ -27,6 +27,15 @@ enum {
 };
 
 struct vcpu_data {
+	/*
+	 * 在以下使用vcpu_data->pi_desc_addr:
+	 *   - arch/x86/kvm/svm/avic.c|797| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|898| <<svm_update_pi_irte>> vcpu_info.pi_desc_addr, set);
+	 *   - rch/x86/kvm/vmx/posted_intr.c|351| <<pi_update_irte>> vcpu_info.pi_desc_addr = __pa(&to_vmx(vcpu)->pi_desc);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|355| <<pi_update_irte>> vcpu_info.vector, vcpu_info.pi_desc_addr, set);
+	 *   - drivers/iommu/intel/irq_remapping.c|1249| <<intel_ir_set_vcpu_affinity>> irte_pi.pda_l = (vcpu_pi_info->pi_desc_addr >>
+	 *   - drivers/iommu/intel/irq_remapping.c|1251| <<intel_ir_set_vcpu_affinity>> irte_pi.pda_h = (vcpu_pi_info->pi_desc_addr >> 32) &
+	 */
 	u64 pi_desc_addr;	/* Physical address of PI Descriptor */
 	u32 vector;		/* Guest vector of the interrupt */
 };
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9c7ced0e3171..48c303cf1133 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -58,6 +58,26 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2831| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2993| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3084| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3093| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4315| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4952| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6096| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|8118| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8179| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9360| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|9645| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10762| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|333| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|350| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * kvm_guest_time_update()
+ * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
@@ -67,11 +87,59 @@
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1270| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2158| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2336| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8243| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9288| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|10748| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 调用kvm_gen_update_masterclock()
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
+/*
+ * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+ *   - arch/x86/kvm/x86.c|2742| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|2767| <<kvm_gen_update_masterclock>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *   - arch/x86/kvm/x86.c|8053| <<kvm_hyperv_tsc_notifier>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *
+ * 给所有vCPU发送KVM_REQ_MCLOCK_INPROGRESS,将它们踢出Guest模式,该请求没有Handler,因此vCPU无法再进入Guest
+ */
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_SCAN_IOAPIC:
+ *   - arch/x86/kvm/hyperv.c|135| <<synic_set_sint>> kvm_make_request(KVM_REQ_SCAN_IOAPIC, hv_synic_to_vcpu(synic));
+ *   - arch/x86/kvm/x86.c|10889| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC,
+ *   - arch/x86/kvm/x86.c|10897| <<kvm_make_scan_ioapic_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
+ *   - arch/x86/kvm/x86.c|11141| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_SCAN_IOAPIC, vcpu))
+ *
+ * 在vcpu_enter_guest()调用vcpu_scan_ioapic()
+ */
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2179| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4339| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9358| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * kvm_gen_kvmclock_update()
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+ * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -87,6 +155,13 @@
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
 #define KVM_REQ_HV_TLB_FLUSH \
 	KVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_APF_READY:
+ *   - arch/x86/kvm/lapic.c|493| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|2675| <<kvm_lapic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *   - arch/x86/kvm/x86.c|11096| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+ *   - arch/x86/kvm/x86.c|13420| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ */
 #define KVM_REQ_APF_READY		KVM_ARCH_REQ(28)
 #define KVM_REQ_MSR_FILTER_CHANGED	KVM_ARCH_REQ(29)
 #define KVM_REQ_UPDATE_CPU_DIRTY_LOGGING \
@@ -581,6 +656,25 @@ struct kvm_vcpu_arch {
 	u32 pkru;
 	u32 hflags;
 	u64 efer;
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|2724| <<kvm_lapic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|2953| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/svm/svm.c|1306| <<svm_vcpu_reset>> vcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE |
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|138| <<kvm_update_cpuid_runtime>> vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|2648| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|2713| <<kvm_lapic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|2758| <<kvm_lapic_set_base>> apic->base_address = apic->vcpu->arch.apic_base &
+	 *   - arch/x86/kvm/lapic.c|2829| <<kvm_lapic_reset>> vcpu->arch.apic_base | MSR_IA32_APICBASE_BSP);
+	 *   - arch/x86/kvm/lapic.c|3086| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/lapic.h|327| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|405| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/svm/svm.c|1309| <<svm_vcpu_reset>> vcpu->arch.apic_base |= MSR_IA32_APICBASE_BSP;
+	 *   - arch/x86/kvm/vmx/nested.c|852| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|573| <<kvm_get_apic_base>> return vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12628| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	bool apicv_active;
@@ -677,15 +771,68 @@ struct kvm_vcpu_arch {
 
 	int halt_request; /* real mode on Intel only */
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpuid_nent:
+	 *   - arch/x86/kvm/cpuid.c|227| <<cpuid_fix_nx_cap>> for (i = 0; i < vcpu->arch.cpuid_nent; ++i) {
+	 *   - arch/x86/kvm/cpuid.c|308| <<kvm_vcpu_ioctl_set_cpuid>> vcpu->arch.cpuid_nent = cpuid->nent;
+	 *   - arch/x86/kvm/cpuid.c|344| <<kvm_vcpu_ioctl_set_cpuid2>> vcpu->arch.cpuid_nent = cpuid->nent;
+	 *   - arch/x86/kvm/cpuid.c|359| <<kvm_vcpu_ioctl_get_cpuid2>> if (cpuid->nent < vcpu->arch.cpuid_nent)
+	 *   - arch/x86/kvm/cpuid.c|363| <<kvm_vcpu_ioctl_get_cpuid2>> vcpu->arch.cpuid_nent * sizeof(struct kvm_cpuid_entry2)))
+	 *   - arch/x86/kvm/cpuid.c|368| <<kvm_vcpu_ioctl_get_cpuid2>> cpuid->nent = vcpu->arch.cpuid_nent;
+	 *   - arch/x86/kvm/cpuid.c|1163| <<kvm_find_cpuid_entry>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 */
 	int cpuid_nent;
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpuid_entries:
+	 *   - arch/x86/kvm/cpuid.c|228| <<cpuid_fix_nx_cap>> e = &vcpu->arch.cpuid_entries[i];
+	 *   - arch/x86/kvm/cpuid.c|306| <<kvm_vcpu_ioctl_set_cpuid>> kvfree(vcpu->arch.cpuid_entries);
+	 *   - arch/x86/kvm/cpuid.c|307| <<kvm_vcpu_ioctl_set_cpuid>> vcpu->arch.cpuid_entries = e2;
+	 *   - arch/x86/kvm/cpuid.c|342| <<kvm_vcpu_ioctl_set_cpuid2>> kvfree(vcpu->arch.cpuid_entries);
+	 *   - arch/x86/kvm/cpuid.c|343| <<kvm_vcpu_ioctl_set_cpuid2>> vcpu->arch.cpuid_entries = e2;
+	 *   - arch/x86/kvm/cpuid.c|362| <<kvm_vcpu_ioctl_get_cpuid2>> if (copy_to_user(entries, vcpu->arch.cpuid_entries,
+	 *   - arch/x86/kvm/cpuid.c|1163| <<kvm_find_cpuid_entry>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_destroy>> kvfree(vcpu->arch.cpuid_entries);
+	 */
 	struct kvm_cpuid_entry2 *cpuid_entries;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->reserved_gpa_bits:
+	 *   - arch/x86/kvm/cpuid.c|210| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.reserved_gpa_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu);
+	 *   - arch/x86/kvm/cpuid.h|43| <<kvm_vcpu_is_legal_gpa>> return !(gpa & vcpu->arch.reserved_gpa_bits);
+	 *   - arch/x86/kvm/mmu/mmu.c|4117| <<reset_rsvds_bits_mask>> vcpu->arch.reserved_gpa_bits,
+	 *   - arch/x86/kvm/mmu/mmu.c|4160| <<reset_rsvds_bits_mask_ept>> vcpu->arch.reserved_gpa_bits, execonly);
+	 *   - arch/x86/kvm/svm/svm.c|3974| <<svm_vcpu_after_set_cpuid>> vcpu->arch.reserved_gpa_bits &= ~(1UL << (best->ebx & 0x3f));
+	 *   - arch/x86/kvm/x86.c|948| <<pdptr_rsvd_bits>> return vcpu->arch.reserved_gpa_bits | rsvd_bits(5, 8) | rsvd_bits(1, 2);
+	 *   - arch/x86/kvm/x86.c|12242| <<kvm_arch_vcpu_create>> vcpu->arch.reserved_gpa_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu);
+	 */
 	u64 reserved_gpa_bits;
 	int maxphyaddr;
 	int max_tdp_level;
 
 	/* emulate context */
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->emulate_ctxt:
+	 *   - arch/x86/kvm/svm/svm.c|2150| <<svm_instr_opcode>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/trace.h|778| <<static_call>> __entry->len = vcpu->arch.emulate_ctxt->fetch.ptr
+	 *   - arch/x86/kvm/trace.h|779| <<static_call>> - vcpu->arch.emulate_ctxt->fetch.data;
+	 *   - arch/x86/kvm/trace.h|780| <<static_call>> __entry->rip = vcpu->arch.emulate_ctxt->_eip - __entry->len;
+	 *   - arch/x86/kvm/trace.h|782| <<static_call>> vcpu->arch.emulate_ctxt->fetch.data,
+	 *   - arch/x86/kvm/trace.h|784| <<static_call>> __entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt->mode);
+	 *   - arch/x86/kvm/x86.c|6512| <<emulator_read_write_onepage>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7219| <<inject_emulated_exception>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7243| <<alloc_emulate_ctxt>> vcpu->arch.emulate_ctxt = ctxt;
+	 *   - arch/x86/kvm/x86.c|7250| <<init_emulate_ctxt>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7280| <<kvm_inject_realmode_interrupt>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7601| <<x86_decode_emulated_instruction>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7627| <<x86_emulate_instruction>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|9789| <<__get_regs>> emulator_writeback_register_cache(vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|9981| <<kvm_task_switch>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|10417| <<kvm_arch_vcpu_create>> kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|10462| <<kvm_arch_vcpu_destroy>> kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|11939| <<kvm_sev_es_outs>> ret = emulator_pio_out_emulated(vcpu->arch.emulate_ctxt, size, port,
+	 *   - arch/x86/kvm/x86.c|11954| <<kvm_sev_es_ins>> ret = emulator_pio_in_emulated(vcpu->arch.emulate_ctxt, size, port,
+	 */
 	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
@@ -693,10 +840,34 @@ struct kvm_vcpu_arch {
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3506| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3510| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	unsigned int hw_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2587| <<kvm_write_system_time>> &vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|4297| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
+	 */
 	struct gfn_to_hva_cache pv_time;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	bool pv_time_enabled;
 	/* set guest stopped flag in pvclock flags field */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|2765| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|4767| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	bool pvclock_set_guest_stopped_request;
 
 	struct {
@@ -706,21 +877,162 @@ struct kvm_vcpu_arch {
 		struct gfn_to_pfn_cache cache;
 	} st;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|526| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2357| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2363| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2482| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3289| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3591| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	u64 l1_tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3358| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset += vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4467| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|23| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|791| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|792| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2535| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4503| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/vmx.c|1991| <<vmx_write_l1_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.tsc_offset - g_tsc_offset, offset);
+	 *   - arch/x86/kvm/x86.c|4478| <<kvm_get_msr_common(MSR_IA32_TSC)>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset : vcpu->arch.tsc_offset;
+	 */
 	u64 tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	u64 last_guest_tsc;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|5282| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+	 *   - arch/x86/kvm/x86.c|11850| <<kvm_arch_hardware_enable>> vcpu->arch.last_host_tsc = local_tsc;
+	 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|5211| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+	 *   - arch/x86/kvm/x86.c|5212| <<kvm_arch_vcpu_load>> rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|11798| <<kvm_arch_hardware_enable>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+	 *   - arch/x86/kvm/x86.c|11800| <<kvm_arch_hardware_enable>> if (vcpu->arch.last_host_tsc > max_tsc)
+	 *   - arch/x86/kvm/x86.c|11801| <<kvm_arch_hardware_enable>> max_tsc = vcpu->arch.last_host_tsc;
+	 */
 	u64 last_host_tsc;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_offset_adjustment:
+	 *   - arch/x86/kvm/x86.c|5199| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+	 *   - arch/x86/kvm/x86.c|5200| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+	 *   - arch/x86/kvm/x86.c|5201| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_offset_adjustment = 0;
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_hardware_enable>> vcpu->arch.tsc_offset_adjustment += delta_cyc;
+	 */
 	u64 tsc_offset_adjustment;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2319| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2517| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 this_tsc_nsec;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2657| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 this_tsc_write;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|3048| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|3103| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2454| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_shift,
+	 *   - arch/x86/kvm/x86.c|2531| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|361| <<nsec_to_cycles>> vcpu->arch.virtual_tsc_shift);
+	 */
 	s8 virtual_tsc_shift;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2530| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_mult,
+	 *   - arch/x86/kvm/x86.h|360| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+	 */
 	u32 virtual_tsc_mult;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1552| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1570| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1590| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1595| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1697| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2113| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2118| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|3903| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2434| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2445| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2463| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2496| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_ioctl>> r = vcpu->arch.virtual_tsc_khz;
+	 *
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	u32 virtual_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+	 *   - arch/x86/kvm/x86.c|4571| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+	 *   - arch/x86/kvm/x86.c|4574| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> vcpu->arch.ia32_tsc_adjust_msr = data;
+	 *   - arch/x86/kvm/x86.c|4602| <<kvm_set_msr_common(MSR_IA32_TSC)>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+	 *   - arch/x86/kvm/x86.c|4963| <<kvm_get_msr_common(MSR_IA32_TSC_ADJUST)>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+	 */
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	u64 tsc_scaling_ratio;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
@@ -731,6 +1043,22 @@ struct kvm_vcpu_arch {
 	struct kvm_mtrr mtrr_state;
 	u64 pat;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->switch_db_regs:
+	 *   - arch/x86/kvm/svm/svm.c|1868| <<svm_sync_dirty_debug_regs>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/svm/svm.c|2563| <<dr_interception>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/svm/svm.c|3769| <<svm_vcpu_run>> if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))
+	 *   - arch/x86/kvm/vmx/vmx.c|4400| <<vmx_exec_control>> if (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)
+	 *   - arch/x86/kvm/vmx/vmx.c|5381| <<handle_dr>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|5409| <<vmx_sync_dirty_debug_regs>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/x86.c|1154| <<kvm_update_dr0123>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_RELOAD;
+	 *   - arch/x86/kvm/x86.c|1167| <<kvm_update_dr7>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_BP_ENABLED;
+	 *   - arch/x86/kvm/x86.c|1169| <<kvm_update_dr7>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_BP_ENABLED;
+	 *   - arch/x86/kvm/x86.c|9362| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.switch_db_regs)) {
+	 *   - arch/x86/kvm/x86.c|9369| <<vcpu_enter_guest>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
+	 *   - arch/x86/kvm/x86.c|9392| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)) {
+	 *   - arch/x86/kvm/x86.c|9397| <<vcpu_enter_guest>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
+	 */
 	unsigned switch_db_regs;
 	unsigned long db[KVM_NR_DB_REGS];
 	unsigned long dr6;
@@ -767,6 +1095,14 @@ struct kvm_vcpu_arch {
 	unsigned long last_retry_addr;
 
 	struct {
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.halted:
+		 *   - arch/x86/kvm/x86.c|10922| <<vcpu_enter_guest>> vcpu->arch.apf.halted = true;
+		 *   - arch/x86/kvm/x86.c|11222| <<vcpu_block>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|11238| <<kvm_vcpu_running>> !vcpu->arch.apf.halted);
+		 *   - arch/x86/kvm/x86.c|12230| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|13270| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+		 */
 		bool halted;
 		gfn_t gfns[ASYNC_PF_PER_VCPU];
 		struct gfn_to_hva_cache data;
@@ -792,6 +1128,13 @@ struct kvm_vcpu_arch {
 		struct gfn_to_hva_cache data;
 	} pv_eoi;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->msr_kvm_poll_control:
+	 *   - arch/x86/kvm/x86.c|4946| <<kvm_set_msr_common>> vcpu->arch.msr_kvm_poll_control = data;
+	 *   - arch/x86/kvm/x86.c|5277| <<kvm_get_msr_common>> msr_info->data = vcpu->arch.msr_kvm_poll_control;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_postcreate>> vcpu->arch.msr_kvm_poll_control = 1;
+	 *   - arch/x86/kvm/x86.c|13662| <<kvm_arch_no_poll>> return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
+	 */
 	u64 msr_kvm_poll_control;
 
 	/*
@@ -820,6 +1163,15 @@ struct kvm_vcpu_arch {
 	} pv;
 
 	int pending_ioapic_eoi;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|37| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_get_extint>> int vector = v->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|123| <<kvm_cpu_get_extint>> v->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5958| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5961| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12371| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	int pending_external_vector;
 
 	/* be preempted when it's in kernel-mode(cpl=0) */
@@ -860,6 +1212,17 @@ struct kvm_lpage_info {
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	/*
+	 * 在以下使用kvm_arch_memory_slot->gfn_track:
+	 *   - arch/x86/kvm/mmu/page_track.c|26| <<kvm_page_track_free_memslot>> kvfree(slot->arch.gfn_track[i]);
+	 *   - arch/x86/kvm/mmu/page_track.c|27| <<kvm_page_track_free_memslot>> slot->arch.gfn_track[i] = NULL;
+	 *   - arch/x86/kvm/mmu/page_track.c|37| <<kvm_page_track_create_memslot>> slot->arch.gfn_track[i] =
+	 *   - arch/x86/kvm/mmu/page_track.c|38| <<kvm_page_track_create_memslot>> kvcalloc(npages, sizeof(*slot->arch.gfn_track[i]),
+	 *   - arch/x86/kvm/mmu/page_track.c|40| <<kvm_page_track_create_memslot>> if (!slot->arch.gfn_track[i])
+	 *   - arch/x86/kvm/mmu/page_track.c|66| <<update_gfn_track>> val = slot->arch.gfn_track[mode][index];
+	 *   - arch/x86/kvm/mmu/page_track.c|71| <<update_gfn_track>> slot->arch.gfn_track[mode][index] += count;
+	 *   - arch/x86/kvm/mmu/page_track.c|155| <<kvm_page_track_is_active>> return !!READ_ONCE(slot->arch.gfn_track[mode][index]);
+	 */
 	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
 };
 
@@ -999,36 +1362,230 @@ struct kvm_arch {
 	struct kvm_pit *vpit;
 	atomic_t vapics_in_nmi_mode;
 	struct mutex apic_map_lock;
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|301| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|303| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1032| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1074| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1211| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10053| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|12612| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	struct kvm_apic_map __rcu *apic_map;
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|218| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|226| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|300| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|323| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|334| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|340| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|346| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|362| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|2376| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_t apic_map_dirty;
 
 	bool apic_access_page_done;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10214| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10222| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10225| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10876| <<kvm_request_apicv_update>> old = READ_ONCE(kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10885| <<kvm_request_apicv_update>> old = cmpxchg(&kvm->arch.apicv_inhibit_reasons, expected, new);
+	 */
 	unsigned long apicv_inhibit_reasons;
 
+	/*
+	 * 似乎在以下使用kvm_arch->wall_clock (gpa_t):
+	 *   - arch/x86/kvm/x86.c|3690| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+	 *   - arch/x86/kvm/x86.c|3697| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+	 *   - arch/x86/kvm/x86.c|4026| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+	 *   - arch/x86/kvm/x86.c|4032| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+	 */
 	gpa_t wall_clock;
 
+	/*
+	 * 在以下使用kvm_arch->mwait_in_guest:
+	 *   - arch/x86/kvm/x86.c|7106| <<kvm_vm_ioctl_enable_cap>> kvm->arch.mwait_in_guest = true;
+	 *   - arch/x86/kvm/x86.h|380| <<kvm_mwait_in_guest>> return kvm->arch.mwait_in_guest;
+	 *
+	 * Executing the HLT instruction on a idle logical processor puts the
+	 * targeted processor in a non-execution state. This requires another
+	 * processor (when posting work for the halted logical processor) to
+	 * wake up the halted processor using an inter-processor interrupt. The
+	 * posting and servicing of such an interrupt introduces a delay in the
+	 * servicing of new work requests.
+	 *
+	 * MONITOR sets up an effective address range that is monitored for
+	 * write-to-memory activities; MWAIT places the processor in an
+	 * optimized state (this may vary between different implementations)
+	 * until a write to the monitored address range occurs.
+	 */
 	bool mwait_in_guest;
 	bool hlt_in_guest;
 	bool pause_in_guest;
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 *
+	 * get_kvmclock_base_ns() + ka->kvmclock_offset应该就是当前的VM的启动了的ns
+	 */
 	s64 kvmclock_offset;
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2542| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|2599| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_write = data;
+	 *   - arch/x86/kvm/x86.c|11075| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2562| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2600| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 */
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2889| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2918| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2890| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = data;
+	 *   - arch/x86/kvm/x86.c|2919| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 cur_tsc_write;
+	/*
+	 * cur_tsc_offset = 18443709364331608385,
+	 *
+	 * crash> eval 18443709364331608385
+	 * hexadecimal: fff537f2a987f941
+	 *     decimal: 18443709364331608385  (-3034709377943231)
+	 *       octal: 1777651577125141774501
+	 *      binary: 1111111111110101001101111111001010101001100001111111100101000001
+	 *
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu0/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu1/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu2/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu3/tsc-offset
+	 * -3034709377943231
+	 *
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 */
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 *
+	 * 测试的例子是cur_tsc_generation = 1
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_synchronize_tsc>> spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2516| <<kvm_synchronize_tsc>> spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2745| <<kvm_gen_update_masterclock>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2747| <<kvm_gen_update_masterclock>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2765| <<get_kvmclock_ns>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2767| <<get_kvmclock_ns>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2773| <<get_kvmclock_ns>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2877| <<kvm_guest_time_update>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2883| <<kvm_guest_time_update>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5985| <<kvm_arch_vm_ioctl>> spin_lock_irq(&ka->pvclock_gtod_sync_lock);
+	 *   - arch/x86/kvm/x86.c|5991| <<kvm_arch_vm_ioctl>> spin_unlock_irq(&ka->pvclock_gtod_sync_lock);
+	 *   - arch/x86/kvm/x86.c|8010| <<kvm_hyperv_tsc_notifier>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|8012| <<kvm_hyperv_tsc_notifier>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|10850| <<kvm_arch_init_vm>> spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
+	 */
 	spinlock_t pvclock_gtod_sync_lock;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched 
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|2780| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|2853| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2984| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6090| <<kvm_arch_vm_ioctl>> now_ns = ka->master_kernel_ns;
+	 *
+	 * 猜测是host内核启动了的ns
+	 */
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|2945| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3079| <<get_kvmclock_ns>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3216| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *
+	 * 猜测是host内核启动了的cycle
+	 */
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_fn:
+	 *   - arch/x86/kvm/x86.c|3703| <<kvmclock_update_fn>> kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3737| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3764| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11787| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11835| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3754| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3765| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11435| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11788| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
@@ -1043,7 +1600,22 @@ struct kvm_arch {
 	int audit_point;
 	#endif
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2950| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|11175| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 */
 	bool backwards_tsc_observed;
+	/*
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 *
+	 * 如果写入的是MSR_KVM_SYSTEM_TIME,表明Guest使用的是旧版kvmclock,不支持Master Clock模式,
+	 * 此时要设置kvm->arch.boot_vcpu_runs_old_kvmclock = 1,并对当前vCPU(即vCPU0)发送一个KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 这最终会导致kvm->arch.use_master_clock = 0
+	 */
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
 
@@ -1157,6 +1729,11 @@ struct kvm_vcpu_stat {
 	u64 halt_successful_poll;
 	u64 halt_attempted_poll;
 	u64 halt_poll_invalid;
+	/*
+	 * x86在以下使用kvm_vcpu_stat->halt_wakeup:
+	 *   - arch/x86/kvm/x86.c|368| <<global>> VCPU_STAT("halt_wakeup", halt_wakeup),
+	 *   - virt/kvm/kvm_main.c|3205| <<kvm_vcpu_wake_up>> ++vcpu->stat.halt_wakeup;
+	 */
 	u64 halt_wakeup;
 	u64 request_irq_exits;
 	u64 irq_exits;
@@ -1166,6 +1743,12 @@ struct kvm_vcpu_stat {
 	u64 insn_emulation_fail;
 	u64 hypercalls;
 	u64 irq_injections;
+	/*
+	 * 在以下使用kvm_vcpu_stat->nmi_injections:
+	 *   - arch/x86/kvm/x86.c|377| <<global>> VCPU_STAT("nmi_injections", nmi_injections),
+	 *   - arch/x86/kvm/svm/svm.c|3364| <<svm_inject_nmi>> ++vcpu->stat.nmi_injections;
+	 *   - arch/x86/kvm/vmx/vmx.c|4849| <<vmx_inject_nmi>> ++vcpu->stat.nmi_injections;
+	 */
 	u64 nmi_injections;
 	u64 req_event;
 	u64 halt_poll_success_ns;
@@ -1178,6 +1761,18 @@ struct kvm_vcpu_stat {
 struct x86_instruction_info;
 
 struct msr_data {
+	/*
+	 * 在以下设置msr_data->host_initiated:
+	 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+	 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+	 *
+	 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+	 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+	 * msr_info->host_initiated就为false.
+	 */
 	bool host_initiated;
 	u32 index;
 	u64 data;
@@ -1550,6 +2145,14 @@ extern u64 kvm_mce_cap_supported;
  */
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
+/*
+ * 在以下使用EMULTYPE_SKIP:
+ *   - arch/x86/kvm/svm/svm.c|347| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/vmx/vmx.c|1816| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/x86.c|7315| <<handle_emulation_failure>> if (emulation_type & EMULTYPE_SKIP) {
+ *   - arch/x86/kvm/x86.c|7610| <<x86_decode_emulated_instruction>> if (!(emulation_type & EMULTYPE_SKIP) &&
+ *   - arch/x86/kvm/x86.c|7683| <<x86_emulate_instruction>> if (emulation_type & EMULTYPE_SKIP) {
+ */
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_ALLOW_RETRY_PF	    (1 << 3)
 #define EMULTYPE_TRAP_UD_FORCED	    (1 << 4)
@@ -1835,11 +2438,19 @@ static inline bool kvm_irq_is_postable(struct kvm_lapic_irq *irq)
 
 static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 似乎intel没有
+	 * amd是svm_vcpu_blocking()
+	 */
 	static_call_cond(kvm_x86_vcpu_blocking)(vcpu);
 }
 
 static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 似乎intel没有
+	 * amd是svm_vcpu_unblocking()
+	 */
 	static_call_cond(kvm_x86_vcpu_unblocking)(vcpu);
 }
 
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 211ba3375ee9..c67004588857 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -126,6 +126,14 @@
 
 #define MSR_IA32_TSX_CTRL		0x00000122
 #define TSX_CTRL_RTM_DISABLE		BIT(0)	/* Disable RTM feature */
+/*
+ * 在以下使用TSX_CTRL_CPUID_CLEAR:
+ *   - arch/x86/kernel/cpu/tsx.c|37| <<tsx_disable>> tsx |= TSX_CTRL_CPUID_CLEAR;
+ *   - arch/x86/kernel/cpu/tsx.c|56| <<tsx_enable>> tsx &= ~TSX_CTRL_CPUID_CLEAR;
+ *   - arch/x86/kvm/cpuid.c|1214| <<kvm_cpuid>> (data & TSX_CTRL_CPUID_CLEAR))
+ *   - arch/x86/kvm/vmx/vmx.c|2344| <<vmx_set_msr>> if (data & ~(TSX_CTRL_RTM_DISABLE | TSX_CTRL_CPUID_CLEAR))
+ *   - arch/x86/kvm/vmx/vmx.c|7110| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].mask = ~(u64)TSX_CTRL_CPUID_CLEAR;
+ */
 #define TSX_CTRL_CPUID_CLEAR		BIT(1)	/* Disable TSX enumeration */
 
 /* SRBDS support */
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 556b2b17c3e2..8e0e73046c9e 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -208,6 +208,22 @@ static inline int have_cpuid_p(void)
 	return 1;
 }
 #endif
+/*
+ * called by:
+ *   - arch/x86/kernel/paravirt.c|246| <<global>> .cpu.cpuid = native_cpuid,
+ *   - arch/x86/include/asm/microcode.h|108| <<x86_cpuid_vendor>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/include/asm/microcode.h|124| <<x86_cpuid_family>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/include/asm/processor.h|229| <<native_cpuid_reg>> native_cpuid(&eax, &ebx, &ecx, &edx); \
+ *   - arch/x86/include/asm/processor.h|569| <<__cpuid>> #define __cpuid native_cpuid
+ *   - arch/x86/kernel/cpu/microcode/intel.c|356| <<collect_cpu_info_early>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/kernel/cpu/microcode/intel.c|464| <<load_builtin_intel_microcode>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/kernel/smpboot.c|1751| <<mwait_play_dead>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/mm/mem_encrypt_identity.c|500| <<sme_enable>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/mm/mem_encrypt_identity.c|517| <<sme_enable>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/mm/mem_encrypt_identity.c|541| <<sme_enable>> native_cpuid(&eax, &ebx, &ecx, &edx);
+ *   - arch/x86/xen/enlighten_pv.c|233| <<xen_check_mwait>> native_cpuid(&ax, &bx, &cx, &dx);
+ *   - arch/x86/xen/enlighten_pv.c|250| <<xen_check_mwait>> native_cpuid(&ax, &bx, &cx, &dx);
+ */
 static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
 				unsigned int *ecx, unsigned int *edx)
 {
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226efe3e..63f1da735cbe 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -23,12 +23,54 @@
  * and after reading them.
  */
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ *
+ *
+ * 在kvm vm上测试的时候, 下面的都不变
+ * system_time=23867275
+ * tsc_timestamp=206742471, version=4, tsc_to_system_mul=2532092704, tsc_shift=255
+ * kvm_clock_read()一直在变
+ */
 struct pvclock_vcpu_time_info {
+	/*
+	 * 奇数表示Host正在修改,偶数表示已修改完毕
+	 */
 	u32   version;
 	u32   pad0;
+	/*
+	 * 更新pvti时,vCPU的vTSC 
+	 */
 	u64   tsc_timestamp;
+	/*
+	 * 当dmesg是14.903088的时候,
+	 * kvm_clock_read()返回15169858774 (15.169858774)
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	u64   system_time;
+	/*
+	 * 用于将tsc转换为nsec
+	 */
 	u32   tsc_to_system_mul;
+	/*
+	 * 用于将tsc转换为nsec
+	 */
 	s8    tsc_shift;
 	u8    flags;
 	u8    pad[2];
@@ -41,6 +83,15 @@ struct pvclock_wall_clock {
 } __attribute__((__packed__));
 
 #define PVCLOCK_TSC_STABLE_BIT	(1 << 0)
+/*
+ * 在以下使用PVCLOCK_GUEST_STOPPED:
+ *   - arch/x86/kernel/kvmclock.c|152| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+ *   - arch/x86/kernel/kvmclock.c|153| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kernel/pvclock.c|80| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+ *   - arch/x86/kernel/pvclock.c|81| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kvm/x86.c|2769| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
+ *   - arch/x86/kvm/x86.c|2772| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+ */
 #define PVCLOCK_GUEST_STOPPED	(1 << 1)
 /* PVCLOCK_COUNTS_FROM_ZERO broke ABI and can't be used anymore. */
 #define PVCLOCK_COUNTS_FROM_ZERO (1 << 2)
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2c68..269fb08ac2be 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -39,6 +39,12 @@ bool pvclock_read_retry(const struct pvclock_vcpu_time_info *src,
  * Scale a 64-bit delta by scaling and multiplying by a 32-bit fraction,
  * yielding a 64-bit result.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/pvclock.h|99| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
+ *   - arch/x86/kvm/x86.c|2378| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.h|360| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+ */
 static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 {
 	u64 product;
@@ -78,10 +84,24 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 	return product;
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+ *   - arch/x86/kernel/pvclock.c|76| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+ *   - arch/x86/kvm/x86.c|3072| <<get_kvmclock_ns>> ret = __pvclock_read_cycles(&hv_clock, rdtsc());
+ *   - drivers/ptp/ptp_kvm_x86.c|91| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair.tsc);
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
+	/*
+	 * 更新pvti时,vCPU的vTSC
+	 */
 	u64 delta = tsc - src->tsc_timestamp;
+	/*
+	 * src->tsc_to_system_mul和src->tsc_shift:
+	 * 用于将tsc转换为nsec
+	 */
 	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
 	return src->system_time + offset;
diff --git a/arch/x86/include/asm/xen/interface.h b/arch/x86/include/asm/xen/interface.h
index baca0b00ef76..59b4351367ba 100644
--- a/arch/x86/include/asm/xen/interface.h
+++ b/arch/x86/include/asm/xen/interface.h
@@ -104,6 +104,13 @@ DEFINE_GUEST_HANDLE(xen_ulong_t);
 #define MACH2PHYS_NR_ENTRIES  ((MACH2PHYS_VIRT_END-MACH2PHYS_VIRT_START)>>__MACH2PHYS_SHIFT)
 
 /* Maximum number of virtual CPUs in multi-processor guests. */
+/*
+ * 在以下使用MAX_VIRT_CPUS:
+ *   - arch/x86/kvm/xen.h|125| <<global>> struct compat_vcpu_info vcpu_info[MAX_VIRT_CPUS];
+ *   - include/xen/interface/xen.h|560| <<global>> struct vcpu_info vcpu_info[MAX_VIRT_CPUS];
+ *   - arch/x86/xen/enlighten.c|190| <<xen_vcpu_info_reset>> if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS) {
+ *   - arch/x86/xen/smp.c|135| <<xen_smp_cpus_done>> if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS)
+ */
 #define MAX_VIRT_CPUS 32
 
 /*
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 950afebfba88..9f6c6be2552c 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -34,6 +34,13 @@
 #define KVM_FEATURE_ASYNC_PF_INT	14
 #define KVM_FEATURE_MSI_EXT_DEST_ID	15
 
+/*
+ * 在以下使用KVM_HINTS_REALTIME:
+ *   - arch/x86/kernel/kvm.c|459| <<pv_tlb_flush_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|471| <<pv_sched_yield_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|946| <<kvm_spinlock_init>> if (kvm_para_has_hint(KVM_HINTS_REALTIME)) {
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|99| <<haltpoll_want>> return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
+ */
 #define KVM_HINTS_REALTIME      0
 
 /* The last 8 bits are used to indicate how to interpret the flags field
@@ -65,6 +72,12 @@ struct kvm_steal_time {
 };
 
 #define KVM_VCPU_PREEMPTED          (1 << 0)
+/*
+ * 在以下使用KVM_VCPU_FLUSH_TLB:
+ *   - arch/x86/kernel/kvm.c|603| <<kvm_flush_tlb_multi>> state | KVM_VCPU_FLUSH_TLB))
+ *   - arch/x86/kvm/x86.c|4626| <<record_steal_time>> st_preempted & KVM_VCPU_FLUSH_TLB);
+ *   - arch/x86/kvm/x86.c|4627| <<record_steal_time>> if (st_preempted & KVM_VCPU_FLUSH_TLB)
+ */
 #define KVM_VCPU_FLUSH_TLB          (1 << 1)
 
 #define KVM_CLOCK_PAIRING_WALLCLOCK 0
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index d5c691a3208b..f3becec4bfdf 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -869,6 +869,10 @@ void ioapic_set_alloc_attr(struct irq_alloc_info *info, int node,
 	info->ioapic.valid = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|1055| <<mp_map_pin_to_irq>> ioapic_copy_alloc_attr(&tmp, info, gsi, ioapic, pin);
+ */
 static void ioapic_copy_alloc_attr(struct irq_alloc_info *dst,
 				   struct irq_alloc_info *src,
 				   u32 gsi, int ioapic_idx, int pin)
diff --git a/arch/x86/kernel/apic/probe_64.c b/arch/x86/kernel/apic/probe_64.c
index c46720f185c0..8615a29899df 100644
--- a/arch/x86/kernel/apic/probe_64.c
+++ b/arch/x86/kernel/apic/probe_64.c
@@ -34,6 +34,11 @@ void __init default_setup_apic_routing(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/acpi/boot.c|144| <<acpi_parse_madt>> default_acpi_madt_oem_check(madt->header.oem_id,
+ *   - arch/x86/kernel/jailhouse.c|88| <<jailhouse_x2apic_init>> default_acpi_madt_oem_check("", "");
+ */
 int __init default_acpi_madt_oem_check(char *oem_id, char *oem_table_id)
 {
 	struct apic **drv;
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index fb67ed5e7e6a..1a2382030ed7 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -218,6 +218,15 @@ static int reserve_irq_vector(struct irq_data *irqd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|265| <<assign_irq_vector>> ret = assign_vector_locked(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|279| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, vector_searchmask))
+ *   - arch/x86/kernel/apic/vector.c|285| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, vector_searchmask))
+ *   - arch/x86/kernel/apic/vector.c|290| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, cpumask_of_node(node)))
+ *   - arch/x86/kernel/apic/vector.c|295| <<assign_irq_vector_any_locked>> return assign_vector_locked(irqd, cpu_online_mask);
+ *   - arch/x86/kernel/apic/vector.c|868| <<apic_set_affinity>> err = assign_vector_locked(irqd, vector_searchmask);
+ */
 static int
 assign_vector_locked(struct irq_data *irqd, const struct cpumask *dest)
 {
@@ -255,6 +264,10 @@ assign_vector_locked(struct irq_data *irqd, const struct cpumask *dest)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|313| <<assign_irq_vector_policy>> return assign_irq_vector(irqd, info->mask);
+ */
 static int assign_irq_vector(struct irq_data *irqd, const struct cpumask *dest)
 {
 	unsigned long flags;
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a1b756c49a93..c7119fe8c371 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -64,7 +64,26 @@
 u32 elf_hwcap2 __read_mostly;
 
 /* all of these masks are initialized in setup_cpu_local_masks() */
+/*
+ * 在以下使用cpu_initialized_mask:
+ *   - arch/x86/kernel/cpu/common.c|84| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+ *   - arch/x86/kernel/cpu/common.c|1842| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+ *   - arch/x86/kernel/smpboot.c|1082| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+ *   - arch/x86/kernel/smpboot.c|1104| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+ *   - arch/x86/kernel/smpboot.c|1582| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+ */
 cpumask_var_t cpu_initialized_mask;
+/*
+ * 在以下使用cpu_callout_mask:
+ *   - arch/x86/kernel/cpu/common.c|86| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_callout_mask);
+ *   - arch/x86/kernel/cpu/common.c|1843| <<wait_for_master_cpu>> while (!cpumask_test_cpu(cpu, cpu_callout_mask))
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|261| <<set_mtrr_from_inactive_cpu>> cpu_callout_mask);
+ *   - arch/x86/kernel/smpboot.c|668| <<impress_friends>> if (cpumask_test_cpu(cpu, cpu_callout_mask))
+ *   - arch/x86/kernel/smpboot.c|1108| <<do_boot_cpu>> cpumask_set_cpu(cpu, cpu_callout_mask);
+ *   - arch/x86/kernel/smpboot.c|1407| <<native_smp_prepare_boot_cpu>> cpumask_set_cpu(me, cpu_callout_mask);
+ *   - arch/x86/kernel/smpboot.c|1579| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_callout_mask);
+ *   - arch/x86/xen/smp_pv.c|284| <<cpu_initialize_context>> cpumask_set_cpu(cpu, cpu_callout_mask);
+ */
 cpumask_var_t cpu_callout_mask;
 cpumask_var_t cpu_callin_mask;
 
@@ -1839,6 +1858,14 @@ static void wait_for_master_cpu(int cpu)
 	 * wait for ACK from master CPU before continuing
 	 * with AP initialization
 	 */
+	/*
+	 * 在以下使用cpu_initialized_mask:
+	 *   - arch/x86/kernel/cpu/common.c|84| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+	 *   - arch/x86/kernel/cpu/common.c|1842| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+	 *   - arch/x86/kernel/smpboot.c|1082| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 *   - arch/x86/kernel/smpboot.c|1104| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+	 *   - arch/x86/kernel/smpboot.c|1582| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 */
 	WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
 	while (!cpumask_test_cpu(cpu, cpu_callout_mask))
 		cpu_relax();
diff --git a/arch/x86/kernel/crash.c b/arch/x86/kernel/crash.c
index 54ce999ed321..2598591a9769 100644
--- a/arch/x86/kernel/crash.c
+++ b/arch/x86/kernel/crash.c
@@ -56,6 +56,12 @@ struct crash_memmap_data {
  *
  * protected by rcu.
  */
+/*
+ * 在以下使用crash_vmclear_loaded_vmcss:
+ *   - arch/x86/kernel/crash.c|67| <<cpu_crash_vmclear_loaded_vmcss>> do_vmclear_operation = rcu_dereference(crash_vmclear_loaded_vmcss);
+ *   - arch/x86/kvm/vmx/vmx.c|8159| <<vmx_exit>> RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+ *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_init>> rcu_assign_pointer(crash_vmclear_loaded_vmcss, crash_vmclear_local_loaded_vmcss);
+ */
 crash_vmclear_fn __rcu *crash_vmclear_loaded_vmcss = NULL;
 EXPORT_SYMBOL_GPL(crash_vmclear_loaded_vmcss);
 
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index e28f6a5d14f1..d126174e283c 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -285,6 +285,12 @@ DEFINE_IDTENTRY_SYSVEC(sysvec_x86_platform_ipi)
 
 #ifdef CONFIG_HAVE_KVM
 static void dummy_handler(void) {}
+/*
+ * 在以下使用kvm_posted_intr_wakeup_handler():
+ *   - arch/x86/kernel/irq.c|293| <<kvm_set_posted_intr_wakeup_handler>> kvm_posted_intr_wakeup_handler = handler;
+ *   - arch/x86/kernel/irq.c|295| <<kvm_set_posted_intr_wakeup_handler>> kvm_posted_intr_wakeup_handler = dummy_handler;
+ *   - arch/x86/kernel/irq.c|315| <<DEFINE_IDTENTRY_SYSVEC>> kvm_posted_intr_wakeup_handler();
+ */
 static void (*kvm_posted_intr_wakeup_handler)(void) = dummy_handler;
 
 void kvm_set_posted_intr_wakeup_handler(void (*handler)(void))
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index a26643dc6bd6..bf11b1875a71 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -237,6 +237,10 @@ noinstr u32 kvm_read_and_reset_apf_flags(void)
 }
 EXPORT_SYMBOL_GPL(kvm_read_and_reset_apf_flags);
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/kvm_para.h|102| <<kvm_handle_async_pf>> return __kvm_handle_async_pf(regs, token);
+ */
 noinstr bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token)
 {
 	u32 flags = kvm_read_and_reset_apf_flags();
@@ -451,8 +455,20 @@ static int kvm_cpu_online(unsigned int cpu)
 
 #ifdef CONFIG_SMP
 
+/*
+ * 在以下使用__pv_cpu_mask:
+ *   - arch/x86/kernel/kvm.c|454| <<global>> static DEFINE_PER_CPU(cpumask_var_t, __pv_cpu_mask);
+ *   - arch/x86/kernel/kvm.c|546| <<kvm_send_ipi_mask_allbutself>> struct cpumask *new_mask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
+ *   - arch/x86/kernel/kvm.c|590| <<kvm_flush_tlb_multi>> struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
+ *   - arch/x86/kernel/kvm.c|624| <<kvm_alloc_cpumask>> zalloc_cpumask_var_node(per_cpu_ptr(&__pv_cpu_mask, cpu),
+ */
 static DEFINE_PER_CPU(cpumask_var_t, __pv_cpu_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|613| <<kvm_alloc_cpumask>> if (pv_tlb_flush_supported() || pv_ipi_supported())
+ *   - arch/x86/kernel/kvm.c|719| <<kvm_guest_init>> if (pv_tlb_flush_supported()) {
+ */
 static bool pv_tlb_flush_supported(void)
 {
 	return (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &&
@@ -465,6 +481,10 @@ static bool pv_ipi_supported(void)
 	return kvm_para_has_feature(KVM_FEATURE_PV_SEND_IPI);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|755| <<kvm_guest_init>> if (pv_sched_yield_supported()) {
+ */
 static bool pv_sched_yield_supported(void)
 {
 	return (kvm_para_has_feature(KVM_FEATURE_PV_SCHED_YIELD) &&
@@ -572,12 +592,23 @@ static void kvm_smp_send_call_func_ipi(const struct cpumask *mask)
 	}
 }
 
+/*
+ * 在以下使用kvm_flush_tlb_multi():
+ *   - arch/x86/kernel/kvm.c|725| <<kvm_guest_init>> pv_ops.mmu.flush_tlb_multi = kvm_flush_tlb_multi;
+ */
 static void kvm_flush_tlb_multi(const struct cpumask *cpumask,
 			const struct flush_tlb_info *info)
 {
 	u8 state;
 	int cpu;
 	struct kvm_steal_time *src;
+	/*
+	 * 在以下使用__pv_cpu_mask:
+	 *   - arch/x86/kernel/kvm.c|454| <<global>> static DEFINE_PER_CPU(cpumask_var_t, __pv_cpu_mask);
+	 *   - arch/x86/kernel/kvm.c|546| <<kvm_send_ipi_mask_allbutself>> struct cpumask *new_mask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
+	 *   - arch/x86/kernel/kvm.c|590| <<kvm_flush_tlb_multi>> struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
+	 *   - arch/x86/kernel/kvm.c|624| <<kvm_alloc_cpumask>> zalloc_cpumask_var_node(per_cpu_ptr(&__pv_cpu_mask, cpu),
+	 */
 	struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
 
 	cpumask_copy(flushmask, cpumask);
@@ -594,6 +625,12 @@ static void kvm_flush_tlb_multi(const struct cpumask *cpumask,
 		src = &per_cpu(steal_time, cpu);
 		state = READ_ONCE(src->preempted);
 		if ((state & KVM_VCPU_PREEMPTED)) {
+			/*
+			 * 在以下使用KVM_VCPU_FLUSH_TLB:
+			 *   - arch/x86/kernel/kvm.c|603| <<kvm_flush_tlb_multi>> state | KVM_VCPU_FLUSH_TLB))
+			 *   - arch/x86/kvm/x86.c|4626| <<record_steal_time>> st_preempted & KVM_VCPU_FLUSH_TLB);
+			 *   - arch/x86/kvm/x86.c|4627| <<record_steal_time>> if (st_preempted & KVM_VCPU_FLUSH_TLB)
+			 */
 			if (try_cmpxchg(&src->preempted, &state,
 					state | KVM_VCPU_FLUSH_TLB))
 				__cpumask_clear_cpu(cpu, flushmask);
@@ -868,6 +905,10 @@ static void kvm_kick_cpu(int cpu)
 
 #include <asm/qspinlock.h>
 
+/*
+ * 在以下使用kvm_wait():
+ *   - arch/x86/kernel/kvm.c|987| <<kvm_spinlock_init>> pv_ops.lock.wait = kvm_wait;
+ */
 static void kvm_wait(u8 *ptr, u8 val)
 {
 	if (in_nmi())
@@ -884,6 +925,9 @@ static void kvm_wait(u8 *ptr, u8 val)
 	} else {
 		local_irq_disable();
 
+		/*
+		 * 缺少的注释: safe_halt() will enable IRQ
+		 */
 		if (READ_ONCE(*ptr) == val)
 			safe_halt();
 
@@ -943,6 +987,13 @@ void __init kvm_spinlock_init(void)
 	 * Disable PV spinlocks and use native qspinlock when dedicated pCPUs
 	 * are available.
 	 */
+	/*
+	 * 在以下使用KVM_HINTS_REALTIME:
+	 *   - arch/x86/kernel/kvm.c|459| <<pv_tlb_flush_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+	 *   - arch/x86/kernel/kvm.c|471| <<pv_sched_yield_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+	 *   - arch/x86/kernel/kvm.c|946| <<kvm_spinlock_init>> if (kvm_para_has_hint(KVM_HINTS_REALTIME)) {
+	 *   - drivers/cpuidle/cpuidle-haltpoll.c|99| <<haltpoll_want>> return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
+	 */
 	if (kvm_para_has_hint(KVM_HINTS_REALTIME)) {
 		pr_info("PV spinlocks disabled with KVM_HINTS_REALTIME hints\n");
 		goto out;
@@ -960,6 +1011,21 @@ void __init kvm_spinlock_init(void)
 
 	pr_info("PV spinlocks enabled\n");
 
+
+	/*
+	 * Implement paravirt qspinlocks; the general idea is to halt the vcpus instead
+	 * of spinning them.
+	 *
+	 * This relies on the architecture to provide two paravirt hypercalls:
+	 *
+	 *   pv_wait(u8 *ptr, u8 val) -- suspends the vcpu if *ptr == val
+	 *   pv_kick(cpu)             -- wakes a suspended vcpu
+	 *
+	 * Using these we implement __pv_queued_spin_lock_slowpath() and
+	 * __pv_queued_spin_unlock() to replace native_queued_spin_lock_slowpath() and
+	 * native_queued_spin_unlock().
+	 */
+
 	__pv_init_lock_hash();
 	pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_ops.lock.queued_spin_unlock =
@@ -994,6 +1060,10 @@ static void kvm_enable_host_haltpoll(void *i)
 	wrmsrl(MSR_KVM_POLL_CONTROL, 1);
 }
 
+/*
+ * called by:
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|68| <<haltpoll_cpu_online>> arch_haltpoll_enable(cpu);
+ */
 void arch_haltpoll_enable(unsigned int cpu)
 {
 	if (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL)) {
@@ -1007,6 +1077,10 @@ void arch_haltpoll_enable(unsigned int cpu)
 }
 EXPORT_SYMBOL_GPL(arch_haltpoll_enable);
 
+/*
+ * called by:
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|80| <<haltpoll_cpu_offline>> arch_haltpoll_disable(cpu);
+ */
 void arch_haltpoll_disable(unsigned int cpu)
 {
 	if (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL))
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ad273e5861c1..05b9a7d18e1b 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -67,6 +67,10 @@ static inline struct pvclock_vsyscall_time_info *this_cpu_hvclock(void)
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
+/*
+ * 在以下使用kvm_get_wallclock():
+ *   - arch/x86/kernel/kvmclock.c|349| <<kvmclock_init>> x86_platform.get_wallclock = kvm_get_wallclock;
+ */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
@@ -80,6 +84,24 @@ static int kvm_set_wallclock(const struct timespec64 *now)
 	return -ENODEV;
 }
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ */
 static u64 kvm_clock_read(void)
 {
 	u64 ret;
@@ -163,6 +185,10 @@ static int kvm_cs_enable(struct clocksource *cs)
 	return 0;
 }
 
+/*
+ * kvm_clock 作为一个clocksource,其频率为1GHz,实际上其read回调返回的值
+ * 是就是System Time(即Host Boot Time + Kvmclock Offset)的读数,单位为纳秒
+ */
 struct clocksource kvm_clock = {
 	.name	= "kvm-clock",
 	.read	= kvm_clock_get_cycles,
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index eda37df016f0..c892c866d7fb 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -36,6 +36,11 @@ unsigned long pvclock_tsc_khz(struct pvclock_vcpu_time_info *src)
 	return pv_tsc_khz;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|154| <<kvm_check_and_clear_guest_paused>> pvclock_touch_watchdogs();
+ *   - arch/x86/kernel/pvclock.c|82| <<pvclock_clocksource_read>> pvclock_touch_watchdogs();
+ */
 void pvclock_touch_watchdogs(void)
 {
 	touch_softlockup_watchdog_sync();
@@ -44,6 +49,12 @@ void pvclock_touch_watchdogs(void)
 	reset_hung_task_detector();
 }
 
+/*
+ * 在以下使用atomic的last_value:
+ *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+ *   - arch/x86/kernel/pvclock.c|128| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+ *   - arch/x86/kernel/pvclock.c|132| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+ */
 static atomic64_t last_value = ATOMIC64_INIT(0);
 
 void pvclock_resume(void)
@@ -64,6 +75,26 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ *
+ * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time
+ */
 u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
@@ -100,6 +131,12 @@ u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 	 * updating at the same time, and one of them could be slightly behind,
 	 * making the assumption that last_value always go forward fail to hold.
 	 */
+	/*
+	 * 在以下使用atomic的last_value:
+	 *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+	 *   - arch/x86/kernel/pvclock.c|128| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+	 *   - arch/x86/kernel/pvclock.c|132| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+	 */
 	last = atomic64_read(&last_value);
 	do {
 		if (ret < last)
@@ -143,12 +180,22 @@ void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 	set_normalized_timespec64(ts, now.tv_sec, now.tv_nsec);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|343| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+ *   - arch/x86/xen/time.c|486| <<xen_setup_vsyscall_time_info>> pvclock_set_pvti_cpu0_va(xen_clock);
+ */
 void pvclock_set_pvti_cpu0_va(struct pvclock_vsyscall_time_info *pvti)
 {
 	WARN_ON(vclock_was_used(VDSO_CLOCKMODE_PVCLOCK));
 	pvti_cpu0_va = pvti;
 }
 
+/*
+ * called by:
+ *   - arch/x86/entry/vdso/vma.c|206| <<vvar_fault>> pvclock_get_pvti_cpu0_va();
+ *   - drivers/ptp/ptp_kvm_x86.c|31| <<kvm_arch_ptp_init>> hv_clock = pvclock_get_pvti_cpu0_va();
+ */
 struct pvclock_vsyscall_time_info *pvclock_get_pvti_cpu0_va(void)
 {
 	return pvti_cpu0_va;
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 7770245cc7fa..fbc888bd194e 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -391,6 +391,11 @@ void __init smp_store_boot_cpu_info(void)
  * The bootstrap kernel entry code has set these up. Save them for
  * a given CPU
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|186| <<smp_callin>> smp_store_cpu_info(cpuid);
+ *   - arch/x86/xen/smp_pv.c|75| <<cpu_bringup>> smp_store_cpu_info(cpu);
+ */
 void smp_store_cpu_info(int id)
 {
 	struct cpuinfo_x86 *c = &cpu_data(id);
@@ -1096,10 +1101,29 @@ static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle,
 		boot_error = -1;
 		timeout = jiffies + 10*HZ;
 		while (time_before(jiffies, timeout)) {
+			/*
+			 * 在以下使用cpu_initialized_mask:
+			 *   - arch/x86/kernel/cpu/common.c|84| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+			 *   - arch/x86/kernel/cpu/common.c|1842| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+			 *   - arch/x86/kernel/smpboot.c|1082| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+			 *   - arch/x86/kernel/smpboot.c|1104| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+			 *   - arch/x86/kernel/smpboot.c|1582| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+			 */
 			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
 				/*
 				 * Tell AP to proceed with initialization
 				 */
+				/*
+				 * 在以下使用cpu_callout_mask:
+				 *   - arch/x86/kernel/cpu/common.c|86| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_callout_mask);
+				 *   - arch/x86/kernel/cpu/common.c|1843| <<wait_for_master_cpu>> while (!cpumask_test_cpu(cpu, cpu_callout_mask))
+				 *   - arch/x86/kernel/cpu/mtrr/mtrr.c|261| <<set_mtrr_from_inactive_cpu>> cpu_callout_mask);
+				 *   - arch/x86/kernel/smpboot.c|668| <<impress_friends>> if (cpumask_test_cpu(cpu, cpu_callout_mask))
+				 *   - arch/x86/kernel/smpboot.c|1108| <<do_boot_cpu>> cpumask_set_cpu(cpu, cpu_callout_mask);
+				 *   - arch/x86/kernel/smpboot.c|1407| <<native_smp_prepare_boot_cpu>> cpumask_set_cpu(me, cpu_callout_mask);
+				 *   - arch/x86/kernel/smpboot.c|1579| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_callout_mask);
+				 *   - arch/x86/xen/smp_pv.c|284| <<cpu_initialize_context>> cpumask_set_cpu(cpu, cpu_callout_mask);
+				 */
 				cpumask_set_cpu(cpu, cpu_callout_mask);
 				boot_error = 0;
 				break;
@@ -1574,6 +1598,14 @@ static void remove_cpu_from_maps(int cpu)
 	cpumask_clear_cpu(cpu, cpu_callout_mask);
 	cpumask_clear_cpu(cpu, cpu_callin_mask);
 	/* was set by cpu_init() */
+	/*
+	 * 在以下使用cpu_initialized_mask:
+	 *   - arch/x86/kernel/cpu/common.c|84| <<setup_cpu_local_masks>> alloc_bootmem_cpumask_var(&cpu_initialized_mask);
+	 *   - arch/x86/kernel/cpu/common.c|1842| <<wait_for_master_cpu>> WARN_ON(cpumask_test_and_set_cpu(cpu, cpu_initialized_mask));
+	 *   - arch/x86/kernel/smpboot.c|1082| <<do_boot_cpu>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 *   - arch/x86/kernel/smpboot.c|1104| <<do_boot_cpu>> if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
+	 *   - arch/x86/kernel/smpboot.c|1582| <<remove_cpu_from_maps>> cpumask_clear_cpu(cpu, cpu_initialized_mask);
+	 */
 	cpumask_clear_cpu(cpu, cpu_initialized_mask);
 	numa_remove_cpu(cpu);
 }
@@ -1625,6 +1657,25 @@ int native_cpu_disable(void)
 	return 0;
 }
 
+/*
+ * [0] CPU: 1 PID: 7 Comm: kworker/u16:0
+ * [0] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] dump_stack
+ * [0] common_cpu_die
+ * [0] takedown_cpu
+ * [0] cpuhp_invoke_callback
+ * [0] cpuhp_invoke_callback_range
+ * [0] _cpu_down
+ * [0] cpu_down
+ * [0] device_offline
+ * [0] acpi_bus_offline
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 int common_cpu_die(unsigned int cpu)
 {
 	int ret = 0;
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57ec01192180..d2bb06ca9b1e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -30,9 +30,15 @@
 #include <asm/i8259.h>
 #include <asm/uv/uv.h>
 
+/*
+ * cpu频率
+ */
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
 
+/*
+ * tsc频率
+ */
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index b4da665bb892..74e88fcc80be 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -56,6 +56,11 @@ static u32 xstate_required_size(u64 xstate_bv, bool compacted)
 #define F feature_bit
 #define SF(name) (boot_cpu_has(X86_FEATURE_##name) ? F(name) : 0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|84| <<kvm_check_cpuid>> best = cpuid_entry2_find(entries, nent, 0x80000008, 0);
+ *   - arch/x86/kvm/cpuid.c|1118| <<kvm_find_cpuid_entry>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+ */
 static inline struct kvm_cpuid_entry2 *cpuid_entry2_find(
 	struct kvm_cpuid_entry2 *entries, int nent, u32 function, u32 index)
 {
@@ -106,6 +111,18 @@ void kvm_update_pv_runtime(struct kvm_vcpu *vcpu)
 		vcpu->arch.pv_cpuid.features = best->eax;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|311| <<kvm_vcpu_ioctl_set_cpuid>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/cpuid.c|346| <<kvm_vcpu_ioctl_set_cpuid2>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/lapic.c|2603| <<kvm_lapic_set_base>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/svm/sev.c|2072| <<sev_es_sync_from_ghcb>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|1779| <<svm_set_cr4>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3478| <<vmx_set_cr4>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/x86.c|1164| <<__kvm_set_xcr>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/x86.c|4696| <<kvm_set_msr_common>> kvm_update_cpuid_runtime(vcpu);
+ *   - arch/x86/kvm/x86.c|10705| <<enter_smm>> kvm_update_cpuid_runtime(vcpu);
+ */
 void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpuid_entry2 *best;
@@ -257,6 +274,10 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 }
 
 /* when an old userspace process fills a new kernel module */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6468| <<kvm_arch_vcpu_ioctl(KVM_SET_CPUID)>> r = kvm_vcpu_ioctl_set_cpuid(vcpu, &cpuid, cpuid_arg->entries);
+ */
 int kvm_vcpu_ioctl_set_cpuid(struct kvm_vcpu *vcpu,
 			     struct kvm_cpuid *cpuid,
 			     struct kvm_cpuid_entry __user *entries)
@@ -399,6 +420,11 @@ static __always_inline void kvm_cpu_cap_mask(enum cpuid_leafs leaf, u32 mask)
 	__kvm_cpu_cap_mask(leaf);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|907| <<svm_set_cpu_caps>> kvm_set_cpu_caps();
+ *   - arch/x86/kvm/vmx/vmx.c|7454| <<vmx_set_cpu_caps>> kvm_set_cpu_caps();
+ */
 void kvm_set_cpu_caps(void)
 {
 	unsigned int f_nx = is_efer_nx() ? F(NX) : 0;
@@ -663,6 +689,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1011| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -1103,9 +1133,72 @@ int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|99| <<kvm_update_pv_runtime>> best = kvm_find_cpuid_entry(vcpu, KVM_CPUID_FEATURES, 0);
+ *   - arch/x86/kvm/cpuid.c|113| <<kvm_update_cpuid_runtime>> best = kvm_find_cpuid_entry(vcpu, 1, 0);
+ *   - arch/x86/kvm/cpuid.c|124| <<kvm_update_cpuid_runtime>> best = kvm_find_cpuid_entry(vcpu, 7, 0);
+ *   - arch/x86/kvm/cpuid.c|129| <<kvm_update_cpuid_runtime>> best = kvm_find_cpuid_entry(vcpu, 0xD, 0);
+ *   - arch/x86/kvm/cpuid.c|133| <<kvm_update_cpuid_runtime>> best = kvm_find_cpuid_entry(vcpu, 0xD, 1);
+ *   - arch/x86/kvm/cpuid.c|138| <<kvm_update_cpuid_runtime>> best = kvm_find_cpuid_entry(vcpu, KVM_CPUID_FEATURES, 0);
+ *   - arch/x86/kvm/cpuid.c|144| <<kvm_update_cpuid_runtime>> best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ *   - arch/x86/kvm/cpuid.c|158| <<kvm_vcpu_after_set_cpuid>> best = kvm_find_cpuid_entry(vcpu, 1, 0);
+ *   - arch/x86/kvm/cpuid.c|168| <<kvm_vcpu_after_set_cpuid>> best = kvm_find_cpuid_entry(vcpu, 0xD, 0);
+ *   - arch/x86/kvm/cpuid.c|183| <<kvm_vcpu_after_set_cpuid>> best = kvm_find_cpuid_entry(vcpu, 0x12, 0x1);
+ *   - arch/x86/kvm/cpuid.c|239| <<cpuid_query_maxphyaddr>> best = kvm_find_cpuid_entry(vcpu, 0x80000000, 0);
+ *   - arch/x86/kvm/cpuid.c|242| <<cpuid_query_maxphyaddr>> best = kvm_find_cpuid_entry(vcpu, 0x80000008, 0);
+ *   - arch/x86/kvm/cpuid.c|1157| <<get_out_of_range_cpuid_entry>> basic = kvm_find_cpuid_entry(vcpu, 0, 0);
+ *   - arch/x86/kvm/cpuid.c|1166| <<get_out_of_range_cpuid_entry>> class = kvm_find_cpuid_entry(vcpu, function & 0xffffff00, 0);
+ *   - arch/x86/kvm/cpuid.c|1168| <<get_out_of_range_cpuid_entry>> class = kvm_find_cpuid_entry(vcpu, 0xc0000000, 0);
+ *   - arch/x86/kvm/cpuid.c|1170| <<get_out_of_range_cpuid_entry>> class = kvm_find_cpuid_entry(vcpu, function & 0x80000000, 0);
+ *   - arch/x86/kvm/cpuid.c|1188| <<get_out_of_range_cpuid_entry>> return kvm_find_cpuid_entry(vcpu, basic->eax, index);
+ *   - arch/x86/kvm/cpuid.c|1198| <<kvm_cpuid>> entry = kvm_find_cpuid_entry(vcpu, function, index);
+ *   - arch/x86/kvm/cpuid.c|1227| <<kvm_cpuid>> entry = kvm_find_cpuid_entry(vcpu, function, 1);
+ *   - arch/x86/kvm/cpuid.h|77| <<guest_cpuid_get_register>> entry = kvm_find_cpuid_entry(vcpu, cpuid.function, cpuid.index);
+ *   - arch/x86/kvm/cpuid.h|110| <<guest_cpuid_is_amd_or_hygon>> best = kvm_find_cpuid_entry(vcpu, 0, 0);
+ *   - arch/x86/kvm/cpuid.h|120| <<guest_cpuid_is_intel>> best = kvm_find_cpuid_entry(vcpu, 0, 0);
+ *   - arch/x86/kvm/cpuid.h|128| <<guest_cpuid_family>> best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ *   - arch/x86/kvm/cpuid.h|139| <<guest_cpuid_model>> best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ *   - arch/x86/kvm/cpuid.h|150| <<guest_cpuid_stepping>> best = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ *   - arch/x86/kvm/hyperv.c|278| <<kvm_hv_is_syndbg_enabled>> entry = kvm_find_cpuid_entry(vcpu,
+ *   - arch/x86/kvm/hyperv.c|1817| <<kvm_hv_set_cpuid>> entry = kvm_find_cpuid_entry(vcpu, HYPERV_CPUID_INTERFACE, 0);
+ *   - arch/x86/kvm/svm/svm.c|3972| <<svm_vcpu_after_set_cpuid>> best = kvm_find_cpuid_entry(vcpu, 0x8000001F, 0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|481| <<intel_pmu_refresh>> entry = kvm_find_cpuid_entry(vcpu, 0xa, 0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|523| <<intel_pmu_refresh>> entry = kvm_find_cpuid_entry(vcpu, 7, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|155| <<__handle_encls_ecreate>> sgx_12_0 = kvm_find_cpuid_entry(vcpu, 0x12, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|156| <<__handle_encls_ecreate>> sgx_12_1 = kvm_find_cpuid_entry(vcpu, 0x12, 1);
+ *   - arch/x86/kvm/vmx/sgx.c|440| <<sgx_intercept_encls_ecreate>> guest_cpuid = kvm_find_cpuid_entry(vcpu, 0x12, 0);
+ *   - arch/x86/kvm/vmx/sgx.c|448| <<sgx_intercept_encls_ecreate>> guest_cpuid = kvm_find_cpuid_entry(vcpu, 0x12, 1);
+ *   - arch/x86/kvm/vmx/vmx.c|7339| <<nested_vmx_cr_fixed1_bits_update>> entry = kvm_find_cpuid_entry(vcpu, 0x1, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|7355| <<nested_vmx_cr_fixed1_bits_update>> entry = kvm_find_cpuid_entry(vcpu, 0x7, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|7390| <<update_intel_pt_cfg>> best = kvm_find_cpuid_entry(vcpu, 0x14, i);
+ */
 struct kvm_cpuid_entry2 *kvm_find_cpuid_entry(struct kvm_vcpu *vcpu,
 					      u32 function, u32 index)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpuid_nent:
+	 *   - arch/x86/kvm/cpuid.c|227| <<cpuid_fix_nx_cap>> for (i = 0; i < vcpu->arch.cpuid_nent; ++i) {
+	 *   - arch/x86/kvm/cpuid.c|308| <<kvm_vcpu_ioctl_set_cpuid>> vcpu->arch.cpuid_nent = cpuid->nent;
+	 *   - arch/x86/kvm/cpuid.c|344| <<kvm_vcpu_ioctl_set_cpuid2>> vcpu->arch.cpuid_nent = cpuid->nent;
+	 *   - arch/x86/kvm/cpuid.c|359| <<kvm_vcpu_ioctl_get_cpuid2>> if (cpuid->nent < vcpu->arch.cpuid_nent)
+	 *   - arch/x86/kvm/cpuid.c|363| <<kvm_vcpu_ioctl_get_cpuid2>> vcpu->arch.cpuid_nent * sizeof(struct kvm_cpuid_entry2)))
+	 *   - arch/x86/kvm/cpuid.c|368| <<kvm_vcpu_ioctl_get_cpuid2>> cpuid->nent = vcpu->arch.cpuid_nent;
+	 *   - arch/x86/kvm/cpuid.c|1163| <<kvm_find_cpuid_entry>> return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> int cpuid_nent;
+	 *    -> struct kvm_cpuid_entry2 *cpuid_entries;
+	 *       -> __u32 function;
+	 *       -> __u32 index;
+	 *       -> __u32 flags;              
+	 *       -> __u32 eax;
+	 *       -> __u32 ebx;
+	 *       -> __u32 ecx;
+	 *       -> __u32 edx;
+	 *       -> __u32 padding[3];
+	 */
 	return cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,
 				 function, index);
 }
@@ -1179,6 +1272,12 @@ get_out_of_range_cpuid_entry(struct kvm_vcpu *vcpu, u32 *fn_ptr, u32 index)
 	return kvm_find_cpuid_entry(vcpu, basic->eax, index);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1249| <<kvm_emulate_cpuid>> kvm_cpuid(vcpu, &eax, &ebx, &ecx, &edx, false);
+ *   - arch/x86/kvm/svm/svm.c|1313| <<svm_vcpu_reset>> kvm_cpuid(vcpu, &eax, &dummy, &dummy, &dummy, false);
+ *   - arch/x86/kvm/x86.c|8653| <<emulator_get_cpuid>> return kvm_cpuid(emul_to_vcpu(ctxt), eax, ebx, ecx, edx, exact_only);
+ */
 bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 	       u32 *ecx, u32 *edx, bool exact_only)
 {
@@ -1228,6 +1327,11 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 }
 EXPORT_SYMBOL_GPL(kvm_cpuid);
 
+/*
+ * 在以下使用kvm_emulate_cpuid():
+ *   - svm_exit_handlers[SVM_EXIT_CPUID] = kvm_emulate_cpuid
+ *   - kvm_vmx_exit_handlers[EXIT_REASON_CPUID] = kvm_emulate_cpuid
+ */
 int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)
 {
 	u32 eax, ebx, ecx, edx;
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 5e5de05a8fbf..64c13b03fe8f 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -4625,6 +4625,10 @@ static const struct mode_dual mode_dual_63 = {
 	N, I(DstReg | SrcMem32 | ModRM | Mov, em_movsxd)
 };
 
+/*
+ * 在以下使用opcode_table[256]:
+ *   - arch/x86/kvm/emulate.c|5235| <<x86_decode_insn>> opcode = opcode_table[ctxt->b];
+ */
 static const struct opcode opcode_table[256] = {
 	/* 0x00 - 0x07 */
 	F6ALU(Lock, em_add),
@@ -5111,6 +5115,10 @@ static int decode_operand(struct x86_emulate_ctxt *ctxt, struct operand *op,
 	return rc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9513| <<x86_decode_emulated_instruction>> r = x86_decode_insn(ctxt, insn, insn_len, emulation_type);
+ */
 int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len, int emulation_type)
 {
 	int rc = X86EMUL_CONTINUE;
@@ -5481,6 +5489,10 @@ void init_decode_cache(struct x86_emulate_ctxt *ctxt)
 	ctxt->mem_read.end = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7715| <<x86_emulate_instruction>> r = x86_emulate_insn(ctxt);
+ */
 int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index f00830e5202f..5fdebf552c2d 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -494,6 +494,12 @@ void kvm_hv_irq_routing_update(struct kvm *kvm)
 
 	for (gsi = 0; gsi < irq_rt->nr_rt_entries; gsi++) {
 		hlist_for_each_entry(e, &irq_rt->map[gsi], link) {
+			/*
+			 * #define KVM_IRQ_ROUTING_IRQCHIP 1
+			 * #define KVM_IRQ_ROUTING_MSI 2
+			 * #define KVM_IRQ_ROUTING_S390_ADAPTER 3
+			 * #define KVM_IRQ_ROUTING_HV_SINT 4
+			 */
 			if (e->type == KVM_IRQ_ROUTING_HV_SINT)
 				kvm_hv_set_sint_gsi(kvm, e->hv_sint.vcpu,
 						    e->hv_sint.sint, gsi);
@@ -1168,6 +1174,10 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 	mutex_unlock(&hv->hv_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2753| <<kvm_gen_update_masterclock>> kvm_hv_invalidate_tsc_page(kvm);
+ */
 void kvm_hv_invalidate_tsc_page(struct kvm *kvm)
 {
 	struct kvm_hv *hv = to_kvm_hv(kvm);
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index a6e218c6140d..28a1c861edc2 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -656,6 +656,10 @@ static const struct kvm_io_device_ops speaker_dev_ops = {
 	.write    = speaker_ioport_write,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7410| <<kvm_arch_vm_ioctl>> kvm->arch.vpit = kvm_create_pit(kvm, u.pit_config.flags);
+ */
 struct kvm_pit *kvm_create_pit(struct kvm *kvm, u32 flags)
 {
 	struct kvm_pit *pit;
@@ -678,6 +682,10 @@ struct kvm_pit *kvm_create_pit(struct kvm *kvm, u32 flags)
 	pid_nr = pid_vnr(pid);
 	put_pid(pid);
 
+	/*
+	 * root      6987  0.0  0.0      0     0 ?        S    Feb15   0:00 [kvm-nx-lpage-re]
+	 * root      7010  0.0  0.0      0     0 ?        S    Feb15   0:00 [kvm-pit/6984]
+	 */
 	pit->worker = kthread_create_worker(0, "kvm-pit/%d", pid_nr);
 	if (IS_ERR(pit->worker))
 		goto fail_kthread;
diff --git a/arch/x86/kvm/i8259.c b/arch/x86/kvm/i8259.c
index 629a09ca9860..c58c756cd9ce 100644
--- a/arch/x86/kvm/i8259.c
+++ b/arch/x86/kvm/i8259.c
@@ -582,6 +582,10 @@ static const struct kvm_io_device_ops picdev_eclr_ops = {
 	.write    = picdev_eclr_write,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7373| <<kvm_arch_vm_ioctl>> r = kvm_pic_init(kvm);
+ */
 int kvm_pic_init(struct kvm *kvm)
 {
 	struct kvm_pic *s;
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 172b05343cfd..14dbbf9c99bf 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -20,6 +20,11 @@
  * check if there are pending timer events
  * to be processed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11511| <<vcpu_run>> if (kvm_cpu_has_pending_timer(vcpu))
+ *   - virt/kvm/kvm_main.c|3071| <<kvm_vcpu_check_block>> if (kvm_cpu_has_pending_timer(vcpu))
+ */
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
@@ -34,6 +39,15 @@ EXPORT_SYMBOL(kvm_cpu_has_pending_timer);
  */
 static int pending_userspace_extint(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|37| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_get_extint>> int vector = v->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|123| <<kvm_cpu_get_extint>> v->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5958| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5961| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12371| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	return v->arch.pending_external_vector != -1;
 }
 
@@ -104,6 +118,10 @@ EXPORT_SYMBOL_GPL(kvm_cpu_has_interrupt);
  * Read pending interrupt(from non-APIC source)
  * vector and intack.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|148| <<kvm_cpu_get_interrupt>> int vector = kvm_cpu_get_extint(v);
+ */
 static int kvm_cpu_get_extint(struct kvm_vcpu *v)
 {
 	if (!kvm_cpu_has_extint(v)) {
@@ -129,6 +147,11 @@ static int kvm_cpu_get_extint(struct kvm_vcpu *v)
 /*
  * Read pending interrupt vector and intack.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4544| <<nested_vmx_vmexit>> int irq = kvm_cpu_get_interrupt(vcpu);
+ *   - arch/x86/kvm/x86.c|10535| <<inject_pending_event>> kvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu), false);
+ */
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 {
 	int vector = kvm_cpu_get_extint(v);
@@ -139,6 +162,10 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_get_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11512| <<vcpu_run>> kvm_inject_pending_timer_irqs(vcpu);
+ */
 void kvm_inject_pending_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
@@ -146,6 +173,10 @@ void kvm_inject_pending_timer_irqs(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_inject_pending_timer_irqs);
 
+/*
+ * called by(处理KVM_REQ_MIGRATE_TIMER):
+ *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> __kvm_migrate_timers(vcpu);
+ */
 void __kvm_migrate_timers(struct kvm_vcpu *vcpu)
 {
 	__kvm_migrate_apic_timer(vcpu);
@@ -153,6 +184,11 @@ void __kvm_migrate_timers(struct kvm_vcpu *vcpu)
 	static_call_cond(kvm_x86_migrate_timers)(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|36| <<kvm_arch_irqfd_allowed>> kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
+ *   - virt/kvm/eventfd.c|299| <<kvm_irqfd_assign>> if (!kvm_arch_irqfd_allowed(kvm, args))
+ */
 bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	bool resample = args->flags & KVM_IRQFD_FLAG_RESAMPLE;
diff --git a/arch/x86/kvm/irq.h b/arch/x86/kvm/irq.h
index 9b64abf9b3f1..50f5e12561cc 100644
--- a/arch/x86/kvm/irq.h
+++ b/arch/x86/kvm/irq.h
@@ -18,7 +18,23 @@
 #include <kvm/iodev.h>
 #include "lapic.h"
 
+/*
+ * 在以下使用PIC_NUM_PINS:
+ *   - arch/x86/kvm/irq.h|60| <<global>> unsigned long irq_states[PIC_NUM_PINS];
+ *   - arch/x86/kvm/i8259.c|190| <<kvm_pic_set_irq>> BUG_ON(irq < 0 || irq >= PIC_NUM_PINS);
+ *   - arch/x86/kvm/i8259.c|209| <<kvm_pic_clear_all>> for (i = 0; i < PIC_NUM_PINS; i++)
+ *   - arch/x86/kvm/i8259.c|300| <<kvm_pic_reset>> for (irq = 0; irq < PIC_NUM_PINS/2; irq++)
+ *   - arch/x86/kvm/i8259.c|370| <<pic_ioport_write>> for (irq = 0; irq < PIC_NUM_PINS/2; irq++)
+ *   - arch/x86/kvm/irq_comm.c|290| <<kvm_set_routing_entry>> e->irqchip.pin += PIC_NUM_PINS / 2;
+ *   - arch/x86/kvm/irq_comm.c|293| <<kvm_set_routing_entry>> if (ue->u.irqchip.pin >= PIC_NUM_PINS / 2)
+ */
 #define PIC_NUM_PINS 16
+/*
+ * 在以下使用SELECT_PIC:
+ *   - arch/x86/kvm/i8259.c|82| <<pic_clear_isr>> kvm_notify_acked_irq(s->pics_state->kvm, SELECT_PIC(irq), irq);
+ *   - arch/x86/kvm/i8259.c|374| <<pic_ioport_write>> SELECT_PIC(irq + off),
+ *   - arch/x86/kvm/irq_comm.c|362| <<PIC_ROUTING_ENTRY>> .u.irqchip = { .irqchip = SELECT_PIC(irq), .pin = (irq) % 8 } }
+ */
 #define SELECT_PIC(irq) \
 	((irq) < 8 ? KVM_IRQCHIP_PIC_MASTER : KVM_IRQCHIP_PIC_SLAVE)
 
@@ -94,6 +110,13 @@ static inline int irqchip_in_kernel(struct kvm *kvm)
 
 	/* Matches smp_wmb() when setting irqchip_mode */
 	smp_rmb();
+	/*
+	 * enum kvm_irqchip_mode {
+	 *     KVM_IRQCHIP_NONE,
+	 *     KVM_IRQCHIP_KERNEL,       // created with KVM_CREATE_IRQCHIP
+	 *     KVM_IRQCHIP_SPLIT,        // created with KVM_CAP_SPLIT_IRQCHIP
+	 * };
+	 */
 	return mode != KVM_IRQCHIP_NONE;
 }
 
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index d5b72a08e566..84a0e64c5fb6 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -42,6 +42,15 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|443| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|444| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
+ *   - arch/x86/kvm/ioapic.c|448| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq_comm.c|150| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1722| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|10243| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -50,6 +59,33 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	unsigned long dest_vcpu_bitmap[BITS_TO_LONGS(KVM_MAX_VCPUS)];
 	unsigned int dest_vcpus = 0;
 
+	/*
+	 * struct kvm_lapic_irq {
+	 *     u32 vector;
+	 *     u16 delivery_mode;
+	 *     u16 dest_mode;
+	 *     bool level;
+	 *     u16 trig_mode;
+	 *     u32 shorthand;
+	 *     u32 dest_id;
+	 *     bool msi_redir_hint;
+	 * };
+	 *
+	 * struct dest_map {
+	 *     // vcpu bitmap where IRQ has been sent
+	 *     DECLARE_BITMAP(map, KVM_MAX_VCPU_ID);
+	 *
+	 *     //
+	 *     // Vector sent to a given vcpu, only valid when
+	 *     // the vcpu's bit in map is set
+	 *     //
+	 *     u8 vectors[KVM_MAX_VCPU_ID];
+	 * };
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/irq_comm.c|62| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+	 *   - arch/x86/kvm/irq_comm.c|216| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+	 */
 	if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
 		return r;
 
@@ -99,6 +135,16 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|140| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|172| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq_comm.c|413| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/svm/avic.c|785| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/vmx/posted_intr.c|320| <<pi_update_irte>> kvm_set_msi_irq(kvm, e, &irq);
+ *
+ * 利用kvm_kernel_irq_routing_entry结构填充参数的kvm_lapic_irq结构
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
@@ -109,6 +155,18 @@ void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 	trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
 			      (u64)msg.address_hi << 32 : 0), msg.data);
 
+	/*
+	 * struct kvm_lapic_irq {
+	 *     u32 vector;
+	 *     u16 delivery_mode;
+	 *     u16 dest_mode;
+	 *     bool level;
+	 *     u16 trig_mode;
+	 *     u32 shorthand;
+	 *     u32 dest_id;
+	 *     bool msi_redir_hint;
+	 * };
+	 */
 	irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
 	irq->vector = msg.arch_data.vector;
 	irq->dest_mode = kvm_lapic_irq_dest_mode(msg.arch_addr_lo.dest_mode_logical);
@@ -126,6 +184,13 @@ static inline bool kvm_msi_route_invalid(struct kvm *kvm,
 	return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
 }
 
+/*
+ * 在以下使用kvm_set_msi():
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|54| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - arch/powerpc/kvm/mpic.c|1840| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - arch/x86/kvm/irq_comm.c|321| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+ *   - virt/kvm/irqchip.c|86| <<kvm_send_userspace_msi>> return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
+ */
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id, int level, bool line_status)
 {
@@ -139,6 +204,15 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 
 	kvm_set_msi_irq(kvm, e, &irq);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/hyperv.c|443| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|444| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
+	 *   - arch/x86/kvm/ioapic.c|448| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq_comm.c|150| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|1722| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10243| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 */
 	return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
 }
 
@@ -153,6 +227,29 @@ static int kvm_hv_set_sint(struct kvm_kernel_irq_routing_entry *e,
 	return kvm_hv_synic_set_irq(kvm, e->hv_sint.vcpu, e->hv_sint.sint);
 }
 
+/*
+ * kvm_apic_set_irq
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * vhost_tx_batch.isra.23
+ * handle_tx_copy
+ * handle_tx
+ * handle_tx_kick
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|242| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm,
+ *                                                       KVM_USERSPACE_IRQ_SOURCE_ID, 1,
+ *                                                       false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -160,6 +257,12 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 	struct kvm_lapic_irq irq;
 	int r;
 
+	/*
+	 * #define KVM_IRQ_ROUTING_IRQCHIP 1
+	 * #define KVM_IRQ_ROUTING_MSI 2
+	 * #define KVM_IRQ_ROUTING_S390_ADAPTER 3
+	 * #define KVM_IRQ_ROUTING_HV_SINT 4
+	 */
 	switch (e->type) {
 	case KVM_IRQ_ROUTING_HV_SINT:
 		return kvm_hv_set_sint(e, kvm, irq_source_id, level,
@@ -169,8 +272,23 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (kvm_msi_route_invalid(kvm, e))
 			return -EINVAL;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/irq_comm.c|140| <<kvm_set_msi>> kvm_set_msi_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq_comm.c|172| <<kvm_arch_set_irq_inatomic>> kvm_set_msi_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq_comm.c|413| <<kvm_scan_ioapic_routes>> kvm_set_msi_irq(vcpu->kvm, entry, &irq);
+		 *   - arch/x86/kvm/svm/avic.c|785| <<get_pi_vcpu_info>> kvm_set_msi_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/vmx/posted_intr.c|320| <<pi_update_irte>> kvm_set_msi_irq(kvm, e, &irq);
+		 *
+		 * 利用kvm_kernel_irq_routing_entry结构填充参数的kvm_lapic_irq结构
+		 */
 		kvm_set_msi_irq(kvm, e, &irq);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/irq_comm.c|62| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+		 *   - arch/x86/kvm/irq_comm.c|216| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+		 */
 		if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
 			return r;
 		break;
@@ -264,6 +382,10 @@ bool kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return irqchip_in_kernel(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|149| <<setup_routing_entry>> r = kvm_set_routing_entry(kvm, e, ue);
+ */
 int kvm_set_routing_entry(struct kvm *kvm,
 			  struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue)
@@ -317,6 +439,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|787| <<get_pi_vcpu_info>> if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
+ *   - arch/x86/kvm/vmx/posted_intr.c|334| <<pi_update_irte>> if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
+ */
 bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			     struct kvm_vcpu **dest_vcpu)
 {
@@ -355,6 +482,11 @@ EXPORT_SYMBOL_GPL(kvm_intr_is_single_vcpu);
 #define ROUTING_ENTRY2(irq) \
 	IOAPIC_ROUTING_ENTRY(irq), PIC_ROUTING_ENTRY(irq)
 
+/*
+ * 在以下使用default_routing[]:
+ *   - arch/x86/kvm/irq_comm.c|444| <<kvm_setup_default_irq_routing>> return kvm_set_irq_routing(kvm, default_routing,
+ *   - arch/x86/kvm/irq_comm.c|445| <<kvm_setup_default_irq_routing>> ARRAY_SIZE(default_routing), 0);
+ */
 static const struct kvm_irq_routing_entry default_routing[] = {
 	ROUTING_ENTRY2(0), ROUTING_ENTRY2(1),
 	ROUTING_ENTRY2(2), ROUTING_ENTRY2(3),
@@ -370,6 +502,10 @@ static const struct kvm_irq_routing_entry default_routing[] = {
 	ROUTING_ENTRY1(22), ROUTING_ENTRY1(23),
 };
 
+/*
+ * called by (处理KVM_CREATE_IRQCHIP):
+ *   - arch/x86/kvm/x86.c|7376| <<kvm_arch_vm_ioctl>> r = kvm_setup_default_irq_routing(kvm);
+ */
 int kvm_setup_default_irq_routing(struct kvm *kvm)
 {
 	return kvm_set_irq_routing(kvm, default_routing,
@@ -378,11 +514,19 @@ int kvm_setup_default_irq_routing(struct kvm *kvm)
 
 static const struct kvm_irq_routing_entry empty_routing[] = {};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7111| <<kvm_vm_ioctl_enable_cap>> r = kvm_setup_empty_irq_routing(kvm);
+ */
 int kvm_setup_empty_irq_routing(struct kvm *kvm)
 {
 	return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|273| <<kvm_set_irq_routing>> kvm_arch_post_irq_routing_update(kvm);
+ */
 void kvm_arch_post_irq_routing_update(struct kvm *kvm)
 {
 	if (!irqchip_split(kvm))
@@ -390,6 +534,10 @@ void kvm_arch_post_irq_routing_update(struct kvm *kvm)
 	kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10985| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);
+ */
 void kvm_scan_ioapic_routes(struct kvm_vcpu *vcpu,
 			    ulong *ioapic_handled_vectors)
 {
@@ -421,6 +569,10 @@ void kvm_scan_ioapic_routes(struct kvm_vcpu *vcpu,
 	srcu_read_unlock(&kvm->irq_srcu, idx);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|232| <<kvm_set_irq_routing>> kvm_arch_irq_routing_update(kvm);
+ */
 void kvm_arch_irq_routing_update(struct kvm *kvm)
 {
 	kvm_hv_irq_routing_update(kvm);
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 3db5c42c9ecd..c099ba1d65c1 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -159,6 +159,11 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|542| <<nested_vmcb02_prepare_control>> enter_guest_mode(&svm->vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3356| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 17fa4ab1b834..af49e9891ccd 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -42,6 +42,65 @@
 #include "cpuid.h"
 #include "hyperv.h"
 
+/*
+ * The x2APIC architecture extends the xAPIC architecture in a backward
+ * compatible manner and provides forward extendability for future Intel
+ * platform innovations. Specifically, the x2APIC architecture does the
+ * following:
+ *
+ * - Retains all key elements of compatibility to the xAPIC architecture:
+ *   1. delivery modes,
+ *   2. interrupt and processor priorities,
+ *   3. interrupt sources,
+ *   4. interrupt destination types;
+ * - Provides extensions to scal processor addressability for both the logical
+ *   and physical destination modes;
+ * - Adds new features to enhance performance of interrupt delivery;
+ * - Reduces complexity of logical destination mode interrupt delivery on link
+ *   based platform architectures/
+ * - Use MSR programming interface to access APIC registers when operating in
+ *   XAPIC mode.
+ *
+ * TPR (Task Priority Register)
+ * 任务优先级寄存器,确定当前CPU能够处理什么优先级别的中断,CPU只处理比TPR中级别更高的中断.
+ * 比它低的中断暂时屏蔽掉,也就是在IRR中继续等到.
+ *
+ * 另外 优先级别=vector/16, vector为每个中断对应的中断向量号.
+ *
+ * PPR (Processor Priority Register)
+ * 处理器优先级寄存器，表示当前正处理的中断的优先级,以此来决定处于IRR中的中断是否发送给CPU.
+ * 处于IRR中的中断只有优先级高于处理器优先级才会被发送给处理器. PPR的值为ISR中正服务的最高
+ * 优先级中断和TPR两者之间选取优先级较大的,所以TPR就是靠间接控制PPR来实现暂时屏蔽比TPR优先
+ * 级小的中断的.
+ */
+
+/*
+ * pv-eoi (减少vmexit的数量)
+ *
+ * x86 PC体系架构中的中断控制器,早先是8259A,现在更普遍使用的是APIC,他们处理中断的流程遵循如下流程:
+ *
+ * 1. 外部设备产生一个中断,如果该中断没有被屏蔽掉,中断控制器将IRR寄存器中相应的位置1,表示收到中断,但是还未提交给CPU处理.
+ * 2. 中断控制器将该中断提交给CPU,CPU收到中断请求后,会应答中断控制器.
+ * 3. 中断控制器收到CPU的中断应答后,将IRR寄存器中相应的位清0,并将ISR寄存器相应的位置1,表示CPU正在处理该中断.
+ * 4. 当该中断的处理程序结束以前,需要将中断控制器的EOI寄存器对应的位置1,表示CPU完成了对该中断的处理.
+ * 5. 中断控制器收到EOI后,ISR寄存器中相应的位清0，允许下次中断.
+ * 6. 在虚拟化场景中,该流程至少会导致两次VM Exit: 第一次是VMM截获到设备中断的时候,通知客户机退出,将这个中断注入到客户机中;
+ *    另外一次是当客户机操作系统处理完该中断后,写中断控制器的EOI寄存器,这是个MMIO操作,也会导致客户机退出.
+ *    在一个外部IO比较频繁的场景中,外部中断会导致大量的VM Exit,影响客户机的整体性能.
+ *
+ * PV-EOI其实就是通过半虚拟化的办法来优化上述的VM Exit影响,virtio也是使用这个思想来优化网络和磁盘;就EOI的优化来说,其思想本质上很简单:
+ *
+ * 1. 客户机和VMM协商,首先确定双方是否都能支持PV-EOI特性,如果成功,则进一步协商一块2 bytes的内存区间作为双方处理EOI的共享缓存;
+ * 2. 在VMM向客户机注入中断之前,会把缓存的最低位置1,表示客户机不需要通过写EOI寄存器;
+ * 3. 客户机在写EOI之前,如果发现该位被设置,则将该位清0;VMM轮询这个标志位,当检查到清0后,会更新模拟中断控制器中的EOI寄存器;
+ *    如果客户机发现该位未被设置,则继续使用MMIO或者MSR写EOI寄存器;
+ *
+ * 需要注意的是,为了保证客户机和VMM同时处理共享内存的性能和可靠性,目前KVM的PV-EOF方案采用了如下的优化措施:
+ *
+ * 1. VMM保障仅会在客户机VCPU的上下文中更改共享内存中的最低位,从而避免了客户机采用任何锁机制来与VMM进行同步;
+ * 2. 客户机必须使用原子的test_and_clear操作来更改共享内存中的最低位,这是因为VMM在任何时候都有可能设置或者清除该位;
+ */
+
 #ifndef CONFIG_X86_64
 #define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
 #else
@@ -57,15 +116,32 @@
 #define APIC_VERSION			(0x14UL | ((KVM_APIC_LVT_NUM - 1) << 16))
 #define LAPIC_MMIO_LENGTH		(1 << 12)
 /* followed define is not in apicdef.h */
+/*
+ * 在以下使用MAX_APIC_VECTOR:
+ *   - arch/x86/kvm/lapic.c|514| <<find_highest_vector>> for (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;
+ *   - arch/x86/kvm/lapic.c|534| <<count_vectors>> for (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {
+ *   - arch/x86/kvm/lapic.c|649| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+ */
 #define MAX_APIC_VECTOR			256
 #define APIC_VECTORS_PER_REG		32
 
+/*
+ * 在以下使用lapic_timer_advance_dynamic:
+ *   - arch/x86/kvm/lapic.c|1754| <<__kvm_wait_lapic_expire>> if (lapic_timer_advance_dynamic) {
+ *   - arch/x86/kvm/lapic.c|2693| <<kvm_create_lapic>> lapic_timer_advance_dynamic = true;
+ *   - arch/x86/kvm/lapic.c|2696| <<kvm_create_lapic>> lapic_timer_advance_dynamic = false;
+ */
 static bool lapic_timer_advance_dynamic __read_mostly;
 #define LAPIC_TIMER_ADVANCE_ADJUST_MIN	100	/* clock cycles */
 #define LAPIC_TIMER_ADVANCE_ADJUST_MAX	10000	/* clock cycles */
 #define LAPIC_TIMER_ADVANCE_NS_INIT	1000
 #define LAPIC_TIMER_ADVANCE_NS_MAX     5000
 /* step-by-step approximation to mitigate fluctuation */
+/*
+ * 在以下使用LAPIC_TIMER_ADVANCE_ADJUST_STEP:
+ *   - arch/x86/kvm/lapic.c|1843| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+ *   - arch/x86/kvm/lapic.c|1848| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+ */
 #define LAPIC_TIMER_ADVANCE_ADJUST_STEP 8
 
 static inline int apic_test_vector(int vec, void *bitmap)
@@ -73,6 +149,14 @@ static inline int apic_test_vector(int vec, void *bitmap)
 	return test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|123| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+ *   - arch/x86/kvm/ioapic.c|195| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/ioapic.c|302| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+ *
+ * 检测vector是否在ISR和IRR中设置了
+ */
 bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -91,7 +175,52 @@ static inline int __apic_test_and_clear_vector(int vec, void *bitmap)
 	return __test_and_clear_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * 在以下使用apic_hw_disabled:
+ *   - arch/x86/kvm/lapic.c|2312| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2387| <<kvm_lapic_set_base>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2391| <<kvm_lapic_set_base>> static_branch_inc(&apic_hw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3046| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.h|236| <<kvm_apic_hw_enabled>> if (static_branch_unlikely(&apic_hw_disabled.key))
+ *
+ * apic_hw_disabled代表HW enabled APIC in APIC_BASE MSR
+ * 比如过去是return (apic)->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+ *
+ * commit c5cc421ba3219b90f11d151bc55f1608c12830fa
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:30 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for HW enabled APIC in APIC_BASE MSR
+ *
+ * Usually all APICs are HW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);
+/*
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|327| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|329| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2315| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|2588| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3047| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.h|245| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+ *
+ * apic_sw_disabled代表SW enabled apic in spurious interrupt register,
+ * 比如过去是return apic_get_reg(apic, APIC_SPIV) & APIC_SPIV_APIC_ENABLED;
+ * 
+ * commit f8c1ea103947038b7197bdd4c8451886a58af0c0
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:31 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for SW enabled apic in spurious interrupt register
+ *
+ * Usually all APICs are SW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);
 
 static inline int apic_enabled(struct kvm_lapic *apic)
@@ -106,29 +235,92 @@ static inline int apic_enabled(struct kvm_lapic *apic)
 	(LVT_MASK | APIC_MODE_MASK | APIC_INPUT_POLARITY | \
 	 APIC_LVT_REMOTE_IRR | APIC_LVT_LEVEL_TRIGGER)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|260| <<kvm_recalculate_apic_map>> max_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));
+ *   - arch/x86/kvm/lapic.c|283| <<kvm_recalculate_apic_map>> x2apic_id = kvm_x2apic_id(apic);
+ *   - arch/x86/kvm/lapic.c|822| <<kvm_apic_match_physical_addr>> return mda == kvm_x2apic_id(apic);
+ *   - arch/x86/kvm/lapic.c|830| <<kvm_apic_match_physical_addr>> if (kvm_x2apic_id(apic) > 0xff && mda == kvm_x2apic_id(apic))
+ *
+ * 返回apic->vcpu->vcpu_id
+ */
 static inline u32 kvm_x2apic_id(struct kvm_lapic *apic)
 {
 	return apic->vcpu->vcpu_id;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|247| <<kvm_can_use_hv_timer>> kvm_can_post_timer_interrupt(vcpu));
+ *   - arch/x86/kvm/lapic.c|257| <<kvm_use_posted_timer_interrupt>> return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
+ *   - arch/x86/kvm/lapic.c|3148| <<__kvm_migrate_apic_timer>> kvm_can_post_timer_interrupt(vcpu))
+ */
 static bool kvm_can_post_timer_interrupt(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用pi_inject_timer:
+	 *   - arch/x86/kvm/x86.c|256| <<global>> module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
+	 *   - arch/x86/kvm/x86.h|352| <<global>> extern int pi_inject_timer
+	 *   - arch/x86/kvm/lapic.c|116| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
+	 *   - arch/x86/kvm/x86.c|9858| <<kvm_arch_init>> if (pi_inject_timer == -1)
+	 *   - arch/x86/kvm/x86.c|9859| <<kvm_arch_init>> pi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);
+	 */
 	return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2026| <<start_hv_timer>> if (!kvm_can_use_hv_timer(vcpu))
+ *   - arch/x86/kvm/x86.c|2078| <<handle_fastpath_set_tscdeadline>> if (!kvm_can_use_hv_timer(vcpu))
+ */
 bool kvm_can_use_hv_timer(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 满足下面条件
+	 * - kvm_x86_ops.set_hv_timer
+	 * 并且
+	 * - !(kvm_mwait_in_guest(vcpu->kvm) || kvm_can_post_timer_interrupt(vcpu))
+	 *
+	 * 也就是说, 如果mwait不exit, 或者支持poste_timer_interrupt, 都会使用sw timer
+	 */
 	return kvm_x86_ops.set_hv_timer
 	       && !(kvm_mwait_in_guest(vcpu->kvm) ||
 		    kvm_can_post_timer_interrupt(vcpu));
 }
 EXPORT_SYMBOL_GPL(kvm_can_use_hv_timer);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1733| <<apic_timer_expired>> if (kvm_use_posted_timer_interrupt(apic->vcpu)) {
+ */
 static bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)
 {
 	return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
 }
 
+/*
+ * struct kvm_apic_map {
+ *     struct rcu_head rcu;
+ *     u8 mode;
+ *     u32 max_apic_id;
+ *     union {
+ *         struct kvm_lapic *xapic_flat_map[8];
+ *         struct kvm_lapic *xapic_cluster_map[16][4];
+ *     };
+ *     struct kvm_lapic *phys_map[];
+ *};
+ *
+ * 为了加速对于目标VCPU的查找,在kvm.arch.apic_map中保存了kvm_apic_map结构体.
+ * phys_map成员和logical_map成员记录了RTE的destination filed同VLAPIC结构体的
+ * 对应的关系,分别对应physical mode和logic mode.在发送中断的时候,如果有该map表,
+ * 且中断不是lowest priority和广播,则通过RTE的destination filed就可以直接找到
+ * 目标VCPU，进行快速的分发.否则需要遍历所有的VCPU,逐一的和RTE的destination
+ * field进行匹配.
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|322| <<kvm_recalculate_apic_map>> if (!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))
+ *   - arch/x86/kvm/lapic.c|1006| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+ */
 static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 		u32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {
 	switch (map->mode) {
@@ -181,6 +373,32 @@ enum {
 	DIRTY
 };
 
+/*
+ * 5.4的例子
+ * kvm_make_vcpus_request_mask
+ * kvm_make_all_cpus_request_except
+ * kvm_make_all_cpus_request
+ * kvm_make_scan_ioapic_request
+ * kvm_recalculate_apic_map
+ * kvm_lapic_reg_write
+ * handle_fastpath_set_msr_irqoff
+ * vmx_vcpu_run
+ * vcpu_enter_guest
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_66
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2192| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2443| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2680| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2686| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|531| <<kvm_set_apic_base>> kvm_recalculate_apic_map(vcpu->kvm);
+ */
 void kvm_recalculate_apic_map(struct kvm *kvm)
 {
 	struct kvm_apic_map *new, *old = NULL;
@@ -208,6 +426,25 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 		if (kvm_apic_present(vcpu))
 			max_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));
 
+	/*
+	 * struct kvm_apic_map {
+	 *     struct rcu_head rcu;
+	 *     u8 mode;
+	 *     u32 max_apic_id;
+	 *     union {
+	 *         struct kvm_lapic *xapic_flat_map[8];
+	 *         struct kvm_lapic *xapic_cluster_map[16][4];
+	 *     };
+	 *     struct kvm_lapic *phys_map[];
+	 * };
+	 *
+	 * 为了加速对于目标VCPU的查找,在kvm.arch.apic_map中保存了kvm_apic_map结构体.
+	 * phys_map成员和logical_map成员记录了RTE的destination filed同VLAPIC结构体的
+	 * 对应的关系,分别对应physical mode和logic mode.在发送中断的时候,如果有该map表,
+	 * 且中断不是lowest priority和广播,则通过RTE的destination filed就可以直接找到
+	 * 目标VCPU，进行快速的分发.否则需要遍历所有的VCPU,逐一的和RTE的destination
+	 * field进行匹配.
+	 */
 	new = kvzalloc(sizeof(struct kvm_apic_map) +
 	                   sizeof(struct kvm_lapic *) * ((u64)max_id + 1),
 			   GFP_KERNEL_ACCOUNT);
@@ -281,12 +518,33 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 	kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2164| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_set_spiv(apic, val & mask);
+ *   - arch/x86/kvm/lapic.c|2493| <<kvm_lapic_reset>> apic_set_spiv(apic, 0xff);
+ *   - arch/x86/kvm/lapic.c|2758| <<kvm_apic_set_state>> apic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));
+ */
 static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 {
 	bool enabled = val & APIC_SPIV_APIC_ENABLED;
 
 	kvm_lapic_set_reg(apic, APIC_SPIV, val);
 
+	/*
+	 * 在以下使用apic_sw_disabled:
+	 *   - arch/x86/kvm/lapic.c|327| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|329| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2315| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2588| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|3047| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.h|245| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+	 *
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|341| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|342| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2331| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|246| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (enabled != apic->sw_enabled) {
 		apic->sw_enabled = enabled;
 		if (enabled)
@@ -294,6 +552,18 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 		else
 			static_branch_inc(&apic_sw_disabled.key);
 
+		/*
+		 * 在以下使用kvm_arch->apic_map_dirty:
+		 *   - arch/x86/kvm/lapic.c|218| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+		 *   - arch/x86/kvm/lapic.c|226| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|300| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|323| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|334| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|340| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|346| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|362| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|2376| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 */
 		atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 	}
 
@@ -302,18 +572,34 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 		kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2378| <<kvm_lapic_reg_write>> kvm_apic_set_xapic_id(apic, val >> 24);
+ *   - arch/x86/kvm/lapic.c|2672| <<kvm_lapic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ *   - arch/x86/kvm/lapic.c|2728| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_xapic_id(struct kvm_lapic *apic, u8 id)
 {
 	kvm_lapic_set_reg(apic, APIC_ID, id << 24);
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2547| <<kvm_lapic_reg_write>> kvm_apic_set_ldr(apic, val & APIC_LDR_MASK);
+ *   - arch/x86/kvm/lapic.c|2932| <<kvm_lapic_reset>> kvm_apic_set_ldr(apic, 0);
+ */
 static inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)
 {
 	kvm_lapic_set_reg(apic, APIC_LDR, id);
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2259| <<kvm_lapic_reg_write(APIC_DFR)>> kvm_apic_set_dfr(apic, val | 0x0FFFFFFF);
+ *   - arch/x86/kvm/lapic.c|2599| <<kvm_lapic_reset>> kvm_apic_set_dfr(apic, 0xffffffffU);
+ */
 static inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)
 {
 	kvm_lapic_set_reg(apic, APIC_DFR, val);
@@ -325,6 +611,10 @@ static inline u32 kvm_apic_calc_x2apic_ldr(u32 id)
 	return ((id >> 4) << 16) | (1 << (id & 0xf));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2437| <<kvm_lapic_set_base>> kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 {
 	u32 ldr = kvm_apic_calc_x2apic_ldr(id);
@@ -333,6 +623,18 @@ static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 
 	kvm_lapic_set_reg(apic, APIC_ID, id);
 	kvm_lapic_set_reg(apic, APIC_LDR, ldr);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|218| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|226| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|300| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|323| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|334| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|340| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|346| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|362| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|2376| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
@@ -351,6 +653,17 @@ static inline int apic_lvtt_period(struct kvm_lapic *apic)
 	return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1526| <<__apic_read>> if (apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|1691| <<apic_update_lvtt>> if (apic_lvtt_tscdeadline(apic) != (timer_mode ==
+ *   - arch/x86/kvm/lapic.c|1815| <<kvm_apic_inject_pending_timer_irqs>> if (apic_lvtt_tscdeadline(apic)) {
+ *   - arch/x86/kvm/lapic.c|1839| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+ *   - arch/x86/kvm/lapic.c|2119| <<start_sw_timer>> else if (apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|2328| <<kvm_lapic_reg_write>> if (apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|2464| <<kvm_get_lapic_tscdeadline_msr>> if (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|2474| <<kvm_set_lapic_tscdeadline_msr>> if (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))
+ */
 static inline int apic_lvtt_tscdeadline(struct kvm_lapic *apic)
 {
 	return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
@@ -361,6 +674,12 @@ static inline int apic_lvt_nmi_mode(u32 lvt_val)
 	return (lvt_val & (APIC_MODE_MASK | APIC_LVT_MASKED)) == APIC_DM_NMI;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|165| <<kvm_vcpu_after_set_cpuid>> kvm_apic_set_version(vcpu);
+ *   - arch/x86/kvm/lapic.c|2557| <<kvm_lapic_reset>> kvm_apic_set_version(apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|2845| <<kvm_apic_set_state>> kvm_apic_set_version(vcpu);
+ */
 void kvm_apic_set_version(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -390,6 +709,11 @@ static const unsigned int apic_lvt_mask[KVM_APIC_LVT_NUM] = {
 	LVT_MASK		/* LVTERR */
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|524| <<apic_search_irr>> return find_highest_vector(apic->regs + APIC_IRR);
+ *   - arch/x86/kvm/lapic.c|610| <<apic_find_highest_isr>> result = find_highest_vector(apic->regs + APIC_ISR);
+ */
 static int find_highest_vector(void *bitmap)
 {
 	int vec;
@@ -405,6 +729,10 @@ static int find_highest_vector(void *bitmap)
 	return -1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2460| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+ */
 static u8 count_vectors(void *bitmap)
 {
 	int vec;
@@ -419,6 +747,13 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|607| <<kvm_apic_update_irr>> return __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|3705| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+ *
+ * 应该是把pir给sync到irr去, 然后返回最大的irr
+ */
 bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 {
 	u32 i, vec;
@@ -449,36 +784,79 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6576| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ *
+ * 应该是把pir给sync到irr去, 然后返回最大的irr
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 应该是把pir给sync到irr去, 然后返回最大的irr
+	 */
 	return __kvm_apic_update_irr(pir, apic->regs, max_irr);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|711| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+ *   - arch/x86/kvm/lapic.c|731| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+ *   - arch/x86/kvm/lapic.c|2708| <<kvm_apic_update_apicv>> apic->irr_pending = (apic_search_irr(apic) != -1);
+ *
+ * 返回irr中最大的vector
+ */
 static inline int apic_search_irr(struct kvm_lapic *apic)
 {
 	return find_highest_vector(apic->regs + APIC_IRR);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|727| <<apic_clear_irr>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|835| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|993| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+ *   - arch/x86/kvm/lapic.c|3046| <<kvm_apic_set_state>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|3168| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+ */
 static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 {
 	int result;
 
 	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|729| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|732| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2705| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2708| <<kvm_apic_update_apicv>> apic->irr_pending = (apic_search_irr(apic) != -1);
+	 *   - arch/x86/kvm/lapic.h|214| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|708| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|3141| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *
 	 * Note that irr_pending is just a hint. It will be always
 	 * true with virtual interrupt delivery enabled.
 	 */
 	if (!apic->irr_pending)
 		return -1;
 
+	/*
+	 * 返回irr中最大的vector
+	 */
 	result = apic_search_irr(apic);
 	ASSERT(result == -1 || result >= 16);
 
 	return result;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|742| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|2953| <<kvm_get_apic_interrupt>> apic_clear_irr(vector, apic);
+ */
 static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -493,17 +871,28 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 	} else {
 		apic->irr_pending = false;
 		kvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);
+		/*
+		 * 返回irr中最大的vector
+		 */
 		if (apic_search_irr(apic) != -1)
 			apic->irr_pending = true;
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3531| <<nested_vmx_run>> kvm_apic_clear_irr(vcpu, vmx->nested.posted_intr_nv);
+ */
 void kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)
 {
 	apic_clear_irr(vec, vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_clear_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2705| <<kvm_get_apic_interrupt>> apic_set_isr(vector, apic);
+ */
 static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -532,6 +921,14 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|814| <<apic_clear_isr>> apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1020| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+ *   - arch/x86/kvm/lapic.c|1551| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+ *   - arch/x86/kvm/lapic.c|3048| <<kvm_apic_set_state>> apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|3171| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+ */
 static inline int apic_find_highest_isr(struct kvm_lapic *apic)
 {
 	int result;
@@ -551,6 +948,10 @@ static inline int apic_find_highest_isr(struct kvm_lapic *apic)
 	return result;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1704| <<apic_set_eoi>> apic_clear_isr(vector, apic);
+ */
 static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -576,6 +977,12 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * 在以下使用kvm_lapic_find_highest_irr():
+ *   - arch/x86/kvm/svm/svm.c|4514| <<global>> .sync_pir_to_irr = kvm_lapic_find_highest_irr,
+ *   - arch/x86/kvm/vmx/vmx.c|6593| <<vmx_sync_pir_to_irr>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ *   - arch/x86/kvm/x86.c|10209| <<update_cr8_intercept>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ */
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
 	/* This may race with setting of irr in __apic_accept_irq() and
@@ -591,15 +998,36 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|787| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|1741| <<kvm_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|84| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|106| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|972| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1403| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1416| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|13551| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|703| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+	 *   - arch/x86/kvm/lapic.c|2606| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+	 */
 	return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
 			irq->level, irq->trig_mode, dest_map);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1017| <<kvm_pv_send_ipi>> count = __pv_send_ipi(&ipi_bitmap_low, map, &irq, min);
+ *   - arch/x86/kvm/lapic.c|1019| <<kvm_pv_send_ipi>> count += __pv_send_ipi(&ipi_bitmap_high, map, &irq, min);
+ */
 static int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,
 			 struct kvm_lapic_irq *irq, u32 min)
 {
@@ -651,6 +1079,36 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 	return count;
 }
 
+/*
+ * pv-eoi (减少vmexit的数量)
+ *
+ * x86 PC体系架构中的中断控制器,早先是8259A,现在更普遍使用的是APIC,他们处理中断的流程遵循如下流程:
+ *
+ * 1. 外部设备产生一个中断,如果该中断没有被屏蔽掉,中断控制器将IRR寄存器中相应的位置1,表示收到中断,但是还未提交给CPU处理.
+ * 2. 中断控制器将该中断提交给CPU,CPU收到中断请求后,会应答中断控制器.
+ * 3. 中断控制器收到CPU的中断应答后,将IRR寄存器中相应的位清0,并将ISR寄存器相应的位置1,表示CPU正在处理该中断.
+ * 4. 当该中断的处理程序结束以前,需要将中断控制器的EOI寄存器对应的位置1,表示CPU完成了对该中断的处理.
+ * 5. 中断控制器收到EOI后,ISR寄存器中相应的位清0，允许下次中断.
+ * 6. 在虚拟化场景中,该流程至少会导致两次VM Exit: 第一次是VMM截获到设备中断的时候,通知客户机退出,将这个中断注入到客户机中;
+ *    另外一次是当客户机操作系统处理完该中断后,写中断控制器的EOI寄存器,这是个MMIO操作,也会导致客户机退出.
+ *    在一个外部IO比较频繁的场景中,外部中断会导致大量的VM Exit,影响客户机的整体性能.
+ *
+ * PV-EOI其实就是通过半虚拟化的办法来优化上述的VM Exit影响,virtio也是使用这个思想来优化网络和磁盘;就EOI的优化来说,其思想本质上很简单:
+ *
+ * 1. 客户机和VMM协商,首先确定双方是否都能支持PV-EOI特性,如果成功,则进一步协商一块2 bytes的内存区间作为双方处理EOI的共享缓存;
+ * 2. 在VMM向客户机注入中断之前,会把缓存的最低位置1,表示客户机不需要通过写EOI寄存器;
+ * 3. 客户机在写EOI之前,如果发现该位被设置,则将该位清0;VMM轮询这个标志位,当检查到清0后,会更新模拟中断控制器中的EOI寄存器;
+ *    如果客户机发现该位未被设置,则继续使用MMIO或者MSR写EOI寄存器;
+ *
+ * 需要注意的是,为了保证客户机和VMM同时处理共享内存的性能和可靠性,目前KVM的PV-EOF方案采用了如下的优化措施:
+ *
+ * 1. VMM保障仅会在客户机VCPU的上下文中更改共享内存中的最低位,从而避免了客户机采用任何锁机制来与VMM进行同步;
+ * 2. 客户机必须使用原子的test_and_clear操作来更改共享内存中的最低位,这是因为VMM在任何时候都有可能设置或者清除该位;
+ *
+ * https://blog.csdn.net/luo_brian/article/details/8744025?utm_source=tuicool&utm_medium=referral
+ * https://fedoraproject.org/wiki/QA:Testcase_Virtualization_PV_EOI
+ */
+
 static int pv_eoi_put_user(struct kvm_vcpu *vcpu, u8 val)
 {
 
@@ -691,6 +1149,10 @@ static void pv_eoi_set_pending(struct kvm_vcpu *vcpu)
 	__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2983| <<apic_sync_pv_eoi_from_guest>> pv_eoi_clr_pending(vcpu);
+ */
 static void pv_eoi_clr_pending(struct kvm_vcpu *vcpu)
 {
 	if (pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0) {
@@ -701,6 +1163,17 @@ static void pv_eoi_clr_pending(struct kvm_vcpu *vcpu)
 	__clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
 }
 
+/*
+ * PPR (Processor Priority Register)
+ * 处理器优先级寄存器，表示当前正处理的中断的优先级,以此来决定处于IRR中的中断是否发送给CPU.
+ * 处于IRR中的中断只有优先级高于处理器优先级才会被发送给处理器. PPR的值为ISR中正服务的最高
+ * 优先级中断和TPR两者之间选取优先级较大的,所以TPR就是靠间接控制PPR来实现暂时屏蔽比TPR优先
+ * 级小的中断的.
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1163| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+ *   - arch/x86/kvm/lapic.c|3113| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+ */
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
@@ -713,6 +1186,20 @@ static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 	return highest_irr;
 }
 
+/*
+ * TPR (Task Priority Register)
+ * 任务优先级寄存器,确定当前CPU能够处理什么优先级别的中断,CPU只处理比TPR中级别更高的中断.
+ * 比它低的中断暂时屏蔽掉,也就是在IRR中继续等到.
+ *
+ * 另外 优先级别=vector/16, vector为每个中断对应的中断向量号.
+ *
+ * PPR (Processor Priority Register)
+ * 处理器优先级寄存器，表示当前正处理的中断的优先级,以此来决定处于IRR中的中断是否发送给CPU.
+ * 处于IRR中的中断只有优先级高于处理器优先级才会被发送给处理器. PPR的值为ISR中正服务的最高
+ * 优先级中断和TPR两者之间选取优先级较大的,所以TPR就是靠间接控制PPR来实现暂时屏蔽比TPR优先
+ * 级小的中断的.
+ */
+
 static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 {
 	u32 tpr, isrv, ppr, old_ppr;
@@ -744,6 +1231,10 @@ static void apic_update_ppr(struct kvm_lapic *apic)
 		kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5481| <<handle_tpr_below_threshold>> kvm_apic_update_ppr(vcpu);
+ */
 void kvm_apic_update_ppr(struct kvm_vcpu *vcpu)
 {
 	apic_update_ppr(vcpu->arch.apic);
@@ -912,6 +1403,12 @@ static bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,
  * means that the interrupt should be dropped.  In this case, *bitmap would be
  * zero and *dst undefined.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1062| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+ *   - arch/x86/kvm/lapic.c|1104| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+ *   - arch/x86/kvm/lapic.c|1241| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+ */
 static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 		struct kvm_lapic **src, struct kvm_lapic_irq *irq,
 		struct kvm_apic_map *map, struct kvm_lapic ***dst,
@@ -978,6 +1475,22 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|62| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq_comm.c|216| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ *
+ * struct kvm_lapic_irq {
+ *     u32 vector;
+ *     u16 delivery_mode;
+ *     u16 dest_mode;
+ *     bool level;
+ *     u16 trig_mode;
+ *     u32 shorthand;
+ *     u32 dest_id;
+ *     bool msi_redir_hint;
+ * };
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -997,6 +1510,9 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 	rcu_read_lock();
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 这里是唯一设置ret的地方
+	 */
 	ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
 	if (ret) {
 		*r = 0;
@@ -1025,6 +1541,10 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
  *	   interrupt.
  * - Otherwise, use remapped mode to inject the interrupt.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|434| <<kvm_intr_is_single_vcpu>> if (kvm_intr_is_single_vcpu_fast(kvm, irq, dest_vcpu))
+ */
 bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			struct kvm_vcpu **dest_vcpu)
 {
@@ -1057,6 +1577,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|703| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+ *   - arch/x86/kvm/lapic.c|2606| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1081,6 +1606,18 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 		result = 1;
 
 		if (dest_map) {
+			/*
+			 * struct dest_map {
+			 *     // vcpu bitmap where IRQ has been sent
+			 *     DECLARE_BITMAP(map, KVM_MAX_VCPU_ID);
+			 *
+			 *     //
+			 *     // Vector sent to a given vcpu, only valid when
+			 *     // the vcpu's bit in map is set
+			 *     //
+			 *     u8 vectors[KVM_MAX_VCPU_ID];
+			 * };
+			 */
 			__set_bit(vcpu->vcpu_id, dest_map->map);
 			dest_map->vectors[vcpu->vcpu_id] = vector;
 		}
@@ -1094,6 +1631,9 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_posted_interrupt()
+		 */
 		if (static_call(kvm_x86_deliver_posted_interrupt)(vcpu, vector)) {
 			kvm_lapic_set_irr(vector, apic);
 			kvm_make_request(KVM_REQ_EVENT, vcpu);
@@ -1233,6 +1773,11 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 	kvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2049| <<kvm_lapic_reg_write>> apic_set_eoi(apic);
+ *   - arch/x86/kvm/lapic.c|2736| <<apic_sync_pv_eoi_from_guest>> vector = apic_set_eoi(apic);
+ */
 static int apic_set_eoi(struct kvm_lapic *apic)
 {
 	int vector = apic_find_highest_isr(apic);
@@ -1273,6 +1818,11 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2095| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/x86.c|2018| <<handle_fastpath_set_x2apic_icr_irqoff>> kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
+ */
 void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 {
 	struct kvm_lapic_irq irq;
@@ -1506,6 +2056,13 @@ static void cancel_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2535| <<kvm_lapic_reg_write>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2580| <<kvm_lapic_reg_write>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2882| <<kvm_lapic_reset>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|3202| <<kvm_apic_set_state>> apic_update_lvtt(apic);
+ */
 static void apic_update_lvtt(struct kvm_lapic *apic)
 {
 	u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
@@ -1644,6 +2201,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1749| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1857| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1949| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1998| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2526| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1845,6 +2410,21 @@ static void cancel_hv_timer(struct kvm_lapic *apic)
 	WARN_ON(preemptible());
 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
 	static_call(kvm_x86_cancel_hv_timer)(apic->vcpu);
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	apic->lapic_timer.hv_timer_in_use = false;
 }
 
@@ -1864,6 +2444,21 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	ktimer->hv_timer_in_use = true;
 	hrtimer_cancel(&ktimer->timer);
 
@@ -1920,6 +2515,10 @@ static void restart_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5827| <<handle_fastpath_preemption_timer>> kvm_lapic_expired_hv_timer(vcpu);
+ */
 void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -1978,6 +2577,11 @@ static void __start_apic_timer(struct kvm_lapic *apic, u32 count_reg)
 	restart_apic_timer(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2699| <<kvm_lapic_reg_write(APIC_TMICT)>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2850| <<kvm_set_lapic_tscdeadline_msr>> start_apic_timer(apic);
+ */
 static void start_apic_timer(struct kvm_lapic *apic)
 {
 	__start_apic_timer(apic, APIC_TMICT);
@@ -1996,6 +2600,20 @@ static void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2163| <<kvm_lapic_reg_write>> kvm_lapic_reg_write(apic, APIC_ICR,
+ *   - arch/x86/kvm/lapic.c|2207| <<apic_mmio_write>> kvm_lapic_reg_write(apic, offset & 0xff0, val);
+ *   - arch/x86/kvm/lapic.c|2218| <<kvm_lapic_set_eoi>> kvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);
+ *   - arch/x86/kvm/lapic.c|2237| <<kvm_apic_write_nodecode>> kvm_lapic_reg_write(vcpu->arch.apic, offset, val);
+ *   - arch/x86/kvm/lapic.c|2841| <<kvm_x2apic_msr_write>> kvm_lapic_reg_write(apic, APIC_ICR2, (u32)(data >> 32));
+ *   - arch/x86/kvm/lapic.c|2842| <<kvm_x2apic_msr_write>> return kvm_lapic_reg_write(apic, reg, (u32)data);
+ *   - arch/x86/kvm/lapic.c|2875| <<kvm_hv_vapic_msr_write>> kvm_lapic_reg_write(apic, APIC_ICR2, (u32)(data >> 32));
+ *   - arch/x86/kvm/lapic.c|2876| <<kvm_hv_vapic_msr_write>> return kvm_lapic_reg_write(apic, reg, (u32)data);
+ *   - arch/x86/kvm/svm/avic.c|340| <<avic_incomplete_ipi_interception>> kvm_lapic_reg_write(apic, APIC_ICR2, icrh);
+ *   - arch/x86/kvm/svm/avic.c|341| <<avic_incomplete_ipi_interception>> kvm_lapic_reg_write(apic, APIC_ICR, icrl);
+ *   - arch/x86/kvm/svm/avic.c|507| <<avic_unaccel_trap_write>> kvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));
+ */
 int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 {
 	int ret = 0;
@@ -2174,6 +2792,10 @@ static int apic_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5286| <<handle_apic_access>> kvm_lapic_set_eoi(vcpu);
+ */
 void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 {
 	kvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);
@@ -2181,6 +2803,10 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
 /* emulate APIC access in a trap manner */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5287| <<handle_apic_write>> kvm_apic_write_nodecode(vcpu, offset);
+ */
 void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
@@ -2195,6 +2821,11 @@ void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_write_nodecode);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12286| <<kvm_arch_vcpu_create>> kvm_free_lapic(vcpu);
+ *   - arch/x86/kvm/x86.c|12337| <<kvm_arch_vcpu_destroy>> kvm_free_lapic(vcpu);
+ */
 void kvm_free_lapic(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2260,9 +2891,30 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * called:
+ *   - arch/x86/kvm/lapic.c|2437| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu, APIC_DEFAULT_PHYS_BASE |
+ *   - arch/x86/kvm/lapic.c|2472| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu,
+ *   - arch/x86/kvm/lapic.c|2716| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+ *   - arch/x86/kvm/x86.c|544| <<kvm_set_apic_base>> kvm_lapic_set_base(vcpu, msr_info->data);
+ */
 void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|2724| <<kvm_lapic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|2953| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/svm/svm.c|1306| <<svm_vcpu_reset>> vcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE |
+	 *
+	 * 因为kvm_create_lapic()一开始就设置了MSR_IA32_APICBASE_ENABLE=2048,
+	 * 所以下面(old_value ^ value) & MSR_IA32_APICBASE_ENABLE大概率是0
+	 */
 	u64 old_value = vcpu->arch.apic_base;
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_lapic *apic;
+	 */
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	if (!apic)
@@ -2276,7 +2928,15 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	if (!apic)
 		return;
 
+	/*
+	 * 简单的测试value=2048=MSR_IA32_APICBASE_ENABLE
+	 */
+
 	/* update jump label if enable bit changes */
+	/*
+	 * 因为kvm_create_lapic()一开始就设置了MSR_IA32_APICBASE_ENABLE=2048,
+	 * 所以下面(old_value ^ value) & MSR_IA32_APICBASE_ENABLE大概率是0
+	 */
 	if ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE) {
 		if (value & MSR_IA32_APICBASE_ENABLE) {
 			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
@@ -2284,6 +2944,14 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 			/* Check if there are APF page ready requests pending */
 			kvm_make_request(KVM_REQ_APF_READY, vcpu);
 		} else {
+			/*
+			 * 在以下使用apic_hw_disabled:
+			 *   - arch/x86/kvm/lapic.c|2312| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+			 *   - arch/x86/kvm/lapic.c|2387| <<kvm_lapic_set_base>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+			 *   - arch/x86/kvm/lapic.c|2391| <<kvm_lapic_set_base>> static_branch_inc(&apic_hw_disabled.key);
+			 *   - arch/x86/kvm/lapic.c|3046| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+			 *   - arch/x86/kvm/lapic.h|236| <<kvm_apic_hw_enabled>> if (static_branch_unlikely(&apic_hw_disabled.key))
+			 */
 			static_branch_inc(&apic_hw_disabled.key);
 			atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 		}
@@ -2292,6 +2960,9 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	if (((old_value ^ value) & X2APIC_ENABLE) && (value & X2APIC_ENABLE))
 		kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
 
+	/*
+	 * vmx+x86_set_virtual_apic_mode()
+	 */
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE))
 		static_call(kvm_x86_set_virtual_apic_mode)(vcpu);
 
@@ -2318,6 +2989,10 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_apicv);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12398| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2402,6 +3077,12 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2073| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2973| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|395| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2445,6 +3126,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11997| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
@@ -2470,6 +3155,12 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	apic->lapic_timer.timer.function = apic_timer_fn;
 	if (timer_advance_ns == -1) {
 		apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+		/*
+		 * 在以下使用lapic_timer_advance_dynamic:
+		 *   - arch/x86/kvm/lapic.c|1754| <<__kvm_wait_lapic_expire>> if (lapic_timer_advance_dynamic) {
+		 *   - arch/x86/kvm/lapic.c|2693| <<kvm_create_lapic>> lapic_timer_advance_dynamic = true;
+		 *   - arch/x86/kvm/lapic.c|2696| <<kvm_create_lapic>> lapic_timer_advance_dynamic = false;
+		 */
 		lapic_timer_advance_dynamic = true;
 	} else {
 		apic->lapic_timer.timer_advance_ns = timer_advance_ns;
@@ -2480,7 +3171,20 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	 * APIC is created enabled. This will prevent kvm_lapic_set_base from
 	 * thinking that APIC state has changed.
 	 */
+	/*
+	 * 注意这里apic_base初始化成了MSR_IA32_APICBASE_ENABL
+	 * 注意上面的comment !!!!!
+	 */
 	vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	/*
+	 * 在以下使用apic_sw_disabled:
+	 *   - arch/x86/kvm/lapic.c|327| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|329| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2315| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2588| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|3047| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.h|245| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+	 */
 	static_branch_inc(&apic_sw_disabled.key); /* sw disabled at reset */
 	kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
 
@@ -2492,6 +3196,13 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|100| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|113| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/lapic.c|3216| <<kvm_get_apic_interrupt>> int vector = kvm_apic_has_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3539| <<nested_vmx_run>> kvm_apic_has_interrupt(vcpu) == vmx->nested.posted_intr_nv) {
+ */
 int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2527,6 +3238,10 @@ void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|138| <<kvm_cpu_get_interrupt>> return kvm_get_apic_interrupt(v);
+ */
 int kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)
 {
 	int vector = kvm_apic_has_interrupt(vcpu);
@@ -2604,6 +3319,10 @@ int kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	return kvm_apic_state_fixup(vcpu, s, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5900| <<kvm_vcpu_ioctl_set_lapic>> r = kvm_apic_set_state(vcpu, s);
+ */
 int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2669,6 +3388,10 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
  * last entry. If yes, set EOI on guests's behalf.
  * Clear PV EOI in guest memory in any case.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2995| <<kvm_lapic_sync_from_vapic>> apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
+ */
 static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
@@ -2799,6 +3522,10 @@ int kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	return kvm_lapic_reg_write(apic, reg, (u32)data);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5029| <<kvm_get_msr_common(APIC_BASE_MSR)>> return kvm_x2apic_msr_read(vcpu, msr_info->index, &msr_info->data);
+ */
 int kvm_x2apic_msr_read(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2851,6 +3578,12 @@ int kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1379| <<kvm_hv_set_msr>> if (kvm_lapic_enable_pv_eoi(vcpu, 0, 0))
+ *   - arch/x86/kvm/hyperv.c|1397| <<kvm_hv_set_msr>> if (kvm_lapic_enable_pv_eoi(vcpu,
+ *   - arch/x86/kvm/x86.c|4902| <<kvm_set_msr_common>> if (kvm_lapic_enable_pv_eoi(vcpu, data, sizeof(u8)))
+ */
 int kvm_lapic_enable_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 {
 	u64 addr = data & ~KVM_MSR_ENABLED;
@@ -2872,6 +3605,13 @@ int kvm_lapic_enable_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10934| <<vcpu_enter_guest>> kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11152| <<vcpu_block>> kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11376| <<kvm_arch_vcpu_ioctl_run>> kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11582| <<kvm_arch_vcpu_ioctl_get_mpstate>> kvm_apic_accept_events(vcpu);
+ */
 void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2925,6 +3665,17 @@ void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		else
 			vcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;
 	}
+	/*
+	 * 在以下使用KVM_APIC_SIPI:
+	 *   - arch/x86/kvm/lapic.c|1341| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3203| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe))
+	 *   - arch/x86/kvm/lapic.c|3204| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3216| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe)) {
+	 *   - arch/x86/kvm/lapic.c|3217| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|3803| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3807| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|11619| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	if (test_bit(KVM_APIC_SIPI, &pe)) {
 		clear_bit(KVM_APIC_SIPI, &apic->pending_events);
 		if (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {
@@ -2937,6 +3688,10 @@ void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9912| <<kvm_arch_exit>> kvm_lapic_exit();
+ */
 void kvm_lapic_exit(void)
 {
 	static_key_deferred_flush(&apic_hw_disabled);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 997c45a5963a..d853d250f8a9 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -9,6 +9,17 @@
 #include "hyperv.h"
 
 #define KVM_APIC_INIT		0
+/*
+ * 在以下使用KVM_APIC_SIPI:
+ *   - arch/x86/kvm/lapic.c|1341| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/lapic.c|3203| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe))
+ *   - arch/x86/kvm/lapic.c|3204| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/lapic.c|3216| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe)) {
+ *   - arch/x86/kvm/lapic.c|3217| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/vmx/nested.c|3803| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+ *   - arch/x86/kvm/vmx/nested.c|3807| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/x86.c|11619| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+ */
 #define KVM_APIC_SIPI		1
 #define KVM_APIC_LVT_NUM	6
 
@@ -40,6 +51,21 @@ struct kvm_timer {
 	u32 timer_advance_ns;
 	s64 advance_expire_delta;
 	atomic_t pending;			/* accumulated triggered timers */
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	bool hv_timer_in_use;
 };
 
@@ -49,7 +75,28 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|341| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|342| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2331| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|246| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	bool sw_enabled;
+	/*
+	 * 在以下设置kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|729| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|732| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2705| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2708| <<kvm_apic_update_apicv>> apic->irr_pending = (apic_search_irr(apic) != -1);
+	 *   - arch/x86/kvm/lapic.h|214| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|708| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|3141| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *
+	 * Note that irr_pending is just a hint. It will be always
+	 * true with virtual interrupt delivery enabled.
+	 */
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
 	/* Number of bits set in ISR. */
@@ -62,8 +109,36 @@ struct kvm_lapic {
 	 * Note: Only one register, the TPR, is used by the microcode.
 	 */
 	void *regs;
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3057| <<kvm_lapic_set_vapic_addr>> int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
+	 *   - arch/x86/kvm/lapic.c|3059| <<kvm_lapic_set_vapic_addr>> if (vapic_addr) {
+	 *   - arch/x86/kvm/lapic.c|3062| <<kvm_lapic_set_vapic_addr>> vapic_addr, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3069| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|6476| <<kvm_arch_vcpu_ioctl>> r = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);
+	 *   - arch/x86/kvm/x86.c|10208| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	gpa_t vapic_addr;
 	struct gfn_to_hva_cache vapic_cache;
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1330| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1341| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3177| <<kvm_apic_accept_events>> pe = smp_load_acquire(&apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3204| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3209| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3217| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.h|354| <<kvm_apic_has_events>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|365| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1098| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3792| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3796| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|3803| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3807| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6052| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6054| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|11619| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	unsigned long pending_events;
 	unsigned int sipi_vector;
 };
@@ -143,6 +218,11 @@ static inline void kvm_lapic_set_vector(int vec, void *bitmap)
 	set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1619| <<__apic_accept_irq>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/svm/avic.c|687| <<svm_deliver_avic_intr>> kvm_lapic_set_irr(vec, vcpu->arch.apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	kvm_lapic_set_vector(vec, apic->regs + APIC_IRR);
@@ -153,6 +233,43 @@ static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 	apic->irr_pending = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|523| <<kvm_ioapic_update_eoi_one>> kvm_lapic_get_reg(apic, APIC_SPIV) & APIC_SPIV_DIRECTED_EOI)
+ *   - arch/x86/kvm/lapic.c|274| <<kvm_recalculate_apic_map>> ldr = kvm_lapic_get_reg(apic, APIC_LDR);
+ *   - arch/x86/kvm/lapic.c|280| <<kvm_recalculate_apic_map>> if (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)
+ *   - arch/x86/kvm/lapic.c|367| <<apic_lvt_enabled>> return !(kvm_lapic_get_reg(apic, lvt_type) & APIC_LVT_MASKED);
+ *   - arch/x86/kvm/lapic.c|747| <<__apic_update_ppr>> old_ppr = kvm_lapic_get_reg(apic, APIC_PROCPRI);
+ *   - arch/x86/kvm/lapic.c|748| <<__apic_update_ppr>> tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI);
+ *   - arch/x86/kvm/lapic.c|818| <<kvm_apic_match_logical_addr>> logical_id = kvm_lapic_get_reg(apic, APIC_LDR);
+ *   - arch/x86/kvm/lapic.c|826| <<kvm_apic_match_logical_addr>> switch (kvm_lapic_get_reg(apic, APIC_DFR)) {
+ *   - arch/x86/kvm/lapic.c|1342| <<apic_get_tmcct>> if (kvm_lapic_get_reg(apic, APIC_TMICT) == 0 ||
+ *   - arch/x86/kvm/lapic.c|1393| <<__apic_read>> val = kvm_lapic_get_reg(apic, offset);
+ *   - arch/x86/kvm/lapic.c|1399| <<__apic_read>> val = kvm_lapic_get_reg(apic, offset);
+ *   - arch/x86/kvm/lapic.c|1507| <<update_divide_count>> tdcr = kvm_lapic_get_reg(apic, APIC_TDCR);
+ *   - arch/x86/kvm/lapic.c|1547| <<apic_update_lvtt>> u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
+ *   - arch/x86/kvm/lapic.c|1571| <<lapic_timer_int_injected>> u32 reg = kvm_lapic_get_reg(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|1773| <<update_target_expiration>> tmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));
+ *   - arch/x86/kvm/lapic.c|1799| <<set_target_expiration>> tmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));
+ *   - arch/x86/kvm/lapic.c|1812| <<set_target_expiration>> kvm_lapic_get_reg(apic, count_reg));
+ *   - arch/x86/kvm/lapic.c|1822| <<set_target_expiration>> kvm_lapic_get_reg(apic, count_reg),
+ *   - arch/x86/kvm/lapic.c|2126| <<kvm_lapic_reg_write>> if (kvm_lapic_get_reg(apic, APIC_LVR) & APIC_LVR_DIRECTED_EOI)
+ *   - arch/x86/kvm/lapic.c|2134| <<kvm_lapic_reg_write>> lvt_val = kvm_lapic_get_reg(apic,
+ *   - arch/x86/kvm/lapic.c|2148| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/lapic.c|2347| <<kvm_lapic_set_tpr>> | (kvm_lapic_get_reg(apic, APIC_TASKPRI) & 4));
+ *   - arch/x86/kvm/lapic.c|2354| <<kvm_lapic_get_cr8>> tpr = (u64) kvm_lapic_get_reg(vcpu->arch.apic, APIC_TASKPRI);
+ *   - arch/x86/kvm/lapic.c|2442| <<kvm_lapic_reset>> apic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));
+ *   - arch/x86/kvm/lapic.c|2503| <<kvm_apic_local_deliver>> u32 reg = kvm_lapic_get_reg(apic, lvt_type);
+ *   - arch/x86/kvm/lapic.c|2606| <<kvm_apic_accept_pic_intr>> u32 lvt0 = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LVT0);
+ *   - arch/x86/kvm/lapic.c|2727| <<kvm_apic_set_state>> apic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));
+ *   - arch/x86/kvm/lapic.c|2852| <<kvm_lapic_sync_to_vapic>> tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;
+ *   - arch/x86/kvm/lapic.h|274| <<kvm_xapic_id>> return kvm_lapic_get_reg(apic, APIC_ID) >> 24;
+ *   - arch/x86/kvm/svm/avic.c|399| <<avic_ldr_write>> flat = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR) == APIC_DFR_FLAT;
+ *   - arch/x86/kvm/svm/avic.c|427| <<avic_handle_ldr_update>> u32 ldr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LDR);
+ *   - arch/x86/kvm/svm/avic.c|476| <<avic_handle_dfr_update>> u32 dfr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR);
+ *   - arch/x86/kvm/svm/avic.c|507| <<avic_unaccel_trap_write>> kvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));
+ *   - arch/x86/kvm/vmx/nested.c|3269| <<vmx_has_apicv_interrupt>> u8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);
+ */
 static inline u32 kvm_lapic_get_reg(struct kvm_lapic *apic, int reg_off)
 {
 	return *((u32 *) (apic->regs + reg_off));
@@ -168,6 +285,14 @@ static inline void kvm_lapic_set_reg(struct kvm_lapic *apic, int reg_off, u32 va
 	__kvm_lapic_set_reg(apic->regs, reg_off, val);
 }
 
+/*
+ * 在以下使用kvm_has_noapic_vcpu:
+ *   - arch/x86/kvm/lapic.h|269| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/x86.c|12588| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|273| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+ *   - arch/x86/kvm/x86.c|12195| <<kvm_arch_vcpu_create>> static_branch_inc(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/x86.c|12327| <<kvm_arch_vcpu_destroy>> static_branch_dec(&kvm_has_noapic_vcpu);
+ */
 DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 
 static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
@@ -177,19 +302,99 @@ static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * apic_hw_disabled代表HW enabled APIC in APIC_BASE MSR
+ * 比如过去是return (apic)->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+ *
+ * commit c5cc421ba3219b90f11d151bc55f1608c12830fa
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:30 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for HW enabled APIC in APIC_BASE MSR
+ *
+ * Usually all APICs are HW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ *
+ *
+ * apic_sw_disabled代表SW enabled apic in spurious interrupt register,
+ * 比如过去是return apic_get_reg(apic, APIC_SPIV) & APIC_SPIV_APIC_ENABLED;
+ *
+ * commit f8c1ea103947038b7197bdd4c8451886a58af0c0
+ * Author: Gleb Natapov <gleb@redhat.com>
+ * Date:   Sun Aug 5 15:58:31 2012 +0300
+ *
+ * KVM: use jump label to optimize checking for SW enabled apic in spurious interrupt register
+ *
+ * Usually all APICs are SW enabled so the check can be optimized out.
+ *
+ * Signed-off-by: Gleb Natapov <gleb@redhat.com>
+ * Signed-off-by: Avi Kivity <avi@redhat.com>
+ */
+
+/*
+ * 在以下使用apic_hw_disabled:
+ *   - arch/x86/kvm/lapic.c|2312| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2387| <<kvm_lapic_set_base>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2391| <<kvm_lapic_set_base>> static_branch_inc(&apic_hw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3046| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.h|236| <<kvm_apic_hw_enabled>> if (static_branch_unlikely(&apic_hw_disabled.key))
+ */
 extern struct static_key_false_deferred apic_hw_disabled;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|116| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|1573| <<apic_mmio_read>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|1657| <<lapic_timer_int_injected>> if (kvm_apic_hw_enabled(apic)) {
+ *   - arch/x86/kvm/lapic.c|2326| <<apic_mmio_write>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2602| <<kvm_apic_local_deliver>> if (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {
+ *   - arch/x86/kvm/lapic.c|2704| <<kvm_apic_accept_pic_intr>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ *   - arch/x86/kvm/lapic.h|259| <<kvm_apic_present>> return lapic_in_kernel(vcpu) && kvm_apic_hw_enabled(vcpu->arch.apic);
+ *   - arch/x86/kvm/x86.c|10743| <<vcpu_load_eoi_exitmap>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ */
 static inline int kvm_apic_hw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 猜测如果没有VM disable了APIC, 直接返回MSR_IA32_APICBASE_ENABLE就行
+	 */
 	if (static_branch_unlikely(&apic_hw_disabled.key))
 		return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
 	return MSR_IA32_APICBASE_ENABLE;
 }
 
+/*
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|327| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|329| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2315| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|2588| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3047| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.h|245| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+ *
+ * 这是extern的
+ */
 extern struct static_key_false_deferred apic_sw_disabled;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|76| <<kvm_irq_delivery_to_apic>> } else if (kvm_apic_sw_enabled(vcpu->arch.apic)) {
+ *   - arch/x86/kvm/lapic.c|116| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|319| <<kvm_recalculate_apic_map>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2253| <<kvm_lapic_reg_write>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2264| <<kvm_lapic_reg_write>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.h|264| <<kvm_lapic_enabled>> return kvm_apic_present(vcpu) && kvm_apic_sw_enabled(vcpu->arch.apic);
+ */
 static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|341| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|342| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2331| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|246| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (static_branch_unlikely(&apic_sw_disabled.key))
 		return apic->sw_enabled;
 	return true;
@@ -205,6 +410,32 @@ static inline int kvm_lapic_enabled(struct kvm_vcpu *vcpu)
 	return kvm_apic_present(vcpu) && kvm_apic_sw_enabled(vcpu->arch.apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|269| <<kvm_recalculate_apic_map>> if ((apic_x2apic_mode(apic) || x2apic_id > 0xff) &&
+ *   - arch/x86/kvm/lapic.c|276| <<kvm_recalculate_apic_map>> if (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])
+ *   - arch/x86/kvm/lapic.c|284| <<kvm_recalculate_apic_map>> if (apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|795| <<kvm_apic_broadcast>> return mda == (apic_x2apic_mode(apic) ?
+ *   - arch/x86/kvm/lapic.c|804| <<kvm_apic_match_physical_addr>> if (apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|828| <<kvm_apic_match_logical_addr>> if (apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|867| <<kvm_apic_mda>> !ipi && dest_id == APIC_BROADCAST && apic_x2apic_mode(target))
+ *   - arch/x86/kvm/lapic.c|933| <<kvm_apic_is_broadcast_dest>> bool x2apic_ipi = src && *src && apic_x2apic_mode(*src);
+ *   - arch/x86/kvm/lapic.c|1331| <<kvm_apic_send_ipi>> if (apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|1454| <<kvm_lapic_reg_read>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|1497| <<apic_mmio_read>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2103| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2119| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2126| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2161| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2218| <<kvm_lapic_reg_write>> if (apic_x2apic_mode(apic) && val != 0)
+ *   - arch/x86/kvm/lapic.c|2223| <<kvm_lapic_reg_write>> if (apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2250| <<apic_mmio_write>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2455| <<kvm_lapic_reset>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2675| <<kvm_apic_state_fixup>> if (apic_x2apic_mode(vcpu->arch.apic)) {
+ *   - arch/x86/kvm/lapic.c|2894| <<kvm_x2apic_msr_write>> if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2911| <<kvm_x2apic_msr_read>> if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/x86.c|2057| <<handle_fastpath_set_x2apic_icr_irqoff>> if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))
+ */
 static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 {
 	return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
@@ -212,6 +443,12 @@ static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 
 static inline bool kvm_vcpu_apicv_active(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_lapic *apic;
+	 *    -> bool apicv_active;
+	 */
 	return vcpu->arch.apic && vcpu->arch.apicv_active;
 }
 
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 88d0ed5225a4..9adca9195974 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -114,6 +114,11 @@ static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 		       bool prefault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5120| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
+ *   - arch/x86/kvm/x86.c|13606| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefault)
 {
@@ -175,6 +180,11 @@ static inline bool is_write_protection(struct kvm_vcpu *vcpu)
  * Return zero if the access does not fault; return the page fault error code
  * if the access faults.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|431| <<FNAME(walk_addr_generic)>> errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
+ *   - arch/x86/kvm/x86.c|8280| <<vcpu_mmio_gva_to_gpa>> && !permission_fault(vcpu, vcpu->arch.walk_mmu,
+ */
 static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 				  unsigned pte_access, unsigned pte_pkey,
 				  unsigned pfec)
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 8d5876dfc6b7..a730123b470b 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -213,6 +213,12 @@ bool is_nx_huge_page_enabled(void)
 	return READ_ONCE(nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2556| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|3986| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ *   - arch/x86/kvm/mmu/mmutrace.h|204| <<KVM_MMU_PAGE_ASSIGN>> mark_mmio_spte,
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -1253,6 +1259,11 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1280| <<rmap_write_protect>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|103| <<kvm_slot_page_track_add_page>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn)
 {
@@ -2533,6 +2544,13 @@ static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2650| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, start, access, false, sp->role.level, gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|2867| <<__direct_map>> ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|557| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, spte, pte_access, false, PG_LEVEL_4K, gfn, pfn,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|730| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault,
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 			unsigned int pte_access, bool write_fault, int level,
 			gfn_t gfn, kvm_pfn_t pfn, bool speculative,
@@ -2813,6 +2831,10 @@ void disallowed_hugepage_adjust(u64 spte, gfn_t gfn, int cur_level,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3778| <<direct_page_fault>> r = __direct_map(vcpu, gpa, error_code, map_writable, max_level, pfn,
+ */
 static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 			int map_writable, int max_level, kvm_pfn_t pfn,
 			bool prefault, bool is_tdp)
@@ -3657,11 +3679,33 @@ static void shadow_page_table_clear_flood(struct kvm_vcpu *vcpu, gva_t addr)
 	walk_shadow_page_lockless_end(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3713| <<try_async_pf>> } else if (kvm_arch_setup_async_pf(vcpu, cr2_or_gpa, gfn))
+ */
 static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				    gfn_t gfn)
 {
 	struct kvm_arch_async_pf arch;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct {
+	 *           bool halted;
+	 *           gfn_t gfns[ASYNC_PF_PER_VCPU];
+	 *           struct gfn_to_hva_cache data;
+	 *           u64 msr_en_val; // MSR_KVM_ASYNC_PF_EN
+	 *           u64 msr_int_val; // MSR_KVM_ASYNC_PF_INT
+	 *           u16 vec;
+	 *           u32 id;
+	 *           bool send_user_only;
+	 *           u32 host_apf_flags;
+	 *           unsigned long nested_apf_token;
+	 *           bool delivery_as_pf_vmexit;
+	 *           bool pageready_pending;
+	 *       } apf
+	 */
 	arch.token = (vcpu->arch.apf.id++ << 12) | vcpu->vcpu_id;
 	arch.gfn = gfn;
 	arch.direct_map = vcpu->arch.mmu->direct_map;
@@ -3714,6 +3758,11 @@ static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3796| <<nonpaging_page_fault>> return direct_page_fault(vcpu, gpa & PAGE_MASK, error_code, prefault,
+ *   - arch/x86/kvm/mmu/mmu.c|3848| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, gpa, error_code, prefault,
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 			     bool prefault, int max_level, bool is_tdp)
 {
@@ -3788,6 +3837,11 @@ static int nonpaging_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa,
 				 PG_LEVEL_2M, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1890| <<pf_interception>> return kvm_handle_page_fault(vcpu, error_code, fault_address,
+ *   - arch/x86/kvm/vmx/vmx.c|5308| <<handle_exception_nmi>> return kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);
+ */
 int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 				u64 fault_address, char *insn, int insn_len)
 {
@@ -3821,6 +3875,12 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu.h|121| <<kvm_mmu_do_page_fault>> if (likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault))
+ *   - arch/x86/kvm/mmu.h|122| <<kvm_mmu_do_page_fault>> return kvm_tdp_page_fault(vcpu, cr2_or_gpa, err, prefault);
+ *   - arch/x86/kvm/mmu/mmu.c|4546| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 		       bool prefault)
 {
@@ -4850,6 +4910,20 @@ kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu)
 	return role.base;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|208| <<kvm_vcpu_after_set_cpuid>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|1230| <<init_vmcb>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4362| <<nested_vmx_restore_host_state>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2942| <<enter_rmode>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|863| <<kvm_post_set_cr0>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1046| <<kvm_post_set_cr4>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1556| <<set_efer>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|7118| <<emulator_set_hflags>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|7451| <<kvm_smm_changed>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|8970| <<enter_smm>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|10057| <<__set_sregs>> kvm_mmu_reset_context(vcpu);
+ */
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -4882,6 +4956,14 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4855| <<kvm_mmu_reset_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5943| <<kvm_mmu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|3090| <<kvm_vcpu_flush_tlb_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|9156| <<vcpu_enter_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|10781| <<kvm_unload_vcpu_mmu>> kvm_mmu_unload(vcpu);
+ */
 void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_free_roots(vcpu, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
@@ -5207,6 +5289,11 @@ void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|993| <<svm_hardware_setup>> kvm_configure_mmu(npt_enabled, get_max_npt_level(), PG_LEVEL_1G);
+ *   - arch/x86/kvm/vmx/vmx.c|8157| <<hardware_setup>> kvm_configure_mmu(enable_ept, vmx_get_max_tdp_level(), ept_lpage_level);
+ */
 void kvm_configure_mmu(bool enable_tdp, int tdp_max_root_level,
 		       int tdp_huge_page_level)
 {
@@ -5979,6 +6066,10 @@ static int set_nx_huge_pages_recovery_ratio(const char *val, const struct kernel
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6088| <<kvm_nx_lpage_recovery_worker>> kvm_recover_nx_lpages(kvm);
+ */
 static void kvm_recover_nx_lpages(struct kvm *kvm)
 {
 	int rcu_idx;
@@ -6032,6 +6123,10 @@ static long get_nx_lpage_recovery_timeout(u64 start_time)
 		: MAX_SCHEDULE_TIMEOUT;
 }
 
+/*
+ * 在以下使用kvm_nx_lpage_recovery_worker():
+ *   - arch/x86/kvm/mmu/mmu.c|6096| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_lpage_recovery_worker, 0,
+ */
 static int kvm_nx_lpage_recovery_worker(struct kvm *kvm, uintptr_t data)
 {
 	u64 start_time;
diff --git a/arch/x86/kvm/mmu/page_track.c b/arch/x86/kvm/mmu/page_track.c
index 34bb0ec69bd8..4226ce244ddb 100644
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@ -28,6 +28,10 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *slot)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12690| <<kvm_alloc_memslot_metadata>> if (kvm_page_track_create_memslot(slot, npages))
+ */
 int kvm_page_track_create_memslot(struct kvm_memory_slot *slot,
 				  unsigned long npages)
 {
@@ -83,6 +87,11 @@ static void update_gfn_track(struct kvm_memory_slot *slot, gfn_t gfn,
  * @gfn: the guest page.
  * @mode: tracking mode, currently only write track is supported.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|762| <<account_shadowed>> return kvm_slot_page_track_add_page(kvm, slot, gfn,
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1814| <<kvmgt_page_track_add>> kvm_slot_page_track_add_page(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE);
+ */
 void kvm_slot_page_track_add_page(struct kvm *kvm,
 				  struct kvm_memory_slot *slot, gfn_t gfn,
 				  enum kvm_page_track_mode mode)
@@ -163,6 +172,10 @@ void kvm_page_track_cleanup(struct kvm *kvm)
 	cleanup_srcu_struct(&head->track_srcu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12461| <<kvm_arch_init_vm>> kvm_page_track_init(kvm);
+ */
 void kvm_page_track_init(struct kvm *kvm)
 {
 	struct kvm_page_track_notifier_head *head;
@@ -176,6 +189,11 @@ void kvm_page_track_init(struct kvm *kvm)
  * register the notifier so that event interception for the tracked guest
  * pages can be received.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5520| <<kvm_mmu_init_vm>> kvm_page_track_register_notifier(kvm, node);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1948| <<kvmgt_guest_init>> kvm_page_track_register_notifier(kvm, &info->track_node);
+ */
 void
 kvm_page_track_register_notifier(struct kvm *kvm,
 				 struct kvm_page_track_notifier_node *n)
@@ -216,6 +234,11 @@ EXPORT_SYMBOL_GPL(kvm_page_track_unregister_notifier);
  * The node should figure out if the written page is the one that node is
  * interested in by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7929| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|8203| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ */
 void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			  int bytes)
 {
@@ -243,6 +266,10 @@ void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
  * The node should figure out it has any write-protected pages in this slot
  * by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12867| <<kvm_arch_flush_shadow_memslot>> kvm_page_track_flush_slot(kvm, slot);
+ */
 void kvm_page_track_flush_slot(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
 	struct kvm_page_track_notifier_head *head;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 827886c12c16..fb17f708a1bb 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -168,6 +168,12 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|283| <<reprogram_counter>> reprogram_gp_counter(pmc, pmc->eventsel);
+ *   - arch/x86/kvm/svm/pmu.c|267| <<amd_pmu_set_msr>> reprogram_gp_counter(pmc, data);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|454| <<intel_pmu_set_msr>> reprogram_gp_counter(pmc, data);
+ */
 void reprogram_gp_counter(struct kvm_pmc *pmc, u64 eventsel)
 {
 	unsigned config, type = PERF_TYPE_RAW;
@@ -272,6 +278,11 @@ void reprogram_fixed_counter(struct kvm_pmc *pmc, u8 ctrl, int idx)
 }
 EXPORT_SYMBOL_GPL(reprogram_fixed_counter);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|312| <<kvm_pmu_handle_event>> reprogram_counter(pmu, bit);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|68| <<global_ctrl_changed>> reprogram_counter(pmu, bit);
+ */
 void reprogram_counter(struct kvm_pmu *pmu, int pmc_idx)
 {
 	struct kvm_pmc *pmc = kvm_x86_ops.pmu_ops->pmc_idx_to_pmc(pmu, pmc_idx);
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index 5e8d8443154e..2dc8b636c99a 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -114,6 +114,16 @@ static void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|533| <<nested_vmcb02_prepare_control>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|302| <<set_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|317| <<clr_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|327| <<set_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|337| <<clr_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|346| <<svm_set_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|355| <<svm_clr_intercept>> recalc_intercepts(svm);
+ */
 void recalc_intercepts(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *c, *h, *g;
@@ -478,6 +488,11 @@ static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|575| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm);
+ *   - arch/x86/kvm/svm/nested.c|1343| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm);
+ */
 static void nested_vmcb02_prepare_control(struct vcpu_svm *svm)
 {
 	const u32 mask = V_INTR_MASKING_MASK | V_GIF_ENABLE_MASK | V_GIF_MASK;
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index aa0e7872fcc9..ce767d4ffabc 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -246,6 +246,12 @@ static inline bool cpu_has_vmx_pml(void)
 	return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_ENABLE_PML;
 }
 
+/*
+ * 在以下使用cpu_has_vmx_xsaves():
+ *   - arch/x86/kvm/vmx/vmx.c|4559| <<vmx_compute_secondary_exec_control>> if (cpu_has_vmx_xsaves()) {
+ *   - arch/x86/kvm/vmx/vmx.c|4683| <<init_vmcs>> if (cpu_has_vmx_xsaves())
+ *   - arch/x86/kvm/vmx/vmx.c|7574| <<vmx_set_cpu_caps>> if (!cpu_has_vmx_xsaves())
+ */
 static inline bool cpu_has_vmx_xsaves(void)
 {
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
@@ -377,6 +383,13 @@ static inline bool vmx_pt_mode_is_host_guest(void)
 	return pt_mode == PT_MODE_HOST_GUEST;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/capabilities.h|409| <<vmx_supported_debugctl>> if (vmx_get_perf_capabilities() & PMU_CAP_LBR_FMT)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|565| <<intel_pmu_init>> vcpu->arch.perf_capabilities = vmx_get_perf_capabilities();
+ *   - arch/x86/kvm/vmx/vmx.c|2060| <<vmx_get_msr_feature>> msr->data = vmx_get_perf_capabilities();
+ *   - arch/x86/kvm/vmx/vmx.c|2533| <<vmx_set_msr>> (vmx_get_perf_capabilities() & PMU_CAP_LBR_FMT))
+ */
 static inline u64 vmx_get_perf_capabilities(void)
 {
 	u64 perf_cap = 0;
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 6058a65a6ede..b2fd7c4da107 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -155,6 +155,17 @@ static int nested_vmx_succeed(struct kvm_vcpu *vcpu)
 	return kvm_skip_emulated_instruction(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|191| <<nested_vmx_fail>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3467| <<nested_vmx_run>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3471| <<nested_vmx_run>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3482| <<nested_vmx_run>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4895| <<handle_vmon>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4899| <<handle_vmon>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5032| <<handle_vmread>> return nested_vmx_failInvalid(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5124| <<handle_vmwrite>> return nested_vmx_failInvalid(vcpu);
+ */
 static int nested_vmx_failInvalid(struct kvm_vcpu *vcpu)
 {
 	vmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)
@@ -5196,6 +5207,11 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|5268| <<handle_vmptrld>> set_current_vmptr(vmx, vmptr);
+ *   - arch/x86/kvm/vmx/nested.c|6250| <<vmx_set_nested_state>> set_current_vmptr(vmx, kvm_state->hdr.vmx.vmcs12_pa);
+ */
 static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 {
 	vmx->nested.current_vmptr = vmptr;
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 9efc1a6b8693..b12809536894 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -35,6 +35,10 @@ static struct kvm_event_hw_type_mapping intel_arch_events[] = {
 /* mapping between fixed pmc index and intel_arch_events array */
 static int fixed_pmc_events[] = {1, 0, 7};
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|404| <<intel_pmu_set_msr>> reprogram_fixed_counters(pmu, data);
+ */
 static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 {
 	int i;
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index 5f81ef092bd4..2d6255711aea 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -13,6 +13,13 @@
  * We maintain a per-CPU linked-list of vCPU, so in wakeup_handler() we
  * can find which vCPU should be waken up.
  */
+/*
+ * 在以下使用percpu的blocked_vcpu_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|16| <<global>> static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|215| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/posted_intr.c|227| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
 static DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);
 
@@ -21,6 +28,25 @@ static inline struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)
 	return &(to_vmx(vcpu)->pi_desc);
 }
 
+/*
+ * vmx_vcpu_load
+ * kvm_sched_in
+ * __fire_sched_in_preempt_notifiers
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1607| <<vmx_vcpu_load>> vmx_vcpu_pi_load(vcpu, cpu);
+ */
 void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);
@@ -151,6 +177,13 @@ int pi_pre_block(struct kvm_vcpu *vcpu)
 	if (!WARN_ON_ONCE(vcpu->pre_pcpu != -1)) {
 		vcpu->pre_pcpu = vcpu->cpu;
 		spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));
+		/*
+		 * 在以下使用percpu的blocked_vcpu_on_cpu:
+		 *   - arch/x86/kvm/vmx/posted_intr.c|16| <<global>> static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
+		 *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+		 *   - arch/x86/kvm/vmx/posted_intr.c|215| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/posted_intr.c|227| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+		 */
 		list_add_tail(&vcpu->blocked_vcpu_list,
 			      &per_cpu(blocked_vcpu_on_cpu,
 				       vcpu->pre_pcpu));
@@ -260,6 +293,15 @@ void vmx_pi_start_assignment(struct kvm *kvm)
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * 在以下使用pi_update_irte():
+ *   - arch/x86/kvm/vmx/vmx.c|7993| <<global>> .update_pi_irte = pi_update_irte,
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|13639| <<kvm_arch_irq_bypass_add_producer>> ret = static_call(kvm_x86_update_pi_irte)(irqfd->kvm,
+ *   - arch/x86/kvm/x86.c|13664| <<kvm_arch_irq_bypass_del_producer>> ret = static_call(kvm_x86_update_pi_irte)(irqfd->kvm, prod->irq, irqfd->gsi, 0);
+ *   - arch/x86/kvm/x86.c|13675| <<kvm_arch_update_irqfd_routing>> return static_call(kvm_x86_update_pi_irte)(kvm, host_irq, guest_irq, set);
+ */
 int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 		   bool set)
 {
@@ -321,6 +363,15 @@ int pi_update_irte(struct kvm *kvm, unsigned int host_irq, uint32_t guest_irq,
 			continue;
 		}
 
+		/*
+		 * 在以下使用vcpu_data->pi_desc_addr:
+		 *   - arch/x86/kvm/svm/avic.c|797| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+		 *   - arch/x86/kvm/svm/avic.c|898| <<svm_update_pi_irte>> vcpu_info.pi_desc_addr, set);
+		 *   - rch/x86/kvm/vmx/posted_intr.c|351| <<pi_update_irte>> vcpu_info.pi_desc_addr = __pa(&to_vmx(vcpu)->pi_desc);
+		 *   - arch/x86/kvm/vmx/posted_intr.c|355| <<pi_update_irte>> vcpu_info.vector, vcpu_info.pi_desc_addr, set);
+		 *   - drivers/iommu/intel/irq_remapping.c|1249| <<intel_ir_set_vcpu_affinity>> irte_pi.pda_l = (vcpu_pi_info->pi_desc_addr >>
+		 *   - drivers/iommu/intel/irq_remapping.c|1251| <<intel_ir_set_vcpu_affinity>> irte_pi.pda_h = (vcpu_pi_info->pi_desc_addr >> 32) &
+		 */
 		vcpu_info.pi_desc_addr = __pa(&to_vmx(vcpu)->pi_desc);
 		vcpu_info.vector = irq.vector;
 
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index 1472c6c376f7..c10002683851 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -64,7 +64,19 @@ struct loaded_vmcs {
 	bool hv_timer_soft_disabled;
 	/* Support for vnmi-less CPUs */
 	int soft_vnmi_blocked;
+	/*
+	 * 在以下使用loaded_vmcs->entry_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|6521| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->entry_time));
+	 *   - arch/x86/kvm/vmx/vmx.c|6689| <<vmx_vcpu_run>> vmx->loaded_vmcs->entry_time = ktime_get();
+	 */
 	ktime_t entry_time;
+	/*
+	 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|4632| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|4670| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6065| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
+	 *   - arch/x86/kvm/vmx/vmx.c|6519| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time +=
+	 */
 	s64 vnmi_blocked_time;
 	unsigned long *msr_bitmap;
 	struct list_head loaded_vmcss_on_cpu_link;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c2a779b688e6..7e91376cc3a9 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -76,31 +76,95 @@ static const struct x86_cpu_id vmx_cpu_id[] = {
 MODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);
 #endif
 
+/*
+ * 在以下使用enable_vpid:
+ *   - arch/x86/kvm/vmx/vmx.c|80| <<global>> module_param_named(vpid, enable_vpid, bool, 0444);
+ *   - arch/x86/kvm/vmx/nested.c|1177| <<nested_vmx_transition_tlb_flush>> if (!enable_vpid)
+ *   - arch/x86/kvm/vmx/nested.c|2198| <<prepare_vmcs02_early_rare>> if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/nested.c|6526| <<nested_vmx_setup_ctls_msrs>> if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/vmx.c|3146| <<vmx_flush_tlb_all>> } else if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/vmx.c|3879| <<allocate_vpid>> if (!enable_vpid)
+ *   - arch/x86/kvm/vmx/vmx.c|3893| <<free_vpid>> if (!enable_vpid || vpid == 0)
+ *   - arch/x86/kvm/vmx/vmx.c|7955| <<hardware_setup>> enable_vpid = 0;
+ *
+ * The original architecture for VMX operation required VMX transitions to flush the TLBs and paging-structure caches.
+ * This ensured that translations cached for the old linear-address space would not be used after the transition.
+ * Virtual-processor identifiers (VPIDs) introduce to VMX operation a facility by which a logical processor may cache
+ * information for multiple linear-address spaces. When VPIDs are used, VMX transitions may retain cached informa-
+ * tion and the logical processor switches to a different linear-address space.
+ */
 bool __read_mostly enable_vpid = 1;
 module_param_named(vpid, enable_vpid, bool, 0444);
 
 static bool __read_mostly enable_vnmi = 1;
 module_param_named(vnmi, enable_vnmi, bool, S_IRUGO);
 
+/*
+ * 在以下使用flexpriority_enabled:
+ *   - arch/x86/kvm/vmx/vmx.c|86| <<global>> module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6537| <<nested_vmx_setup_ctls_msrs>> if (flexpriority_enabled)
+ *   - arch/x86/kvm/vmx/vmx.c|731| <<cpu_need_virtualize_apic_accesses>> return flexpriority_enabled && lapic_in_kernel(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|736| <<report_flexpriority>> return flexpriority_enabled;
+ *   - arch/x86/kvm/vmx/vmx.c|6346| <<vmx_set_virtual_apic_mode>> if (!flexpriority_enabled &&
+ *   - arch/x86/kvm/vmx/vmx.c|6367| <<vmx_set_virtual_apic_mode>> if (flexpriority_enabled) {
+ *   - arch/x86/kvm/vmx/vmx.c|7936| <<hardware_setup>> flexpriority_enabled = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7946| <<hardware_setup>> if (!flexpriority_enabled)
+ *
+ * 参考cpu_has_vmx_flexpriority()
+ */
 bool __read_mostly flexpriority_enabled = 1;
 module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
 
 bool __read_mostly enable_ept = 1;
 module_param_named(ept, enable_ept, bool, S_IRUGO);
 
+/*
+ * This control determines whether guest software may run
+ * in unpaged protected mode or in real-address mode.
+ */
 bool __read_mostly enable_unrestricted_guest = 1;
 module_param_named(unrestricted_guest,
 			enable_unrestricted_guest, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_ept_ad_bits:
+ *   - arch/x86/kvm/vmx/vmx.c|96| <<global>> module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6501| <<nested_vmx_setup_ctls_msrs>> if (enable_ept_ad_bits) {
+ *   - arch/x86/kvm/vmx/vmx.c|3269| <<construct_eptp>> if (enable_ept_ad_bits &&
+ *   - arch/x86/kvm/vmx/vmx.c|7930| <<hardware_setup>> enable_ept_ad_bits = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7985| <<hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits,
+ *   - arch/x86/kvm/vmx/vmx.c|8002| <<hardware_setup>> if (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())
+ */
 bool __read_mostly enable_ept_ad_bits = 1;
 module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
 
+/*
+ * 在以下使用emulate_invalid_guest_state:
+ *   - arch/x86/kvm/vmx/vmx.c|99| <<global>> module_param(emulate_invalid_guest_state, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|1521| <<emulation_required>> return emulate_invalid_guest_state && !vmx_guest_state_valid(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2914| <<fix_pmode_seg>> if (!emulate_invalid_guest_state) {
+ *   - arch/x86/kvm/vmx/vmx.c|2977| <<fix_rmode_seg>> if (!emulate_invalid_guest_state) {
+ *   - arch/x86/kvm/vmx/vmx.c|6586| <<vmx_has_emulated_msr>> return enable_unrestricted_guest || emulate_invalid_guest_state;
+ */
 static bool __read_mostly emulate_invalid_guest_state = true;
 module_param(emulate_invalid_guest_state, bool, S_IRUGO);
 
+/*
+ * 在以下使用fasteoi:
+ *   - arch/x86/kvm/vmx/vmx.c|102| <<global>> module_param(fasteoi, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|5273| <<handle_apic_access>> if (likely(fasteoi)) {
+ */
 static bool __read_mostly fasteoi = 1;
 module_param(fasteoi, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_apicv:
+ *   - arch/x86/kvm/vmx/vmx.c|105| <<global>> module_param(enable_apicv, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6378| <<nested_vmx_setup_ctls_msrs>> (enable_apicv ? PIN_BASED_POSTED_INTR : 0);
+ *   - arch/x86/kvm/vmx/vmx.c|3901| <<vmx_msr_bitmap_mode>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7026| <<vmx_vm_init>> kvm_apicv_init(kvm, enable_apicv);
+ *   - arch/x86/kvm/vmx/vmx.c|7879| <<hardware_setup>> enable_apicv = 0;
+ */
 bool __read_mostly enable_apicv = 1;
 module_param(enable_apicv, bool, S_IRUGO);
 
@@ -112,9 +176,25 @@ module_param(enable_apicv, bool, S_IRUGO);
 static bool __read_mostly nested = 1;
 module_param(nested, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_pml (Page Modification Logging):
+ *   - arch/x86/kvm/vmx/vmx.c|116| <<global>> module_param_named(pml, enable_pml, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|2171| <<prepare_vmcs02_constant_state>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|4478| <<init_vmcs>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|5996| <<__vmx_handle_exit>> if (enable_pml && !is_guest_mode(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|6866| <<vmx_free_vcpu>> if (enable_pml)
+ *   - arch/x86/kvm/vmx/vmx.c|6892| <<vmx_create_vcpu>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|7912| <<hardware_setup>> enable_pml = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7914| <<hardware_setup>> if (!enable_pml)
+ */
 bool __read_mostly enable_pml = 1;
 module_param_named(pml, enable_pml, bool, S_IRUGO);
 
+/*
+ * 在以下使用dump_invalid_vmcs:
+ *   - arch/x86/kvm/vmx/vmx.c|119| <<global>> module_param(dump_invalid_vmcs, bool, 0644);
+ *   - arch/x86/kvm/vmx/vmx.c|5821| <<dump_vmcs>> if (!dump_invalid_vmcs) {
+ */
 static bool __read_mostly dump_invalid_vmcs = 0;
 module_param(dump_invalid_vmcs, bool, 0644);
 
@@ -124,12 +204,40 @@ module_param(dump_invalid_vmcs, bool, 0644);
 #define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL
 
 /* Guest_tsc -> host_tsc conversion requires 64-bit division.  */
+/*
+ * 在以下使用cpu_preemption_timer_multi:
+ *   - arch/x86/kvm/vmx/vmx.c|6649| <<vmx_update_hv_timer>> cpu_preemption_timer_multi);
+ *   - arch/x86/kvm/vmx/vmx.c|7490| <<vmx_set_hv_timer>> if (delta_tsc >> (cpu_preemption_timer_multi + 32))
+ *   - arch/x86/kvm/vmx/vmx.c|7925| <<hardware_setup>> cpu_preemption_timer_multi =
+ *   - arch/x86/kvm/vmx/vmx.c|7930| <<hardware_setup>> use_timer_freq >>= cpu_preemption_timer_multi;
+ */
 static int __read_mostly cpu_preemption_timer_multi;
+/*
+ * 在以下使用enable_preemption_timer:
+ *   - arch/x86/kvm/vmx/vmx.c|130| <<global>> module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|4199| <<vmx_pin_based_exec_ctrl>> if (!enable_preemption_timer)
+ *   - arch/x86/kvm/vmx/vmx.c|6764| <<vmx_vcpu_run>> if (enable_preemption_timer)
+ *   - arch/x86/kvm/vmx/vmx.c|7918| <<hardware_setup>> enable_preemption_timer = false;
+ *   - arch/x86/kvm/vmx/vmx.c|7920| <<hardware_setup>> if (enable_preemption_timer) {
+ *   - arch/x86/kvm/vmx/vmx.c|7938| <<hardware_setup>> enable_preemption_timer = false;
+ *   - arch/x86/kvm/vmx/vmx.c|7941| <<hardware_setup>> if (!enable_preemption_timer) {
+ */
 static bool __read_mostly enable_preemption_timer = 1;
 #ifdef CONFIG_X86_64
 module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
 #endif
 
+/*
+ * 在以下使用allow_smaller_maxphyaddr:
+ *   - arch/x86/kvm/vmx/vmx.c|134| <<global>> module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|209| <<global>> bool __read_mostly allow_smaller_maxphyaddr = 0;
+ *   - arch/x86/kvm/svm/svm.c|1057| <<svm_hardware_setup>> allow_smaller_maxphyaddr = !npt_enabled;
+ *   - arch/x86/kvm/vmx/vmx.c|4923| <<handle_exception_nmi>> WARN_ON_ONCE(!allow_smaller_maxphyaddr);
+ *   - arch/x86/kvm/vmx/vmx.c|5420| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/vmx/vmx.c|8104| <<vmx_init>> allow_smaller_maxphyaddr = true;
+ *   - arch/x86/kvm/vmx/vmx.h|578| <<vmx_need_pf_intercept>> return allow_smaller_maxphyaddr && cpuid_maxphyaddr(vcpu) < boot_cpu_data.x86_phys_bits;
+ *   - arch/x86/kvm/x86.c|3992| <<kvm_vm_ioctl_check_extension>> r = (int ) allow_smaller_maxphyaddr;
+ */
 extern bool __read_mostly allow_smaller_maxphyaddr;
 module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
 
@@ -154,6 +262,13 @@ module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
  * List of MSRs that can be directly passed to the guest.
  * In addition to these x2apic and PT MSRs are handled specially.
  */
+/*
+ * 在以下使用vmx_possible_passthrough_msrs[]:
+ *   - arch/x86/kvm/vmx/vmx.c|823| <<possible_passthrough_msr_slot>> for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++)
+ *   - arch/x86/kvm/vmx/vmx.c|824| <<possible_passthrough_msr_slot>> if (vmx_possible_passthrough_msrs[i] == msr)
+ *   - arch/x86/kvm/vmx/vmx.c|4223| <<vmx_msr_filter_changed>> for (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {
+ *   - arch/x86/kvm/vmx/vmx.c|4224| <<vmx_msr_filter_changed>> u32 msr = vmx_possible_passthrough_msrs[i];
+ */
 static u32 vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS] = {
 	MSR_IA32_SPEC_CTRL,
 	MSR_IA32_PRED_CMD,
@@ -190,26 +305,77 @@ static unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;
 module_param(ple_window, uint, 0444);
 
 /* Default doubles per-vcpu window every exit. */
+/*
+ * 在以下使用ple_window_grow:
+ *   - arch/x86/kvm/vmx/vmx.c|194| <<global>> module_param(ple_window_grow, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5508| <<grow_ple_window>> ple_window_grow,
+ *   - arch/x86/kvm/vmx/vmx.c|7873| <<hardware_setup>> ple_window_grow = 0;
+ */
 static unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;
 module_param(ple_window_grow, uint, 0444);
 
 /* Default resets per-vcpu window every exit to ple_window. */
+/*
+ * 在以下使用ple_window_shrink:
+ *   - arch/x86/kvm/vmx/vmx.c|198| <<global>> module_param(ple_window_shrink, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5524| <<shrink_ple_window>> ple_window_shrink,
+ *   - arch/x86/kvm/vmx/vmx.c|7875| <<hardware_setup>> ple_window_shrink = 0;
+ */
 static unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;
 module_param(ple_window_shrink, uint, 0444);
 
 /* Default is to compute the maximum so we can never overflow. */
+/*
+ * 在以下使用ple_window_max:
+ *   - arch/x86/kvm/vmx/vmx.c|202| <<global>> module_param(ple_window_max, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5509| <<grow_ple_window>> ple_window_max);
+ *   - arch/x86/kvm/vmx/vmx.c|7874| <<hardware_setup>> ple_window_max = 0;
+ */
 static unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
 module_param(ple_window_max, uint, 0444);
 
 /* Default is SYSTEM mode, 1 for host-guest mode */
+/*
+ * Processor Trace virtualization can be work in one of 3 possible
+ * modes by set new option "pt_mode". Default value is system mode.
+ * a. system-wide: trace both host/guest and output to host buffer;
+ * b. host-only: only trace host and output to host buffer;
+ * c. host-guest: trace host/guest simultaneous and output to their respective buffer.
+ *
+ * KVM currently only supports (a) and (c).
+ *
+ * 在以下使用pt_mode (Processor Trace !!!):
+ *   - arch/x86/kvm/vmx/vmx.c|206| <<global>> module_param(pt_mode, int , S_IRUGO);
+ *   - arch/x86/kvm/vmx/capabilities.h|373| <<vmx_pt_mode_is_system>> return pt_mode == PT_MODE_SYSTEM;
+ *   - arch/x86/kvm/vmx/capabilities.h|377| <<vmx_pt_mode_is_host_guest>> return pt_mode == PT_MODE_HOST_GUEST;
+ *   - arch/x86/kvm/vmx/vmx.c|7951| <<hardware_setup>> if (pt_mode != PT_MODE_SYSTEM && pt_mode != PT_MODE_HOST_GUEST)
+ *   - arch/x86/kvm/vmx/vmx.c|7954| <<hardware_setup>> pt_mode = PT_MODE_SYSTEM;
+ */
 int __read_mostly pt_mode = PT_MODE_SYSTEM;
 module_param(pt_mode, int, S_IRUGO);
 
+/*
+ * 在以下使用vmx_l1d_should_flush:
+ *   - arch/x86/kvm/vmx/vmx.c|300| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|6665| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+/*
+ * 在以下使用vmx_l1d_flush_cond:
+ *   - arch/x86/kvm/vmx/vmx.c|453| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|455| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|6321| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
 static DEFINE_MUTEX(vmx_l1d_flush_mutex);
 
 /* Storage for pre module init parameter parsing */
+/*
+ * 在以下使用vmentry_l1d_flush_param:
+ *   - arch/x86/kvm/vmx/vmx.c|507| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|8242| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
 
 static const struct {
@@ -225,6 +391,16 @@ static const struct {
 };
 
 #define L1D_CACHE_ORDER 4
+/*
+ * 在以下使用vmx_l1d_flush_pages:
+ *   - arch/x86/kvm/vmx/vmx.c|423| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ *   - arch/x86/kvm/vmx/vmx.c|432| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+ *   - arch/x86/kvm/vmx/vmx.c|440| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1,
+ *   - arch/x86/kvm/vmx/vmx.c|6368| <<vmx_l1d_flush>> :: [flush_pages] "r" (vmx_l1d_flush_pages),
+ *   - arch/x86/kvm/vmx/vmx.c|8128| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+ *   - arch/x86/kvm/vmx/vmx.c|8129| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ *   - arch/x86/kvm/vmx/vmx.c|8130| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+ */
 static void *vmx_l1d_flush_pages;
 
 static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
@@ -368,6 +544,15 @@ static u32 vmx_segment_access_rights(struct kvm_segment *var);
 
 void vmx_vmexit(void);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|551| <<vmread_error>> vmx_insn_failed("kvm: vmread failed: field=%lx\n", field);
+ *   - arch/x86/kvm/vmx/vmx.c|556| <<vmwrite_error>> vmx_insn_failed("kvm: vmwrite failed: field=%lx val=%lx err=%d\n",
+ *   - arch/x86/kvm/vmx/vmx.c|562| <<vmclear_error>> vmx_insn_failed("kvm: vmclear failed: %p/%llx\n", vmcs, phys_addr);
+ *   - arch/x86/kvm/vmx/vmx.c|567| <<vmptrld_error>> vmx_insn_failed("kvm: vmptrld failed: %p/%llx\n", vmcs, phys_addr);
+ *   - arch/x86/kvm/vmx/vmx.c|572| <<invvpid_error>> vmx_insn_failed("kvm: invvpid failed: ext=0x%lx vpid=%u gva=0x%lx\n",
+ *   - arch/x86/kvm/vmx/vmx.c|578| <<invept_error>> vmx_insn_failed("kvm: invept failed: ext=0x%lx eptp=%llx gpa=0x%llx\n",
+ */
 #define vmx_insn_failed(fmt...)		\
 do {					\
 	WARN_ONCE(1, fmt);		\
@@ -411,6 +596,14 @@ noinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)
 }
 
 static DEFINE_PER_CPU(struct vmcs *, vmxarea);
+/*
+ * 在以下使用loaded_vmcss_on_cpu:
+ *   - arch/x86/kvm/vmx/vmx.c|583| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|883| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|1516| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+ *   - arch/x86/kvm/vmx/vmx.c|2607| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|8249| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+ */
 DEFINE_PER_CPU(struct vmcs *, current_vmcs);
 /*
  * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed
@@ -432,6 +625,16 @@ struct vmx_capability vmx_capability;
 		.ar_bytes = GUEST_##seg##_AR_BYTES,	   	\
 	}
 
+/*
+ * 在以下使用kvm_vmx_segment_fields[]:
+ *   - arch/x86/kvm/vmx/vmx.c|980| <<vmx_read_guest_seg_selector>> *p = vmcs_read16(kvm_vmx_segment_fields[seg].selector);
+ *   - arch/x86/kvm/vmx/vmx.c|989| <<vmx_read_guest_seg_base>> *p = vmcs_readl(kvm_vmx_segment_fields[seg].base);
+ *   - arch/x86/kvm/vmx/vmx.c|998| <<vmx_read_guest_seg_limit>> *p = vmcs_read32(kvm_vmx_segment_fields[seg].limit);
+ *   - arch/x86/kvm/vmx/vmx.c|1007| <<vmx_read_guest_seg_ar>> *p = vmcs_read32(kvm_vmx_segment_fields[seg].ar_bytes);
+ *   - arch/x86/kvm/vmx/vmx.c|3090| <<fix_rmode_seg>> const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
+ *   - arch/x86/kvm/vmx/vmx.c|3593| <<vmx_set_segment>> const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
+ *   - arch/x86/kvm/vmx/vmx.c|3920| <<seg_setup>> const struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];
+ */
 static const struct kvm_vmx_segment_field {
 	unsigned selector;
 	unsigned base;
@@ -586,6 +789,11 @@ static void hv_track_root_ept(struct kvm_vcpu *vcpu, hpa_t root_ept)
  * Refer from
  * https://www.virtualbox.org/svn/vbox/trunk/src/VBox/VMM/VMMR0/HMR0.cpp
  */
+/*
+ * 在以下使用vmx_preemption_cpu_tfms[]:
+ *   - arch/x86/kvm/vmx/vmx.c|802| <<cpu_has_broken_vmx_preemption_timer>> for (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)
+ *   - arch/x86/kvm/vmx/vmx.c|803| <<cpu_has_broken_vmx_preemption_timer>> if (eax == vmx_preemption_cpu_tfms[i])
+ */
 static u32 vmx_preemption_cpu_tfms[] = {
 /* 323344.pdf - BA86   - D0 - Xeon 7500 Series */
 0x000206E6,
@@ -616,6 +824,10 @@ static u32 vmx_preemption_cpu_tfms[] = {
 0x000306A8,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|2829| <<setup_vmcs_config>> if (cpu_has_broken_vmx_preemption_timer())
+ */
 static inline bool cpu_has_broken_vmx_preemption_timer(void)
 {
 	u32 eax = cpuid_eax(0x00000001), i;
@@ -682,6 +894,16 @@ static bool is_valid_passthrough_msr(u32 msr)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4307| <<nested_vmx_get_vmcs01_guest_efer>> efer_msr = vmx_find_uret_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|1923| <<vmx_setup_uret_msr>> uret_msr = vmx_find_uret_msr(vmx, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2175| <<vmx_get_msr>> msr = vmx_find_uret_msr(vmx, msr_info->index);
+ *   - arch/x86/kvm/vmx/vmx.c|2503| <<vmx_set_msr>> msr = vmx_find_uret_msr(vmx, msr_index);
+ *   - arch/x86/kvm/vmx/vmx.c|3133| <<vmx_set_efer>> struct vmx_uret_msr *msr = vmx_find_uret_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|7098| <<vmx_create_vcpu>> tsx_ctrl = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7474| <<vmx_vcpu_after_set_cpuid>> msr = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ */
 struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)
 {
 	int i;
@@ -711,6 +933,13 @@ static int vmx_set_guest_uret_msr(struct vcpu_vmx *vmx,
 }
 
 #ifdef CONFIG_KEXEC_CORE
+/*
+ * 在以下使用crash_vmclear_local_loaded_vmcss():
+ *   - arch/x86/kvm/vmx/vmx.c|8399| <<vmx_init>> crash_vmclear_local_loaded_vmcss);
+ *
+ * 8439         rcu_assign_pointer(crash_vmclear_loaded_vmcss,
+ * 8440                            crash_vmclear_local_loaded_vmcss);
+ */
 static void crash_vmclear_local_loaded_vmcss(void)
 {
 	int cpu = raw_smp_processor_id();
@@ -722,6 +951,11 @@ static void crash_vmclear_local_loaded_vmcss(void)
 }
 #endif /* CONFIG_KEXEC_CORE */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|947| <<loaded_vmcs_clear>> __loaded_vmcs_clear, loaded_vmcs, 1);
+ *   - arch/x86/kvm/vmx/vmx.c|2636| <<vmclear_local_loaded_vmcss>> __loaded_vmcs_clear(v);
+ */
 static void __loaded_vmcs_clear(void *arg)
 {
 	struct loaded_vmcs *loaded_vmcs = arg;
@@ -751,6 +985,11 @@ static void __loaded_vmcs_clear(void *arg)
 	loaded_vmcs->launched = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1527| <<vmx_vcpu_load_vmcs>> loaded_vmcs_clear(vmx->loaded_vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|2915| <<free_loaded_vmcs>> loaded_vmcs_clear(loaded_vmcs);
+ */
 void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs)
 {
 	int cpu = loaded_vmcs->cpu;
@@ -775,6 +1014,12 @@ static bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|3519| <<vmx_get_segment>> || var->selector == vmx_read_guest_seg_selector(vmx, seg))
+ *   - arch/x86/kvm/vmx/vmx.c|3522| <<vmx_get_segment>> var->selector = vmx_read_guest_seg_selector(vmx, seg);
+ *   - arch/x86/kvm/vmx/vmx.c|3527| <<vmx_get_segment>> var->selector = vmx_read_guest_seg_selector(vmx, seg);
+ */
 static u16 vmx_read_guest_seg_selector(struct vcpu_vmx *vmx, unsigned seg)
 {
 	u16 *p = &vmx->segment_cache.seg[seg].selector;
@@ -811,6 +1056,15 @@ static u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)
 	return *p;
 }
 
+/*
+ * 在以下使用vmx_update_exception_bitmap():
+ *   - struct kvm_x86_ops vmx_x86_ops.update_exception_bitmap = vmx_update_exception_bitmap,
+ *   - arch/x86/kvm/vmx/nested.c|2534| <<prepare_vmcs02>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3126| <<enter_pmode>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|3206| <<enter_rmode>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4865| <<vmx_vcpu_reset>> vmx_update_exception_bitmap(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7633| <<vmx_vcpu_after_set_cpuid>> vmx_update_exception_bitmap(vcpu);
+ */
 void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
 {
 	u32 eb;
@@ -860,6 +1114,10 @@ void vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)
 /*
  * Check if MSR is intercepted for currently loaded MSR bitmap.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7115| <<vmx_vcpu_run>> if (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))
+ */
 static bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)
 {
 	unsigned long *msr_bitmap;
@@ -1138,6 +1396,10 @@ static inline void pt_save_msr(struct pt_ctx *ctx, u32 addr_range)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_vcpu_run>> pt_guest_enter(vmx);
+ */
 static void pt_guest_enter(struct vcpu_vmx *vmx)
 {
 	if (vmx_pt_mode_is_system())
@@ -1401,6 +1663,22 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
  * Switches to specified vcpu, until a matching vcpu_put(), but assumes
  * vcpu mutex is already taken.
  */
+/*
+ * vmx_vcpu_load
+ * kvm_sched_in
+ * __fire_sched_in_preempt_notifiers
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1680,6 +1958,14 @@ static int vmx_skip_emulated_instruction(struct kvm_vcpu *vcpu)
 	return skip_emulated_instruction(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1730| <<vmx_queue_exception>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4571| <<vmx_vcpu_reset>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4615| <<vmx_inject_irq>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4646| <<vmx_inject_nmi>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_enter_smm>> vmx_clear_hlt(vcpu);
+ */
 static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -2026,6 +2312,10 @@ static u64 vcpu_supported_debugctl(struct kvm_vcpu *vcpu)
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * 在以下也会间接调用:
+ *   - arch/x86/kvm/x86.c|1840| <<__kvm_set_msr>> return static_call(kvm_x86_set_msr)(vcpu, &msr);
+ */
 static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3050,6 +3340,11 @@ static void vmx_flush_tlb_guest(struct kvm_vcpu *vcpu)
 	 * disabled (VM-Enter with vpid enabled and vpid==0 is disallowed),
 	 * i.e. no explicit INVVPID is necessary.
 	 */
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> int vpid;
+	 */
 	vpid_sync_context(to_vmx(vcpu)->vpid);
 }
 
@@ -3278,6 +3573,34 @@ void vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 		kvm_update_cpuid_runtime(vcpu);
 }
 
+/*
+ * 在以下使用vmx_get_segment():
+ *   - arch/x86/kvm/vmx/vmx.c|7987| <<global>> .get_segment = vmx_get_segment,
+ *   - arch/x86/kvm/vmx/nested.c|4645| <<get_vmx_mem_address>> vmx_get_segment(vcpu, &s, seg_reg);
+ *   - arch/x86/kvm/vmx/sgx.c|32| <<sgx_get_encls_gva>> vmx_get_segment(vcpu, &s, VCPU_SREG_DS);
+ *   - arch/x86/kvm/vmx/vmx.c|3107| <<enter_pmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
+ *   - arch/x86/kvm/vmx/vmx.c|3108| <<enter_pmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
+ *   - arch/x86/kvm/vmx/vmx.c|3109| <<enter_pmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
+ *   - arch/x86/kvm/vmx/vmx.c|3110| <<enter_pmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
+ *   - arch/x86/kvm/vmx/vmx.c|3111| <<enter_pmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
+ *   - arch/x86/kvm/vmx/vmx.c|3112| <<enter_pmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);
+ *   - arch/x86/kvm/vmx/vmx.c|3175| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);
+ *   - arch/x86/kvm/vmx/vmx.c|3176| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);
+ *   - arch/x86/kvm/vmx/vmx.c|3177| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);
+ *   - arch/x86/kvm/vmx/vmx.c|3178| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);
+ *   - arch/x86/kvm/vmx/vmx.c|3179| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);
+ *   - arch/x86/kvm/vmx/vmx.c|3180| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);
+ *   - arch/x86/kvm/vmx/vmx.c|3181| <<enter_rmode>> vmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);
+ *   - arch/x86/kvm/vmx/vmx.c|3600| <<vmx_get_segment_base>> vmx_get_segment(vcpu, &s, seg);
+ *   - arch/x86/kvm/vmx/vmx.c|3715| <<rmode_segment_valid>> vmx_get_segment(vcpu, &var, seg);
+ *   - arch/x86/kvm/vmx/vmx.c|3736| <<code_segment_valid>> vmx_get_segment(vcpu, &cs, VCPU_SREG_CS);
+ *   - arch/x86/kvm/vmx/vmx.c|3764| <<stack_segment_valid>> vmx_get_segment(vcpu, &ss, VCPU_SREG_SS);
+ *   - arch/x86/kvm/vmx/vmx.c|3786| <<data_segment_valid>> vmx_get_segment(vcpu, &var, seg);
+ *   - arch/x86/kvm/vmx/vmx.c|3810| <<tr_valid>> vmx_get_segment(vcpu, &tr, VCPU_SREG_TR);
+ *   - arch/x86/kvm/vmx/vmx.c|3828| <<ldtr_valid>> vmx_get_segment(vcpu, &ldtr, VCPU_SREG_LDTR);
+ *   - arch/x86/kvm/vmx/vmx.c|3846| <<cs_ss_rpl_check>> vmx_get_segment(vcpu, &cs, VCPU_SREG_CS);
+ *   - arch/x86/kvm/vmx/vmx.c|3847| <<cs_ss_rpl_check>> vmx_get_segment(vcpu, &ss, VCPU_SREG_SS);
+ */
 void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3700,6 +4023,37 @@ static void seg_setup(int seg)
 	vmcs_write32(sf->ar_bytes, ar);
 }
 
+/*
+ * APICv consists of 4 parts:
+ *
+ * 1. virtualization of host->guest interrupts.
+ *
+ * That allows KVM to deliver interrupts to a vCPU without VMexit,
+ * interrupts that can be sent from say main qemu thread or from iothread,
+ * or from in-kernel timers, etc.
+ *
+ * As long as the sender runs on a different core, you get VMexit less interrupt.
+ * If enable_apicv is true, your cpu ought to have this.
+ *
+ * 2. virtualization of apic registers.
+ *
+ * That allows to avoid VM exits on some guest apic acceses like writing EOI
+ * register, and such. Very primitive support for this exits even without APIVc,
+ * called FlexPriority/TPR virtualization.
+ *
+ * 3. virtualization of IPIs (inter process interrupts)
+ * Intel currenlty only supports self-ipi, where a vCPU sends an interrupt to itself,
+ * and it seems that finally they on track to support ful IPI virtualization.
+ *
+ * 4. Delivery of interrupts from passed-through devices to VM through virtual apic.
+ * That feature apparently you don't have enabled. Its optional but still very nice to
+ * have.
+ */
+
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7326| <<vmx_create_vcpu>> err = alloc_apic_access_page(vcpu->kvm);
+ */
 static int alloc_apic_access_page(struct kvm *kvm)
 {
 	struct page *page;
@@ -3733,6 +4087,11 @@ static int alloc_apic_access_page(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4823| <<enter_vmx_operation>> vmx->nested.vpid02 = allocate_vpid();
+ *   - arch/x86/kvm/vmx/vmx.c|7264| <<vmx_create_vcpu>> vmx->vpid = allocate_vpid();
+ */
 int allocate_vpid(void)
 {
 	int vpid;
@@ -4003,6 +4362,11 @@ static void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)
 	vmx_update_msr_bitmap_x2apic(vcpu, vmx_msr_bitmap_mode(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4273| <<vmx_deliver_nested_posted_interrupt>> if (!kvm_vcpu_trigger_posted_interrupt(vcpu, true))
+ *   - arch/x86/kvm/vmx/vmx.c|4306| <<vmx_deliver_posted_interrupt>> !kvm_vcpu_trigger_posted_interrupt(vcpu, false))
+ */
 static inline bool kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,
 						     bool nested)
 {
@@ -4069,6 +4433,13 @@ static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
  * 2. If target vcpu isn't running(root mode), kick it to pick up the
  * interrupt from PIR in next vmentry.
  */
+/*
+ * struct kvm_x86_ops.deliver_posted_interrupt = vmx_deliver_posted_interrupt()
+ *
+ * 可在__apic_accept_irq()调用
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1618| <<__apic_accept_irq>> if (static_call(kvm_x86_deliver_posted_interrupt)(vcpu, vector)) {
+ */
 static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4208,6 +4579,11 @@ static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 		vmx_update_msr_bitmap(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|2244| <<prepare_vmcs02_early>> exec_control = vmx_exec_control(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|4744| <<init_vmcs>> exec_controls_set(vmx, vmx_exec_control(vmx));
+ */
 u32 vmx_exec_control(struct vcpu_vmx *vmx)
 {
 	u32 exec_control = vmcs_config.cpu_based_exec_ctrl;
@@ -4239,6 +4615,12 @@ u32 vmx_exec_control(struct vcpu_vmx *vmx)
  * instruction in the guest.  This is usually done based on whether or not a
  * feature has been exposed to the guest in order to correctly emulate faults.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4616| <<vmx_adjust_sec_exec_control>> vmx_adjust_secondary_exec_control(vmx, exec_control, \
+ *   - arch/x86/kvm/vmx/vmx.c|4681| <<vmx_compute_secondary_exec_control>> vmx_adjust_secondary_exec_control(vmx, &exec_control,
+ *   - arch/x86/kvm/vmx/vmx.c|4699| <<vmx_compute_secondary_exec_control>> vmx_adjust_secondary_exec_control(vmx, &exec_control,
+ */
 static inline void
 vmx_adjust_secondary_exec_control(struct vcpu_vmx *vmx, u32 *exec_control,
 				  u32 control, bool enabled, bool exiting)
@@ -4290,6 +4672,11 @@ vmx_adjust_secondary_exec_control(struct vcpu_vmx *vmx, u32 *exec_control,
 #define vmx_adjust_sec_exec_exiting(vmx, exec_control, lname, uname) \
 	vmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, uname##_EXITING, true)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4747| <<init_vmcs>> vmx_compute_secondary_exec_control(vmx);
+ *   - arch/x86/kvm/vmx/vmx.c|7640| <<vmx_vcpu_after_set_cpuid>> vmx_compute_secondary_exec_control(vmx);
+ */
 static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
 {
 	struct kvm_vcpu *vcpu = &vmx->vcpu;
@@ -4379,12 +4766,20 @@ static void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)
 	vmx->secondary_exec_control = exec_control;
 }
 
+/*
+ * 在以下使用VMX_XSS_EXIT_BITMAP:
+ *   - arch/x86/kvm/vmx/vmx.c|4684| <<init_vmcs>> vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);
+ */
 #define VMX_XSS_EXIT_BITMAP 0
 
 /*
  * Noting that the initialization of Guest-state Area of VMCS is in
  * vmx_vcpu_reset().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7178| <<vmx_create_vcpu>> init_vmcs(vmx);
+ */
 static void init_vmcs(struct vcpu_vmx *vmx)
 {
 	if (nested)
@@ -4643,6 +5038,14 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/vmx.c|1730| <<vmx_queue_exception>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4571| <<vmx_vcpu_reset>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4615| <<vmx_inject_irq>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4646| <<vmx_inject_nmi>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_enter_smm>> vmx_clear_hlt(vcpu);
+	 */
 	vmx_clear_hlt(vcpu);
 }
 
@@ -5432,6 +5835,10 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * calld by:
+ *   - arch/x86/kvm/vmx/vmx.c|6180| <<__vmx_handle_exit>> return handle_invalid_guest_state(vcpu);
+ */
 static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5646,6 +6053,13 @@ static int handle_bus_lock_vmexit(struct kvm_vcpu *vcpu)
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
  * to be done to userspace and return 0.
  */
+/*
+ * 在以下使用kvm_vmx_exit_handlers[]:
+ *   - arch/x86/kvm/vmx/vmx.c|5883| <<global>> static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
+ *   - arch/x86/kvm/vmx/vmx.c|6336| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+ *   - arch/x86/kvm/vmx/vmx.c|6339| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8191| <<hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,
 	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
@@ -5703,6 +6117,9 @@ static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 static const int kvm_vmx_max_exit_handlers =
 	ARRAY_SIZE(kvm_vmx_exit_handlers);
 
+/*
+ * struct kvm_x86_ops vmx_x86_ops.get_exit_info = vmx_get_exit_info()
+ */
 static void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2,
 			      u32 *intr_info, u32 *error_code)
 {
@@ -5956,6 +6373,10 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6363| <<vmx_handle_exit>> int ret = __vmx_handle_exit(vcpu, exit_fastpath);
+ */
 static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6146,6 +6567,10 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
  * information but as all relevant affected CPUs have 32KiB L1D cache size
  * there is no point in doing so.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6830| <<vmx_vcpu_enter_exit>> vmx_l1d_flush(vcpu);
+ */
 static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 {
 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
@@ -6321,6 +6746,10 @@ static void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6727| <<vmx_hwapic_irr_update>> vmx_set_rvi(max_irr);
+ */
 static void vmx_set_rvi(int vector)
 {
 	u16 status;
@@ -6338,6 +6767,11 @@ static void vmx_set_rvi(int vector)
 	}
 }
 
+/*
+ * 在以下使用vmx_hwapic_irr_update():
+ *   - struct kvm_x86_ops vmx_x86_ops.hwapic_irr_update = vmx_hwapic_irr_update()
+ *   - arch/x86/kvm/vmx/vmx.c|6727| <<vmx_sync_pir_to_irr>> vmx_hwapic_irr_update(vcpu, max_irr);
+ */
 static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 {
 	/*
@@ -6352,6 +6786,16 @@ static void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 		vmx_set_rvi(max_irr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1181| <<apic_has_interrupt_for_ppr>> highest_irr = static_call(kvm_x86_sync_pir_to_irr)(apic->vcpu);
+ *   - arch/x86/kvm/x86.c|5902| <<kvm_vcpu_ioctl_get_lapic>> static_call(kvm_x86_sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|10991| <<vcpu_scan_ioapic>> static_call(kvm_x86_sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|11255| <<vcpu_enter_guest>> static_call(kvm_x86_sync_pir_to_irr)(vcpu);
+ *   - arch/x86/kvm/x86.c|11310| <<vcpu_enter_guest>> static_call(kvm_x86_sync_pir_to_irr)(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.sync_pir_to_irr = vmx_sync_pir_to_irr()
+ */
 static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6366,6 +6810,9 @@ static int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)
 		 * But on x86 this is just a compiler barrier anyway.
 		 */
 		smp_mb__after_atomic();
+		/*
+		 * 应该是把pir给sync到irr去, 然后返回最大的irr
+		 */
 		max_irr_updated =
 			kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
 
@@ -6575,6 +7022,10 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7174| <<vmx_vcpu_run>> vmx_complete_interrupts(vmx);
+ */
 static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 {
 	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
@@ -6644,6 +7095,10 @@ void noinstr vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7089| <<vmx_vcpu_run>> return vmx_exit_handlers_fastpath(vcpu);
+ */
 static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 {
 	switch (to_vmx(vcpu)->exit_reason.basic) {
@@ -6848,6 +7303,12 @@ static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 	free_loaded_vmcs(vmx->loaded_vmcs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11852| <<kvm_arch_vcpu_create>> r = static_call(kvm_x86_vcpu_create)(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_create = vmx_create_vcpu()
+ */
 static int vmx_create_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vmx_uret_msr *tsx_ctrl;
@@ -7612,6 +8073,20 @@ static bool vmx_check_apicv_inhibit_reasons(ulong bit)
 	return supported & BIT(bit);
 }
 
+/*
+ * 在以下使用vmx_x86_ops:
+ *   - arch/x86/kvm/vmx/vmx.c|8311| <<global>> .runtime_ops = &vmx_x86_ops,
+ *   - arch/x86/kvm/vmx/vmx.c|8186| <<hardware_setup>> vmx_x86_ops.set_apic_access_page_addr = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8189| <<hardware_setup>> vmx_x86_ops.update_cr8_intercept = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8194| <<hardware_setup>> vmx_x86_ops.tlb_remote_flush = hv_remote_flush_tlb;
+ *   - arch/x86/kvm/vmx/vmx.c|8195| <<hardware_setup>> vmx_x86_ops.tlb_remote_flush_with_range =
+ *   - arch/x86/kvm/vmx/vmx.c|8210| <<hardware_setup>> vmx_x86_ops.sync_pir_to_irr = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8245| <<hardware_setup>> vmx_x86_ops.cpu_dirty_log_size = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|8272| <<hardware_setup>> vmx_x86_ops.set_hv_timer = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8273| <<hardware_setup>> vmx_x86_ops.cancel_hv_timer = NULL;
+ *   - arch/x86/kvm/vmx/vmx.c|8274| <<hardware_setup>> vmx_x86_ops.request_immediate_exit = __kvm_request_immediate_exit;
+ *   - arch/x86/kvm/vmx/vmx.c|8394| <<vmx_init>> vmx_x86_ops.enable_direct_tlbflush
+ */
 static struct kvm_x86_ops vmx_x86_ops __initdata = {
 	.hardware_unsetup = hardware_unsetup,
 
@@ -7745,6 +8220,10 @@ static struct kvm_x86_ops vmx_x86_ops __initdata = {
 	.vcpu_deliver_sipi_vector = kvm_vcpu_deliver_sipi_vector,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8001| <<hardware_setup>> vmx_setup_user_return_msrs();
+ */
 static __init void vmx_setup_user_return_msrs(void)
 {
 
@@ -7771,6 +8250,12 @@ static __init void vmx_setup_user_return_msrs(void)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10652| <<kvm_arch_hardware_setup>> r = ops->hardware_setup();
+ *
+ * struct kvm_x86_init_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
@@ -7944,6 +8429,10 @@ static __init int hardware_setup(void)
 	return r;
 }
 
+/*
+ * 在以下使用vmx_init_ops:
+ *   - arch/x86/kvm/vmx/vmx.c|8402| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 static struct kvm_x86_init_ops vmx_init_ops __initdata = {
 	.cpu_has_kvm_support = cpu_has_kvm_support,
 	.disabled_by_bios = vmx_disabled_by_bios,
@@ -7999,6 +8488,10 @@ static void vmx_exit(void)
 }
 module_exit(vmx_exit);
 
+/*
+ * 在以下使用vmx_init():
+ *   - arch/x86/kvm/vmx/vmx.c|8270| <<global>> module_init(vmx_init);
+ */
 static int __init vmx_init(void)
 {
 	int r, cpu;
@@ -8049,6 +8542,13 @@ static int __init vmx_init(void)
 	 * contain 'auto' which will be turned into the default 'cond'
 	 * mitigation mode.
 	 */
+	/*
+	 * 在以下使用vmentry_l1d_flush_param:
+	 *   - 
+	 *   在以下使用crash_vmclear_local_loaded_vmcss():
+	 *     - arch/x86/kvm/vmx/vmx.c|8399| <<vmx_init>> crash_vmclear_local_loaded_vmcss);arch/x86/kvm/vmx/vmx.c|507| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|8242| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+	 */
 	r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
 	if (r) {
 		vmx_exit();
@@ -8056,12 +8556,26 @@ static int __init vmx_init(void)
 	}
 
 	for_each_possible_cpu(cpu) {
+		/*
+		 * 在以下使用loaded_vmcss_on_cpu:
+		 *   - arch/x86/kvm/vmx/vmx.c|583| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|883| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/vmx.c|1516| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+		 *   - arch/x86/kvm/vmx/vmx.c|2607| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/vmx.c|8249| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+		 */
 		INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
 
 		pi_init_cpu(cpu);
 	}
 
 #ifdef CONFIG_KEXEC_CORE
+	/*
+	 * 在以下使用crash_vmclear_loaded_vmcss:
+	 *   - arch/x86/kernel/crash.c|67| <<cpu_crash_vmclear_loaded_vmcss>> do_vmclear_operation = rcu_dereference(crash_vmclear_loaded_vmcss);
+	 *   - arch/x86/kvm/vmx/vmx.c|8159| <<vmx_exit>> RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_init>> rcu_assign_pointer(crash_vmclear_loaded_vmcss, crash_vmclear_local_loaded_vmcss);
+	 */
 	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
 			   crash_vmclear_local_loaded_vmcss);
 #endif
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 16e4e457ba23..3888a1bd9a94 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -129,6 +129,27 @@ struct nested_vmx {
 	bool pml_full;
 
 	/* The guest-physical address of the current VMCS L1 keeps for L2 */
+	/*
+	 * 在以下设置nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx/nested.c|290| <<free_nested>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx/nested.c|1988| <<nested_vmx_handle_enlightened_vmptrld>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx/nested.c|4933| <<nested_release_vmcs12>> vmx->nested.current_vmptr = -1ull;
+	 *   - arch/x86/kvm/vmx/nested.c|5201| <<set_current_vmptr>> vmx->nested.current_vmptr = vmptr;
+	 *   - arch/x86/kvm/vmx/vmx.c|7179| <<vmx_create_vcpu>> vmx->nested.current_vmptr = -1ull;
+	 * 在以下使用nested_vmx->current_vmptr:
+	 *   - arch/x86/kvm/vmx/nested.c|190| <<nested_vmx_fail>> if (vmx->nested.current_vmptr == -1ull && !vmx->nested.hv_evmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|3470| <<nested_vmx_run>> if (CC(!vmx->nested.hv_evmcs && vmx->nested.current_vmptr == -1ull))
+	 *   - arch/x86/kvm/vmx/nested.c|4913| <<nested_release_vmcs12>> if (vmx->nested.current_vmptr == -1ull)
+	 *   - arch/x86/kvm/vmx/nested.c|4928| <<nested_release_vmcs12>> vmx->nested.current_vmptr >> PAGE_SHIFT,
+	 *   - arch/x86/kvm/vmx/nested.c|4983| <<handle_vmclear>> if (vmptr == vmx->nested.current_vmptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5029| <<handle_vmread>> if (vmx->nested.current_vmptr == -1ull ||
+	 *   - arch/x86/kvm/vmx/nested.c|5121| <<handle_vmwrite>> if (vmx->nested.current_vmptr == -1ull ||
+	 *   - arch/x86/kvm/vmx/nested.c|5234| <<handle_vmptrld>> if (vmx->nested.current_vmptr != vmptr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5279| <<handle_vmptrst>> gpa_t current_vmptr = to_vmx(vcpu)->nested.current_vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<handle_vmptrst>> r = kvm_write_guest_virt_system(vcpu, gva, (void *)&current_vmptr,
+	 *   - arch/x86/kvm/vmx/nested.c|6054| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmcs12_pa = vmx->nested.current_vmptr;
+	 *   - arch/x86/kvm/vmx/nested.h|65| <<vmx_has_valid_vmcs12>> return is_guest_mode(vcpu) || vmx->nested.current_vmptr != -1ull ||
+	 */
 	gpa_t current_vmptr;
 	/*
 	 * Cache of the guest's VMCS, existing outside of guest memory.
@@ -242,6 +263,27 @@ struct vcpu_vmx {
 
 	unsigned long         exit_qualification;
 	u32                   exit_intr_info;
+	/*
+	 * 在以下使用vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|4871| <<handle_exception_nmi>> vect_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<handle_task_switch>> idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5323| <<handle_task_switch>> idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5324| <<handle_task_switch>> type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5340| <<handle_task_switch>> if (vmx->idt_vectoring_info &
+	 *   - arch/x86/kvm/vmx/vmx.c|5384| <<handle_ept_violation>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5604| <<handle_pml_full>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5735| <<vmx_get_exit_info>> *info2 = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5985| <<__vmx_handle_exit>> u32 vectoring_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6513| <<vmx_recover_nmi_blocking>> idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6547| <<__vmx_complete_interrupts>> u32 idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|6555| <<__vmx_complete_interrupts>> idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6566| <<__vmx_complete_interrupts>> vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6567| <<__vmx_complete_interrupts>> type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6583| <<__vmx_complete_interrupts>> if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6602| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|6832| <<vmx_vcpu_run>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6844| <<vmx_vcpu_run>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -254,6 +296,19 @@ struct vcpu_vmx {
 	 * into hardware when running the guest.  guest_uret_msrs[] is resorted
 	 * whenever the number of "active" uret MSRs is modified.
 	 */
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs:
+	 *   - arch/x86/kvm/vmx/vmx.c|878| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|885| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1253| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1254| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1407| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1411| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1412| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7099| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].data = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7100| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 *   - arch/x86/kvm/vmx/vmx.c|7110| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].mask = ~(u64)TSX_CTRL_CPUID_CLEAR;
+	 */
 	struct vmx_uret_msr   guest_uret_msrs[MAX_NR_USER_RETURN_MSRS];
 	int                   nr_active_uret_msrs;
 	bool                  guest_uret_msrs_loaded;
@@ -311,6 +366,14 @@ struct vcpu_vmx {
 
 	/* Dynamic PLE window. */
 	unsigned int ple_window;
+	/*
+	 * 在以下使用vcpu_vmx->ple_window_dirty:
+	 *   - arch/x86/kvm/vmx/vmx.c|4423| <<init_vmcs>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|5490| <<grow_ple_window>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|5506| <<shrink_ple_window>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6698| <<vmx_vcpu_run>> if (vmx->ple_window_dirty) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6699| <<vmx_vcpu_run>> vmx->ple_window_dirty = false;
+	 */
 	bool ple_window_dirty;
 
 	bool req_immediate_exit;
diff --git a/arch/x86/kvm/vmx/vmx_ops.h b/arch/x86/kvm/vmx/vmx_ops.h
index 164b64f65a8f..3d80e9617b38 100644
--- a/arch/x86/kvm/vmx/vmx_ops.h
+++ b/arch/x86/kvm/vmx/vmx_ops.h
@@ -239,6 +239,18 @@ static __always_inline void vmcs_set_bits(unsigned long field, u32 mask)
 	__vmcs_writel(field, __vmcs_readl(field) | mask);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|293| <<free_nested>> vmcs_clear(vmx->vmcs01.shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1549| <<copy_shadow_to_vmcs12>> vmcs_clear(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1585| <<copy_vmcs12_to_shadow>> vmcs_clear(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|4783| <<alloc_shadow_vmcs>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5189| <<handle_vmwrite>> vmcs_clear(vmx->vmcs01.shadow_vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|908| <<crash_vmclear_local_loaded_vmcss>> vmcs_clear(v->vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|922| <<__loaded_vmcs_clear>> vmcs_clear(loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|924| <<__loaded_vmcs_clear>> vmcs_clear(loaded_vmcs->shadow_vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|2929| <<alloc_loaded_vmcs>> vmcs_clear(loaded_vmcs->vmcs);
+ */
 static inline void vmcs_clear(struct vmcs *vmcs)
 {
 	u64 phys_addr = __pa(vmcs);
@@ -246,6 +258,16 @@ static inline void vmcs_clear(struct vmcs *vmcs)
 	vmx_asm1(vmclear, "m"(phys_addr), vmcs, phys_addr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|1541| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1550| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1574| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1586| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5185| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5190| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|1546| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ */
 static inline void vmcs_load(struct vmcs *vmcs)
 {
 	u64 phys_addr = __pa(vmcs);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e0f4a46649d7..d3793ecf5979 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -125,6 +125,10 @@ EXPORT_STATIC_CALL_GPL(kvm_x86_get_cs_db_l_bits);
 EXPORT_STATIC_CALL_GPL(kvm_x86_cache_reg);
 EXPORT_STATIC_CALL_GPL(kvm_x86_tlb_flush_current);
 
+/*
+ * 在以下使用ignore_msrs:
+ *   - arch/x86/kvm/x86.c|415| <<kvm_msr_ignored_check>> if (ignore_msrs) {
+ */
 static bool __read_mostly ignore_msrs = 0;
 module_param(ignore_msrs, bool, S_IRUGO | S_IWUSR);
 
@@ -132,26 +136,105 @@ bool __read_mostly report_ignored_msrs = true;
 module_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);
 EXPORT_SYMBOL_GPL(report_ignored_msrs);
 
+/*
+ * 在以下使用min_timer_period_us:
+ *   - arch/x86/kvm/x86.c|136| <<global>> module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.h|348| <<global>> extern unsigned int min_timer_period_us;
+ *   - arch/x86/kvm/i8254.c|351| <<create_pit_timer>> s64 min_period = min_timer_period_us * 1000LL;
+ *   - arch/x86/kvm/lapic.c|1490| <<limit_periodic_timer_frequency>> s64 min_period = min_timer_period_us * 1000LL;
+ */
 unsigned int min_timer_period_us = 200;
 module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用kvmclock_periodic_sync:
+ *   - arch/x86/kvm/x86.c|139| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|3206| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+ *   - arch/x86/kvm/x86.c|10792| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+ */
 static bool __read_mostly kvmclock_periodic_sync = true;
 module_param(kvmclock_periodic_sync, bool, S_IRUGO);
 
+/*
+ * 在以下设置kvm_has_tsc_control:
+ *   - arch/x86/kvm/svm/svm.c|962| <<svm_hardware_setup>> kvm_has_tsc_control = true;
+ *   - arch/x86/kvm/vmx/vmx.c|8063| <<hardware_setup>> kvm_has_tsc_control = true;
+ * 在以下使用kvm_has_tsc_control:
+ *   - arch/x86/kvm/debugfs.c|56| <<kvm_arch_create_vcpu_debugfs>> if (kvm_has_tsc_control) {
+ *   - arch/x86/kvm/vmx/nested.c|2537| <<prepare_vmcs02>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/vmx/nested.c|4507| <<nested_vmx_vmexit>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/vmx/vmx.c|1572| <<vmx_vcpu_load_vmcs>> if (kvm_has_tsc_control &&
+ *   - arch/x86/kvm/x86.c|2244| <<set_tsc_khz>> if (!kvm_has_tsc_control) {
+ *   - arch/x86/kvm/x86.c|2975| <<kvm_guest_time_update>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/x86.c|4099| <<kvm_vm_ioctl_check_extension>> r = kvm_has_tsc_control;
+ *   - arch/x86/kvm/x86.c|5218| <<kvm_arch_vcpu_ioctl>> if (kvm_has_tsc_control &&
+ *   - arch/x86/kvm/x86.c|10813| <<kvm_arch_hardware_setup>> if (kvm_has_tsc_control) {
+ *
+ * 旧的机器是false
+ * crash> kvm_has_tsc_control
+ * kvm_has_tsc_control = $1 = false
+ */
 bool __read_mostly kvm_has_tsc_control;
 EXPORT_SYMBOL_GPL(kvm_has_tsc_control);
+/*
+ * 在以下使用kvm_max_guest_tsc_khz:
+ *   - arch/x86/kvm/x86.c|6236| <<kvm_arch_vcpu_ioctl>> user_tsc_khz >= kvm_max_guest_tsc_khz)
+ *   - arch/x86/kvm/x86.c|9085| <<kvm_hyperv_tsc_notifier>> kvm_max_guest_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|11982| <<kvm_arch_hardware_setup>> kvm_max_guest_tsc_khz = max;
+ *
+ * 旧的机器是0
+ */
 u32  __read_mostly kvm_max_guest_tsc_khz;
 EXPORT_SYMBOL_GPL(kvm_max_guest_tsc_khz);
+/*
+ * 在以下设置kvm_tsc_scaling_ratio_frac_bits:
+ *   - arch/x86/kvm/svm/svm.c|964| <<svm_hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 32;
+ *   - arch/x86/kvm/vmx/vmx.c|8065| <<hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 48;
+ * 在以下使用kvm_tsc_scaling_ratio_frac_bits:
+ *   - arch/x86/kvm/x86.c|172| <<global>> EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
+ *   - arch/x86/kvm/debugfs.c|40| <<vcpu_get_tsc_scaling_frac_bits>> *val = kvm_tsc_scaling_ratio_frac_bits;
+ *   - arch/x86/kvm/vmx/vmx.c|7659| <<vmx_set_hv_timer>> kvm_tsc_scaling_ratio_frac_bits,
+ *   - arch/x86/kvm/x86.c|2501| <<set_tsc_khz>> ratio = mul_u64_u32_div(1ULL << kvm_tsc_scaling_ratio_frac_bits,
+ *   - arch/x86/kvm/x86.c|2719| <<__scale_tsc>> return mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);
+ *   - arch/x86/kvm/x86.c|11984| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+ *
+ * 4.14简单测试
+ * crash> kvm_tsc_scaling_ratio_frac_bits
+ * kvm_tsc_scaling_ratio_frac_bits = $6 = 0 '\000'
+ */
 u8   __read_mostly kvm_tsc_scaling_ratio_frac_bits;
 EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
 u64  __read_mostly kvm_max_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_max_tsc_scaling_ratio);
+/*
+ * 在以下设置kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/x86.c|11984| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+ * 在以下使用kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+ *   - arch/x86/kvm/x86.c|2474| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/x86.c|2545| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/x86.c|2743| <<kvm_scale_tsc>> if (ratio != kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|3216| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+ */
 u64 __read_mostly kvm_default_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_default_tsc_scaling_ratio);
+/*
+ * 在以下使用kvm_has_bus_lock_exit:
+ *   - arch/x86/kvm/vmx/vmx.c|8171| <<hardware_setup>> kvm_has_bus_lock_exit = cpu_has_vmx_bus_lock_detection();
+ *   - arch/x86/kvm/x86.c|5603| <<kvm_vm_ioctl_check_extension>> if (kvm_has_bus_lock_exit)
+ *   - arch/x86/kvm/x86.c|7179| <<kvm_vm_ioctl_enable_cap>> if (kvm_has_bus_lock_exit &&
+ */
 bool __read_mostly kvm_has_bus_lock_exit;
 EXPORT_SYMBOL_GPL(kvm_has_bus_lock_exit);
 
 /* tsc tolerance in parts per million - default to 1/2 of the NTP threshold */
+/*
+ * 在以下使用tsc_tolerance_ppm:
+ *   - arch/x86/kvm/x86.c|182| <<global>> module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.c|2578| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2579| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 __read_mostly tsc_tolerance_ppm = 250;
 module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
 
@@ -161,6 +244,11 @@ module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
  * advancement entirely.  Any other value is used as-is and disables adaptive
  * tuning, i.e. allows privileged userspace to set an exact advancement time.
  */
+/*
+ * 在以下使用lapic_timer_advance_ns:
+ *   - arch/x86/kvm/x86.c|191| <<global>> module_param(lapic_timer_advance_ns, int , S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.c|11627| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 static int __read_mostly lapic_timer_advance_ns = -1;
 module_param(lapic_timer_advance_ns, int, S_IRUGO | S_IWUSR);
 
@@ -174,6 +262,14 @@ EXPORT_SYMBOL_GPL(enable_vmware_backdoor);
 static bool __read_mostly force_emulation_prefix = false;
 module_param(force_emulation_prefix, bool, S_IRUGO);
 
+/*
+ * 在以下使用pi_inject_timer:
+ *   - arch/x86/kvm/x86.c|256| <<global>> module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.h|352| <<global>> extern int pi_inject_timer
+ *   - arch/x86/kvm/lapic.c|116| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
+ *   - arch/x86/kvm/x86.c|9858| <<kvm_arch_init>> if (pi_inject_timer == -1)
+ *   - arch/x86/kvm/x86.c|9859| <<kvm_arch_init>> pi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);
+ */
 int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
 
@@ -193,8 +289,58 @@ struct kvm_user_return_msrs {
 	} values[KVM_MAX_NR_USER_RETURN_MSRS];
 };
 
+/*
+ * commit 7e34fbd05c6350b30161be84d4f8954c9d321292
+ * Author: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Date:   Wed Sep 23 11:03:55 2020 -0700
+ *
+ * KVM: x86: Rename "shared_msrs" to "user_return_msrs"
+ *
+ * Rename the "shared_msrs" mechanism, which is used to defer restoring
+ * MSRs that are only consumed when running in userspace, to a more banal
+ * but less likely to be confusing "user_return_msrs".
+ *
+ * The "shared" nomenclature is confusing as it's not obvious who is
+ * sharing what, e.g. reasonable interpretations are that the guest value
+ * is shared by vCPUs in a VM, or that the MSR value is shared/common to
+ * guest and host, both of which are wrong.
+ *
+ * "shared" is also misleading as the MSR value (in hardware) is not
+ * guaranteed to be shared/reused between VMs (if that's indeed the correct
+ * interpretation of the name), as the ability to share values between VMs
+ * is simply a side effect (albiet a very nice side effect) of deferring
+ * restoration of the host value until returning from userspace.
+ *
+ * "user_return" avoids the above confusion by describing the mechanism
+ * itself instead of its effects.
+ *
+ * Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Message-Id: <20200923180409.32255-2-sean.j.christopherson@intel.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
+
+/*
+ * 在以下使用kvm_nr_uret_msrs:
+ *   - arch/x86/kvm/vmx/vmx.c|1396| <<vmx_prepare_switch_to_guest>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/vmx/vmx.c|7088| <<vmx_create_vcpu>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|416| <<kvm_on_user_return>> for (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {
+ *   - arch/x86/kvm/x86.c|442| <<kvm_add_user_return_msr>> BUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);
+ *   - arch/x86/kvm/x86.c|447| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|448| <<kvm_add_user_return_msr>> return kvm_nr_uret_msrs++;
+ *   - arch/x86/kvm/x86.c|456| <<kvm_find_user_return_msr>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|471| <<kvm_user_return_msr_cpu_online>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|9870| <<kvm_arch_init>> kvm_nr_uret_msrs = 0;
+ */
 u32 __read_mostly kvm_nr_uret_msrs;
 EXPORT_SYMBOL_GPL(kvm_nr_uret_msrs);
+/*
+ * 在以下使用kvm_uret_msrs_list:
+ *   - arch/x86/kvm/x86.c|461| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+ *   - arch/x86/kvm/x86.c|494| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|504| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+ *   - arch/x86/kvm/x86.c|519| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+ *   - arch/x86/kvm/x86.c|534| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+ */
 static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
 static struct kvm_user_return_msrs __percpu *user_return_msrs;
 
@@ -309,6 +455,11 @@ static inline void kvm_async_pf_hash_reset(struct kvm_vcpu *vcpu)
 		vcpu->arch.apf.gfns[i] = ~0;
 }
 
+/*
+ * 在以下使用kvm_on_user_return():
+ *   - arch/x86/kvm/x86.c|554| <<kvm_set_user_return_msr>> msrs->urn.on_user_return = kvm_on_user_return;
+ *   - arch/x86/kvm/x86.c|568| <<drop_user_return_notifiers>> kvm_on_user_return(&msrs->urn);
+ */
 static void kvm_on_user_return(struct user_return_notifier *urn)
 {
 	unsigned slot;
@@ -351,6 +502,11 @@ static int kvm_probe_user_return_msr(u32 msr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|967| <<svm_hardware_setup>> tsc_aux_uret_slot = kvm_add_user_return_msr(MSR_TSC_AUX);
+ *   - arch/x86/kvm/vmx/vmx.c|7983| <<vmx_setup_user_return_msrs>> kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
+ */
 int kvm_add_user_return_msr(u32 msr)
 {
 	BUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);
@@ -363,6 +519,12 @@ int kvm_add_user_return_msr(u32 msr)
 }
 EXPORT_SYMBOL_GPL(kvm_add_user_return_msr);
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/kvm_host.h|2258| <<kvm_is_supported_user_return_msr>> return kvm_find_user_return_msr(msr) >= 0;
+ *   - arch/x86/kvm/vmx/vmx.c|876| <<vmx_find_uret_msr>> i = kvm_find_user_return_msr(msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1244| <<update_transition_efer>> i = kvm_find_user_return_msr(MSR_EFER);
+ */
 int kvm_find_user_return_msr(u32 msr)
 {
 	int i;
@@ -389,6 +551,13 @@ static void kvm_user_return_msr_cpu_online(void)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1463| <<svm_prepare_guest_switch>> kvm_set_user_return_msr(tsc_aux_uret_slot, svm->tsc_aux, -1ull);
+ *   - arch/x86/kvm/svm/svm.c|2894| <<svm_set_msr>> r = kvm_set_user_return_msr(tsc_aux_uret_slot, data, -1ull);
+ *   - arch/x86/kvm/vmx/vmx.c|892| <<vmx_set_guest_uret_msr>> ret = kvm_set_user_return_msr(slot, msr->data, msr->mask);
+ *   - arch/x86/kvm/vmx/vmx.c|1420| <<vmx_prepare_switch_to_guest>> kvm_set_user_return_msr(i,
+ */
 int kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)
 {
 	unsigned int cpu = smp_processor_id();
@@ -433,6 +602,12 @@ enum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_get_apic_mode);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4685| <<vmx_vcpu_reset>> kvm_set_apic_base(vcpu, &apic_base_msr);
+ *   - arch/x86/kvm/x86.c|4453| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_set_apic_base(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|11491| <<__set_sregs>> if (kvm_set_apic_base(vcpu, &apic_base_msr))
+ */
 int kvm_set_apic_base(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	enum lapic_mode old_mode = kvm_get_apic_mode(vcpu);
@@ -455,9 +630,24 @@ int kvm_set_apic_base(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 }
 EXPORT_SYMBOL_GPL(kvm_set_apic_base);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm_ops.h|16| <<svm_asm>> kvm_spurious_fault(); \
+ *   - arch/x86/kvm/svm/svm_ops.h|26| <<svm_asm1>> kvm_spurious_fault(); \
+ *   - arch/x86/kvm/svm/svm_ops.h|36| <<svm_asm2>> kvm_spurious_fault(); \
+ *   - arch/x86/kvm/vmx/vmx.c|549| <<vmread_error>> kvm_spurious_fault();
+ *   - arch/x86/kvm/vmx/vmx.c|2670| <<hardware_disable>> kvm_spurious_fault();
+ *   - arch/x86/kvm/vmx/vmx_ops.h|158| <<vmx_asm1>> kvm_spurious_fault(); \
+ *   - arch/x86/kvm/vmx/vmx_ops.h|175| <<vmx_asm2>> kvm_spurious_fault(); \
+ */
 asmlinkage __visible noinstr void kvm_spurious_fault(void)
 {
 	/* Fault while not rebooting.  We want the trace. */
+	/*
+	 * 在以下使用kvm_rebooting:
+	 *   - arch/x86/kvm/x86.c|539| <<kvm_spurious_fault>> BUG_ON(!kvm_rebooting);
+	 *   - virt/kvm/kvm_main.c|4583| <<kvm_reboot>> kvm_rebooting = true;
+	 */
 	BUG_ON(!kvm_rebooting);
 }
 EXPORT_SYMBOL_GPL(kvm_spurious_fault);
@@ -508,6 +698,15 @@ static int exception_type(int vector)
 	return EXCPT_FAULT;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|1074| <<nested_svm_inject_exception_vmexit>> kvm_deliver_exception_payload(&svm->vcpu);
+ *   - arch/x86/kvm/svm/svm.c|366| <<svm_queue_exception>> kvm_deliver_exception_payload(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|1925| <<vmx_queue_exception>> kvm_deliver_exception_payload(vcpu);
+ *   - arch/x86/kvm/x86.c|764| <<kvm_multiple_exception>> kvm_deliver_exception_payload(vcpu);
+ *   - arch/x86/kvm/x86.c|6086| <<kvm_vcpu_ioctl_x86_get_vcpu_events>> kvm_deliver_exception_payload(vcpu);
+ *   - arch/x86/kvm/x86.c|10559| <<inject_pending_event>> kvm_deliver_exception_payload(vcpu);
+ */
 void kvm_deliver_exception_payload(struct kvm_vcpu *vcpu)
 {
 	unsigned nr = vcpu->arch.exception.nr;
@@ -637,6 +836,39 @@ static void kvm_multiple_exception(struct kvm_vcpu *vcpu,
 		goto queue;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1911| <<kvm_hv_hypercall>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/nested.c|617| <<nested_svm_vmrun>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/nested.c|842| <<nested_svm_vmexit>> kvm_queue_exception(&(svm->vcpu), DB_VECTOR);
+ *   - arch/x86/kvm/svm/nested.c|1032| <<nested_svm_check_permissions>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/svm.c|2314| <<skinit_interception>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/svm.c|2483| <<cr_interception>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/svm.c|2505| <<cr_interception>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/svm.c|2542| <<cr_trap>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/svm/svm.c|3002| <<invpcid_interception>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|3265| <<nested_vmx_check_permission>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|3475| <<nested_vmx_run>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|4630| <<get_vmx_mem_address>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|4875| <<handle_vmon>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|5343| <<handle_invept>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|5423| <<handle_invvpid>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/nested.c|5547| <<handle_vmfunc>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/sgx.c|380| <<handle_encls>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|1793| <<vmx_can_emulate_instruction>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|5069| <<handle_rmode_exception>> kvm_queue_exception(vcpu, vec);
+ *   - arch/x86/kvm/vmx/vmx.c|5809| <<handle_invpcid>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|5882| <<handle_vmx_instruction>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/vmx/vmx.c|5894| <<handle_encls>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/x86.c|915| <<kvm_require_dr>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/x86.c|2096| <<kvm_handle_invalid_op>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/x86.c|6052| <<kvm_vcpu_ioctl_x86_set_mce>> kvm_queue_exception(vcpu, MC_VECTOR);
+ *   - arch/x86/kvm/x86.c|8933| <<inject_emulated_exception>> kvm_queue_exception(vcpu, ctxt->exception.vector);
+ *   - arch/x86/kvm/x86.c|9036| <<handle_emulation_failure>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/x86.c|9371| <<x86_emulate_instruction>> kvm_queue_exception(vcpu, UD_VECTOR);
+ *   - arch/x86/kvm/x86.c|12116| <<kvm_arch_vcpu_ioctl_set_guest_debug>> kvm_queue_exception(vcpu, DB_VECTOR);
+ *   - arch/x86/kvm/x86.c|12118| <<kvm_arch_vcpu_ioctl_set_guest_debug>> kvm_queue_exception(vcpu, BP_VECTOR);
+ */
 void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr)
 {
 	kvm_multiple_exception(vcpu, nr, false, 0, false, 0, false);
@@ -674,6 +906,16 @@ int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)
 }
 EXPORT_SYMBOL_GPL(kvm_complete_insn_gp);
 
+/*
+ * 在以下使用kvm_inject_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4553| <<init_kvm_tdp_mmu>> context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|4748| <<init_kvm_softmmu>> context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|4786| <<init_kvm_nested_mmu>> g_context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/svm/nested.c|69| <<svm_inject_page_fault_nested>> kvm_inject_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/vmx/nested.c|449| <<vmx_inject_page_fault_nested>> kvm_inject_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/vmx/sgx.c|136| <<sgx_inject_fault>> kvm_inject_page_fault(vcpu, &ex);
+ *   - arch/x86/kvm/x86.c|13532| <<kvm_arch_async_page_not_present>> kvm_inject_page_fault(vcpu, &fault);
+ */
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 {
 	++vcpu->stat.pf_guest;
@@ -712,6 +954,11 @@ bool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1485| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5945| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	atomic_inc(&vcpu->arch.nmi_queued);
@@ -735,6 +982,10 @@ EXPORT_SYMBOL_GPL(kvm_requeue_exception_e);
  * Checks if cpl <= required_cpl; if true, return true.  Otherwise queue
  * a #GP and return false.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1339| <<kvm_emulate_cpuid>> if (cpuid_fault_enabled(vcpu) && !kvm_require_cpl(vcpu, 0))
+ */
 bool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl)
 {
 	if (static_call(kvm_x86_get_cpl)(vcpu) <= required_cpl)
@@ -793,6 +1044,16 @@ static inline u64 pdptr_rsvd_bits(struct kvm_vcpu *vcpu)
 /*
  * Load the pae pdptrs.  Return 1 if they are all valid, 0 otherwise.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|405| <<nested_svm_load_cr3>> if (CC(!load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3)))
+ *   - arch/x86/kvm/svm/svm.c|1526| <<svm_cache_reg>> load_pdptrs(vcpu, vcpu->arch.walk_mmu, kvm_read_cr3(vcpu));
+ *   - arch/x86/kvm/vmx/nested.c|1133| <<nested_vmx_load_cr3>> if (CC(!load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3))) {
+ *   - arch/x86/kvm/x86.c|1069| <<kvm_set_cr0>> !load_pdptrs(vcpu, vcpu->arch.walk_mmu, kvm_read_cr3(vcpu)))
+ *   - arch/x86/kvm/x86.c|1234| <<kvm_set_cr4>> && !load_pdptrs(vcpu, vcpu->arch.walk_mmu,
+ *   - arch/x86/kvm/x86.c|1283| <<kvm_set_cr3>> if (is_pae_paging(vcpu) && !load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3))
+ *   - arch/x86/kvm/x86.c|12047| <<__set_sregs>> load_pdptrs(vcpu, vcpu->arch.walk_mmu, kvm_read_cr3(vcpu));
+ */
 int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3)
 {
 	gfn_t pdpt_gfn = cr3 >> PAGE_SHIFT;
@@ -923,6 +1184,11 @@ void kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw)
 }
 EXPORT_SYMBOL_GPL(kvm_lmsw);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3775| <<svm_vcpu_run>> kvm_load_guest_xsave_state(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6986| <<vmx_vcpu_run>> kvm_load_guest_xsave_state(vcpu);
+ */
 void kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->arch.guest_state_protected)
@@ -1084,6 +1350,12 @@ int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 }
 EXPORT_SYMBOL_GPL(kvm_set_cr4);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2473| <<cr_interception>> err = kvm_set_cr3(vcpu, val);
+ *   - arch/x86/kvm/vmx/vmx.c|5343| <<handle_cr>> err = kvm_set_cr3(vcpu, val);
+ *   - arch/x86/kvm/x86.c|8607| <<emulator_set_cr>> res = kvm_set_cr3(vcpu, val);
+ */
 int kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)
 {
 	bool skip_tlb_flush = false;
@@ -1231,6 +1503,11 @@ void kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val)
 }
 EXPORT_SYMBOL_GPL(kvm_get_dr);
 
+/*
+ * 在以下使用kvm_emulate_rdpmc():
+ *   - arch/x86/kvm/svm/svm.c|3060| <<global>> [SVM_EXIT_RDPMC] = kvm_emulate_rdpmc,
+ *   - arch/x86/kvm/vmx/vmx.c|5933| <<global>> [EXIT_REASON_RDPMC] = kvm_emulate_rdpmc,
+ */
 int kvm_emulate_rdpmc(struct kvm_vcpu *vcpu)
 {
 	u32 ecx = kvm_rcx_read(vcpu);
@@ -1300,6 +1577,11 @@ static const u32 msrs_to_save_all[] = {
 	MSR_ARCH_PERFMON_EVENTSEL0 + 16, MSR_ARCH_PERFMON_EVENTSEL0 + 17,
 };
 
+/*
+ * 在以下使用msrs_to_save:
+ *   - arch/x86/kvm/x86.c|4034| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6062| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];
 static unsigned num_msrs_to_save;
 
@@ -1613,6 +1895,10 @@ EXPORT_SYMBOL_GPL(kvm_msr_allowed);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1773| <<kvm_set_msr_ignored_check>> int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
+ */
 static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 			 bool host_initiated)
 {
@@ -1678,9 +1964,26 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	return static_call(kvm_x86_set_msr)(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1841| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2094| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
+	/*
+	 * 在以下设置msr_data->host_initiated:
+	 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+	 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+	 *
+	 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+	 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+	 * msr_info->host_initiated就为false.
+	 */
 	int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
 
 	if (ret == KVM_MSR_RET_INVALID)
@@ -1747,6 +2050,15 @@ int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data)
 }
 EXPORT_SYMBOL_GPL(kvm_get_msr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|912| <<nested_vmx_load_msr>> if (kvm_set_msr(vcpu, e.index, e.value)) {
+ *   - arch/x86/kvm/vmx/nested.c|2601| <<prepare_vmcs02>> WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,
+ *   - arch/x86/kvm/vmx/nested.c|4231| <<load_vmcs12_host_state>> WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,
+ *   - arch/x86/kvm/vmx/nested.c|4407| <<nested_vmx_restore_host_state>> if (kvm_set_msr(vcpu, h.index, h.value)) {
+ *   - arch/x86/kvm/x86.c|1940| <<kvm_emulate_wrmsr>> r = kvm_set_msr(vcpu, ecx, data);
+ *   - arch/x86/kvm/x86.c|8390| <<emulator_set_msr>> r = kvm_set_msr(vcpu, msr_index, data);
+ */
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data)
 {
 	return kvm_set_msr_ignored_check(vcpu, index, data, false);
@@ -1902,6 +2214,11 @@ int kvm_emulate_monitor(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_monitor);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10574| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+ */
 static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
 {
 	xfer_to_guest_mode_prepare();
@@ -1916,6 +2233,10 @@ static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
  * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the
  * other cases which must be called after interrupts are enabled on the host.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2046| <<handle_fastpath_set_msr_irqoff>> if (!handle_fastpath_set_x2apic_icr_irqoff(vcpu, data)) {
+ */
 static int handle_fastpath_set_x2apic_icr_irqoff(struct kvm_vcpu *vcpu, u64 data)
 {
 	if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))
@@ -1927,6 +2248,11 @@ static int handle_fastpath_set_x2apic_icr_irqoff(struct kvm_vcpu *vcpu, u64 data
 		((u32)(data >> 32) != X2APIC_BROADCAST)) {
 
 		data &= ~(1 << 12);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|2095| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+		 *   - arch/x86/kvm/x86.c|2018| <<handle_fastpath_set_x2apic_icr_irqoff>> kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
+		 */
 		kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
 		kvm_lapic_set_reg(vcpu->arch.apic, APIC_ICR2, (u32)(data >> 32));
 		kvm_lapic_set_reg(vcpu->arch.apic, APIC_ICR, (u32)data);
@@ -1946,6 +2272,11 @@ static int handle_fastpath_set_tscdeadline(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3696| <<svm_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6852| <<vmx_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+ */
 fastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu)
 {
 	u32 msr = kvm_rcx_read(vcpu);
@@ -1986,6 +2317,10 @@ static int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 	return kvm_get_msr_ignored_check(vcpu, index, data, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6297| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+ */
 static int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
 	return kvm_set_msr_ignored_check(vcpu, index, *data, true);
@@ -1998,6 +2333,13 @@ struct pvclock_clock {
 	u64 mask;
 	u32 mult;
 	u32 shift;
+	/*
+	 * 在以下使用pvclock_clock->base_cycles:
+	 *   - arch/x86/kvm/x86.c|2224| <<update_pvclock_gtod>> vdata->clock.base_cycles = tk->tkr_mono.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|2232| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|3361| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+	 *   - arch/x86/kvm/x86.c|3385| <<do_realtime>> ns = gtod->clock.base_cycles;
+	 */
 	u64 base_cycles;
 	u64 offset;
 };
@@ -2005,6 +2347,13 @@ struct pvclock_clock {
 struct pvclock_gtod_data {
 	seqcount_t	seq;
 
+	/*
+	 * CLOCK_MONOTONIC和CLOCK_MONOTONIC_RAW
+	 *
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 */
 	struct pvclock_clock clock; /* extract of a clocksource struct */
 	struct pvclock_clock raw_clock; /* extract of a clocksource struct */
 
@@ -2012,15 +2361,144 @@ struct pvclock_gtod_data {
 	u64		wall_time_sec;
 };
 
+/*
+ * 这是4.14的例子
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $1 = {
+ *   seq = {
+ *     sequence = 507970306
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 10566905113766286,
+ *     mask = 18446744073709551615,
+ *     mult = 4945956,
+ *     shift = 24
+ *   },
+ *   boot_ns = 2220701952811493,
+ *   nsec_base = 334196164124419,
+ *   wall_time_sec = 1630097708
+ * }
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $4 = {
+ *   seq = {
+ *     sequence = 508028248
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 10567925239710761,
+ *     mask = 18446744073709551615,
+ *     mult = 4945954,
+ *     shift = 24
+ *   },
+ *   boot_ns = 2221001952811493,
+ *   nsec_base = 12666403808125002,
+ *   wall_time_sec = 1630098008
+ * }
+ *
+ * 这是5.4的例子
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $3 = {
+ *   seq = {
+ *     sequence = 43526056
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 11962450506088969,
+ *     mask = 18446744073709551615,
+ *     mult = 4945955,
+ *     shift = 24,
+ *     base_cycles = 6161127504543786,
+ *     offset = 240303143085771
+ *   },
+ *   raw_clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 11962450506088969,
+ *     mask = 18446744073709551615,
+ *     mult = 4945492,
+ *     shift = 24,
+ *     base_cycles = 14693326820319724,
+ *     offset = 240280000000000
+ *   },
+ *   offs_boot = 0,
+ *   wall_time_sec = 1630509117
+ * }
+ *
+ * 在以下使用pvclock_gtod_data:
+ *   - arch/x86/kvm/x86.c|2024| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2055| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - arch/x86/kvm/x86.c|2275| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2353| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+ *   - arch/x86/kvm/x86.c|2478| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+ *   - arch/x86/kvm/x86.c|2533| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2552| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2575| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2587| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2660| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+ *   - arch/x86/kvm/x86.c|8172| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ */
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * 4.14上的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8175| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
 
+	/*
+	 * start a seqcount_t write side critical section
+	 */
 	write_seqcount_begin(&vdata->seq);
 
 	/* copy pvclock gtod data */
+	/*
+	 * struct tk_read_base - base structure for timekeeping readout
+	 * @clock:      Current clocksource used for timekeeping.
+	 * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+	 * @cycle_last: @clock cycle value at last update
+	 * @mult:       (NTP adjusted) multiplier for scaled math conversion
+	 * @shift:      Shift value for scaled math conversion
+	 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+	 * @base:       ktime_t (nanoseconds) base time for readout
+	 * @base_real:  Nanoseconds base value for clock REALTIME readout
+	 *
+	 * struct timekeeper *tk:
+	 * -> struct tk_read_base tkr_mono; --> The readout base structure for CLOCK_MONOTONIC
+	 *    -> struct clocksource *clock;
+	 *       -> u64     mask;
+	 *       -> u64     cycle_last;
+	 *       -> u32     mult;
+	 *       -> u32     shift;
+	 *       -> u64     xtime_nsec;
+	 *       -> ktime_t base;
+	 *       -> u64     base_real;
+	 * -> struct tk_read_base tkr_raw; --> The readout base structure for CLOCK_MONOTONIC_RAW
+	 */
 	vdata->clock.vclock_mode	= tk->tkr_mono.clock->vdso_clock_mode;
 	vdata->clock.cycle_last		= tk->tkr_mono.cycle_last;
 	vdata->clock.mask		= tk->tkr_mono.mask;
@@ -2037,6 +2515,17 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	vdata->raw_clock.base_cycles	= tk->tkr_raw.xtime_nsec;
 	vdata->raw_clock.offset		= tk->tkr_raw.base;
 
+	/*
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 *
+	 * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+	 * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+	 * 每来一个timer interrupt,也需要去更新xtime.
+	 *
+	 * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+	 */
 	vdata->wall_time_sec            = tk->xtime_sec;
 
 	vdata->offs_boot		= tk->offs_boot;
@@ -2044,6 +2533,18 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	write_seqcount_end(&vdata->seq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2079| <<get_kvmclock_base_ns>> static s64 get_kvmclock_base_ns(void )
+ *   - arch/x86/kvm/x86.c|2391| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|2728| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2744| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2855| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|5949| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|10812| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+ *
+ * 应该是返回启动以后的ns
+ */
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Count up from boot time, but with the frequency of the raw clock.  */
@@ -2053,10 +2554,21 @@ static s64 get_kvmclock_base_ns(void)
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Master clock not used, so we can just use CLOCK_BOOTTIME.  */
+	/*
+	 * monotonic time since boot in ns format
+	 */
 	return ktime_get_boottime_ns();
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3317| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|3324| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_write_wall_clock(kvm, gpa + wc_ofs, sec_hi_ofs - wc_ofs);
+ *
+ * 根据分析, 这里分享给VM的wall clock是不变的 (就是启动的时间)
+ */
 void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2085,6 +2597,22 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 *
+	 * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+	 * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+	 * 每来一个timer interrupt,也需要去更新xtime.
+	 *
+	 * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+	 *
+	 * ktime_get_real_ns(): get the real (wall-) time in ns
+	 *
+	 * 这里的wall_nsec指的应该是VM启动的时候的时间
+	 * 所以可以用当前的KVM的ns减去VM已经运行了的ns
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -2103,26 +2631,81 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3473| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3479| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ *
+ * MSR_KVM_SYSTEM_TIME_NEW: old_msr是false
+ * MSR_KVM_SYSTEM_TIME    : old_msr是true
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->boot_vcpu_runs_old_kvmclock:
+		 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+		 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+		 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+		 *
+		 * 如果运行的是vcpu0,且是否使用旧的kvmclock msr与当前的
+		 * boot_vcpu_runs_old_kvmclock标志不一致,那么一定是出了
+		 * 一些什么问题,需要校准MASTERCLOCK,发出KVM_REQ_MASTERCLOCK_UPDATE
+		 * 请求
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
 		ka->boot_vcpu_runs_old_kvmclock = old_msr;
 	}
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> gpa_t time;
+	 *    -> struct gfn_to_hva_cache pv_time;
+	 */
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2179| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4339| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9358| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * kvm_gen_kvmclock_update()
+	 * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+	 * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	vcpu->arch.pv_time_enabled = false;
+	/*
+	 * 在guest的kvm_register_clock()会在最后一位设置1
+	 */
 	if (!(system_time & 1))
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2587| <<kvm_write_system_time>> &vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|4297| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
+	 *
+	 * 会把传过来的地址参数data记录到pv_time中.
+	 * 这样子就可以通过pv_time来直接修改Guest中的pv time
+	 */
 	if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
 				       &vcpu->arch.pv_time, system_time & ~1ULL,
 				       sizeof(struct pvclock_vcpu_time_info)))
@@ -2131,12 +2714,24 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2351| <<kvm_get_time_scale>> *pmultiplier = div_frac(scaled64, tps32);
+ */
 static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 {
 	do_shl32_div32(dividend, divisor);
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2415| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3515| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ *
+ * 这里应该是要计算pshift和pmultiplier来让base_hz能转换成scaled_hz
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2166,12 +2761,44 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|3308| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+ *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3453| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|8696| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|8711| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|8730| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ *
+ * 在4.14上是3392425, 对应dmesg
+ * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+/*
+ * 在以下使用max_tsc_khz:
+ *   - arch/x86/kvm/x86.c|8927| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|8938| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+ *   - arch/x86/kvm/x86.c|11328| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ *
+ * crash> max_tsc_khz
+ * max_tsc_khz = $4 = 3392426
+ */
 static unsigned long max_tsc_khz;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2447| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 {
 	u64 v = (u64)khz * (1000000 + ppm);
@@ -2179,12 +2806,34 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 	return v;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2452| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
 
 	/* Guest TSC same frequency as host TSC? */
 	if (!scale) {
+		/*
+		 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+		 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+		 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+		 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+		 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+		 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+		 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+		 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+		 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+		 *
+		 * 在普通测试上是0 (可能没有migration吧)
+		 */
 		vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
 		return 0;
 	}
@@ -2192,7 +2841,22 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	/* TSC scaling supported? */
 	if (!kvm_has_tsc_control) {
 		if (user_tsc_khz > tsc_khz) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+			 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+			 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+			 *
+			 * 如果在KVM_SET_TSC_KHZ时设置的vTSCfreq > pTSCfreq,且Host不支持TSC Scaling,
+			 * 便会设置vcpu->arch.always_catchup = 1,每次VMExit就会发送KVM_REQ_CLOCK_UPDATE,
+			 * 从而保证TSC Offset不断增加,令Guest TSC不断catchup设定的vTSCfreq下的理论值.
+			 */
 			vcpu->arch.tsc_catchup = 1;
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+			 *   - arch/x86/kvm/x86.c|2454| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+			 */
 			vcpu->arch.tsc_always_catchup = 1;
 			return 0;
 		} else {
@@ -2211,10 +2875,33 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 		return -1;
 	}
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	vcpu->arch.tsc_scaling_ratio = ratio;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6044| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|11412| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2228,9 +2915,26 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	}
 
 	/* Compute a scale to convert nanoseconds in TSC cycles */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2415| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3515| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+	 *
+	 * 这里应该是要计算pshift和pmultiplier来让base_hz能转换成scaled_hz
+	 */
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2248,20 +2952,62 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3625| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ *
+ * 这个函数只在vcpu->tsc_catchup的情况下调用
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2319| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2517| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 注意: 这里是kernel_ns - vcpu->arch.this_tsc_nsec (中间有减号)
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2657| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	tsc += vcpu->arch.this_tsc_write;
 	return tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2382| <<kvm_track_tsc_matching>> (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
+ *   - arch/x86/kvm/x86.c|2727| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2730| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ *   - arch/x86/kvm/x86.c|2739| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2742| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ *   - arch/x86/kvm/x86.c|8556| <<pvclock_gtod_notify>> if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
+ */
 static inline int gtod_is_based_on_tsc(int mode)
 {
+	/*
+	 * 在以下使用VDSO_CLOCKMODE_HVCLOCK:
+	 *   - drivers/clocksource/hyperv_timer.c|440| <<global>> .vdso_clock_mode = VDSO_CLOCKMODE_HVCLOCK,
+	 *   - arch/x86/entry/vdso/vma.c|215| <<vvar_fault>> if (tsc_pg && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))
+	 *   - arch/x86/include/asm/vdso/clocksource.h|8| <<VDSO_ARCH_CLOCKMODES>> VDSO_CLOCKMODE_HVCLOCK
+	 *   - arch/x86/include/asm/vdso/gettimeofday.h|263| <<__arch_get_hw_counter>> if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
+	 *   - arch/x86/kvm/x86.c|2396| <<gtod_is_based_on_tsc>> return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
+	 *   - arch/x86/kvm/x86.c|2878| <<vgettsc>> case VDSO_CLOCKMODE_HVCLOCK:
+	 *   - arch/x86/kvm/x86.c|2883| <<vgettsc>> *mode = VDSO_CLOCKMODE_HVCLOCK;
+	 *   - drivers/clocksource/hyperv_timer.c|425| <<hv_cs_enable>> vclocks_set_used(VDSO_CLOCKMODE_HVCLOCK);
+	 */
 	return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2552| <<kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2269,6 +3015,16 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *
+	 * 在4.14上8个vcpu的VM nr_vcpus_matched_tsc是7
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			 atomic_read(&vcpu->kvm->online_vcpus));
 
@@ -2280,6 +3036,27 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 调用kvm_gen_update_masterclock()
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+	 * 似乎用来填充"struct pvclock_vcpu_time_info"
+	 *
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 这里其实是两个条件 ('or'不是'and'):
+	 * - ka->use_master_clock: 是不是要再evaluate一次???
+	 * - (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched)
+	 *   其实这里有隐藏条件就是!ka->use_master_clock
+	 *   也就是说没用master clock但是符合用master clock的条件
+	 *   这个时候可以再次evaluate一次!
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2300,16 +3077,55 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
  *
  * N equals to kvm_tsc_scaling_ratio_frac_bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2661| <<kvm_scale_tsc>> _tsc = __scale_tsc(ratio, tsc);
+ *   - arch/x86/kvm/x86.c|11773| <<kvm_arch_hardware_setup>> __scale_tsc(kvm_max_tsc_scaling_ratio, tsc_khz));
+ *
+ * Multiply tsc by a fixed point number represented by ratio.
+ */
 static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 {
+	/*
+	 * 4.14简单测试
+	 * crash> kvm_tsc_scaling_ratio_frac_bits
+	 * kvm_tsc_scaling_ratio_frac_bits = $6 = 0 '\000'
+	 */
 	return mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2484| <<kvm_compute_tsc_offset>> tsc = kvm_scale_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/x86.c|2506| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+ *   - arch/x86/kvm/x86.c|2718| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc(vcpu, (u64) adjustment);
+ *   - arch/x86/kvm/x86.c|3281| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
+ *   - arch/x86/kvm/x86.c|4067| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(vcpu, rdtsc()) + tsc_offset;
+ */
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 {
 	u64 _tsc = tsc;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	u64 ratio = vcpu->arch.tsc_scaling_ratio;
 
+	/*
+	 * 在以下设置kvm_default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|11984| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+	 * 在以下使用kvm_default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2474| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2545| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_scale_tsc>> if (ratio != kvm_default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3216| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 */
 	if (ratio != kvm_default_tsc_scaling_ratio)
 		_tsc = __scale_tsc(ratio, tsc);
 
@@ -2317,6 +3133,13 @@ u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2570| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2649| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3755| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4681| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu,
+ */
 static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2326,18 +3149,109 @@ static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|530| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1611| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1622| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1707| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1791| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|1815| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|943| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2088| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|7646| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|2858| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|8386| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|9451| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|526| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2357| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2363| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2482| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3289| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3591| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
+/*
+ * cur_tsc_offset = 18443709364331608385,
+ *
+ * crash> eval 18443709364331608385
+ * hexadecimal: fff537f2a987f941
+ *     decimal: 18443709364331608385  (-3034709377943231)
+ *       octal: 1777651577125141774501
+ *      binary: 1111111111110101001101111111001010101001100001111111100101000001
+ *
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu0/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu1/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu2/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu3/tsc-offset
+ * -3034709377943231
+ *
+ * 
+ * 但是当5.4 kvm上notsc的时候, tsc_offset一直在变化:
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu0/tsc-offset 
+ * -3672408965251004
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu1/tsc-offset 
+ * -3672408670022485
+ * ... ...
+ * ... ...
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu0/tsc-offset 
+ * -3672408820963291
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu1/tsc-offset 
+ * -3672408669450647
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	vcpu->arch.l1_tsc_offset = offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3358| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset += vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4467| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|23| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|791| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|792| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2535| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4503| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/vmx.c|1991| <<vmx_write_l1_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.tsc_offset - g_tsc_offset, offset);
+	 *   - arch/x86/kvm/x86.c|4478| <<kvm_get_msr_common(MSR_IA32_TSC)>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset : vcpu->arch.tsc_offset;
+	 *
+	 * vmx_write_l1_tsc_offset()
+	 */
 	vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2923| <<kvm_synchronize_tsc>> if (!kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5087| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5093| <<kvm_arch_vcpu_load>> if (kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|11395| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+ *   - arch/x86/kvm/x86.c|11660| <<kvm_arch_hardware_enable>> stable = !kvm_check_tsc_unstable();
+ *
+ * 返回true说明unstable
+ * 返回false说明stable
+ */
 static inline bool kvm_check_tsc_unstable(void)
 {
 #ifdef CONFIG_X86_64
@@ -2345,12 +3259,110 @@ static inline bool kvm_check_tsc_unstable(void)
 	 * TSC is marked unstable when we're running on Hyper-V,
 	 * 'TSC page' clocksource is good.
 	 */
+	/*
+	 * 在以下使用VDSO_CLOCKMODE_HVCLOCK:
+	 *   - drivers/clocksource/hyperv_timer.c|440| <<global>> .vdso_clock_mode = VDSO_CLOCKMODE_HVCLOCK,
+	 *   - arch/x86/entry/vdso/vma.c|215| <<vvar_fault>> if (tsc_pg && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))
+	 *   - arch/x86/include/asm/vdso/clocksource.h|8| <<VDSO_ARCH_CLOCKMODES>> VDSO_CLOCKMODE_HVCLOCK
+	 *   - arch/x86/include/asm/vdso/gettimeofday.h|263| <<__arch_get_hw_counter>> if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
+	 *   - arch/x86/kvm/x86.c|2396| <<gtod_is_based_on_tsc>> return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
+	 *   - arch/x86/kvm/x86.c|2878| <<vgettsc>> case VDSO_CLOCKMODE_HVCLOCK:
+	 *   - arch/x86/kvm/x86.c|2883| <<vgettsc>> *mode = VDSO_CLOCKMODE_HVCLOCK;
+	 *   - drivers/clocksource/hyperv_timer.c|425| <<hv_cs_enable>> vclocks_set_used(VDSO_CLOCKMODE_HVCLOCK);
+	 */
 	if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
 		return false;
 #endif
 	return check_tsc_unstable();
 }
 
+/*
+ * live migration的时候 ...
+ *
+ * 1. QEMU src为每一个vcpu通过kvm的kvm_get_msr_common():MSR_IA32_TSC获取tsc. 核心代码:
+ *
+ * u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+ *                                             vcpu->arch.tsc_offset;
+ * msr_info->data = kvm_scale_tsc(vcpu, rdtsc()) + tsc_offset;
+ *
+ * 2. QEMU src通过KVM_GET_CLOCK获取ns.
+ *
+ * struct kvm_clock_data user_ns;
+ * u64 now_ns;
+ *
+ * now_ns = get_kvmclock_ns(kvm); // 返回的应该是VM启动了的ns
+ * user_ns.clock = now_ns;
+ * user_ns.flags = kvm->arch.use_master_clock ? KVM_CLOCK_TSC_STABLE : 0;
+ * memset(&user_ns.pad, 0, sizeof(user_ns.pad));
+ *
+ * 3. 为VM设置kvm_set_tsc_khz().
+ *
+ * 4. QEMU dst为每一个vcpu通过kvm的kvm_set_msr_common():MSR_IA32_TSC设置tsc.
+ *
+ * kvm_synchronize_tsc(vcpu, data);
+ *
+ * 这个kvm_synchronize_tsc()函数最大的目的:
+ * - 计算kvm->arch.nr_vcpus_matched_tsc, 稍后好同步master clock
+ *   - 使用kvm_vcpu_write_tsc_offset(vcpu, offset)来计算vcpu->arch.l1_tsc_offset和vcpu->arch.tsc_offset
+ *
+ * 5. QEMU dst通过KVM_SET_CLOCK设置ns. user_ns.clock是写入的ns.
+ *
+ * if (kvm->arch.use_master_clock)
+ *     now_ns = ka->master_kernel_ns;
+ * else
+ *     now_ns = get_kvmclock_base_ns();
+ * ka->kvmclock_offset = user_ns.clock - now_ns;
+ */
+
+/*
+ * 在4.14上, 2个vcpu调用6次, 4个vcpu调用12次
+ * /usr/share/bcc/tools/trace -t -C 'kvm_write_tsc'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 3.001404 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.002039 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.002591 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.003144 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.018026 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.018118 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.018204 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.018262 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.031890 1   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.031936 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.031979 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.032021 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ *
+ * 根据分析, 应该是QEMU来的 (不是VM kernel写的MSR TSC)
+ *
+ *
+ * KVM希望各个vCPU的TSC处在同步状态,称之为Master Clock模式,每当vTSC被修改,就有两种可能:
+ *
+ * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+ * 此时TSC进入下一代,体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+ * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offset,
+ * 其中offset即上述L1 TSC Offset.重新同步后，可以以此为基点导出任意vCPU的TSC值.
+ *
+ * vCPU在尝试重新同步TSC，此时不能将L1 offset设置为offset
+ * 此时会设置kvm->arch.nr_vcpus_matched_tsc++,一旦所有vCPU都处于matched状态,就可以重新回到Master Clock模式
+ * 在满足vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz的前提下(vTSCfreq相同是TSC能同步的前提),以下情形视为尝试同步TSC:
+ *
+ * Host Initiated且写入值为0,视为正在初始化
+ * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+ * 按照Host TSC是否同步(即是否stable),会为L1 TSC offset设置不同的值
+ *
+ * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+ * 对于Unstable Host,则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+ * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差值
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|3395| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|10581| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ *
+ * 4.14测试kvm_write_tsc的时候data=0
+ *
+ * 这个kvm_synchronize_tsc()函数最大的目的:
+ * - 计算kvm->arch.nr_vcpus_matched_tsc, 稍后好同步master clock
+ * - 使用kvm_vcpu_write_tsc_offset(vcpu, offset)来计算vcpu->arch.l1_tsc_offset和vcpu->arch.tsc_offset
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2360,11 +3372,53 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool already_matched;
 	bool synchronizing = false;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * offset = vTSC - (pTSC * scale)
+	 * 这里是根据tsc计算的
+	 *
+	 * 根据写入的guest tsc和host的tsc计算scaling的offset
+	 */
 	offset = kvm_compute_tsc_offset(vcpu, data);
+	/*
+	 * monotonic time since boot in ns format
+	 *
+	 * 应该是返回启动以后的ns
+	 */
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 在以下尝试同步TSC:
+	 *   - Host Initiated且写入值为0,视为正在初始化
+	 *   - 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+	 *
+	 * vcpu->arch.virtual_tsc_khz为vCPU的vTSCfreq
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	if (vcpu->arch.virtual_tsc_khz) {
 		if (data == 0) {
 			/*
@@ -2374,6 +3428,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 */
 			synchronizing = true;
 		} else {
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2542| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|2599| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_write = data;
+			 *   - arch/x86/kvm/x86.c|11075| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2382,6 +3442,9 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 * of virtual cycle time against real time is
 			 * interpreted as an attempt to synchronize the CPU.
 			 */
+			/*
+			 * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+			 */
 			synchronizing = data < tsc_exp + tsc_hz &&
 					data + tsc_hz > tsc_exp;
 		}
@@ -2393,11 +3456,59 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	 * compensation code attempt to catch up if we fall behind, but
 	 * it's better to try to match offsets from the beginning.
          */
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 *
+	 * 在以下使用kvm_vcpu_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2562| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2600| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *
+	 * !!!! 第一次进来elapsed非常大, 但是不满足"vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz"
+	 */
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+		/*
+		 * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+		 * 对于Unstable Host, 则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+		 * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差
+		 */
+
+		/*
+		 * kvm_check_tsc_unstable():
+		 * 返回true说明unstable
+		 * 返回false说明stable
+		 */
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 如果是第一次进来, offset其实就是offset = kvm_compute_tsc_offset(vcpu, data);
+			 *
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *
+			 * 如果tsc是stable的话, 所有的用一个就可以
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_tsc_nsec:
+			 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+			 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+			 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+			 *
+			 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+			 *
+			 * elapsed = ns - kvm->arch.last_tsc_nsec;
+			 */
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
 			data += delta;
 			offset = kvm_compute_tsc_offset(vcpu, data);
@@ -2405,6 +3516,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		matched = true;
 		already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
 	} else {
+		/*
+		 * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+		 * 此时TSC进入下一代m体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+		 * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offsetm
+		 * 其中offset即上述L1 TSC Offset.重新同步后, 可以以此为基点导出任意vCPU的TSC值.
+		 */
+
 		/*
 		 * We split periods of matched TSC writes into generations.
 		 * For each generation, we track the original measured
@@ -2414,9 +3532,29 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+		 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 *
+		 * 测试的例子是cur_tsc_generation = 1
+		 */
 		kvm->arch.cur_tsc_generation++;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_nsec:
+		 *   - arch/x86/kvm/x86.c|2889| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+		 *   - arch/x86/kvm/x86.c|2918| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+		 *
+		 * 这个是根据get_kvmclock_base_ns()得来的, 返回的是host启动以后经历的时间
+		 */
 		kvm->arch.cur_tsc_nsec = ns;
 		kvm->arch.cur_tsc_write = data;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+		 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+		 */
 		kvm->arch.cur_tsc_offset = offset;
 		matched = false;
 	}
@@ -2425,22 +3563,59 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	 * We also track th most recent recorded KHZ, write and time to
 	 * allow the matching interval to be extended at each write.
 	 */
+	/*
+	 * 这个是根据get_kvmclock_base_ns()得来的, 返回的是host启动以后经历的时间
+	 */
 	kvm->arch.last_tsc_nsec = ns;
 	kvm->arch.last_tsc_write = data;
 	kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	vcpu->arch.last_guest_tsc = data;
 
 	/* Keep track of which generation this VCPU has synchronized to */
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
 	spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
 	if (!matched) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 */
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (!already_matched) {
 		kvm->arch.nr_vcpus_matched_tsc++;
@@ -2450,13 +3625,32 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3035| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3627| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|4140| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|4169| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *
+ * 核心思想是在kvm_vcpu_write_tsc_offset()加上一个adjustment
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
 	u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5082| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+ */
 static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 {
 	if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
@@ -2467,6 +3661,10 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2578| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2487,6 +3685,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2602| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2622| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2523,6 +3726,24 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+ * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+ * jiffies一定是单调递增的.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+ *
+ * CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock
+ * is affected by NTP adjustments and can jump forward and backward when a
+ * system administrator adjusts system time.
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2962| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2532,6 +3753,13 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 
 	do {
 		seq = read_seqcount_begin(&gtod->seq);
+		/*
+		 * CLOCK_MONOTONIC和CLOCK_MONOTONIC_RAW
+		 *
+		 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+		 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+		 * jiffies一定是单调递增的.
+		 */
 		ns = gtod->raw_clock.base_cycles;
 		ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
 		ns >>= gtod->raw_clock.shift;
@@ -2542,6 +3770,24 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+ * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+ * jiffies一定是单调递增的.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+ *
+ * CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock
+ * is affected by NTP adjustments and can jump forward and backward when a
+ * system administrator adjusts system time.
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2651| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2564,17 +3810,30 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2932| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
 	if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
 		return false;
 
+	/*
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 */
 	return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
 						      tsc_timestamp));
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9670| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+ */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
 {
@@ -2627,6 +3886,20 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2759| <<kvm_gen_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|8046| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|10888| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ *
+ * GTOD: Generic time of Day
+ *
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2634,6 +3907,14 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	int vclock_mode;
 	bool host_tsc_clocksource, vcpus_matched;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
@@ -2641,14 +3922,46 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
 	 */
+	/*
+	 * 若Host Clocksource为TSC, 则读取当前时刻的TSC值, 记为pTSC1,
+	 * pvclock中记录的TSC值记为pTSC0, 我们据此设置Master Clock:
+	 *
+	 * kvm->arch.master_cycle_now设置为pTSC1, 即该时刻的Host TSC值
+	 * kvm->arch.master_kernel_ns设置为nsec_base + tsc_to_nsec(pTSC1 - pTSC0) + boot_ns 即该时刻的Host Boot Time
+	 *
+	 * kvm_get_time_and_clockread()的返回值: returns true if host is using TSC based clocksource
+	 */
 	host_tsc_clocksource = kvm_get_time_and_clockread(
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2950| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|11175| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 *
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 *
+	 * 如果写入的是MSR_KVM_SYSTEM_TIME,表明Guest使用的是旧版kvmclock,不支持Master Clock模式,
+	 * 此时要设置kvm->arch.boot_vcpu_runs_old_kvmclock = 1,并对当前vCPU(即vCPU0)发送一个KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 这最终会导致kvm->arch.use_master_clock = 0
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2658,11 +3971,48 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3606| <<kvm_gen_update_masterclock>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9130| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * 调用的次数非常少, 就是qemu创建和VM kernel启动的时候, 下面是4.14的例子
+ * kvm_gen_update_masterclock
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1270| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2158| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2336| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8243| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9288| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|10748| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|6011| <<kvm_arch_vm_ioctl>> kvm_gen_update_masterclock(kvm);
+ *   - arch/x86/kvm/x86.c|9289| <<vcpu_enter_guest>> kvm_gen_update_masterclock(vcpu->kvm);
+ *
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvm_gen_update_masterclock(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2671,15 +4021,35 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 	struct kvm_arch *ka = &kvm->arch;
 	unsigned long flags;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2753| <<kvm_gen_update_masterclock>> kvm_hv_invalidate_tsc_page(kvm);
+	 */
 	kvm_hv_invalidate_tsc_page(kvm);
 
+	/*
+	 * 申请kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *
+	 * 给所有vCPU发送KVM_REQ_MCLOCK_INPROGRESS,将它们踢出Guest模式
+	 * 该请求没有Handler,因此vCPU无法再进入Guest
+	 */
 	kvm_make_mclock_inprogress_request(kvm);
 
 	/* no guest entries from this point */
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	/*
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 	spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
 
+	/*
+	 * KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -2689,6 +4059,17 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|527| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2164| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/x86.c|6578| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|69| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|405| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|446| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *
+ * 返回的应该是VM启动了的ns
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2699,20 +4080,43 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
 	if (!ka->use_master_clock) {
 		spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+		/*
+		 * get_kvmclock_base_ns()应该是返回启动以后的ns
+		 */
 		return get_kvmclock_base_ns() + ka->kvmclock_offset;
 	}
 
 	hv_clock.tsc_timestamp = ka->master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|2780| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|2853| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2984| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6090| <<kvm_arch_vm_ioctl>> now_ns = ka->master_kernel_ns;
+	 *
+	 * 猜测是host内核启动了的ns
+	 */
 	hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 	spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
 
 	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
 	get_cpu();
 
+	/*
+	 * 在4.14上是3392425, 对应dmesg
+	 * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+	 */
 	if (__this_cpu_read(cpu_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
+		/*
+		 * called by:
+		 *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+		 *   - arch/x86/kernel/pvclock.c|76| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+		 *   - arch/x86/kvm/x86.c|3072| <<get_kvmclock_ns>> ret = __pvclock_read_cycles(&hv_clock, rdtsc());
+		 *   - drivers/ptp/ptp_kvm_x86.c|91| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair.tsc);
+		 */
 		ret = __pvclock_read_cycles(&hv_clock, rdtsc());
 	} else
 		ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
@@ -2722,6 +4126,16 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2868| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|2870| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|2873| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ *
+ * vcpu_enter_guest()
+ * -> kvm_guest_time_update()
+ *    -> kvm_setup_pvclock_page()
+ */
 static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				   struct gfn_to_hva_cache *cache,
 				   unsigned int offset)
@@ -2752,6 +4166,9 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	if (guest_hv_clock.version & 1)
 		++guest_hv_clock.version;  /* first time write, random junk */
 
+	/* struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.version = guest_hv_clock.version + 1;
 	kvm_write_guest_offset_cached(v->kvm, cache,
 				      &vcpu->hv_clock, offset,
@@ -2760,8 +4177,23 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	smp_wmb();
 
 	/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
+	/*
+	 * 在以下使用PVCLOCK_GUEST_STOPPED:
+	 *   - arch/x86/kernel/kvmclock.c|152| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+	 *   - arch/x86/kernel/kvmclock.c|153| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+	 *   - arch/x86/kernel/pvclock.c|80| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+	 *   - arch/x86/kernel/pvclock.c|81| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+	 *   - arch/x86/kvm/x86.c|2769| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+	 */
 	vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|2765| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|4767| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	if (vcpu->pvclock_set_guest_stopped_request) {
 		vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
 		vcpu->pvclock_set_guest_stopped_request = false;
@@ -2781,6 +4213,12 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				     sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9174| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2799,8 +4237,20 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 * to the guest.
 	 */
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 */
 	use_master_clock = ka->use_master_clock;
 	if (use_master_clock) {
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|2945| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3079| <<get_kvmclock_ns>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3216| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *
+		 * live crash了好多次, 这两个都不变
+		 */
 		host_tsc = ka->master_cycle_now;
 		kernel_ns = ka->master_kernel_ns;
 	}
@@ -2808,14 +4258,33 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* Keep irq disabled to prevent changes to the clock */
 	local_irq_save(flags);
+	/*
+	 * 在以下使用cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3308| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3453| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|8696| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|8711| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|8730| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 *
+	 * 在4.14上是3392425, 对应dmesg
+	 * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+	 */
 	tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
 	if (unlikely(tgt_tsc_khz == 0)) {
 		local_irq_restore(flags);
+		/*
+		 * kvm_guest_time_update()
+		 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 		return 1;
 	}
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
+		/*
+		 * kernel启动后的时间ns
+		 */
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
@@ -2831,6 +4300,14 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 *
+	 * 测试的一个例子是vcpu->tsc_catchup=false
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -2843,9 +4320,22 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* With all the info we got, fill in the values */
 
+	/*
+	 * 在以下设置kvm_has_tsc_control:
+	 *   - arch/x86/kvm/svm/svm.c|962| <<svm_hardware_setup>> kvm_has_tsc_control = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|8063| <<hardware_setup>> kvm_has_tsc_control = true;
+	 */
 	if (kvm_has_tsc_control)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
 
+	/*
+	 * 将1KHZ TSC转换成guest TSC
+	 * 如果当前guest时钟(kvmclock)的频率不同，则更新转换比例
+	 *
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3506| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3510| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
@@ -2853,8 +4343,46 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * 在kvm vm上测试的时候, 下面的都不变
+	 * system_time=23867275
+	 * tsc_timestamp=206742471, version=4, tsc_to_system_mul=2532092704, tsc_shift=255
+	 * kvm_clock_read()一直在变
+	 *
+	 * 把hypervisor clock换成hpet就开始增长了!!!!!
+	 *
+	 *
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 *    -> u64 tsc_timestamp;
+	 *    -> u64 system_time;
+	 *
+	 * 更新pvti时,vCPU的vTSC
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 *
+	 * 当dmesg是14.903088的时候,
+	 * kvm_clock_read()返回15169858774 (15.169858774)
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	vcpu->last_guest_tsc = tsc_timestamp;
 
 	/* If the host uses TSC clocksource, then it is stable */
@@ -2864,6 +4392,35 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	vcpu->hv_clock.flags = pvclock_flags;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *
+	 * pv_time = {
+         *   generation = 182, 
+	 *   gpa = 15401488384, 
+	 *   hva = 140509217558528, 
+	 *   len = 32, 
+	 *   memslot = 0xffff8c237bd80008
+	 * }, 
+	 * pv_time_enabled = true, 
+	 * pvclock_set_guest_stopped_request = false, 
+	 * st = {
+	 *   preempted = 1 '\001', 
+	 *   msr_val = 17979068481, 
+	 *   last_steal = 81611316161, 
+	 *   cache = {
+	 *     generation = 182, 
+	 *     gfn = 4389421, 
+	 *     pfn = 28505645, 
+	 *     dirty = true
+	 *   }
+	 * },
+	 */
 	if (vcpu->pv_time_enabled)
 		kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
 	if (vcpu->xen.vcpu_info_set)
@@ -2892,6 +4449,13 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvmclock_update_fn():
+ *   - arch/x86/kvm/x86.c|11121| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *
+ * 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	int i;
@@ -2902,33 +4466,81 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9359| <<vcpu_enter_guest>> kvm_gen_kvmclock_update(vcpu);
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+ * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 调用kvm_guest_time_update()
+	 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 调用kvmclock_update_fn(), 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
+/*
+ * 在以下使用KVMCLOCK_SYNC_PERIOD:
+ *   - arch/x86/kvm/x86.c|3281| <<kvmclock_sync_fn>> KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|10875| <<kvm_arch_vcpu_postcreate>> KVMCLOCK_SYNC_PERIOD);
+ *
+ * 300秒
+ */
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
+	/*
+	 * struct kvm_arch:
+	 * -> struct delayed_work kvmclock_sync_work;
+	 */
 	struct kvm_arch *ka = container_of(dwork, struct kvm_arch,
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|139| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|3206| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|10792| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (!kvmclock_periodic_sync)
 		return;
 
+	/*
+	 * 调用kvmclock_update_fn(), 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3754| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3765| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11435| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11788| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -2945,6 +4557,10 @@ static bool can_set_mci_status(struct kvm_vcpu *vcpu)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4956| <<kvm_set_msr_common>> return set_msr_mce(vcpu, msr_info);
+ */
 static int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	u64 mcg_cap = vcpu->arch.mcg_cap;
@@ -3002,6 +4618,10 @@ static inline bool kvm_pv_async_pf_enabled(struct kvm_vcpu *vcpu)
 	return (vcpu->arch.apf.msr_en_val & mask) == mask;
 }
 
+/*
+ * 处理MSR_KVM_ASYNC_PF_EN:
+ *   - arch/x86/kvm/x86.c|4898| <<kvm_set_msr_common>> if (kvm_pv_enable_async_pf(vcpu, data))
+ */
 static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 {
 	gpa_t gpa = data & ~0x3f;
@@ -3057,6 +4677,11 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11798| <<kvm_arch_vcpu_destroy>> kvmclock_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11847| <<kvm_vcpu_reset>> kvmclock_reset(vcpu);
+ */
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.pv_time_enabled = false;
@@ -3069,6 +4694,11 @@ static void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_tlb_flush_all)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4606| <<record_steal_time>> kvm_vcpu_flush_tlb_guest(vcpu);
+ *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> kvm_vcpu_flush_tlb_guest(vcpu);
+ */
 static void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)
 {
 	++vcpu->stat.tlb_flush;
@@ -3085,9 +4715,16 @@ static void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)
 		return;
 	}
 
+	/*
+	 * vmx_flush_tlb_guest()
+	 */
 	static_call(kvm_x86_tlb_flush_guest)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11117| <<vcpu_enter_guest>> record_steal_time(vcpu);
+ */
 static void record_steal_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
@@ -3133,8 +4770,27 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 
 	smp_wmb();
 
+	/*
+	 * struct kvm_steal_time {
+	 * -> __u64 steal;
+	 * -> __u32 version;
+	 * -> __u32 flags;
+	 * -> __u8  preempted;
+	 * -> __u8  u8_pad[3];
+	 * -> __u32 pad[11];
+	 * };
+	 */
 	st->steal += current->sched_info.run_delay -
 		vcpu->arch.st.last_steal;
+	/*
+	 * struct kvm_vcpu_arch:
+	 * -> struct {
+	 *        -> u8 preempted;
+	 *        -> u64 msr_val;
+	 *        -> u64 last_steal;
+	 *        -> struct gfn_to_pfn_cache cache;
+	 *    } st;
+	 */
 	vcpu->arch.st.last_steal = current->sched_info.run_delay;
 
 	smp_wmb();
@@ -3144,9 +4800,28 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 	kvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2613| <<efer_trap>> ret = kvm_set_msr_common(vcpu, &msr_info);
+ *   - arch/x86/kvm/svm/svm.c|2948| <<svm_set_msr>> return kvm_set_msr_common(vcpu, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2235| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2387| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2390| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2508| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2517| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *
+ * 在以下也会间接调用:
+ *   - arch/x86/kvm/x86.c|1840| <<__kvm_set_msr>> return static_call(kvm_x86_set_msr)(vcpu, &msr);
+ */
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
+	/*
+	 * struct msr_data:
+	 * -> bool host_initiated;
+	 * -> u32 index; 
+	 * -> u64 data;
+	 */
 	u32 msr = msr_info->index;
 	u64 data = msr_info->data;
 
@@ -3219,11 +4894,24 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		kvm_set_lapic_tscdeadline_msr(vcpu, data);
 		break;
 	case MSR_IA32_TSC_ADJUST:
+		/*
+		 * 关于msr_tsc_adjust
+		 * 如果想要修改tsc的counter, 可以直接写入msr_tsc.
+		 * 然而这样很难保证所有的CPU是同步的 (因为每个CPU都要往msr_tsc写一个absolute value)
+		 * 使用msr_tsc_adjust就可以解决这个问题, 因为为每个CPU写入的是offset, 不是absolute value
+		 */
 		if (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {
 			if (!msr_info->host_initiated) {
 				s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
 				adjust_tsc_offset_guest(vcpu, adj);
 			}
+			/*
+			 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+			 *   - arch/x86/kvm/x86.c|4571| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+			 *   - arch/x86/kvm/x86.c|4574| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> vcpu->arch.ia32_tsc_adjust_msr = data;
+			 *   - arch/x86/kvm/x86.c|4602| <<kvm_set_msr_common(MSR_IA32_TSC)>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+			 *   - arch/x86/kvm/x86.c|4963| <<kvm_get_msr_common(MSR_IA32_TSC_ADJUST)>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+			 */
 			vcpu->arch.ia32_tsc_adjust_msr = data;
 		}
 		break;
@@ -3247,11 +4935,30 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		vcpu->arch.msr_ia32_power_ctl = data;
 		break;
 	case MSR_IA32_TSC:
+		/*
+		 * 在以下设置msr_data->host_initiated:
+		 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+		 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+		 *
+		 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+		 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+		 * msr_info->host_initiated就为false.
+		 */
 		if (msr_info->host_initiated) {
 			kvm_synchronize_tsc(vcpu, data);
 		} else {
 			u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
 			adjust_tsc_offset_guest(vcpu, adj);
+			/*
+			 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+			 *   - arch/x86/kvm/x86.c|4571| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+			 *   - arch/x86/kvm/x86.c|4574| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> vcpu->arch.ia32_tsc_adjust_msr = data;
+			 *   - arch/x86/kvm/x86.c|4602| <<kvm_set_msr_common(MSR_IA32_TSC)>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+			 *   - arch/x86/kvm/x86.c|4963| <<kvm_get_msr_common(MSR_IA32_TSC_ADJUST)>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+			 */
 			vcpu->arch.ia32_tsc_adjust_msr += adj;
 		}
 		break;
@@ -3284,6 +4991,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))
 			return 1;
 
+		/*
+		 * 似乎在以下使用kvm_vcpu_arch->wall_clock (gpa_t):
+		 *   - arch/x86/kvm/x86.c|3690| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+		 *   - arch/x86/kvm/x86.c|3697| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+		 *   - arch/x86/kvm/x86.c|4026| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+		 *   - arch/x86/kvm/x86.c|4032| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+		 */
 		vcpu->kvm->arch.wall_clock = data;
 		kvm_write_wall_clock(vcpu->kvm, data, 0);
 		break;
@@ -3476,6 +5190,13 @@ static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2725| <<svm_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/svm/svm.c|2738| <<svm_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2055| <<vmx_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2180| <<vmx_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ */
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	switch (msr_info->index) {
@@ -3552,6 +5273,18 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		 * return L1's TSC value to ensure backwards-compatible
 		 * behavior for migration.
 		 */
+		/*
+		 * 在以下设置msr_data->host_initiated:
+		 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+		 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+		 *
+		 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+		 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+		 * msr_info->host_initiated就为false.
+		 */
 		u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
 							    vcpu->arch.tsc_offset;
 
@@ -3775,6 +5508,12 @@ static int __msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs *msrs,
  *
  * @return number of msrs set successfully.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5826| <<kvm_arch_dev_ioctl(KVM_GET_MSRS)>> r = msr_io(NULL, argp, do_get_msr_feature, 1);
+ *   - arch/x86/kvm/x86.c|6736| <<kvm_arch_vcpu_ioctl(KVM_GET_MSRS)>> r = msr_io(vcpu, argp, do_get_msr, 1);
+ *   - arch/x86/kvm/x86.c|6742| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+ */
 static int msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs __user *user_msrs,
 		  int (*do_msr)(struct kvm_vcpu *vcpu,
 				unsigned index, u64 *data),
@@ -3823,6 +5562,11 @@ static inline bool kvm_can_mwait_in_guest(void)
 		boot_cpu_has(X86_FEATURE_ARAT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5829| <<kvm_arch_dev_ioctl(KVM_GET_SUPPORTED_HV_CPUID)>> r = kvm_ioctl_get_supported_hv_cpuid(NULL, argp);
+ *   - arch/x86/kvm/x86.c|6985| <<kvm_arch_vcpu_ioctl(KVM_GET_SUPPORTED_HV_CPUID)>> r = kvm_ioctl_get_supported_hv_cpuid(vcpu, argp);
+ */
 static int kvm_ioctl_get_supported_hv_cpuid(struct kvm_vcpu *vcpu,
 					    struct kvm_cpuid2 __user *cpuid_arg)
 {
@@ -4018,6 +5762,11 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		if (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))
 			goto out;
 		n = msr_list.nmsrs;
+		/*
+		 * 在以下使用msrs_to_save:
+		 *   - arch/x86/kvm/x86.c|4034| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+		 *   - arch/x86/kvm/x86.c|6062| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+		 */
 		msr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;
 		if (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))
 			goto out;
@@ -4100,6 +5849,9 @@ long kvm_arch_dev_ioctl(struct file *filp,
 
 static void wbinvd_ipi(void *garbage)
 {
+	/*
+	 * looks used to flush l-1 cache
+	 */
 	wbinvd();
 }
 
@@ -4108,6 +5860,32 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * 4.14的例子.
+ * kvm_arch_vcpu_load
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_arch_vcpu_load
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|211| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5058| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -4119,6 +5897,9 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 					wbinvd_ipi, NULL, 1);
 	}
 
+	/*
+	 * vmx_vcpu_load()
+	 */
 	static_call(kvm_x86_vcpu_load)(vcpu, cpu);
 
 	/* Save host pkru register if supported */
@@ -4132,14 +5913,39 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	}
 
 	if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+		/*
+		 * 在以下设置kvm_vcpu_arch->last_host_tsc:
+		 *   - arch/x86/kvm/x86.c|5282| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+		 *   - arch/x86/kvm/x86.c|11850| <<kvm_arch_hardware_enable>> vcpu->arch.last_host_tsc = local_tsc;
+		 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+		 *   - arch/x86/kvm/x86.c|5211| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+		 *   - arch/x86/kvm/x86.c|5212| <<kvm_arch_vcpu_load>> rdtsc() - vcpu->arch.last_host_tsc;
+		 *   - arch/x86/kvm/x86.c|11798| <<kvm_arch_hardware_enable>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+		 *   - arch/x86/kvm/x86.c|11800| <<kvm_arch_hardware_enable>> if (vcpu->arch.last_host_tsc > max_tsc)
+		 *   - arch/x86/kvm/x86.c|11801| <<kvm_arch_hardware_enable>> max_tsc = vcpu->arch.last_host_tsc;
+		 */
 		s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
 				rdtsc() - vcpu->arch.last_host_tsc;
 		if (tsc_delta < 0)
 			mark_tsc_unstable("KVM discovered backwards TSC");
 
 		if (kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+			 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+			 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+			 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+			 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+			 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+			 */
 			u64 offset = kvm_compute_tsc_offset(vcpu,
 						vcpu->arch.last_guest_tsc);
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+			 *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 */
 			kvm_vcpu_write_tsc_offset(vcpu, offset);
 			vcpu->arch.tsc_catchup = 1;
 		}
@@ -4161,6 +5967,10 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5405| <<kvm_arch_vcpu_put>> kvm_steal_time_set_preempted(vcpu);
+ */
 static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
@@ -4184,6 +5994,11 @@ static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 	kvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, true);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|219| <<vcpu_put>> kvm_arch_vcpu_put(vcpu);
+ *   - virt/kvm/kvm_main.c|5115| <<kvm_sched_out>> kvm_arch_vcpu_put(vcpu);
+ */
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	int idx;
@@ -4212,6 +6027,10 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	set_debugreg(0, 6);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6051| <<kvm_arch_vcpu_ioctl(KVM_GET_LAPIC)>> r = kvm_vcpu_ioctl_get_lapic(vcpu, u.lapic);
+ */
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
@@ -4234,6 +6053,11 @@ static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * calle by:
+ *   - arch/x86/kvm/x86.c|6061| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu);
+ *   - arch/x86/kvm/x86.c|11192| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+ */
 static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -4278,6 +6102,15 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 	if (vcpu->arch.pending_external_vector != -1)
 		return -EEXIST;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|37| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_get_extint>> int vector = v->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|123| <<kvm_cpu_get_extint>> v->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5958| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5961| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12371| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	vcpu->arch.pending_external_vector = irq->irq;
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 	return 0;
@@ -4818,6 +6651,10 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4215| <<kvm_vcpu_ioctl>> r = kvm_arch_vcpu_ioctl(filp, ioctl, arg);
+ */
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
@@ -5393,12 +7230,30 @@ void kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * 处理KVM_IRQ_LINE_STATUS或者KVM_IRQ_LINE:
+ *   - virt/kvm/kvm_main.c|4739| <<kvm_vm_ioctl>> r = kvm_vm_ioctl_irq_line(kvm, &irq_event,
+ */
 int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,
 			bool line_status)
 {
 	if (!irqchip_in_kernel(kvm))
 		return -ENXIO;
 
+	/*
+	 * called by:
+	 *   - arch/powerpc/kvm/book3s.c|1005| <<kvm_arch_set_irq_inatomic>> return kvm_set_irq(kvm, irq_source_id, irq_entry->gsi,
+	 *   - arch/powerpc/kvm/book3s.c|1012| <<kvmppc_book3s_set_irq>> return kvm_set_irq(kvm, irq_source_id, e->gsi, level, line_status);
+	 *   - arch/powerpc/kvm/powerpc.c|2136| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+	 *   - arch/x86/kvm/i8254.c|250| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 1, false);
+	 *   - arch/x86/kvm/i8254.c|251| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 0, false);
+	 *   - arch/x86/kvm/x86.c|7082| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+	 *   - virt/kvm/eventfd.c|49| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,
+	 *   - virt/kvm/eventfd.c|51| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,
+	 *   - virt/kvm/eventfd.c|54| <<irqfd_inject>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+	 *   - virt/kvm/eventfd.c|75| <<irqfd_resampler_ack>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+	 *   - virt/kvm/eventfd.c|100| <<irqfd_resampler_shutdown>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+	 */
 	irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
 					irq_event->irq, irq_event->level,
 					line_status);
@@ -5875,6 +7730,13 @@ long kvm_arch_vm_ioctl(struct file *filp,
 #endif
 	case KVM_SET_CLOCK: {
 		struct kvm_arch *ka = &kvm->arch;
+		/*
+		 * struct kvm_clock_data {
+		 *     __u64 clock;
+		 *     __u32 flags;
+		 *     __u32 pad[9];
+		 * };
+		 */
 		struct kvm_clock_data user_ns;
 		u64 now_ns;
 
@@ -5892,6 +7754,15 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		 * kvm_gen_update_masterclock() can be cut down to locked
 		 * pvclock_update_vm_gtod_copy().
 		 */
+		/*
+		 * 心思想是为整个kvm更新:
+		 *   - ka->use_master_clock
+		 *   - ka->master_kernel_ns
+		 *   - ka->master_cycle_now
+		 *   - kvm_guest_has_master_clock
+		 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+		 * 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_gen_update_masterclock(kvm);
 
 		/*
@@ -5902,13 +7773,33 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		 * 'system_time' when 'user_ns.clock' is very small.
 		 */
 		spin_lock_irq(&ka->pvclock_gtod_sync_lock);
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 */
 		if (kvm->arch.use_master_clock)
 			now_ns = ka->master_kernel_ns;
 		else
 			now_ns = get_kvmclock_base_ns();
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+		 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 *
+		 * get_kvmclock_base_ns() + ka->kvmclock_offset应该就是当前的VM的启动了的ns
+		 */
 		ka->kvmclock_offset = user_ns.clock - now_ns;
 		spin_unlock_irq(&ka->pvclock_gtod_sync_lock);
 
+		/*
+		 * kvm_guest_time_update()
+		 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
 		break;
 	}
@@ -5916,6 +7807,9 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		struct kvm_clock_data user_ns;
 		u64 now_ns;
 
+		/*
+		 * 返回的应该是VM启动了的ns
+		 */
 		now_ns = get_kvmclock_ns(kvm);
 		user_ns.clock = now_ns;
 		user_ns.flags = kvm->arch.use_master_clock ? KVM_CLOCK_TSC_STABLE : 0;
@@ -5979,6 +7873,10 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12831| <<kvm_arch_hardware_setup>> kvm_init_msr_list();
+ */
 static void kvm_init_msr_list(void)
 {
 	struct x86_pmu_capability x86_pmu;
@@ -6614,6 +8512,9 @@ static int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,
 	(cmpxchg64((u64 *)(ptr), *(u64 *)(old), *(u64 *)(new)) == *(u64 *)(old))
 #endif
 
+/*
+ * struct x86_emulate_ops emulate_ops.cmpxchg_emulated = emulator_cmpxchg_emulated()
+ */
 static int emulator_cmpxchg_emulated(struct x86_emulate_ctxt *ctxt,
 				     unsigned long addr,
 				     const void *old,
@@ -7128,6 +9029,10 @@ static int emulator_set_xcr(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr)
 	return __kvm_set_xcr(emul_to_vcpu(ctxt), index, xcr);
 }
 
+/*
+ * 在以下使用emulate_ops:
+ *   - arch/x86/kvm/x86.c|9070| <<alloc_emulate_ctxt>> ctxt->ops = &emulate_ops;
+ */
 static const struct x86_emulate_ops emulate_ops = {
 	.read_gpr            = emulator_read_gpr,
 	.write_gpr           = emulator_write_gpr,
@@ -7225,6 +9130,12 @@ static struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)
 	return ctxt;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9168| <<kvm_inject_realmode_interrupt>> init_emulate_ctxt(vcpu);
+ *   - arch/x86/kvm/x86.c|9496| <<x86_decode_emulated_instruction>> init_emulate_ctxt(vcpu);
+ *   - arch/x86/kvm/x86.c|12140| <<kvm_task_switch>> init_emulate_ctxt(vcpu);
+ */
 static void init_emulate_ctxt(struct kvm_vcpu *vcpu)
 {
 	struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
@@ -7277,6 +9188,11 @@ void kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip)
 }
 EXPORT_SYMBOL_GPL(kvm_inject_realmode_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7663| <<x86_emulate_instruction>> return handle_emulation_failure(vcpu, emulation_type);
+ *   - arch/x86/kvm/x86.c|7720| <<x86_emulate_instruction>> return handle_emulation_failure(vcpu, emulation_type);
+ */
 static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 {
 	++vcpu->stat.insn_emulation_fail;
@@ -7287,6 +9203,14 @@ static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用EMULTYPE_SKIP:
+	 *   - arch/x86/kvm/svm/svm.c|347| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+	 *   - arch/x86/kvm/vmx/vmx.c|1816| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+	 *   - arch/x86/kvm/x86.c|7315| <<handle_emulation_failure>> if (emulation_type & EMULTYPE_SKIP) {
+	 *   - arch/x86/kvm/x86.c|7610| <<x86_decode_emulated_instruction>> if (!(emulation_type & EMULTYPE_SKIP) &&
+	 *   - arch/x86/kvm/x86.c|7683| <<x86_emulate_instruction>> if (emulation_type & EMULTYPE_SKIP) {
+	 */
 	if (emulation_type & EMULTYPE_SKIP) {
 		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
@@ -7595,6 +9519,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5149| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|7783| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|7790| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -7865,12 +9795,22 @@ int kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in)
 }
 EXPORT_SYMBOL_GPL(kvm_fast_pio);
 
+/*
+ * 在以下使用kvmclock_cpu_down_prep():
+ *   - arch/x86/kvm/x86.c|9384| <<kvm_timer_init>> cpuhp_setup_state(CPUHP_AP_X86_KVM_CLK_ONLINE, "x86/kvm/clk:online", kvmclock_cpu_online, kvmclock_cpu_down_prep);
+ */
 static int kvmclock_cpu_down_prep(unsigned int cpu)
 {
 	__this_cpu_write(cpu_tsc_khz, 0);
 	return 0;
 }
 
+/*
+ * 在以下使用tsc_khz_changed():
+ *   - arch/x86/kvm/x86.c|9897| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|9924| <<__kvmclock_cpufreq_notifier>> smp_call_function_single(cpu, tsc_khz_changed, freq, 1);
+ *   - arch/x86/kvm/x86.c|9951| <<kvmclock_cpu_online>> tsc_khz_changed(NULL);
+ */
 static void tsc_khz_changed(void *data)
 {
 	struct cpufreq_freqs *freq = data;
@@ -8095,6 +10035,15 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 };
 
 #ifdef CONFIG_X86_64
+/*
+ * 在一下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|8628| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|8637| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|8829| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+ *
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|8385| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -8103,9 +10052,25 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	int i;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+	 * 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
@@ -8119,20 +10084,88 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
+	/*
+	 * 在一下使用pvclock_gtod_work:
+	 *   - arch/x86/kvm/x86.c|8628| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+	 *   - arch/x86/kvm/x86.c|8637| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+	 *   - arch/x86/kvm/x86.c|8829| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+	 *
+	 * 在以下使用pvclock_gtod_update_fn():
+	 *   - arch/x86/kvm/x86.c|8385| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+	 */
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|8640| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|8710| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|8828| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 4.14上的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * kvm通过pvclock_gtod_register_notifier向timekeeper层注册了一个回调pvclock_gtod_notify,
+ * 每当Host Kernel时钟更新时(即timekeeping_update被调用时),就会调用pvclock_gtod_notify.
+ *
+ * 参数unused不使用
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
+	/*
+	 * struct pvclock_gtod_data {
+	 *     seqcount_t      seq;
+	 *
+	 *     struct pvclock_clock clock; // extract of a clocksource struct
+	 *     struct pvclock_clock raw_clock; // extract of a clocksource struct
+	 *
+	 *     ktime_t         offs_boot;
+	 *     u64             wall_time_sec;
+	 * };
+	 *
+	 * 在以下使用pvclock_gtod_data:
+	 *   - arch/x86/kvm/x86.c|2024| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2055| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+	 *   - arch/x86/kvm/x86.c|2275| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+	 *   - arch/x86/kvm/x86.c|2478| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2533| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2552| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2575| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+	 *   - arch/x86/kvm/x86.c|2587| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+	 *   - arch/x86/kvm/x86.c|2660| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+	 *   - arch/x86/kvm/x86.c|8172| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 */
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	struct timekeeper *tk = priv;
 
+	/*
+	 * 只在此处调用
+	 */
 	update_pvclock_gtod(tk);
 
 	/*
@@ -8140,17 +10173,45 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|8640| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|8710| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|8828| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+	 *
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 *
+	 * 根据注释,若clocksource不是TSC,但全局变量kvm_guest_has_master_clock非零,
+	 * 说明clocksource从TSC变为了非TSC,此时向所有vCPU发送KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 然后令kvm_guest_has_master_clock = 0.
+	 *
+	 * 也就是说要同时满足两个条件
+	 * 1. !gtod_is_based_on_tsc(gtod->clock.vclock_mode) --> host的clock不是tsc
+	 * 2. atomic_read(&kvm_guest_has_master_clock) != 0  --> kvm_guest_has_master_clock不是0
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
 	return 0;
 }
 
+/*
+ * 在以下使用pvclock_gtod_notifier:
+ *   - arch/x86/kvm/x86.c|8518| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|8550| <<kvm_arch_exit>> pvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);
+ */
 static struct notifier_block pvclock_gtod_notifier = {
 	.notifier_call = pvclock_gtod_notify,
 };
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5908| <<kvm_init>> r = kvm_arch_init(opaque);
+ */
 int kvm_arch_init(void *opaque)
 {
 	struct kvm_x86_init_ops *ops = opaque;
@@ -8269,6 +10330,11 @@ void kvm_arch_exit(void)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10143| <<kvm_vcpu_halt>> return __kvm_vcpu_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);
+ *   - arch/x86/kvm/x86.c|10169| <<kvm_emulate_ap_reset_hold>> return __kvm_vcpu_halt(vcpu, KVM_MP_STATE_AP_RESET_HOLD, KVM_EXIT_AP_RESET_HOLD) && ret;
+ */
 static int __kvm_vcpu_halt(struct kvm_vcpu *vcpu, int state, int reason)
 {
 	++vcpu->stat.halt_exits;
@@ -8281,12 +10347,32 @@ static int __kvm_vcpu_halt(struct kvm_vcpu *vcpu, int state, int reason)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3561| <<nested_vmx_run>> return kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5007| <<handle_rmode_exception>> return kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5677| <<handle_invalid_guest_state>> return kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/x86.c|10021| <<kvm_emulate_halt>> return kvm_vcpu_halt(vcpu) && ret;
+ */
 int kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 {
 	return __kvm_vcpu_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_halt);
 
+/*
+ * 在以下使用kvm_vmx_exit_handlers[]:
+ *   - arch/x86/kvm/vmx/vmx.c|5883| <<global>> static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
+ *   - arch/x86/kvm/vmx/vmx.c|6336| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+ *   - arch/x86/kvm/vmx/vmx.c|6339| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8191| <<hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ *
+ * 在以下使用kvm_emulate_halt():
+ *   - svm_exit_handlers[SVM_EXIT_HLT] = kvm_emulate_halt,
+ *   - kvm_vmx_exit_handlers[EXIT_REASON_HLT] = kvm_emulate_halt,
+ *   - arch/x86/kvm/svm/svm.c|3248| <<svm_invoke_exit_handler>> return kvm_emulate_halt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6307| <<__vmx_handle_exit>> return kvm_emulate_halt(vcpu);
+ */
 int kvm_emulate_halt(struct kvm_vcpu *vcpu)
 {
 	int ret = kvm_skip_emulated_instruction(vcpu);
@@ -8307,6 +10393,10 @@ int kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9909| <<kvm_emulate_hypercall(KVM_HC_CLOCK_PAIRING)>> ret = kvm_pv_clock_pairing(vcpu, a0, a1);
+ */
 static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 			        unsigned long clock_type)
 {
@@ -8355,8 +10445,25 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, unsigned long flags, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|230| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/svm/avic.c|204| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|274| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/svm.c|1366| <<svm_create_vcpu>> if (irqchip_in_kernel(vcpu->kvm) && kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|10854| <<kvm_vcpu_update_apicv>> vcpu->arch.apicv_active = kvm_apicv_activated(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|12209| <<kvm_arch_vcpu_create>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10214| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10222| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10225| <<kvm_apicv_init>> &kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10876| <<kvm_request_apicv_update>> old = READ_ONCE(kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10885| <<kvm_request_apicv_update>> old = cmpxchg(&kvm->arch.apicv_inhibit_reasons, expected, new);
+	 */
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
 }
 EXPORT_SYMBOL_GPL(kvm_apicv_activated);
@@ -8572,6 +10679,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_queue_exception)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11114| <<vcpu_enter_guest>> inject_pending_event(vcpu, &req_immediate_exit);
+ */
 static void inject_pending_event(struct kvm_vcpu *vcpu, bool *req_immediate_exit)
 {
 	int r;
@@ -8985,6 +11096,15 @@ void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
 
 void kvm_make_scan_ioapic_request(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用KVM_REQ_SCAN_IOAPIC:
+	 *   - arch/x86/kvm/hyperv.c|135| <<synic_set_sint>> kvm_make_request(KVM_REQ_SCAN_IOAPIC, hv_synic_to_vcpu(synic));
+	 *   - arch/x86/kvm/x86.c|10889| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC,
+	 *   - arch/x86/kvm/x86.c|10897| <<kvm_make_scan_ioapic_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
+	 *   - arch/x86/kvm/x86.c|11141| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_SCAN_IOAPIC, vcpu))
+	 *
+	 * 在vcpu_enter_guest()调用vcpu_scan_ioapic()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
@@ -9006,6 +11126,15 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);
  * locked, because it calls __x86_set_memory_region() which does
  * synchronize_srcu(&kvm->srcu).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|939| <<kvm_hv_activate_synic>> kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_HYPERV);
+ *   - arch/x86/kvm/i8254.c|307| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, false,
+ *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_request_apicv_update(kvm, true,
+ *   - arch/x86/kvm/svm/avic.c|600| <<svm_toggle_avic_for_irq_window>> kvm_request_apicv_update(vcpu->kvm, activate,
+ *   - arch/x86/kvm/svm/svm.c|3983| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false,
+ *   - arch/x86/kvm/svm/svm.c|3991| <<svm_vcpu_after_set_cpuid>> kvm_request_apicv_update(vcpu->kvm, false,
+ */
 void kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
 {
 	struct kvm_vcpu *except;
@@ -9120,6 +11249,10 @@ EXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);
  * exiting to the userspace.  Otherwise, the value will be returned to the
  * userspace.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11443| <<vcpu_run>> r = vcpu_enter_guest(vcpu);
+ */
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -9320,6 +11453,11 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (kvm_lapic_enabled(vcpu) && vcpu->arch.apicv_active)
 		static_call(kvm_x86_sync_pir_to_irr)(vcpu);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10574| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+	 */
 	if (kvm_vcpu_exit_request(vcpu)) {
 		vcpu->mode = OUTSIDE_GUEST_MODE;
 		smp_wmb();
@@ -9350,10 +11488,18 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	}
 
 	for (;;) {
+		/*
+		 * vmx_vcpu_run()
+		 */
 		exit_fastpath = static_call(kvm_x86_run)(vcpu);
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10574| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+		 */
                 if (unlikely(kvm_vcpu_exit_request(vcpu))) {
 			exit_fastpath = EXIT_FASTPATH_EXIT_HANDLED;
 			break;
@@ -9457,6 +11603,44 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * vcpu_run()
+ * -> if (kvm_vcpu_running(vcpu))
+ *      r = vcpu_enter_guest(vcpu);
+ *      -> kvm_x86_run = vmx_vcpu_run()
+ *      -> kvm_x86_handle_exit = vmx_handle_exit()
+ *         -> __vmx_handle_exit()
+ *            -> kvm_vmx_exit_handlers[exit_handler_index](vcpu)
+ *               kvm_emulate_halt()
+ *               -> kvm_vcpu_halt()
+ *                  -> __kvm_vcpu_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);
+ *                     -> vcpu->arch.mp_state = KVM_MP_STATE_HALTED;
+ * -> else
+ *      r = vcpu_block(kvm, vcpu);
+ *          -> kvm_vcpu_block()
+ *          -> vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
+ *
+ * 从vcpu_run()首先进入vcpu_enter_guest(), 从而进入guest mode.
+ * 当guest HLT的时候, 会trap到__kvm_vcpu_halt(), 并且vcpu->arch.mp_state = KVM_MP_STATE_HALTED.
+ *
+ * 当从vcpu_run()再次进入for loop的时候, 因为vcpu->arch.mp_state = KVM_MP_STATE_HALTED,
+ * 所以kvm_vcpu_running()会返回false. 因此进入了vcpu_block().
+ *
+ * static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
+ * {
+ *     if (is_guest_mode(vcpu))
+ *         kvm_check_nested_events(vcpu);
+ *
+ *     return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+ *             !vcpu->arch.apf.halted);
+ * }
+ *
+ * kvm_vcpu_block()的时候, 会先poll (spin)一会, 然后再schedule().
+ * 最后vcpu_block()设置vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE.
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|11253| <<vcpu_run>> r = vcpu_block(kvm, vcpu);
+ */
 static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 {
 	if (!kvm_arch_vcpu_runnable(vcpu) &&
@@ -9500,6 +11684,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11483| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -9690,6 +11878,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 	kvm_run->flags = 0;
 	kvm_load_guest_fpu(vcpu);
 
+	/*
+	 * 在以下使用KVM_MP_STATE_UNINITIALIZED:
+	 *   - arch/x86/kvm/x86.c|11439| <<kvm_arch_vcpu_ioctl_run>> if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {
+	 *   - arch/x86/kvm/x86.c|12057| <<kvm_arch_vcpu_create>> vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
+	 */
 	if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {
 		if (kvm_run->immediate_exit) {
 			r = -EINTR;
@@ -9950,6 +12143,11 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2379| <<task_switch_interception>> return kvm_task_switch(vcpu, tss_selector, int_vec, reason,
+ *   - arch/x86/kvm/vmx/vmx.c|5601| <<handle_task_switch>> return kvm_task_switch(vcpu, tss_selector,
+ */
 int kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,
 		    int reason, bool has_error_code, u32 error_code)
 {
@@ -10300,16 +12498,34 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3610| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
 	int r;
 
+	/*
+	 * 在以下使用KVM_MP_STATE_UNINITIALIZED:
+	 *   - arch/x86/kvm/x86.c|11439| <<kvm_arch_vcpu_ioctl_run>> if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {
+	 *   - arch/x86/kvm/x86.c|12057| <<kvm_arch_vcpu_create>> vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
+	 */
 	if (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))
 		vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
 	else
 		vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
 
+	/*
+	 * 在以下使用max_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|8927| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+	 *   - arch/x86/kvm/x86.c|8938| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+	 *   - arch/x86/kvm/x86.c|11328| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+	 *
+	 * crash> max_tsc_khz
+	 * max_tsc_khz = $4 = 3392426
+	 */
 	kvm_set_tsc_khz(vcpu, max_tsc_khz);
 
 	r = kvm_mmu_create(vcpu);
@@ -10403,6 +12619,12 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3414| <<kvm_vm_ioctl_create_vcpu>> kvm_arch_vcpu_postcreate(vcpu);
+ *
+ * 大概是创建vcpu到最后的时候调用
+ */
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -10452,6 +12674,11 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		static_branch_dec(&kvm_has_noapic_vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3283| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true); --> 处理KVM_APIC_INIT
+ *   - arch/x86/kvm/x86.c|12152| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	kvm_lapic_reset(vcpu, init_event);
@@ -10536,6 +12763,10 @@ void kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_deliver_sipi_vector);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4992| <<hardware_enable_nolock>> r = kvm_arch_hardware_enable();
+ */
 int kvm_arch_hardware_enable(void)
 {
 	struct kvm *kvm;
@@ -10700,14 +12931,33 @@ bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_is_reset_bsp);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|223| <<__kvm_migrate_pit_timer>> if (!kvm_vcpu_is_bsp(vcpu) || !pit)
+ *   - arch/x86/kvm/lapic.c|3042| <<kvm_lapic_reset>> if (kvm_vcpu_is_bsp(vcpu))
+ *   - arch/x86/kvm/lapic.c|3663| <<kvm_apic_accept_events>> if (kvm_vcpu_is_bsp(apic->vcpu))
+ *   - arch/x86/kvm/x86.c|12259| <<__set_sregs>> if (kvm_vcpu_is_bsp(vcpu) && kvm_rip_read(vcpu) == 0xfff0 &&
+ */
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 {
 	return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
 }
 
+/*
+ * 在以下使用kvm_has_noapic_vcpu:
+ *   - arch/x86/kvm/lapic.h|269| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/x86.c|12588| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|273| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+ *   - arch/x86/kvm/x86.c|12195| <<kvm_arch_vcpu_create>> static_branch_inc(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/x86.c|12327| <<kvm_arch_vcpu_destroy>> static_branch_dec(&kvm_has_noapic_vcpu);
+ */
 __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 EXPORT_SYMBOL_GPL(kvm_has_noapic_vcpu);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5900| <<kvm_sched_in>> kvm_arch_sched_in(vcpu, cpu);
+ */
 void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -10749,7 +12999,24 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	mutex_init(&kvm->arch.apic_map_lock);
 	spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	/*
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 
 	kvm->arch.guest_can_read_msr_platform_info = true;
@@ -10830,6 +13097,16 @@ void kvm_arch_sync_events(struct kvm *kvm)
  * address, i.e. its accessibility is not guaranteed, and must be
  * accessed via __copy_{to,from}_user().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|247| <<avic_update_access_page>> ret = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3893| <<init_rmode_identity_map>> uaddr = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|3942| <<alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/vmx/vmx.c|4999| <<vmx_set_tss_addr>> ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|13151| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|13153| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|13155| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 void __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
 				      u32 size)
 {
@@ -11248,6 +13525,13 @@ bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
 	return vcpu->arch.preempted_in_kernel;
 }
 
+/*
+ * 注释Note, the vCPU could get migrated to a different pCPU at any point
+ * after kvm_arch_vcpu_should_kick(), which could result in sending an
+ * IPI to the previous pCPU.  But, that's ok because the purpose of the
+ * IPI is to force the vCPU to leave IN_GUEST_MODE, and migrating the
+ * vCPU also requires it to leave IN_GUEST_MODE.
+ */
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
@@ -11417,6 +13701,11 @@ static inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)
 	return !val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13723| <<kvm_can_do_async_pf>> if (kvm_hlt_in_guest(vcpu->kvm) && !kvm_can_deliver_async_pf(vcpu))
+ *   - arch/x86/kvm/x86.c|13741| <<kvm_arch_async_page_not_present>> if (kvm_can_deliver_async_pf(vcpu) &&
+ */
 static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->arch.apf.delivery_as_pf_vmexit && is_guest_mode(vcpu))
@@ -11429,6 +13718,10 @@ static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3711| <<try_async_pf>> if (!prefault && kvm_can_do_async_pf(vcpu)) {
+ */
 bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!lapic_in_kernel(vcpu) ||
@@ -11446,6 +13739,10 @@ bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 	return kvm_arch_interrupt_allowed(vcpu);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|228| <<kvm_setup_async_pf>> work->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);
+ */
 bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work)
 {
@@ -11478,6 +13775,11 @@ bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|78| <<async_pf_execute>> kvm_arch_async_page_present(vcpu, apf);
+ *   - virt/kvm/async_pf.c|178| <<kvm_check_async_pf_completion>> kvm_arch_async_page_present(vcpu, work);
+ */
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work)
 {
@@ -11503,6 +13805,11 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 	vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/async_pf.c|87| <<async_pf_execute>> kvm_arch_async_page_present_queued(vcpu);
+ *   - virt/kvm/async_pf.c|260| <<kvm_async_pf_wakeup_all>> kvm_arch_async_page_present_queued(vcpu);
+ */
 void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)
 {
 	kvm_make_request(KVM_REQ_APF_READY, vcpu);
@@ -11510,6 +13817,10 @@ void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * called by:
+ *   -  virt/kvm/async_pf.c|169| <<kvm_check_async_pf_completion>> kvm_arch_can_dequeue_async_page_present(vcpu)) {
+ */
 bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_pv_async_pf_enabled(vcpu))
@@ -11531,6 +13842,15 @@ void kvm_arch_end_assignment(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_arch_end_assignment);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|625| <<svm_set_pi_irte_mode>> if (!kvm_arch_has_assigned_device(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|819| <<svm_update_pi_irte>> if (!kvm_arch_has_assigned_device(kvm) ||
+ *   - arch/x86/kvm/svm/avic.c|938| <<avic_update_iommu_vcpu_affinity>> if (!kvm_arch_has_assigned_device(vcpu->kvm))
+ *   - arch/x86/kvm/vmx/posted_intr.c|91| <<vmx_vcpu_pi_put>> if (!kvm_arch_has_assigned_device(vcpu->kvm) ||
+ *   - arch/x86/kvm/vmx/posted_intr.c|151| <<pi_pre_block>> if (!kvm_arch_has_assigned_device(vcpu->kvm) ||
+ *   - arch/x86/kvm/vmx/posted_intr.c|287| <<pi_update_irte>> if (!kvm_arch_has_assigned_device(kvm) ||
+ */
 bool kvm_arch_has_assigned_device(struct kvm *kvm)
 {
 	return atomic_read(&kvm->arch.assigned_device_count);
@@ -11560,6 +13880,12 @@ bool kvm_arch_has_irq_bypass(void)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2012| <<kvm_arch_irq_bypass_add_producer>> int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
+ *   - arch/powerpc/kvm/powerpc.c|842| <<kvm_arch_irq_bypass_add_producer>> int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
+ *   - virt/kvm/eventfd.c|414| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ */
 int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 				      struct irq_bypass_producer *prod)
 {
@@ -11569,6 +13895,9 @@ int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 
 	irqfd->producer = prod;
 	kvm_arch_start_assignment(irqfd->kvm);
+	/*
+	 * pi_update_irte
+	 */
 	ret = static_call(kvm_x86_update_pi_irte)(irqfd->kvm,
 					 prod->irq, irqfd->gsi, 1);
 
@@ -11602,9 +13931,16 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	kvm_arch_end_assignment(irqfd->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|701| <<kvm_irq_routing_update>> int ret = kvm_arch_update_irqfd_routing(
+ */
 int kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,
 				   uint32_t guest_irq, bool set)
 {
+	/*
+	 * pi_update_irte()
+	 */
 	return static_call(kvm_x86_update_pi_irte)(kvm, host_irq, guest_irq, set);
 }
 
@@ -11613,6 +13949,11 @@ bool kvm_vector_hashing_enabled(void)
 	return vector_hashing;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3092| <<kvm_vcpu_block>> if (vcpu->halt_poll_ns && !kvm_arch_no_poll(vcpu)) {
+ *   - virt/kvm/kvm_main.c|3142| <<kvm_vcpu_block>> if (!kvm_arch_no_poll(vcpu)) {
+ */
 bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 {
 	return (vcpu->arch.msr_kvm_poll_control & 1) == 0;
@@ -11620,6 +13961,11 @@ bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_arch_no_poll);
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2804| <<svm_set_msr>> if (kvm_spec_ctrl_test_value(data))
+ *   - arch/x86/kvm/vmx/vmx.c|2347| <<vmx_set_msr>> if (kvm_spec_ctrl_test_value(data))
+ */
 int kvm_spec_ctrl_test_value(u64 value)
 {
 	/*
@@ -11674,6 +14020,16 @@ EXPORT_SYMBOL_GPL(kvm_fixup_and_inject_pf_error);
  * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value
  * indicates whether exit to userspace is needed.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4766| <<nested_vmx_get_vmptr>> *ret = kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5073| <<handle_vmread>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5146| <<handle_vmwrite>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5313| <<handle_vmptrst>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5366| <<handle_invept>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/vmx/nested.c|5448| <<handle_invvpid>> return kvm_handle_memory_failure(vcpu, r, &e);
+ *   - arch/x86/kvm/x86.c|14002| <<kvm_handle_invpcid>> return kvm_handle_memory_failure(vcpu, r, &e);
+ */
 int kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,
 			      struct x86_exception *e)
 {
@@ -11697,6 +14053,11 @@ int kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,
 }
 EXPORT_SYMBOL_GPL(kvm_handle_memory_failure);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3019| <<invpcid_interception>> return kvm_handle_invpcid(vcpu, type, gva);
+ *   - arch/x86/kvm/vmx/vmx.c|5829| <<handle_invpcid>> return kvm_handle_invpcid(vcpu, type, gva);
+ */
 int kvm_handle_invpcid(struct kvm_vcpu *vcpu, unsigned long type, gva_t gva)
 {
 	bool pcid_enabled;
@@ -11820,6 +14181,10 @@ static int complete_sev_es_emulated_mmio(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/sev.c|2493| <<sev_handle_vmgexit>> ret = kvm_sev_es_mmio_write(vcpu,
+ */
 int kvm_sev_es_mmio_write(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,
 			  void *data)
 {
@@ -11897,6 +14262,10 @@ int kvm_sev_es_mmio_read(struct kvm_vcpu *vcpu, gpa_t gpa, unsigned int bytes,
 }
 EXPORT_SYMBOL_GPL(kvm_sev_es_mmio_read);
 
+/*
+ * 在以下使用complete_sev_es_emulated_ins():
+ *   - arch/x86/kvm/x86.c|13861| <<kvm_sev_es_ins>> vcpu->arch.complete_userspace_io = complete_sev_es_emulated_ins;
+ */
 static int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)
 {
 	memcpy(vcpu->arch.guest_ins_data, vcpu->arch.pio_data,
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 521f74e5bbf2..e04d11452ec8 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -377,6 +377,19 @@ static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 
 static inline bool kvm_mwait_in_guest(struct kvm *kvm)
 {
+	/*
+	 * Executing the HLT instruction on a idle logical processor puts the
+	 * targeted processor in a non-execution state. This requires another
+	 * processor (when posting work for the halted logical processor) to
+	 * wake up the halted processor using an inter-processor interrupt. The
+	 * posting and servicing of such an interrupt introduces a delay in the
+	 * servicing of new work requests.
+	 *
+	 * MONITOR sets up an effective address range that is monitored for
+	 * write-to-memory activities; MWAIT places the processor in an
+	 * optimized state (this may vary between different implementations)
+	 * until a write to the monitored address range occurs.
+	 */
 	return kvm->arch.mwait_in_guest;
 }
 
diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index aa9f50fccc5d..773eeb1f002f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -38,6 +38,31 @@ EXPORT_SYMBOL_GPL(hypercall_page);
  * hypercall.
  *
  */
+/*
+ * 在以下使用xen_vcpu:
+ *   - arch/x86/xen/enlighten.c|41| <<global>> DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - include/xen/xen-ops.h|12| <<global>> DECLARE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - rch/x86/xen/enlighten.c|198| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
+ *   - arch/x86/xen/enlighten.c|202| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = NULL;
+ *   - arch/x86/xen/enlighten.c|233| <<xen_vcpu_setup>> if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+ *   - arch/x86/xen/enlighten.c|264| <<xen_vcpu_setup>> per_cpu(xen_vcpu, cpu) = vcpup;
+ *   - arch/x86/xen/enlighten.c|271| <<xen_vcpu_setup>> return ((per_cpu(xen_vcpu, cpu) == NULL) ? -ENODEV : 0);
+ *   - arch/x86/xen/enlighten_pv.c|1436| <<xen_cpu_up_prepare_pv>> if (per_cpu(xen_vcpu, cpu) == NULL)
+ *   - arch/x86/xen/irq.c|32| <<xen_save_fl>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/irq.c|51| <<xen_irq_disable>> this_cpu_read(xen_vcpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/irq.c|67| <<xen_irq_enable>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/mmu_pv.c|1209| <<xen_write_cr2>> this_cpu_read(xen_vcpu)->arch.cr2 = cr2;
+ *   - arch/x86/xen/smp_pv.c|365| <<xen_pv_cpu_up>> per_cpu(xen_vcpu, cpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/time.c|52| <<xen_clocksource_read>> src = &__this_cpu_read(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|74| <<xen_read_wallclock>> vcpu_time = &get_cpu_var(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|76| <<xen_read_wallclock>> put_cpu_var(xen_vcpu);
+ *   - arch/x86/xen/time.c|507| <<xen_time_init>> pvti = &__this_cpu_read(xen_vcpu)->time;
+ *   - drivers/xen/events/events_2l.c|123| <<evtchn_2l_unmask>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|173| <<evtchn_2l_handle_events>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|280| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, i);
+ *   - drivers/xen/events/events_2l.c|289| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, cpu);
+ *   - drivers/xen/events/events_base.c|1699| <<__xen_evtchn_do_upcall>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ */
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 
 /*
@@ -45,6 +70,13 @@ DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
  * hypercall. This can be used both in PV and PVHVM mode. The structure
  * overrides the default per_cpu(xen_vcpu, cpu) value.
  */
+/*
+ * 在以下使用xen_vcpu_info:
+ *   - arch/x86/xen/enlighten.c|48| <<global>> DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
+ *   - arch/x86/xen/xen-ops.h|24| <<global>> DECLARE_PER_CPU(struct vcpu_info, xen_vcpu_info);
+ *   - arch/x86/xen/enlighten.c|219| <<xen_vcpu_setup>> if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+ *   - arch/x86/xen/enlighten.c|224| <<xen_vcpu_setup>> vcpup = &per_cpu(xen_vcpu_info, cpu);
+ */
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
 /* Linux <-> Xen vCPU id mapping */
@@ -196,6 +228,13 @@ void xen_vcpu_info_reset(int cpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/enlighten.c|136| <<xen_vcpu_setup_restore>> rc = xen_vcpu_setup(cpu);
+ *   - arch/x86/xen/enlighten_hvm.c|166| <<xen_cpu_up_prepare_hvm>> rc = xen_vcpu_setup(cpu);
+ *   - arch/x86/xen/enlighten_pv.c|1031| <<xen_setup_vcpu_info_placement>> (void ) xen_vcpu_setup(cpu);
+ *   - arch/x86/xen/smp_hvm.c|20| <<xen_hvm_smp_prepare_boot_cpu>> xen_vcpu_setup(0);
+ */
 int xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
@@ -235,6 +274,11 @@ int xen_vcpu_setup(int cpu)
 		 * hypercall does not allow to over-write info.mfn and
 		 * info.offset.
 		 */
+		/*
+		 * 在以下使用VCPUOP_register_vcpu_info:
+		 *   - arch/arm/xen/enlighten.c|156| <<xen_starting_cpu>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
+		 *   - arch/x86/xen/enlighten.c|252| <<xen_vcpu_setup>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
+		 */
 		err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
 					 xen_vcpu_nr(cpu), &info);
 
diff --git a/arch/x86/xen/time.c b/arch/x86/xen/time.c
index d9c945ee1100..5e89b705e3f7 100644
--- a/arch/x86/xen/time.c
+++ b/arch/x86/xen/time.c
@@ -31,6 +31,13 @@
 /* Minimum amount of time until next clock event fires */
 #define TIMER_SLOP	100000
 
+/*
+ * 在以下使用xen_sched_clock_offset:
+ *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+ *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+ */
 static u64 xen_sched_clock_offset __read_mostly;
 
 /* Get the TSC speed from Xen */
@@ -62,6 +69,13 @@ static u64 xen_clocksource_get_cycles(struct clocksource *cs)
 
 static u64 xen_sched_clock(void)
 {
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 */
 	return xen_clocksource_read() - xen_sched_clock_offset;
 }
 
@@ -520,8 +534,20 @@ static void __init xen_time_init(void)
 		pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/time.c|556| <<xen_init_time_ops>> xen_init_time_common();
+ *   - arch/x86/xen/time.c|599| <<xen_hvm_init_time_ops>> xen_init_time_common();
+ */
 static void __init xen_init_time_common(void)
 {
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 */ 
 	xen_sched_clock_offset = xen_clocksource_read();
 	static_call_update(pv_steal_clock, xen_steal_clock);
 	paravirt_set_sched_clock(xen_sched_clock);
@@ -530,8 +556,17 @@ static void __init xen_init_time_common(void)
 	x86_platform.get_wallclock = xen_get_wallclock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/enlighten_pv.c|135| <<xen_pv_init_platform>> xen_init_time_ops();
+ */
 void __init xen_init_time_ops(void)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/xen/time.c|556| <<xen_init_time_ops>> xen_init_time_common();
+	 *   - arch/x86/xen/time.c|599| <<xen_hvm_init_time_ops>> xen_init_time_common();
+	 */
 	xen_init_time_common();
 
 	x86_init.timers.timer_init = xen_time_init;
@@ -556,6 +591,10 @@ static void xen_hvm_setup_cpu_clockevents(void)
 	xen_setup_cpu_clockevents();
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/enlighten_hvm.c|219| <<xen_hvm_guest_init>> xen_hvm_init_time_ops();
+ */
 void __init xen_hvm_init_time_ops(void)
 {
 	/*
diff --git a/block/blk-exec.c b/block/blk-exec.c
index beae70a0e5e5..f8b596711b17 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -17,6 +17,10 @@
  * @rq: request to complete
  * @error: end I/O status of the request
  */
+/*
+ * 在以下使用blk_end_sync_rq():
+ *   - block/blk-exec.c|82| <<blk_execute_rq>> blk_execute_rq_nowait(bd_disk, rq, at_head, blk_end_sync_rq);
+ */
 static void blk_end_sync_rq(struct request *rq, blk_status_t error)
 {
 	struct completion *waiting = rq->end_io_data;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index c86c01bfecdb..873c30a157ed 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -676,6 +676,9 @@ EXPORT_SYMBOL_GPL(blk_mq_complete_request_remote);
  **/
 void blk_mq_complete_request(struct request *rq)
 {
+	/*
+	 * scsi_complete()
+	 */
 	if (!blk_mq_complete_request_remote(rq))
 		rq->q->mq_ops->complete(rq);
 }
diff --git a/block/genhd.c b/block/genhd.c
index 9f8cb7beaad1..68597357343b 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -597,6 +597,18 @@ EXPORT_SYMBOL(device_add_disk_no_queue_reg);
  *
  * Context: can sleep
  */
+/*
+ * 部分调用del_gendisk():
+ *   - drivers/block/loop.c|2180| <<loop_remove>> del_gendisk(lo->lo_disk);
+ *   - drivers/block/null_blk/main.c|1592| <<null_del_dev>> del_gendisk(nullb->disk);
+ *   - drivers/block/virtio_blk.c|924| <<virtblk_remove>> del_gendisk(vblk->disk);
+ *   - drivers/block/xen-blkfront.c|1172| <<xlvbd_alloc_gendisk>> del_gendisk(gd);
+ *   - drivers/block/xen-blkfront.c|1221| <<xlvbd_release_gendisk>> del_gendisk(info->gd);
+ *   - drivers/nvme/host/core.c|3780| <<nvme_ns_remove>> del_gendisk(ns->disk);
+ *   - drivers/nvme/host/multipath.c|766| <<nvme_mpath_remove_disk>> del_gendisk(head->disk);
+ *   - drivers/scsi/sd.c|3538| <<sd_remove>> del_gendisk(sdkp->disk);
+ *   - drivers/scsi/sr.c|1043| <<sr_remove>> del_gendisk(cd->disk);
+ */
 void del_gendisk(struct gendisk *disk)
 {
 	might_sleep();
@@ -607,6 +619,11 @@ void del_gendisk(struct gendisk *disk)
 	blk_integrity_del(disk);
 	disk_del_events(disk);
 
+	/*
+	 * struct gendisk *disk:
+	 * -> struct block_device *part0;
+	 *    -> struct mutex bd_mutex;
+	 */
 	mutex_lock(&disk->part0->bd_mutex);
 	disk->flags &= ~GENHD_FL_UP;
 	blk_drop_partitions(disk);
diff --git a/block/scsi_ioctl.c b/block/scsi_ioctl.c
index 1b3fe99b83a6..3192e5b09a40 100644
--- a/block/scsi_ioctl.c
+++ b/block/scsi_ioctl.c
@@ -243,6 +243,10 @@ static int blk_fill_sghdr_rq(struct request_queue *q, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/scsi_ioctl.c|360| <<sg_io>> ret = blk_complete_sghdr_rq(rq, hdr, bio);
+ */
 static int blk_complete_sghdr_rq(struct request *rq, struct sg_io_hdr *hdr,
 				 struct bio *bio)
 {
diff --git a/drivers/acpi/acpica/nseval.c b/drivers/acpi/acpica/nseval.c
index 63748ac699f7..fe26edf54aab 100644
--- a/drivers/acpi/acpica/nseval.c
+++ b/drivers/acpi/acpica/nseval.c
@@ -39,6 +39,18 @@ ACPI_MODULE_NAME("nseval")
  * MUTEX:       Locks interpreter
  *
  ******************************************************************************/
+/*
+ * called by:
+ *   - drivers/acpi/acpica/evgpe.c|506| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/evregion.c|630| <<acpi_ev_execute_reg_method>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/hwxface.c|362| <<acpi_get_sleep_type_data>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/nsinit.c|156| <<acpi_ns_initialize_devices>> status = acpi_ns_evaluate(info.evaluate_info);
+ *   - drivers/acpi/acpica/nsinit.c|176| <<acpi_ns_initialize_devices>> status = acpi_ns_evaluate(info.evaluate_info);
+ *   - drivers/acpi/acpica/nsinit.c|645| <<acpi_ns_init_one_device>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/nsxfeval.c|354| <<acpi_evaluate_object>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/rsutils.c|746| <<acpi_rs_set_srs_method_data>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/uteval.c|60| <<acpi_ut_evaluate_object>> status = acpi_ns_evaluate(info);
+ */
 acpi_status acpi_ns_evaluate(struct acpi_evaluate_info *info)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/psparse.c b/drivers/acpi/acpica/psparse.c
index 7eb7a81619a3..ef8e5d37ae0d 100644
--- a/drivers/acpi/acpica/psparse.c
+++ b/drivers/acpi/acpica/psparse.c
@@ -405,6 +405,99 @@ acpi_ps_next_parse_state(struct acpi_walk_state *walk_state,
  *
  ******************************************************************************/
 
+/*
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ut_evaluate_object
+ * [0] acpi_rs_get_prt_method_data
+ * [0] acpi_get_irq_routing_table
+ * [0] acpi_pci_irq_find_prt_entry
+ * [0] acpi_pci_irq_lookup
+ * [0] acpi_pci_irq_enable
+ * [0] do_pci_enable_device
+ * [0] pci_enable_device_flags
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_bus_get_status
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_bus_get_status
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_processor_add
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_ost
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/acpica/dbmethod.c|325| <<acpi_db_disassemble_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dbutils.c|366| <<acpi_db_second_pass_parse>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsargs.c|86| <<acpi_ds_execute_arguments>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsargs.c|125| <<acpi_ds_execute_arguments>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsmethod.c|100| <<acpi_ds_auto_serialize_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/nsparse.c|228| <<acpi_ns_one_complete_parse>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/psxface.c|190| <<acpi_ps_execute_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/psxface.c|295| <<acpi_ps_execute_table>> status = acpi_ps_parse_aml(walk_state);
+ */
 acpi_status acpi_ps_parse_aml(struct acpi_walk_state *walk_state)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/psxface.c b/drivers/acpi/acpica/psxface.c
index fd0f28c7af1e..3cc77b619493 100644
--- a/drivers/acpi/acpica/psxface.c
+++ b/drivers/acpi/acpica/psxface.c
@@ -81,6 +81,10 @@ acpi_debug_trace(const char *name, u32 debug_level, u32 debug_layer, u32 flags)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/nseval.c|205| <<acpi_ns_evaluate>> status = acpi_ps_execute_method(info);
+ */
 acpi_status acpi_ps_execute_method(struct acpi_evaluate_info *info)
 {
 	acpi_status status;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 327e1b4eb6b0..4a24d38dae0d 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -1058,6 +1058,18 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/dbexec.c|695| <<acpi_db_create_execution_thread>> status = acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbexec.c|849| <<acpi_db_create_execution_threads>> acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbxface.c|448| <<acpi_initialize_debugger>> status = acpi_os_execute(OSL_DEBUGGER_MAIN_THREAD,
+ *   - drivers/acpi/acpica/evgpe.c|526| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/acpica/evgpe.c|823| <<acpi_ev_gpe_dispatch>> status = acpi_os_execute(OSL_GPE_HANDLER,
+ *   - drivers/acpi/acpica/evmisc.c|139| <<acpi_ev_queue_notify_request>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/bus.c|516| <<acpi_device_fixed_event>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_device_notify_fixed, data);
+ *   - drivers/acpi/sbshc.c|232| <<smbus_alarm>> acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/platform/x86/dell/dell-rbtn.c|278| <<rbtn_resume>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1152,6 +1164,10 @@ struct acpi_hp_work {
 	u32 src;
 };
 
+/*
+ * 在以下使用acpi_hotplug_work_fn():
+ *   - drivers/acpi/osl.c|1176| <<acpi_hotplug_schedule>> INIT_WORK(&hpw->work, acpi_hotplug_work_fn);
+ */
 static void acpi_hotplug_work_fn(struct work_struct *work)
 {
 	struct acpi_hp_work *hpw = container_of(work, struct acpi_hp_work, work);
@@ -1161,6 +1177,20 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/bus.c|490| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|381| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index e10d38ac7cf2..f8da0cdcc45a 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -321,6 +321,12 @@ static int acpi_scan_device_check(struct acpi_device *adev)
 			dev_warn(&adev->dev, "Already enumerated\n");
 			return -EALREADY;
 		}
+		/*
+		 * cpu hotplug会跑到这里
+		 * 在这里加一个while(1)就不会触发下面的event了
+		 * {"timestamp": {"seconds": 1628873592, "microseconds": 941033}, "event": "ACPI_DEVICE_OST",
+		 * "data": {"info": {"device": "core1", "source": 1, "status": 0, "slot": "2", "slot-type": "CPU"}}}
+		 */
 		error = acpi_bus_scan(adev->handle);
 		if (error) {
 			dev_warn(&adev->dev, "Namespace scan failure\n");
@@ -363,6 +369,10 @@ static int acpi_scan_bus_check(struct acpi_device *adev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|419| <<acpi_device_hotplug>> error = acpi_generic_hotplug_event(adev, src);
+ */
 static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 {
 	switch (type) {
@@ -383,6 +393,32 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1160| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ *
+ * (qemu) device_add host-x86_64-cpu,id=core1,socket-id=1,core-id=0,thread-id=0
+ *
+ * Comm: kworker/u8:0
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn0
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * (qemu) device_del core1
+ *
+ * Comm: kworker/u8:3
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
diff --git a/drivers/acpi/utils.c b/drivers/acpi/utils.c
index 3b54b8fd7396..64ef25517fb3 100644
--- a/drivers/acpi/utils.c
+++ b/drivers/acpi/utils.c
@@ -404,6 +404,20 @@ EXPORT_SYMBOL(acpi_get_physical_device_location);
  * must call this function when evaluating _OST for hotplug operations.
  * When the platform does not support _OST, this function has no effect.
  */
+/*
+ * https://zhuanlan.zhihu.com/p/113296734
+ *
+ * called by:
+ *   - drivers/acpi/acpi_pad.c|404| <<acpi_pad_handle_notify>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_AGGREGATOR_NOTIFY, 0, &param);
+ *   - drivers/acpi/bus.c|496| <<acpi_bus_notify>> acpi_evaluate_ost(handle, type, ost_code, NULL);
+ *   - drivers/acpi/bus.c|577| <<sb_notify_work>> acpi_evaluate_ost(sb_handle, ACPI_OST_EC_OSPM_SHUTDOWN,
+ *   - drivers/acpi/device_sysfs.c|386| <<eject_store>> acpi_evaluate_ost(acpi_device->handle, ACPI_OST_EC_OSPM_EJECT,
+ *   - drivers/acpi/processor_perflib.c|105| <<acpi_processor_ppc_ost>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_NOTIFY_PERFORMANCE,
+ *   - drivers/acpi/scan.c|389| <<acpi_generic_hotplug_event>> acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_EJECT_REQUEST,
+ *   - drivers/acpi/scan.c|455| <<acpi_device_hotplug>> acpi_evaluate_ost(adev->handle, src, ost_code, NULL);
+ *   - drivers/pci/pcie/edr.c|137| <<acpi_send_edr_status>> status = acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_DISCONNECT_RECOVER,
+ *   - drivers/xen/xen-acpi-pad.c|92| <<acpi_pad_handle_notify>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_AGGREGATOR_NOTIFY,
+ */
 acpi_status
 acpi_evaluate_ost(acpi_handle handle, u32 source_event, u32 status_code,
 		  struct acpi_buffer *status_buf)
diff --git a/drivers/base/base.h b/drivers/base/base.h
index e5f9b7e656c3..0236d33344d6 100644
--- a/drivers/base/base.h
+++ b/drivers/base/base.h
@@ -141,6 +141,30 @@ extern void driver_detach(struct device_driver *drv);
 extern void driver_deferred_probe_del(struct device *dev);
 extern void device_set_deferred_probe_reason(const struct device *dev,
 					     struct va_format *vaf);
+/*
+ * driver_match_device+0x3e/0x60
+[   28.727273]  __device_attach_driver+0x15/0x68
+[   28.727281]  bus_for_each_drv+0x74/0xc0
+[   28.727292]  __device_attach+0xdb/0x140
+[   28.727296]  bus_probe_device+0x82/0x90
+[   28.727299]  device_add+0x433/0x8b0
+[   28.727302]  ? _vdpa_register_device.cold.13+0x2/0x2
+[   28.727307]  vdpasim_net_dev_add+0xa2/0xc0
+[   28.727311]  ? vdpasim_net_dev_add+0xc0/0xc0
+[   28.727313]  ? vdpasim_create+0x260/0x260
+[   28.727315]  vdpa_nl_cmd_dev_add_set_doit+0x49/0x90
+[   28.727318]  genl_family_rcv_msg_doit.isra.17+0x10a/0x150
+[   28.727321]  genl_rcv_msg+0xd9/0x1e0
+[   28.727329]  netlink_rcv_skb+0x4b/0xf0
+[   28.727333]  genl_rcv+0x1f/0x30
+[   28.727335]  netlink_unicast+0x1a0/0x280
+[   28.727338]  netlink_sendmsg+0x216/0x440
+[   28.727341]  sock_sendmsg+0x56/0x60
+[   28.727344]  __sys_sendto+0xe9/0x150
+[   28.727351]  __x64_sys_sendto+0x1f/0x30
+[   28.727354]  do_syscall_64+0x3c/0x80
+[   28.727357]  entry_SYSCALL_64_after_hwframe+0x44/0xae
+ */
 static inline int driver_match_device(struct device_driver *drv,
 				      struct device *dev)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index b9fa3ef5b57c..0115bd38a699 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -327,11 +327,20 @@ static int virtblk_get_id(struct gendisk *disk, char *id_str)
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|352| <<virtblk_open>> virtblk_get(vblk);
+ */
 static void virtblk_get(struct virtio_blk *vblk)
 {
 	refcount_inc(&vblk->refs);
 }
 
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|364| <<virtblk_release>> virtblk_put(vblk);
+ *   - drivers/block/virtio_blk.c|934| <<virtblk_remove>> virtblk_put(vblk);
+ */
 static void virtblk_put(struct virtio_blk *vblk)
 {
 	if (refcount_dec_and_test(&vblk->refs)) {
diff --git a/drivers/block/xen-blkfront.c b/drivers/block/xen-blkfront.c
index 10df39a8b18d..f0b51cd37ba3 100644
--- a/drivers/block/xen-blkfront.c
+++ b/drivers/block/xen-blkfront.c
@@ -62,6 +62,35 @@
 
 #include <asm/xen/hypervisor.h>
 
+/*
+ * 启动的时候blkback_changed()一共执行2次:
+ * 
+ * 1. XenbusStateInitWait
+ * 2. XenbusStateConnected
+ *
+ * 当在dom0上hot-remove某一个xvdb的时候.
+ *
+ * blkback_changed(): XenbusStateClosing
+ * -> blkfront_closing()
+ *
+ * [0] blkback_changed
+ * [0] xenwatch_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * v5.13没有commit 05d69d950d9d ("xen-blkfront: sanitize the removal state machine") hotplug xen-blkfront的时候会遇到下面的hang.
+ *
+ * xenwatch是D state
+ *
+ * # cat /proc/102/stack
+ * [<0>] del_gendisk+0x80/0x210
+ * [<0>] xlvbd_release_gendisk+0x54/0x93 [xen_blkfront]
+ * [<0>] blkback_changed+0x3af/0xbd0 [xen_blkfront]
+ * [<0>] xenwatch_thread+0x98/0x160
+ * [<0>] kthread+0x112/0x130
+ * [<0>] ret_from_fork+0x22/0x30
+ */
+
 /*
  * The minimal size of segment supported by the block framework is PAGE_SIZE.
  * When Linux is using a different page size than Xen, it may not be possible
@@ -149,6 +178,22 @@ static unsigned int xen_blkif_max_ring_order;
 module_param_named(max_ring_page_order, xen_blkif_max_ring_order, int, 0444);
 MODULE_PARM_DESC(max_ring_page_order, "Maximum order of pages to be used for the shared ring");
 
+/*
+ * 在以下使用BLK_RING_SIZE():
+ *   - drivers/block/xen-blkfront.c|309| <<get_id_from_freelist>> BUG_ON(free >= BLK_RING_SIZE(rinfo->dev_info));
+ *   - drivers/block/xen-blkfront.c|1021| <<xlvbd_init_blk_queue>> info->tag_set.queue_depth = BLK_RING_SIZE(info) / 2;
+ *   - drivers/block/xen-blkfront.c|1023| <<xlvbd_init_blk_queue>> info->tag_set.queue_depth = BLK_RING_SIZE(info);
+ *   - drivers/block/xen-blkfront.c|1330| <<blkif_free_ring>> for (i = 0; i < BLK_RING_SIZE(info); i++) {
+ *   - drivers/block/xen-blkfront.c|1631| <<blkif_interrupt>> if (id >= BLK_RING_SIZE(info)) {
+ *   - drivers/block/xen-blkfront.c|1944| <<talk_to_blkback>> for (j = 0; j < BLK_RING_SIZE(info); j++)
+ *   - drivers/block/xen-blkfront.c|1946| <<talk_to_blkback>> rinfo->shadow[BLK_RING_SIZE(info)-1].req.u.rw.id = 0x0fffffff;
+ *   - drivers/block/xen-blkfront.c|1985| <<negotiate_mq>> BLK_RING_SIZE(info));
+ *   - drivers/block/xen-blkfront.c|2156| <<blkfront_resume>> for (j = 0; j < BLK_RING_SIZE(info); j++) {
+ *   - drivers/block/xen-blkfront.c|2298| <<blkfront_setup_indirect>> (grants + INDIRECT_GREFS(grants)) * BLK_RING_SIZE(info));
+ *   - drivers/block/xen-blkfront.c|2308| <<blkfront_setup_indirect>> int num = INDIRECT_GREFS(grants) * BLK_RING_SIZE(info);
+ *   - drivers/block/xen-blkfront.c|2319| <<blkfront_setup_indirect>> for (i = 0; i < BLK_RING_SIZE(info); i++) {
+ *   - drivers/block/xen-blkfront.c|2345| <<blkfront_setup_indirect>> for (i = 0; i < BLK_RING_SIZE(info); i++) {
+ */
 #define BLK_RING_SIZE(info)	\
 	__CONST_RING_SIZE(blkif, XEN_PAGE_SIZE * (info)->nr_ring_pages)
 
@@ -218,6 +263,12 @@ struct blkfront_info
 	/* Save uncomplete reqs and bios for migration. */
 	struct list_head requests;
 	struct bio_list bio_list;
+	/*
+	 * 在以下使用blkfront_info->info_list:
+	 *   - drivers/block/xen-blkfront.c|1874| <<free_info>> list_del(&info->info_list);
+	 *   - drivers/block/xen-blkfront.c|2142| <<blkfront_probe>> list_add(&info->info_list, &info_list);
+	 *   - drivers/block/xen-blkfront.c|2914| <<blkfront_delay_work>> list_for_each_entry(info, &info_list, info_list) {
+	 */
 	struct list_head info_list;
 };
 
@@ -467,6 +518,11 @@ static int xlbd_reserve_minors(unsigned int minor, unsigned int nr)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|1219| <<xlvbd_alloc_gendisk>> xlbd_release_minors(minor, nr_minors);
+ *   - drivers/block/xen-blkfront.c|1254| <<xlvbd_release_gendisk>> xlbd_release_minors(minor, nr_minors);
+ */
 static void xlbd_release_minors(unsigned int minor, unsigned int nr)
 {
 	unsigned int end = minor + nr;
@@ -968,6 +1024,10 @@ static void blkif_set_queue_limits(struct blkfront_info *info)
 	blk_queue_dma_alignment(rq, 511);
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|1221| <<xlvbd_alloc_gendisk>> if (xlvbd_init_blk_queue(gd, sector_size, physical_sector_size)) {
+ */
 static int xlvbd_init_blk_queue(struct gendisk *gd, u16 sector_size,
 				unsigned int physical_sector_size)
 {
@@ -1099,6 +1159,10 @@ static char *encode_disk_name(char *ptr, unsigned int n)
 	return ptr + 1;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|2600| <<blkfront_connect>> err = xlvbd_alloc_gendisk(sectors, info, binfo, sector_size,
+ */
 static int xlvbd_alloc_gendisk(blkif_sector_t capacity,
 			       struct blkfront_info *info,
 			       u16 vdisk_info, u16 sector_size,
@@ -1192,11 +1256,23 @@ static int xlvbd_alloc_gendisk(blkif_sector_t capacity,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|2173| <<blkfront_closing>> xlvbd_release_gendisk(info);
+ *   - drivers/block/xen-blkfront.c|2542| <<blkfront_remove>> xlvbd_release_gendisk(info);
+ *   - drivers/block/xen-blkfront.c|2610| <<blkif_release>> xlvbd_release_gendisk(info);
+ *   - drivers/block/xen-blkfront.c|2619| <<blkif_release>> xlvbd_release_gendisk(info);
+ */
 static void xlvbd_release_gendisk(struct blkfront_info *info)
 {
 	unsigned int minor, nr_minors, i;
 	struct blkfront_ring_info *rinfo;
 
+	/*
+	 * 在以下设置blkfront_info->rq:
+	 *   - drivers/block/xen-blkfront.c|1054| <<xlvbd_init_blk_queue>> info->rq = gd->queue = rq;
+	 *   - drivers/block/xen-blkfront.c|1293| <<xlvbd_release_gendisk>> info->rq = NULL;
+	 */
 	if (info->rq == NULL)
 		return;
 
@@ -1211,6 +1287,20 @@ static void xlvbd_release_gendisk(struct blkfront_info *info)
 		flush_work(&rinfo->work);
 	}
 
+	/*
+	 * Removes the gendisk and all its associated resources. This deletes the
+	 * partitions associated with the gendisk, and unregisters the associated
+	 * request_queue.
+	 *
+	 * This is the counter to the respective __device_add_disk() call.
+	 *
+	 * The final removal of the struct gendisk happens when its refcount reaches 0
+	 * with put_disk(), which should be called after del_gendisk(), if
+	 * __device_add_disk() was used.
+	 *
+	 * Drivers exist which depend on the release of the gendisk to be synchronous,
+	 * it should not be deferred.
+	 */
 	del_gendisk(info->gd);
 
 	minor = info->gd->first_minor;
@@ -1222,6 +1312,11 @@ static void xlvbd_release_gendisk(struct blkfront_info *info)
 	info->rq = NULL;
 
 	put_disk(info->gd);
+	/*
+	 * 设置blkfront_info->gd的地方:
+	 *   - drivers/block/xen-blkfront.c|1055| <<xlvbd_init_blk_queue>> info->gd = gd;
+	 *   - drivers/block/xen-blkfront.c|1296| <<xlvbd_release_gendisk>> info->gd = NULL;
+	 */
 	info->gd = NULL;
 }
 
@@ -1349,6 +1444,20 @@ static void blkif_free_ring(struct blkfront_ring_info *rinfo)
 	rinfo->evtchn = rinfo->irq = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|1802| <<setup_blkring>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|1999| <<talk_to_blkback>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|2238| <<blkfront_resume>> blkif_free(info, info->connected == BLKIF_STATE_CONNECTED);
+ *   - drivers/block/xen-blkfront.c|2578| <<blkfront_connect>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|2604| <<blkfront_connect>> blkif_free(info, 0);
+ *   - drivers/block/xen-blkfront.c|2710| <<blkfront_remove>> blkif_free(info, 0);
+ *
+ * 会释放很多ring的data, 最后三行
+ * kvfree(info->rinfo);
+ * info->rinfo = NULL;
+ * info->nr_rings = 0;
+ */
 static void blkif_free(struct blkfront_info *info, int suspend)
 {
 	unsigned int i;
@@ -1376,6 +1485,10 @@ struct copy_from_grant {
 	char *bvec_data;
 };
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|1605| <<blkif_completion>> blkif_copy_from_grant,
+ */
 static void blkif_copy_from_grant(unsigned long gfn, unsigned int offset,
 				  unsigned int len, void *data)
 {
@@ -1433,6 +1546,17 @@ static bool blkif_completion(unsigned long *id,
 	struct scatterlist *sg;
 	int num_sg, num_grant;
 	struct blkfront_info *info = rinfo->dev_info;
+	/*
+	 * struct blkfront_ring_info *rinfo:
+	 * -> struct blk_shadow shadow[];
+	 *    -> struct blkif_request req;
+	 *    -> struct request *request;
+	 *    -> struct grant **grants_used;
+	 *    -> struct grant **indirect_grants;
+	 *    -> struct scatterlist *sg;
+	 *    -> unsigned int num_sg;
+	 *    -> enum blk_req_status status;
+	 */
 	struct blk_shadow *s = &rinfo->shadow[*id];
 	struct copy_from_grant data = {
 		.grant_idx = 0,
@@ -1774,6 +1898,16 @@ static int write_per_ring_nodes(struct xenbus_transaction xbt,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|1903| <<talk_to_blkback>> free_info(info);
+ *   - drivers/block/xen-blkfront.c|2523| <<blkfront_remove>> free_info(info);
+ *   - drivers/block/xen-blkfront.c|2545| <<blkfront_remove>> free_info(info);
+ *   - drivers/block/xen-blkfront.c|2621| <<blkif_release>> free_info(info);
+ *
+ * 把blkfront_info->info_list从全局链表删除
+ * free这个blkfront_info
+ */
 static void free_info(struct blkfront_info *info)
 {
 	list_del(&info->info_list);
@@ -1781,6 +1915,12 @@ static void free_info(struct blkfront_info *info)
 }
 
 /* Common code used when first setting up, and when resuming. */
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|2204| <<blkfront_resume>> err = talk_to_blkback(dev, info);
+ *   - drivers/block/xen-blkfront.c|2603| <<blkback_changed(XenbusStateInitWait)>> if (talk_to_blkback(dev, info))
+ *   - drivers/block/xen-blkfront.c|2627| <<blkback_changed(XenbusStateConnected)>> if (talk_to_blkback(dev, info))
+ */
 static int talk_to_blkback(struct xenbus_device *dev,
 			   struct blkfront_info *info)
 {
@@ -1897,9 +2037,25 @@ static int talk_to_blkback(struct xenbus_device *dev,
 	if (message)
 		xenbus_dev_fatal(dev, err, "%s", message);
  destroy_blkring:
+	/*
+	 * 会释放很多ring的data, 最后三行
+	 * kvfree(info->rinfo);
+	 * info->rinfo = NULL;
+	 * info->nr_rings = 0;
+	 */
 	blkif_free(info, 0);
 
 	mutex_lock(&blkfront_mutex);
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkfront.c|1903| <<talk_to_blkback>> free_info(info);
+	 *   - drivers/block/xen-blkfront.c|2523| <<blkfront_remove>> free_info(info);
+	 *   - drivers/block/xen-blkfront.c|2545| <<blkfront_remove>> free_info(info);
+	 *   - drivers/block/xen-blkfront.c|2621| <<blkif_release>> free_info(info);
+	 *
+	 * 把blkfront_info->info_list从全局链表删除
+	 * free这个blkfront_info
+	 */
 	free_info(info);
 	mutex_unlock(&blkfront_mutex);
 
@@ -2081,6 +2237,9 @@ static int blkif_recover(struct blkfront_info *info)
  * leave the device-layer structures intact so that this is transparent to the
  * rest of the kernel.
  */
+/*
+ * struct xenbus_driver blkfront_driver.resume()
+ */
 static int blkfront_resume(struct xenbus_device *dev)
 {
 	struct blkfront_info *info = dev_get_drvdata(&dev->dev);
@@ -2126,8 +2285,28 @@ static int blkfront_resume(struct xenbus_device *dev)
 		}
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkfront.c|1802| <<setup_blkring>> blkif_free(info, 0);
+	 *   - drivers/block/xen-blkfront.c|1999| <<talk_to_blkback>> blkif_free(info, 0);
+	 *   - drivers/block/xen-blkfront.c|2238| <<blkfront_resume>> blkif_free(info, info->connected == BLKIF_STATE_CONNECTED);
+	 *   - drivers/block/xen-blkfront.c|2578| <<blkfront_connect>> blkif_free(info, 0);
+	 *   - drivers/block/xen-blkfront.c|2604| <<blkfront_connect>> blkif_free(info, 0);
+	 *   - drivers/block/xen-blkfront.c|2710| <<blkfront_remove>> blkif_free(info, 0);
+	 *
+	 * 会释放很多ring的data, 最后三行
+	 * kvfree(info->rinfo);
+	 * info->rinfo = NULL;
+	 * info->nr_rings = 0;
+	 */
 	blkif_free(info, info->connected == BLKIF_STATE_CONNECTED);
 
+	/*
+	 * called by:
+	 *   - drivers/block/xen-blkfront.c|2204| <<blkfront_resume>> err = talk_to_blkback(dev, info);
+	 *   - drivers/block/xen-blkfront.c|2603| <<blkback_changed(XenbusStateInitWait)>> if (talk_to_blkback(dev, info))
+	 *   - drivers/block/xen-blkfront.c|2627| <<blkback_changed(XenbusStateConnected)>> if (talk_to_blkback(dev, info))
+	 */
 	err = talk_to_blkback(dev, info);
 	if (!err)
 		blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
@@ -2141,6 +2320,37 @@ static int blkfront_resume(struct xenbus_device *dev)
 	return err;
 }
 
+/*
+ * 启动的时候blkback_changed()一共执行2次:
+ * 
+ * 1. XenbusStateInitWait
+ * 2. XenbusStateConnected
+ *
+ * 当在dom0上hot-remove某一个xvdb的时候.
+ *
+ * blkback_changed(): XenbusStateClosing
+ * -> blkfront_closing()
+ *
+ * [0] blkback_changed
+ * [0] xenwatch_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * v5.13没有commit 05d69d950d9d ("xen-blkfront: sanitize the removal state machine") hotplug xen-blkfront的时候会遇到下面的hang.
+ *
+ * xenwatch是D state
+ *
+ * # cat /proc/102/stack
+ * [<0>] del_gendisk+0x80/0x210
+ * [<0>] xlvbd_release_gendisk+0x54/0x93 [xen_blkfront]
+ * [<0>] blkback_changed+0x3af/0xbd0 [xen_blkfront]
+ * [<0>] xenwatch_thread+0x98/0x160
+ * [<0>] kthread+0x112/0x130
+ * [<0>] ret_from_fork+0x22/0x30
+ *
+ * called by:
+ *   - drivers/block/xen-blkfront.c|2517| <<blkback_changed(XenbusStateClosing)>> blkfront_closing(info);
+ */
 static void blkfront_closing(struct blkfront_info *info)
 {
 	struct xenbus_device *xbdev = info->xbdev;
@@ -2153,6 +2363,11 @@ static void blkfront_closing(struct blkfront_info *info)
 		return;
 	}
 
+	/*
+	 * 设置blkfront_info->gd的地方:
+	 *   - drivers/block/xen-blkfront.c|1055| <<xlvbd_init_blk_queue>> info->gd = gd;
+	 *   - drivers/block/xen-blkfront.c|1296| <<xlvbd_release_gendisk>> info->gd = NULL;
+	 */
 	if (info->gd)
 		bdev = bdgrab(info->gd->part0);
 
@@ -2165,11 +2380,36 @@ static void blkfront_closing(struct blkfront_info *info)
 
 	mutex_lock(&bdev->bd_mutex);
 
+	/*
+	 * 在以下设置block_device->bd_openers:
+	 *   - fs/block_dev.c|1363| <<__blkdev_get>> bdev->bd_openers++;
+	 *   - fs/block_dev.c|1573| <<__blkdev_put>> if (!--bdev->bd_openers) {
+	 * 在以下使用block_device->bd_openers:
+	 *   - block/partitions/core.c|478| <<bdev_del_partition>> if (part->bd_openers)
+	 *   - drivers/block/nbd.c|1141| <<nbd_bdev_reset>> if (bdev->bd_openers > 1)
+	 *   - drivers/block/nbd.c|1503| <<nbd_release>> disk->part0->bd_openers == 0)
+	 *   - drivers/block/xen-blkfront.c|2224| <<blkfront_closing>> if (bdev->bd_openers) {
+	 *   - drivers/block/xen-blkfront.c|2622| <<blkfront_remove>> xbdev->nodename, bdev->bd_openers);
+	 *   - drivers/block/xen-blkfront.c|2624| <<blkfront_remove>> if (info && !bdev->bd_openers) {
+	 *   - drivers/block/xen-blkfront.c|2679| <<blkif_release>> if (disk->part0->bd_openers)
+	 *   - drivers/block/zram/zram_drv.c|1786| <<reset_store>> if (bdev->bd_openers || zram->claim) {
+	 *   - drivers/block/zram/zram_drv.c|1986| <<zram_remove>> if (bdev->bd_openers || zram->claim) {
+	 *   - fs/block_dev.c|1307| <<__blkdev_get>> if (!bdev->bd_openers) {
+	 *   - fs/block_dev.c|1566| <<__blkdev_put>> if (bdev->bd_openers == 1)
+	 *   - fs/block_dev.c|1944| <<iterate_bdevs>> if (bdev->bd_openers)
+	 */
 	if (bdev->bd_openers) {
 		xenbus_dev_error(xbdev, -EBUSY,
 				 "Device in use; refusing to close");
 		xenbus_switch_state(xbdev, XenbusStateClosing);
 	} else {
+		/*
+		 * called by:
+		 *   - drivers/block/xen-blkfront.c|2173| <<blkfront_closing>> xlvbd_release_gendisk(info);
+		 *   - drivers/block/xen-blkfront.c|2542| <<blkfront_remove>> xlvbd_release_gendisk(info);
+		 *   - drivers/block/xen-blkfront.c|2610| <<blkif_release>> xlvbd_release_gendisk(info);
+		 *   - drivers/block/xen-blkfront.c|2619| <<blkif_release>> xlvbd_release_gendisk(info);
+		 */
 		xlvbd_release_gendisk(info);
 		xenbus_frontend_closed(xbdev);
 	}
@@ -2443,6 +2683,23 @@ static void blkfront_connect(struct blkfront_info *info)
 /*
  * Callback received when the backend's state changes.
  */
+/*
+ * 启动的时候blkback_changed()一共执行2次:
+ *
+ * 1. XenbusStateInitWait
+ * 2. XenbusStateConnected
+ *
+ *
+ * 当在dom0上hot-remove某一个xvdb的时候.
+ *
+ * blkback_changed(): XenbusStateClosing
+ * -> blkfront_closing()
+ *
+ * [0] blkback_changed
+ * [0] xenwatch_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void blkback_changed(struct xenbus_device *dev,
 			    enum xenbus_state backend_state)
 {
@@ -2496,6 +2753,18 @@ static void blkback_changed(struct xenbus_device *dev,
 	}
 }
 
+/*
+ * [0] blkfront_remove
+ * [0] xenbus_dev_remove
+ * [0] device_release_driver_internal
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] xenbus_dev_changed
+ * [0] xenwatch_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static int blkfront_remove(struct xenbus_device *xbdev)
 {
 	struct blkfront_info *info = dev_get_drvdata(&xbdev->dev);
@@ -2507,6 +2776,12 @@ static int blkfront_remove(struct xenbus_device *xbdev)
 	if (!info)
 		return 0;
 
+	/*
+	 * 会释放很多ring的data, 最后三行
+	 * kvfree(info->rinfo);
+	 * info->rinfo = NULL;
+	 * info->nr_rings = 0;
+	 */
 	blkif_free(info, 0);
 
 	mutex_lock(&info->mutex);
@@ -2531,6 +2806,10 @@ static int blkfront_remove(struct xenbus_device *xbdev)
 	 * isn't closed yet, we let release take care of it.
 	 */
 
+	/*
+	 * struct block_device *bdev:
+	 * -> struct mutex bd_mutex;
+	 */
 	mutex_lock(&bdev->bd_mutex);
 	info = disk->private_data;
 
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index f7e83613ae94..57637c9de6cb 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -10,6 +10,13 @@
 
 #define POLL_IDLE_RELAX_COUNT	200
 
+/*
+ * called by:
+ *   - drivers/cpuidle/cpuidle.c|237| <<cpuidle_enter_state>> entered_state = target_state->enter(dev, drv, index);
+ *
+ * 在以下使用poll_idle():
+ *   - drivers/cpuidle/poll_state.c|55| <<cpuidle_poll_state_init>> state->enter = poll_idle;
+ */
 static int __cpuidle poll_idle(struct cpuidle_device *dev,
 			       struct cpuidle_driver *drv, int index)
 {
diff --git a/drivers/crypto/virtio/virtio_crypto_algs.c b/drivers/crypto/virtio/virtio_crypto_algs.c
index 583c0b535d13..f07f201af5cb 100644
--- a/drivers/crypto/virtio/virtio_crypto_algs.c
+++ b/drivers/crypto/virtio/virtio_crypto_algs.c
@@ -613,6 +613,10 @@ static struct virtio_crypto_algo virtio_crypto_algs[] = { {
 	},
 } };
 
+/*
+ * called by:
+ *   - drivers/crypto/virtio/virtio_crypto_mgr.c|240| <<virtcrypto_dev_start>> if (virtio_crypto_algs_register(vcrypto)) {
+ */
 int virtio_crypto_algs_register(struct virtio_crypto *vcrypto)
 {
 	int ret = 0;
diff --git a/drivers/iommu/intel/dmar.c b/drivers/iommu/intel/dmar.c
index 84057cb9596c..eed94b655907 100644
--- a/drivers/iommu/intel/dmar.c
+++ b/drivers/iommu/intel/dmar.c
@@ -56,6 +56,21 @@ struct dmar_res_callback {
  * 2) Use RCU in interrupt context
  */
 DECLARE_RWSEM(dmar_global_lock);
+/*
+ * 在以下使用dmar_drhd_units:
+ *   - drivers/iommu/intel/cap_audit.c|149| <<cap_audit_static>> if (list_empty(&dmar_drhd_units))
+ *   - drivers/iommu/intel/dmar.c|77| <<dmar_register_drhd_unit>> list_add_tail_rcu(&drhd->list, &dmar_drhd_units);
+ *   - drivers/iommu/intel/dmar.c|79| <<dmar_register_drhd_unit>> list_add_rcu(&drhd->list, &dmar_drhd_units);
+ *   - drivers/iommu/intel/dmar.c|397| <<dmar_find_dmaru>> list_for_each_entry_rcu(dmaru, &dmar_drhd_units, list,
+ *   - drivers/iommu/intel/dmar.c|812| <<dmar_dev_scope_init>> if (list_empty(&dmar_drhd_units)) {
+ *   - drivers/iommu/intel/dmar.c|853| <<dmar_table_init>> } else if (list_empty(&dmar_drhd_units)) {
+ *   - drivers/iommu/intel/dmar.c|2127| <<dmar_free_unused_resources>> if (dmar_dev_scope_status != 1 && !list_empty(&dmar_drhd_units))
+ *   - drivers/iommu/intel/dmar.c|2131| <<dmar_free_unused_resources>> list_for_each_entry_safe(dmaru, dmaru_n, &dmar_drhd_units, list) {
+ *   - include/linux/dmar.h|74| <<for_each_drhd_unit>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|78| <<for_each_active_drhd_unit>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|83| <<for_each_active_iommu>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|88| <<for_each_iommu>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ */
 LIST_HEAD(dmar_drhd_units);
 
 struct acpi_table_header * __initdata dmar_tbl;
@@ -1819,6 +1834,10 @@ static const char *irq_remap_fault_reasons[] =
 	"Blocked an interrupt request due to source-id verification failure",
 };
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/dmar.c|1913| <<dmar_fault_do_one>> reason = dmar_get_fault_reason(fault_reason, &fault_type);
+ */
 static const char *dmar_get_fault_reason(u8 fault_reason, int *fault_type)
 {
 	if (fault_reason >= 0x20 && (fault_reason - 0x20 <
@@ -1903,6 +1922,10 @@ void dmar_msi_read(int irq, struct msi_msg *msg)
 	raw_spin_unlock_irqrestore(&iommu->register_lock, flag);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/dmar.c|1991| <<dmar_fault>> dmar_fault_do_one(iommu, type, fault_reason,
+ */
 static int dmar_fault_do_one(struct intel_iommu *iommu, int type,
 		u8 fault_reason, u32 pasid, u16 source_id,
 		unsigned long long addr)
@@ -1927,6 +1950,13 @@ static int dmar_fault_do_one(struct intel_iommu *iommu, int type,
 }
 
 #define PRIMARY_FAULT_REG_LEN (16)
+/*
+ * 在以下使用dmar_fault():
+ *   - drivers/iommu/intel/dmar.c|2027| <<dmar_set_interrupt>> ret = request_irq(irq, dmar_fault, IRQF_NO_THREAD, iommu->name, iommu);
+ *   - drivers/iommu/intel/dmar.c|2054| <<enable_drhd_fault_handling>> dmar_fault(iommu->irq, iommu);
+ *   - drivers/iommu/intel/iommu.c|2905| <<intel_iommu_init_qi>> dmar_fault(-1, iommu);
+ *   - drivers/iommu/intel/irq_remapping.c|593| <<intel_setup_irq_remapping>> dmar_fault(-1, iommu);
+ */
 irqreturn_t dmar_fault(int irq, void *dev_id)
 {
 	struct intel_iommu *iommu = dev_id;
diff --git a/drivers/iommu/intel/irq_remapping.c b/drivers/iommu/intel/irq_remapping.c
index f912fe45bea2..8c37bc33d00e 100644
--- a/drivers/iommu/intel/irq_remapping.c
+++ b/drivers/iommu/intel/irq_remapping.c
@@ -470,6 +470,11 @@ static int iommu_load_old_irte(struct intel_iommu *iommu)
 }
 
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|618| <<intel_setup_irq_remapping>> iommu_set_irq_remapping(iommu, eim_mode);
+ *   - drivers/iommu/intel/irq_remapping.c|1072| <<reenable_irq_remapping>> iommu_set_irq_remapping(iommu, eim);
+ */
 static void iommu_set_irq_remapping(struct intel_iommu *iommu, int mode)
 {
 	unsigned long flags;
@@ -1104,6 +1109,10 @@ void intel_irq_remap_add_device(struct dmar_pci_notify_info *info)
 	dev_set_msi_domain(&info->dev->dev, map_dev_to_ir(info->dev));
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1285| <<intel_irq_remapping_prepare_irte>> prepare_irte(irte, irq_cfg->vector, irq_cfg->dest_apicid);
+ */
 static void prepare_irte(struct irte *irte, int vector, unsigned int dest)
 {
 	memset(irte, 0, sizeof(*irte));
@@ -1187,6 +1196,23 @@ intel_ir_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	return IRQ_SET_MASK_OK_DONE;
 }
 
+/*
+ * [0] intel_ir_compose_msi_msg
+ * [0] irq_chip_compose_msi_msg
+ * [0] msi_domain_activate
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void intel_ir_compose_msi_msg(struct irq_data *irq_data,
 				     struct msi_msg *msg)
 {
@@ -1254,6 +1280,23 @@ static void fill_msi_msg(struct msi_msg *msg, u32 index, u32 subhandle)
 	msg->arch_data.dmar_subhandle = subhandle;
 }
 
+/*
+ * [0] intel_irq_remapping_alloc
+ * [0] intel_irq_remapping_alloc
+ * [0] msi_domain_alloc
+ * [0] __irq_domain_alloc_irqs
+ * [0] __msi_domain_alloc_irqs
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1381| <<intel_irq_remapping_alloc>> intel_irq_remapping_prepare_irte(ird, irq_cfg, info, index, i);
+ */
 static void intel_irq_remapping_prepare_irte(struct intel_ir_data *data,
 					     struct irq_cfg *irq_cfg,
 					     struct irq_alloc_info *info,
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index 808ab70d5df5..96a182d0556a 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -26,6 +26,12 @@
 #include <trace/events/iommu.h>
 
 static struct kset *iommu_group_kset;
+/*
+ * 在以下使用iommu_group_ida:
+ *   - drivers/iommu/iommu.c|597| <<iommu_group_release>> ida_simple_remove(&iommu_group_ida, group->id);
+ *   - drivers/iommu/iommu.c|653| <<iommu_group_alloc>> ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
+ *   - drivers/iommu/iommu.c|663| <<iommu_group_alloc>> ida_simple_remove(&iommu_group_ida, group->id);
+ */
 static DEFINE_IDA(iommu_group_ida);
 
 static unsigned int iommu_def_domain_type __read_mostly;
@@ -150,6 +156,29 @@ subsys_initcall(iommu_subsys_init);
  *
  * Return: 0 on success, or an error.
  */
+/*
+ * called by:
+ *   - drivers/iommu/amd/init.c|1891| <<iommu_init_pci>> iommu_device_register(&iommu->iommu, &amd_iommu_ops, NULL);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c|3669| <<arm_smmu_device_probe>> ret = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);
+ *   - drivers/iommu/arm/arm-smmu/arm-smmu.c|2164| <<arm_smmu_device_probe>> err = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);
+ *   - drivers/iommu/arm/arm-smmu/qcom_iommu.c|850| <<qcom_iommu_device_probe>> ret = iommu_device_register(&qcom_iommu->iommu, &qcom_iommu_ops, dev);
+ *   - drivers/iommu/exynos-iommu.c|633| <<exynos_sysmmu_probe>> ret = iommu_device_register(&data->iommu, &exynos_iommu_ops, dev);
+ *   - drivers/iommu/fsl_pamu_domain.c|477| <<pamu_domain_init>> ret = iommu_device_register(&pamu_iommu, &fsl_pamu_ops, NULL);
+ *   - drivers/iommu/intel/dmar.c|1158| <<alloc_iommu>> err = iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);
+ *   - drivers/iommu/intel/iommu.c|4402| <<intel_iommu_init>> iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);
+ *   - drivers/iommu/ipmmu-vmsa.c|1079| <<ipmmu_probe>> ret = iommu_device_register(&mmu->iommu, &ipmmu_ops, &pdev->dev);
+ *   - drivers/iommu/msm_iommu.c|795| <<msm_iommu_probe>> ret = iommu_device_register(&iommu->iommu, &msm_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/mtk_iommu.c|895| <<mtk_iommu_probe>> ret = iommu_device_register(&data->iommu, &mtk_iommu_ops, dev);
+ *   - drivers/iommu/mtk_iommu_v1.c|619| <<mtk_iommu_probe>> ret = iommu_device_register(&data->iommu, &mtk_iommu_ops, dev);
+ *   - drivers/iommu/omap-iommu.c|1238| <<omap_iommu_probe>> err = iommu_device_register(&obj->iommu, &omap_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/rockchip-iommu.c|1199| <<rk_iommu_probe>> err = iommu_device_register(&iommu->iommu, &rk_iommu_ops, dev);
+ *   - drivers/iommu/s390-iommu.c|336| <<zpci_init_iommu>> rc = iommu_device_register(&zdev->iommu_dev, &s390_iommu_ops, NULL);
+ *   - drivers/iommu/sprd-iommu.c|511| <<sprd_iommu_probe>> ret = iommu_device_register(&sdev->iommu, &sprd_iommu_ops, dev);
+ *   - drivers/iommu/sun50i-iommu.c|971| <<sun50i_iommu_probe>> ret = iommu_device_register(&iommu->iommu, &sun50i_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/tegra-gart.c|356| <<tegra_gart_probe>> err = iommu_device_register(&gart->iommu, &gart_iommu_ops, dev);
+ *   - drivers/iommu/tegra-smmu.c|1148| <<tegra_smmu_probe>> err = iommu_device_register(&smmu->iommu, &tegra_smmu_ops, dev);
+ *   - drivers/iommu/virtio-iommu.c|1069| <<viommu_probe>> iommu_device_register(&viommu->iommu, &viommu_ops, parent_dev);
+ */
 int iommu_device_register(struct iommu_device *iommu,
 			  const struct iommu_ops *ops, struct device *hwdev)
 {
@@ -596,6 +625,22 @@ static struct kobj_type iommu_group_ktype = {
  * group to be automatically reclaimed once it has no devices or external
  * references.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|985| <<iommu_register_group>> grp = iommu_group_alloc();
+ *   - drivers/iommu/fsl_pamu_domain.c|342| <<get_device_iommu_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1462| <<generic_device_group>> return iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1540| <<pci_device_group>> return iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1552| <<fsl_mc_device_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/ipmmu-vmsa.c|879| <<ipmmu_find_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/mtk_iommu.c|609| <<mtk_iommu_device_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/omap-iommu.c|1229| <<omap_iommu_probe>> obj->group = iommu_group_alloc();
+ *   - drivers/iommu/rockchip-iommu.c|1189| <<rk_iommu_probe>> iommu->group = iommu_group_alloc();
+ *   - drivers/iommu/sprd-iommu.c|501| <<sprd_iommu_probe>> sdev->group = iommu_group_alloc();
+ *   - drivers/iommu/sun50i-iommu.c|934| <<sun50i_iommu_probe>> iommu->group = iommu_group_alloc();
+ *   - drivers/vfio/mdev/mdev_driver.c|21| <<mdev_attach_iommu>> group = iommu_group_alloc();
+ *   - drivers/vfio/vfio.c|124| <<vfio_iommu_group_get>> group = iommu_group_alloc();
+ */
 struct iommu_group *iommu_group_alloc(void)
 {
 	struct iommu_group *group;
@@ -611,6 +656,12 @@ struct iommu_group *iommu_group_alloc(void)
 	INIT_LIST_HEAD(&group->entry);
 	BLOCKING_INIT_NOTIFIER_HEAD(&group->notifier);
 
+	/*
+	 * 在以下使用iommu_group_ida:
+	 *   - drivers/iommu/iommu.c|597| <<iommu_group_release>> ida_simple_remove(&iommu_group_ida, group->id);
+	 *   - drivers/iommu/iommu.c|653| <<iommu_group_alloc>> ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
+	 *   - drivers/iommu/iommu.c|663| <<iommu_group_alloc>> ida_simple_remove(&iommu_group_ida, group->id);
+	 */
 	ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
 	if (ret < 0) {
 		kfree(group);
@@ -834,6 +885,13 @@ static bool iommu_is_attach_deferred(struct iommu_domain *domain,
  * This function is called by an iommu driver to add a device into a
  * group.  Adding a device increments the group reference count.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|1161| <<iommu_add_device>> return iommu_group_add_device(table_group->group, dev);
+ *   - drivers/iommu/iommu.c|1609| <<iommu_group_get_for_dev>> ret = iommu_group_add_device(group, dev);
+ *   - drivers/vfio/mdev/mdev_driver.c|25| <<mdev_attach_iommu>> ret = iommu_group_add_device(group, &mdev->dev);
+ *   - drivers/vfio/vfio.c|130| <<vfio_iommu_group_get>> ret = iommu_group_add_device(group, dev);
+ */
 int iommu_group_add_device(struct iommu_group *group, struct device *dev)
 {
 	int ret, i = 0;
@@ -890,6 +948,16 @@ int iommu_group_add_device(struct iommu_group *group, struct device *dev)
 
 	trace_add_device_to_group(group->id, dev);
 
+	/*
+	 * [    0.316738] iommu: Default domain type: Translated
+	 * [    0.498945] pci 0000:00:00.0: Adding to iommu group 0
+	 * [    0.499739] pci 0000:00:01.0: Adding to iommu group 1
+	 * [    0.500488] pci 0000:00:02.0: Adding to iommu group 2
+	 * [    0.501261] pci 0000:00:03.0: Adding to iommu group 3
+	 * [    0.502012] pci 0000:00:1f.0: Adding to iommu group 4
+	 * [    0.502787] pci 0000:00:1f.2: Adding to iommu group 4
+	 * [    0.503506] pci 0000:00:1f.3: Adding to iommu group 4
+	 */
 	dev_info(dev, "Adding to iommu group %d\n", group->id);
 
 	return 0;
@@ -1427,6 +1495,16 @@ EXPORT_SYMBOL_GPL(generic_device_group);
  * Use standard PCI bus topology, isolation features, and DMA alias quirks
  * to find or create an IOMMU group for a device.
  */
+/*
+ * called by:
+ *   - drivers/iommu/amd/iommu.c|1738| <<amd_iommu_device_group>> return pci_device_group(dev);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c|2546| <<arm_smmu_device_group>> group = pci_device_group(dev);
+ *   - drivers/iommu/arm/arm-smmu/arm-smmu.c|1473| <<arm_smmu_device_group>> group = pci_device_group(dev);
+ *   - drivers/iommu/fsl_pamu_domain.c|394| <<get_pci_device_group>> group = pci_device_group(&pdev->dev);
+ *   - drivers/iommu/intel/iommu.c|5290| <<intel_iommu_device_group>> return pci_device_group(dev);
+ *   - drivers/iommu/tegra-smmu.c|929| <<tegra_smmu_device_group>> group->group = pci_device_group(dev);
+ *   - drivers/iommu/virtio-iommu.c|924| <<viommu_device_group>> return pci_device_group(dev);
+ */
 struct iommu_group *pci_device_group(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
diff --git a/drivers/misc/pvpanic/pvpanic.c b/drivers/misc/pvpanic/pvpanic.c
index 65f70a4da8c0..3e299e3f2945 100644
--- a/drivers/misc/pvpanic/pvpanic.c
+++ b/drivers/misc/pvpanic/pvpanic.c
@@ -25,6 +25,13 @@ MODULE_AUTHOR("Mihai Carabas <mihai.carabas@oracle.com>");
 MODULE_DESCRIPTION("pvpanic device driver ");
 MODULE_LICENSE("GPL");
 
+/*
+ * 在以下使用pvpanic_list:
+ *   - drivers/misc/pvpanic/pvpanic.c|37| <<pvpanic_send_event>> list_for_each_entry(pi_cur, &pvpanic_list, list) {
+ *   - drivers/misc/pvpanic/pvpanic.c|69| <<pvpanic_probe>> list_add(&pi->list, &pvpanic_list);
+ *   - drivers/misc/pvpanic/pvpanic.c|84| <<pvpanic_remove>> list_for_each_entry_safe(pi_cur, pi_next, &pvpanic_list, list) {
+ *   - drivers/misc/pvpanic/pvpanic.c|96| <<pvpanic_init>> INIT_LIST_HEAD(&pvpanic_list);
+ */
 static struct list_head pvpanic_list;
 static spinlock_t pvpanic_lock;
 
@@ -60,6 +67,25 @@ static struct notifier_block pvpanic_panic_nb = {
 	.priority = 1, /* let this called before broken drm_fb_helper */
 };
 
+/*
+ * [0] pvpanic_probe
+ * [0] platform_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/misc/pvpanic/pvpanic-mmio.c|109| <<pvpanic_mmio_probe>> return pvpanic_probe(pi);
+ *   - drivers/misc/pvpanic/pvpanic-pci.c|102| <<pvpanic_pci_probe>> return pvpanic_probe(pi);
+ */
 int pvpanic_probe(struct pvpanic_instance *pi)
 {
 	if (!pi || !pi->base)
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 8e3a28ba6b28..58dded18b2e0 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -1117,6 +1117,12 @@ static long tap_ioctl(struct file *file, unsigned int cmd,
 	}
 }
 
+/*
+ * 在以下使用tap_fops:
+ *   - drivers/net/tap.c|1252| <<tap_get_socket>> if (file->f_op != &tap_fops)
+ *   - drivers/net/tap.c|1265| <<tap_get_ptr_ring>> if (file->f_op != &tap_fops)
+ *   - drivers/net/tap.c|1335| <<tap_create_cdev>> cdev_init(tap_cdev, &tap_fops);
+ */
 static const struct file_operations tap_fops = {
 	.owner		= THIS_MODULE,
 	.open		= tap_open,
@@ -1213,6 +1219,11 @@ static int tap_sendmsg(struct socket *sock, struct msghdr *m,
 static int tap_recvmsg(struct socket *sock, struct msghdr *m,
 		       size_t total_len, int flags)
 {
+	/*
+	 * struct tap_queue:
+	 * -> struct sock sk;
+	 * -> struct socket sock;
+	 */
 	struct tap_queue *q = container_of(sock, struct tap_queue, sock);
 	struct sk_buff *skb = m->msg_control;
 	int ret;
@@ -1295,6 +1306,10 @@ int tap_queue_resize(struct tap_dev *tap)
 }
 EXPORT_SYMBOL_GPL(tap_queue_resize);
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|1332| <<tap_create_cdev>> err = tap_list_add(*tap_major, device_name);
+ */
 static int tap_list_add(dev_t major, const char *device_name)
 {
 	struct major_info *tap_major;
@@ -1314,6 +1329,11 @@ static int tap_list_add(dev_t major, const char *device_name)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/net/ipvlan/ipvtap.c|201| <<ipvtap_init>> err = tap_create_cdev(&ipvtap_cdev, &ipvtap_major, "ipvtap",
+ *   - drivers/net/macvtap.c|208| <<macvtap_init>> err = tap_create_cdev(&macvtap_cdev, &macvtap_major, "macvtap",
+ */
 int tap_create_cdev(struct cdev *tap_cdev, dev_t *tap_major,
 		    const char *device_name, struct module *module)
 {
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 84f832806313..dbe39a145d57 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -2102,6 +2102,11 @@ static void *tun_ring_recv(struct tun_file *tfile, int noblock, int *err)
 	return ptr;
 }
 
+/*
+ * called by:
+ *   - drivers/net/tun.c|2156| <<tun_chr_read_iter>> ret = tun_do_read(tun, tfile, to, noblock, NULL);
+ *   - drivers/net/tun.c|2498| <<tun_recvmsg>> ret = tun_do_read(tun, tfile, &m->msg_iter, flags & MSG_DONTWAIT, ptr);
+ */
 static ssize_t tun_do_read(struct tun_struct *tun, struct tun_file *tfile,
 			   struct iov_iter *to,
 			   int noblock, void *ptr)
@@ -2792,6 +2797,11 @@ static void tun_get_iff(struct tun_struct *tun, struct ifreq *ifr)
 
 /* This is like a cut-down ethtool ops, except done via tun fd so no
  * privs required. */
+/*
+ * called by:
+ *   - drivers/net/tap.c|1081| <<tap_ioctl(TUNSETOFFLOAD)>> ret = set_offload(q, arg);
+ *   - drivers/net/tun.c|3147| <<__tun_chr_ioctl(TUNSETOFFLOAD)>> ret = set_offload(tun, arg);
+ */
 static int set_offload(struct tun_struct *tun, unsigned long arg)
 {
 	netdev_features_t features = 0;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 78a01c71a17c..014d6fe5a818 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -214,6 +214,19 @@ struct virtnet_info {
 	u8 hdr_len;
 
 	/* Work struct for refilling if we run low on memory. */
+	/*
+	 * 在以下使用virtnet_info->refill:
+	 *   - drivers/net/virtio_net.c|1383| <<refill_work>> container_of(work, struct virtnet_info, refill.work);
+	 *   - drivers/net/virtio_net.c|1398| <<refill_work>> schedule_delayed_work(&vi->refill, HZ/2);
+	 *   - drivers/net/virtio_net.c|1429| <<virtnet_receive>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1552| <<virtnet_open>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1877| <<_virtnet_set_queues>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1899| <<virtnet_close>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2375| <<virtnet_freeze_down>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2401| <<virtnet_restore_up>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|2883| <<virtnet_alloc_queues>> INIT_DELAYED_WORK(&vi->refill, refill_work);
+	 *   - drivers/net/virtio_net.c|3234| <<virtnet_probe>> cancel_delayed_work_sync(&vi->refill);
+	 */
 	struct delayed_work refill;
 
 	/* Work struct for config space updates */
@@ -319,6 +332,13 @@ static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 	return p;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|339| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|354| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|1340| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|1352| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
@@ -1340,6 +1360,15 @@ static void skb_recv_done(struct virtqueue *rvq)
 	virtqueue_napi_schedule(&rq->napi, rvq);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1371| <<virtnet_napi_tx_enable>> return virtnet_napi_enable(vq, napi);
+ *   - drivers/net/virtio_net.c|1392| <<refill_work>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|1565| <<virtnet_open>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2404| <<virtnet_restore_up>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2538| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2555| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ */
 static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 {
 	napi_enable(napi);
@@ -1854,6 +1883,12 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1888| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2161| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2517| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -2690,6 +2725,10 @@ static void virtnet_free_queues(struct virtnet_info *vi)
 	kfree(vi->ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2712| <<free_receive_bufs>> _free_receive_bufs(vi);
+ */
 static void _free_receive_bufs(struct virtnet_info *vi)
 {
 	struct bpf_prog *old_prog;
@@ -2706,6 +2745,10 @@ static void _free_receive_bufs(struct virtnet_info *vi)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3249| <<remove_vq_common>> free_receive_bufs(vi);
+ */
 static void free_receive_bufs(struct virtnet_info *vi)
 {
 	rtnl_lock();
@@ -3021,6 +3064,41 @@ static int virtnet_validate(struct virtio_device *vdev)
 	return 0;
 }
 
+/*
+ * [0] virtnet_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] register_virtio_device
+ * [0] virtio_vdpa_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] ? _vdpa_register_device
+ * [0] ? bus_find_device
+ * [0] vdpasim_net_dev_add
+ * [0] ? vdpasim_net_dev_add
+ * [0] ? vdpasim_create
+ * [0] vdpa_nl_cmd_dev_add_set_doit
+ * [0] genl_family_rcv_msg_doit.isra.17
+ * [0] genl_rcv_msg
+ * [0] netlink_rcv_skb
+ * [0] genl_rcv
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] __sys_sendto
+ * [0] __x64_sys_sendto
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int virtnet_probe(struct virtio_device *vdev)
 {
 	int i, err = -ENOMEM;
diff --git a/drivers/net/xen-netback/rx.c b/drivers/net/xen-netback/rx.c
index accc991d153f..02a4c84c60f1 100644
--- a/drivers/net/xen-netback/rx.c
+++ b/drivers/net/xen-netback/rx.c
@@ -137,6 +137,11 @@ static void xenvif_rx_queue_drop_expired(struct xenvif_queue *queue)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|183| <<xenvif_rx_copy_add>> xenvif_rx_copy_flush(queue);
+ *   - drivers/net/xen-netback/rx.c|487| <<xenvif_rx_action>> xenvif_rx_copy_flush(queue);
+ */
 static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 {
 	unsigned int i;
@@ -171,6 +176,10 @@ static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 	__skb_queue_purge(queue->rx_copy.completed);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|387| <<xenvif_rx_data_slot>> xenvif_rx_copy_add(queue, req, offset, data, len);
+ */
 static void xenvif_rx_copy_add(struct xenvif_queue *queue,
 			       struct xen_netif_rx_request *req,
 			       unsigned int offset, void *data, size_t len)
@@ -371,6 +380,10 @@ static void xenvif_rx_next_chunk(struct xenvif_queue *queue,
 	*len = chunk_len;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|461| <<xenvif_rx_skb>> xenvif_rx_data_slot(queue, &pkt, req, rsp);
+ */
 static void xenvif_rx_data_slot(struct xenvif_queue *queue,
 				struct xenvif_pkt_state *pkt,
 				struct xen_netif_rx_request *req,
@@ -439,6 +452,10 @@ static void xenvif_rx_extra_slot(struct xenvif_queue *queue,
 	BUG();
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|482| <<xenvif_rx_action>> xenvif_rx_skb(queue);
+ */
 static void xenvif_rx_skb(struct xenvif_queue *queue)
 {
 	struct xenvif_pkt_state pkt;
@@ -469,6 +486,10 @@ static void xenvif_rx_skb(struct xenvif_queue *queue)
 
 #define RX_BATCH_SIZE 64
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|646| <<xenvif_kthread_guest_rx>> xenvif_rx_action(queue);
+ */
 void xenvif_rx_action(struct xenvif_queue *queue)
 {
 	struct sk_buff_head completed_skbs;
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 44275908d61a..fbc4a707a916 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -356,6 +356,25 @@ static void xennet_alloc_rx_buffers(struct netfront_queue *queue)
 		notify_remote_via_irq(queue->rx_irq);
 }
 
+/*
+ * [0] xennet_open
+ * [0] __dev_open
+ * [0] __dev_change_flags
+ * [0] dev_change_flags
+ * [0] do_setlink
+ * [0] __rtnl_newlink
+ * [0] rtnl_newlink
+ * [0] rtnetlink_rcv_msg
+ * [0] netlink_rcv_skb
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int xennet_open(struct net_device *dev)
 {
 	struct netfront_info *np = netdev_priv(dev);
@@ -380,6 +399,9 @@ static int xennet_open(struct net_device *dev)
 		spin_unlock_bh(&queue->rx_lock);
 	}
 
+	/*
+	 * xen-netfront只在这调用netif_tx_start_all_queues()
+	 */
 	netif_tx_start_all_queues(dev);
 
 	return 0;
@@ -1515,6 +1537,10 @@ static void xennet_free_netdev(struct net_device *netdev)
 	free_netdev(netdev);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|1617| <<netfront_probe>> netdev = xennet_create_dev(dev);
+ */
 static struct net_device *xennet_create_dev(struct xenbus_device *dev)
 {
 	int err;
@@ -1615,6 +1641,12 @@ static void xennet_end_access(int ref, void *page)
 		gnttab_end_foreign_access(ref, 0, (unsigned long)page);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|1694| <<netfront_resume>> xennet_disconnect_backend(info);
+ *   - drivers/net/xen-netfront.c|2256| <<talk_to_netback>> xennet_disconnect_backend(info);
+ *   - drivers/net/xen-netfront.c|2502| <<xennet_remove>> xennet_disconnect_backend(info);
+ */
 static void xennet_disconnect_backend(struct netfront_info *info)
 {
 	unsigned int i = 0;
@@ -1669,6 +1701,12 @@ static int netfront_resume(struct xenbus_device *dev)
 
 	dev_dbg(&dev->dev, "%s\n", dev->nodename);
 
+	/*
+	 * called by:
+	 *   - drivers/net/xen-netfront.c|1694| <<netfront_resume>> xennet_disconnect_backend(info);
+	 *   - drivers/net/xen-netfront.c|2256| <<talk_to_netback>> xennet_disconnect_backend(info);
+	 *   - drivers/net/xen-netfront.c|2502| <<xennet_remove>> xennet_disconnect_backend(info);
+	 */
 	xennet_disconnect_backend(info);
 	return 0;
 }
@@ -1973,6 +2011,12 @@ static int write_queue_xenstore_keys(struct netfront_queue *queue,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|2165| <<talk_to_netback>> xennet_destroy_queues(info);
+ *   - drivers/net/xen-netfront.c|2272| <<talk_to_netback>> xennet_destroy_queues(info);
+ *   - drivers/net/xen-netfront.c|2527| <<xennet_remove>> xennet_destroy_queues(info);
+ */
 static void xennet_destroy_queues(struct netfront_info *info)
 {
 	unsigned int i;
@@ -2034,6 +2078,14 @@ static int xennet_create_page_pool(struct netfront_queue *queue)
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|2159| <<talk_to_netback>> err = xennet_create_queues(info, &num_queues);
+ *
+ * netback_changed()
+ * -> xennet_connect()
+ *    -> talk_to_netback()
+ */
 static int xennet_create_queues(struct netfront_info *info,
 				unsigned int *num_queues)
 {
@@ -2083,6 +2135,10 @@ static int xennet_create_queues(struct netfront_info *info,
 }
 
 /* Common code used when first setting up, and when resuming. */
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|2286| <<xennet_connect>> err = talk_to_netback(np->xbdev, np);
+ */
 static int talk_to_netback(struct xenbus_device *dev,
 			   struct netfront_info *info)
 {
@@ -2241,6 +2297,10 @@ static int talk_to_netback(struct xenbus_device *dev,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|2358| <<netback_changed>> if (xennet_connect(netdev) != 0)
+ */
 static int xennet_connect(struct net_device *dev)
 {
 	struct netfront_info *np = netdev_priv(dev);
@@ -2255,6 +2315,10 @@ static int xennet_connect(struct net_device *dev)
 		return -ENODEV;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/net/xen-netfront.c|2286| <<xennet_connect>> err = talk_to_netback(np->xbdev, np);
+	 */
 	err = talk_to_netback(np->xbdev, np);
 	if (err)
 		return err;
@@ -2484,6 +2548,12 @@ static int xennet_remove(struct xenbus_device *dev)
 
 	if (info->queues) {
 		rtnl_lock();
+		/*
+		 * called by:
+		 *   - drivers/net/xen-netfront.c|2165| <<talk_to_netback>> xennet_destroy_queues(info);
+		 *   - drivers/net/xen-netfront.c|2272| <<talk_to_netback>> xennet_destroy_queues(info);
+		 *   - drivers/net/xen-netfront.c|2527| <<xennet_remove>> xennet_destroy_queues(info);
+		 */
 		xennet_destroy_queues(info);
 		rtnl_unlock();
 	}
diff --git a/drivers/ptp/ptp_clock.c b/drivers/ptp/ptp_clock.c
index 21c4c34c52d8..fe8f427f6a78 100644
--- a/drivers/ptp/ptp_clock.c
+++ b/drivers/ptp/ptp_clock.c
@@ -201,6 +201,52 @@ static void ptp_aux_kworker(struct kthread_work *work)
 
 /* public interface */
 
+/*
+ * called by:
+ *   - drivers/hv/hv_util.c|751| <<hv_timesync_init>> hv_ptp_clock = ptp_clock_register(&ptp_hyperv_info, NULL);
+ *   - drivers/net/dsa/hirschmann/hellcreek_ptp.c|412| <<hellcreek_ptp_setup>> hellcreek->ptp_clock = ptp_clock_register(&hellcreek->ptp_clock_info,
+ *   - drivers/net/dsa/mv88e6xxx/ptp.c|494| <<mv88e6xxx_ptp_setup>> chip->ptp_clock = ptp_clock_register(&chip->ptp_clock_info, chip->dev);
+ *   - drivers/net/dsa/sja1105/sja1105_ptp.c|871| <<sja1105_ptp_clock_register>> ptp_data->clock = ptp_clock_register(&ptp_data->caps, ds->dev);
+ *   - drivers/net/ethernet/amd/xgbe/xgbe-ptp.c|244| <<xgbe_ptp_register>> clock = ptp_clock_register(info, pdata->dev);
+ *   - drivers/net/ethernet/aquantia/atlantic/aq_ptp.c|1208| <<aq_ptp_init>> clock = ptp_clock_register(&aq_ptp->ptp_info, &aq_nic->ndev->dev);
+ *   - drivers/net/ethernet/broadcom/bnx2x/bnx2x_main.c|13860| <<bnx2x_register_phc>> bp->ptp_clock = ptp_clock_register(&bp->ptp_clock_info, &bp->pdev->dev);
+ *   - drivers/net/ethernet/broadcom/tg3.c|17931| <<tg3_init_one>> tp->ptp_clock = ptp_clock_register(&tp->ptp_info,
+ *   - drivers/net/ethernet/cadence/macb_ptp.c|361| <<gem_ptp_init>> bp->ptp_clock = ptp_clock_register(&bp->ptp_clock_info, &dev->dev);
+ *   - drivers/net/ethernet/cavium/common/cavium_ptp.c|282| <<cavium_ptp_probe>> clock->ptp_clock = ptp_clock_register(&clock->ptp_info, dev);
+ *   - drivers/net/ethernet/cavium/liquidio/lio_main.c|1658| <<oct_ptp_open>> lio->ptp_clock = ptp_clock_register(&lio->ptp_info,
+ *   - drivers/net/ethernet/chelsio/cxgb4/cxgb4_ptp.c|431| <<cxgb4_ptp_init>> adapter->ptp_clock = ptp_clock_register(&adapter->ptp_clock_info,
+ *   - drivers/net/ethernet/freescale/fec_ptp.c|631| <<fec_ptp_init>> fep->ptp_clock = ptp_clock_register(&fep->ptp_caps, &pdev->dev);
+ *   - drivers/net/ethernet/intel/e1000e/ptp.c|328| <<e1000e_ptp_init>> adapter->ptp_clock = ptp_clock_register(&adapter->ptp_clock_info,
+ *   - drivers/net/ethernet/intel/i40e/i40e_ptp.c|715| <<i40e_ptp_create_clock>> pf->ptp_clock = ptp_clock_register(&pf->ptp_caps, &pf->pdev->dev);
+ *   - drivers/net/ethernet/intel/igb/igb_ptp.c|1275| <<igb_ptp_init>> adapter->ptp_clock = ptp_clock_register(&adapter->ptp_caps,
+ *   - drivers/net/ethernet/intel/igc/igc_ptp.c|806| <<igc_ptp_init>> adapter->ptp_clock = ptp_clock_register(&adapter->ptp_caps,
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_ptp.c|1409| <<ixgbe_ptp_create_clock>> adapter->ptp_clock = ptp_clock_register(&adapter->ptp_caps,
+ *   - drivers/net/ethernet/marvell/mvpp2/mvpp2_tai.c|450| <<mvpp22_tai_probe>> tai->ptp_clock = ptp_clock_register(&tai->caps, dev);
+ *   - drivers/net/ethernet/marvell/octeontx2/nic/otx2_ptp.c|165| <<otx2_ptp_init>> ptp_ptr->ptp_clock = ptp_clock_register(&ptp_ptr->ptp_info, pfvf->dev);
+ *   - drivers/net/ethernet/mellanox/mlx4/en_clock.c|294| <<mlx4_en_init_timestamp>> mdev->ptp_clock = ptp_clock_register(&mdev->ptp_clock_info,
+ *   - drivers/net/ethernet/mellanox/mlx5/core/lib/clock.c|893| <<mlx5_init_clock>> clock->ptp = ptp_clock_register(&clock->ptp_info,
+ *   - drivers/net/ethernet/mellanox/mlxsw/spectrum_ptp.c|290| <<mlxsw_sp1_ptp_clock_init>> clock->ptp = ptp_clock_register(&clock->ptp_info, dev);
+ *   - drivers/net/ethernet/microchip/lan743x_ptp.c|1037| <<lan743x_ptp_open>> ptp->ptp_clock = ptp_clock_register(&ptp->ptp_clock_info,
+ *   - drivers/net/ethernet/mscc/ocelot_ptp.c|323| <<ocelot_init_timestamp>> ptp_clock = ptp_clock_register(&ocelot->ptp_info, ocelot->dev);
+ *   - drivers/net/ethernet/pensando/ionic/ionic_phc.c|465| <<ionic_lif_register_phc>> lif->phc->ptp = ptp_clock_register(&lif->phc->ptp_info, lif->ionic->dev);
+ *   - drivers/net/ethernet/qlogic/qede/qede_ptp.c|476| <<qede_ptp_enable>> ptp->clock = ptp_clock_register(&ptp->clock_info, &edev->pdev->dev);
+ *   - drivers/net/ethernet/renesas/ravb_ptp.c|345| <<ravb_ptp_init>> priv->ptp.clock = ptp_clock_register(&priv->ptp.info, &pdev->dev);
+ *   - drivers/net/ethernet/sfc/ptp.c|1498| <<efx_ptp_probe>> ptp->phc_clock = ptp_clock_register(&ptp->phc_clock_info,
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_ptp.c|280| <<stmmac_ptp_register>> priv->ptp_clock = ptp_clock_register(&priv->ptp_clock_ops,
+ *   - drivers/net/ethernet/ti/am65-cpts.c|1000| <<am65_cpts_create>> cpts->ptp_clock = ptp_clock_register(&cpts->ptp_info, cpts->dev);
+ *   - drivers/net/ethernet/ti/cpts.c|578| <<cpts_register>> cpts->clock = ptp_clock_register(&cpts->info, cpts->dev);
+ *   - drivers/net/ethernet/xscale/ptp_ixp46x.c|295| <<ptp_ixp_init>> ixp_clock.ptp_clock = ptp_clock_register(&ixp_clock.caps, NULL);
+ *   - drivers/net/phy/dp83640.c|1483| <<dp83640_probe>> clock->ptp_clock = ptp_clock_register(&clock->caps,
+ *   - drivers/net/phy/mscc/mscc_ptp.c|1495| <<__vsc8584_init_ptp>> vsc8531->ptp->ptp_clock = ptp_clock_register(&vsc8531->ptp->caps,
+ *   - drivers/ptp/ptp_clockmatrix.c|2128| <<idtcm_enable_channel>> channel->ptp_clock = ptp_clock_register(&channel->caps, NULL);
+ *   - drivers/ptp/ptp_dte.c|256| <<ptp_dte_probe>> ptp_dte->ptp_clk = ptp_clock_register(&ptp_dte->caps, &pdev->dev);
+ *   - drivers/ptp/ptp_idt82p33.c|902| <<idt82p33_enable_channel>> channel->ptp_clock = ptp_clock_register(&channel->caps, NULL);
+ *   - drivers/ptp/ptp_kvm_common.c|148| <<ptp_kvm_init>> kvm_ptp_clock.ptp_clock = ptp_clock_register(&kvm_ptp_clock.caps, NULL);
+ *   - drivers/ptp/ptp_ocp.c|338| <<ptp_ocp_probe>> bp->ptp = ptp_clock_register(&bp->ptp_info, &pdev->dev);
+ *   - drivers/ptp/ptp_pch.c|577| <<pch_probe>> chip->ptp_clock = ptp_clock_register(&chip->caps, &pdev->dev);
+ *   - drivers/ptp/ptp_qoriq.c|530| <<ptp_qoriq_init>> ptp_qoriq->clock = ptp_clock_register(&ptp_qoriq->caps, ptp_qoriq->dev);
+ *   - drivers/ptp/ptp_vmw.c|94| <<ptp_vmw_acpi_add>> ptp_vmw_clock = ptp_clock_register(&ptp_vmw_clock_info, NULL);
+ */
 struct ptp_clock *ptp_clock_register(struct ptp_clock_info *info,
 				     struct device *parent)
 {
diff --git a/drivers/ptp/ptp_kvm_common.c b/drivers/ptp/ptp_kvm_common.c
index fcae32f56f25..520a84b0b098 100644
--- a/drivers/ptp/ptp_kvm_common.c
+++ b/drivers/ptp/ptp_kvm_common.c
@@ -125,6 +125,13 @@ static const struct ptp_clock_info ptp_kvm_caps = {
 
 /* module operations */
 
+/*
+ * 在以下使用kvm_ptp_clock:
+ *   - drivers/ptp/ptp_kvm_common.c|132| <<ptp_kvm_exit>> ptp_clock_unregister(kvm_ptp_clock.ptp_clock);
+ *   - drivers/ptp/ptp_kvm_common.c|146| <<ptp_kvm_init>> kvm_ptp_clock.caps = ptp_kvm_caps;
+ *   - drivers/ptp/ptp_kvm_common.c|148| <<ptp_kvm_init>> kvm_ptp_clock.ptp_clock = ptp_clock_register(&kvm_ptp_clock.caps, NULL);
+ *   - drivers/ptp/ptp_kvm_common.c|150| <<ptp_kvm_init>> return PTR_ERR_OR_ZERO(kvm_ptp_clock.ptp_clock);
+ */
 static struct kvm_ptp_clock kvm_ptp_clock;
 
 static void __exit ptp_kvm_exit(void)
diff --git a/drivers/scsi/scsi.c b/drivers/scsi/scsi.c
index e9e2f0e15ac8..22b54afc7364 100644
--- a/drivers/scsi/scsi.c
+++ b/drivers/scsi/scsi.c
@@ -164,6 +164,12 @@ void scsi_log_completion(struct scsi_cmnd *cmd, int disposition)
  *              request, waking processes that are waiting on results,
  *              etc.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|184| <<scmd_eh_abort_handler>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_error.c|2160| <<scsi_eh_flush_done_q>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_lib.c|1439| <<scsi_complete>> scsi_finish_command(cmd);
+ */
 void scsi_finish_command(struct scsi_cmnd *cmd)
 {
 	struct scsi_device *sdev = cmd->device;
diff --git a/drivers/scsi/scsi_error.c b/drivers/scsi/scsi_error.c
index d8fafe77dbbe..f35489c88fc6 100644
--- a/drivers/scsi/scsi_error.c
+++ b/drivers/scsi/scsi_error.c
@@ -498,6 +498,14 @@ static void scsi_report_sense(struct scsi_device *sdev,
  *	When a deferred error is detected the current command has
  *	not been executed and needs retrying.
  */
+/*
+ * called by:
+ *   - drivers/ata/libata-eh.c|1602| <<ata_eh_analyze_tf>> enum scsi_disposition ret = scsi_check_sense(qc->scsicmd);
+ *   - drivers/scsi/scsi_error.c|739| <<scsi_eh_completed_normally>> return scsi_check_sense(scmd);
+ *   - drivers/scsi/scsi_error.c|761| <<scsi_eh_completed_normally>> return scsi_check_sense(scmd);
+ *   - drivers/scsi/scsi_error.c|1451| <<scsi_eh_stu>> scsi_check_sense(scmd) == FAILED ) {
+ *   - drivers/scsi/scsi_error.c|1950| <<scsi_decide_disposition>> rtn = scsi_check_sense(scmd);
+ */
 enum scsi_disposition scsi_check_sense(struct scsi_cmnd *scmd)
 {
 	struct scsi_device *sdev = scmd->device;
@@ -723,6 +731,10 @@ static void scsi_handle_queue_full(struct scsi_device *sdev)
  *    don't allow for the possibility of retries here, and we are a lot
  *    more restrictive about what we consider acceptable.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|1148| <<scsi_send_eh_cmnd>> rtn = scsi_eh_completed_normally(scmd);
+ */
 static enum scsi_disposition scsi_eh_completed_normally(struct scsi_cmnd *scmd)
 {
 	/*
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 532304d42f00..8c16d50faed6 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -203,6 +203,10 @@ void scsi_queue_insert(struct scsi_cmnd *cmd, int reason)
  * Returns the scsi_cmnd result field if a command was executed, or a negative
  * Linux error code if we didn't get that far.
  */
+/*
+ * called by:
+ *   - include/scsi/scsi_device.h|460| <<scsi_execute>> __scsi_execute(sdev, cmd, data_direction, buffer, bufflen, \
+ */
 int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
 		 int data_direction, void *buffer, unsigned bufflen,
 		 unsigned char *sense, struct scsi_sense_hdr *sshdr,
@@ -213,6 +217,9 @@ int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
 	struct scsi_request *rq;
 	int ret = DRIVER_ERROR << 24;
 
+	/*
+	 * probe lun是DMA_FROM_DEVICE, 对应REQ_OP_SCSI_IN
+	 */
 	req = blk_get_request(sdev->request_queue,
 			data_direction == DMA_TO_DEVICE ?
 			REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN,
@@ -221,6 +228,9 @@ int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
 		return ret;
 	rq = scsi_req(req);
 
+	/*
+	 * map kernel data to a request, for passthrough requests
+	 */
 	if (bufflen &&	blk_rq_map_kern(sdev->request_queue, req,
 					buffer, bufflen, GFP_NOIO))
 		goto out;
@@ -642,6 +652,19 @@ static bool scsi_cmd_runtime_exceeced(struct scsi_cmnd *cmd)
 }
 
 /* Helper for scsi_io_completion() when special action required. */
+/*
+ * [0] scsi_io_completion.cold
+ * [0] virtscsi_req_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] __common_interrupt
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|956| <<scsi_io_completion>> scsi_io_completion_action(cmd, result);
+ */
 static void scsi_io_completion_action(struct scsi_cmnd *cmd, int result)
 {
 	struct request_queue *q = cmd->device->request_queue;
@@ -904,6 +927,10 @@ static int scsi_io_completion_nz_result(struct scsi_cmnd *cmd, int result,
  *   c) We can call scsi_end_request() with blk_stat other than
  *	BLK_STS_OK, to fail the remainder of the request.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi.c|214| <<scsi_finish_command>> scsi_io_completion(cmd, good_bytes);
+ */
 void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)
 {
 	int result = cmd->result;
@@ -1007,6 +1034,18 @@ blk_status_t scsi_alloc_sgtables(struct scsi_cmnd *cmd)
 	 * Next, walk the list, and fill in the addresses and sizes of
 	 * each segment.
 	 */
+	/*
+	 * struct scsi_cmnd *cmd:
+	 * -> struct scsi_data_buffer sdb;
+	 *    -> struct sg_table table;
+	 *       -> struct scatterlist *sgl;        // the list
+	 *       -> unsigned int nents;             // number of mapped entries
+	 *       -> unsigned int orig_nents;        // original size of list
+	 *    -> unsigned length;
+	 *
+	 * 注释: map a request to scatterlist, return number of sg entries setup. Caller
+	 * must make sure sg can hold rq->nr_phys_segments entries
+	 */
 	count = __blk_rq_map_sg(rq->q, rq, cmd->sdb.table.sgl, &last_sg);
 
 	if (blk_rq_bytes(rq) & rq->q->dma_pad_mask) {
@@ -1164,6 +1203,13 @@ static blk_status_t scsi_setup_scsi_cmnd(struct scsi_device *sdev,
 	} else {
 		BUG_ON(blk_rq_bytes(req));
 
+		/*
+		 * struct scsi_cmnd *cmd:
+		 * -> unsigned char *cmnd;
+		 * -> struct scsi_data_buffer sdb;
+		 *    -> struct sg_table table;
+		 *    -> unsigned length;
+		 */
 		memset(&cmd->sdb, 0, sizeof(cmd->sdb));
 	}
 
@@ -1475,6 +1521,13 @@ static int scsi_dispatch_cmd(struct scsi_cmnd *cmd)
 	}
 
 	/* Store the LUN value in cmnd, if needed. */
+	/*
+	 * 在以下使用scsi_device->lun_in_cdb:
+	 *   - drivers/scsi/scsi_lib.c|1495| <<scsi_dispatch_cmd>> if (cmd->device->lun_in_cdb)
+	 *   - drivers/scsi/scsi_scan.c|757| <<scsi_probe_lun>> sdev->lun_in_cdb = 0;
+	 *   - drivers/scsi/scsi_scan.c|761| <<scsi_probe_lun>> sdev->lun_in_cdb = 1;
+	 *   - drivers/scsi/scsi_sysfs.c|1624| <<scsi_sysfs_device_initialize>> sdev->lun_in_cdb = 1;
+	 */
 	if (cmd->device->lun_in_cdb)
 		cmd->cmnd[1] = (cmd->cmnd[1] & 0x1f) |
 			       (cmd->device->lun << 5 & 0xe0);
@@ -1525,6 +1578,10 @@ static unsigned int scsi_mq_inline_sgl_size(struct Scsi_Host *shost)
 		sizeof(struct scatterlist);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1687| <<scsi_queue_rq>> ret = scsi_prepare_cmd(req);
+ */
 static blk_status_t scsi_prepare_cmd(struct request *req)
 {
 	struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
@@ -1542,6 +1599,11 @@ static blk_status_t scsi_prepare_cmd(struct request *req)
 	else
 		cmd->sc_data_direction = DMA_NONE;
 
+	/*
+	 * 初始化的时候:
+	 * sgl_size = max_t(unsigned int, sizeof(struct scatterlist), scsi_mq_inline_sgl_size(shost));
+	 * cmd_size = sizeof(struct scsi_cmnd) + shost->hostt->cmd_size + sgl_size;
+	 */
 	sg = (void *)cmd + sizeof(struct scsi_cmnd) + shost->hostt->cmd_size;
 	cmd->sdb.table.sgl = sg;
 
@@ -1556,6 +1618,9 @@ static blk_status_t scsi_prepare_cmd(struct request *req)
 	 * Special handling for passthrough commands, which don't go to the ULP
 	 * at all:
 	 */
+	/*
+	 * probe lun是DMA_FROM_DEVICE, 对应REQ_OP_SCSI_IN
+	 */
 	if (blk_rq_is_scsi(req))
 		return scsi_setup_scsi_cmnd(sdev, req);
 
@@ -1571,6 +1636,10 @@ static blk_status_t scsi_prepare_cmd(struct request *req)
 	return scsi_cmd_to_driver(cmd)->init_command(cmd);
 }
 
+/*
+ * 在以下使用scsi_mq_done():
+ *   - drivers/scsi/scsi_lib.c|1682| <<scsi_queue_rq>> cmd->scsi_done = scsi_mq_done;
+ */
 static void scsi_mq_done(struct scsi_cmnd *cmd)
 {
 	if (unlikely(blk_should_fake_timeout(cmd->request->q)))
diff --git a/drivers/scsi/scsi_logging.c b/drivers/scsi/scsi_logging.c
index 8ea44c6595ef..ea0602d1ce87 100644
--- a/drivers/scsi/scsi_logging.c
+++ b/drivers/scsi/scsi_logging.c
@@ -32,6 +32,16 @@ static inline const char *scmd_name(const struct scsi_cmnd *scmd)
 		scmd->request->rq_disk->disk_name : NULL;
 }
 
+/*
+ * called bu:
+ *   - drivers/scsi/scsi_logging.c|93| <<scmd_printk>> off = sdev_format_header(logbuf, logbuf_len, scmd_name(scmd),
+ *   - drivers/scsi/scsi_logging.c|190| <<scsi_print_command>> off = sdev_format_header(logbuf, logbuf_len,
+ *   - drivers/scsi/scsi_logging.c|211| <<scsi_print_command>> off = sdev_format_header(logbuf, logbuf_len,
+ *   - drivers/scsi/scsi_logging.c|307| <<scsi_log_dump_sense>> off = sdev_format_header(logbuf, logbuf_len,
+ *   - drivers/scsi/scsi_logging.c|327| <<scsi_log_print_sense_hdr>> off = sdev_format_header(logbuf, logbuf_len, name, tag);
+ *   - drivers/scsi/scsi_logging.c|335| <<scsi_log_print_sense_hdr>> off = sdev_format_header(logbuf, logbuf_len, name, tag);
+ *   - drivers/scsi/scsi_logging.c|395| <<scsi_print_result>> off = sdev_format_header(logbuf, logbuf_len,
+ */
 static size_t sdev_format_header(char *logbuf, size_t logbuf_len,
 				 const char *name, int tag)
 {
diff --git a/drivers/scsi/scsi_scan.c b/drivers/scsi/scsi_scan.c
index 12f54571b83e..115b223c8abe 100644
--- a/drivers/scsi/scsi_scan.c
+++ b/drivers/scsi/scsi_scan.c
@@ -574,6 +574,10 @@ EXPORT_SYMBOL(scsi_sanitize_inquiry_string);
  *     INQUIRY data is in @inq_result; the scsi_level and INQUIRY length
  *     are copied to the scsi_device any flags value is stored in *@bflags.
  **/
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1104| <<scsi_probe_and_add_lun>> if (scsi_probe_lun(sdev, result, result_len, &bflags))
+ */
 static int scsi_probe_lun(struct scsi_device *sdev, unsigned char *inq_result,
 			  int result_len, blist_flags_t *bflags)
 {
@@ -1058,6 +1062,14 @@ static unsigned char *scsi_inq_str(unsigned char *buf, unsigned char *inq,
  *         attached at the LUN
  *   - SCSI_SCAN_LUN_PRESENT: a new scsi_device was allocated and initialized
  **/
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1280| <<scsi_sequential_lun_scan>> if ((scsi_probe_and_add_lun(starget, lun, NULL, NULL, rescan,
+ *   - drivers/scsi/scsi_scan.c|1449| <<scsi_report_lun_scan>> res = scsi_probe_and_add_lun(starget,
+ *   - drivers/scsi/scsi_scan.c|1496| <<__scsi_add_device>> scsi_probe_and_add_lun(starget, lun, NULL, &sdev, 1, hostdata);
+ *   - drivers/scsi/scsi_scan.c|1570| <<__scsi_scan_target>> scsi_probe_and_add_lun(starget, lun, NULL, NULL, rescan, NULL);
+ *   - drivers/scsi/scsi_scan.c|1578| <<__scsi_scan_target>> res = scsi_probe_and_add_lun(starget, 0, &bflags, NULL, rescan, NULL);
+ */
 static int scsi_probe_and_add_lun(struct scsi_target *starget,
 				  u64 lun, blist_flags_t *bflagsp,
 				  struct scsi_device **sdevp,
diff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c
index a2c3d9ad9ee4..0bf490bbc335 100644
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@ -1325,6 +1325,9 @@ static blk_status_t sd_setup_read_write_cmnd(struct scsi_cmnd *cmd)
 	return ret;
 }
 
+/*
+ * struct scsi_driver sd_template.init_command = sd_init_command()
+ */
 static blk_status_t sd_init_command(struct scsi_cmnd *cmd)
 {
 	struct request *rq = cmd->request;
diff --git a/drivers/scsi/sym53c8xx_2/sym_hipd.c b/drivers/scsi/sym53c8xx_2/sym_hipd.c
index 255a2d48d421..652cb9cc296d 100644
--- a/drivers/scsi/sym53c8xx_2/sym_hipd.c
+++ b/drivers/scsi/sym53c8xx_2/sym_hipd.c
@@ -1578,6 +1578,11 @@ void sym_start_next_ccbs(struct sym_hcb *np, struct sym_lcb *lp, int maxn)
  *  prevent out of order LOADs by the CPU from having 
  *  prefetched stale data prior to DMA having occurred.
  */
+/*
+ * called by:
+ *   - drivers/scsi/sym53c8xx_2/sym_glue.c|466| <<sym_timer>> sym_wakeup_done(np);
+ *   - drivers/scsi/sym53c8xx_2/sym_hipd.c|2824| <<sym_interrupt>> sym_wakeup_done(np);
+ */
 static int sym_wakeup_done (struct sym_hcb *np)
 {
 	struct sym_ccb *cp;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index b9c86a7e3b97..80869ac052a9 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -81,6 +81,12 @@ struct virtio_scsi {
 	/* Protected by event_vq lock */
 	bool stop_events;
 
+	/*
+	 * 在以下使用virtio_scsi->ctrl_vq:
+	 *   - drivers/scsi/virtio_scsi.c|276| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|747| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *   - drivers/scsi/virtio_scsi.c|970| <<virtscsi_init>> virtscsi_init_vq(&vscsi->ctrl_vq, vqs[0]);
+	 */
 	struct virtio_scsi_vq ctrl_vq;
 	struct virtio_scsi_vq event_vq;
 	struct virtio_scsi_vq req_vqs[];
@@ -94,6 +100,10 @@ static inline struct Scsi_Host *virtio_scsi_host(struct virtio_device *vdev)
 	return vdev->priv;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|125| <<virtscsi_complete_cmd>> virtscsi_compute_resid(sc, virtio32_to_cpu(vscsi->vdev, resp->resid));
+ */
 static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
 {
 	if (resid)
@@ -105,20 +115,47 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|199| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|209| <<virtscsi_poll_requests>> virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|604| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
 	struct scsi_cmnd *sc = cmd->sc;
 	struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
 
+	/*
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> union {
+	 *        struct virtio_scsi_cmd_resp      cmd;
+	 *          -> __virtio32 sense_len;           // Sense data length
+	 *          -> __virtio32 resid;               // Residual bytes in data buffer
+	 *          -> __virtio16 status_qualifier;    // Status qualifier
+	 *          -> __u8 status;            // Command completion status
+	 *          -> __u8 response;          // Response values
+	 *          -> __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+	 *        struct virtio_scsi_ctrl_tmf_resp tmf;
+	 *        struct virtio_scsi_ctrl_an_resp  an;
+	 *        struct virtio_scsi_event         evt;
+	 * } resp;
+	 */
 	dev_dbg(&sc->device->sdev_gendev,
 		"cmd %p response %u status %#02x sense_len %u\n",
 		sc, resp->response, resp->status, resp->sense_len);
 
+	/*
+	 * 没测试过,  猜测resp->status=GOOD (0x00)
+	 */
 	sc->result = resp->status;
 	virtscsi_compute_resid(sc, virtio32_to_cpu(vscsi->vdev, resp->resid));
 	switch (resp->response) {
 	case VIRTIO_SCSI_S_OK:
+		/*
+		 * 设置scsi_cmnd->result
+		 */
 		set_host_byte(sc, DID_OK);
 		break;
 	case VIRTIO_SCSI_S_OVERRUN:
@@ -165,9 +202,19 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 			set_driver_byte(sc, DRIVER_SENSE);
 	}
 
+	/*
+	 * scsi_mq_done()
+	 */
 	sc->scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|231| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|240| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+ *   - drivers/scsi/virtio_scsi.c|257| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|447| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -189,6 +236,10 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_unlock_irqrestore(&virtscsi_vq->vq_lock, flags);
 }
 
+/*
+ * 在以下使用virtscsi_req_done():
+ *   - drivers/scsi/virtio_scsi.c|902| <<virtscsi_init>> callbacks[i] = virtscsi_req_done;
+ */
 static void virtscsi_req_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -199,6 +250,10 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|742| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
@@ -209,6 +264,10 @@ static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 				 virtscsi_complete_cmd);
 }
 
+/*
+ * 在以下使用virtscsi_complete_free():
+ *   - drivers/scsi/virtio_scsi.c|257| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ */
 static void virtscsi_complete_free(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -217,6 +276,10 @@ static void virtscsi_complete_free(struct virtio_scsi *vscsi, void *buf)
 		complete(cmd->comp);
 }
 
+/*
+ * 在以下使用virtscsi_ctrl_done():
+ *   - drivers/scsi/virtio_scsi.c|923| <<virtscsi_init>> callbacks[0] = virtscsi_ctrl_done;
+ */
 static void virtscsi_ctrl_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -415,6 +478,35 @@ static void virtscsi_event_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
 };
 
+/*
+ * struct scsi_cmnd *sc:
+ * -> enum dma_data_direction sc_data_direction;
+ * -> unsigned char *cmnd;
+ * -> struct scsi_data_buffer sdb;
+ *    -> struct sg_table table;
+ *    -> unsigned length;
+ *
+ * struct virtio_scsi_cmd *cmd:
+ * -> union req:
+ *    -> struct virtio_scsi_cmd_req cmd;
+ *       -> __u8 lun[8];            // Logical Unit Number
+ *       -> __virtio64 tag;         // Command identifier
+ *       -> __u8 task_attr;         // Task attribute
+ *       -> __u8 prio;              // SAM command priority field
+ *       -> __u8 crn;
+ *       -> __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+ * -> union resp:
+ *    -> struct virtio_scsi_cmd_resp cmd;
+ *       -> __virtio32 sense_len;           // Sense data length
+ *       -> __virtio32 resid;               // Residual bytes in data buffer
+ *       -> __virtio16 status_qualifier;    // Status qualifier
+ *       -> __u8 status;            // Command completion status
+ *       -> __u8 response;          // Response values
+ *       -> __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+ *
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|494| <<virtscsi_add_cmd>> err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int __virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -426,6 +518,17 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 
 	out = in = NULL;
 
+	/*
+	 * struct scsi_cmnd *sc:
+	 * -> enum dma_data_direction sc_data_direction;
+	 * -> unsigned char *cmnd;
+	 * -> struct scsi_data_buffer sdb;
+	 *    -> struct sg_table table;
+	 *       -> struct scatterlist *sgl;        // the list
+	 *       -> unsigned int nents;             // number of mapped entries
+	 *       -> unsigned int orig_nents;        // original size of list
+	 *    -> unsigned length;
+	 */
 	if (sc && sc->sc_data_direction != DMA_NONE) {
 		if (sc->sc_data_direction != DMA_FROM_DEVICE)
 			out = &sc->sdb.table;
@@ -433,6 +536,9 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 			in = &sc->sdb.table;
 	}
 
+	/*
+	 * struct scatterlist *sgs[6], req, resp;
+	 */
 	/* Request header.  */
 	sg_init_one(&req, &cmd->req, req_size);
 	sgs[out_num++] = &req;
@@ -454,6 +560,9 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 		/* Place READ protection SGLs before Data IN payload */
 		if (scsi_prot_sg_count(sc))
 			sgs[out_num + in_num++] = scsi_prot_sglist(sc);
+		/*
+		 * struct sg_table *out, *in;
+		 */
 		sgs[out_num + in_num++] = in->sgl;
 	}
 
@@ -481,6 +590,11 @@ static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
  * @resp_size	: size of the response buffer
  * @kick	: whether to kick the virtqueue immediately
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|600| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|618| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ */
 static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size,
@@ -506,6 +620,23 @@ static void virtio_scsi_init_hdr(struct virtio_device *vdev,
 				 struct virtio_scsi_cmd_req *cmd,
 				 struct scsi_cmnd *sc)
 {
+	/*
+	 * struct scsi_cmnd *sc:
+	 * -> struct scsi_request req;
+	 * -> struct scsi_device *device;
+	 *    -> unsigned int id, channel;
+	 *    -> u64 lun;
+	 *
+	 * // SCSI command request, followed by data-out
+	 * struct virtio_scsi_cmd_req {
+	 *     __u8 lun[8];            // Logical Unit Number
+	 *     __virtio64 tag;         // Command identifier
+	 *     __u8 task_attr;         // Task attribute
+	 *     __u8 prio;              // SAM command priority field
+	 *     __u8 crn;
+	 *     __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+	 * } __attribute__((packed));
+	 */
 	cmd->lun[0] = 1;
 	cmd->lun[1] = sc->device->id;
 	cmd->lun[2] = (sc->device->lun >> 8) | 0x40;
@@ -554,7 +685,16 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
+	/*
+	 * 返回(void *)shost->hostdata
+	 */
 	struct virtio_scsi *vscsi = shost_priv(shost);
+	/*
+	 * struct virtio_scsi_vq {
+	 *	spinlock_t vq_lock;
+	 *	struct virtqueue *vq;
+	 * };
+	 */
 	struct virtio_scsi_vq *req_vq = virtscsi_pick_vq_mq(vscsi, sc);
 	struct virtio_scsi_cmd *cmd = scsi_cmd_priv(sc);
 	bool kick;
@@ -567,6 +707,11 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	/* TODO: check feature bit and fail if unsupported?  */
 	BUG_ON(sc->sc_data_direction == DMA_BIDIRECTIONAL);
 
+	/*
+	 * struct scsi_cmnd *sc:
+	 * -> enum dma_data_direction sc_data_direction;
+	 * -> unsigned char *cmnd;
+	 */
 	dev_dbg(&sc->device->sdev_gendev,
 		"cmd %p CDB: %#02x\n", sc, sc->cmnd[0]);
 
@@ -582,6 +727,29 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	} else
 #endif
 	{
+		/*
+		 * struct scsi_cmnd *sc:
+		 * -> enum dma_data_direction sc_data_direction;
+		 * -> unsigned char *cmnd;
+		 *
+		 * struct virtio_scsi_cmd *cmd:
+		 * -> union req:
+		 *    -> struct virtio_scsi_cmd_req cmd;
+		 *       -> __u8 lun[8];            // Logical Unit Number
+		 *       -> __virtio64 tag;         // Command identifier
+		 *       -> __u8 task_attr;         // Task attribute
+		 *       -> __u8 prio;              // SAM command priority field
+		 *       -> __u8 crn;
+		 *       -> __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+		 * -> union resp:
+		 *    -> struct virtio_scsi_cmd_resp cmd;
+		 *       -> __virtio32 sense_len;           // Sense data length
+		 *       -> __virtio32 resid;               // Residual bytes in data buffer
+		 *       -> __virtio16 status_qualifier;    // Status qualifier
+		 *       -> __u8 status;            // Command completion status
+		 *       -> __u8 response;          // Response values
+		 *       -> __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+		 */
 		virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
 		memcpy(cmd->req.cmd.cdb, sc->cmnd, sc->cmd_len);
 		req_size = sizeof(cmd->req.cmd);
@@ -600,12 +768,23 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|769| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|827| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+ */
 static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 {
 	DECLARE_COMPLETION_ONSTACK(comp);
 	int ret = FAILED;
 
 	cmd->comp = &comp;
+	/*
+	 * 在以下使用virtio_scsi->ctrl_vq:
+	 *   - drivers/scsi/virtio_scsi.c|276| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|747| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *   - drivers/scsi/virtio_scsi.c|970| <<virtscsi_init>> virtscsi_init_vq(&vscsi->ctrl_vq, vqs[0]);
+	 */
 	if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
 			      sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
 		goto out;
@@ -701,6 +880,15 @@ static int virtscsi_abort(struct scsi_cmnd *sc)
 		return FAILED;
 
 	memset(cmd, 0, sizeof(*cmd));
+	/*
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> union req:
+	 *    -> struct virtio_scsi_ctrl_tmf_req tmf;
+	 *       -> __virtio32 type;
+	 *       -> __virtio32 subtype;
+	 *       -> __u8 lun[8];
+	 *       -> __virtio64 tag;
+	 */
 	cmd->req.tmf = (struct virtio_scsi_ctrl_tmf_req){
 		.type = VIRTIO_SCSI_T_TMF,
 		.subtype = VIRTIO_SCSI_T_TMF_ABORT_TASK,
diff --git a/drivers/target/target_core_configfs.c b/drivers/target/target_core_configfs.c
index 4b2e49341ad6..4690749025a8 100644
--- a/drivers/target/target_core_configfs.c
+++ b/drivers/target/target_core_configfs.c
@@ -446,6 +446,20 @@ static int target_fabric_tf_ops_check(const struct target_core_fabric_ops *tfo)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3895| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4107| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1936| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1940| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+ *   - drivers/target/iscsi/iscsi_target.c|696| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+ *   - drivers/target/loopback/tcm_loop.c|1173| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+ *   - drivers/target/sbp/sbp_target.c|2337| <<sbp_init>> return target_register_template(&sbp_ops);
+ *   - drivers/target/tcm_fc/tfc_conf.c|460| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+ *   - drivers/usb/gadget/function/f_tcm.c|2336| <<tcm_init>> ret = target_register_template(&usbg_ops);
+ *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/xen/xen-scsiback.c|1833| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+ */
 int target_register_template(const struct target_core_fabric_ops *fo)
 {
 	struct target_fabric_configfs *tf;
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index 7b07e557dc8d..5ef6fb6c2350 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -818,6 +818,12 @@ sbc_check_dpofua(struct se_device *dev, struct se_cmd *cmd, unsigned char *cdb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|912| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_sbc_ops);
+ *   - drivers/target/target_core_iblock.c|888| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_sbc_ops);
+ *   - drivers/target/target_core_rd.c|653| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_sbc_ops);
+ */
 sense_reason_t
 sbc_parse_cdb(struct se_cmd *cmd, struct sbc_ops *ops)
 {
diff --git a/drivers/target/target_core_spc.c b/drivers/target/target_core_spc.c
index 70a661801cb9..0450a9e3434d 100644
--- a/drivers/target/target_core_spc.c
+++ b/drivers/target/target_core_spc.c
@@ -697,6 +697,10 @@ spc_emulate_evpd_00(struct se_cmd *cmd, unsigned char *buf)
 	return 0;
 }
 
+/*
+ * 在以下使用spc_emulate_inquiry():
+ *   - drivers/target/target_core_spc.c|1357| <<spc_parse_cdb>> cmd->execute_cmd = spc_emulate_inquiry;
+ */
 static sense_reason_t
 spc_emulate_inquiry(struct se_cmd *cmd)
 {
@@ -1277,6 +1281,10 @@ spc_emulate_testunitready(struct se_cmd *cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_sbc.c|1133| <<sbc_parse_cdb>> ret = spc_parse_cdb(cmd, &size);
+ */
 sense_reason_t
 spc_parse_cdb(struct se_cmd *cmd, unsigned int *size)
 {
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 7e35eddd9eb7..3e50240dc653 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -855,6 +855,50 @@ static bool target_cmd_interrupted(struct se_cmd *cmd)
 }
 
 /* May be called from interrupt context so must not sleep. */
+/*
+ * 例子太多了, 这里只是5.4的几个.
+ *
+ * target_complete_cmd
+ * fd_execute_rw
+ * sbc_execute_rw
+ * __target_execute_cmd
+ * target_execute_cmd
+ * transport_generic_new_cmd
+ * transport_handle_cdb_direct
+ * target_submit
+ * target_queued_submit_work
+ * process_one_work
+ * worker_thread
+ * kthread
+ * ret_from_fork
+ *
+ * target_complete_cmd
+ * spc_emulate_inquiry
+ * __target_execute_cmd
+ * target_execute_cmd
+ * transport_generic_new_cmd
+ * transport_handle_cdb_direct
+ * target_submit
+ * target_queued_submit_work
+ * process_one_work
+ * worker_thread
+ * kthread
+ * ret_from_fork
+ *
+ * target_complete_cmd
+ * fd_execute_rw
+ * sbc_execute_rw
+ * __target_execute_cmd
+ * target_execute_cmd
+ * transport_generic_new_cmd
+ * transport_handle_cdb_direct
+ * target_submit
+ * target_queued_submit_work
+ * process_one_work
+ * worker_thread
+ * kthread
+ * ret_from_fork
+ */
 void target_complete_cmd(struct se_cmd *cmd, u8 scsi_status)
 {
 	struct se_wwn *wwn = cmd->se_sess->se_tpg->se_tpg_wwn;
@@ -895,6 +939,14 @@ void target_complete_cmd(struct se_cmd *cmd, u8 scsi_status)
 }
 EXPORT_SYMBOL(target_complete_cmd);
 
+/*
+ * called by:
+ *   - drivers/target/target_core_pr.c|3730| <<core_scsi3_pri_read_keys>> target_set_cmd_data_length(cmd, 8 + add_len);
+ *   - drivers/target/target_core_pr.c|3805| <<core_scsi3_pri_read_reservation>> target_set_cmd_data_length(cmd, 8 + add_len);
+ *   - drivers/target/target_core_pr.c|3866| <<core_scsi3_pri_report_capabilities>> target_set_cmd_data_length(cmd, len);
+ *   - drivers/target/target_core_pr.c|4028| <<core_scsi3_pri_read_full_status>> target_set_cmd_data_length(cmd, 8 + add_len);
+ *   - drivers/target/target_core_transport.c|917| <<target_complete_cmd_with_length>> target_set_cmd_data_length(cmd, length);
+ */
 void target_set_cmd_data_length(struct se_cmd *cmd, int length)
 {
 	if (length < cmd->data_length) {
@@ -910,6 +962,17 @@ void target_set_cmd_data_length(struct se_cmd *cmd, int length)
 }
 EXPORT_SYMBOL(target_set_cmd_data_length);
 
+/*
+ * called by:
+ *   - drivers/target/target_core_alua.c|258| <<target_emulate_report_target_port_groups>> target_complete_cmd_with_length(cmd, GOOD, rd_len + 4);
+ *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_req_done>> target_complete_cmd_with_length(cmd, scsi_status,
+ *   - drivers/target/target_core_sbc.c|70| <<sbc_emulate_readcapacity>> target_complete_cmd_with_length(cmd, GOOD, 8);
+ *   - drivers/target/target_core_sbc.c|133| <<sbc_emulate_readcapacity_16>> target_complete_cmd_with_length(cmd, GOOD, 32);
+ *   - drivers/target/target_core_spc.c|753| <<spc_emulate_inquiry>> target_complete_cmd_with_length(cmd, GOOD, len);
+ *   - drivers/target/target_core_spc.c|1107| <<spc_emulate_modesense>> target_complete_cmd_with_length(cmd, GOOD, length);
+ *   - drivers/target/target_core_spc.c|1268| <<spc_emulate_report_luns>> target_complete_cmd_with_length(cmd, GOOD, 8 + lun_count * 8);
+ *   - drivers/target/target_core_user.c|1376| <<tcmu_handle_completion>> target_complete_cmd_with_length(cmd->se_cmd,
+ */
 void target_complete_cmd_with_length(struct se_cmd *cmd, u8 scsi_status, int length)
 {
 	if (scsi_status == SAM_STAT_GOOD ||
@@ -1324,6 +1387,15 @@ target_check_max_data_sg_nents(struct se_cmd *cmd, struct se_device *dev,
  *
  * Return: TCM_NO_SENSE
  */
+/*
+ * called by:
+ *   - drivers/target/target_core_device.c|1123| <<sense_reason_t>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1128| <<sense_reason_t>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1137| <<sense_reason_t>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1145| <<sense_reason_t>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_internal.h|152| <<sense_reason_t>> sense_reason_t target_cmd_size_check(struct se_cmd *cmd, unsigned int size);
+ *   - drivers/target/target_core_sbc.c|1164| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+ */
 sense_reason_t
 target_cmd_size_check(struct se_cmd *cmd, unsigned int size)
 {
@@ -1387,6 +1459,17 @@ target_cmd_size_check(struct se_cmd *cmd, unsigned int size)
  *
  * Preserves the value of @cmd->tag.
  */
+/*
+ * called by:
+ *   - drivers/target/iscsi/iscsi_target.c|1157| <<iscsit_setup_scsi_cmd>> __target_init_cmd(&cmd->se_cmd, &iscsi_ops,
+ *   - drivers/target/iscsi/iscsi_target.c|2017| <<iscsit_handle_task_mgt_cmd>> __target_init_cmd(&cmd->se_cmd, &iscsi_ops,
+ *   - drivers/target/target_core_transport.c|1728| <<target_init_cmd>> __target_init_cmd(se_cmd, se_tpg->se_tpg_tfo, se_sess, data_length,
+ *   - drivers/target/target_core_transport.c|2038| <<target_submit_tmr>> __target_init_cmd(se_cmd, se_tpg->se_tpg_tfo, se_sess,
+ *   - drivers/target/target_core_xcopy.c|618| <<target_xcopy_read_source>> __target_init_cmd(se_cmd, &xcopy_pt_tfo, &xcopy_pt_sess, length,
+ *   - drivers/target/target_core_xcopy.c|663| <<target_xcopy_write_destination>> __target_init_cmd(se_cmd, &xcopy_pt_tfo, &xcopy_pt_sess, length,
+ *   - drivers/usb/gadget/function/f_tcm.c|1053| <<usbg_cmd_work>> __target_init_cmd(se_cmd,
+ *   - drivers/usb/gadget/function/f_tcm.c|1182| <<bot_cmd_work>> __target_init_cmd(se_cmd,
+ */
 void __target_init_cmd(
 	struct se_cmd *cmd,
 	const struct target_core_fabric_ops *tfo,
@@ -1494,12 +1577,33 @@ target_cmd_init_cdb(struct se_cmd *cmd, unsigned char *cdb, gfp_t gfp)
 }
 EXPORT_SYMBOL(target_cmd_init_cdb);
 
+/*
+ * called by:
+ *   - drivers/target/iscsi/iscsi_target.c|1186| <<iscsit_setup_scsi_cmd>> cmd->sense_reason = target_cmd_parse_cdb(&cmd->se_cmd);
+ *   - drivers/target/target_core_transport.c|1714| <<target_submit_prep>> rc = target_cmd_parse_cdb(se_cmd);
+ *   - drivers/target/target_core_xcopy.c|561| <<target_xcopy_setup_pt_cmd>> if (target_cmd_parse_cdb(cmd))
+ *
+ * 比如tcm_loop_target_queue_cmd()或者vhost_scsi_target_queue_cmd()
+ * -> target_submit_prep()
+ */
 sense_reason_t
 target_cmd_parse_cdb(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
 	sense_reason_t ret;
 
+	/*
+	 * drivers/target/target_core_file.c <<parse_cdb>>
+	 *     .parse_cdb = fd_parse_cdb,
+	 * drivers/target/target_core_iblock.c <<parse_cdb>>
+	 *     .parse_cdb = iblock_parse_cdb,
+	 * drivers/target/target_core_pscsi.c <<parse_cdb>>
+	 *     .parse_cdb = pscsi_parse_cdb,
+	 * drivers/target/target_core_rd.c <<parse_cdb>>
+	 *     .parse_cdb = rd_parse_cdb,
+	 * drivers/target/target_core_user.c <<parse_cdb>>
+	 *     .parse_cdb = tcmu_parse_cdb,
+	 */
 	ret = dev->transport->parse_cdb(cmd);
 	if (ret == TCM_UNSUPPORTED_SCSI_OPCODE)
 		pr_warn_ratelimited("%s/%s: Unsupported SCSI Opcode 0x%02x, sending CHECK_CONDITION.\n",
@@ -1607,6 +1711,16 @@ transport_generic_map_mem_to_cmd(struct se_cmd *cmd, struct scatterlist *sgl,
  * return code and handle failures. This will never fail for other drivers,
  * and the return code can be ignored.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1531| <<srpt_handle_cmd>> rc = target_init_cmd(cmd, ch->sess, &send_ioctx->sense_data[0],
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|489| <<tcm_qla2xxx_handle_cmd>> rc = target_init_cmd(se_cmd, se_sess, &cmd->sense_buffer[0],
+ *   - drivers/target/loopback/tcm_loop.c|154| <<tcm_loop_target_queue_cmd>> target_init_cmd(se_cmd, tl_nexus->se_sess, &tl_cmd->tl_sense_buf[0],
+ *   - drivers/target/target_core_transport.c|1907| <<target_submit_cmd>> rc = target_init_cmd(se_cmd, se_sess, sense, unpacked_lun, data_length,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|551| <<ft_send_work>> if (target_init_cmd(&cmd->se_cmd, cmd->sess->se_sess,
+ *   - drivers/vhost/scsi.c|862| <<vhost_scsi_target_queue_cmd>> target_init_cmd(se_cmd, tv_nexus->tvn_se_sess, &cmd->tvc_sense_buf[0],
+ *   - drivers/xen/xen-scsiback.c|366| <<scsiback_cmd_exec>> target_init_cmd(se_cmd, sess, pending_req->sense_buffer,
+ */
 int target_init_cmd(struct se_cmd *se_cmd, struct se_session *se_sess,
 		    unsigned char *sense, u64 unpacked_lun,
 		    u32 data_length, int task_attr, int data_dir, int flags)
@@ -1663,6 +1777,16 @@ EXPORT_SYMBOL_GPL(target_init_cmd);
  * If failure is returned, lio will the callers queue_status to complete
  * the cmd.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1540| <<srpt_handle_cmd>> if (target_submit_prep(cmd, srp_cmd->cdb, sg, sg_cnt, NULL, 0, NULL, 0,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|495| <<tcm_qla2xxx_handle_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0, NULL, 0,
+ *   - drivers/target/loopback/tcm_loop.c|158| <<tcm_loop_target_queue_cmd>> if (target_submit_prep(se_cmd, sc->cmnd, scsi_sglist(sc),
+ *   - drivers/target/target_core_transport.c|1829| <<target_submit_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0, NULL, 0,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|557| <<ft_send_work>> if (target_submit_prep(&cmd->se_cmd, fcp->fc_cdb, NULL, 0, NULL, 0,
+ *   - drivers/vhost/scsi.c|829| <<vhost_scsi_target_queue_cmd>> if (target_submit_prep(se_cmd, cmd->tvc_cdb, sg_ptr,
+ *   - drivers/xen/xen-scsiback.c|370| <<scsiback_cmd_exec>> if (target_submit_prep(se_cmd, pending_req->cmnd, pending_req->sgl,
+ */
 int target_submit_prep(struct se_cmd *se_cmd, unsigned char *cdb,
 		       struct scatterlist *sgl, u32 sgl_count,
 		       struct scatterlist *sgl_bidi, u32 sgl_bidi_count,
@@ -1795,6 +1919,13 @@ EXPORT_SYMBOL_GPL(target_submit);
  * is a failure this function will call into the fabric driver's
  * queue_status with a CHECK_CONDITION.
  */
+/*
+ * called by:
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2727| <<ibmvscsis_parse_cmd>> target_submit_cmd(&cmd->se_cmd, nexus->se_sess, srp->cdb,
+ *   - drivers/target/sbp/sbp_target.c|1221| <<sbp_handle_command>> target_submit_cmd(&req->se_cmd, sess->se_sess, req->cmd_buf,
+ *   - drivers/usb/gadget/function/f_tcm.c|1061| <<usbg_cmd_work>> target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess, cmd->cmd_buf,
+ *   - drivers/usb/gadget/function/f_tcm.c|1190| <<bot_cmd_work>> target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess,
+ */
 void target_submit_cmd(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *cdb, unsigned char *sense, u64 unpacked_lun,
 		u32 data_length, int task_attr, int data_dir, int flags)
@@ -1876,6 +2007,11 @@ void target_queued_submit_work(struct work_struct *work)
  * target_queue_submission - queue the cmd to run on the LIO workqueue
  * @se_cmd: command descriptor to submit
  */
+/*
+ * called by:
+ *   - drivers/target/loopback/tcm_loop.c|164| <<tcm_loop_target_queue_cmd>> target_queue_submission(se_cmd);
+ *   - drivers/vhost/scsi.c|834| <<vhost_scsi_target_queue_cmd>> target_queue_submission(se_cmd);
+ */
 void target_queue_submission(struct se_cmd *se_cmd)
 {
 	struct se_device *se_dev = se_cmd->se_dev;
@@ -1968,6 +2104,21 @@ EXPORT_SYMBOL(target_submit_tmr);
 /*
  * Handle SAM-esque emulation for generic transport request failures.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1602| <<isert_rdma_write_done>> transport_generic_request_failure(cmd, cmd->pi_err);
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1653| <<isert_rdma_read_done>> transport_generic_request_failure(se_cmd, se_cmd->pi_err);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1293| <<srpt_abort_cmd>> transport_generic_request_failure(&ioctx->cmd,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|401| <<tcm_qla2xxx_write_pending>> transport_generic_request_failure(&cmd->se_cmd,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|514| <<tcm_qla2xxx_handle_data_work>> transport_generic_request_failure(&cmd->se_cmd,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|540| <<tcm_qla2xxx_handle_data_work>> transport_generic_request_failure(&cmd->se_cmd,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|543| <<tcm_qla2xxx_handle_data_work>> transport_generic_request_failure(&cmd->se_cmd,
+ *   - drivers/target/target_core_transport.c|739| <<target_complete_failure_work>> transport_generic_request_failure(cmd,
+ *   - drivers/target/target_core_transport.c|1650| <<transport_handle_cdb_direct>> transport_generic_request_failure(cmd, ret);
+ *   - drivers/target/target_core_transport.c|1824| <<target_submit_prep>> transport_generic_request_failure(se_cmd, rc);
+ *   - drivers/target/target_core_transport.c|2219| <<__target_execute_cmd>> transport_generic_request_failure(cmd, ret);
+ *   - drivers/target/target_core_transport.c|2246| <<target_write_prot_action>> transport_generic_request_failure(cmd, cmd->pi_err);
+ */
 void transport_generic_request_failure(struct se_cmd *cmd,
 		sense_reason_t sense_reason)
 {
@@ -3353,6 +3504,21 @@ static const struct sense_detail sense_detail_table[] = {
  *
  * Return: 0 upon success or -EINVAL if the sense buffer is too small.
  */
+/*
+ * 5.4的例子
+ * [0] translate_sense_reason
+ * [0] transport_generic_request_failure
+ * [0] target_submit_prep
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/target/target_core_transport.c|2366| <<transport_complete_qf>> translate_sense_reason(cmd, TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE);
+ *   - drivers/target/target_core_transport.c|3468| <<transport_send_check_condition_and_sense>> translate_sense_reason(cmd, reason);
+ */
 static void translate_sense_reason(struct se_cmd *cmd, sense_reason_t reason)
 {
 	const struct sense_detail *sd;
@@ -3393,6 +3559,24 @@ static void translate_sense_reason(struct se_cmd *cmd, sense_reason_t reason)
 							cmd->sense_info) < 0);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2816| <<ibmvscsis_parse_task>> transport_send_check_condition_and_sense(&cmd->se_cmd, 0, 0);
+ *   - drivers/target/iscsi/iscsi_target.c|1493| <<__iscsit_check_dataout_hdr>> transport_send_check_condition_and_sense(&cmd->se_cmd,
+ *   - drivers/target/iscsi/iscsi_target_erl0.c|157| <<iscsit_dataout_check_unsolicited_sequence>> transport_send_check_condition_and_sense(&cmd->se_cmd,
+ *   - drivers/target/iscsi/iscsi_target_erl0.c|182| <<iscsit_dataout_check_unsolicited_sequence>> transport_send_check_condition_and_sense(&cmd->se_cmd,
+ *   - drivers/target/iscsi/iscsi_target_erl1.c|916| <<iscsit_execute_cmd>> return transport_send_check_condition_and_sense(se_cmd,
+ *   - drivers/target/iscsi/iscsi_target_util.c|346| <<iscsit_check_unsolicited_dataout>> transport_send_check_condition_and_sense(se_cmd,
+ *   - drivers/target/iscsi/iscsi_target_util.c|357| <<iscsit_check_unsolicited_dataout>> transport_send_check_condition_and_sense(se_cmd,
+ *   - drivers/target/iscsi/iscsi_target_util.c|373| <<iscsit_check_unsolicited_dataout>> transport_send_check_condition_and_sense(se_cmd,
+ *   - drivers/target/target_core_transport.c|1819| <<target_submit_prep>> transport_send_check_condition_and_sense(se_cmd, rc, 0);
+ *   - drivers/target/target_core_transport.c|2162| <<transport_generic_request_failure>> ret = transport_send_check_condition_and_sense(cmd, sense_reason, 0);
+ *   - drivers/target/target_core_transport.c|2555| <<target_complete_ok_work>> ret = transport_send_check_condition_and_sense(
+ *   - drivers/target/target_core_transport.c|2581| <<target_complete_ok_work>> ret = transport_send_check_condition_and_sense(cmd,
+ *   - drivers/target/target_core_transport.c|2617| <<target_complete_ok_work>> ret = transport_send_check_condition_and_sense(cmd,
+ *   - drivers/usb/gadget/function/f_tcm.c|1067| <<usbg_cmd_work>> transport_send_check_condition_and_sense(se_cmd,
+ *   - drivers/usb/gadget/function/f_tcm.c|1196| <<bot_cmd_work>> transport_send_check_condition_and_sense(se_cmd,
+ */
 int
 transport_send_check_condition_and_sense(struct se_cmd *cmd,
 		sense_reason_t reason, int from_transport)
diff --git a/drivers/vdpa/ifcvf/ifcvf_main.c b/drivers/vdpa/ifcvf/ifcvf_main.c
index ab0ab5cf0f6e..ebda06969961 100644
--- a/drivers/vdpa/ifcvf/ifcvf_main.c
+++ b/drivers/vdpa/ifcvf/ifcvf_main.c
@@ -475,6 +475,13 @@ static int ifcvf_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 		return ret;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|478| <<ifcvf_probe>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2014| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|252| <<vdpasim_create>> vdpasim = vdpa_alloc_device(struct vdpasim, vdpa, NULL, ops,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|401| <<vp_vdpa_probe>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+	 */
 	adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
 				    dev, &ifc_vdpa_ops, NULL);
 	if (adapter == NULL) {
diff --git a/drivers/vdpa/mlx5/net/mlx5_vnet.c b/drivers/vdpa/mlx5/net/mlx5_vnet.c
index dda5dc6f7737..8c482efce1d9 100644
--- a/drivers/vdpa/mlx5/net/mlx5_vnet.c
+++ b/drivers/vdpa/mlx5/net/mlx5_vnet.c
@@ -2011,6 +2011,13 @@ static int mlx5_vdpa_dev_add(struct vdpa_mgmt_dev *v_mdev, const char *name)
 	max_vqs = MLX5_CAP_DEV_VDPA_EMULATION(mdev, max_num_virtio_queues);
 	max_vqs = min_t(u32, max_vqs, MLX5_MAX_SUPPORTED_VQS);
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|478| <<ifcvf_probe>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2014| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|252| <<vdpasim_create>> vdpasim = vdpa_alloc_device(struct vdpasim, vdpa, NULL, ops,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|401| <<vp_vdpa_probe>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+	 */
 	ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,
 				 name);
 	if (IS_ERR(ndev))
@@ -2103,6 +2110,12 @@ static int mlx5v_probe(struct auxiliary_device *adev,
 	mgtdev->mgtdev.id_table = id_table;
 	mgtdev->madev = madev;
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2106| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|315| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|246| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 */
 	err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
 	if (err)
 		goto reg_err;
diff --git a/drivers/vdpa/vdpa.c b/drivers/vdpa/vdpa.c
index bb3f1d1f0422..edb0124378ef 100644
--- a/drivers/vdpa/vdpa.c
+++ b/drivers/vdpa/vdpa.c
@@ -15,15 +15,42 @@
 #include <net/genetlink.h>
 #include <linux/mod_devicetable.h>
 
+/*
+ * 在以下使用mdev_head:
+ *   - drivers/vdpa/vdpa.c|280| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+ *   - drivers/vdpa/vdpa.c|340| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+ *   - drivers/vdpa/vdpa.c|431| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+ */
 static LIST_HEAD(mdev_head);
 /* A global mutex that protects vdpa management device and device level operations. */
 static DEFINE_MUTEX(vdpa_dev_mutex);
 static DEFINE_IDA(vdpa_index_ida);
 
+/*
+ * 在以下使用vdpa_nl_family:
+ *   - drivers/vdpa/vdpa.c|29| <<global>> static struct genl_family vdpa_nl_family;
+ *   - drivers/vdpa/vdpa.c|692| <<global>> static struct genl_family vdpa_nl_family __ro_after_init = {
+ *   - drivers/vdpa/vdpa.c|392| <<vdpa_mgmtdev_fill>> hdr = genlmsg_put(msg, portid, seq, &vdpa_nl_family, flags, VDPA_CMD_MGMTDEV_NEW);
+ *   - drivers/vdpa/vdpa.c|544| <<vdpa_dev_fill>> hdr = genlmsg_put(msg, portid, seq, &vdpa_nl_family, flags, VDPA_CMD_DEV_NEW);
+ *   - drivers/vdpa/vdpa.c|710| <<vdpa_init>> err = genl_register_family(&vdpa_nl_family);
+ *   - drivers/vdpa/vdpa.c|722| <<vdpa_exit>> genl_unregister_family(&vdpa_nl_family);
+ */
 static struct genl_family vdpa_nl_family;
 
+/*
+ * 只在virtio和vhost中使用vdpa_driver:
+ *   - drivers/virtio/virtio_vdpa.c:static struct vdpa_driver virtio_vdpa_driver = {
+ *   - drivers/vhost/vdpa.c:static struct vdpa_driver vhost_vdpa_driver = {
+ *
+ * struct bus_type vdpa_bus.probe = vdpa_dev_probe()
+ */
 static int vdpa_dev_probe(struct device *d)
 {
+	/*
+	 * 只在virtio和vhost中使用vdpa_driver:
+	 *   - drivers/virtio/virtio_vdpa.c:static struct vdpa_driver virtio_vdpa_driver = {
+	 *   - drivers/vhost/vdpa.c:static struct vdpa_driver vhost_vdpa_driver = {
+	 */
 	struct vdpa_device *vdev = dev_to_vdpa(d);
 	struct vdpa_driver *drv = drv_to_vdpa(vdev->dev.driver);
 	int ret = 0;
@@ -34,8 +61,16 @@ static int vdpa_dev_probe(struct device *d)
 	return ret;
 }
 
+/*
+ * struct bus_type vdpa_bus.remove = vdpa_dev_remove()
+ */
 static int vdpa_dev_remove(struct device *d)
 {
+	/*
+	 * 只在virtio和vhost中使用vdpa_driver:
+	 *   - drivers/virtio/virtio_vdpa.c:static struct vdpa_driver virtio_vdpa_driver = {
+	 *   - drivers/vhost/vdpa.c:static struct vdpa_driver vhost_vdpa_driver = {
+	 */
 	struct vdpa_device *vdev = dev_to_vdpa(d);
 	struct vdpa_driver *drv = drv_to_vdpa(vdev->dev.driver);
 
@@ -45,6 +80,19 @@ static int vdpa_dev_remove(struct device *d)
 	return 0;
 }
 
+/*
+ * 在以下使用vdpa_bus:
+ *   - drivers/vdpa/vdpa.c|103| <<__vdpa_alloc_device>> vdev->dev.bus = &vdpa_bus;
+ *   - drivers/vdpa/vdpa.c|144| <<__vdpa_register_device>> dev = bus_find_device(&vdpa_bus, NULL, dev_name(&vdev->dev), vdpa_name_match);
+ *   - drivers/vdpa/vdpa.c|230| <<__vdpa_register_driver>> drv->driver.bus = &vdpa_bus;
+ *   - drivers/vdpa/vdpa.c|286| <<vdpa_mgmtdev_unregister>> bus_for_each_dev(&vdpa_bus, NULL, mdev, vdpa_match_remove);
+ *   - drivers/vdpa/vdpa.c|469| <<vdpa_nl_cmd_dev_del_set_doit>> dev = bus_find_device(&vdpa_bus, NULL, name, vdpa_name_match);
+ *   - drivers/vdpa/vdpa.c|548| <<vdpa_nl_cmd_dev_get_doit>> dev = bus_find_device(&vdpa_bus, NULL, devname, vdpa_name_match);
+ *   - drivers/vdpa/vdpa.c|609| <<vdpa_nl_cmd_dev_get_dumpit>> bus_for_each_dev(&vdpa_bus, NULL, &info, vdpa_dev_dump);
+ *   - drivers/vdpa/vdpa.c|663| <<vdpa_init>> err = bus_register(&vdpa_bus);
+ *   - drivers/vdpa/vdpa.c|672| <<vdpa_init>> bus_unregister(&vdpa_bus);
+ *   - drivers/vdpa/vdpa.c|679| <<vdpa_exit>> bus_unregister(&vdpa_bus);
+ */
 static struct bus_type vdpa_bus = {
 	.name  = "vdpa",
 	.probe = vdpa_dev_probe,
@@ -78,6 +126,10 @@ static void vdpa_release_dev(struct device *d)
  * Return: Returns an error when parent/config/dma_dev is not set or fail to get
  *	   ida.
  */
+/*
+ * called by:
+ *   - include/linux/vdpa.h|260| <<vdpa_alloc_device>> container_of(__vdpa_alloc_device( \
+ */
 struct vdpa_device *__vdpa_alloc_device(struct device *parent,
 					const struct vdpa_config_ops *config,
 					size_t size, const char *name)
@@ -127,6 +179,12 @@ struct vdpa_device *__vdpa_alloc_device(struct device *parent,
 }
 EXPORT_SYMBOL_GPL(__vdpa_alloc_device);
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa.c|201| <<__vdpa_register_device>> dev = bus_find_device(&vdpa_bus, NULL, dev_name(&vdev->dev), vdpa_name_match);
+ *   - drivers/vdpa/vdpa.c|552| <<vdpa_nl_cmd_dev_del_set_doit>> dev = bus_find_device(&vdpa_bus, NULL, name, vdpa_name_match);
+ *   - drivers/vdpa/vdpa.c|631| <<vdpa_nl_cmd_dev_get_doit>> dev = bus_find_device(&vdpa_bus, NULL, devname, vdpa_name_match);
+ */
 static int vdpa_name_match(struct device *dev, const void *data)
 {
 	struct vdpa_device *vdev = container_of(dev, struct vdpa_device, dev);
@@ -134,6 +192,11 @@ static int vdpa_name_match(struct device *dev, const void *data)
 	return (strcmp(dev_name(&vdev->dev), data) == 0);
 }
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa.c|184| <<_vdpa_register_device>> return __vdpa_register_device(vdev, nvqs);
+ *   - drivers/vdpa/vdpa.c|206| <<vdpa_register_device>> err = __vdpa_register_device(vdev, nvqs);
+ */
 static int __vdpa_register_device(struct vdpa_device *vdev, int nvqs)
 {
 	struct device *dev;
@@ -159,6 +222,12 @@ static int __vdpa_register_device(struct vdpa_device *vdev, int nvqs)
  *
  * Return: Returns an error when fail to add device to vDPA bus
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2050| <<mlx5_vdpa_dev_add>> err = _vdpa_register_device(&mvdev->vdev, 2 * mlx5_vdpa_max_qps(max_vqs));
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|272| <<vdpasim_blk_dev_add>> ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_BLK_VQ_NUM);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|195| <<vdpasim_net_dev_add>> ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_NET_VQ_NUM);
+ */
 int _vdpa_register_device(struct vdpa_device *vdev, int nvqs)
 {
 	if (!vdev->mdev)
@@ -176,6 +245,11 @@ EXPORT_SYMBOL_GPL(_vdpa_register_device);
  *
  * Return: Returns an error when fail to add to vDPA bus
  */
+/*
+ * called by:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|518| <<ifcvf_probe>> ret = vdpa_register_device(&adapter->vdpa, IFCVF_MAX_QUEUE_PAIRS * 2);
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|451| <<vp_vdpa_probe>> ret = vdpa_register_device(&vp_vdpa->vdpa, vp_vdpa->queues);
+ */
 int vdpa_register_device(struct vdpa_device *vdev, int nvqs)
 {
 	int err;
@@ -220,6 +294,14 @@ EXPORT_SYMBOL_GPL(vdpa_unregister_device);
  *
  * Return: Returns an err when fail to do the registration
  */
+/*
+ * called by:
+ *   - include/linux/vdpa.h|298| <<vdpa_register_driver>> __vdpa_register_driver(drv, THIS_MODULE)
+ *
+ * 只在virtio和vhost中使用vdpa_driver:
+ *   - drivers/virtio/virtio_vdpa.c:static struct vdpa_driver virtio_vdpa_driver = {
+ *   - drivers/vhost/vdpa.c:static struct vdpa_driver vhost_vdpa_driver = {
+ */
 int __vdpa_register_driver(struct vdpa_driver *drv, struct module *owner)
 {
 	drv->driver.bus = &vdpa_bus;
@@ -248,6 +330,12 @@ EXPORT_SYMBOL_GPL(vdpa_unregister_driver);
  * Return: Returns 0 on success or failure when required callback ops are not
  *         initialized.
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2106| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|315| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|246| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+ */
 int vdpa_mgmtdev_register(struct vdpa_mgmt_dev *mdev)
 {
 	if (!mdev->device || !mdev->ops || !mdev->ops->dev_add || !mdev->ops->dev_del)
@@ -255,6 +343,12 @@ int vdpa_mgmtdev_register(struct vdpa_mgmt_dev *mdev)
 
 	INIT_LIST_HEAD(&mdev->list);
 	mutex_lock(&vdpa_dev_mutex);
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|280| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|340| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|431| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_add_tail(&mdev->list, &mdev_head);
 	mutex_unlock(&vdpa_dev_mutex);
 	return 0;
@@ -640,6 +734,15 @@ static const struct genl_ops vdpa_nl_ops[] = {
 	},
 };
 
+/*
+ * 在以下使用vdpa_nl_family:
+ *   - drivers/vdpa/vdpa.c|29| <<global>> static struct genl_family vdpa_nl_family;
+ *   - drivers/vdpa/vdpa.c|692| <<global>> static struct genl_family vdpa_nl_family __ro_after_init = {
+ *   - drivers/vdpa/vdpa.c|392| <<vdpa_mgmtdev_fill>> hdr = genlmsg_put(msg, portid, seq, &vdpa_nl_family, flags, VDPA_CMD_MGMTDEV_NEW);
+ *   - drivers/vdpa/vdpa.c|544| <<vdpa_dev_fill>> hdr = genlmsg_put(msg, portid, seq, &vdpa_nl_family, flags, VDPA_CMD_DEV_NEW);
+ *   - drivers/vdpa/vdpa.c|710| <<vdpa_init>> err = genl_register_family(&vdpa_nl_family);
+ *   - drivers/vdpa/vdpa.c|722| <<vdpa_exit>> genl_unregister_family(&vdpa_nl_family);
+ */
 static struct genl_family vdpa_nl_family __ro_after_init = {
 	.name = VDPA_GENL_NAME,
 	.version = VDPA_GENL_VERSION,
@@ -655,6 +758,19 @@ static int vdpa_init(void)
 {
 	int err;
 
+	/*
+	 * 在以下使用vdpa_bus:
+	 *   - drivers/vdpa/vdpa.c|103| <<__vdpa_alloc_device>> vdev->dev.bus = &vdpa_bus;
+	 *   - drivers/vdpa/vdpa.c|144| <<__vdpa_register_device>> dev = bus_find_device(&vdpa_bus, NULL, dev_name(&vdev->dev), vdpa_name_match);
+	 *   - drivers/vdpa/vdpa.c|230| <<__vdpa_register_driver>> drv->driver.bus = &vdpa_bus;
+	 *   - drivers/vdpa/vdpa.c|286| <<vdpa_mgmtdev_unregister>> bus_for_each_dev(&vdpa_bus, NULL, mdev, vdpa_match_remove);
+	 *   - drivers/vdpa/vdpa.c|469| <<vdpa_nl_cmd_dev_del_set_doit>> dev = bus_find_device(&vdpa_bus, NULL, name, vdpa_name_match);
+	 *   - drivers/vdpa/vdpa.c|548| <<vdpa_nl_cmd_dev_get_doit>> dev = bus_find_device(&vdpa_bus, NULL, devname, vdpa_name_match);
+	 *   - drivers/vdpa/vdpa.c|609| <<vdpa_nl_cmd_dev_get_dumpit>> bus_for_each_dev(&vdpa_bus, NULL, &info, vdpa_dev_dump);
+	 *   - drivers/vdpa/vdpa.c|663| <<vdpa_init>> err = bus_register(&vdpa_bus);
+	 *   - drivers/vdpa/vdpa.c|672| <<vdpa_init>> bus_unregister(&vdpa_bus);
+	 *   - drivers/vdpa/vdpa.c|679| <<vdpa_exit>> bus_unregister(&vdpa_bus);
+	 */
 	err = bus_register(&vdpa_bus);
 	if (err)
 		return err;
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim.c b/drivers/vdpa/vdpa_sim/vdpa_sim.c
index 98f793bc9376..c861baa16941 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim.c
@@ -51,6 +51,14 @@ static struct vdpasim *dev_to_sim(struct device *dev)
 	return vdpa_to_sim(vdpa);
 }
 
+/*
+ * [0] vdpasim_vq_notify.cold.13
+ * [0] vdpasim_net_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void vdpasim_vq_notify(struct vringh *vring)
 {
 	struct vdpasim_virtqueue *vq =
@@ -74,6 +82,11 @@ static void vdpasim_queue_ready(struct vdpasim *vdpasim, unsigned int idx)
 			  (struct vring_used *)
 			  (uintptr_t)vq->device_addr);
 
+	/*
+	 * struct vdpasim_virtqueue *vq:
+	 * -> struct vringh vring;
+	 *    -> void (*notify)(struct vringh *);
+	 */
 	vq->vring.notify = vdpasim_vq_notify;
 }
 
@@ -237,6 +250,11 @@ static const struct dma_map_ops vdpasim_dma_ops = {
 static const struct vdpa_config_ops vdpasim_config_ops;
 static const struct vdpa_config_ops vdpasim_batch_config_ops;
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|268| <<vdpasim_blk_dev_add>> simdev = vdpasim_create(&dev_attr);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|145| <<vdpasim_net_dev_add>> simdev = vdpasim_create(&dev_attr);
+ */
 struct vdpasim *vdpasim_create(struct vdpasim_dev_attr *dev_attr)
 {
 	const struct vdpa_config_ops *ops;
@@ -249,6 +267,13 @@ struct vdpasim *vdpasim_create(struct vdpasim_dev_attr *dev_attr)
 	else
 		ops = &vdpasim_config_ops;
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|478| <<ifcvf_probe>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2014| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|252| <<vdpasim_create>> vdpasim = vdpa_alloc_device(struct vdpasim, vdpa, NULL, ops,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|401| <<vp_vdpa_probe>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+	 */
 	vdpasim = vdpa_alloc_device(struct vdpasim, vdpa, NULL, ops,
 				    dev_attr->name);
 	if (!vdpasim)
@@ -259,6 +284,21 @@ struct vdpasim *vdpasim_create(struct vdpasim_dev_attr *dev_attr)
 	spin_lock_init(&vdpasim->lock);
 	spin_lock_init(&vdpasim->iommu_lock);
 
+	/*
+	 * struct vdpasim *simdev;
+	 * -> struct vdpa_device vdpa;
+	 *    -> struct device dev;
+	 *    -> struct device *dma_dev;
+	 *    -> const struct vdpa_config_ops *config;
+	 *    -> unsigned int index;
+	 *    -> bool features_valid;
+	 *    -> int nvqs;
+	 *    -> struct vdpa_mgmt_dev *mdev;
+	 *       -> struct device *device;
+	 *       -> const struct vdpa_mgmtdev_ops *ops;
+	 *       -> const struct virtio_device_id *id_table;
+	 *       -> struct list_head list;
+	 */
 	dev = &vdpasim->vdpa.dev;
 	dev->dma_mask = &dev->coherent_dma_mask;
 	if (dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64)))
@@ -327,6 +367,52 @@ static void vdpasim_set_vq_num(struct vdpa_device *vdpa, u16 idx, u32 num)
 	vq->num = num;
 }
 
+/*
+ * [0] vdpasim_kick_vq
+ * [0] virtio_vdpa_notify
+ * [0] virtqueue_notify
+ * [0] start_xmit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_mc_output
+ * [0] ip_send_skb
+ * [0] udp_send_skb.isra.59
+ * [0] udp_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vdpasim_kick_vq
+ * [0] virtio_vdpa_notify
+ * [0] virtqueue_notify
+ * [0] try_fill_recv
+ * [0] virtnet_poll
+ * [0] __napi_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] do_softirq
+ * [0] </IRQ>
+ * [0] __local_bh_enable_ip
+ * [0] vdpasim_net_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] skb_recv_done
+ * [0] vring_interrupt
+ * [0] vdpasim_net_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void vdpasim_kick_vq(struct vdpa_device *vdpa, u16 idx)
 {
 	struct vdpasim *vdpasim = vdpa_to_sim(vdpa);
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim_net.c b/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
index a1ab6163f7d1..ac436880da99 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
@@ -35,6 +35,10 @@ MODULE_PARM_DESC(macaddr, "Ethernet MAC address");
 
 static u8 macaddr_buf[ETH_ALEN];
 
+/*
+ * 在以下使用vdpasim_net_work:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|177| <<vdpasim_net_dev_add>> dev_attr.work_fn = vdpasim_net_work;
+ */
 static void vdpasim_net_work(struct work_struct *work)
 {
 	struct vdpasim *vdpasim = container_of(work, struct vdpasim, work);
@@ -126,6 +130,41 @@ static struct device vdpasim_net_mgmtdev = {
 	.release = vdpasim_net_mgmtdev_release,
 };
 
+/*
+ * [0] virtnet_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] register_virtio_device
+ * [0] virtio_vdpa_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] ? _vdpa_register_device
+ * [0] ? bus_find_device
+ * [0] vdpasim_net_dev_add
+ * [0] ? vdpasim_net_dev_add
+ * [0] ? vdpasim_create
+ * [0] vdpa_nl_cmd_dev_add_set_doit
+ * [0] genl_family_rcv_msg_doit.isra.17
+ * [0] genl_rcv_msg
+ * [0] netlink_rcv_skb
+ * [0] genl_rcv
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] __sys_sendto
+ * [0] __x64_sys_sendto
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int vdpasim_net_dev_add(struct vdpa_mgmt_dev *mdev, const char *name)
 {
 	struct vdpasim_dev_attr dev_attr = {};
@@ -146,6 +185,17 @@ static int vdpasim_net_dev_add(struct vdpa_mgmt_dev *mdev, const char *name)
 	if (IS_ERR(simdev))
 		return PTR_ERR(simdev);
 
+	/*
+	 * struct vdpasim *simdev;
+	 * -> struct vdpa_device vdpa;
+	 *    -> struct device dev;
+	 *    -> struct device *dma_dev;
+	 *    -> const struct vdpa_config_ops *config;
+	 *    -> unsigned int index;
+	 *    -> bool features_valid;
+	 *    -> int nvqs;
+	 *    -> struct vdpa_mgmt_dev *mdev;
+	 */
 	ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_NET_VQ_NUM);
 	if (ret)
 		goto reg_err;
@@ -197,6 +247,12 @@ static int __init vdpasim_net_init(void)
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2106| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|315| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|246| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 */
 	ret = vdpa_mgmtdev_register(&mgmt_dev);
 	if (ret)
 		goto parent_err;
diff --git a/drivers/vdpa/virtio_pci/vp_vdpa.c b/drivers/vdpa/virtio_pci/vp_vdpa.c
index c76ebb531212..c804173ee553 100644
--- a/drivers/vdpa/virtio_pci/vp_vdpa.c
+++ b/drivers/vdpa/virtio_pci/vp_vdpa.c
@@ -398,6 +398,13 @@ static int vp_vdpa_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|478| <<ifcvf_probe>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2014| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|252| <<vdpasim_create>> vdpasim = vdpa_alloc_device(struct vdpasim, vdpa, NULL, ops,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|401| <<vp_vdpa_probe>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+	 */
 	vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
 				    dev, &vp_vdpa_ops, NULL);
 	if (vp_vdpa == NULL) {
diff --git a/drivers/vfio/pci/vfio_pci_intrs.c b/drivers/vfio/pci/vfio_pci_intrs.c
index 869dce5f134d..a4717269c271 100644
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@ -285,6 +285,11 @@ static int vfio_msi_enable(struct vfio_pci_device *vdev, int nvec, bool msix)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|377| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|382| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, j, -1, msix);
+ */
 static int vfio_msi_set_vector_signal(struct vfio_pci_device *vdev,
 				      int vector, int fd, bool msix)
 {
@@ -349,6 +354,17 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_device *vdev,
 		return ret;
 	}
 
+	/*
+	 * struct vfio_pci_device *vdev:
+	 * -> struct vfio_pci_irq_ctx *ctx;
+	 *    -> struct eventfd_ctx      *trigger;
+	 *    -> struct virqfd           *unmask;
+	 *    -> struct virqfd           *mask;
+	 *    -> char                    *name;
+	 *    -> bool                    masked;
+	 *    -> struct irq_bypass_producer      producer
+	 * -> int num_ctx;
+	 */
 	vdev->ctx[vector].producer.token = trigger;
 	vdev->ctx[vector].producer.irq = irq;
 	ret = irq_bypass_register_producer(&vdev->ctx[vector].producer);
diff --git a/drivers/vhost/iotlb.c b/drivers/vhost/iotlb.c
index 0fd3f87e913c..8b43a1e0e605 100644
--- a/drivers/vhost/iotlb.c
+++ b/drivers/vhost/iotlb.c
@@ -25,6 +25,12 @@ INTERVAL_TREE_DEFINE(struct vhost_iotlb_map,
  * @iotlb: the IOTLB
  * @map: the map that want to be remove and freed
  */
+/*
+ * called by:
+ *   - drivers/vhost/iotlb.c|71| <<vhost_iotlb_add_range>> vhost_iotlb_map_free(iotlb, map);
+ *   - drivers/vhost/iotlb.c|106| <<vhost_iotlb_del_range>> vhost_iotlb_map_free(iotlb, map);
+ *   - drivers/vhost/vdpa.c|587| <<vhost_vdpa_iotlb_unmap>> vhost_iotlb_map_free(iotlb, map);
+ */
 void vhost_iotlb_map_free(struct vhost_iotlb *iotlb,
 			  struct vhost_iotlb_map *map)
 {
@@ -46,6 +52,15 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_map_free);
  * Returns an error last is smaller than start or memory allocation
  * fails
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|160| <<vdpasim_map_range>> ret = vhost_iotlb_add_range(vdpasim->iommu, (u64)dma_addr,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|616| <<vdpasim_set_map>> ret = vhost_iotlb_add_range(vdpasim->iommu, map->start,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|637| <<vdpasim_dma_map>> ret = vhost_iotlb_add_range(vdpasim->iommu, iova, iova + size - 1, pa,
+ *   - drivers/vhost/vdpa.c|630| <<vhost_vdpa_map>> r = vhost_iotlb_add_range(dev->iotlb, iova, iova + size - 1,
+ *   - drivers/vhost/vhost.c|1125| <<vhost_process_iotlb_msg>> if (vhost_iotlb_add_range(dev->iotlb, msg->iova,
+ *   - drivers/vhost/vhost.c|1468| <<vhost_set_memory>> if (vhost_iotlb_add_range(newumem,
+ */
 int vhost_iotlb_add_range(struct vhost_iotlb *iotlb,
 			  u64 start, u64 last,
 			  u64 addr, unsigned int perm)
@@ -88,6 +103,14 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_add_range);
  * @start: start of the IOVA range
  * @last: last of IOVA range
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|176| <<vdpasim_unmap_range>> vhost_iotlb_del_range(vdpasim->iommu, (u64)dma_addr,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|649| <<vdpasim_dma_unmap>> vhost_iotlb_del_range(vdpasim->iommu, iova, iova + size - 1);
+ *   - drivers/vhost/iotlb.c|140| <<vhost_iotlb_reset>> vhost_iotlb_del_range(iotlb, 0ULL, 0ULL - 1);
+ *   - drivers/vhost/vdpa.c|646| <<vhost_vdpa_map>> vhost_iotlb_del_range(dev->iotlb, iova, iova + size - 1);
+ *   - drivers/vhost/vhost.c|1139| <<vhost_process_iotlb_msg>> vhost_iotlb_del_range(dev->iotlb, msg->iova,
+ */
 void vhost_iotlb_del_range(struct vhost_iotlb *iotlb, u64 start, u64 last)
 {
 	struct vhost_iotlb_map *map;
@@ -105,6 +128,12 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_del_range);
  *
  * Returns an error is memory allocation fails
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|318| <<vdpasim_create>> vdpasim->iommu = vhost_iotlb_alloc(max_iotlb_entries, 0);
+ *   - drivers/vhost/vdpa.c|946| <<vhost_vdpa_open>> dev->iotlb = vhost_iotlb_alloc(0, 0);
+ *   - drivers/vhost/vhost.c|647| <<iotlb_alloc>> return vhost_iotlb_alloc(max_iotlb_entries,
+ */
 struct vhost_iotlb *vhost_iotlb_alloc(unsigned int limit, unsigned int flags)
 {
 	struct vhost_iotlb *iotlb = kzalloc(sizeof(*iotlb), GFP_KERNEL);
@@ -126,6 +155,13 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_alloc);
  * vhost_iotlb_reset - reset vhost IOTLB (free all IOTLB entries)
  * @iotlb: the IOTLB to be reset
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|116| <<vdpasim_reset>> vhost_iotlb_reset(vdpasim->iommu);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|612| <<vdpasim_set_map>> vhost_iotlb_reset(vdpasim->iommu);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|625| <<vdpasim_set_map>> vhost_iotlb_reset(vdpasim->iommu);
+ *   - drivers/vhost/iotlb.c|151| <<vhost_iotlb_free>> vhost_iotlb_reset(iotlb);
+ */
 void vhost_iotlb_reset(struct vhost_iotlb *iotlb)
 {
 	vhost_iotlb_del_range(iotlb, 0ULL, 0ULL - 1);
@@ -136,6 +172,15 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_reset);
  * vhost_iotlb_free - reset and free vhost IOTLB
  * @iotlb: the IOTLB to be freed
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|671| <<vdpasim_free>> vhost_iotlb_free(vdpasim->iommu);
+ *   - drivers/vhost/vhost.c|723| <<vhost_dev_cleanup>> vhost_iotlb_free(dev->umem);
+ *   - drivers/vhost/vhost.c|725| <<vhost_dev_cleanup>> vhost_iotlb_free(dev->iotlb);
+ *   - drivers/vhost/vhost.c|1491| <<vhost_set_memory>> vhost_iotlb_free(oldumem);
+ *   - drivers/vhost/vhost.c|1495| <<vhost_set_memory>> vhost_iotlb_free(newumem);
+ *   - drivers/vhost/vhost.c|1771| <<vhost_init_device_iotlb>> vhost_iotlb_free(oiotlb);
+ */
 void vhost_iotlb_free(struct vhost_iotlb *iotlb)
 {
 	if (iotlb) {
@@ -151,6 +196,19 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_free);
  * @start: start of IOVA range
  * @last: last byte in IOVA range
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/core/mr.c|244| <<map_direct_mr>> for (map = vhost_iotlb_itree_first(iotlb, mr->start, mr->end - 1);
+ *   - drivers/vdpa/mlx5/core/mr.c|260| <<map_direct_mr>> for (map = vhost_iotlb_itree_first(iotlb, mr->start, mr->end - 1);
+ *   - drivers/vdpa/mlx5/core/mr.c|381| <<_mlx5_vdpa_create_mr>> for (map = vhost_iotlb_itree_first(iotlb, start, last); map;
+ *   - drivers/vdpa/mlx5/core/mr.c|464| <<map_empty>> return !vhost_iotlb_itree_first(iotlb, 0, U64_MAX);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|614| <<vdpasim_set_map>> for (map = vhost_iotlb_itree_first(iotlb, start, last); map;
+ *   - drivers/vhost/vdpa.c|577| <<vhost_vdpa_iotlb_unmap>> while ((map = vhost_iotlb_itree_first(iotlb, start, last)) != NULL) {
+ *   - drivers/vhost/vdpa.c|689| <<vhost_vdpa_process_iotlb_update>> if (vhost_iotlb_itree_first(iotlb, msg->iova,
+ *   - drivers/vhost/vhost.c|1350| <<iotlb_access_ok>> map = vhost_iotlb_itree_first(umem, addr, last);
+ *   - drivers/vhost/vhost.c|2094| <<translate_desc>> map = vhost_iotlb_itree_first(umem, addr, addr + len - 1);
+ *   - drivers/vhost/vringh.c|1110| <<iotlb_translate>> map = vhost_iotlb_itree_first(iotlb, addr,
+ */
 struct vhost_iotlb_map *
 vhost_iotlb_itree_first(struct vhost_iotlb *iotlb, u64 start, u64 last)
 {
@@ -164,6 +222,13 @@ EXPORT_SYMBOL_GPL(vhost_iotlb_itree_first);
  * @start: start of IOVA range
  * @last: last byte IOVA range
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/core/mr.c|245| <<map_direct_mr>> map; map = vhost_iotlb_itree_next(map, start, mr->end - 1)) {
+ *   - drivers/vdpa/mlx5/core/mr.c|261| <<map_direct_mr>> map; map = vhost_iotlb_itree_next(map, mr->start, mr->end - 1)) {
+ *   - drivers/vdpa/mlx5/core/mr.c|382| <<_mlx5_vdpa_create_mr>> map = vhost_iotlb_itree_next(map, start, last)) {
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|615| <<vdpasim_set_map>> map = vhost_iotlb_itree_next(map, start, last)) {
+ */
 struct vhost_iotlb_map *
 vhost_iotlb_itree_next(struct vhost_iotlb_map *map, u64 start, u64 last)
 {
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index d16c04dcc144..8b63ce5b5eec 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -79,6 +79,11 @@ struct vhost_scsi_cmd {
 	/* virtio-scsi initiator data direction */
 	enum dma_data_direction tvc_data_direction;
 	/* Expected data transfer length from virtio-scsi header */
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_exp_data_len:
+	 *   - drivers/vhost/scsi.c|684| <<vhost_scsi_get_cmd>> cmd->tvc_exp_data_len = exp_data_len;
+	 *   - drivers/vhost/scsi.c|863| <<vhost_scsi_target_queue_cmd>> cmd->tvc_lun, cmd->tvc_exp_data_len,
+	 */
 	u32 tvc_exp_data_len;
 	/* The Tag from include/linux/virtio_scsi.h:struct virtio_scsi_cmd_req */
 	u64 tvc_tag;
@@ -102,6 +107,12 @@ struct vhost_scsi_cmd {
 	/* The TCM I/O descriptor that is accessed via container_of() */
 	struct se_cmd tvc_se_cmd;
 	/* Copy of the incoming SCSI command descriptor block (CDB) */
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE]:
+	 *   - drivers/vhost/scsi.c|694| <<vhost_scsi_get_cmd>> memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
+	 *   - drivers/vhost/scsi.c|887| <<vhost_scsi_target_queue_cmd>> if (target_submit_prep(se_cmd, cmd->tvc_cdb, sg_ptr,
+	 *   - drivers/vhost/scsi.c|1299| <<vhost_scsi_handle_vq>> cmd->tvc_cdb[0], cmd->tvc_lun);
+	 */
 	unsigned char tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE];
 	/* Sense buffer that will be mapped into outgoing status */
 	unsigned char tvc_sense_buf[TRANSPORT_SENSE_BUFFER];
@@ -118,6 +129,15 @@ struct vhost_scsi_nexus {
 
 struct vhost_scsi_tpg {
 	/* Vhost port target portal group tag for TCM */
+	/*
+	 * 在以下使用vhost_scsi_tpg:
+	 *   - drivers/vhost/scsi.c|327| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|1546| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|1751| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|1771| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+	 *   - drivers/vhost/scsi.c|1875| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+	 *   - drivers/vhost/scsi.c|2462| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+	 */
 	u16 tport_tpgt;
 	/* Used to track number of TPG Port/Lun Links wrt to explict I_T Nexus shutdown */
 	int tv_tpg_port_count;
@@ -200,6 +220,12 @@ struct vhost_scsi {
 	struct vhost_dev dev;
 	struct vhost_scsi_virtqueue vqs[VHOST_SCSI_MAX_VQ];
 
+	/*
+	 * 在以下使用vhost_scsi->vs_completion_work:
+	 *   - drivers/vhost/scsi.c|383| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+	 *   - drivers/vhost/scsi.c|1467| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_completion_work);
+	 *   - drivers/vhost/scsi.c|1819| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+	 */
 	struct vhost_work vs_completion_work; /* cmd completion work item */
 	struct llist_head vs_completion_list; /* cmd completion queue */
 
@@ -240,6 +266,11 @@ struct vhost_scsi_ctx {
 
 /* Global spinlock to protect vhost_scsi TPG list for vhost IOCTL access */
 static DEFINE_MUTEX(vhost_scsi_mutex);
+/*
+ * 在以下使用vhost_scsi_list():
+ *   - drivers/vhost/scsi.c|1738| <<vhost_scsi_set_endpoint>> list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
+ *   - drivers/vhost/scsi.c|2470| <<vhost_scsi_make_tpg>> list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
+ */
 static LIST_HEAD(vhost_scsi_list);
 
 static void vhost_scsi_done_inflight(struct kref *kref)
@@ -367,6 +398,10 @@ static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_transport.c|2941| <<target_release_cmd_kref>> se_cmd->se_tfo->release_cmd(se_cmd);
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	if (se_cmd->se_cmd_flags & SCF_SCSI_TMR_CDB) {
@@ -541,6 +576,15 @@ static void vhost_scsi_evt_work(struct vhost_work *work)
  * This is scheduled in the vhost work queue so we are called with the owner
  * process mm and can access the vring.
  */
+/*
+ * 在以下使用vhost_scsi->vs_completion_work:
+ *   - drivers/vhost/scsi.c|383| <<vhost_scsi_release_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1467| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1819| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *
+ * 在以下使用vhost_scsi_complete_cmd_work():
+ *   - drivers/vhost/scsi.c|1819| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ */
 static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 {
 	struct vhost_scsi *vs = container_of(work, struct vhost_scsi,
@@ -564,6 +608,22 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		memset(&v_rsp, 0, sizeof(v_rsp));
 		v_rsp.resid = cpu_to_vhost32(cmd->tvc_vq, se_cmd->residual_count);
 		/* TODO is status_qualifier field needed? */
+		/*
+		 * 在以下设置se_cmd->scsi_status:
+		 *   - drivers/target/target_core_transport.c|794| <<target_handle_abort>> cmd->scsi_status = SAM_STAT_TASK_ABORTED;
+		 *   - drivers/target/target_core_transport.c|867| <<target_complete_cmd>> cmd->scsi_status = scsi_status;
+		 *   - drivers/target/target_core_transport.c|2073| <<transport_generic_request_failure>> cmd->scsi_status = SAM_STAT_TASK_SET_FULL;
+		 *   - drivers/target/target_core_transport.c|2076| <<transport_generic_request_failure>> cmd->scsi_status = SAM_STAT_BUSY;
+		 *   - drivers/target/target_core_transport.c|2085| <<transport_generic_request_failure>> cmd->scsi_status = SAM_STAT_RESERVATION_CONFLICT;
+		 *   - drivers/target/target_core_transport.c|2153| <<__target_execute_cmd>> cmd->scsi_status = SAM_STAT_RESERVATION_CONFLICT;
+		 *   - drivers/target/target_core_transport.c|3429| <<translate_sense_reason>> cmd->scsi_status = SAM_STAT_BUSY;
+		 *   - drivers/target/target_core_transport.c|3442| <<translate_sense_reason>> cmd->scsi_status = SAM_STAT_CHECK_CONDITION;
+		 *   - drivers/target/target_core_transport.c|3485| <<target_send_busy>> cmd->scsi_status = SAM_STAT_BUSY;
+		 *   - drivers/target/target_core_xcopy.c|624| <<target_xcopy_read_source>> ec_cmd->scsi_status = se_cmd->scsi_status;
+		 *   - drivers/target/target_core_xcopy.c|633| <<target_xcopy_read_source>> ec_cmd->scsi_status = se_cmd->scsi_status;
+		 *   - drivers/target/target_core_xcopy.c|669| <<target_xcopy_write_destination>> ec_cmd->scsi_status = se_cmd->scsi_status;
+		 *   - drivers/target/target_core_xcopy.c|675| <<target_xcopy_write_destination>> ec_cmd->scsi_status = se_cmd->scsi_status;
+		 */
 		v_rsp.status = se_cmd->scsi_status;
 		v_rsp.sense_len = cpu_to_vhost32(cmd->tvc_vq,
 						 se_cmd->scsi_sense_length);
@@ -591,6 +651,10 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_signal(&vs->dev, &vs->vqs[vq].vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1206| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -715,6 +779,10 @@ vhost_scsi_iov_to_sgl(struct vhost_scsi_cmd *cmd, bool write,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1225| <<vhost_scsi_handle_vq>> if (unlikely(vhost_scsi_mapal(cmd, prot_bytes,
+ */
 static int
 vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 		 size_t prot_bytes, struct iov_iter *prot_iter,
@@ -778,6 +846,10 @@ static int vhost_scsi_to_tcm_attr(int attr)
 	return TCM_SIMPLE_TAG;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1132| <<vhost_scsi_handle_vq>> vhost_scsi_target_queue_cmd(cmd);
+ */
 static void vhost_scsi_target_queue_cmd(struct vhost_scsi_cmd *cmd)
 {
 	struct se_cmd *se_cmd = &cmd->tvc_se_cmd;
@@ -798,19 +870,44 @@ static void vhost_scsi_target_queue_cmd(struct vhost_scsi_cmd *cmd)
 	tv_nexus = cmd->tvc_nexus;
 
 	se_cmd->tag = 0;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_exp_data_len:
+	 *   - drivers/vhost/scsi.c|684| <<vhost_scsi_get_cmd>> cmd->tvc_exp_data_len = exp_data_len;
+	 *   - drivers/vhost/scsi.c|863| <<vhost_scsi_target_queue_cmd>> cmd->tvc_lun, cmd->tvc_exp_data_len,
+	 */
 	target_init_cmd(se_cmd, tv_nexus->tvn_se_sess, &cmd->tvc_sense_buf[0],
 			cmd->tvc_lun, cmd->tvc_exp_data_len,
 			vhost_scsi_to_tcm_attr(cmd->tvc_task_attr),
 			cmd->tvc_data_direction, TARGET_SCF_ACK_KREF);
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1540| <<srpt_handle_cmd>> if (target_submit_prep(cmd, srp_cmd->cdb, sg, sg_cnt, NULL, 0, NULL, 0,
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|495| <<tcm_qla2xxx_handle_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0, NULL, 0,
+	 *   - drivers/target/loopback/tcm_loop.c|158| <<tcm_loop_target_queue_cmd>> if (target_submit_prep(se_cmd, sc->cmnd, scsi_sglist(sc),
+	 *   - drivers/target/target_core_transport.c|1829| <<target_submit_cmd>> if (target_submit_prep(se_cmd, cdb, NULL, 0, NULL, 0, NULL, 0,
+	 *   - drivers/target/tcm_fc/tfc_cmd.c|557| <<ft_send_work>> if (target_submit_prep(&cmd->se_cmd, fcp->fc_cdb, NULL, 0, NULL, 0,
+	 *   - drivers/vhost/scsi.c|829| <<vhost_scsi_target_queue_cmd>> if (target_submit_prep(se_cmd, cmd->tvc_cdb, sg_ptr,
+	 *   - drivers/xen/xen-scsiback.c|370| <<scsiback_cmd_exec>> if (target_submit_prep(se_cmd, pending_req->cmnd, pending_req->sgl,
+	 */
 	if (target_submit_prep(se_cmd, cmd->tvc_cdb, sg_ptr,
 			       cmd->tvc_sgl_count, NULL, 0, sg_prot_ptr,
 			       cmd->tvc_prot_sgl_count, GFP_KERNEL))
 		return;
 
+	/*
+	 * called by:
+	 *   - drivers/target/loopback/tcm_loop.c|164| <<tcm_loop_target_queue_cmd>> target_queue_submission(se_cmd);
+	 *   - drivers/vhost/scsi.c|834| <<vhost_scsi_target_queue_cmd>> target_queue_submission(se_cmd);
+	 */
 	target_queue_submission(se_cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1139| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ *   - drivers/vhost/scsi.c|1362| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ */
 static void
 vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 			   struct vhost_virtqueue *vq,
@@ -830,12 +927,39 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1011| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1345| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * This looks in the virtqueue and for the first available buffer, and converts
+	 * it to an iovec for convenient access.  Since descriptors consist of some
+	 * number of output then some number of input descriptors, it's actually two
+	 * iovecs, but we pack them into one and note how many of each there were.
+	 *
+	 * This function returns the descriptor number found, or vq->num (which is
+	 * never a valid descriptor number) if none was found.  A negative code is
+	 * returned on error.
+	 *
+	 * struct vhost_virtqueue *vq:
+	 * -> struct iovec iov[UIO_MAXIOV]; (1024)
+	 *
+	 * struct vhost_scsi_ctx *vc:
+	 * -> int head;
+	 * -> unsigned int out, in;
+	 * -> size_t req_size, rsp_size;
+	 * -> size_t out_size, in_size;
+	 * -> u8 *target, *lunp;
+	 * -> void *req;
+	 * -> struct iov_iter out_iter;
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -860,6 +984,10 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	 * Get the size of request and response buffers.
 	 * FIXME: Not correct for BIDI operation
 	 */
+	/*
+	 * iov_length():
+	 * Total number of bytes covered by an iovec.
+	 */
 	vc->out_size = iov_length(vq->iov, vc->out);
 	vc->in_size = iov_length(&vq->iov[vc->out], vc->in);
 
@@ -873,6 +1001,16 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	 * point at the start of the outgoing WRITE payload, if
 	 * DMA_TO_DEVICE is set.
 	 */
+	/*
+	 * struct vhost_scsi_ctx *vc:
+	 * -> int head;
+	 * -> unsigned int out, in;
+	 * -> size_t req_size, rsp_size;
+	 * -> size_t out_size, in_size;
+	 * -> u8 *target, *lunp;
+	 * -> void *req;
+	 * -> struct iov_iter out_iter;
+	 */
 	iov_iter_init(&vc->out_iter, WRITE, vq->iov, vc->out, vc->out_size);
 	ret = 0;
 
@@ -898,6 +1036,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1452| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
@@ -913,8 +1056,17 @@ vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 	} else {
 		struct vhost_scsi_tpg **vs_tpg, *tpg;
 
+		/*
+		 * 其实返回的是struct vhost_scsi
+		 */
 		vs_tpg = vhost_vq_get_backend(vq);	/* validated at handler entry */
 
+		/*
+		 * struct vhost_scsi *vs:
+		 * -> struct vhost_scsi_tpg **vs_tpg;
+		 *
+		 * 在vhost_scsi_set_endpoint()分配
+		 */
 		tpg = READ_ONCE(vs_tpg[*vc->target]);
 		if (unlikely(!tpg)) {
 			vq_err(vq, "Target 0x%x does not exist\n", *vc->target);
@@ -933,6 +1085,17 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * 5.4的例子
+ * [0] translate_sense_reason
+ * [0] transport_generic_request_failure
+ * [0] target_submit_prep
+ * [0] vhost_scsi_handle_vq
+ * [0] vhost_scsi_handle_kick
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -955,11 +1118,19 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	 * We can handle the vq only after the endpoint is setup by calling the
 	 * VHOST_SCSI_SET_ENDPOINT ioctl.
 	 */
+	/*
+	 * 这里是获取, 大概在以下设置scsi:
+	 *   - drivers/vhost/scsi.c|1644| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+	 *   - drivers/vhost/scsi.c|1746| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+	 */
 	vs_tpg = vhost_vq_get_backend(vq);
 	if (!vs_tpg)
 		goto out;
 
 	memset(&vc, 0, sizeof(vc));
+	/*
+	 * 注意: 这里一开始就设置了vhost_scsi_ctx.rsp_size
+	 */
 	vc.rsp_size = sizeof(struct virtio_scsi_cmd_resp);
 
 	vhost_disable_notify(&vs->dev, vq);
@@ -979,6 +1150,42 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			vc.lunp = &v_req_pi.lun[0];
 			vc.target = &v_req_pi.lun[1];
 		} else {
+			/*
+			 * struct vhost_scsi_ctx vc:
+			 * -> int head;
+			 * -> unsigned int out, in;
+			 * -> size_t req_size, rsp_size;
+			 * -> size_t out_size, in_size;
+			 * -> u8 *target, *lunp;
+			 * -> void *req;
+			 * -> struct iov_iter out_iter;
+			 *
+			 *
+			 * 在virtio_scsi_init_hdr()初始化的例子
+			 * cmd->lun[0] = 1;
+			 * cmd->lun[1] = sc->device->id;
+			 * cmd->lun[2] = (sc->device->lun >> 8) | 0x40;
+			 * cmd->lun[3] = sc->device->lun & 0xff;
+			 * cmd->tag = cpu_to_virtio64(vdev, (unsigned long)sc);
+			 * cmd->task_attr = VIRTIO_SCSI_S_SIMPLE;
+			 * cmd->prio = 0;
+			 * cmd->crn = 0;
+			 *
+			 * struct virtio_scsi_cmd_req v_req;
+			 *
+			 * // SCSI command request, followed by data-out
+			 * struct virtio_scsi_cmd_req {
+			 *     __u8 lun[8];            // Logical Unit Number
+			 *     __virtio64 tag;         // Command identifier
+			 *     __u8 task_attr;         // Task attribute
+			 *     __u8 prio;              // SAM command priority field
+			 *     __u8 crn;
+			 *     __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+			 * } __attribute__((packed));
+			 *
+			 * 此时v_req应该还没初始化.
+			 * 注意v_req的类型是virtio/vhost共享的struct virtio_scsi_cmd_req
+			 */
 			vc.req = &v_req;
 			vc.req_size = sizeof(v_req);
 			vc.lunp = &v_req.lun[0];
@@ -994,6 +1201,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		if (ret)
 			goto err;
 
+		/*
+		 * 一开始tpg并未初始化
+		 */
 		ret = vhost_scsi_get_req(vq, &vc, &tpg);
 		if (ret)
 			goto err;
@@ -1104,6 +1314,12 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		cmd->tvc_resp_iov = vq->iov[vc.out];
 		cmd->tvc_in_iovs = vc.in;
 
+		/*
+		 * 在以下使用vhost_scsi_cmd->tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE]:
+		 *   - drivers/vhost/scsi.c|694| <<vhost_scsi_get_cmd>> memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
+		 *   - drivers/vhost/scsi.c|887| <<vhost_scsi_target_queue_cmd>> if (target_submit_prep(se_cmd, cmd->tvc_cdb, sg_ptr,
+		 *   - drivers/vhost/scsi.c|1299| <<vhost_scsi_handle_vq>> cmd->tvc_cdb[0], cmd->tvc_lun);
+		 */
 		pr_debug("vhost_scsi got command opcode: %#02x, lun: %d\n",
 			 cmd->tvc_cdb[0], cmd->tvc_lun);
 		pr_debug("cmd: %p exp_data_len: %d, prot_bytes: %d data_direction:"
@@ -1234,6 +1450,10 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 				 VIRTIO_SCSI_S_FUNCTION_REJECTED);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1509| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_an_resp(vs, vq, &vc);
+ */
 static void
 vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 			struct vhost_virtqueue *vq,
@@ -1256,6 +1476,10 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_ctrl_an_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1380| <<vhost_scsi_ctl_handle_kick>> vhost_scsi_ctl_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1365,6 +1589,10 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_ctl_handle_kick():
+ *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_open>> vs->vqs[VHOST_SCSI_VQ_CTL].vq.handle_kick = vhost_scsi_ctl_handle_kick;
+ */
 static void vhost_scsi_ctl_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1547,6 +1775,10 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
  *  The lock nesting rule is:
  *    vhost_scsi_mutex -> vs->dev.mutex -> tpg->tv_tpg_mutex -> vq->mutex
  */
+/*
+ * 处理VHOST_SCSI_SET_ENDPOINT:
+ *   - drivers/vhost/scsi.c|2007| <<vhost_scsi_ioctl(VHOST_SCSI_SET_ENDPOINT)>> return vhost_scsi_set_endpoint(vs, &backend);
+ */
 static int
 vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			struct vhost_scsi_target *t)
@@ -1577,9 +1809,22 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 		ret = -ENOMEM;
 		goto out;
 	}
+	/*
+	 * struct vhost_scsi *vs:
+	 * -> struct vhost_scsi_tpg **vs_tpg;
+	 *
+	 * 注意: 如果已经分配了要拷贝过去!!!
+	 */
 	if (vs->vs_tpg)
 		memcpy(vs_tpg, vs->vs_tpg, len);
 
+	/*
+	 * 在以下使用vhost_scsi_list:
+	 *   - drivers/vhost/scsi.c|1738| <<vhost_scsi_set_endpoint>> list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
+	 *   - drivers/vhost/scsi.c|2470| <<vhost_scsi_make_tpg>> list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
+	 *
+	 * struct vhost_scsi_tpg *tpg;
+	 */
 	list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
 		mutex_lock(&tpg->tv_tpg_mutex);
 		if (!tpg->tpg_nexus) {
@@ -1613,6 +1858,15 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			}
 			tpg->tv_tpg_vhost_count++;
 			tpg->vhost_scsi = vs;
+			/*
+			 * 在以下使用vhost_scsi_tpg:
+			 *   - drivers/vhost/scsi.c|327| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+			 *   - drivers/vhost/scsi.c|1546| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+			 *   - drivers/vhost/scsi.c|1751| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+			 *   - drivers/vhost/scsi.c|1771| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+			 *   - drivers/vhost/scsi.c|1875| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+			 *   - drivers/vhost/scsi.c|2462| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+			 */
 			vs_tpg[tpg->tport_tpgt] = tpg;
 			match = true;
 		}
@@ -2280,6 +2534,16 @@ static struct configfs_attribute *vhost_scsi_tpg_attrs[] = {
 	NULL,
 };
 
+/*
+ * 5.4的例子:
+ * [0] vhost_scsi_make_tpg
+ * [0] configfs_mkdir
+ * [0] vfs_mkdir
+ * [0] do_mkdirat
+ * [0] __x64_sys_mkdir
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static struct se_portal_group *
 vhost_scsi_make_tpg(struct se_wwn *wwn, const char *name)
 {
@@ -2425,6 +2689,11 @@ static struct configfs_attribute *vhost_scsi_wwn_attrs[] = {
 	NULL,
 };
 
+/*
+ * 在以下使用vhost_scsi_ops:
+ *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/vhost/scsi.c|2501| <<vhost_scsi_exit>> target_unregister_template(&vhost_scsi_ops);
+ */
 static const struct target_core_fabric_ops vhost_scsi_ops = {
 	.module				= THIS_MODULE,
 	.fabric_name			= "vhost",
@@ -2475,6 +2744,20 @@ static int __init vhost_scsi_init(void)
 	if (ret < 0)
 		goto out;
 
+	/*
+	 * 在以下调用target_register_template():
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3895| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4107| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1936| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1940| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+	 *   - drivers/target/iscsi/iscsi_target.c|696| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+	 *   - drivers/target/loopback/tcm_loop.c|1173| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+	 *   - drivers/target/sbp/sbp_target.c|2337| <<sbp_init>> return target_register_template(&sbp_ops);
+	 *   - drivers/target/tcm_fc/tfc_conf.c|460| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+	 *   - drivers/usb/gadget/function/f_tcm.c|2336| <<tcm_init>> ret = target_register_template(&usbg_ops);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+	 *   - drivers/xen/xen-scsiback.c|1833| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+	 */
 	ret = target_register_template(&vhost_scsi_ops);
 	if (ret < 0)
 		goto out_vhost_scsi_deregister;
diff --git a/drivers/vhost/vdpa.c b/drivers/vhost/vdpa.c
index fb41db3da611..5e0684c628ea 100644
--- a/drivers/vhost/vdpa.c
+++ b/drivers/vhost/vdpa.c
@@ -25,6 +25,60 @@
 
 #include "vhost.h"
 
+/*
+ * [0] vdpasim_kick_vq
+ * [0] vhost_poll_wakeup
+ * [0] vhost_poll_start
+ * [0] vhost_vring_ioctl
+ * [0] vhost_vdpa_unlocked_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vdpasim_kick_vq
+ * [0] vhost_poll_wakeup
+ * [0] __wake_up_common
+ * [0] eventfd_signal
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] handle_ept_misconfig
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vdpasim_kick_vq
+ * [0] vhost_poll_wakeup
+ * [0] __wake_up_common
+ * [0] eventfd_signal
+ * [0] ioeventfd_write
+ * [0] __kvm_io_bus_write
+ * [0] kvm_io_bus_write
+ * [0] write_mmio
+ * [0] emulator_read_write_onepage
+ * [0] emulator_read_write.isra.160
+ * [0] emulator_write_emulated
+ * [0] segmented_write
+ * [0] writeback
+ * [0] x86_emulate_insn
+ * [0] x86_emulate_instruction
+ * [0] kvm_mmu_page_fault
+ * [0] ? vmx_set_msr
+ * [0] ? vmx_vmexit
+ * [0] ? vmx_vmexit
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
+
 enum {
 	VHOST_VDPA_BACKEND_FEATURES =
 	(1ULL << VHOST_BACKEND_F_IOTLB_MSG_V2) |
@@ -54,6 +108,11 @@ static DEFINE_IDA(vhost_vdpa_ida);
 
 static dev_t vhost_vdpa_major;
 
+/*
+ * 在以下使用handle_vq_kick():
+ *   - drivers/vhost/test.c|121| <<vhost_test_open>> n->vqs[VHOST_TEST_VQ].handle_kick = handle_vq_kick;
+ *   - drivers/vhost/vdpa.c|873| <<vhost_vdpa_open>> vqs[i]->handle_kick = handle_vq_kick;
+ */
 static void handle_vq_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -64,6 +123,10 @@ static void handle_vq_kick(struct vhost_work *work)
 	ops->kick_vq(v->vdpa, vq - v->vqs);
 }
 
+/*
+ * 在以下使用vhost_vdpa_virtqueue_cb():
+ *   - drivers/vhost/vdpa.c|411| <<vhost_vdpa_vring_ioctl>> cb.callback = vhost_vdpa_virtqueue_cb;
+ */
 static irqreturn_t vhost_vdpa_virtqueue_cb(void *private)
 {
 	struct vhost_virtqueue *vq = private;
@@ -86,6 +149,11 @@ static irqreturn_t vhost_vdpa_config_cb(void *private)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vdpa.c|179| <<vhost_vdpa_set_status>> vhost_vdpa_setup_vq_irq(v, i);
+ *   - drivers/vhost/vdpa.c|418| <<vhost_vdpa_vring_ioctl>> vhost_vdpa_setup_vq_irq(v, idx);
+ */
 static void vhost_vdpa_setup_vq_irq(struct vhost_vdpa *v, u16 qid)
 {
 	struct vhost_virtqueue *vq = &v->vqs[qid];
@@ -1012,6 +1080,32 @@ static void vhost_vdpa_release_dev(struct device *device)
 	kfree(v);
 }
 
+/*
+ * [0] vhost_vdpa_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] bus_for_each_drv
+ * [0] __device_attach
+ * [0] bus_probe_device
+ * [0] device_add
+ * [0] ? _vdpa_register_device.cold.13
+ * [0] ? bus_find_device
+ * [0] vdpasim_net_dev_add
+ * [0] ? vdpasim_net_dev_add
+ * [0] ? vdpasim_create
+ * [0] vdpa_nl_cmd_dev_add_set_doit
+ * [0] genl_family_rcv_msg_doit.isra.17
+ * [0] genl_rcv_msg
+ * [0] netlink_rcv_skb
+ * [0] genl_rcv
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] __sys_sendto
+ * [0] __x64_sys_sendto
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int vhost_vdpa_probe(struct vdpa_device *vdpa)
 {
 	const struct vdpa_config_ops *ops = vdpa->config;
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 5ccb0705beae..45775a2626b9 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -201,6 +201,12 @@ EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|445| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/test.c|299| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|1704| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	__poll_t mask;
@@ -286,6 +292,15 @@ static void __vhost_vq_meta_reset(struct vhost_virtqueue *vq)
 {
 	int j;
 
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[VHOST_NUM_ADDRS]:
+	 *   - drivers/vhost/vhost.c|296| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|788| <<vhost_vq_meta_fetch>> const struct vhost_iotlb_map *map = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1336| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = map;
+	 *
+	 * struct vhost_virtqueue *vq:
+	 * -> const struct vhost_iotlb_map *meta_iotlb[VHOST_NUM_ADDRS];
+	 */
 	for (j = 0; j < VHOST_NUM_ADDRS; j++)
 		vq->meta_iotlb[j] = NULL;
 }
@@ -464,6 +479,14 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1332| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
+ *   - drivers/vhost/scsi.c|1824| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, UIO_MAXIOV,
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+ *   - drivers/vhost/vdpa.c|875| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,
+ *   - drivers/vhost/vsock.c|644| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -628,6 +651,12 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|653| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+ *   - drivers/vhost/vhost.c|1459| <<vhost_set_memory>> newumem = iotlb_alloc();
+ *   - drivers/vhost/vhost.c|1755| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+ */
 static struct vhost_iotlb *iotlb_alloc(void)
 {
 	return vhost_iotlb_alloc(max_iotlb_entries,
@@ -641,6 +670,11 @@ struct vhost_iotlb *vhost_dev_reset_owner_prepare(void)
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1611| <<vhost_net_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ *   - drivers/vhost/test.c|242| <<vhost_test_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ */
 void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 {
 	int i;
@@ -767,6 +801,13 @@ static bool vq_memory_access_ok(void __user *log_base, struct vhost_iotlb *umem,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|839| <<vhost_copy_to_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|873| <<vhost_copy_from_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|935| <<__vhost_get_user>> void __user *uaddr = vhost_vq_meta_fetch(vq,
+ *   - drivers/vhost/vhost.c|1346| <<iotlb_access_ok>> if (vhost_vq_meta_fetch(vq, addr, len, type))
+ */
 static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 					       u64 addr, unsigned int size,
 					       int type)
@@ -1012,6 +1053,12 @@ static void vhost_dev_unlock_vqs(struct vhost_dev *d)
 static inline int vhost_get_avail_idx(struct vhost_virtqueue *vq,
 				      __virtio16 *idx)
 {
+	/*
+	 * struct vhost_virtqueue *vq:
+	 * -> vring_desc_t __user *desc;
+	 * -> vring_avail_t __user *avail;
+	 * -> vring_used_t __user *used;
+	 */
 	return vhost_get_avail(vq, *idx, &vq->avail->idx);
 }
 
@@ -1043,6 +1090,12 @@ static inline int vhost_get_used_idx(struct vhost_virtqueue *vq,
 static inline int vhost_get_desc(struct vhost_virtqueue *vq,
 				 struct vring_desc *desc, int idx)
 {
+	/*
+	 * struct vhost_virtqueue *vq:
+	 * -> vring_desc_t __user *desc;
+	 * -> vring_avail_t __user *avail;
+	 * -> vring_used_t __user *used;
+	 */
 	return vhost_copy_from_user(vq, desc, vq->desc + idx, sizeof(*desc));
 }
 
@@ -1194,6 +1247,11 @@ __poll_t vhost_chr_poll(struct file *file, struct vhost_dev *dev,
 }
 EXPORT_SYMBOL(vhost_chr_poll);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1747| <<vhost_net_chr_read_iter>> return vhost_chr_read_iter(dev, to, noblock);
+ *   - drivers/vhost/vsock.c|864| <<vhost_vsock_chr_read_iter>> return vhost_chr_read_iter(dev, to, noblock);
+ */
 ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,
 			    int noblock)
 {
@@ -1498,6 +1556,10 @@ static long vhost_vring_set_num(struct vhost_dev *d,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1567| <<vhost_vring_set_num_addr>> r = vhost_vring_set_addr(d, vq, argp);
+ */
 static long vhost_vring_set_addr(struct vhost_dev *d,
 				 struct vhost_virtqueue *vq,
 				 void __user *argp)
@@ -1550,6 +1612,10 @@ static long vhost_vring_set_addr(struct vhost_dev *d,
 	return 0;
 }
 
+/*
+ * 处理VHOST_SET_VRING_NUM和VHOST_SET_VRING_ADDR:
+ *   - drivers/vhost/vhost.c|1600| <<vhost_vring_ioctl>> return vhost_vring_set_num_addr(d, vq, ioctl, argp);
+ */
 static long vhost_vring_set_num_addr(struct vhost_dev *d,
 				     struct vhost_virtqueue *vq,
 				     unsigned int ioctl,
@@ -1574,6 +1640,14 @@ static long vhost_vring_set_num_addr(struct vhost_dev *d,
 
 	return r;
 }
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1732| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/scsi.c|1923| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+ *   - drivers/vhost/test.c|357| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/vdpa.c|390| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+ *   - drivers/vhost/vsock.c|849| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+ */
 long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
 	struct file *eventfp, *filep = NULL;
@@ -1711,6 +1785,11 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 }
 EXPORT_SYMBOL_GPL(vhost_vring_ioctl);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1646| <<vhost_net_set_features>> if (vhost_init_device_iotlb(&n->dev, true))
+ *   - drivers/vhost/vsock.c|784| <<vhost_vsock_set_features>> if (vhost_init_device_iotlb(&vsock->dev, true))
+ */
 int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)
 {
 	struct vhost_iotlb *niotlb, *oiotlb;
@@ -1720,6 +1799,12 @@ int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)
 	if (!niotlb)
 		return -ENOMEM;
 
+	/*
+	 * struct vhost_dev *d:
+	 * -> struct vhost_iotlb *umem;
+	 * -> struct vhost_iotlb *iotlb;
+	 * -> spinlock_t iotlb_lock;
+	 */
 	oiotlb = d->iotlb;
 	d->iotlb = niotlb;
 
@@ -2028,6 +2113,16 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|846| <<vhost_copy_to_user>> ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|881| <<vhost_copy_from_user>> ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|906| <<__vhost_get_user_slow>> ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
+ *   - drivers/vhost/vhost.c|1931| <<log_used>> ret = translate_desc(vq, (uintptr_t)vq->used + used_offset,
+ *   - drivers/vhost/vhost.c|2135| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
+ *   - drivers/vhost/vhost.c|2176| <<get_indirect>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+ *   - drivers/vhost/vhost.c|2327| <<vhost_get_vq_desc>> ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
@@ -2045,6 +2140,19 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			break;
 		}
 
+		/*
+		 * struct vhost_iotlb_map {
+		 * -> struct rb_node rb;
+		 * -> struct list_head link;
+		 * -> u64 start;
+		 * -> u64 last;
+		 * -> u64 size;
+		 * -> u64 addr;
+		 * -> u32 perm;
+		 * -> u32 flags_padding;
+		 * -> u64 __subtree_last;
+		 * };
+		 */
 		map = vhost_iotlb_itree_first(umem, addr, addr + len - 1);
 		if (map == NULL || map->start > addr) {
 			if (umem != dev->iotlb) {
@@ -2190,6 +2298,17 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|569| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
+ *   - drivers/vhost/net.c|581| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
+ *   - drivers/vhost/net.c|1056| <<get_rx_bufs>> r = vhost_get_vq_desc(vq, vq->iov + seg,
+ *   - drivers/vhost/scsi.c|488| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/scsi.c|839| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(vq, vq->iov,
+ *   - drivers/vhost/vsock.c|128| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ *   - drivers/vhost/vsock.c|475| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2256,6 +2375,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	i = head;
 	do {
+		/*
+		 * 第一次来到这里的时候in_num和out_num都是0
+		 */
 		unsigned iov_count = *in_num + *out_num;
 		if (unlikely(i >= vq->num)) {
 			vq_err(vq, "Desc index is %u > %u, head = %u",
@@ -2268,6 +2390,18 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			       i, vq->num, head);
 			return -EINVAL;
 		}
+		/*
+		 * // Virtio ring descriptors: 16 bytes.  These can chain together via "next".
+		 * struct vring_desc desc;
+		 *     // Address (guest-physical).
+		 *     __virtio64 addr;
+		 *     // Length.
+		 *     __virtio32 len;
+		 *     // The flags as indicated above.
+		 *     __virtio16 flags;
+		 *     // We chain unused descriptors via this, too
+		 *     __virtio16 next;
+		 */
 		ret = vhost_get_desc(vq, &desc, i);
 		if (unlikely(ret)) {
 			vq_err(vq, "Failed to get descriptor: idx %d addr %p\n",
@@ -2291,6 +2425,11 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * iov的来源
+		 * struct vhost_virtqueue *vq:
+		 * -> struct iovec iov[UIO_MAXIOV]; (1024)
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2319,6 +2458,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			}
 			*out_num += ret;
 		}
+		/*
+		 * next_desc()返回下一个desc的index
+		 */
 	} while ((i = next_desc(vq, &desc)) != -1);
 
 	/* On success, increment avail index. */
@@ -2340,6 +2482,14 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|578| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|2574| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vhost.h|201| <<vhost_add_used_and_signal>> int vhost_add_used(struct vhost_virtqueue *, unsigned int head, int len);
+ *   - drivers/vhost/vsock.c|197| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|508| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, len);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2585,6 +2735,10 @@ void vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,
 }
 EXPORT_SYMBOL_GPL(vhost_enqueue_msg);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1260| <<vhost_chr_read_iter>> node = vhost_dequeue_msg(dev, &dev->read_list);
+ */
 struct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,
 					 struct list_head *head)
 {
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index b063324c7669..a32a3b145395 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -76,6 +76,12 @@ struct vhost_virtqueue {
 	vring_desc_t __user *desc;
 	vring_avail_t __user *avail;
 	vring_used_t __user *used;
+	/*
+	 * 在以下使用vhost_virtqueue->meta_iotlb[VHOST_NUM_ADDRS]:
+	 *   - drivers/vhost/vhost.c|296| <<__vhost_vq_meta_reset>> vq->meta_iotlb[j] = NULL;
+	 *   - drivers/vhost/vhost.c|788| <<vhost_vq_meta_fetch>> const struct vhost_iotlb_map *map = vq->meta_iotlb[type];
+	 *   - drivers/vhost/vhost.c|1336| <<vhost_vq_meta_update>> vq->meta_iotlb[type] = map;
+	 */
 	const struct vhost_iotlb_map *meta_iotlb[VHOST_NUM_ADDRS];
 	struct file *kick;
 	struct vhost_vring_call call_ctx;
@@ -251,6 +257,22 @@ enum {
  *
  * Context: Need to call with vq->mutex acquired.
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1357| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/net.c|1541| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+ *   - drivers/vhost/net.c|1578| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+ *   - drivers/vhost/scsi.c|1644| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+ *   - drivers/vhost/scsi.c|1746| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|137| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|202| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+ *   - drivers/vhost/test.c|294| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|296| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+ *   - drivers/vhost/vsock.c|554| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+ *   - drivers/vhost/vsock.c|572| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|579| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|602| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+ */
 static inline void vhost_vq_set_backend(struct vhost_virtqueue *vq,
 					void *private_data)
 {
@@ -265,6 +287,9 @@ static inline void vhost_vq_set_backend(struct vhost_virtqueue *vq,
  * Context: Need to call with vq->mutex acquired.
  * Return: Private data previously set with vhost_vq_set_backend.
  */
+/*
+ * 返回struct vhost_scsi
+ */
 static inline void *vhost_vq_get_backend(struct vhost_virtqueue *vq)
 {
 	return vq->private_data;
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index 4b15c00c0a0a..44a2dac412a7 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -141,6 +141,11 @@ void virtio_config_changed(struct virtio_device *dev)
 }
 EXPORT_SYMBOL_GPL(virtio_config_changed);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|286| <<virtio_dev_remove>> virtio_config_disable(dev);
+ *   - drivers/virtio/virtio.c|419| <<virtio_device_freeze>> virtio_config_disable(dev);
+ */
 static void virtio_config_disable(struct virtio_device *dev)
 {
 	spin_lock_irq(&dev->config_lock);
@@ -148,6 +153,11 @@ static void virtio_config_disable(struct virtio_device *dev)
 	spin_unlock_irq(&dev->config_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|272| <<virtio_dev_probe>> virtio_config_enable(dev);
+ *   - drivers/virtio/virtio.c|466| <<virtio_device_restore>> virtio_config_enable(dev);
+ */
 static void virtio_config_enable(struct virtio_device *dev)
 {
 	spin_lock_irq(&dev->config_lock);
@@ -165,6 +175,11 @@ void virtio_add_status(struct virtio_device *dev, unsigned int status)
 }
 EXPORT_SYMBOL_GPL(virtio_add_status);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|267| <<virtio_dev_probe>> err = virtio_finalize_features(dev);
+ *   - drivers/virtio/virtio.c|463| <<virtio_device_restore>> ret = virtio_finalize_features(dev);
+ */
 int virtio_finalize_features(struct virtio_device *dev)
 {
 	int ret = dev->config->finalize_features(dev);
@@ -174,6 +189,9 @@ int virtio_finalize_features(struct virtio_device *dev)
 	if (ret)
 		return ret;
 
+	/*
+	 * 返回sev_active()
+	 */
 	ret = arch_has_restricted_virtio_memory_access();
 	if (ret) {
 		if (!virtio_has_feature(dev, VIRTIO_F_VERSION_1)) {
@@ -203,10 +221,17 @@ int virtio_finalize_features(struct virtio_device *dev)
 }
 EXPORT_SYMBOL_GPL(virtio_finalize_features);
 
+/*
+ * struct bus_type virtio_bus.probe = virtio_dev_probe()
+ */
 static int virtio_dev_probe(struct device *_d)
 {
 	int err, i;
 	struct virtio_device *dev = dev_to_virtio(_d);
+	/*
+	 * virtio_driver的例子:
+	 *   - struct virtio_driver virtio_blk
+	 */
 	struct virtio_driver *drv = drv_to_virtio(dev->dev.driver);
 	u64 device_features;
 	u64 driver_features;
@@ -218,6 +243,22 @@ static int virtio_dev_probe(struct device *_d)
 	/* Figure out what features the device supports. */
 	device_features = dev->config->get_features(dev);
 
+	/*
+	 * 984 static unsigned int features_legacy[] = {
+	 * 985         VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,
+	 * 986         VIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,
+	 * 987         VIRTIO_BLK_F_FLUSH, VIRTIO_BLK_F_TOPOLOGY, VIRTIO_BLK_F_CONFIG_WCE,
+	 * 988         VIRTIO_BLK_F_MQ, VIRTIO_BLK_F_DISCARD, VIRTIO_BLK_F_WRITE_ZEROES,
+	 * 989 }
+	 * 990 ;
+	 * 991 static unsigned int features[] = {
+	 * 992         VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_GEOMETRY,
+	 * 993         VIRTIO_BLK_F_RO, VIRTIO_BLK_F_BLK_SIZE,
+	 * 994         VIRTIO_BLK_F_FLUSH, VIRTIO_BLK_F_TOPOLOGY, VIRTIO_BLK_F_CONFIG_WCE,
+	 * 995         VIRTIO_BLK_F_MQ, VIRTIO_BLK_F_DISCARD, VIRTIO_BLK_F_WRITE_ZEROES,
+	 * 996 };
+	 */
+
 	/* Figure out what features the driver supports. */
 	driver_features = 0;
 	for (i = 0; i < drv->feature_table_size; i++) {
@@ -238,12 +279,21 @@ static int virtio_dev_probe(struct device *_d)
 		driver_features_legacy = driver_features;
 	}
 
+	/*
+	 * struct virtio_device *dev:
+	 * -> u64 features;
+	 */
 	if (device_features & (1ULL << VIRTIO_F_VERSION_1))
 		dev->features = driver_features & device_features;
 	else
 		dev->features = driver_features_legacy & device_features;
 
 	/* Transport features always preserved to pass to finalize_features. */
+	/*
+	 * 查看vring_transport_features(), 比如VIRTIO_RING_F_EVENT_IDX
+	 *
+	 * __virtio_set_bit()的核心思想是设置virtio_device->features
+	 */
 	for (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++)
 		if (device_features & (1ULL << i))
 			__virtio_set_bit(dev, i);
@@ -295,6 +345,14 @@ static int virtio_dev_remove(struct device *_d)
 	return 0;
 }
 
+/*
+ * 在以下使用virtio_bus:
+ *   - drivers/virtio/virtio.c|311| <<register_virtio_driver>> driver->driver.bus = &virtio_bus;
+ *   - drivers/virtio/virtio.c|345| <<register_virtio_device>> dev->dev.bus = &virtio_bus;
+ *   - drivers/virtio/virtio.c|385| <<is_virtio_device>> return dev->bus == &virtio_bus;
+ *   - drivers/virtio/virtio.c|463| <<virtio_init>> if (bus_register(&virtio_bus) != 0)
+ *   - drivers/virtio/virtio.c|470| <<virtio_exit>> bus_unregister(&virtio_bus);
+ */
 static struct bus_type virtio_bus = {
 	.name  = "virtio",
 	.match = virtio_dev_match,
@@ -328,10 +386,28 @@ EXPORT_SYMBOL_GPL(unregister_virtio_driver);
  *
  * Returns: 0 on suceess, -error on failure
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1141| <<virtio_uml_probe>> rc = register_virtio_device(&vu_dev->vdev);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1095| <<mlxbf_tmfifo_create_vdev>> ret = register_virtio_device(&tm_vdev->vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|416| <<rproc_add_virtio_dev>> ret = register_virtio_device(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1321| <<virtio_ccw_online>> ret = register_virtio_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|626| <<virtio_mmio_probe>> rc = register_virtio_device(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|552| <<virtio_pci_probe>> rc = register_virtio_device(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|363| <<virtio_vdpa_probe>> ret = register_virtio_device(&vd_dev->vdev);
+ */
 int register_virtio_device(struct virtio_device *dev)
 {
 	int err;
 
+	/*
+	 * 在以下使用virtio_bus:
+	 *   - drivers/virtio/virtio.c|311| <<register_virtio_driver>> driver->driver.bus = &virtio_bus;
+	 *   - drivers/virtio/virtio.c|345| <<register_virtio_device>> dev->dev.bus = &virtio_bus;
+	 *   - drivers/virtio/virtio.c|385| <<is_virtio_device>> return dev->bus == &virtio_bus;
+	 *   - drivers/virtio/virtio.c|463| <<virtio_init>> if (bus_register(&virtio_bus) != 0)
+	 *   - drivers/virtio/virtio.c|470| <<virtio_exit>> bus_unregister(&virtio_bus);
+	 */
 	dev->dev.bus = &virtio_bus;
 	device_initialize(&dev->dev);
 
diff --git a/drivers/virtio/virtio_pci_common.c b/drivers/virtio/virtio_pci_common.c
index 222d630c41fc..b34757a7c21f 100644
--- a/drivers/virtio/virtio_pci_common.c
+++ b/drivers/virtio/virtio_pci_common.c
@@ -79,6 +79,10 @@ static irqreturn_t vp_vring_interrupt(int irq, void *opaque)
  * the callback may notify the host which results in the host attempting to
  * raise an interrupt that we would then mask once we acknowledged the
  * interrupt. */
+/*
+ * 在以下使用vp_interrupt():
+ *   - drivers/virtio/virtio_pci_common.c|364| <<vp_find_vqs_intx>> err = request_irq(vp_dev->pci_dev->irq, vp_interrupt, IRQF_SHARED,
+ */
 static irqreturn_t vp_interrupt(int irq, void *opaque)
 {
 	struct virtio_pci_device *vp_dev = opaque;
diff --git a/drivers/virtio/virtio_pci_modern.c b/drivers/virtio/virtio_pci_modern.c
index 30654d3a0b41..a4e9b7834a2b 100644
--- a/drivers/virtio/virtio_pci_modern.c
+++ b/drivers/virtio/virtio_pci_modern.c
@@ -26,6 +26,10 @@ static u64 vp_get_features(struct virtio_device *vdev)
 	return vp_modern_get_features(&vp_dev->mdev);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_pci_modern.c|49| <<vp_finalize_features>> vp_transport_features(vdev, features);
+ */
 static void vp_transport_features(struct virtio_device *vdev, u64 features)
 {
 	struct virtio_pci_device *vp_dev = to_vp_device(vdev);
diff --git a/drivers/virtio/virtio_pci_modern_dev.c b/drivers/virtio/virtio_pci_modern_dev.c
index 54f297028586..889936703e7d 100644
--- a/drivers/virtio/virtio_pci_modern_dev.c
+++ b/drivers/virtio/virtio_pci_modern_dev.c
@@ -388,6 +388,11 @@ EXPORT_SYMBOL_GPL(vp_modern_get_features);
  * @mdev: the modern virtio-pci device
  * @features: the features set to device
  */
+/*
+ * called by:
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|67| <<vp_vdpa_set_features>> vp_modern_set_features(mdev, features);
+ *   - drivers/virtio/virtio_pci_modern.c|57| <<vp_finalize_features>> vp_modern_set_features(&vp_dev->mdev, vdev->features);
+ */
 void vp_modern_set_features(struct virtio_pci_modern_device *mdev,
 			    u64 features)
 {
@@ -436,6 +441,18 @@ EXPORT_SYMBOL_GPL(vp_modern_get_status);
 void vp_modern_set_status(struct virtio_pci_modern_device *mdev,
 				 u8 status)
 {
+	/*
+	 * 在以下设置common:
+	 *   - drivers/virtio/virtio_pci_modern_dev.c|234| <<vp_modern_probe>> common = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_COMMON_CFG,
+	 *
+	 * 233         // check for a common config: if not, use legacy mode (bar 0).
+	 * 234         common = virtio_pci_find_capability(pci_dev, VIRTIO_PCI_CAP_COMMON_CFG,
+	 * 235                                             IORESOURCE_IO | IORESOURCE_MEM,
+	 * 236                                             &mdev->modern_bars);
+	 *
+	 * struct virtio_pci_modern_device *mdev:
+	 * -> struct virtio_pci_common_cfg __iomem *common;
+	 */
 	struct virtio_pci_common_cfg __iomem *cfg = mdev->common;
 
 	vp_iowrite8(status, &cfg->device_status);
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 71e16b53e9c1..0cd87eb9ca6b 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -389,6 +389,10 @@ static void vring_unmap_one_split(const struct vring_virtqueue *vq,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|448| <<virtqueue_add_split>> desc = alloc_indirect_split(_vq, total_sg, gfp);
+ */
 static struct vring_desc *alloc_indirect_split(struct virtqueue *_vq,
 					       unsigned int total_sg,
 					       gfp_t gfp)
@@ -1690,6 +1694,13 @@ static struct virtqueue *vring_create_virtqueue_packed(
  * Generic functions and exported symbols.
  */
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_ring.c|1744| <<virtqueue_add_sgs>> return virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs,
+ *   - drivers/virtio/virtio_ring.c|1767| <<virtqueue_add_outbuf>> return virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|1789| <<virtqueue_add_inbuf>> return virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);
+ *   - drivers/virtio/virtio_ring.c|1813| <<virtqueue_add_inbuf_ctx>> return virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);
+ */
 static inline int virtqueue_add(struct virtqueue *_vq,
 				struct scatterlist *sgs[],
 				unsigned int total_sg,
@@ -1721,6 +1732,30 @@ static inline int virtqueue_add(struct virtqueue *_vq,
  *
  * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|111| <<virtblk_add_req>> return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|166| <<virtio_crypto_alg_skcipher_init_session>> err = virtqueue_add_sgs(vcrypto->ctrl_vq, sgs, num_out,
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|240| <<virtio_crypto_alg_skcipher_close_session>> err = virtqueue_add_sgs(vcrypto->ctrl_vq, sgs, num_out,
+ *   - drivers/crypto/virtio/virtio_crypto_algs.c|465| <<__virtio_crypto_skcipher_do_req>> err = virtqueue_add_sgs(data_vq->vq, sgs, num_out,
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|366| <<virtio_gpu_queue_ctrl_sgs>> ret = virtqueue_add_sgs(vq, sgs, outcnt, incnt, vbuf, GFP_ATOMIC);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|474| <<virtio_gpu_queue_cursor>> ret = virtqueue_add_sgs(vq, sgs, outcnt, 0, vbuf, GFP_ATOMIC);
+ *   - drivers/iommu/virtio-iommu.c|247| <<__viommu_add_req>> ret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);
+ *   - drivers/iommu/virtio-iommu.c|251| <<__viommu_add_req>> ret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);
+ *   - drivers/net/virtio_net.c|1777| <<virtnet_send_command>> virtqueue_add_sgs(vi->cvq, sgs, out_num, 1, vi, GFP_ATOMIC);
+ *   - drivers/nvdimm/nd_virtio.c|69| <<virtio_pmem_flush>> while ((err = virtqueue_add_sgs(vpmem->req_vq, sgs, 1, 1, req_data,
+ *   - drivers/scsi/virtio_scsi.c|492| <<__virtscsi_add_cmd>> return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
+ *   - drivers/virtio/virtio_mem.c|1273| <<virtio_mem_send_request>> rc = virtqueue_add_sgs(vm->vq, sgs, 1, 1, vm, GFP_KERNEL);
+ *   - fs/fuse/virtio_fs.c|1188| <<virtio_fs_enqueue_req>> ret = virtqueue_add_sgs(vq, sgs, out_sgs, in_sgs, req, GFP_ATOMIC);
+ *   - net/9p/trans_virtio.c|282| <<p9_virtio_request>> err = virtqueue_add_sgs(chan->vq, sgs, out_sgs, in_sgs, req,
+ *   - net/9p/trans_virtio.c|480| <<p9_virtio_zc_request>> err = virtqueue_add_sgs(chan->vq, sgs, out_sgs, in_sgs, req,
+ *   - net/vmw_vsock/virtio_transport.c|129| <<virtio_transport_send_pkt_work>> ret = virtqueue_add_sgs(vq, sgs, out_sg, in_sg, pkt, GFP_KERNEL);
+ *   - net/vmw_vsock/virtio_transport.c|274| <<virtio_vsock_rx_fill>> ret = virtqueue_add_sgs(vq, sgs, 0, 2, pkt, GFP_KERNEL);
+ *   - sound/virtio/virtio_card.c|41| <<virtsnd_event_send>> if (virtqueue_add_sgs(vqueue, psgs, 0, 1, event, gfp) || !notify)
+ *   - sound/virtio/virtio_ctl_msg.c|151| <<virtsnd_ctl_msg_send>> rc = virtqueue_add_sgs(queue->vqueue, psgs, nouts, nins, msg,
+ *   - sound/virtio/virtio_pcm_msg.c|228| <<virtsnd_pcm_msg_send>> rc = virtqueue_add_sgs(vqueue, psgs, 2, 1, msg,
+ *   - sound/virtio/virtio_pcm_msg.c|231| <<virtsnd_pcm_msg_send>> rc = virtqueue_add_sgs(vqueue, psgs, 1, 2, msg,
+ */
 int virtqueue_add_sgs(struct virtqueue *_vq,
 		      struct scatterlist *sgs[],
 		      unsigned int out_sgs,
@@ -2216,6 +2251,16 @@ void vring_del_virtqueue(struct virtqueue *_vq)
 EXPORT_SYMBOL_GPL(vring_del_virtqueue);
 
 /* Manipulates transport-specific feature bits. */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1059| <<vu_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|234| <<rproc_virtio_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|837| <<virtio_ccw_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_mmio.c|127| <<vm_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_legacy.c|35| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_pci_modern.c|46| <<vp_finalize_features>> vring_transport_features(vdev);
+ *   - drivers/virtio/virtio_vdpa.c|304| <<virtio_vdpa_finalize_features>> vring_transport_features(vdev);
+ */
 void vring_transport_features(struct virtio_device *vdev)
 {
 	unsigned int i;
diff --git a/drivers/virtio/virtio_vdpa.c b/drivers/virtio/virtio_vdpa.c
index e28acf482e0c..a1102fe8df6c 100644
--- a/drivers/virtio/virtio_vdpa.c
+++ b/drivers/virtio/virtio_vdpa.c
@@ -123,6 +123,10 @@ static irqreturn_t virtio_vdpa_config_cb(void *private)
 	return IRQ_HANDLED;
 }
 
+/*
+ * 在以下使用virtio_vdpa_virtqueue_cb():
+ *   - drivers/virtio/virtio_vdpa.c|178| <<virtio_vdpa_setup_vq>> cb.callback = virtio_vdpa_virtqueue_cb;
+ */
 static irqreturn_t virtio_vdpa_virtqueue_cb(void *private)
 {
 	struct virtio_vdpa_vq_info *info = private;
@@ -130,6 +134,10 @@ static irqreturn_t virtio_vdpa_virtqueue_cb(void *private)
 	return vring_interrupt(0, info->vq);
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_vdpa.c|263| <<virtio_vdpa_find_vqs>> vqs[i] = virtio_vdpa_setup_vq(vdev, queue_idx++,
+ */
 static struct virtqueue *
 virtio_vdpa_setup_vq(struct virtio_device *vdev, unsigned int index,
 		     void (*callback)(struct virtqueue *vq),
@@ -347,6 +355,14 @@ static int virtio_vdpa_probe(struct vdpa_device *vdpa)
 	INIT_LIST_HEAD(&vd_dev->virtqueues);
 	spin_lock_init(&vd_dev->lock);
 
+	/*
+	 * struct virtio_vdpa_device *vd_dev:
+	 * -> struct device dev;
+	 * -> struct virtio_device vdev;
+	 *    -> struct virtio_device_id id;
+	 *       -> __u32 device;
+	 *       -> __u32 vendor;
+	 */
 	vd_dev->vdev.id.device = ops->get_device_id(vdpa);
 	if (vd_dev->vdev.id.device == 0)
 		goto err;
@@ -357,6 +373,12 @@ static int virtio_vdpa_probe(struct vdpa_device *vdpa)
 	if (ret)
 		goto err;
 
+	/*
+	 * struct vdpa_device *vdpa:
+	 * -> struct device dev;
+	 *    -> void *driver_data;
+	 * -> struct vdpa_mgmt_dev *mdev;
+	 */
 	vdpa_set_drvdata(vdpa, vd_dev);
 
 	return 0;
diff --git a/drivers/xen/xenbus/xenbus_client.c b/drivers/xen/xenbus/xenbus_client.c
index 0cd728961fce..989c4a9cf424 100644
--- a/drivers/xen/xenbus/xenbus_client.c
+++ b/drivers/xen/xenbus/xenbus_client.c
@@ -270,6 +270,23 @@ int xenbus_switch_state(struct xenbus_device *dev, enum xenbus_state state)
 
 EXPORT_SYMBOL_GPL(xenbus_switch_state);
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|2239| <<blkfront_closing>> xenbus_frontend_closed(xbdev);
+ *   - drivers/block/xen-blkfront.c|2269| <<blkfront_closing>> xenbus_frontend_closed(xbdev);
+ *   - drivers/block/xen-blkfront.c|2733| <<blkif_release>> xenbus_frontend_closed(info->xbdev);
+ *   - drivers/char/tpm/xen-tpmfront.c|413| <<backend_changed>> xenbus_frontend_closed(dev);
+ *   - drivers/gpu/drm/xen/xen_drm_front.c|762| <<xen_drv_remove>> xenbus_frontend_closed(dev);
+ *   - drivers/input/misc/xen-kbdfront.c|529| <<xenkbd_backend_changed>> xenbus_frontend_closed(dev);
+ *   - drivers/net/xen-netfront.c|2363| <<netback_changed>> xenbus_frontend_closed(dev);
+ *   - drivers/scsi/xen-scsifront.c|965| <<scsifront_disconnect>> xenbus_frontend_closed(dev);
+ *   - drivers/tty/hvc/hvc_xen.c|497| <<xencons_backend_changed>> xenbus_frontend_closed(dev);
+ *   - drivers/video/fbdev/xen-fbfront.c|682| <<xenfb_backend_changed>> xenbus_frontend_closed(dev);
+ *   - drivers/xen/pvcalls-front.c|1268| <<pvcalls_front_changed>> xenbus_frontend_closed(dev);
+ *   - drivers/xen/xenbus/xenbus_probe.c|200| <<xenbus_otherend_changed>> xenbus_frontend_closed(dev);
+ *   - net/9p/trans_xen.c|525| <<xen_9pfs_front_changed>> xenbus_frontend_closed(dev);
+ *   - sound/xen/xen_snd_front.c|347| <<xen_drv_remove>> xenbus_frontend_closed(dev);
+ */
 int xenbus_frontend_closed(struct xenbus_device *dev)
 {
 	xenbus_switch_state(dev, XenbusStateClosed);
@@ -350,6 +367,12 @@ EXPORT_SYMBOL_GPL(xenbus_dev_fatal);
  * Equivalent to xenbus_dev_fatal(dev, err, fmt, args), but helps
  * avoiding recursion within xenbus_switch_state.
  */
+/*
+ * called by:
+ *   - drivers/xen/xenbus/xenbus_client.c|230| <<__xenbus_switch_state>> xenbus_switch_fatal(dev, depth, err, "starting transaction");
+ *   - drivers/xen/xenbus/xenbus_client.c|240| <<__xenbus_switch_state>> xenbus_switch_fatal(dev, depth, err, "writing new state");
+ *   - drivers/xen/xenbus/xenbus_client.c|250| <<__xenbus_switch_state>> xenbus_switch_fatal(dev, depth, err, "ending transaction");
+ */
 static void xenbus_switch_fatal(struct xenbus_device *dev, int depth, int err,
 				const char *fmt, ...)
 {
diff --git a/drivers/xen/xenbus/xenbus_probe_frontend.c b/drivers/xen/xenbus/xenbus_probe_frontend.c
index 480944606a3c..123d416eca7a 100644
--- a/drivers/xen/xenbus/xenbus_probe_frontend.c
+++ b/drivers/xen/xenbus/xenbus_probe_frontend.c
@@ -306,6 +306,17 @@ static bool wait_loop(unsigned long start, unsigned int max_delay,
  * boot slightly, but of course needs tools or manual intervention to set up
  * those flags correctly.
  */
+/*
+ * [0] CPU: 6 PID: 153 Comm: systemd-udevd Not tainted 5.13.0xen #4
+ * [0] wait_for_devices
+ * [0] __xenbus_register_frontend
+ * [0] do_one_initcall
+ * [0] do_init_module
+ * [0] load_module
+ * [0] __do_sys_finit_module
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void wait_for_devices(struct xenbus_driver *xendrv)
 {
 	unsigned long start = jiffies;
@@ -331,6 +342,10 @@ static void wait_for_devices(struct xenbus_driver *xendrv)
 			 print_device_status);
 }
 
+/*
+ * called by:
+ *   - include/xen/xenbus.h|142| <<xenbus_register_frontend>> __xenbus_register_frontend(drv, THIS_MODULE, KBUILD_MODNAME)
+ */
 int __xenbus_register_frontend(struct xenbus_driver *drv, struct module *owner,
 			       const char *mod_name)
 {
diff --git a/fs/configfs/configfs_internal.h b/fs/configfs/configfs_internal.h
index c0395363eab9..8a0e92d83854 100644
--- a/fs/configfs/configfs_internal.h
+++ b/fs/configfs/configfs_internal.h
@@ -28,6 +28,14 @@ void put_fragment(struct configfs_fragment *);
 struct configfs_fragment *get_fragment(struct configfs_fragment *);
 
 struct configfs_dirent {
+	/*
+	 * 在以下使用configfs_dirent->s_count:
+	 *   - fs/configfs/configfs_internal.h|147| <<configfs_get>> WARN_ON(!atomic_read(&sd->s_count));
+	 *   - fs/configfs/configfs_internal.h|148| <<configfs_get>> atomic_inc(&sd->s_count);
+	 *   - fs/configfs/configfs_internal.h|155| <<configfs_put>> WARN_ON(!atomic_read(&sd->s_count));
+	 *   - fs/configfs/configfs_internal.h|156| <<configfs_put>> if (atomic_dec_and_test(&sd->s_count))
+	 *   - fs/configfs/dir.c|190| <<configfs_new_dirent>> atomic_set(&sd->s_count, 1);
+	 */
 	atomic_t		s_count;
 	int			s_dependent_count;
 	struct list_head	s_sibling;
diff --git a/fs/eventfd.c b/fs/eventfd.c
index e265b6dd4f34..67c6d1eae432 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -59,6 +59,33 @@ struct eventfd_ctx {
  * Returns the amount by which the counter was incremented.  This will be less
  * than @n if the counter has overflowed.
  */
+/*
+ * irqfd_wakeup
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * vhost_tx_batch.isra.23
+ * handle_tx_copy
+ * handle_tx
+ * handle_tx_kick
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * irqfd_wakeup
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * handle_rx
+ * handle_rx_net
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ */
 __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 {
 	unsigned long flags;
@@ -79,6 +106,12 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 	if (ULLONG_MAX - ctx->count < n)
 		n = ULLONG_MAX - ctx->count;
 	ctx->count += n;
+	/*
+	 * 唤醒irqfd_wakeup()
+	 *
+	 * eventfd_ctx:
+	 * -> wait_queue_head_t wqh;
+	 */
 	if (waitqueue_active(&ctx->wqh))
 		wake_up_locked_poll(&ctx->wqh, EPOLLIN);
 	this_cpu_dec(eventfd_wake_count);
diff --git a/include/kvm/iodev.h b/include/kvm/iodev.h
index d75fc4365746..fcda7950ece2 100644
--- a/include/kvm/iodev.h
+++ b/include/kvm/iodev.h
@@ -47,6 +47,12 @@ static inline int kvm_iodevice_read(struct kvm_vcpu *vcpu,
 				: -EOPNOTSUPP;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7812| <<vcpu_mmio_write>> !kvm_iodevice_write(vcpu, &vcpu->arch.apic->dev, addr, n, v))
+ *   - virt/kvm/kvm_main.c|5181| <<__kvm_io_bus_write>> if (!kvm_iodevice_write(vcpu, bus->range[idx].dev, range->addr,
+ *   - virt/kvm/kvm_main.c|5230| <<kvm_io_bus_write_cookie>> if (!kvm_iodevice_write(vcpu, bus->range[cookie].dev, addr, len,
+ */
 static inline int kvm_iodevice_write(struct kvm_vcpu *vcpu,
 				     struct kvm_io_device *dev, gpa_t addr,
 				     int l, const void *v)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index db026b6ec15a..e1485b6c0e3b 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -26,6 +26,24 @@ struct block_device {
 	unsigned long		bd_stamp;
 	bool			bd_read_only;	/* read-only policy */
 	dev_t			bd_dev;
+	/*
+	 * 在以下设置block_device->bd_openers:
+	 *   - fs/block_dev.c|1363| <<__blkdev_get>> bdev->bd_openers++;
+	 *   - fs/block_dev.c|1573| <<__blkdev_put>> if (!--bdev->bd_openers) {
+	 * 在以下使用block_device->bd_openers:
+	 *   - block/partitions/core.c|478| <<bdev_del_partition>> if (part->bd_openers)
+	 *   - drivers/block/nbd.c|1141| <<nbd_bdev_reset>> if (bdev->bd_openers > 1)
+	 *   - drivers/block/nbd.c|1503| <<nbd_release>> disk->part0->bd_openers == 0)
+	 *   - drivers/block/xen-blkfront.c|2224| <<blkfront_closing>> if (bdev->bd_openers) {
+	 *   - drivers/block/xen-blkfront.c|2622| <<blkfront_remove>> xbdev->nodename, bdev->bd_openers);
+	 *   - drivers/block/xen-blkfront.c|2624| <<blkfront_remove>> if (info && !bdev->bd_openers) {
+	 *   - drivers/block/xen-blkfront.c|2679| <<blkif_release>> if (disk->part0->bd_openers)
+	 *   - drivers/block/zram/zram_drv.c|1786| <<reset_store>> if (bdev->bd_openers || zram->claim) {
+	 *   - drivers/block/zram/zram_drv.c|1986| <<zram_remove>> if (bdev->bd_openers || zram->claim) {
+	 *   - fs/block_dev.c|1307| <<__blkdev_get>> if (!bdev->bd_openers) {
+	 *   - fs/block_dev.c|1566| <<__blkdev_put>> if (bdev->bd_openers == 1)
+	 *   - fs/block_dev.c|1944| <<iterate_bdevs>> if (bdev->bd_openers)
+	 */
 	int			bd_openers;
 	struct inode *		bd_inode;	/* will die */
 	struct super_block *	bd_super;
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index 03faf20a6817..0a0c23d021f8 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -82,6 +82,11 @@
 #define DMAR_IQA_REG	0x90	/* Invalidation queue addr register */
 #define DMAR_ICS_REG	0x9c	/* Invalidation complete status register */
 #define DMAR_IQER_REG	0xb0	/* Invalidation queue error record register */
+/*
+ * 在以下使用DMAR_IRTA_REG:
+ *   - drivers/iommu/intel/irq_remapping.c|440| <<iommu_load_old_irte>> irta = dmar_readq(iommu->reg + DMAR_IRTA_REG);
+ *   - drivers/iommu/intel/irq_remapping.c|483| <<iommu_set_irq_remapping>> dmar_writeq(iommu->reg + DMAR_IRTA_REG,
+ */
 #define DMAR_IRTA_REG	0xb8    /* Interrupt remapping table addr register */
 #define DMAR_PQH_REG	0xc0	/* Page request queue head register */
 #define DMAR_PQT_REG	0xc8	/* Page request queue tail register */
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8583ed3ff344..076e793f2a9b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -236,6 +236,11 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range);
 enum {
 	OUTSIDE_GUEST_MODE,
 	IN_GUEST_MODE,
+	/*
+	 * 在以下使用EXITING_GUEST_MODE:
+	 *   - arch/x86/kvm/x86.c|1939| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+	 *   - include/linux/kvm_host.h|400| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 */
 	EXITING_GUEST_MODE,
 	READING_SHADOW_PAGE_TABLES,
 };
@@ -268,6 +273,11 @@ static inline bool kvm_vcpu_mapped(struct kvm_host_map *map)
 
 static inline bool kvm_vcpu_can_poll(ktime_t cur, ktime_t stop)
 {
+	/*
+	 * single_task_running():
+	 * 返回(raw_rq()->nr_running == 1)
+	 * Check if only the current task is running on the CPU.
+	 */
 	return single_task_running() && !need_resched() && ktime_before(cur, stop);
 }
 
@@ -284,6 +294,12 @@ struct kvm_mmio_fragment {
 struct kvm_vcpu {
 	struct kvm *kvm;
 #ifdef CONFIG_PREEMPT_NOTIFIERS
+	/*
+	 * 在以下使用kvm_vcpu->preempt_notifier:
+	 *   - virt/kvm/kvm_main.c|313| <<vcpu_load>> preempt_notifier_register(&vcpu->preempt_notifier);
+	 *   - virt/kvm/kvm_main.c|333| <<vcpu_put>> preempt_notifier_unregister(&vcpu->preempt_notifier);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_vcpu_init>> preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
+	 */
 	struct preempt_notifier preempt_notifier;
 #endif
 	int cpu;
@@ -305,7 +321,26 @@ struct kvm_vcpu {
 	int sigset_active;
 	sigset_t sigset;
 	struct kvm_vcpu_stat stat;
+	/*
+	 * 在以下使用kvm_vcpu->halt_poll_ns:
+	 *   - virt/kvm/kvm_main.c|3003| <<grow_halt_poll_ns>> old = val = vcpu->halt_poll_ns;
+	 *   - virt/kvm/kvm_main.c|3016| <<grow_halt_poll_ns>> vcpu->halt_poll_ns = val;
+	 *   - virt/kvm/kvm_main.c|3025| <<shrink_halt_poll_ns>> old = val = vcpu->halt_poll_ns;
+	 *   - virt/kvm/kvm_main.c|3032| <<shrink_halt_poll_ns>> vcpu->halt_poll_ns = val;
+	 *   - virt/kvm/kvm_main.c|3079| <<kvm_vcpu_block>> if (vcpu->halt_poll_ns && !kvm_arch_no_poll(vcpu)) {
+	 *   - virt/kvm/kvm_main.c|3080| <<kvm_vcpu_block>> ktime_t stop = ktime_add_ns(ktime_get(), vcpu->halt_poll_ns);
+	 *   - virt/kvm/kvm_main.c|3133| <<kvm_vcpu_block>> if (block_ns <= vcpu->halt_poll_ns)
+	 *   - virt/kvm/kvm_main.c|3136| <<kvm_vcpu_block>> else if (vcpu->halt_poll_ns &&
+	 *   - virt/kvm/kvm_main.c|3140| <<kvm_vcpu_block>> else if (vcpu->halt_poll_ns < vcpu->kvm->max_halt_poll_ns &&
+	 *   - virt/kvm/kvm_main.c|3144| <<kvm_vcpu_block>> vcpu->halt_poll_ns = 0;
+	 */
 	unsigned int halt_poll_ns;
+	/*
+	 * 在以下使用kvm_vcpu->valid_wakeup:
+	 *   - arch/s390/kvm/interrupt.c|1348| <<kvm_s390_vcpu_wakeup>> vcpu->valid_wakeup = true;
+	 *   - arch/s390/kvm/kvm-s390.c|5051| <<kvm_arch_vcpu_block_finish>> vcpu->valid_wakeup = false;
+	 *   - include/linux/kvm_host.h|1631| <<vcpu_valid_wakeup>> return vcpu->valid_wakeup;
+	 */
 	bool valid_wakeup;
 
 #ifdef CONFIG_HAS_IOMEM
@@ -397,6 +432,15 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 	 * memory barrier following the write of vcpu->mode in VCPU RUN.
 	 */
 	smp_mb__before_atomic();
+	/*
+	 * 在以下使用EXITING_GUEST_MODE:
+	 *   - arch/x86/kvm/x86.c|1939| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+	 *   - include/linux/kvm_host.h|400| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 *
+	 * cmpxchg(void* ptr, int old, int new)
+	 * 如果ptr和old的值一样,则把new写到ptr内存,
+	 * 否则返回ptr的值,整个操作是原子的.
+	 */
 	return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
 }
 
@@ -534,7 +578,28 @@ struct kvm {
 	 * and is accessed atomically.
 	 */
 	atomic_t online_vcpus;
+	/*
+	 * x86在以下设置kvm->created_vcpus:
+	 *   - virt/kvm/kvm_main.c|3398| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus++;
+	 *   - virt/kvm/kvm_main.c|3474| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus--;
+	 * x86在以下使用kvm->created_vcpus:
+	 *   - arch/x86/kvm/svm/sev.c|236| <<sev_guest_init>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|6743| <<kvm_vm_ioctl_enable_cap>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|6973| <<kvm_arch_vm_ioctl>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|6997| <<kvm_arch_vm_ioctl>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|7154| <<kvm_arch_vm_ioctl>> if (kvm->created_vcpus)
+	 *   - virt/kvm/kvm_main.c|3393| <<kvm_vm_ioctl_create_vcpu>> if (kvm->created_vcpus == KVM_MAX_VCPUS) {
+	 *   - virt/kvm/kvm_main.c|3977| <<kvm_vm_ioctl_enable_dirty_log_ring>> if (kvm->created_vcpus) {
+	 */
 	int created_vcpus;
+	/*
+	 * 在以下使用kvm->last_boosted_vcpu:
+	 *   - virt/kvm/kvm_main.c|3228| <<kvm_vcpu_on_spin>> int last_boosted_vcpu = me->kvm->last_boosted_vcpu;
+	 *   - virt/kvm/kvm_main.c|3244| <<kvm_vcpu_on_spin>> if (!pass && i <= last_boosted_vcpu) {
+	 *   - virt/kvm/kvm_main.c|3245| <<kvm_vcpu_on_spin>> i = last_boosted_vcpu;
+	 *   - virt/kvm/kvm_main.c|3247| <<kvm_vcpu_on_spin>> } else if (pass && i > last_boosted_vcpu)
+	 *   - virt/kvm/kvm_main.c|3265| <<kvm_vcpu_on_spin>> kvm->last_boosted_vcpu = i;
+	 */
 	int last_boosted_vcpu;
 	struct list_head vm_list;
 	struct mutex lock;
@@ -562,6 +627,18 @@ struct kvm {
 	/*
 	 * Update side is protected by irq_lock.
 	 */
+	/*
+	 * 在以下使用kvm->irq_routing:
+	 *   - arch/x86/kvm/hyperv.c|492| <<kvm_hv_irq_routing_update>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+	 *   - arch/x86/kvm/irq_comm.c|403| <<kvm_scan_ioapic_routes>> table = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/svm/avic.c|827| <<svm_update_pi_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - arch/x86/kvm/vmx/posted_intr.c|293| <<pi_update_irte>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|29| <<kvm_irq_map_gsi>> irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
+	 *   - virt/kvm/irqchip.c|45| <<kvm_irq_map_chip_pin>> irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
+	 *   - virt/kvm/irqchip.c|124| <<kvm_free_irq_routing>> struct kvm_irq_routing_table *rt = rcu_access_pointer(kvm->irq_routing);
+	 *   - virt/kvm/irqchip.c|229| <<kvm_set_irq_routing>> old = rcu_dereference_protected(kvm->irq_routing, 1);
+	 *   - virt/kvm/irqchip.c|230| <<kvm_set_irq_routing>> rcu_assign_pointer(kvm->irq_routing, new);
+	 */
 	struct kvm_irq_routing_table __rcu *irq_routing;
 #endif
 #ifdef CONFIG_HAVE_KVM_IRQFD
@@ -579,10 +656,31 @@ struct kvm {
 	struct list_head devices;
 	u64 manual_dirty_log_protect;
 	struct dentry *debugfs_dentry;
+	/*
+	 * 在以下使用kvm->debugfs_stat_data:
+	 *   - virt/kvm/kvm_main.c|839| <<kvm_destroy_vm_debugfs>> if (kvm->debugfs_stat_data) {
+	 *   - virt/kvm/kvm_main.c|841| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data[i]);
+	 *   - virt/kvm/kvm_main.c|842| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data);
+	 *   - virt/kvm/kvm_main.c|858| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+	 *   - virt/kvm/kvm_main.c|859| <<kvm_create_vm_debugfs>> sizeof(*kvm->debugfs_stat_data),
+	 *   - virt/kvm/kvm_main.c|861| <<kvm_create_vm_debugfs>> if (!kvm->debugfs_stat_data)
+	 *   - virt/kvm/kvm_main.c|871| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data[p - debugfs_entries] = stat_data;
+	 */
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
 	pid_t userspace_pid;
+	/*
+	 * 在以下使用kvm->max_halt_poll_ns:
+	 *   - virt/kvm/kvm_main.c|1032| <<kvm_create_vm>> kvm->max_halt_poll_ns = halt_poll_ns;
+	 *   - virt/kvm/kvm_main.c|3013| <<grow_halt_poll_ns>> if (val > vcpu->kvm->max_halt_poll_ns)
+	 *   - virt/kvm/kvm_main.c|3014| <<grow_halt_poll_ns>> val = vcpu->kvm->max_halt_poll_ns;
+	 *   - virt/kvm/kvm_main.c|3160| <<kvm_vcpu_block>> } else if (vcpu->kvm->max_halt_poll_ns) {
+	 *   - virt/kvm/kvm_main.c|3165| <<kvm_vcpu_block>> block_ns > vcpu->kvm->max_halt_poll_ns)
+	 *   - virt/kvm/kvm_main.c|3168| <<kvm_vcpu_block>> else if (vcpu->halt_poll_ns < vcpu->kvm->max_halt_poll_ns &&
+	 *   - virt/kvm/kvm_main.c|3169| <<kvm_vcpu_block>> block_ns < vcpu->kvm->max_halt_poll_ns)
+	 *   - virt/kvm/kvm_main.c|4170| <<kvm_vm_ioctl_enable_cap_generic>> kvm->max_halt_poll_ns = cap->args[0];
+	 */
 	unsigned int max_halt_poll_ns;
 	u32 dirty_ring_size;
 };
@@ -1176,6 +1274,24 @@ search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1256| <<resize_hpt_rehash_hpte>> __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|120| <<kvmppc_set_dirty_from_hpte>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|136| <<revmap_for_hpte>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|221| <<kvmppc_do_h_enter>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|881| <<kvmppc_get_hpa>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|758| <<account_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|788| <<unaccount_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|946| <<gfn_to_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|139| <<inspect_spte_has_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|202| <<audit_write_protection>> slot = __gfn_to_memslot(slots, sp->gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<handle_changed_spte_dirty_log>> slot = __gfn_to_memslot(__kvm_memslots(kvm, as_id), gfn);
+ *   - virt/kvm/kvm_main.c|1843| <<gfn_to_memslot>> return __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - virt/kvm/kvm_main.c|1849| <<kvm_vcpu_gfn_to_memslot>> return __gfn_to_memslot(kvm_vcpu_memslots(vcpu), gfn);
+ *   - virt/kvm/kvm_main.c|2336| <<__kvm_map_gfn>> struct kvm_memory_slot *slot = __gfn_to_memslot(slots, gfn);
+ *   - virt/kvm/kvm_main.c|2721| <<__kvm_gfn_to_hva_cache_init>> ghc->memslot = __gfn_to_memslot(slots, start_gfn);
+ */
 static inline struct kvm_memory_slot *
 __gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
 {
@@ -1257,8 +1373,14 @@ struct kvm_stats_debugfs_item {
 #define KVM_DBGFS_GET_MODE(dbgfs_item)                                         \
 	((dbgfs_item)->mode ? (dbgfs_item)->mode : 0644)
 
+/*
+ * 这里的base是kvm, 不是kvm_stat
+ */
 #define VM_STAT(n, x, ...) 							\
 	{ n, offsetof(struct kvm, stat.x), KVM_STAT_VM, ## __VA_ARGS__ }
+/*
+ * 这里的base是kvm_vcpu, 不是kvm_vcpu_state
+ */
 #define VCPU_STAT(n, x, ...)							\
 	{ n, offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU, ## __VA_ARGS__ }
 
@@ -1383,6 +1505,18 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	set_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|669| <<check_vcpu_requests>> if (kvm_request_pending(vcpu)) {
+ *   - arch/arm64/kvm/arm.c|790| <<kvm_arch_vcpu_ioctl_run>> kvm_request_pending(vcpu)) {
+ *   - arch/mips/kvm/vz.c|2434| <<kvm_vz_check_requests>> if (!kvm_request_pending(vcpu))
+ *   - arch/powerpc/kvm/booke.c|689| <<kvmppc_core_prepare_to_enter>> if (kvm_request_pending(vcpu)) {
+ *   - arch/powerpc/kvm/powerpc.c|51| <<kvm_arch_vcpu_runnable>> return !!(v->arch.pending_exceptions) || kvm_request_pending(v);
+ *   - arch/powerpc/kvm/powerpc.c|113| <<kvmppc_prepare_to_enter>> if (kvm_request_pending(vcpu)) {
+ *   - arch/s390/kvm/kvm-s390.c|3793| <<kvm_s390_handle_requests>> if (!kvm_request_pending(vcpu))
+ *   - arch/x86/kvm/x86.c|1944| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+ *   - arch/x86/kvm/x86.c|10364| <<vcpu_enter_guest>> if (kvm_request_pending(vcpu)) {
+ */
 static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 {
 	return READ_ONCE(vcpu->requests);
@@ -1390,6 +1524,10 @@ static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> u64 requests;
+	 */
 	return test_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 5cbc950b34df..f52689bd2145 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -286,7 +286,26 @@ struct header_ops {
  */
 
 enum netdev_state_t {
+	/*
+	 * 在以下使用__LINK_STATE_START:
+	 *   - include/linux/netdevice.h|3739| <<netif_running>> return test_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|1603| <<__dev_open>> set_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|1614| <<__dev_open>> clear_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|1669| <<__dev_close_many>> clear_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|10394| <<init_dummy_netdev>> set_bit(__LINK_STATE_START, &dev->state);
+	 */
 	__LINK_STATE_START,
+	/*
+	 * 在以下使用__LINK_STATE_PRESENT:
+	 *   - drivers/net/ethernet/amd/sun3lance.c|412| <<lance_probe>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - include/linux/netdevice.h|4323| <<netif_device_present>> return test_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - net/8021q/vlan_dev.c|564| <<vlan_dev_init>> (1<<__LINK_STATE_PRESENT);
+	 *   - net/bluetooth/6lowpan.c|732| <<setup_netdev>> set_bit(__LINK_STATE_PRESENT, &netdev->state);
+	 *   - net/core/dev.c|3207| <<netif_device_detach>> if (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&
+	 *   - net/core/dev.c|3222| <<netif_device_attach>> if (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&
+	 *   - net/core/dev.c|10318| <<register_netdevice>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - net/core/dev.c|10393| <<init_dummy_netdev>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 */
 	__LINK_STATE_PRESENT,
 	__LINK_STATE_NOCARRIER,
 	__LINK_STATE_LINKWATCH_PENDING,
@@ -610,6 +629,17 @@ struct netdev_queue {
  * write-mostly part
  */
 	spinlock_t		_xmit_lock ____cacheline_aligned_in_smp;
+	/*
+	 * 在以下使用netdev_queue->xmit_lock_owner:
+	 *   - include/linux/netdevice.h|4387| <<__netif_tx_lock>> txq->xmit_lock_owner = cpu;
+	 *   - include/linux/netdevice.h|4404| <<__netif_tx_lock_bh>> txq->xmit_lock_owner = smp_processor_id();
+	 *   - include/linux/netdevice.h|4411| <<__netif_tx_trylock>> txq->xmit_lock_owner = smp_processor_id();
+	 *   - include/linux/netdevice.h|4417| <<__netif_tx_unlock>> txq->xmit_lock_owner = -1;
+	 *   - include/linux/netdevice.h|4423| <<__netif_tx_unlock_bh>> txq->xmit_lock_owner = -1;
+	 *   - include/linux/netdevice.h|4429| <<txq_trans_update>> if (txq->xmit_lock_owner != -1)
+	 *   - net/core/dev.c|4233| <<__dev_queue_xmit>> if (txq->xmit_lock_owner != cpu) {
+	 *   - net/core/dev.c|10114| <<netdev_init_one_queue>> queue->xmit_lock_owner = -1;
+	 */
 	int			xmit_lock_owner;
 	/*
 	 * Time (in jiffies) of last Tx
@@ -2156,6 +2186,13 @@ struct net_device {
 #ifdef CONFIG_PCPU_DEV_REFCNT
 	int __percpu		*pcpu_refcnt;
 #else
+	/*
+	 * 在以下使用net_device->dev_refcnt:
+	 *   - include/linux/netdevice.h|4142| <<dev_put>> refcount_dec(&dev->dev_refcnt);
+	 *   - include/linux/netdevice.h|4157| <<dev_hold>> refcount_inc(&dev->dev_refcnt);
+	 *   - net/core/dev.c|10420| <<netdev_refcnt_read>> return refcount_read(&dev->dev_refcnt);
+	 *   - net/core/dev.c|10789| <<alloc_netdev_mqs>> refcount_set(&dev->dev_refcnt, 1);
+	 */
 	refcount_t		dev_refcnt;
 #endif
 
@@ -2951,6 +2988,16 @@ int dev_queue_xmit(struct sk_buff *skb);
 int dev_queue_xmit_accel(struct sk_buff *skb, struct net_device *sb_dev);
 int __dev_direct_xmit(struct sk_buff *skb, u16 queue_id);
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c|349| <<__stmmac_test_loopback>> ret = dev_direct_xmit(skb, attr->queue_mapping);
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c|939| <<__stmmac_test_vlanfilt>> ret = dev_direct_xmit(skb, 0);
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c|1033| <<__stmmac_test_dvlanfilt>> ret = dev_direct_xmit(skb, 0);
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c|1303| <<stmmac_test_vlanoff_common>> ret = dev_direct_xmit(skb, 0);
+ *   - drivers/net/ethernet/stmicro/stmmac/stmmac_selftests.c|1665| <<stmmac_test_arpoffload>> ret = dev_direct_xmit(skb, 0);
+ *   - net/core/selftests.c|267| <<__net_test_loopback>> ret = dev_direct_xmit(skb, attr->queue_mapping);
+ *   - net/packet/af_packet.c|244| <<packet_direct_xmit>> return dev_direct_xmit(skb, packet_pick_tx_queue(skb));
+ */
 static inline int dev_direct_xmit(struct sk_buff *skb, u16 queue_id)
 {
 	int ret;
@@ -3708,6 +3755,14 @@ static inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)
  */
 static inline bool netif_running(const struct net_device *dev)
 {
+	/*
+	 * 在以下使用__LINK_STATE_START:
+	 *   - include/linux/netdevice.h|3739| <<netif_running>> return test_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|1603| <<__dev_open>> set_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|1614| <<__dev_open>> clear_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|1669| <<__dev_close_many>> clear_bit(__LINK_STATE_START, &dev->state);
+	 *   - net/core/dev.c|10394| <<init_dummy_netdev>> set_bit(__LINK_STATE_START, &dev->state);
+	 */
 	return test_bit(__LINK_STATE_START, &dev->state);
 }
 
@@ -4377,6 +4432,17 @@ static inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)
 static inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)
 {
 	spin_lock(&txq->_xmit_lock);
+	/*
+	 * 在以下使用netdev_queue->xmit_lock_owner:
+	 *   - include/linux/netdevice.h|4387| <<__netif_tx_lock>> txq->xmit_lock_owner = cpu;
+	 *   - include/linux/netdevice.h|4404| <<__netif_tx_lock_bh>> txq->xmit_lock_owner = smp_processor_id();
+	 *   - include/linux/netdevice.h|4411| <<__netif_tx_trylock>> txq->xmit_lock_owner = smp_processor_id();
+	 *   - include/linux/netdevice.h|4417| <<__netif_tx_unlock>> txq->xmit_lock_owner = -1;
+	 *   - include/linux/netdevice.h|4423| <<__netif_tx_unlock_bh>> txq->xmit_lock_owner = -1;
+	 *   - include/linux/netdevice.h|4429| <<txq_trans_update>> if (txq->xmit_lock_owner != -1)
+	 *   - net/core/dev.c|4233| <<__dev_queue_xmit>> if (txq->xmit_lock_owner != cpu) {
+	 *   - net/core/dev.c|10114| <<netdev_init_one_queue>> queue->xmit_lock_owner = -1;
+	 */
 	txq->xmit_lock_owner = cpu;
 }
 
@@ -4489,6 +4555,16 @@ static inline void netif_tx_unlock_bh(struct net_device *dev)
 	local_bh_enable();
 }
 
+/*
+ * called by:
+ *   - net/core/dev.c|4282| <<__dev_queue_xmit>> HARD_TX_LOCK(dev, txq, cpu);
+ *   - net/core/dev.c|4357| <<__dev_direct_xmit>> HARD_TX_LOCK(dev, txq, smp_processor_id());
+ *   - net/core/dev.c|4895| <<generic_xdp_tx>> HARD_TX_LOCK(dev, txq, cpu);
+ *   - net/core/netpoll.c|124| <<queue_process>> HARD_TX_LOCK(dev, txq, smp_processor_id());
+ *   - net/core/pktgen.c|3404| <<pktgen_xmit>> HARD_TX_LOCK(odev, txq, smp_processor_id());
+ *   - net/sched/sch_generic.c|334| <<sch_direct_xmit>> HARD_TX_LOCK(dev, txq, smp_processor_id());
+ *   - net/xfrm/xfrm_device.c|322| <<xfrm_dev_resume>> HARD_TX_LOCK(dev, txq, smp_processor_id());
+ */
 #define HARD_TX_LOCK(dev, txq, cpu) {			\
 	if ((dev->features & NETIF_F_LLTX) == 0) {	\
 		__netif_tx_lock(txq, cpu);		\
diff --git a/include/linux/timekeeping.h b/include/linux/timekeeping.h
index 78a98bdff76d..b6e384cd9e36 100644
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@ -161,6 +161,12 @@ static inline u64 ktime_get_real_ns(void)
 
 static inline u64 ktime_get_boottime_ns(void)
 {
+	/*
+	 * ktime_get_boottime - Returns monotonic time since boot in ktime_t format
+	 *
+	 * This is similar to CLOCK_MONTONIC/ktime_get, but also includes the
+	 * time spent in suspend.
+	 */
 	return ktime_to_ns(ktime_get_boottime());
 }
 
diff --git a/include/linux/vdpa.h b/include/linux/vdpa.h
index f311d227aa1b..85faa931fd1e 100644
--- a/include/linux/vdpa.h
+++ b/include/linux/vdpa.h
@@ -256,6 +256,13 @@ struct vdpa_device *__vdpa_alloc_device(struct device *parent,
 					const struct vdpa_config_ops *config,
 					size_t size, const char *name);
 
+/*
+ * called by:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|478| <<ifcvf_probe>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2014| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|252| <<vdpasim_create>> vdpasim = vdpa_alloc_device(struct vdpasim, vdpa, NULL, ops,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|401| <<vp_vdpa_probe>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+ */
 #define vdpa_alloc_device(dev_struct, member, parent, config, name)   \
 			  container_of(__vdpa_alloc_device( \
 				       parent, config, \
@@ -282,11 +289,20 @@ struct vdpa_driver {
 	void (*remove)(struct vdpa_device *vdev);
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/vdpa.c|1105| <<vhost_vdpa_init>> r = vdpa_register_driver(&vhost_vdpa_driver);
+ *   - include/linux/vdpa.h|291| <<module_vdpa_driver>> module_driver(__vdpa_driver, vdpa_register_driver, \
+ */
 #define vdpa_register_driver(drv) \
 	__vdpa_register_driver(drv, THIS_MODULE)
 int __vdpa_register_driver(struct vdpa_driver *drv, struct module *owner);
 void vdpa_unregister_driver(struct vdpa_driver *drv);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_vdpa.c|387| <<global>> module_vdpa_driver(virtio_vdpa_driver);
+ */
 #define module_vdpa_driver(__vdpa_driver) \
 	module_driver(__vdpa_driver, vdpa_register_driver,	\
 		      vdpa_unregister_driver)
diff --git a/include/scsi/scsi.h b/include/scsi/scsi.h
index 246ced401683..87367e731bb5 100644
--- a/include/scsi/scsi.h
+++ b/include/scsi/scsi.h
@@ -154,6 +154,16 @@ static inline int scsi_is_wlun(u64 lun)
 				      * recover the link. Transport class will
 				      * retry or fail IO */
 #define DID_TRANSPORT_FAILFAST	0x0f /* Transport class fastfailed the io */
+/*
+ * 在以下使用DID_TARGET_FAILURE:
+ *   - drivers/scsi/scsi_error.c|627| <<scsi_check_sense>> set_host_byte(scmd, DID_TARGET_FAILURE);
+ *   - drivers/scsi/scsi_error.c|643| <<scsi_check_sense>> set_host_byte(scmd, DID_TARGET_FAILURE);
+ *   - drivers/scsi/scsi_error.c|653| <<scsi_check_sense>> set_host_byte(scmd, DID_TARGET_FAILURE);
+ *   - drivers/scsi/scsi_lib.c|602| <<scsi_result_to_blk_status>> case DID_TARGET_FAILURE:
+ *   - drivers/scsi/storvsc_drv.c|1071| <<storvsc_handle_error>> set_host_byte(scmnd, DID_TARGET_FAILURE);
+ *   - drivers/scsi/virtio_scsi.c|168| <<virtscsi_complete_cmd>> set_host_byte(sc, DID_TARGET_FAILURE);
+ *   - drivers/usb/storage/uas.c|288| <<uas_evaluate_response_iu>> set_host_byte(cmnd, DID_TARGET_FAILURE);
+ */
 #define DID_TARGET_FAILURE 0x10 /* Permanent target failure, do not retry on
 				 * other paths */
 #define DID_NEXUS_FAILURE 0x11  /* Permanent nexus failure, retry on other
diff --git a/include/scsi/scsi_cmnd.h b/include/scsi/scsi_cmnd.h
index fed024f4c02a..deff56d23a40 100644
--- a/include/scsi/scsi_cmnd.h
+++ b/include/scsi/scsi_cmnd.h
@@ -62,7 +62,21 @@ struct scsi_pointer {
 #define SCMD_PRESERVED_FLAGS	(SCMD_INITIALIZED)
 
 /* for scmd->state */
+/*
+ * 在以下使用SCMD_STATE_COMPLETE:
+ *   - drivers/scsi/scsi_error.c|332| <<scsi_times_out>> if (test_and_set_bit(SCMD_STATE_COMPLETE, &scmd->state))
+ *   - drivers/scsi/scsi_lib.c|1599| <<scsi_mq_done>> if (unlikely(test_and_set_bit(SCMD_STATE_COMPLETE, &cmd->state)))
+ *   - drivers/scsi/scsi_lib.c|1692| <<scsi_queue_rq>> clear_bit(SCMD_STATE_COMPLETE, &cmd->state);
+ */
 #define SCMD_STATE_COMPLETE	0
+/*
+ * 在以下使用SCMD_STATE_INFLIGHT:
+ *   - drivers/scsi/hosts.c|567| <<scsi_host_check_in_flight>> if (test_bit(SCMD_STATE_INFLIGHT, &cmd->state))
+ *   - drivers/scsi/scsi_lib.c|277| <<scsi_dec_host_busy>> __clear_bit(SCMD_STATE_INFLIGHT, &cmd->state);
+ *   - drivers/scsi/scsi_lib.c|1142| <<scsi_init_command>> in_flight = test_bit(SCMD_STATE_INFLIGHT, &cmd->state);
+ *   - drivers/scsi/scsi_lib.c|1161| <<scsi_init_command>> __set_bit(SCMD_STATE_INFLIGHT, &cmd->state);
+ *   - drivers/scsi/scsi_lib.c|1372| <<scsi_host_queue_ready>> __set_bit(SCMD_STATE_INFLIGHT, &cmd->state);
+ */
 #define SCMD_STATE_INFLIGHT	1
 
 struct scsi_cmnd {
@@ -320,6 +334,9 @@ static inline void set_msg_byte(struct scsi_cmnd *cmd, char status)
 	cmd->result = (cmd->result & 0xffff00ff) | (status << 8);
 }
 
+/*
+ * 设置scsi_cmnd->result
+ */
 static inline void set_host_byte(struct scsi_cmnd *cmd, char status)
 {
 	cmd->result = (cmd->result & 0xff00ffff) | (status << 16);
diff --git a/include/scsi/scsi_device.h b/include/scsi/scsi_device.h
index ac6ab16abee7..8bd5ea6dc29d 100644
--- a/include/scsi/scsi_device.h
+++ b/include/scsi/scsi_device.h
@@ -201,6 +201,13 @@ struct scsi_device {
 	unsigned wce_default_on:1;	/* Cache is ON by default */
 	unsigned no_dif:1;	/* T10 PI (DIF) should be disabled */
 	unsigned broken_fua:1;		/* Don't set FUA bit */
+	/*
+	 * 在以下使用scsi_device->lun_in_cdb:
+	 *   - drivers/scsi/scsi_lib.c|1495| <<scsi_dispatch_cmd>> if (cmd->device->lun_in_cdb)
+	 *   - drivers/scsi/scsi_scan.c|757| <<scsi_probe_lun>> sdev->lun_in_cdb = 0;
+	 *   - drivers/scsi/scsi_scan.c|761| <<scsi_probe_lun>> sdev->lun_in_cdb = 1;
+	 *   - drivers/scsi/scsi_sysfs.c|1624| <<scsi_sysfs_device_initialize>> sdev->lun_in_cdb = 1;
+	 */
 	unsigned lun_in_cdb:1;		/* Store LUN bits in CDB[1] */
 	unsigned unmap_limit_for_ws:1;	/* Use the UNMAP limit for WRITE SAME */
 	unsigned rpm_autosuspend:1;	/* Enable runtime autosuspend at device
diff --git a/include/target/target_core_base.h b/include/target/target_core_base.h
index d1f7d2a45354..5343564a4d52 100644
--- a/include/target/target_core_base.h
+++ b/include/target/target_core_base.h
@@ -453,6 +453,22 @@ enum target_core_dif_check {
 
 struct se_cmd {
 	/* SAM response code being sent to initiator */
+	/*
+	 * 在以下设置se_cmd->scsi_status:
+	 *   - drivers/target/target_core_transport.c|794| <<target_handle_abort>> cmd->scsi_status = SAM_STAT_TASK_ABORTED;
+	 *   - drivers/target/target_core_transport.c|867| <<target_complete_cmd>> cmd->scsi_status = scsi_status;
+	 *   - drivers/target/target_core_transport.c|2073| <<transport_generic_request_failure>> cmd->scsi_status = SAM_STAT_TASK_SET_FULL;
+	 *   - drivers/target/target_core_transport.c|2076| <<transport_generic_request_failure>> cmd->scsi_status = SAM_STAT_BUSY;
+	 *   - drivers/target/target_core_transport.c|2085| <<transport_generic_request_failure>> cmd->scsi_status = SAM_STAT_RESERVATION_CONFLICT;
+	 *   - drivers/target/target_core_transport.c|2153| <<__target_execute_cmd>> cmd->scsi_status = SAM_STAT_RESERVATION_CONFLICT;
+	 *   - drivers/target/target_core_transport.c|3429| <<translate_sense_reason>> cmd->scsi_status = SAM_STAT_BUSY;
+	 *   - drivers/target/target_core_transport.c|3442| <<translate_sense_reason>> cmd->scsi_status = SAM_STAT_CHECK_CONDITION;
+	 *   - drivers/target/target_core_transport.c|3485| <<target_send_busy>> cmd->scsi_status = SAM_STAT_BUSY;
+	 *   - drivers/target/target_core_xcopy.c|624| <<target_xcopy_read_source>> ec_cmd->scsi_status = se_cmd->scsi_status;
+	 *   - drivers/target/target_core_xcopy.c|633| <<target_xcopy_read_source>> ec_cmd->scsi_status = se_cmd->scsi_status;
+	 *   - drivers/target/target_core_xcopy.c|669| <<target_xcopy_write_destination>> ec_cmd->scsi_status = se_cmd->scsi_status;
+	 *   - drivers/target/target_core_xcopy.c|675| <<target_xcopy_write_destination>> ec_cmd->scsi_status = se_cmd->scsi_status;
+	 */
 	u8			scsi_status;
 	u8			scsi_asc;
 	u8			scsi_ascq;
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 79d9c44d1ad7..b8c9af249523 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -251,6 +251,25 @@ struct kvm_xen_exit {
 #define KVM_EXIT_S390_RESET       14
 #define KVM_EXIT_DCR              15 /* deprecated */
 #define KVM_EXIT_NMI              16
+/*
+ * x86在以下使用KVM_EXIT_INTERNAL_ERROR:
+ *   - arch/x86/kvm/svm/nested.c|1366| <<svm_get_nested_state_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/svm/sev.c|2199| <<sev_es_validate_vmgexit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/svm/svm.c|3226| <<svm_handle_invalid_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/nested.c|3135| <<nested_get_vmcs12_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/nested.c|3193| <<vmx_get_nested_state_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|56| <<sgx_handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|115| <<sgx_inject_fault>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|158| <<__handle_encls_ecreate>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|5077| <<handle_exception_nmi>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|5649| <<handle_invalid_guest_state>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|6240| <<__vmx_handle_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|6304| <<__vmx_handle_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|7311| <<handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|7320| <<handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|9984| <<kvm_task_switch>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|11712| <<kvm_handle_memory_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ */
 #define KVM_EXIT_INTERNAL_ERROR   17
 #define KVM_EXIT_OSI              18
 #define KVM_EXIT_PAPR_HCALL	  19
@@ -272,12 +291,32 @@ struct kvm_xen_exit {
 
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
+/*
+ * x86在以下使用KVM_INTERNAL_ERROR_EMULATION:
+ *   - arch/x86/kvm/svm/nested.c|1368| <<svm_get_nested_state_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/nested.c|3137| <<nested_get_vmcs12_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/nested.c|3195| <<vmx_get_nested_state_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|57| <<sgx_handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|116| <<sgx_inject_fault>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|159| <<__handle_encls_ecreate>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/vmx.c|5651| <<handle_invalid_guest_state>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|7312| <<handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|7321| <<handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|9985| <<kvm_task_switch>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|11713| <<kvm_handle_memory_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ */
 #define KVM_INTERNAL_ERROR_EMULATION	1
 /* Encounter unexpected simultaneous exceptions. */
 #define KVM_INTERNAL_ERROR_SIMUL_EX	2
 /* Encounter unexpected vm-exit due to delivery event. */
 #define KVM_INTERNAL_ERROR_DELIVERY_EV	3
 /* Encounter unexpected vm-exit reason */
+/*
+ * 在以下使用KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON:
+ *   - arch/x86/kvm/svm/sev.c|2200| <<sev_es_validate_vmgexit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ *   - arch/x86/kvm/svm/svm.c|3227| <<svm_handle_invalid_exit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ *   - arch/x86/kvm/vmx/vmx.c|6306| <<__vmx_handle_exit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ */
 #define KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON	4
 
 /* for KVM_RUN, returned by mmap(vcpu_fd, offset=0) */
@@ -585,6 +624,11 @@ struct kvm_vapic_addr {
 
 /* not all states are valid on all architectures */
 #define KVM_MP_STATE_RUNNABLE          0
+/*
+ * 在以下使用KVM_MP_STATE_UNINITIALIZED:
+ *   - arch/x86/kvm/x86.c|11439| <<kvm_arch_vcpu_ioctl_run>> if (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {
+ *   - arch/x86/kvm/x86.c|12057| <<kvm_arch_vcpu_create>> vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
+ */
 #define KVM_MP_STATE_UNINITIALIZED     1
 #define KVM_MP_STATE_INIT_RECEIVED     2
 #define KVM_MP_STATE_HALTED            3
diff --git a/include/uapi/linux/virtio_ids.h b/include/uapi/linux/virtio_ids.h
index 4fe842c3a3a9..67aff07972c2 100644
--- a/include/uapi/linux/virtio_ids.h
+++ b/include/uapi/linux/virtio_ids.h
@@ -29,7 +29,34 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE. */
 
+/*
+ * 在以下使用VIRTIO_ID_NET:
+ *   - drivers/net/virtio_net.c|3346| <<global>> { VIRTIO_ID_NET, VIRTIO_DEV_ANY_ID },
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2084| <<global>> { VIRTIO_ID_NET, VIRTIO_DEV_ANY_ID },
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|174| <<global>> { VIRTIO_ID_NET, VIRTIO_DEV_ANY_ID },
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|389| <<mlxbf_tmfifo_get_next_pkt>> if (desc && is_rx && vring->vdev_id == VIRTIO_ID_NET)
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|487| <<mlxbf_tmfifo_get_tx_avail>> if (vdev_id == VIRTIO_ID_NET)
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|627| <<mlxbf_tmfifo_rxtx_header>> if (hdr.type == VIRTIO_ID_NET) {
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|628| <<mlxbf_tmfifo_rxtx_header>> vdev_id = VIRTIO_ID_NET;
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|658| <<mlxbf_tmfifo_rxtx_header>> hdr_len = (vring->vdev_id == VIRTIO_ID_NET) ?
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|661| <<mlxbf_tmfifo_rxtx_header>> hdr.type = (vring->vdev_id == VIRTIO_ID_NET) ?
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|662| <<mlxbf_tmfifo_rxtx_header>> VIRTIO_ID_NET : VIRTIO_ID_CONSOLE;
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|776| <<mlxbf_tmfifo_rxtx>> if (WARN_ON(devid != VIRTIO_ID_NET && devid != VIRTIO_ID_CONSOLE))
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1253| <<mlxbf_tmfifo_probe>> rc = mlxbf_tmfifo_create_vdev(dev, fifo, VIRTIO_ID_NET,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|178| <<ifcvf_vdpa_get_features>> case VIRTIO_ID_NET:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|365| <<ifcvf_vdpa_get_config_size>> case VIRTIO_ID_NET:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|1594| <<mlx5_vdpa_get_device_id>> return VIRTIO_ID_NET;
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|137| <<vdpasim_net_dev_add>> dev_attr.id = VIRTIO_ID_NET;
+ */
 #define VIRTIO_ID_NET			1 /* virtio net */
+/*
+ * 在以下使用VIRTIO_ID_BLOCK:
+ *   - drivers/block/virtio_blk.c|980| <<global>> { VIRTIO_ID_BLOCK, VIRTIO_DEV_ANY_ID },
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|297| <<global>> { VIRTIO_ID_BLOCK, VIRTIO_DEV_ANY_ID },
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|181| <<ifcvf_vdpa_get_features>> case VIRTIO_ID_BLOCK:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|368| <<ifcvf_vdpa_get_config_size>> case VIRTIO_ID_BLOCK:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|260| <<vdpasim_blk_dev_add>> dev_attr.id = VIRTIO_ID_BLOCK;
+ */
 #define VIRTIO_ID_BLOCK			2 /* virtio block */
 #define VIRTIO_ID_CONSOLE		3 /* virtio console */
 #define VIRTIO_ID_RNG			4 /* virtio rng */
diff --git a/include/xen/interface/vcpu.h b/include/xen/interface/vcpu.h
index 504c71601511..68edf0a7f19c 100644
--- a/include/xen/interface/vcpu.h
+++ b/include/xen/interface/vcpu.h
@@ -168,6 +168,11 @@ DEFINE_GUEST_HANDLE_STRUCT(vcpu_set_singleshot_timer);
  * The pointer need not be page aligned, but the structure must not
  * cross a page boundary.
  */
+/*
+ * 在以下使用VCPUOP_register_vcpu_info:
+ *   - arch/arm/xen/enlighten.c|156| <<xen_starting_cpu>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
+ *   - arch/x86/xen/enlighten.c|252| <<xen_vcpu_setup>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
+ */
 #define VCPUOP_register_vcpu_info   10  /* arg == struct vcpu_info */
 struct vcpu_register_vcpu_info {
     uint64_t mfn;    /* mfn of page to place vcpu_info */
diff --git a/include/xen/xen-ops.h b/include/xen/xen-ops.h
index 39a5580f8feb..dc30dcf43e1f 100644
--- a/include/xen/xen-ops.h
+++ b/include/xen/xen-ops.h
@@ -9,6 +9,31 @@
 #include <asm/xen/interface.h>
 #include <xen/interface/vcpu.h>
 
+/*
+ * 在以下使用xen_vcpu:
+ *   - arch/x86/xen/enlighten.c|41| <<global>> DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - include/xen/xen-ops.h|12| <<global>> DECLARE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - rch/x86/xen/enlighten.c|198| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
+ *   - arch/x86/xen/enlighten.c|202| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = NULL;
+ *   - arch/x86/xen/enlighten.c|233| <<xen_vcpu_setup>> if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+ *   - arch/x86/xen/enlighten.c|264| <<xen_vcpu_setup>> per_cpu(xen_vcpu, cpu) = vcpup;
+ *   - arch/x86/xen/enlighten.c|271| <<xen_vcpu_setup>> return ((per_cpu(xen_vcpu, cpu) == NULL) ? -ENODEV : 0);
+ *   - arch/x86/xen/enlighten_pv.c|1436| <<xen_cpu_up_prepare_pv>> if (per_cpu(xen_vcpu, cpu) == NULL)
+ *   - arch/x86/xen/irq.c|32| <<xen_save_fl>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/irq.c|51| <<xen_irq_disable>> this_cpu_read(xen_vcpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/irq.c|67| <<xen_irq_enable>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/mmu_pv.c|1209| <<xen_write_cr2>> this_cpu_read(xen_vcpu)->arch.cr2 = cr2;
+ *   - arch/x86/xen/smp_pv.c|365| <<xen_pv_cpu_up>> per_cpu(xen_vcpu, cpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/time.c|52| <<xen_clocksource_read>> src = &__this_cpu_read(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|74| <<xen_read_wallclock>> vcpu_time = &get_cpu_var(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|76| <<xen_read_wallclock>> put_cpu_var(xen_vcpu);
+ *   - arch/x86/xen/time.c|507| <<xen_time_init>> pvti = &__this_cpu_read(xen_vcpu)->time;
+ *   - drivers/xen/events/events_2l.c|123| <<evtchn_2l_unmask>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|173| <<evtchn_2l_handle_events>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|280| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, i);
+ *   - drivers/xen/events/events_2l.c|289| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, cpu);
+ *   - drivers/xen/events/events_base.c|1699| <<__xen_evtchn_do_upcall>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ */
 DECLARE_PER_CPU(struct vcpu_info *, xen_vcpu);
 
 DECLARE_PER_CPU(uint32_t, xen_vcpu_id);
diff --git a/include/xen/xenbus.h b/include/xen/xenbus.h
index b94074c82772..82358619199b 100644
--- a/include/xen/xenbus.h
+++ b/include/xen/xenbus.h
@@ -138,6 +138,21 @@ int __must_check __xenbus_register_backend(struct xenbus_driver *drv,
 					   struct module *owner,
 					   const char *mod_name);
 
+/*
+ * called by:
+ *   - drivers/block/xen-blkfront.c|3021| <<xlblk_init>> ret = xenbus_register_frontend(&blkfront_driver);
+ *   - drivers/char/tpm/xen-tpmfront.c|442| <<xen_tpmfront_init>> return xenbus_register_frontend(&tpmfront_driver);
+ *   - drivers/gpu/drm/xen/xen_drm_front.c|794| <<xen_drv_init>> return xenbus_register_frontend(&xen_driver);
+ *   - drivers/input/misc/xen-kbdfront.c|559| <<xenkbd_init>> return xenbus_register_frontend(&xenkbd_driver);
+ *   - drivers/net/xen-netfront.c|2595| <<netif_init>> return xenbus_register_frontend(&netfront_driver);
+ *   - drivers/pci/xen-pcifront.c|1169| <<pcifront_init>> return xenbus_register_frontend(&xenpci_driver);
+ *   - drivers/scsi/xen-scsifront.c|1150| <<scsifront_init>> return xenbus_register_frontend(&scsifront_driver);
+ *   - drivers/tty/hvc/hvc_xen.c|563| <<xen_hvc_init>> r = xenbus_register_frontend(&xencons_driver);
+ *   - drivers/video/fbdev/xen-fbfront.c|712| <<xenfb_init>> return xenbus_register_frontend(&xenfb_driver);
+ *   - drivers/xen/pvcalls-front.c|1287| <<pvcalls_frontend_init>> return xenbus_register_frontend(&pvcalls_front_driver);
+ *   - net/9p/trans_xen.c|548| <<p9_trans_xen_init>> rc = xenbus_register_frontend(&xen_9pfs_front_driver);
+ *   - sound/xen/xen_snd_front.c|379| <<xen_drv_init>> return xenbus_register_frontend(&xen_driver);
+ */
 #define xenbus_register_frontend(drv) \
 	__xenbus_register_frontend(drv, THIS_MODULE, KBUILD_MODNAME)
 #define xenbus_register_backend(drv) \
diff --git a/kernel/cpu.c b/kernel/cpu.c
index e538518556f4..f4002f9e0b31 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -1136,6 +1136,17 @@ static int cpu_down(unsigned int cpu, enum cpuhp_state target)
  *
  * Other subsystems should use remove_cpu() instead.
  */
+/*
+ * [0] cpu_device_down
+ * [0] device_offline
+ * [0] online_store
+ * [0] kernfs_fop_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 int cpu_device_down(struct device *dev)
 {
 	return cpu_down(dev->id, CPUHP_OFFLINE);
diff --git a/kernel/irq/matrix.c b/kernel/irq/matrix.c
index 578596e41cb6..f5dccc6c7075 100644
--- a/kernel/irq/matrix.c
+++ b/kernel/irq/matrix.c
@@ -486,6 +486,21 @@ unsigned int irq_matrix_allocated(struct irq_matrix *m)
  *
  * Note, this is a lockless snapshot.
  */
+/*
+ * cat /sys/kernel/debug/irq/domains/VECTOR
+ * name:   VECTOR
+ *  size:   0
+ *  mapped: 26
+ *  flags:  0x00000003
+ * Online bitmaps:        2
+ * Global available:    387
+ * Global reserved:      11
+ * Total allocated:      15
+ * System: 38: 0-19,32,50,128,236,240-242,244,246-255
+ *  | CPU | avl | man | mac | act | vectors
+ *      0   190     1     1   11  33-42,48
+ *      1   197     1     1    4  33-34,40-41
+ */
 void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)
 {
 	unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index cbff6ba53d56..816ec8e6b420 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -105,6 +105,13 @@ struct qnode {
  *
  * PV doubles the storage and uses the second cacheline for PV state.
  */
+/*
+ * 在以下使用percpu的qnodes[]:
+ *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+ *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+ *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+ *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+ */
 static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
 
 /*
@@ -122,11 +129,22 @@ static inline __pure u32 encode_tail(int cpu, int idx)
 	return tail;
 }
 
+/*
+ * called by:
+ *   - kernel/locking/qspinlock.c|508| <<queued_spin_lock_slowpath>> prev = decode_tail(old);
+ */
 static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 {
 	int cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;
 	int idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
 
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 */
 	return per_cpu_ptr(&qnodes[idx].mcs, cpu);
 }
 
@@ -312,6 +330,22 @@ static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,
  * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
  *   queue               :         ^--'                             :
  */
+/*
+ * 在以下使用queued_spin_lock_slowpath():
+ *   - arch/x86/kernel/paravirt.c|362| <<global>> .lock.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
+ *   - include/asm-generic/qspinlock.h|71| <<global>> extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);
+ *   - kernel/locking/qspinlock.c|289| <<global>> #define queued_spin_lock_slowpath native_queued_spin_lock_slowpath
+ *   - kernel/locking/qspinlock.c|595| <<global>> #undef queued_spin_lock_slowpath
+ *   - kernel/locking/qspinlock.c|596| <<global>> #define queued_spin_lock_slowpath __pv_queued_spin_lock_slowpath
+ *   - arch/powerpc/include/asm/qspinlock.h|15| <<queued_spin_lock_slowpath>> static __always_inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+ *   - arch/powerpc/include/asm/qspinlock.h|43| <<queued_spin_lock>> queued_spin_lock_slowpath(lock, val);
+ *   - arch/x86/hyperv/hv_spinlock.c|80| <<hv_init_spinlocks>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - arch/x86/include/asm/paravirt.h|585| <<pv_queued_spin_lock_slowpath>> PVOP_VCALL2(lock.queued_spin_lock_slowpath, lock, val);
+ *   - arch/x86/include/asm/qspinlock.h|49| <<queued_spin_lock_slowpath>> static inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+ *   - arch/x86/kernel/kvm.c|979| <<kvm_spinlock_init>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - arch/x86/xen/spinlock.c|139| <<xen_init_spinlocks>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - include/asm-generic/qspinlock.h|85| <<queued_spin_lock>> queued_spin_lock_slowpath(lock, val);
+ */
 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 {
 	struct mcs_spinlock *prev, *next, *node;
@@ -397,6 +431,15 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 queue:
 	lockevent_inc(lock_slowpath);
 pv_queue:
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 *
+	 * struct mcs_spinlock *prev, *next, *node;
+	 */
 	node = this_cpu_ptr(&qnodes[0].mcs);
 	idx = node->count++;
 	tail = encode_tail(smp_processor_id(), idx);
@@ -412,6 +455,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 */
 	if (unlikely(idx >= MAX_NODES)) {
 		lockevent_inc(lock_no_node);
+		/*
+		 * called by:
+		 *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+		 *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+		 * 重新定义的queued_spin_trylock():
+		 * #define queued_spin_trylock(l)       pv_hybrid_queued_unfair_trylock(l)
+		 */
 		while (!queued_spin_trylock(lock))
 			cpu_relax();
 		goto release;
@@ -440,6 +490,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 * attempt the trylock once more in the hope someone let go while we
 	 * weren't watching.
 	 */
+	/*
+	 * called by:
+	 *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+	 *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+	 * 重新定义的queued_spin_trylock():
+	 * #define queued_spin_trylock(l)       pv_hybrid_queued_unfair_trylock(l)
+	 */
 	if (queued_spin_trylock(lock))
 		goto release;
 
@@ -557,6 +614,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	/*
 	 * release the node
 	 */
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 */
 	__this_cpu_dec(qnodes[0].mcs.count);
 }
 EXPORT_SYMBOL(queued_spin_lock_slowpath);
diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h
index e84d21aa0722..12bbb17628de 100644
--- a/kernel/locking/qspinlock_paravirt.h
+++ b/kernel/locking/qspinlock_paravirt.h
@@ -21,6 +21,13 @@
  * native_queued_spin_unlock().
  */
 
+/*
+ * 在以下使用_Q_SLOW_VAL:
+ *   - kernel/locking/qspinlock_paravirt.h|391| <<pv_kick_node>> WRITE_ONCE(lock->locked, _Q_SLOW_VAL);
+ *   - kernel/locking/qspinlock_paravirt.h|456| <<pv_wait_head_or_lock>> if (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {
+ *   - kernel/locking/qspinlock_paravirt.h|470| <<pv_wait_head_or_lock>> pv_wait(&lock->locked, _Q_SLOW_VAL);
+ *   - kernel/locking/qspinlock_paravirt.h|501| <<__pv_queued_spin_unlock_slowpath>> if (unlikely(locked != _Q_SLOW_VAL)) {
+ */
 #define _Q_SLOW_VAL	(3U << _Q_LOCKED_OFFSET)
 
 /*
@@ -35,12 +42,24 @@
  * controlled by PV_PREV_CHECK_MASK. This is to ensure that we won't
  * pound on the cacheline of the previous node too heavily.
  */
+/*
+ * 只在以下使用PV_PREV_CHECK_MASK:
+ *   - kernel/locking/qspinlock_paravirt.h|269| <<pv_wait_early>> if ((loop & PV_PREV_CHECK_MASK) != 0)
+ */
 #define PV_PREV_CHECK_MASK	0xff
 
 /*
  * Queue node uses: vcpu_running & vcpu_halted.
  * Queue head uses: vcpu_running & vcpu_hashed.
  */
+/*
+ * 在以下使用vcpu_halted:
+ *   - kernel/locking/qspinlock_paravirt.h|320| <<pv_wait_node>> smp_store_mb(pn->state, vcpu_halted);
+ *   - kernel/locking/qspinlock_paravirt.h|325| <<pv_wait_node>> pv_wait(&pn->state, vcpu_halted);
+ *   - kernel/locking/qspinlock_paravirt.h|333| <<pv_wait_node>> cmpxchg(&pn->state, vcpu_halted, vcpu_running);
+ *   - kernel/locking/qspinlock_paravirt.h|380| <<pv_kick_node>> if (cmpxchg_relaxed(&pn->state, vcpu_halted, vcpu_hashed)
+ *   - kernel/locking/qspinlock_paravirt.h|381| <<pv_kick_node>> != vcpu_halted)
+ */
 enum vcpu_state {
 	vcpu_running = 0,
 	vcpu_halted,		/* Used only in pv_wait_node */
@@ -77,6 +96,13 @@ struct pv_node {
  * queued lock (no lock starvation) and an unfair lock (good performance
  * on not heavily contended locks).
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+ *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+ * 重新定义的queued_spin_trylock():
+ * #define queued_spin_trylock(l)	pv_hybrid_queued_unfair_trylock(l)
+ */
 #define queued_spin_trylock(l)	pv_hybrid_queued_unfair_trylock(l)
 static inline bool pv_hybrid_queued_unfair_trylock(struct qspinlock *lock)
 {
@@ -173,10 +199,34 @@ struct pv_hash_entry {
 	struct pv_node   *node;
 };
 
+/*
+ * 在以下使用PV_HE_PER_LINE:
+ *   - kernel/locking/qspinlock_paravirt.h|230| <<__pv_init_lock_hash>> int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);
+ *   - kernel/locking/qspinlock_paravirt.h|248| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+ */
 #define PV_HE_PER_LINE	(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))
+/*
+ * 在以下使用PV_HE_MIN:
+ *   - kernel/locking/qspinlock_paravirt.h|232| <<__pv_init_lock_hash>> if (pv_hash_size < PV_HE_MIN)
+ *   - kernel/locking/qspinlock_paravirt.h|233| <<__pv_init_lock_hash>> pv_hash_size = PV_HE_MIN;
+ */
 #define PV_HE_MIN	(PAGE_SIZE / sizeof(struct pv_hash_entry))
 
+/*
+ * 在以下使用pv_lock_hash:
+ *   - kernel/locking/qspinlock_paravirt.h|199| <<__pv_init_lock_hash>> pv_lock_hash = alloc_large_system_hash("PV qspinlock",
+ *   - kernel/locking/qspinlock_paravirt.h|208| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+ *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+ */
 static struct pv_hash_entry *pv_lock_hash;
+/*
+ * 在以下使用pv_lock_hash_bits:
+ *   - kernel/locking/qspinlock_paravirt.h|203| <<__pv_init_lock_hash>> &pv_lock_hash_bits, NULL,
+ *   - kernel/locking/qspinlock_paravirt.h|209| <<for_each_hash_entry>> offset < (1 << pv_lock_hash_bits); \
+ *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+ *   - kernel/locking/qspinlock_paravirt.h|214| <<pv_hash>> unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
+ *   - kernel/locking/qspinlock_paravirt.h|241| <<pv_unhash>> unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
+ */
 static unsigned int pv_lock_hash_bits __read_mostly;
 
 /*
@@ -185,6 +235,13 @@ static unsigned int pv_lock_hash_bits __read_mostly;
  * This function should be called from the paravirt spinlock initialization
  * routine.
  */
+/*
+ * called by:
+ *   - arch/powerpc/include/asm/qspinlock.h|70| <<pv_spinlocks_init>> __pv_init_lock_hash();
+ *   - arch/x86/hyperv/hv_spinlock.c|79| <<hv_init_spinlocks>> __pv_init_lock_hash();
+ *   - arch/x86/kernel/kvm.c|978| <<kvm_spinlock_init>> __pv_init_lock_hash();
+ *   - arch/x86/xen/spinlock.c|138| <<xen_init_spinlocks>> __pv_init_lock_hash();
+ */
 void __init __pv_init_lock_hash(void)
 {
 	int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);
@@ -196,6 +253,12 @@ void __init __pv_init_lock_hash(void)
 	 * Allocate space from bootmem which should be page-size aligned
 	 * and hence cacheline aligned.
 	 */
+	/*
+	 * 在以下使用pv_lock_hash:
+	 *   - kernel/locking/qspinlock_paravirt.h|199| <<__pv_init_lock_hash>> pv_lock_hash = alloc_large_system_hash("PV qspinlock",
+	 *   - kernel/locking/qspinlock_paravirt.h|208| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+	 *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+	 */
 	pv_lock_hash = alloc_large_system_hash("PV qspinlock",
 					       sizeof(struct pv_hash_entry),
 					       pv_hash_size, 0,
@@ -204,11 +267,21 @@ void __init __pv_init_lock_hash(void)
 					       pv_hash_size, pv_hash_size);
 }
 
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|268| <<pv_hash>> for_each_hash_entry(he, offset, hash) {
+ *   - kernel/locking/qspinlock_paravirt.h|295| <<pv_unhash>> for_each_hash_entry(he, offset, hash) {
+ */
 #define for_each_hash_entry(he, offset, hash)						\
 	for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0;	\
 	     offset < (1 << pv_lock_hash_bits);						\
 	     offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
 
+/*
+ * 在以下使用pv_hash():
+ *   - kernel/locking/qspinlock_paravirt.h|480| <<pv_kick_node>> (void )pv_hash(lock, pn);
+ *   - kernel/locking/qspinlock_paravirt.h|531| <<pv_wait_head_or_lock>> lp = pv_hash(lock, pn);
+ */
 static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)
 {
 	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
@@ -236,6 +309,10 @@ static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)
 	BUG();
 }
 
+/*
+ * 在以下使用pv_unhash():
+ *   - kernel/locking/qspinlock_paravirt.h|609| <<__pv_queued_spin_unlock_slowpath>> node = pv_unhash(lock);
+ */
 static struct pv_node *pv_unhash(struct qspinlock *lock)
 {
 	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
@@ -245,6 +322,12 @@ static struct pv_node *pv_unhash(struct qspinlock *lock)
 	for_each_hash_entry(he, offset, hash) {
 		if (READ_ONCE(he->lock) == lock) {
 			node = READ_ONCE(he->node);
+			/*
+			 * truct pv_hash_entry {
+			 *     struct qspinlock *lock;
+			 *     struct pv_node   *node;
+			 * };
+			 */
 			WRITE_ONCE(he->lock, NULL);
 			return node;
 		}
@@ -263,9 +346,29 @@ static struct pv_node *pv_unhash(struct qspinlock *lock)
  * Return true if when it is time to check the previous node which is not
  * in a running state.
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|304| <<pv_wait_node>> if (pv_wait_early(pp, loop)) {
+ *   - kernel/locking/qspinlock_paravirt.h|324| <<pv_wait_node>> lockevent_cond_inc(pv_wait_early, wait_early);
+ */
 static inline bool
 pv_wait_early(struct pv_node *prev, int loop)
 {
+	/*
+	 * Queue Node Adaptive Spinning
+	 *
+	 * A queue node vCPU will stop spinning if the vCPU in the previous node is
+	 * not running. The one lock stealing attempt allowed at slowpath entry
+	 * mitigates the slight slowdown for non-overcommitted guest with this
+	 * aggressive wait-early mechanism.
+	 *      
+	 * The status of the previous node will be checked at fixed interval
+	 * controlled by PV_PREV_CHECK_MASK. This is to ensure that we won't
+	 * pound on the cacheline of the previous node too heavily.
+	 *
+	 * 只在以下使用PV_PREV_CHECK_MASK:
+	 *   - kernel/locking/qspinlock_paravirt.h|269| <<pv_wait_early>> if ((loop & PV_PREV_CHECK_MASK) != 0)
+	 */
 	if ((loop & PV_PREV_CHECK_MASK) != 0)
 		return false;
 
@@ -275,6 +378,12 @@ pv_wait_early(struct pv_node *prev, int loop)
 /*
  * Initialize the PV part of the mcs_spinlock node.
  */
+/*
+ * 在以下使用pv_init_node():
+ *   - kernel/locking/qspinlock.c|283| <<global>> #define pv_init_node __pv_init_node
+ *   - kernel/locking/qspinlock.c|590| <<global>> #undef pv_init_node
+ *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> pv_init_node(node);
+ */
 static void pv_init_node(struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -290,6 +399,14 @@ static void pv_init_node(struct mcs_spinlock *node)
  * pv_kick_node() is used to set _Q_SLOW_VAL and fill in hash table on its
  * behalf.
  */
+/*
+ * 在以下使用pv_wait_node():
+ *   - kernel/locking/qspinlock.c|284| <<global>> #define pv_wait_node __pv_wait_node
+ *   - kernel/locking/qspinlock.c|591| <<global>> #undef pv_wait_node
+ *   - kernel/locking/lock_events_list.h|35| <<LOCK_EVENT>> LOCK_EVENT(pv_wait_node)
+ *   - kernel/locking/qspinlock.c|490| <<queued_spin_lock_slowpath>> pv_wait_node(node, prev);
+ *   - kernel/locking/qspinlock_paravirt.h|411| <<pv_wait_node>> lockevent_inc(pv_wait_node);
+ */
 static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -357,6 +474,12 @@ static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
  * such that they're waiting in pv_wait_head_or_lock(), this avoids a
  * wake/sleep cycle.
  */
+/*
+ * 在以下使用pv_kick_node():
+ *   - kernel/locking/qspinlock.c|285| <<global>> #define pv_kick_node __pv_kick_node
+ *   - kernel/locking/qspinlock.c|592| <<global>> #undef pv_kick_node
+ *   - kernel/locking/qspinlock.c|571| <<queued_spin_lock_slowpath>> pv_kick_node(lock, next);
+ */
 static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -399,6 +522,12 @@ static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
  *
  * The current value of the lock will be returned for additional processing.
  */
+/*
+ * 在以下使用pv_wait_head_or_lock():
+ *   - kernel/locking/qspinlock.c|286| <<global>> #define pv_wait_head_or_lock __pv_wait_head_or_lock
+ *   - kernel/locking/qspinlock.c|593| <<global>> #undef pv_wait_head_or_lock
+ *   - kernel/locking/qspinlock.c|525| <<queued_spin_lock_slowpath>> if ((val = pv_wait_head_or_lock(lock, node)))
+ */
 static u32
 pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
 {
@@ -489,6 +618,10 @@ pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
  * PV versions of the unlock fastpath and slowpath functions to be used
  * instead of queued_spin_unlock().
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|560| <<__pv_queued_spin_unlock>> __pv_queued_spin_unlock_slowpath(lock, locked);
+ */
 __visible void
 __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 {
@@ -544,6 +677,14 @@ __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 #include <asm/qspinlock_paravirt.h>
 
 #ifndef __pv_queued_spin_unlock
+/*
+ * 在以下使用__pv_queued_spin_unlock():
+ *   - arch/x86/include/asm/qspinlock_paravirt.h|66| <<global>> PV_CALLEE_SAVE_REGS_THUNK(__pv_queued_spin_unlock);
+ *   - arch/powerpc/include/asm/qspinlock.h|29| <<queued_spin_unlock>> __pv_queued_spin_unlock(lock);
+ *   - arch/x86/hyperv/hv_spinlock.c|81| <<hv_init_spinlocks>> pv_ops.lock.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ *   - arch/x86/kernel/kvm.c|981| <<kvm_spinlock_init>> PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ *   - arch/x86/xen/spinlock.c|141| <<xen_init_spinlocks>> PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ */
 __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 {
 	u8 locked;
@@ -553,7 +694,18 @@ __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 	 * unhash. Otherwise it would be possible to have multiple @lock
 	 * entries, which would be BAD.
 	 */
+	/*
+	 * cmpxchg(void *ptr, unsigned long old, unsigned long new);
+	 * 函数完成的功能是: 将old和ptr指向的内容比较,如果相等,
+	 * 则将new写入到ptr中,返回old,如果不相等,则返回ptr指向的内容.
+	 *
+	 * struct qspinlock *lock:
+	 * -> u8 locked;
+	 */
 	locked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);
+	/*
+	 * 如果slow的话最后两位可能都是11
+	 */
 	if (likely(locked == _Q_LOCKED_VAL))
 		return;
 
diff --git a/kernel/sched/clock.c b/kernel/sched/clock.c
index c2b2859ddd82..4f264e32e6fb 100644
--- a/kernel/sched/clock.c
+++ b/kernel/sched/clock.c
@@ -78,12 +78,27 @@ static DEFINE_STATIC_KEY_FALSE(sched_clock_running);
  * Similarly we start with __sched_clock_stable_early, thereby assuming we
  * will become stable, such that there's only a single 1 -> 0 transition.
  */
+/*
+ * 在以下使用__sched_clock_stable:
+ *   - kernel/sched/clock.c|110| <<sched_clock_stable>> return static_branch_likely(&__sched_clock_stable);
+ *   - kernel/sched/clock.c|139| <<__set_sched_clock_stable>> static_branch_enable(&__sched_clock_stable);
+ *   - kernel/sched/clock.c|175| <<__sched_clock_work>> static_branch_disable(&__sched_clock_stable);
+ */
 static DEFINE_STATIC_KEY_FALSE(__sched_clock_stable);
 static int __sched_clock_stable_early = 1;
 
 /*
  * We want: ktime_get_ns() + __gtod_offset == sched_clock() + __sched_clock_offset
  */
+/*
+ * 在以下使用__sched_clock_offset:
+ *   - arch/x86/events/core.c|2703| <<arch_perf_update_userpage>> offset = data.cyc2ns_offset + __sched_clock_offset;
+ *   - kernel/sched/clock.c|132| <<__set_sched_clock_stable>> __sched_clock_offset = (scd->tick_gtod + __gtod_offset) - (scd->tick_raw);
+ *   - kernel/sched/clock.c|137| <<__set_sched_clock_stable>> scd->tick_raw, __sched_clock_offset);
+ *   - kernel/sched/clock.c|173| <<__sched_clock_work>> scd->tick_raw, __sched_clock_offset);
+ *   - kernel/sched/clock.c|204| <<__sched_clock_gtod_offset>> __gtod_offset = (scd->tick_raw + __sched_clock_offset) - scd->tick_gtod;
+ *   - kernel/sched/clock.c|371| <<sched_clock_cpu>> return sched_clock() + __sched_clock_offset;
+ */
 __read_mostly u64 __sched_clock_offset;
 static __read_mostly u64 __gtod_offset;
 
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 4ca80df205ce..1af6b4ca9684 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4371,6 +4371,10 @@ unsigned long nr_running(void)
  *
  * - in a loop with very short iterations (e.g. a polling loop)
  */
+/*
+ * 返回(raw_rq()->nr_running == 1)
+ * Check if only the current task is running on the CPU.
+ */
 bool single_task_running(void)
 {
 	return raw_rq()->nr_running == 1;
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index cbc30271ea4d..97a0d380e8d9 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -28,6 +28,11 @@
  * be shared by works on different cpus.
  */
 struct cpu_stop_done {
+	/*
+	 * 在以下使用cpu_stop_done->nr_todo:
+	 *   - kernel/stop_machine.c|73| <<cpu_stop_init_done>> atomic_set(&done->nr_todo, nr_todo);
+	 *   - kernel/stop_machine.c|85| <<cpu_stop_signal_done>> if (atomic_dec_and_test(&done->nr_todo))
+	 */
 	atomic_t		nr_todo;	/* nr left to execute */
 	int			ret;		/* collected return value */
 	struct completion	completion;	/* fired if nr_todo reaches 0 */
@@ -38,6 +43,13 @@ struct cpu_stopper {
 	struct task_struct	*thread;
 
 	raw_spinlock_t		lock;
+	/*
+	 * 在以下使用cpu_stopper->enabled:
+	 *   - kernel/stop_machine.c|129| <<cpu_stop_queue_work>> enabled = stopper->enabled; 
+	 *   - kernel/stop_machine.c|425| <<cpu_stop_queue_two_works>> if (!stopper1->enabled || !stopper2->enabled) {
+	 *   - kernel/stop_machine.c|681| <<stop_machine_park>> stopper->enabled = false;
+	 *   - kernel/stop_machine.c|703| <<stop_machine_unpark>> stopper->enabled = true;
+	 */
 	bool			enabled;	/* is this stopper enabled? */
 	struct list_head	works;		/* list of pending works */
 
@@ -46,9 +58,36 @@ struct cpu_stopper {
 	cpu_stop_fn_t		fn;
 };
 
+/*
+ * 在以下使用percpu的cpu_stopper:
+ *   - kernel/stop_machine.c|54| <<global>> static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
+ *   - kernel/stop_machine.c|708| <<global>> .store = &cpu_stopper.thread,
+ *   - kernel/stop_machine.c|63| <<print_stop_info>> struct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));
+ *   - kernel/stop_machine.c|122| <<cpu_stop_queue_work>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|408| <<cpu_stop_queue_two_works>> struct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);
+ *   - kernel/stop_machine.c|409| <<cpu_stop_queue_two_works>> struct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);
+ *   - kernel/stop_machine.c|552| <<queue_stop_cpus_work>> work = &per_cpu(cpu_stopper.stop_work, cpu);
+ *   - kernel/stop_machine.c|620| <<cpu_stop_should_run>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|635| <<cpu_stopper_thread>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|675| <<stop_machine_park>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|689| <<cpu_stop_create>> sched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));
+ *   - kernel/stop_machine.c|694| <<cpu_stop_park>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|701| <<stop_machine_unpark>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ *   - kernel/stop_machine.c|722| <<cpu_stop_init>> struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ */
 static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
+/*
+ * 在以下使用stop_machine_initialized:
+ *   - kernel/stop_machine.c|730| <<cpu_stop_init>> stop_machine_initialized = true;
+ *   - kernel/stop_machine.c|765| <<stop_machine_cpuslocked>> if (!stop_machine_initialized) {
+ */
 static bool stop_machine_initialized = false;
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|7373| <<sched_show_task>> print_stop_info(KERN_INFO, p);
+ *   - lib/dump_stack.c|61| <<dump_stack_print_info>> print_stop_info(log_lvl, current);
+ */
 void print_stop_info(const char *log_lvl, struct task_struct *task)
 {
 	/*
@@ -64,19 +103,55 @@ void print_stop_info(const char *log_lvl, struct task_struct *task)
 }
 
 /* static data for stop_cpus */
+/*
+ * 在以下使用stop_cpus_mutex:
+ *   - kernel/stop_machine.c|579| <<stop_cpus>> mutex_lock(&stop_cpus_mutex);
+ *   - kernel/stop_machine.c|581| <<stop_cpus>> mutex_unlock(&stop_cpus_mutex);
+ *   - kernel/stop_machine.c|831| <<stop_machine_from_inactive_cpu>> while (!mutex_trylock(&stop_cpus_mutex))
+ *   - kernel/stop_machine.c|848| <<stop_machine_from_inactive_cpu>> mutex_unlock(&stop_cpus_mutex);
+ */
 static DEFINE_MUTEX(stop_cpus_mutex);
+/*
+ * 在以下使用stop_cpus_in_progress:
+ *   - kernel/stop_machine.c|440| <<cpu_stop_queue_two_works>> if (unlikely(stop_cpus_in_progress)) {
+ *   - kernel/stop_machine.c|456| <<cpu_stop_queue_two_works>> while (stop_cpus_in_progress)
+ *   - kernel/stop_machine.c|549| <<queue_stop_cpus_work>> stop_cpus_in_progress = true;
+ *   - kernel/stop_machine.c|561| <<queue_stop_cpus_work>> stop_cpus_in_progress = false;
+ */
 static bool stop_cpus_in_progress;
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|171| <<stop_one_cpu>> cpu_stop_init_done(&done, 1);
+ *   - kernel/stop_machine.c|498| <<stop_two_cpus>> cpu_stop_init_done(&done, 2);
+ *   - kernel/stop_machine.c|572| <<__stop_cpus>> cpu_stop_init_done(&done, cpumask_weight(cpumask));
+ *   - kernel/stop_machine.c|882| <<stop_machine_from_inactive_cpu>> cpu_stop_init_done(&done, num_active_cpus());
+ */
 static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)
 {
 	memset(done, 0, sizeof(*done));
+	/*
+	 * 在以下使用cpu_stop_done->nr_todo:
+	 *   - kernel/stop_machine.c|73| <<cpu_stop_init_done>> atomic_set(&done->nr_todo, nr_todo);
+	 *   - kernel/stop_machine.c|85| <<cpu_stop_signal_done>> if (atomic_dec_and_test(&done->nr_todo))
+	 */
 	atomic_set(&done->nr_todo, nr_todo);
 	init_completion(&done->completion);
 }
 
 /* signal completion unless @done is NULL */
+/*
+ * called by:
+ *   - kernel/stop_machine.c|106| <<cpu_stop_queue_work>> cpu_stop_signal_done(work->done);
+ *   - kernel/stop_machine.c|530| <<cpu_stopper_thread>> cpu_stop_signal_done(done);
+ */
 static void cpu_stop_signal_done(struct cpu_stop_done *done)
 {
+	/*
+	 * 在以下使用cpu_stop_done->nr_todo:
+	 *   - kernel/stop_machine.c|73| <<cpu_stop_init_done>> atomic_set(&done->nr_todo, nr_todo);
+	 *   - kernel/stop_machine.c|85| <<cpu_stop_signal_done>> if (atomic_dec_and_test(&done->nr_todo))
+	 */
 	if (atomic_dec_and_test(&done->nr_todo))
 		complete(&done->completion);
 }
@@ -171,25 +246,80 @@ struct multi_stop_data {
 	cpu_stop_fn_t		fn;
 	void			*data;
 	/* Like num_online_cpus(), but hotplug cpu uses us, so we need this. */
+	/*
+	 * 在以下使用multi_stop_data->num_threads:
+	 *   - kernel/stop_machine.c|185| <<set_state>> atomic_set(&msdata->thread_ack, msdata->num_threads);
+	 *   - kernel/stop_machine.c|358| <<stop_two_cpus>> .num_threads = 2,
+	 *   - kernel/stop_machine.c|627| <<stop_machine_cpuslocked>> .num_threads = num_online_cpus(),
+	 *   - kernel/stop_machine.c|642| <<stop_machine_cpuslocked>> WARN_ON_ONCE(msdata.num_threads != 1);
+	 *   - kernel/stop_machine.c|701| <<stop_machine_from_inactive_cpu>> msdata.num_threads = num_active_cpus() + 1;
+	 */
 	unsigned int		num_threads;
 	const struct cpumask	*active_cpus;
 
+	/*
+	 * 在以下使用multi_stop_data->state:
+	 *   - kernel/stop_machine.c|187| <<set_state>> WRITE_ONCE(msdata->state, newstate);
+	 *   - kernel/stop_machine.c|194| <<ack_state>> set_state(msdata, msdata->state + 1);
+	 *   - kernel/stop_machine.c|245| <<multi_cpu_stop>> newstate = READ_ONCE(msdata->state);
+	 */
 	enum multi_stop_state	state;
+	/*
+	 * 在以下使用multi_stop_date->thread_ack:
+	 *   - kernel/stop_machine.c|185| <<set_state>> atomic_set(&msdata->thread_ack, msdata->num_threads);
+	 *   - kernel/stop_machine.c|193| <<ack_state>> if (atomic_dec_and_test(&msdata->thread_ack))
+	 */
 	atomic_t		thread_ack;
 };
 
+/*
+ * called by:
+ *   - kernel/stop_machine.c|194| <<ack_state>> set_state(msdata, msdata->state + 1);
+ *   - kernel/stop_machine.c|370| <<stop_two_cpus>> set_state(&msdata, MULTI_STOP_PREPARE);
+ *   - kernel/stop_machine.c|653| <<stop_machine_cpuslocked>> set_state(&msdata, MULTI_STOP_PREPARE); 
+ *   - kernel/stop_machine.c|708| <<stop_machine_from_inactive_cpu>> set_state(&msdata, MULTI_STOP_PREPARE);
+ */
 static void set_state(struct multi_stop_data *msdata,
 		      enum multi_stop_state newstate)
 {
 	/* Reset ack counter. */
+	/*
+	 * 在以下使用multi_stop_data->num_threads:
+	 *   - kernel/stop_machine.c|185| <<set_state>> atomic_set(&msdata->thread_ack, msdata->num_threads);
+	 *   - kernel/stop_machine.c|358| <<stop_two_cpus>> .num_threads = 2,
+	 *   - kernel/stop_machine.c|627| <<stop_machine_cpuslocked>> .num_threads = num_online_cpus(),
+	 *   - kernel/stop_machine.c|642| <<stop_machine_cpuslocked>> WARN_ON_ONCE(msdata.num_threads != 1);
+	 *   - kernel/stop_machine.c|701| <<stop_machine_from_inactive_cpu>> msdata.num_threads = num_active_cpus() + 1;
+	 */
 	atomic_set(&msdata->thread_ack, msdata->num_threads);
 	smp_wmb();
 	WRITE_ONCE(msdata->state, newstate);
 }
 
 /* Last one to ack a state moves to the next state. */
+/*
+ * called by:
+ *   - kernel/stop_machine.c|351| <<multi_cpu_stop>> ack_state(msdata);
+ */
 static void ack_state(struct multi_stop_data *msdata)
 {
+	/*
+	 * Atomically decrements @v by 1 and
+	 * returns true if the result is 0, or false for all other
+	 * cases.
+	 */
+	/*
+	 *  在以下使用multi_stop_data->num_threads:
+	 *   - kernel/stop_machine.c|185| <<set_state>> atomic_set(&msdata->thread_ack, msdata->num_threads);
+	 *   - kernel/stop_machine.c|358| <<stop_two_cpus>> .num_threads = 2,
+	 *   - kernel/stop_machine.c|627| <<stop_machine_cpuslocked>> .num_threads = num_online_cpus(),
+	 *   - kernel/stop_machine.c|642| <<stop_machine_cpuslocked>> WARN_ON_ONCE(msdata.num_threads != 1);
+	 *   - kernel/stop_machine.c|701| <<stop_machine_from_inactive_cpu>> msdata.num_threads = num_active_cpus() + 1;
+	 *
+	 * 在以下使用multi_stop_date->thread_ack:
+	 *   - kernel/stop_machine.c|185| <<set_state>> atomic_set(&msdata->thread_ack, msdata->num_threads);
+	 *   - kernel/stop_machine.c|193| <<ack_state>> if (atomic_dec_and_test(&msdata->thread_ack))
+	 */
 	if (atomic_dec_and_test(&msdata->thread_ack))
 		set_state(msdata, msdata->state + 1);
 }
@@ -200,15 +330,54 @@ notrace void __weak stop_machine_yield(const struct cpumask *cpumask)
 }
 
 /* This is the cpu_stop function which stops the CPU. */
+/*
+ * [   38.649989] CPU: 4 PID: 0 Comm: swapper/4 Not tainted 5.13.0+ #6
+ * ... ...
+ * [   38.649992] Call Trace:
+ * [   38.649993]  dump_stack+0x76/0x94
+ * [   38.649996]  multi_cpu_stop+0x28/0xe2
+ * [   38.652144]  stop_machine_from_inactive_cpu+0xd7/0x120
+ * [   38.652146]  ? mtrr_restore+0xa0/0xa0
+ * [   38.652148]  mtrr_ap_init+0x81/0x90
+ * [   38.652149]  identify_secondary_cpu+0x18/0x80
+ * [   38.652151]  smp_store_cpu_info+0x40/0x50
+ * [   38.652153]  start_secondary+0x6d/0x14f
+ * [   38.652155]  secondary_startup_64_no_verify+0xc2/0xcb
+ *
+ *
+ * [    0.296288] CPU: 2 PID: 19 Comm: migration/2 Not tainted 5.13.0+ #3
+ * ... ...
+ * [    0.296288] Stopper: multi_cpu_stop+0x0/0xe2 <- stop_machine_cpuslocked+0xfd/0x140
+ * [    0.296288] Call Trace:
+ * [    0.296298]  dump_stack+0x76/0x94
+ * [    0.296304]  multi_cpu_stop+0x28/0xe2
+ * [    0.296307]  cpu_stopper_thread+0x4c/0x100
+ * [    0.296313]  smpboot_thread_fn+0xc0/0x160
+ * [    0.296315]  kthread+0x111/0x130
+ * [    0.296318]  ret_from_fork+0x22/0x30
+ *
+ * 在以下使用multi_cpu_stop():
+ *   - kernel/stop_machine.c|540| <<stop_two_cpus>> .fn = multi_cpu_stop,
+ *   - kernel/stop_machine.c|841| <<stop_machine_cpuslocked>> return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
+ *   - kernel/stop_machine.c|931| <<stop_machine_from_inactive_cpu>> queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
+ *   - kernel/stop_machine.c|933| <<stop_machine_from_inactive_cpu>> ret = multi_cpu_stop(&msdata);
+ */
 static int multi_cpu_stop(void *data)
 {
 	struct multi_stop_data *msdata = data;
+	/*
+	 * 除了加减法,只在这里使用MULTI_STOP_NONE
+	 */
 	enum multi_stop_state newstate, curstate = MULTI_STOP_NONE;
 	int cpu = smp_processor_id(), err = 0;
 	const struct cpumask *cpumask;
 	unsigned long flags;
 	bool is_active;
 
+	/*
+	 * 在stop_machine_from_inactive_cpu()设置multi_stop_data->state=MULTI_STOP_PREPARE
+	 */
+
 	/*
 	 * When called from stop_machine_from_inactive_cpu(), irq might
 	 * already be disabled.  Save the state and restore it on exit.
@@ -226,6 +395,9 @@ static int multi_cpu_stop(void *data)
 	/* Simple state machine */
 	do {
 		/* Chill out and ensure we re-read multi_stop_state. */
+		/*
+		 * stop_machine_yield()就是cpu_relax()
+		 */
 		stop_machine_yield(cpumask);
 		newstate = READ_ONCE(msdata->state);
 		if (newstate != curstate) {
@@ -243,6 +415,21 @@ static int multi_cpu_stop(void *data)
 				break;
 			}
 			ack_state(msdata);
+			/*
+			 * // This controls the threads on each CPU.
+			 * enum multi_stop_state {
+			 *     // Dummy starting state for thread.
+			 *     MULTI_STOP_NONE,
+			 *     // Awaiting everyone to be scheduled.
+			 *     MULTI_STOP_PREPARE,
+			 *     // Disable interrupts.
+			 *     MULTI_STOP_DISABLE_IRQ,
+			 *     // Run the function
+			 *     MULTI_STOP_RUN,
+			 *     // Exit
+			 *     MULTI_STOP_EXIT,
+			 * };
+			 */
 		} else if (curstate > MULTI_STOP_PREPARE) {
 			/*
 			 * At this stage all other CPUs we depend on must spin
@@ -251,6 +438,17 @@ static int multi_cpu_stop(void *data)
 			 */
 			touch_nmi_watchdog();
 		}
+		/*
+		 * Let the RCU core know that this CPU has gone through the scheduler,
+		 * which is a quiescent state.  This is called when the need for a
+		 * quiescent state is urgent, so we burn an atomic operation and full
+		 * memory barriers to let the RCU core know about it, regardless of what
+		 * this CPU might (or might not) do in the near future.
+		 *
+		 * We inform the RCU core by emulating a zero-duration dyntick-idle period.
+		 *
+		 * The caller must have disabled interrupts and must not be idle.
+		 */
 		rcu_momentary_dyntick_idle();
 	} while (curstate != MULTI_STOP_EXIT);
 
@@ -483,6 +681,9 @@ static int cpu_stop_should_run(unsigned int cpu)
 	return run;
 }
 
+/*
+ * struct smp_hotplug_thread cpu_stop_threads.thread_fn = cpu_stopper_thread()
+ */
 static void cpu_stopper_thread(unsigned int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
@@ -585,6 +786,24 @@ static int __init cpu_stop_init(void)
 }
 early_initcall(cpu_stop_init);
 
+/*
+ * [0] stop_machine_cpuslocked
+ * [0] stop_machine
+ * [0] mtrr_aps_init
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * [0] stop_machine_cpuslocked
+ * [0] stop_machine
+ * [0] timekeeping_notify
+ * [0] __clocksource_select
+ * [0] clocksource_done_booting
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 			    const struct cpumask *cpus)
 {
@@ -617,10 +836,37 @@ int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 	}
 
 	/* Set the initial state and stop all online cpus. */
+	/*
+	 * called by:
+	 *   - kernel/stop_machine.c|194| <<ack_state>> set_state(msdata, msdata->state + 1);
+	 *   - kernel/stop_machine.c|370| <<stop_two_cpus>> set_state(&msdata, MULTI_STOP_PREPARE);
+	 *   - kernel/stop_machine.c|653| <<stop_machine_cpuslocked>> set_state(&msdata, MULTI_STOP_PREPARE);
+	 *   - kernel/stop_machine.c|708| <<stop_machine_from_inactive_cpu>> set_state(&msdata, MULTI_STOP_PREPARE);
+	 */
 	set_state(&msdata, MULTI_STOP_PREPARE);
 	return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
 }
 
+/*
+ * called by:
+ *   - arch/arm/kernel/ftrace.c|46| <<arch_ftrace_update_code>> stop_machine(__ftrace_modify_code, &command, NULL);
+ *   - arch/arm/mm/init.c|483| <<fix_kernmem_perms>> stop_machine(__fix_kernmem_perms, NULL, NULL);
+ *   - arch/arm/mm/init.c|494| <<mark_rodata_ro>> stop_machine(__mark_rodata_ro, NULL, NULL);
+ *   - arch/arm64/kernel/alternative.c|229| <<apply_alternatives_all>> stop_machine(__apply_alternatives_multi_stop, NULL, cpu_online_mask);
+ *   - arch/arm64/kernel/cpufeature.c|2527| <<enable_cpu_capabilities>> stop_machine(cpu_enable_non_boot_scope_capabilities,
+ *   - arch/csky/kernel/ftrace.c|233| <<arch_ftrace_update_code>> stop_machine(__ftrace_modify_code, &param, cpu_online_mask);
+ *   - arch/powerpc/lib/feature-fixups.c|249| <<do_stf_barrier_fixups>> stop_machine(__do_stf_barrier_fixups, &types, NULL);
+ *   - arch/powerpc/lib/feature-fixups.c|412| <<do_entry_flush_fixups>> stop_machine(__do_entry_flush_fixups, &types, NULL);
+ *   - arch/powerpc/platforms/pseries/mobility.c|582| <<pseries_suspend>> ret = stop_machine(do_join, &info, cpu_online_mask);
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|236| <<set_mtrr>> stop_machine(mtrr_rendezvous_handler, &data, cpu_online_mask);
+ *   - drivers/char/hw_random/intel-rng.c|372| <<mod_init>> err = stop_machine(intel_rng_hw_init, intel_rng_hw, NULL);
+ *   - drivers/edac/thunderx_edac.c|431| <<thunderx_lmc_inject_ecc_write>> stop_machine(inject_ecc_fn, lmc, NULL);
+ *   - drivers/gpu/drm/i915/gt/intel_ggtt.c|372| <<bxt_vtd_ggtt_insert_page__BKL>> stop_machine(bxt_vtd_ggtt_insert_page__cb, &arg, NULL);
+ *   - drivers/gpu/drm/i915/gt/intel_ggtt.c|399| <<bxt_vtd_ggtt_insert_entries__BKL>> stop_machine(bxt_vtd_ggtt_insert_entries__cb, &arg, NULL);
+ *   - drivers/xen/manage.c|136| <<do_suspend>> err = stop_machine(xen_suspend, &si, cpumask_of(0));
+ *   - kernel/time/timekeeping.c|1492| <<timekeeping_notify>> stop_machine(change_clocksource, clock, NULL);
+ *   - kernel/trace/ftrace.c|2744| <<ftrace_run_stop_machine>> stop_machine(__ftrace_modify_code, &command, NULL);
+ */
 int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)
 {
 	int ret;
@@ -655,6 +901,10 @@ EXPORT_SYMBOL_GPL(stop_machine);
  * 0 if all executions of @fn returned 0, any non zero return value if any
  * returned non zero.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/mtrr/mtrr.c|260| <<set_mtrr_from_inactive_cpu>> stop_machine_from_inactive_cpu(mtrr_rendezvous_handler, &data, cpu_callout_mask);
+ */
 int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,
 				  const struct cpumask *cpus)
 {
@@ -668,10 +918,20 @@ int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,
 	msdata.num_threads = num_active_cpus() + 1;	/* +1 for local */
 
 	/* No proper task established and can't sleep - busy wait for lock. */
+	/*
+	 * 在以下使用stop_cpus_mutex:
+	 *   - kernel/stop_machine.c|579| <<stop_cpus>> mutex_lock(&stop_cpus_mutex);
+	 *   - kernel/stop_machine.c|581| <<stop_cpus>> mutex_unlock(&stop_cpus_mutex);
+	 *   - kernel/stop_machine.c|831| <<stop_machine_from_inactive_cpu>> while (!mutex_trylock(&stop_cpus_mutex))
+	 *   - kernel/stop_machine.c|848| <<stop_machine_from_inactive_cpu>> mutex_unlock(&stop_cpus_mutex);
+	 */
 	while (!mutex_trylock(&stop_cpus_mutex))
 		cpu_relax();
 
 	/* Schedule work on other CPUs and execute directly for local CPU */
+	/*
+	 * 设置multi_stop_data->state=MULTI_STOP_PREPARE
+	 */
 	set_state(&msdata, MULTI_STOP_PREPARE);
 	cpu_stop_init_done(&done, num_active_cpus());
 	queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 2cd902592fc1..fcb00e8a592c 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -727,6 +727,35 @@ static struct clocksource *clocksource_find_best(bool oneshot, bool skipcur)
 	return NULL;
 }
 
+/*
+ * [    0.383250] clocksource: Switched to clocksource kvm-clock
+ * [    0.383953] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 5.13.0+ #2
+ * [    0.384161] Call Trace:
+ * [    0.384161]  dump_stack+0x76/0x94
+ * [    0.384161]  __clocksource_select.cold+0x3e/0x65
+ * [    0.384161]  ? boot_override_clock+0x3c/0x3c
+ * [    0.384161]  clocksource_done_booting+0x2e/0x3d
+ * [    0.384161]  do_one_initcall+0x3f/0x1c0
+ * [    0.384161]  kernel_init_freeable+0x1db/0x223
+ * [    0.384161]  ? rest_init+0xaf/0xaf
+ * [    0.384161]  kernel_init+0x5/0x101
+ * [    0.384161]  ret_from_fork+0x22/0x30
+ *
+ * [    0.513750] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x30e659ec535, max_idle_ns: 440795283326 ns
+ * [    0.515042] clocksource: Switched to clocksource tsc
+ * [    0.515690] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 5.13.0+ #2
+ * [    0.516687] Call Trace:
+ * [    0.516687]  dump_stack+0x76/0x94
+ * [    0.516687]  __clocksource_select.cold+0x3e/0x65
+ * [    0.516687]  __clocksource_register_scale+0x107/0x190
+ * [    0.516687]  ? tsc_setup+0x61/0x61
+ * [    0.516687]  init_tsc_clocksource+0x8b/0xb6
+ * [    0.516687]  do_one_initcall+0x3f/0x1c0
+ * [    0.516687]  kernel_init_freeable+0x1db/0x223
+ * [    0.516687]  ? rest_init+0xaf/0xaf
+ * [    0.516687]  kernel_init+0x5/0x101
+ * [    0.516687]  ret_from_fork+0x22/0x30
+ */
 static void __clocksource_select(bool skipcur)
 {
 	bool oneshot = tick_oneshot_mode_active();
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 8a364aa9881a..7d359593b30a 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -657,6 +657,12 @@ static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
  * pvclock_gtod_register_notifier - register a pvclock timedata update listener
  * @nb: Pointer to the notifier block to register
  */
+/*
+ * called by:
+ *   - arch/arm/xen/enlighten.c|393| <<xen_guest_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|8528| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/xen/time.c|520| <<xen_time_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ */
 int pvclock_gtod_register_notifier(struct notifier_block *nb)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1754,6 +1760,11 @@ void timekeeping_inject_sleeptime64(const struct timespec64 *delta)
 /**
  * timekeeping_resume - Resumes the generic timekeeping subsystem.
  */
+/*
+ * 在以下使用timekeeping_resume():
+ *   - struct syscore_ops timekeeping_syscore_ops.resume = timekeeping_resume()
+ *   - kernel/time/tick-common.c|549| <<tick_unfreeze>> timekeeping_resume();
+ */
 void timekeeping_resume(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index 6f5f78885ab4..cc364cb43033 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -59,8 +59,20 @@
 #include "internal.h"
 #include "ras/ras_event.h"
 
+/*
+ * 在以下使用sysctl_memory_failure_early_kill:
+ *   - kernel/sysctl.c|3072| <<global>> .data = &sysctl_memory_failure_early_kill,
+ *   - kernel/sysctl.c|3073| <<global>> .maxlen = sizeof(sysctl_memory_failure_early_kill),
+ *   - mm/memory-failure.c|436| <<find_early_kill_thread>> if (sysctl_memory_failure_early_kill)
+ */
 int sysctl_memory_failure_early_kill __read_mostly = 0;
 
+/*
+ * 在以下使用sysctl_memory_failure_recovery:
+ *   - kernel/sysctl.c|3081| <<global>> .data = &sysctl_memory_failure_recovery,
+ *   - kernel/sysctl.c|3082| <<global>> .maxlen = sizeof(sysctl_memory_failure_recovery),
+ *   - mm/memory-failure.c|1451| <<memory_failure>> if (!sysctl_memory_failure_recovery)
+ */
 int sysctl_memory_failure_recovery __read_mostly = 1;
 
 atomic_long_t num_poisoned_pages __read_mostly = ATOMIC_LONG_INIT(0);
@@ -1437,6 +1449,19 @@ static int memory_failure_dev_pagemap(unsigned long pfn, int flags,
  * Must run in process context (e.g. a work queue) with interrupts
  * enabled and no spinlocks hold.
  */
+/*
+ * called by:
+ *   - arch/parisc/kernel/pdt.c|331| <<pdt_mainloop>> memory_failure(pde >> PAGE_SHIFT, 0);
+ *   - arch/powerpc/kernel/mce.c|313| <<machine_process_ue_event>> memory_failure(pfn, 0);
+ *   - arch/powerpc/platforms/powernv/opal-memory-errors.c|50| <<handle_memory_error_event>> memory_failure(paddr_start >> PAGE_SHIFT, 0);
+ *   - arch/x86/kernel/cpu/mce/core.c|649| <<uc_decode_notifier>> if (!memory_failure(pfn, 0)) {
+ *   - arch/x86/kernel/cpu/mce/core.c|1266| <<kill_me_maybe>> if (!memory_failure(p->mce_addr >> PAGE_SHIFT, flags) &&
+ *   - arch/x86/kernel/cpu/mce/core.c|1455| <<memory_failure>> int memory_failure(unsigned long pfn, int flags)
+ *   - drivers/base/memory.c|549| <<hard_offline_page_store>> ret = memory_failure(pfn, 0);
+ *   - mm/hwpoison-inject.c|51| <<hwpoison_inject>> return memory_failure(pfn, 0);
+ *   - mm/madvise.c|911| <<madvise_inject_error>> ret = memory_failure(pfn, MF_COUNT_INCREASED);
+ *   - mm/memory-failure.c|1694| <<memory_failure_work_func>> memory_failure(entry.pfn, entry.flags);
+ */
 int memory_failure(unsigned long pfn, int flags)
 {
 	struct page *p;
diff --git a/net/core/dev.c b/net/core/dev.c
index ef8cf7619baf..966d0dc386f1 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -1203,6 +1203,10 @@ EXPORT_SYMBOL(dev_valid_name);
  *	Returns the number of the unit assigned or a negative errno code.
  */
 
+/*
+ * called by:
+ *   - net/core/dev.c|1279| <<dev_alloc_name_ns>> ret = __dev_alloc_name(net, name, buf);
+ */
 static int __dev_alloc_name(struct net *net, const char *name, char *buf)
 {
 	int i = 0;
@@ -1268,6 +1272,11 @@ static int __dev_alloc_name(struct net *net, const char *name, char *buf)
 	return -ENFILE;
 }
 
+/*
+ * called by:
+ *   - net/core/dev.c|1301| <<dev_alloc_name>> return dev_alloc_name_ns(dev_net(dev), dev, name);
+ *   - net/core/dev.c|1314| <<dev_get_valid_name>> return dev_alloc_name_ns(net, dev, name);
+ */
 static int dev_alloc_name_ns(struct net *net,
 			     struct net_device *dev,
 			     const char *name)
@@ -1328,6 +1337,22 @@ static int dev_get_valid_name(struct net *net, struct net_device *dev,
  *	Change name of a device, can pass format strings "eth%d".
  *	for wildcarding.
  */
+/*
+ * CPU: 2 PID: 256 Comm: systemd-udevd
+ * [0] Call Trace:
+ * [0]  dev_change_name
+ * [0]  do_setlink
+ * [0]  rtnl_setlink
+ * [0]  rtnetlink_rcv_msg
+ * [0]  netlink_rcv_skb
+ * [0]  netlink_unicast
+ * [0]  netlink_sendmsg
+ * [0]  sock_sendmsg
+ * [0]  __sys_sendto
+ * [0]  __x64_sys_sendto
+ * [0]  do_syscall_64
+ * [0]  entry_SYSCALL_64_after_hwframe
+ */
 int dev_change_name(struct net_device *dev, const char *newname)
 {
 	unsigned char old_assign_type;
@@ -1653,6 +1678,11 @@ int dev_open(struct net_device *dev, struct netlink_ext_ack *extack)
 }
 EXPORT_SYMBOL(dev_open);
 
+/*
+ * called by:
+ *   - net/core/dev.c|1705| <<__dev_close>> __dev_close_many(&single);
+ *   - net/core/dev.c|1718| <<dev_close_many>> __dev_close_many(head);
+ */
 static void __dev_close_many(struct list_head *head)
 {
 	struct net_device *dev;
@@ -1697,6 +1727,10 @@ static void __dev_close_many(struct list_head *head)
 	}
 }
 
+/*
+ * called by:
+ *   - net/core/dev.c|8803| <<__dev_change_flags>> __dev_close(dev);
+ */
 static void __dev_close(struct net_device *dev)
 {
 	LIST_HEAD(single);
@@ -3204,6 +3238,17 @@ EXPORT_SYMBOL(__dev_kfree_skb_any);
  */
 void netif_device_detach(struct net_device *dev)
 {
+	/*
+	 * 在以下使用__LINK_STATE_PRESENT:
+	 *   - drivers/net/ethernet/amd/sun3lance.c|412| <<lance_probe>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - include/linux/netdevice.h|4323| <<netif_device_present>> return test_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - net/8021q/vlan_dev.c|564| <<vlan_dev_init>> (1<<__LINK_STATE_PRESENT);
+	 *   - net/bluetooth/6lowpan.c|732| <<setup_netdev>> set_bit(__LINK_STATE_PRESENT, &netdev->state);
+	 *   - net/core/dev.c|3207| <<netif_device_detach>> if (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&
+	 *   - net/core/dev.c|3222| <<netif_device_attach>> if (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&
+	 *   - net/core/dev.c|10318| <<register_netdevice>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - net/core/dev.c|10393| <<init_dummy_netdev>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 */
 	if (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&
 	    netif_running(dev)) {
 		netif_tx_stop_all_queues(dev);
@@ -3219,6 +3264,17 @@ EXPORT_SYMBOL(netif_device_detach);
  */
 void netif_device_attach(struct net_device *dev)
 {
+	/*
+	 * 在以下使用__LINK_STATE_PRESENT:
+	 *   - drivers/net/ethernet/amd/sun3lance.c|412| <<lance_probe>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - include/linux/netdevice.h|4323| <<netif_device_present>> return test_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - net/8021q/vlan_dev.c|564| <<vlan_dev_init>> (1<<__LINK_STATE_PRESENT);
+	 *   - net/bluetooth/6lowpan.c|732| <<setup_netdev>> set_bit(__LINK_STATE_PRESENT, &netdev->state);
+	 *   - net/core/dev.c|3207| <<netif_device_detach>> if (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&
+	 *   - net/core/dev.c|3222| <<netif_device_attach>> if (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&
+	 *   - net/core/dev.c|10318| <<register_netdevice>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 *   - net/core/dev.c|10393| <<init_dummy_netdev>> set_bit(__LINK_STATE_PRESENT, &dev->state);
+	 */
 	if (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&
 	    netif_running(dev)) {
 		netif_tx_wake_all_queues(dev);
@@ -3851,6 +3907,10 @@ static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
 
 	qdisc_calculate_pkt_len(skb, q);
 
+	/*
+	 * q->enqueue的例子: pfifo_fast_enqueue()
+	 */
+
 	if (q->flags & TCQ_F_NOLOCK) {
 		rc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;
 		if (likely(!netif_xmit_frozen_or_stopped(txq)))
@@ -4210,6 +4270,9 @@ static int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)
 	q = rcu_dereference_bh(txq->qdisc);
 
 	trace_net_dev_queue(skb);
+	/*
+	 * q->enqueue的例子: pfifo_fast_enqueue()
+	 */
 	if (q->enqueue) {
 		rc = __dev_xmit_skb(skb, q, dev, txq);
 		goto out;
@@ -4230,6 +4293,17 @@ static int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)
 	if (dev->flags & IFF_UP) {
 		int cpu = smp_processor_id(); /* ok because BHs are off */
 
+		/*
+		 * 在以下使用netdev_queue->xmit_lock_owner:
+		 *   - include/linux/netdevice.h|4387| <<__netif_tx_lock>> txq->xmit_lock_owner = cpu;
+		 *   - include/linux/netdevice.h|4404| <<__netif_tx_lock_bh>> txq->xmit_lock_owner = smp_processor_id();
+		 *   - include/linux/netdevice.h|4411| <<__netif_tx_trylock>> txq->xmit_lock_owner = smp_processor_id();
+		 *   - include/linux/netdevice.h|4417| <<__netif_tx_unlock>> txq->xmit_lock_owner = -1;
+		 *   - include/linux/netdevice.h|4423| <<__netif_tx_unlock_bh>> txq->xmit_lock_owner = -1;
+		 *   - include/linux/netdevice.h|4429| <<txq_trans_update>> if (txq->xmit_lock_owner != -1)
+		 *   - net/core/dev.c|4233| <<__dev_queue_xmit>> if (txq->xmit_lock_owner != cpu) {
+		 *   - net/core/dev.c|10114| <<netdev_init_one_queue>> queue->xmit_lock_owner = -1;
+		 */
 		if (txq->xmit_lock_owner != cpu) {
 			if (dev_xmit_recursion())
 				goto recursion_alert;
@@ -4286,6 +4360,11 @@ int dev_queue_xmit_accel(struct sk_buff *skb, struct net_device *sb_dev)
 }
 EXPORT_SYMBOL(dev_queue_xmit_accel);
 
+/*
+ * called by:
+ *   - include/linux/netdevice.h|2976| <<dev_direct_xmit>> ret = __dev_direct_xmit(skb, queue_id);
+ *   - net/xdp/xsk.c|579| <<xsk_generic_xmit>> err = __dev_direct_xmit(skb, xs->queue_id);
+ */
 int __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)
 {
 	struct net_device *dev = skb->dev;
@@ -10438,6 +10517,10 @@ int netdev_unregister_timeout_secs __read_mostly = 10;
  * We can get stuck here if buggy protocols don't correctly
  * call dev_put.
  */
+/*
+ * called by:
+ *   - net/core/dev.c|10563| <<netdev_run_todo>> netdev_wait_allrefs(dev);
+ */
 static void netdev_wait_allrefs(struct net_device *dev)
 {
 	unsigned long rebroadcast_time, warning_time;
@@ -10519,6 +10602,10 @@ static void netdev_wait_allrefs(struct net_device *dev)
  * We must not return until all unregister events added during
  * the interval the lock was held have been completed.
  */
+/*
+ * called by:
+ *   - net/core/rtnetlink.c|112| <<rtnl_unlock>> netdev_run_todo();
+ */
 void netdev_run_todo(void)
 {
 	struct list_head list;
@@ -10743,6 +10830,23 @@ void netdev_freemem(struct net_device *dev)
  * and performs basic initialization.  Also allocates subqueue structs
  * for each queue on the device.
  */
+/*
+ * [0] alloc_netdev_mqs
+ * [0] virtnet_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] virtio_net_driver_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
 		unsigned char name_assign_type,
 		void (*setup)(struct net_device *),
diff --git a/net/sched/sch_generic.c b/net/sched/sch_generic.c
index fc8b56bcabf3..f10d90451df1 100644
--- a/net/sched/sch_generic.c
+++ b/net/sched/sch_generic.c
@@ -435,6 +435,10 @@ unsigned long dev_trans_start(struct net_device *dev)
 }
 EXPORT_SYMBOL(dev_trans_start);
 
+/*
+ * 在以下使用dev_watchdog():
+ *   - net/sched/sch_generic.c|1345| <<dev_init_scheduler>> timer_setup(&dev->watchdog_timer, dev_watchdog, 0);
+ */
 static void dev_watchdog(struct timer_list *t)
 {
 	struct net_device *dev = from_timer(dev, t, watchdog_timer);
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index a2b732cf96ea..2dbf1bc3e3ba 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -1636,6 +1636,15 @@ void vcpu_regs_get(struct kvm_vm *vm, uint32_t vcpuid, struct kvm_regs *regs)
  * Sets the regs of the VCPU specified by vcpuid to the values
  * given by regs.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|180| <<vm_vcpu_add_default>> vcpu_regs_set(vm, vcpuid, &regs);
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|208| <<vcpu_args_set>> vcpu_regs_set(vm, vcpuid, &regs);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|598| <<vm_vcpu_add_default>> vcpu_regs_set(vm, vcpuid, &regs);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|927| <<vcpu_args_set>> vcpu_regs_set(vm, vcpuid, &regs);
+ *   - tools/testing/selftests/kvm/x86_64/debug_regs.c|60| <<SET_RIP>> vcpu_regs_set(vm, VCPU_ID, &regs); \
+ *   - tools/testing/selftests/kvm/x86_64/sync_regs_test.c|206| <<main>> vcpu_regs_set(vm, VCPU_ID, &regs);
+ */
 void vcpu_regs_set(struct kvm_vm *vm, uint32_t vcpuid, struct kvm_regs *regs)
 {
 	struct vcpu *vcpu = vcpu_find(vm, vcpuid);
diff --git a/tools/testing/selftests/kvm/lib/x86_64/ucall.c b/tools/testing/selftests/kvm/lib/x86_64/ucall.c
index a3489973e290..4208d1ba6f4f 100644
--- a/tools/testing/selftests/kvm/lib/x86_64/ucall.c
+++ b/tools/testing/selftests/kvm/lib/x86_64/ucall.c
@@ -35,6 +35,41 @@ void ucall(uint64_t cmd, int nargs, ...)
 		: : [port] "d" (UCALL_PIO_PORT), "D" (&uc) : "rax", "memory");
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/demand_paging_test.c|63| <<vcpu_worker>> if (get_ucall(vm, vcpu_id, NULL) != UCALL_SYNC) {
+ *   - tools/testing/selftests/kvm/dirty_log_perf_test.c|58| <<vcpu_worker>> TEST_ASSERT(get_ucall(vm, vcpu_id, NULL) == UCALL_SYNC,
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|264| <<default_after_vcpu_run>> TEST_ASSERT(get_ucall(vm, VCPU_ID, NULL) == UCALL_SYNC,
+ *   - tools/testing/selftests/kvm/dirty_log_test.c|378| <<dirty_ring_after_vcpu_run>> if (get_ucall(vm, VCPU_ID, NULL) == UCALL_SYNC) {
+ *   - tools/testing/selftests/kvm/kvm_page_table_test.c|220| <<vcpu_worker>> TEST_ASSERT(get_ucall(vm, vcpu_id, NULL) == UCALL_SYNC,
+ *   - tools/testing/selftests/kvm/memslot_modification_stress_test.c|56| <<vcpu_worker>> if (get_ucall(vm, vcpu_id, NULL) == UCALL_SYNC)
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|142| <<vcpu_worker>> cmd = get_ucall(vm->vm, VCPU_ID, &uc);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|73| <<vcpu_worker>> cmd = get_ucall(vm, VCPU_ID, &uc);
+ *   - tools/testing/selftests/kvm/steal_time.c|286| <<run_vcpu>> switch (get_ucall(vm, vcpuid, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/cr4_cpuid_sync_test.c|95| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/debug_regs.c|195| <<main>> cmd = get_ucall(vm, VCPU_ID, &uc);
+ *   - tools/testing/selftests/kvm/x86_64/evmcs_test.c|169| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/get_cpuid_test.c|127| <<run_vcpu>> switch (get_ucall(vm, vcpuid, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_clock.c|238| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/kvm_pv_test.c|191| <<enter_guest>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/platform_info_test.c|59| <<test_msr_platform_info_enabled>> get_ucall(vm, VCPU_ID, &uc);
+ *   - tools/testing/selftests/kvm/x86_64/set_boot_cpu_id.c|63| <<run_vcpu>> switch (get_ucall(vm, vcpuid, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/state_test.c|191| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/svm_vmcall_test.c|61| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c|86| <<run_vcpu>> switch (get_ucall(vm, vcpuid, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/userspace_msr_exit_test.c|412| <<check_for_guest_assert>> get_ucall(vm, VCPU_ID, &uc) == UCALL_ABORT) {
+ *   - tools/testing/selftests/kvm/x86_64/userspace_msr_exit_test.c|496| <<process_ucall_done>> TEST_ASSERT(get_ucall(vm, VCPU_ID, &uc) == UCALL_DONE,
+ *   - tools/testing/selftests/kvm/x86_64/userspace_msr_exit_test.c|513| <<process_ucall>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/userspace_msr_exit_test.c|623| <<handle_ucall>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/vmx_apic_access_test.c|124| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/vmx_close_while_nested_test.c|78| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/vmx_dirty_log_test.c|125| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/vmx_preemption_timer_test.c|193| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|150| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/xapic_ipi_test.c|260| <<vcpu_thread>> if (get_ucall(params->vm, params->vcpu_id, &uc) == UCALL_ABORT) {
+ *   - tools/testing/selftests/kvm/x86_64/xen_shinfo_test.c|203| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ *   - tools/testing/selftests/kvm/x86_64/xen_vmcall_test.c|134| <<main>> switch (get_ucall(vm, VCPU_ID, &uc)) {
+ */
 uint64_t get_ucall(struct kvm_vm *vm, uint32_t vcpu_id, struct ucall *uc)
 {
 	struct kvm_run *run = vcpu_state(vm, vcpu_id);
diff --git a/tools/testing/selftests/kvm/steal_time.c b/tools/testing/selftests/kvm/steal_time.c
index fcc840088c91..a6b377d7270f 100644
--- a/tools/testing/selftests/kvm/steal_time.c
+++ b/tools/testing/selftests/kvm/steal_time.c
@@ -18,11 +18,61 @@
 #include "kvm_util.h"
 #include "processor.h"
 
+/*
+ * commit 94c4b76b88d40f9062dc32ff2fff551ae1791c1e
+ * Author: Andrew Jones <drjones@redhat.com>
+ * Date:   Fri Mar 13 16:56:44 2020 +0100
+ *
+ * KVM: selftests: Introduce steal-time test
+ *
+ * The steal-time test confirms what is reported to the guest as stolen
+ * time is consistent with the run_delay reported for the VCPU thread
+ * on the host. Both x86_64 and AArch64 have the concept of steal/stolen
+ * time so this test is introduced for both architectures.
+ *
+ * While adding the test we ensure .gitignore has all tests listed
+ * (it was missing s390x/resets) and that the Makefile has all tests
+ * listed in alphabetical order (not really necessary, but it almost
+ * was already...). We also extend the common API with a new num-guest-
+ * pages call and a new timespec call.
+ *
+ * Signed-off-by: Andrew Jones <drjones@redhat.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
+
 #define NR_VCPUS		4
 #define ST_GPA_BASE		(1 << 30)
 #define MIN_RUN_DELAY_NS	200000UL
 
+/*
+ * 在以下使用st_gva[NR_VCPUS]:
+ *   - tools/testing/selftests/kvm/steal_time.c|61| <<guest_code>> struct kvm_steal_time *st = st_gva[cpu];
+ *   - tools/testing/selftests/kvm/steal_time.c|64| <<guest_code>> GUEST_ASSERT(rdmsr(MSR_KVM_STEAL_TIME) == ((uint64_t)st_gva[cpu] | KVM_MSR_ENABLED));
+ *   - tools/testing/selftests/kvm/steal_time.c|98| <<steal_time_init>> st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
+ *   - tools/testing/selftests/kvm/steal_time.c|99| <<steal_time_init>> sync_global_to_guest(vm, st_gva[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|101| <<steal_time_init>> ret = _vcpu_set_msr(vm, i, MSR_KVM_STEAL_TIME, (ulong)st_gva[i] | KVM_STEAL_RESERVED_MASK);
+ *   - tools/testing/selftests/kvm/steal_time.c|104| <<steal_time_init>> vcpu_set_msr(vm, i, MSR_KVM_STEAL_TIME, (ulong)st_gva[i] | KVM_MSR_ENABLED);
+ *   - tools/testing/selftests/kvm/steal_time.c|114| <<steal_time_dump>> struct kvm_steal_time *st = addr_gva2hva(vm, (ulong)st_gva[vcpuid]);
+ *   - tools/testing/selftests/kvm/steal_time.c|181| <<guest_code>> GUEST_ASSERT(status == (ulong)st_gva[cpu]);
+ *   - tools/testing/selftests/kvm/steal_time.c|217| <<steal_time_init>> st_gva[i] = (void *)(ST_GPA_BASE + i * STEAL_TIME_SIZE);
+ *   - tools/testing/selftests/kvm/steal_time.c|218| <<steal_time_init>> sync_global_to_guest(vm, st_gva[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|220| <<steal_time_init>> st_ipa = (ulong)st_gva[i] | 1;
+ *   - tools/testing/selftests/kvm/steal_time.c|224| <<steal_time_init>> st_ipa = (ulong)st_gva[i];
+ *   - tools/testing/selftests/kvm/steal_time.c|235| <<steal_time_dump>> struct st_time *st = addr_gva2hva(vm, (ulong)st_gva[vcpuid]);
+ */
 static void *st_gva[NR_VCPUS];
+/*
+ * 在以下使用guest_stolen_time[NR_VCPUS]:
+ *   - tools/testing/selftests/kvm/steal_time.c|58| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+ *   - tools/testing/selftests/kvm/steal_time.c|65| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+ *   - tools/testing/selftests/kvm/steal_time.c|171| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+ *   - tools/testing/selftests/kvm/steal_time.c|175| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+ *   - tools/testing/selftests/kvm/steal_time.c|324| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|325| <<main>> stolen_time = guest_stolen_time[i];
+ *   - tools/testing/selftests/kvm/steal_time.c|345| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|346| <<main>> stolen_time = guest_stolen_time[i] - stolen_time;
+ *   - tools/testing/selftests/kvm/steal_time.c|353| <<main>> guest_stolen_time[i], stolen_time);
+ */
 static uint64_t guest_stolen_time[NR_VCPUS];
 
 #if defined(__x86_64__)
@@ -30,8 +80,18 @@ static uint64_t guest_stolen_time[NR_VCPUS];
 /* steal_time must have 64-byte alignment */
 #define STEAL_TIME_SIZE		((sizeof(struct kvm_steal_time) + 63) & ~63)
 
+/*
+ * 这是x86的, called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|50| <<guest_code>> check_status(st);
+ *   - tools/testing/selftests/kvm/steal_time.c|53| <<guest_code>> check_status(st);
+ *   - tools/testing/selftests/kvm/steal_time.c|56| <<guest_code>> check_status(st);
+ *   - tools/testing/selftests/kvm/steal_time.c|59| <<guest_code>> check_status(st);
+ */
 static void check_status(struct kvm_steal_time *st)
 {
+	/*
+	 * 根据record_steal_time(), version单数说明正在写, 双数说明写完了
+	 */
 	GUEST_ASSERT(!(READ_ONCE(st->version) & 1));
 	GUEST_ASSERT(READ_ONCE(st->flags) == 0);
 	GUEST_ASSERT(READ_ONCE(st->preempted) == 0);
@@ -48,6 +108,18 @@ static void guest_code(int cpu)
 	GUEST_SYNC(0);
 
 	check_status(st);
+	/*
+	 * 在以下使用guest_stolen_time[NR_VCPUS]:
+	 *   - tools/testing/selftests/kvm/steal_time.c|58| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+	 *   - tools/testing/selftests/kvm/steal_time.c|65| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->steal);
+	 *   - tools/testing/selftests/kvm/steal_time.c|171| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+	 *   - tools/testing/selftests/kvm/steal_time.c|175| <<guest_code>> WRITE_ONCE(guest_stolen_time[cpu], st->st_time);
+	 *   - tools/testing/selftests/kvm/steal_time.c|324| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+	 *   - tools/testing/selftests/kvm/steal_time.c|325| <<main>> stolen_time = guest_stolen_time[i];
+	 *   - tools/testing/selftests/kvm/steal_time.c|345| <<main>> sync_global_from_guest(vm, guest_stolen_time[i]);
+	 *   - tools/testing/selftests/kvm/steal_time.c|346| <<main>> stolen_time = guest_stolen_time[i] - stolen_time;
+	 *   - tools/testing/selftests/kvm/steal_time.c|353| <<main>> guest_stolen_time[i], stolen_time);
+	 */
 	WRITE_ONCE(guest_stolen_time[cpu], st->steal);
 	version = READ_ONCE(st->version);
 	check_status(st);
@@ -60,6 +132,10 @@ static void guest_code(int cpu)
 	GUEST_DONE();
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|441| <<main>> steal_time_init(vm);
+ */
 static void steal_time_init(struct kvm_vm *vm)
 {
 	int i;
@@ -82,12 +158,31 @@ static void steal_time_init(struct kvm_vm *vm)
 		ret = _vcpu_set_msr(vm, i, MSR_KVM_STEAL_TIME, (ulong)st_gva[i] | KVM_STEAL_RESERVED_MASK);
 		TEST_ASSERT(ret == 0, "Bad GPA didn't fail");
 
+		/*
+		 * 核心是通过ioctl的KVM_SET_MSRS
+		 */
 		vcpu_set_msr(vm, i, MSR_KVM_STEAL_TIME, (ulong)st_gva[i] | KVM_MSR_ENABLED);
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|357| <<main>> steal_time_dump(vm, i);
+ *
+ * 把GVA的st_gva[vcpuid]给map到HVA, 然后dump steal的数据
+ */
 static void steal_time_dump(struct kvm_vm *vm, uint32_t vcpuid)
 {
+	/*
+	 * struct kvm_steal_time {
+	 *     __u64 steal;
+	 *     __u32 version;
+	 *     __u32 flags;
+	 *     __u8  preempted;
+	 *     __u8  u8_pad[3];
+	 *     __u32 pad[11];
+	 * };
+	 */
 	struct kvm_steal_time *st = addr_gva2hva(vm, (ulong)st_gva[vcpuid]);
 	int i;
 
@@ -218,7 +313,35 @@ static void steal_time_dump(struct kvm_vm *vm, uint32_t vcpuid)
 }
 
 #endif
+/*
+ * 上面的是aarch64
+ */
 
+/*
+ * About the steal time, I did confirm recently that it was derived from
+ * hypervisor (linux) current->sched_info.run_delay as below.
+ *
+ * st->steal += current->sched_info.run_delay - vcpu->arch.st.last_steal;
+ * vcpu->arch.st.last_steal = current->sched_info.run_delay;
+ *
+ * From hypervisor side, the current->sched_info.run_delay for a pid can be
+ * obtained from procfs.
+ * # cat /proc/12942/schedstat
+ * 391183709511 5114834787 15215244
+ *
+ * The above three values are:
+ * 1. task->se.sum_exec_runtime
+ * 2. task->sched_info.run_delay
+ * 3. task->sched_info.pcount, respectively.
+ *
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|342| <<main>> run_delay = get_run_delay();
+ *   - tools/testing/selftests/kvm/steal_time.c|348| <<main>> run_delay = get_run_delay();
+ *   - tools/testing/selftests/kvm/steal_time.c|352| <<main>> while (get_run_delay() - run_delay < MIN_RUN_DELAY_NS);
+ *   - tools/testing/selftests/kvm/steal_time.c|354| <<main>> run_delay = get_run_delay() - run_delay;
+ *
+ * 返回/proc/12942/schedstat中的第二项, current->sched_info.run_delay
+ */
 static long get_run_delay(void)
 {
 	char path[64];
@@ -233,6 +356,10 @@ static long get_run_delay(void)
 	return val[1];
 }
 
+/*
+ * 在以下使用do_steal_time():
+ *   - tools/testing/selftests/kvm/steal_time.c|349| <<main>> pthread_create(&thread, &attr, do_steal_time, NULL);
+ */
 static void *do_steal_time(void *arg)
 {
 	struct timespec ts, stop;
@@ -249,6 +376,12 @@ static void *do_steal_time(void *arg)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|336| <<main>> run_vcpu(vm, i);
+ *   - tools/testing/selftests/kvm/steal_time.c|339| <<main>> run_vcpu(vm, i);
+ *   - tools/testing/selftests/kvm/steal_time.c|360| <<main>> run_vcpu(vm, i);
+ */
 static void run_vcpu(struct kvm_vm *vm, uint32_t vcpuid)
 {
 	struct ucall uc;
@@ -293,8 +426,18 @@ int main(int ac, char **av)
 
 	/* Create a one VCPU guest and an identity mapped memslot for the steal time structure */
 	vm = vm_create_default(0, 0, guest_code);
+	/*
+	 * x86_64下的STEAL_TIME_SIZE:
+	 * #define STEAL_TIME_SIZE ((sizeof(struct kvm_steal_time) + 63) & ~63)
+	 */
 	gpages = vm_calc_num_guest_pages(VM_MODE_DEFAULT, STEAL_TIME_SIZE * NR_VCPUS);
+	/*
+	 * 把gpages个page给map到VM的ST_GPA_BASE地址
+	 */
 	vm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, ST_GPA_BASE, 1, gpages, 0);
+	/*
+	 * Map a range of VM virtual address to the VM's physical address
+	 */
 	virt_map(vm, ST_GPA_BASE, ST_GPA_BASE, gpages, 0);
 	ucall_init(vm, NULL);
 
@@ -306,6 +449,12 @@ int main(int ac, char **av)
 
 	/* Run test on each VCPU */
 	for (i = 0; i < NR_VCPUS; ++i) {
+		/*
+		 * run_vcpu()一共被调用3次:
+		 *   - tools/testing/selftests/kvm/steal_time.c|336| <<main>> run_vcpu(vm, i);
+		 *   - tools/testing/selftests/kvm/steal_time.c|339| <<main>> run_vcpu(vm, i);
+		 *   - tools/testing/selftests/kvm/steal_time.c|360| <<main>> run_vcpu(vm, i);
+		 */
 		/* First VCPU run initializes steal-time */
 		run_vcpu(vm, i);
 
diff --git a/tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c b/tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c
index e357d8e222d4..a74c45b0379d 100644
--- a/tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c
+++ b/tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c
@@ -11,9 +11,20 @@
 
 #define VCPU_ID 0
 
+/*
+ * 1 << 30 = 1073741824
+ */
 #define UNITY                  (1ull << 30)
 #define HOST_ADJUST            (UNITY * 64)
+/*
+ * 1073741824 * 4 = 4294967296
+ */
 #define GUEST_STEP             (UNITY * 4)
+/*
+ * UNITY  = 0x40000000
+ * -UNITY = 0xffffffffc0000000
+ * 这样ROUND能够保证前面的是相等的
+ */
 #define ROUND(x)               ((x + UNITY / 2) & -UNITY)
 #define rounded_rdmsr(x)       ROUND(rdmsr(x))
 #define rounded_host_rdmsr(x)  ROUND(vcpu_get_msr(vm, 0, x))
@@ -31,6 +42,9 @@ static void guest_code(void)
 {
 	u64 val = 0;
 
+	/*
+	 * 用rdmsr读取MSR_IA32_TSC
+	 */
 	GUEST_ASSERT_EQ(rounded_rdmsr(MSR_IA32_TSC), val);
 	GUEST_ASSERT_EQ(rounded_rdmsr(MSR_IA32_TSC_ADJUST), val);
 
@@ -75,6 +89,14 @@ static void guest_code(void)
 	GUEST_DONE();
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c|116| <<main>> run_vcpu(vm, VCPU_ID, 1);
+ *   - tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c|122| <<main>> run_vcpu(vm, VCPU_ID, 2);
+ *   - tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c|134| <<main>> run_vcpu(vm, VCPU_ID, 3);
+ *   - tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c|150| <<main>> run_vcpu(vm, VCPU_ID, 4);
+ *   - tools/testing/selftests/kvm/x86_64/tsc_msrs_test.c|159| <<main>> run_vcpu(vm, VCPU_ID, 5);
+ */
 static void run_vcpu(struct kvm_vm *vm, uint32_t vcpuid, int stage)
 {
 	struct ucall uc;
@@ -109,6 +131,9 @@ int main(void)
 	vm = vm_create_default(VCPU_ID, 0, guest_code);
 
 	val = 0;
+	/*
+	 * 用ioctl的KVM_GET_MSRS获取MSR_IA32_TSC
+	 */
 	ASSERT_EQ(rounded_host_rdmsr(MSR_IA32_TSC), val);
 	ASSERT_EQ(rounded_host_rdmsr(MSR_IA32_TSC_ADJUST), val);
 
diff --git a/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c b/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c
index 7e33a350b053..8e4b85dc664d 100644
--- a/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c
+++ b/tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c
@@ -60,15 +60,32 @@ struct kvm_single_msr {
 /* The virtual machine object. */
 static struct kvm_vm *vm;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|77| <<l2_guest_code>> check_ia32_tsc_adjust(-2 * TSC_ADJUST_VALUE);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|92| <<l1_guest_code>> check_ia32_tsc_adjust(-1 * TSC_ADJUST_VALUE);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|111| <<l1_guest_code>> check_ia32_tsc_adjust(-1 * TSC_ADJUST_VALUE);
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|117| <<l1_guest_code>> check_ia32_tsc_adjust(-2 * TSC_ADJUST_VALUE);
+ */
 static void check_ia32_tsc_adjust(int64_t max)
 {
 	int64_t adjust;
 
+	/*
+	 * 关于msr_tsc_adjust
+	 * 如果想要修改tsc的counter, 可以直接写入msr_tsc.
+	 * 然而这样很难保证所有的CPU是同步的 (因为每个CPU都要往msr_tsc写一个absolute value)
+	 * 使用msr_tsc_adjust就可以解决这个问题, 因为为每个CPU写入的是offset, 不是absolute value
+	 */
 	adjust = rdmsr(MSR_IA32_TSC_ADJUST);
 	GUEST_SYNC(adjust);
 	GUEST_ASSERT(adjust <= max);
 }
 
+/*
+ * 在以下使用l2_guest_code():
+ *   - tools/testing/selftests/kvm/x86_64/vmx_tsc_adjust_test.c|98| <<l1_guest_code>> prepare_vmcs(vmx_pages, l2_guest_code, &l2_guest_stack[L2_GUEST_STACK_SIZE]);
+ */
 static void l2_guest_code(void)
 {
 	uint64_t l1_tsc = rdtsc() - TSC_OFFSET_VALUE;
@@ -80,6 +97,24 @@ static void l2_guest_code(void)
 	__asm__ __volatile__("vmcall");
 }
 
+/*
+ * According to the SDM, "if an execution of WRMSR to the
+ * IA32_TIME_STAMP_COUNTER MSR adds (or subtracts) value X from the TSC,
+ * the logical processor also adds (or subtracts) value X from the
+ * IA32_TSC_ADJUST MSR.
+ *
+ * Note that when L1 doesn't intercept writes to IA32_TSC, a
+ * WRMSR(IA32_TSC) from L2 sets L1's TSC value, not L2's perceived TSC
+ * value.
+ *
+ * This test verifies that this unusual case is handled correctly.
+ *
+ * # sudo ./vmx_tsc_adjust_test
+ * IA32_TSC_ADJUST is -4294980787 (-1 * TSC_ADJUST_VALUE + -13491).
+ * IA32_TSC_ADJUST is -4294980787 (-1 * TSC_ADJUST_VALUE + -13491).
+ * IA32_TSC_ADJUST is -8589957192 (-2 * TSC_ADJUST_VALUE + -22600).
+ * IA32_TSC_ADJUST is -8589957192 (-2 * TSC_ADJUST_VALUE + -22600).
+ */
 static void l1_guest_code(struct vmx_pages *vmx_pages)
 {
 #define L2_GUEST_STACK_SIZE 64
@@ -89,6 +124,9 @@ static void l1_guest_code(struct vmx_pages *vmx_pages)
 
 	GUEST_ASSERT(rdtsc() < TSC_ADJUST_VALUE);
 	wrmsr(MSR_IA32_TSC, rdtsc() - TSC_ADJUST_VALUE);
+	/*
+	 * 这里会从l1到VMM: GUEST_SYNC(adjust); 打印第一次
+	 */
 	check_ia32_tsc_adjust(-1 * TSC_ADJUST_VALUE);
 
 	GUEST_ASSERT(prepare_for_vmx_operation(vmx_pages));
@@ -108,12 +146,21 @@ static void l1_guest_code(struct vmx_pages *vmx_pages)
 	GUEST_ASSERT(!vmlaunch());
 	GUEST_ASSERT(vmreadz(VM_EXIT_REASON) ==
 		     (EXIT_REASON_FAILED_VMENTRY | EXIT_REASON_INVALID_STATE));
+	/*
+	 * 这里会从l1到VMM: GUEST_SYNC(adjust); 打印第二次
+	 */
 	check_ia32_tsc_adjust(-1 * TSC_ADJUST_VALUE);
 	vmwrite(GUEST_CR3, save_cr3);
 
+	/*
+	 * 这里会从l2到VMM: GUEST_SYNC(adjust); 打印第三次
+	 */
 	GUEST_ASSERT(!vmlaunch());
 	GUEST_ASSERT(vmreadz(VM_EXIT_REASON) == EXIT_REASON_VMCALL);
 
+	/*
+	 * 这里会从l1到VMM: GUEST_SYNC(adjust); 打印第四次
+	 */
 	check_ia32_tsc_adjust(-2 * TSC_ADJUST_VALUE);
 
 	GUEST_DONE();
@@ -125,6 +172,13 @@ static void report(int64_t val)
 		val, val / TSC_ADJUST_VALUE, val % TSC_ADJUST_VALUE);
 }
 
+/*
+ * # sudo ./vmx_tsc_adjust_test
+ * IA32_TSC_ADJUST is -4294980787 (-1 * TSC_ADJUST_VALUE + -13491).
+ * IA32_TSC_ADJUST is -4294980787 (-1 * TSC_ADJUST_VALUE + -13491).
+ * IA32_TSC_ADJUST is -8589957192 (-2 * TSC_ADJUST_VALUE + -22600).
+ * IA32_TSC_ADJUST is -8589957192 (-2 * TSC_ADJUST_VALUE + -22600).
+ */
 int main(int argc, char *argv[])
 {
 	vm_vaddr_t vmx_pages_gva;
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index dd777688d14a..82fcbd7f073a 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -19,6 +19,10 @@
 
 static struct kmem_cache *async_pf_cache;
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6053| <<kvm_init>> r = kvm_async_pf_init();
+ */
 int kvm_async_pf_init(void)
 {
 	async_pf_cache = KMEM_CACHE(kvm_async_pf, 0);
@@ -42,6 +46,10 @@ void kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)
 	spin_lock_init(&vcpu->async_pf.lock);
 }
 
+/*
+ * 在以下使用async_pf_execute():
+ *   - virt/kvm/async_pf.c|192| <<kvm_setup_async_pf>> INIT_WORK(&work->work, async_pf_execute);
+ */
 static void async_pf_execute(struct work_struct *work)
 {
 	struct kvm_async_pf *apf =
@@ -91,6 +99,19 @@ static void async_pf_execute(struct work_struct *work)
 	kvm_put_kvm(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/s390/kvm/interrupt.c|2684| <<flic_set_attr>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|2770| <<kvm_arch_vcpu_destroy>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3277| <<kvm_arch_vcpu_create>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3506| <<kvm_arch_vcpu_ioctl_set_one_reg>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3537| <<kvm_arch_vcpu_ioctl_normal_reset>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4218| <<sync_regs_fmt2>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|1021| <<kvm_post_set_cr0>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|4525| <<kvm_pv_enable_async_pf>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|12515| <<kvm_vcpu_reset>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|12848| <<kvm_free_vcpus>> kvm_clear_async_pf_completion_queue(vcpu);
+ */
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 {
 	spin_lock(&vcpu->async_pf.lock);
@@ -134,6 +155,12 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4012| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|4913| <<kvm_set_msr_common>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|11198| <<vcpu_enter_guest>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
@@ -160,6 +187,11 @@ void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
  * Try to schedule a job to handle page fault asynchronously. Returns 'true' on
  * success, 'false' on failure (page fault has to be handled synchronously).
  */
+/*
+ * called by:
+ *   - arch/s390/kvm/kvm-s390.c|4000| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, current->thread.gmap_addr, hva, &arch);
+ *   - arch/x86/kvm/mmu/mmu.c|3675| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, cr2_or_gpa,
+ */
 bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			unsigned long hva, struct kvm_arch_async_pf *arch)
 {
@@ -200,6 +232,10 @@ bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4537| <<kvm_pv_enable_async_pf>> kvm_async_pf_wakeup_all(vcpu);
+ */
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
diff --git a/virt/kvm/coalesced_mmio.c b/virt/kvm/coalesced_mmio.c
index f08f5e82460b..d88e535b8a9a 100644
--- a/virt/kvm/coalesced_mmio.c
+++ b/virt/kvm/coalesced_mmio.c
@@ -134,6 +134,10 @@ void kvm_coalesced_mmio_free(struct kvm *kvm)
 		free_page((unsigned long)kvm->coalesced_mmio_ring);
 }
 
+/*
+ * 处理KVM_REGISTER_COALESCED_MMIO:
+ *   - virt/kvm/kvm_main.c|4688| <<kvm_vm_ioctl(KVM_REGISTER_COALESCED_MMIO)>> r = kvm_vm_ioctl_register_coalesced_mmio(kvm, &zone);
+ */
 int kvm_vm_ioctl_register_coalesced_mmio(struct kvm *kvm,
 					 struct kvm_coalesced_mmio_zone *zone)
 {
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index e996989cd580..3c634eb29936 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -30,6 +30,15 @@
 
 #ifdef CONFIG_HAVE_KVM_IRQFD
 
+/*
+ * 在以下使用irqfd_cleanup_wq:
+ *   - virt/kvm/eventfd.c|169| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+ *   - virt/kvm/eventfd.c|571| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+ *   - virt/kvm/eventfd.c|608| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+ *   - virt/kvm/eventfd.c|658| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+ *   - virt/kvm/eventfd.c|659| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+ *   - virt/kvm/eventfd.c|667| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+ */
 static struct workqueue_struct *irqfd_cleanup_wq;
 
 bool __attribute__((weak))
@@ -181,6 +190,40 @@ int __attribute__((weak)) kvm_arch_set_irq_inatomic(
 /*
  * Called with wqh->lock held and interrupts disabled
  */
+/*
+ * irqfd_wakeup
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * vhost_tx_batch.isra.23
+ * handle_tx_copy
+ * handle_tx
+ * handle_tx_kick
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * irqfd_wakeup
+ * __wake_up_locked_key
+ * eventfd_signal
+ * vhost_signal
+ * vhost_add_used_and_signal_n
+ * vhost_net_signal_used
+ * handle_rx
+ * handle_rx_net
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ *
+ * struct wait_queue_entry {
+ *     unsigned int            flags;
+ *     void                    *private;
+ *     wait_queue_func_t       func;
+ *     struct list_head        entry;
+ * };
+ */
 static int
 irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
@@ -202,6 +245,10 @@ irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 			seq = read_seqcount_begin(&irqfd->irq_entry_sc);
 			irq = irqfd->irq_entry;
 		} while (read_seqcount_retry(&irqfd->irq_entry_sc, seq));
+		/*
+		 * #define KVM_USERSPACE_IRQ_SOURCE_ID             0
+		 * #define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID        1
+		 */
 		/* An event has been signaled, inject an interrupt */
 		if (kvm_arch_set_irq_inatomic(&irq, kvm,
 					      KVM_USERSPACE_IRQ_SOURCE_ID, 1,
@@ -211,6 +258,9 @@ irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 		ret = 1;
 	}
 
+	/*
+	 * 注释: The eventfd is closing, detach from KVM
+	 */
 	if (flags & EPOLLHUP) {
 		/* The eventfd is closing, detach from KVM */
 		unsigned long iflags;
@@ -235,6 +285,19 @@ irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 	return ret;
 }
 
+/*
+ * irqfd_ptable_queue_proc
+ * kvm_irqfd
+ * kvm_vm_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用irqfd_ptable_queue_proc():
+ *   - virt/kvm/eventfd.c|395| <<kvm_irqfd_assign>> init_poll_funcptr(&irqfd->pt, irqfd_ptable_queue_proc);
+ */
 static void
 irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,
 			poll_table *pt)
@@ -245,6 +308,11 @@ irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,
 }
 
 /* Must be called under irqfds.lock */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|396| <<kvm_irqfd_assign>> irqfd_update(kvm, irqfd);
+ *   - virt/kvm/eventfd.c|622| <<kvm_irq_routing_update>> irqfd_update(kvm, irqfd);
+ */
 static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
 {
 	struct kvm_kernel_irq_routing_entry *e;
@@ -283,6 +351,18 @@ int  __attribute__((weak)) kvm_arch_update_irqfd_routing(
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|594| <<kvm_irqfd>> return kvm_irqfd_assign(kvm, args);
+ *
+ * struct kvm_irqfd {
+ *     __u32 fd;
+ *     __u32 gsi;
+ *     __u32 flags;
+ *     __u32 resamplefd;
+ *     __u8  pad[16];
+ * };
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -324,6 +404,9 @@ kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 
 	irqfd->eventfd = eventfd;
 
+	/*
+	 * virtio-pci应该是不用的, 是level的才用
+	 */
 	if (args->flags & KVM_IRQFD_FLAG_RESAMPLE) {
 		struct kvm_kernel_irqfd_resampler *resampler;
 
@@ -410,6 +493,11 @@ kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 
 #ifdef CONFIG_HAVE_KVM_IRQ_BYPASS
 	if (kvm_arch_has_irq_bypass()) {
+		/*
+		 * struct kvm_kernel_irqfd *irqfd:
+		 * -> struct irq_bypass_consumer consumer;
+		 * -> struct irq_bypass_producer *producer;
+		 */
 		irqfd->consumer.token = (void *)irqfd->eventfd;
 		irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
 		irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
@@ -479,6 +567,16 @@ void kvm_notify_acked_gsi(struct kvm *kvm, int gsi)
 			kian->irq_acked(kian);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|71| <<vgic_v2_fold_lr_state>> kvm_notify_acked_irq(vcpu->kvm, 0,
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|62| <<vgic_v3_fold_lr_state>> kvm_notify_acked_irq(vcpu->kvm, 0,
+ *   - arch/powerpc/kvm/book3s_xics.c|812| <<ics_eoi>> kvm_notify_acked_irq(vcpu->kvm, 0, irq);
+ *   - arch/powerpc/kvm/book3s_xics.c|866| <<kvmppc_xics_rm_complete>> kvm_notify_acked_irq(vcpu->kvm, 0, icp->rm_eoied_irq);
+ *   - arch/powerpc/kvm/mpic.c|1106| <<openpic_cpu_write_internal>> kvm_notify_acked_irq(opp->kvm, 0, notify_eoi);
+ *   - arch/x86/kvm/i8259.c|82| <<pic_clear_isr>> kvm_notify_acked_irq(s->pics_state->kvm, SELECT_PIC(irq), irq);
+ *   - arch/x86/kvm/ioapic.c|519| <<kvm_ioapic_update_eoi_one>> kvm_notify_acked_irq(ioapic->kvm, KVM_IRQCHIP_IOAPIC, pin);
+ */
 void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)
 {
 	int gsi, idx;
@@ -512,6 +610,10 @@ void kvm_unregister_irq_ack_notifier(struct kvm *kvm,
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1014| <<kvm_create_vm>> kvm_eventfd_init(kvm);
+ */
 void
 kvm_eventfd_init(struct kvm *kvm)
 {
@@ -568,6 +670,10 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	return 0;
 }
 
+/*
+ * 处理KVM_IRQFD:
+ *   - virt/kvm/kvm_main.c|4329| <<kvm_vm_ioctl>> r = kvm_irqfd(kvm, &data);
+ */
 int
 kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -608,12 +714,30 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu(&kvm->irq_srcu) afterwards.
  */
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|222| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ *
+ * 一个例子:
+ * kvm_vm_ioctl(KVM_SET_GSI_ROUTING)
+ * -> kvm_set_irq_routing()
+ *    -> kvm_irq_routing_update()
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
 
 	spin_lock_irq(&kvm->irqfds.lock);
 
+	/*
+	 * struct kvm *kvm:
+	 * -> struct {
+	 *        spinlock_t        lock;
+	 *        struct list_head  items;
+	 *        struct list_head  resampler_list;
+	 *        struct mutex      resampler_lock;
+	 *    } irqfds;
+	 */
 	list_for_each_entry(irqfd, &kvm->irqfds.items, list) {
 		irqfd_update(kvm, irqfd);
 
@@ -635,6 +759,10 @@ void kvm_irq_routing_update(struct kvm *kvm)
  * aggregated from all vm* instances. We need our own isolated
  * queue to ease flushing work items when a VM exits.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5508| <<kvm_init>> r = kvm_irqfd_init();
+ */
 int kvm_irqfd_init(void)
 {
 	irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
@@ -907,6 +1035,10 @@ static int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|1088| <<kvm_ioeventfd>> return kvm_assign_ioeventfd(kvm, args);
+ */
 static int
 kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
@@ -959,6 +1091,10 @@ kvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 	return ret;
 }
 
+/*
+ * called by (处理KVM_IOEVENTFD):
+ *   - virt/kvm/kvm_main.c|4338| <<kvm_vm_ioctl>> r = kvm_ioeventfd(kvm, &data);
+ */
 int
 kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 {
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 58e4f88b2b9f..64a923994b0a 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -19,13 +19,33 @@
 #include <trace/events/kvm.h>
 #include "irq.h"
 
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|254| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+ *   - virt/kvm/irqchip.c|84| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+ */
 int kvm_irq_map_gsi(struct kvm *kvm,
 		    struct kvm_kernel_irq_routing_entry *entries, int gsi)
 {
+	/*
+	 * struct kvm_irq_routing_table {
+	 *     int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
+	 *     u32 nr_rt_entries;
+	 *     //
+	 *     // Array indexed by gsi. Each entry contains list of irq chips
+	 *     // the gsi is connected to.
+	 *     //
+	 *     struct hlist_head map[];
+	 * };
+	 */
 	struct kvm_irq_routing_table *irq_rt;
 	struct kvm_kernel_irq_routing_entry *e;
 	int n = 0;
 
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_irq_routing_table __rcu *irq_routing
+	 */
 	irq_rt = srcu_dereference_check(kvm->irq_routing, &kvm->irq_srcu,
 					lockdep_is_held(&kvm->irq_lock));
 	if (irq_rt && gsi < irq_rt->nr_rt_entries) {
@@ -38,6 +58,12 @@ int kvm_irq_map_gsi(struct kvm *kvm,
 	return n;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|315| <<kvm_fire_mask_notifiers>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ *   - virt/kvm/eventfd.c|545| <<kvm_irq_has_notifier>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ *   - virt/kvm/eventfd.c|587| <<kvm_notify_acked_irq>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ */
 int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 {
 	struct kvm_irq_routing_table *irq_rt;
@@ -46,6 +72,10 @@ int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	return irq_rt->chip[irqchip][pin];
 }
 
+/*
+ * 处理KVM_SIGNAL_MSI:
+ *   - virt/kvm/kvm_main.c|4348| <<kvm_vm_ioctl>> r = kvm_send_userspace_msi(kvm, &msi);
+ */
 int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct kvm_kernel_irq_routing_entry route;
@@ -59,6 +89,13 @@ int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 	route.msi.flags = msi->flags;
 	route.msi.devid = msi->devid;
 
+	/*
+	 * 在以下使用kvm_set_msi():
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|54| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+	 *   - arch/powerpc/kvm/mpic.c|1840| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+	 *   - arch/x86/kvm/irq_comm.c|321| <<kvm_set_routing_entry>> e->set = kvm_set_msi;
+	 *   - virt/kvm/irqchip.c|86| <<kvm_send_userspace_msi>> return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
+	 */
 	return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
 }
 
@@ -68,6 +105,20 @@ int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
  *  = 0   Interrupt was coalesced (previous irq is still pending)
  *  > 0   Number of CPUs interrupt was delivered to
  */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s.c|1005| <<kvm_arch_set_irq_inatomic>> return kvm_set_irq(kvm, irq_source_id, irq_entry->gsi,
+ *   - arch/powerpc/kvm/book3s.c|1012| <<kvmppc_book3s_set_irq>> return kvm_set_irq(kvm, irq_source_id, e->gsi, level, line_status);
+ *   - arch/powerpc/kvm/powerpc.c|2136| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+ *   - arch/x86/kvm/i8254.c|250| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 1, false);
+ *   - arch/x86/kvm/i8254.c|251| <<pit_do_work>> kvm_set_irq(kvm, pit->irq_source_id, 0, 0, false);
+ *   - arch/x86/kvm/x86.c|7082| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|49| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,
+ *   - virt/kvm/eventfd.c|51| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,
+ *   - virt/kvm/eventfd.c|54| <<irqfd_inject>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|75| <<irqfd_resampler_ack>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|100| <<irqfd_resampler_shutdown>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ */
 int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 		bool line_status)
 {
@@ -86,6 +137,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * 比如kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
@@ -97,6 +151,11 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|167| <<kvm_free_irq_routing>> free_irq_routing_table(rt);
+ *   - virt/kvm/irqchip.c|309| <<kvm_set_irq_routing>> free_irq_routing_table(new);
+ */
 static void free_irq_routing_table(struct kvm_irq_routing_table *rt)
 {
 	int i;
@@ -117,6 +176,10 @@ static void free_irq_routing_table(struct kvm_irq_routing_table *rt)
 	kfree(rt);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1127| <<kvm_destroy_vm>> kvm_free_irq_routing(kvm);
+ */
 void kvm_free_irq_routing(struct kvm *kvm)
 {
 	/* Called only during vm destruction. Nobody can use the pointer
@@ -125,6 +188,10 @@ void kvm_free_irq_routing(struct kvm *kvm)
 	free_irq_routing_table(rt);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/irqchip.c|222| <<kvm_set_irq_routing>> r = setup_routing_entry(kvm, new, e, ue);
+ */
 static int setup_routing_entry(struct kvm *kvm,
 			       struct kvm_irq_routing_table *rt,
 			       struct kvm_kernel_irq_routing_entry *e,
@@ -149,6 +216,10 @@ static int setup_routing_entry(struct kvm *kvm,
 	r = kvm_set_routing_entry(kvm, e, ue);
 	if (r)
 		return r;
+	/*
+	 * KVM_IRQ_ROUTING_MSI就不用了
+	 * 似乎只针对PIC和IOAPIC??
+	 */
 	if (e->type == KVM_IRQ_ROUTING_IRQCHIP)
 		rt->chip[e->irqchip.irqchip][e->irqchip.pin] = e->gsi;
 
@@ -166,6 +237,15 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|152| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - arch/powerpc/kvm/mpic.c|1649| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+ *   - arch/s390/kvm/kvm-s390.c|2395| <<kvm_arch_vm_ioctl>> r = kvm_set_irq_routing(kvm, &routing, 0, 0);
+ *   - arch/x86/kvm/irq_comm.c|375| <<kvm_setup_default_irq_routing>> return kvm_set_irq_routing(kvm, default_routing,
+ *   - arch/x86/kvm/irq_comm.c|383| <<kvm_setup_empty_irq_routing>> return kvm_set_irq_routing(kvm, empty_routing, 0, 0);
+ *   - virt/kvm/kvm_main.c|4297| <<kvm_vm_ioctl(KVM_SET_GSI_ROUTING)>> r = kvm_set_irq_routing(kvm, entries, routing.nr,
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
@@ -184,6 +264,12 @@ int kvm_set_irq_routing(struct kvm *kvm,
 
 	nr_rt_entries += 1;
 
+	/*
+	 * struct kvm_irq_routing_table *new:
+	 * -> int chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];
+	 * -> u32 nr_rt_entries;
+	 * -> struct hlist_head map[];
+	 */
 	new = kzalloc(struct_size(new, map, nr_rt_entries), GFP_KERNEL_ACCOUNT);
 	if (!new)
 		return -ENOMEM;
@@ -217,9 +303,16 @@ int kvm_set_irq_routing(struct kvm *kvm,
 	}
 
 	mutex_lock(&kvm->irq_lock);
+	/*
+	 * struct kvm:
+	 * -> struct kvm_irq_routing_table __rcu *irq_routing;
+	 */
 	old = rcu_dereference_protected(kvm->irq_routing, 1);
 	rcu_assign_pointer(kvm->irq_routing, new);
 	kvm_irq_routing_update(kvm);
+	/*
+	 * 似乎只用hyper-v
+	 */
 	kvm_arch_irq_routing_update(kvm);
 	mutex_unlock(&kvm->irq_lock);
 
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 46fb042837d2..dbac4ac928fb 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -99,21 +99,70 @@ EXPORT_SYMBOL_GPL(halt_poll_ns_shrink);
  */
 
 DEFINE_MUTEX(kvm_lock);
+/*
+ * 在以下使用kvm_count_lock:
+ *   - virt/kvm/kvm_main.c|4625| <<kvm_starting_cpu>> raw_spin_lock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4628| <<kvm_starting_cpu>> raw_spin_unlock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4644| <<kvm_dying_cpu>> raw_spin_lock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4647| <<kvm_dying_cpu>> raw_spin_unlock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4662| <<hardware_disable_all>> raw_spin_lock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4664| <<hardware_disable_all>> raw_spin_unlock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4671| <<hardware_enable_all>> raw_spin_lock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|4684| <<hardware_enable_all>> raw_spin_unlock(&kvm_count_lock);
+ *   - virt/kvm/kvm_main.c|5383| <<kvm_resume>> WARN_ON(lockdep_is_held(&kvm_count_lock));
+ */
 static DEFINE_RAW_SPINLOCK(kvm_count_lock);
 LIST_HEAD(vm_list);
 
 static cpumask_var_t cpus_hardware_enabled;
+/*
+ * 在以下使用kvm_usage_count:
+ *   - virt/kvm/kvm_main.c|4626| <<kvm_starting_cpu>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|4645| <<kvm_dying_cpu>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|4653| <<hardware_disable_all_nolock>> BUG_ON(!kvm_usage_count);
+ *   - virt/kvm/kvm_main.c|4655| <<hardware_disable_all_nolock>> kvm_usage_count--;
+ *   - virt/kvm/kvm_main.c|4656| <<hardware_disable_all_nolock>> if (!kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|4673| <<hardware_enable_all>> kvm_usage_count++;
+ *   - virt/kvm/kvm_main.c|4674| <<hardware_enable_all>> if (kvm_usage_count == 1) {
+ *   - virt/kvm/kvm_main.c|5374| <<kvm_suspend>> if (kvm_usage_count)
+ *   - virt/kvm/kvm_main.c|5381| <<kvm_resume>> if (kvm_usage_count) {
+ */
 static int kvm_usage_count;
 static atomic_t hardware_enable_failed;
 
 static struct kmem_cache *kvm_vcpu_cache;
 
 static __read_mostly struct preempt_ops kvm_preempt_ops;
+/*
+ * 在以下使用kvm_running_vcpu:
+ *   - virt/kvm/kvm_main.c|230| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|242| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5345| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+ *   - virt/kvm/kvm_main.c|5360| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+ *   - virt/kvm/kvm_main.c|5377| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+ *   - virt/kvm/kvm_main.c|5389| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+ */
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
+/*
+ * 在以下使用kvm_debugfs_dir:
+ *   - arch/powerpc/kvm/book3s_hv.c|5072| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+ *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+ *   - virt/kvm/kvm_main.c|860| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+ *   - virt/kvm/kvm_main.c|5069| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+ *   - virt/kvm/kvm_main.c|5074| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+ *   - virt/kvm/kvm_main.c|5291| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+ */
 struct dentry *kvm_debugfs_dir;
 EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
 
+/*
+ * 在以下使用kvm_debugfs_num_entries:
+ *   - virt/kvm/kvm_main.c|840| <<kvm_destroy_vm_debugfs>> for (i = 0; i < kvm_debugfs_num_entries; i++)
+ *   - virt/kvm/kvm_main.c|862| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+ *   - virt/kvm/kvm_main.c|5071| <<kvm_init_debug>> kvm_debugfs_num_entries = 0;
+ *   - virt/kvm/kvm_main.c|5072| <<kvm_init_debug>> for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
+ */
 static int kvm_debugfs_num_entries;
 static const struct file_operations stat_fops_per_vm;
 
@@ -146,6 +195,11 @@ static void hardware_disable_all(void);
 
 static void kvm_io_bus_destroy(struct kvm_io_bus *bus);
 
+/*
+ * 在以下使用kvm_rebooting:
+ *   - arch/x86/kvm/x86.c|539| <<kvm_spurious_fault>> BUG_ON(!kvm_rebooting);
+ *   - virt/kvm/kvm_main.c|4583| <<kvm_reboot>> kvm_rebooting = true;
+ */
 __visible bool kvm_rebooting;
 EXPORT_SYMBOL_GPL(kvm_rebooting);
 
@@ -202,12 +256,66 @@ bool kvm_is_transparent_hugepage(kvm_pfn_t pfn)
 /*
  * Switches to specified vcpu, until a matching vcpu_put()
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|723| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/mips/kvm/mips.c|429| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/mips/kvm/mips.c|915| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/mips/kvm/mips.c|1129| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_load(vcpu);
+ *   - arch/mips/kvm/mips.c|1146| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_load(vcpu);
+ *   - arch/mips/kvm/mmu.c|738| <<kvm_arch_vcpu_load>> kvm_mips_callbacks->vcpu_load(vcpu, cpu);
+ *   - arch/powerpc/kvm/book3s.c|485| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/book3s.c|497| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/book3s.c|744| <<kvmppc_core_vcpu_load>> vcpu->kvm->arch.kvm_ops->vcpu_load(vcpu, cpu);
+ *   - arch/powerpc/kvm/book3s.c|772| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1404| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1435| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1588| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1605| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1764| <<kvm_arch_vcpu_ioctl_translate>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1991| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1782| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|2048| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|2074| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3605| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3613| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3622| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3634| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3647| <<kvm_arch_vcpu_ioctl_set_fpu>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3667| <<kvm_arch_vcpu_ioctl_get_fpu>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3710| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3752| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3767| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4382| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4807| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|334| <<nested_vmx_free_vcpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|6517| <<kvm_arch_vcpu_ioctl>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11677| <<kvm_arch_vcpu_ioctl_run>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11786| <<kvm_arch_vcpu_ioctl_get_regs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11826| <<kvm_arch_vcpu_ioctl_set_regs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11886| <<kvm_arch_vcpu_ioctl_get_sregs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11895| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|11918| <<kvm_arch_vcpu_ioctl_set_mpstate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12086| <<kvm_arch_vcpu_ioctl_set_sregs>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12101| <<kvm_arch_vcpu_ioctl_set_guest_debug>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12161| <<kvm_arch_vcpu_ioctl_translate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12182| <<kvm_arch_vcpu_ioctl_get_fpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12205| <<kvm_arch_vcpu_ioctl_set_fpu>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12393| <<kvm_arch_vcpu_create>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12430| <<kvm_arch_vcpu_postcreate>> vcpu_load(vcpu);
+ *   - arch/x86/kvm/x86.c|12822| <<kvm_unload_vcpu_mmu>> vcpu_load(vcpu);
+ */
 void vcpu_load(struct kvm_vcpu *vcpu)
 {
 	int cpu = get_cpu();
 
 	__this_cpu_write(kvm_running_vcpu, vcpu);
 	preempt_notifier_register(&vcpu->preempt_notifier);
+	/*
+	 * called by:
+	 *   - virt/kvm/kvm_main.c|211| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *   - virt/kvm/kvm_main.c|5058| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+	 */
 	kvm_arch_vcpu_load(vcpu, cpu);
 	put_cpu();
 }
@@ -216,6 +324,11 @@ EXPORT_SYMBOL_GPL(vcpu_load);
 void vcpu_put(struct kvm_vcpu *vcpu)
 {
 	preempt_disable();
+	/*
+	 * called by:
+	 *   - virt/kvm/kvm_main.c|219| <<vcpu_put>> kvm_arch_vcpu_put(vcpu);
+	 *   - virt/kvm/kvm_main.c|5115| <<kvm_sched_out>> kvm_arch_vcpu_put(vcpu);
+	 */
 	kvm_arch_vcpu_put(vcpu);
 	preempt_notifier_unregister(&vcpu->preempt_notifier);
 	__this_cpu_write(kvm_running_vcpu, NULL);
@@ -224,6 +337,10 @@ void vcpu_put(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(vcpu_put);
 
 /* TODO: merge with kvm_arch_vcpu_should_kick */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|312| <<kvm_make_vcpus_request_mask>> kvm_request_needs_ipi(vcpu, req))
+ */
 static bool kvm_request_needs_ipi(struct kvm_vcpu *vcpu, unsigned req)
 {
 	int mode = kvm_vcpu_exiting_guest_mode(vcpu);
@@ -257,6 +374,12 @@ static inline bool kvm_kick_many_cpus(const struct cpumask *cpus, bool wait)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1717| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH,
+ *   - arch/x86/kvm/x86.c|10892| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC,
+ *   - virt/kvm/kvm_main.c|330| <<kvm_make_all_cpus_request_except>> called = kvm_make_vcpus_request_mask(kvm, req, except, NULL, cpus);
+ */
 bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 				 struct kvm_vcpu *except,
 				 unsigned long *vcpu_bitmap, cpumask_var_t tmp)
@@ -289,6 +412,11 @@ bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 	return called;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10973| <<kvm_request_apicv_update>> kvm_make_all_cpus_request_except(kvm, KVM_REQ_APICV_UPDATE,
+ *   - virt/kvm/kvm_main.c|338| <<kvm_make_all_cpus_request>> return kvm_make_all_cpus_request_except(kvm, req, NULL);
+ */
 bool kvm_make_all_cpus_request_except(struct kvm *kvm, unsigned int req,
 				      struct kvm_vcpu *except)
 {
@@ -303,6 +431,22 @@ bool kvm_make_all_cpus_request_except(struct kvm *kvm, unsigned int req,
 	return called;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|627| <<kvm_arm_halt_guest>> kvm_make_all_cpus_request(kvm, KVM_REQ_SLEEP);
+ *   - arch/arm64/kvm/psci.c|172| <<kvm_prepare_system_event>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|138| <<vgic_mmio_write_v3_misc>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_RELOAD_GICv4);
+ *   - arch/x86/kvm/vmx/posted_intr.c|284| <<vmx_pi_start_assignment>> kvm_make_all_cpus_request(kvm, KVM_REQ_UNBLOCK);
+ *   - arch/x86/kvm/x86.c|3867| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|7310| <<kvm_vm_ioctl_set_msr_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);
+ *   - arch/x86/kvm/x86.c|7631| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|10909| <<kvm_make_scan_ioapic_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
+ *   - arch/x86/kvm/x86.c|11028| <<kvm_arch_mmu_notifier_invalidate_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+ *   - arch/x86/kvm/x86.c|13093| <<kvm_mmu_update_cpu_dirty_logging>> kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *   - virt/kvm/kvm_main.c|363| <<kvm_flush_remote_tlbs>> || kvm_make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))
+ *   - virt/kvm/kvm_main.c|372| <<kvm_reload_remote_mmus>> kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_RELOAD);
+ */
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 {
 	return kvm_make_all_cpus_request_except(kvm, req, NULL);
@@ -310,6 +454,25 @@ bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 EXPORT_SYMBOL_GPL(kvm_make_all_cpus_request);
 
 #ifndef CONFIG_HAVE_KVM_ARCH_TLB_FLUSH_ALL
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1294| <<kvm_arch_flush_remote_tlbs_memslot>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/arm64/kvm/mmu.c|81| <<kvm_flush_remote_tlbs>> void kvm_flush_remote_tlbs(struct kvm *kvm)
+ *   - arch/arm64/kvm/mmu.c|657| <<kvm_mmu_wp_memory_region>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/mips/kvm/mips.c|200| <<kvm_arch_flush_shadow_all>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/mips/kvm/mips.c|980| <<kvm_arch_flush_remote_tlbs_memslot>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|197| <<kvm_flush_remote_tlbs_with_range>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|1784| <<kvm_mmu_remote_flush_or_zap>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|1926| <<mmu_sync_children>> kvm_flush_remote_tlbs(vcpu->kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|2314| <<kvm_mmu_commit_zap_page>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/page_track.c|113| <<kvm_slot_page_track_add_page>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1101| <<FNAME(sync_page)>> kvm_flush_remote_tlbs(vcpu->kvm);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|685| <<tdp_mmu_iter_cond_resched>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|804| <<kvm_tdp_mmu_zap_all>> kvm_flush_remote_tlbs(kvm);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|872| <<kvm_tdp_mmu_zap_invalidated_roots>> kvm_flush_remote_tlbs(kvm);
+ *   - virt/kvm/kvm_main.c|695| <<__kvm_handle_hva_range>> kvm_flush_remote_tlbs(kvm);
+ *   - virt/kvm/kvm_main.c|4322| <<kvm_vm_ioctl_reset_dirty_pages>> kvm_flush_remote_tlbs(kvm);
+ */
 void kvm_flush_remote_tlbs(struct kvm *kvm)
 {
 	/*
@@ -397,6 +560,10 @@ void *kvm_mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)
 }
 #endif
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3608| <<kvm_vm_ioctl_create_vcpu>> kvm_vcpu_init(vcpu, kvm, id);
+ */
 static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 {
 	mutex_init(&vcpu->mutex);
@@ -414,6 +581,12 @@ static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 	kvm_vcpu_set_dy_eligible(vcpu, false);
 	vcpu->preempted = false;
 	vcpu->ready = false;
+	/*
+	 * 在以下使用kvm_vcpu->preempt_notifier:
+	 *   - virt/kvm/kvm_main.c|313| <<vcpu_load>> preempt_notifier_register(&vcpu->preempt_notifier);
+	 *   - virt/kvm/kvm_main.c|333| <<vcpu_put>> preempt_notifier_unregister(&vcpu->preempt_notifier);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_vcpu_init>> preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
+	 */
 	preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
 }
 
@@ -480,6 +653,13 @@ static void kvm_null_fn(void)
 }
 #define IS_KVM_NULL_FN(fn) ((fn) == (void *)kvm_null_fn)
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|724| <<kvm_handle_hva_range>> return __kvm_handle_hva_range(kvm, &range);
+ *   - virt/kvm/kvm_main.c|743| <<kvm_handle_hva_range_no_flush>> return __kvm_handle_hva_range(kvm, &range);
+ *   - virt/kvm/kvm_main.c|809| <<kvm_mmu_notifier_invalidate_range_start>> __kvm_handle_hva_range(kvm, &hva_range);
+ *   - virt/kvm/kvm_main.c|846| <<kvm_mmu_notifier_invalidate_range_end>> __kvm_handle_hva_range(kvm, &hva_range);
+ */
 static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,
 						  const struct kvm_hva_range *range)
 {
@@ -754,6 +934,10 @@ static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * 在以下使用kvm_mmu_notifier_ops:
+ *   - virt/kvm/kvm_main.c|918| <<kvm_init_mmu_notifier>> kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
+ */
 static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.invalidate_range	= kvm_mmu_notifier_invalidate_range,
 	.invalidate_range_start	= kvm_mmu_notifier_invalidate_range_start,
@@ -765,6 +949,10 @@ static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {
 	.release		= kvm_mmu_notifier_release,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1173| <<kvm_create_vm>> r = kvm_init_mmu_notifier(kvm);
+ */
 static int kvm_init_mmu_notifier(struct kvm *kvm)
 {
 	kvm->mmu_notifier.ops = &kvm_mmu_notifier_ops;
@@ -783,6 +971,16 @@ static int kvm_init_mmu_notifier(struct kvm *kvm)
 static struct kvm_memslots *kvm_alloc_memslots(void)
 {
 	int i;
+	/*
+	 * struct kvm_memslots {
+	 *     u64 generation;
+	 *     // The mapping table from slot id to the index in memslots[].
+	 *     short id_to_index[KVM_MEM_SLOTS_NUM];
+	 *     atomic_t lru_slot;
+	 *     int used_slots;
+	 *     struct kvm_memory_slot memslots[];
+	 * };
+	 */
 	struct kvm_memslots *slots;
 
 	slots = kvzalloc(sizeof(struct kvm_memslots), GFP_KERNEL_ACCOUNT);
@@ -827,6 +1025,10 @@ static void kvm_free_memslots(struct kvm *kvm, struct kvm_memslots *slots)
 	kvfree(slots);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1234| <<kvm_destroy_vm>> kvm_destroy_vm_debugfs(kvm);
+ */
 static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 {
 	int i;
@@ -843,6 +1045,56 @@ static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 	}
 }
 
+/*
+ * struct kvm_stats_debugfs_item debugfs_entries[] = {
+ *		VCPU_STAT("pf_fixed", pf_fixed),
+ *		VCPU_STAT("pf_guest", pf_guest),
+ *		VCPU_STAT("tlb_flush", tlb_flush),
+ *		VCPU_STAT("invlpg", invlpg),
+ *		VCPU_STAT("exits", exits),
+ *		VCPU_STAT("io_exits", io_exits),
+ *		VCPU_STAT("mmio_exits", mmio_exits),
+ *		VCPU_STAT("signal_exits", signal_exits),
+ *		VCPU_STAT("irq_window", irq_window_exits),
+ *		VCPU_STAT("nmi_window", nmi_window_exits),
+ *		VCPU_STAT("halt_exits", halt_exits),
+ *		VCPU_STAT("halt_successful_poll", halt_successful_poll),
+ *		VCPU_STAT("halt_attempted_poll", halt_attempted_poll),
+ *		VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
+ *		VCPU_STAT("halt_wakeup", halt_wakeup),
+ *		VCPU_STAT("hypercalls", hypercalls),
+ *		VCPU_STAT("request_irq", request_irq_exits),
+ *		VCPU_STAT("irq_exits", irq_exits),
+ *		VCPU_STAT("host_state_reload", host_state_reload),
+ *		VCPU_STAT("fpu_reload", fpu_reload),
+ *		VCPU_STAT("insn_emulation", insn_emulation),
+ *		VCPU_STAT("insn_emulation_fail", insn_emulation_fail),
+ *		VCPU_STAT("irq_injections", irq_injections),
+ *		VCPU_STAT("nmi_injections", nmi_injections),
+ *		VCPU_STAT("req_event", req_event),
+ *		VCPU_STAT("l1d_flush", l1d_flush),
+ *		VCPU_STAT("halt_poll_success_ns", halt_poll_success_ns),
+ *		VCPU_STAT("halt_poll_fail_ns", halt_poll_fail_ns),
+ *		VCPU_STAT("nested_run", nested_run),
+ *		VCPU_STAT("directed_yield_attempted", directed_yield_attempted),
+ *		VCPU_STAT("directed_yield_successful", directed_yield_successful),
+ *		VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
+ *		VM_STAT("mmu_pte_write", mmu_pte_write),
+ *		VM_STAT("mmu_pde_zapped", mmu_pde_zapped),
+ *		VM_STAT("mmu_flooded", mmu_flooded),
+ *		VM_STAT("mmu_recycled", mmu_recycled),
+ *		VM_STAT("mmu_cache_miss", mmu_cache_miss),
+ *		VM_STAT("mmu_unsync", mmu_unsync),
+ *		VM_STAT("remote_tlb_flush", remote_tlb_flush),
+ *		VM_STAT("largepages", lpages, .mode = 0444),
+ *		VM_STAT("nx_largepages_splitted", nx_lpage_splits, .mode = 0444),
+ *		VM_STAT("max_mmu_page_hash_collisions", max_mmu_page_hash_collisions),
+ *		{ NULL }
+ * };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|4322| <<kvm_dev_ioctl_create_vm>> if (kvm_create_vm_debugfs(kvm, r) < 0) {
+ */
 static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 {
 	char dir_name[ITOA_MAX_LEN * 2];
@@ -855,6 +1107,13 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 	snprintf(dir_name, sizeof(dir_name), "%d-%d", task_pid_nr(current), fd);
 	kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
 
+	/*
+	 * 在以下使用kvm_debugfs_num_entries:
+	 *   - virt/kvm/kvm_main.c|840| <<kvm_destroy_vm_debugfs>> for (i = 0; i < kvm_debugfs_num_entries; i++)
+	 *   - virt/kvm/kvm_main.c|862| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+	 *   - virt/kvm/kvm_main.c|5071| <<kvm_init_debug>> kvm_debugfs_num_entries = 0;
+	 *   - virt/kvm/kvm_main.c|5072| <<kvm_init_debug>> for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
+	 */
 	kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
 					 sizeof(*kvm->debugfs_stat_data),
 					 GFP_KERNEL_ACCOUNT);
@@ -862,12 +1121,27 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 		return -ENOMEM;
 
 	for (p = debugfs_entries; p->name; p++) {
+		/*
+		 * struct kvm_stat_data *stat_data;
+		 * -> struct kvm *kvm;
+		 * -> struct kvm_stats_debugfs_item *dbgfs_item;
+		 */
 		stat_data = kzalloc(sizeof(*stat_data), GFP_KERNEL_ACCOUNT);
 		if (!stat_data)
 			return -ENOMEM;
 
 		stat_data->kvm = kvm;
 		stat_data->dbgfs_item = p;
+		/*
+		 * 在以下使用kvm->debugfs_stat_data:
+		 *   - virt/kvm/kvm_main.c|839| <<kvm_destroy_vm_debugfs>> if (kvm->debugfs_stat_data) {
+		 *   - virt/kvm/kvm_main.c|841| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data[i]);
+		 *   - virt/kvm/kvm_main.c|842| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data);
+		 *   - virt/kvm/kvm_main.c|858| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+		 *   - virt/kvm/kvm_main.c|859| <<kvm_create_vm_debugfs>> sizeof(*kvm->debugfs_stat_data),
+		 *   - virt/kvm/kvm_main.c|861| <<kvm_create_vm_debugfs>> if (!kvm->debugfs_stat_data)
+		 *   - virt/kvm/kvm_main.c|871| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data[p - debugfs_entries] = stat_data;
+		 */
 		kvm->debugfs_stat_data[p - debugfs_entries] = stat_data;
 		debugfs_create_file(p->name, KVM_DBGFS_GET_MODE(p),
 				    kvm->debugfs_dentry, stat_data,
@@ -893,6 +1167,10 @@ void __weak kvm_arch_pre_destroy_vm(struct kvm *kvm)
 {
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4613| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type);
+ */
 static struct kvm *kvm_create_vm(unsigned long type)
 {
 	struct kvm *kvm = kvm_arch_alloc_vm();
@@ -1044,6 +1322,20 @@ static void kvm_destroy_vm(struct kvm *kvm)
 	mmdrop(mm);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1956| <<kvm_vm_ioctl_get_htab_fd>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1999| <<debugfs_htab_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1248| <<debugfs_radix_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|322| <<kvm_vm_ioctl_create_spapr_tce>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2291| <<debugfs_timings_open>> kvm_get_kvm(vcpu->kvm);
+ *   - arch/x86/kvm/svm/sev.c|1746| <<svm_vm_copy_asid_from>> kvm_get_kvm(source_kvm);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1941| <<kvmgt_guest_init>> kvm_get_kvm(info->kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1117| <<vfio_ap_mdev_set_kvm>> kvm_get_kvm(kvm);
+ *   - virt/kvm/async_pf.c|190| <<kvm_setup_async_pf>> kvm_get_kvm(work->vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|3758| <<kvm_vm_ioctl_create_vcpu>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4201| <<kvm_ioctl_create_device>> kvm_get_kvm(kvm);
+ */
 void kvm_get_kvm(struct kvm *kvm)
 {
 	refcount_inc(&kvm->users_count);
@@ -1392,6 +1684,10 @@ static int kvm_set_memslot(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1697| <<__kvm_set_memory_region>> return kvm_delete_memslot(kvm, mem, &old, as_id);
+ */
 static int kvm_delete_memslot(struct kvm *kvm,
 			      const struct kvm_userspace_memory_region *mem,
 			      struct kvm_memory_slot *old, int as_id)
@@ -1426,6 +1722,11 @@ static int kvm_delete_memslot(struct kvm *kvm,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12922| <<__x86_set_memory_region>> r = __kvm_set_memory_region(kvm, &m);
+ *   - virt/kvm/kvm_main.c|1851| <<kvm_set_memory_region>> r = __kvm_set_memory_region(kvm, mem);
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem)
 {
@@ -1546,6 +1847,10 @@ int __kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(__kvm_set_memory_region);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1863| <<kvm_vm_ioctl_set_memory_region>> return kvm_set_memory_region(kvm, mem);
+ */
 int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem)
 {
@@ -1558,6 +1863,10 @@ int kvm_set_memory_region(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_set_memory_region);
 
+/*
+ * 处理KVM_SET_USER_MEMORY_REGION:
+ *   - virt/kvm/kvm_main.c|4470| <<kvm_vm_ioctl(KVM_SET_USER_MEMORY_REGION)>> r = kvm_vm_ioctl_set_memory_region(kvm, &kvm_userspace_mem);
+ */
 static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 					  struct kvm_userspace_memory_region *mem)
 {
@@ -1575,6 +1884,11 @@ static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
  * @is_dirty:	set to '1' if any dirty pages were found
  * @memslot:	set to the associated memslot, always valid on success
  */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_pr.c|1871| <<kvm_vm_ioctl_get_dirty_log_pr>> r = kvm_get_dirty_log(kvm, log, &is_dirty, &memslot);
+ *   - arch/s390/kvm/kvm-s390.c|657| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log(kvm, log, &is_dirty, &memslot);
+ */
 int kvm_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log,
 		      int *is_dirty, struct kvm_memory_slot **memslot)
 {
@@ -1638,6 +1952,10 @@ EXPORT_SYMBOL_GPL(kvm_get_dirty_log);
  * exiting to userspace will be logged for the next call.
  *
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1739| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log_protect(kvm, log);
+ */
 static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 {
 	struct kvm_memslots *slots;
@@ -1866,6 +2184,10 @@ bool kvm_vcpu_is_visible_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_is_visible_gfn);
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_xive_native.c|644| <<kvmppc_xive_native_set_queue_config>> page_size = kvm_host_page_size(vcpu, gfn);
+ */
 unsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	struct vm_area_struct *vma;
@@ -1895,6 +2217,12 @@ static bool memslot_is_readonly(struct kvm_memory_slot *slot)
 	return slot->flags & KVM_MEM_READONLY;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2234| <<gfn_to_hva_many>> return __gfn_to_hva_many(slot, gfn, nr_pages, true);
+ *   - virt/kvm/kvm_main.c|2267| <<gfn_to_hva_memslot_prot>> unsigned long hva = __gfn_to_hva_many(slot, gfn, NULL, false);
+ *   - virt/kvm/kvm_main.c|2519| <<__gfn_to_pfn_memslot>> unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
+ */
 static unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 				       gfn_t *nr_pages, bool write)
 {
@@ -1910,6 +2238,14 @@ static unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 	return __gfn_to_hva_memslot(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2240| <<gfn_to_hva_memslot>> return gfn_to_hva_many(slot, gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2246| <<gfn_to_hva>> return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2252| <<kvm_vcpu_gfn_to_hva>> return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
+ *   - virt/kvm/kvm_main.c|2591| <<gfn_to_page_many_atomic>> addr = gfn_to_hva_many(slot, gfn, &entry);
+ *   - virt/kvm/kvm_main.c|3044| <<__kvm_gfn_to_hva_cache_init>> ghc->hva = gfn_to_hva_many(ghc->memslot, start_gfn,
+ */
 static unsigned long gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
 				     gfn_t *nr_pages)
 {
@@ -1929,6 +2265,13 @@ unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(gfn_to_hva);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|1390| <<kvm_hv_set_msr>> addr = kvm_vcpu_gfn_to_hva(vcpu, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|2894| <<kvm_handle_bad_page>> kvm_send_hwpoison_signal(kvm_vcpu_gfn_to_hva(vcpu, gfn), current);
+ *   - arch/x86/kvm/mmu/mmu.c|3676| <<kvm_arch_setup_async_pf>> kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);
+ *   - arch/x86/kvm/vmx/sgx.c|94| <<sgx_gpa_to_hva>> *hva = kvm_vcpu_gfn_to_hva(vcpu, PFN_DOWN(gpa));
+ */
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	return gfn_to_hva_many(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, NULL);
@@ -2142,6 +2485,10 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2317| <<__gfn_to_pfn_memslot>> return hva_to_pfn(addr, atomic, async, write_fault,
+ */
 static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 			bool write_fault, bool *writable)
 {
@@ -2314,6 +2661,10 @@ void kvm_release_pfn(kvm_pfn_t pfn, bool dirty, struct gfn_to_pfn_cache *cache)
 		kvm_release_pfn_clean(pfn);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2694| <<__kvm_map_gfn>> kvm_cache_gfn_to_pfn(slot, gfn, cache, gen);
+ */
 static void kvm_cache_gfn_to_pfn(struct kvm_memory_slot *slot, gfn_t gfn,
 				 struct gfn_to_pfn_cache *cache, u64 gen)
 {
@@ -2325,6 +2676,11 @@ static void kvm_cache_gfn_to_pfn(struct kvm_memory_slot *slot, gfn_t gfn,
 	cache->generation = gen;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2733| <<kvm_map_gfn>> return __kvm_map_gfn(kvm_memslots(vcpu->kvm), gfn, map,
+ *   - virt/kvm/kvm_main.c|2740| <<kvm_vcpu_map>> return __kvm_map_gfn(kvm_vcpu_memslots(vcpu), gfn, map,
+ */
 static int __kvm_map_gfn(struct kvm_memslots *slots, gfn_t gfn,
 			 struct kvm_host_map *map,
 			 struct gfn_to_pfn_cache *cache,
@@ -2380,6 +2736,11 @@ static int __kvm_map_gfn(struct kvm_memslots *slots, gfn_t gfn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4620| <<record_steal_time>> if (kvm_map_gfn(vcpu, vcpu->arch.st.msr_val >> PAGE_SHIFT,
+ *   - arch/x86/kvm/x86.c|5849| <<kvm_steal_time_set_preempted>> if (kvm_map_gfn(vcpu, vcpu->arch.st.msr_val >> PAGE_SHIFT, &map,
+ */
 int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
 		struct gfn_to_pfn_cache *cache, bool atomic)
 {
@@ -2509,6 +2870,14 @@ void kvm_get_pfn(kvm_pfn_t pfn)
 }
 EXPORT_SYMBOL_GPL(kvm_get_pfn);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2907| <<kvm_read_guest>> while ((seg = next_segment(len, offset)) != 0) {
+ *   - virt/kvm/kvm_main.c|2927| <<kvm_vcpu_read_guest>> while ((seg = next_segment(len, offset)) != 0) {
+ *   - virt/kvm/kvm_main.c|3011| <<kvm_write_guest>> while ((seg = next_segment(len, offset)) != 0) {
+ *   - virt/kvm/kvm_main.c|3032| <<kvm_vcpu_write_guest>> while ((seg = next_segment(len, offset)) != 0) {
+ *   - virt/kvm/kvm_main.c|3188| <<kvm_clear_guest>> while ((seg = next_segment(len, offset)) != 0) {
+ */
 static int next_segment(unsigned long len, int offset)
 {
 	if (len > PAGE_SIZE - offset)
@@ -2532,6 +2901,10 @@ static int __kvm_read_guest_page(struct kvm_memory_slot *slot, gfn_t gfn,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2908| <<kvm_read_guest>> ret = kvm_read_guest_page(kvm, gfn, data, offset, seg);
+ */
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len)
 {
@@ -2541,6 +2914,16 @@ int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 }
 EXPORT_SYMBOL_GPL(kvm_read_guest_page);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|721| <<synic_deliver_msg>> r = kvm_vcpu_read_guest_page(vcpu, msg_page_gfn, &hv_hdr.message_type,
+ *   - arch/x86/kvm/svm/nested.c|80| <<nested_svm_get_tdp_pdptr>> ret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(cr3), &pdpte,
+ *   - arch/x86/kvm/vmx/nested.c|5511| <<nested_vmx_eptp_switching>> if (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,
+ *   - arch/x86/kvm/x86.c|940| <<kvm_read_guest_page_mmu>> return kvm_vcpu_read_guest_page(vcpu, real_gfn, data, offset, len);
+ *   - arch/x86/kvm/x86.c|7916| <<kvm_read_guest_virt_helper>> ret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, data,
+ *   - arch/x86/kvm/x86.c|7950| <<kvm_fetch_guest_virt>> ret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, val,
+ *   - virt/kvm/kvm_main.c|2928| <<kvm_vcpu_read_guest>> ret = kvm_vcpu_read_guest_page(vcpu, gfn, data, offset, seg);
+ */
 int kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data,
 			     int offset, int len)
 {
@@ -2736,6 +3119,17 @@ static int __kvm_gfn_to_hva_cache_init(struct kvm_memslots *slots,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3495| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/lapic.c|3605| <<kvm_lapic_enable_pv_eoi>> return kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);
+ *   - arch/x86/kvm/x86.c|2595| <<kvm_write_system_time>> if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|4530| <<kvm_pv_enable_async_pf>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,
+ *   - arch/x86/kvm/xen.c|31| <<kvm_xen_shared_info_init>> ret = kvm_gfn_to_hva_cache_init(kvm, &kvm->arch.xen.shinfo_cache,
+ *   - arch/x86/kvm/xen.c|327| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/xen.c|344| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ *   - arch/x86/kvm/xen.c|365| <<kvm_xen_vcpu_set_attr>> r = kvm_gfn_to_hva_cache_init(vcpu->kvm,
+ */
 int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			      gpa_t gpa, unsigned long len)
 {
@@ -2781,6 +3175,13 @@ int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 }
 EXPORT_SYMBOL_GPL(kvm_write_guest_cached);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4018| <<kvm_setup_pvclock_page>> if (unlikely(kvm_read_guest_offset_cached(v->kvm, cache,
+ *   - arch/x86/kvm/x86.c|13373| <<apf_pageready_slot_free>> if (kvm_read_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,
+ *   - arch/x86/kvm/xen.c|223| <<__kvm_xen_has_interrupt>> kvm_read_guest_offset_cached(v->kvm, ghc, &rc, offset,
+ *   - virt/kvm/kvm_main.c|2926| <<kvm_read_guest_cached>> return kvm_read_guest_offset_cached(kvm, ghc, data, 0, len);
+ */
 int kvm_read_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 				 void *data, unsigned int offset,
 				 unsigned long len)
@@ -2810,6 +3211,12 @@ int kvm_read_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 }
 EXPORT_SYMBOL_GPL(kvm_read_guest_offset_cached);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|869| <<kvm_hv_get_assist_page>> return !kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data,
+ *   - arch/x86/kvm/lapic.c|1001| <<pv_eoi_get_user>> return kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, val,
+ *   - arch/x86/kvm/lapic.c|3207| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+ */
 int kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			  void *data, unsigned long len)
 {
@@ -2854,6 +3261,20 @@ void mark_page_dirty_in_slot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(mark_page_dirty_in_slot);
 
+/*
+ * called by:
+ *   - arch/mips/kvm/mmu.c|547| <<_kvm_mips_map_page_fast>> mark_page_dirty(kvm, gfn);
+ *   - arch/mips/kvm/mmu.c|661| <<kvm_mips_map_page>> mark_page_dirty(kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_32_mmu_host.c|200| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_64_mmu_host.c|128| <<kvmppc_mmu_map_page>> mark_page_dirty(vcpu->kvm, gfn);
+ *   - arch/powerpc/kvm/book3s_hv.c|856| <<kvmppc_copy_guest>> mark_page_dirty(kvm, to >> PAGE_SHIFT);
+ *   - arch/powerpc/kvm/book3s_xive_native.c|903| <<kvmppc_xive_native_vcpu_eq_sync>> mark_page_dirty(vcpu->kvm, gpa_to_gfn(q->guest_qaddr));
+ *   - arch/s390/kvm/interrupt.c|2800| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->ind_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/interrupt.c|2806| <<adapter_indicators_set>> mark_page_dirty(kvm, adapter_int->summary_addr >> PAGE_SHIFT);
+ *   - arch/s390/kvm/kvm-s390.c|625| <<kvm_arch_sync_dirty_log>> mark_page_dirty(kvm, cur_gfn + i);
+ *   - arch/s390/kvm/vsie.c|656| <<unpin_guest_page>> mark_page_dirty(kvm, gpa_to_gfn(gpa));
+ *   - include/linux/kvm_host.h|961| <<__kvm_put_guest>> mark_page_dirty(kvm, gfn); \
+ */
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn)
 {
 	struct kvm_memory_slot *memslot;
@@ -2872,6 +3293,14 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|730| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1837| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4390| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/x86/kvm/x86.c|11678| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ */
 void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -2966,6 +3395,33 @@ update_halt_poll_stats(struct kvm_vcpu *vcpu, u64 poll_ns, bool waited)
 /*
  * The vCPU has executed a HLT instruction with in-kernel mode enabled.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|98| <<kvm_handle_wfx>> kvm_vcpu_block(vcpu);
+ *   - arch/arm64/kvm/psci.c|49| <<kvm_psci_vcpu_suspend>> kvm_vcpu_block(vcpu);
+ *   - arch/mips/kvm/emulate.c|955| <<kvm_mips_emul_wait>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|494| <<kvmppc_set_msr_pr>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|379| <<kvmppc_h_pr>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/booke.c|696| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_block(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|239| <<kvmppc_kvm_pv>> kvm_vcpu_block(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1338| <<kvm_s390_handle_wait>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|11203| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|11436| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ *
+ * [PATCH 00/14] KVM: Halt-polling fixes, cleanups and a new stat
+ *
+ * ... from Sean Christopherson ...
+ *
+ * https://lore.kernel.org/kvm/20210925005528.1145584-1-seanjc@google.com/
+ *
+ * kvm_vcpu_block()的时候, 会先poll (spin)一会, 然后再schedule().
+ *
+ * 前者是因为VM造成的, 所以算是halt.
+ * 后者是因为KVM host影响的, 所以算是block.
+ *
+ * 目前KVM对这两个分的不清楚. 这个patchset就是把这两者分开计算stat.
+ * 以后就好分清halt和block的stat了.
+ */
 void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	ktime_t start, cur, poll_end;
@@ -2985,11 +3441,27 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 			 * arrives.
 			 */
 			if (kvm_vcpu_check_block(vcpu) < 0) {
+				/*
+				 * 运行到这里说明在poll的时候有事件发生
+				 * 说明这个poll是有用的(successful!)
+				 */
 				++vcpu->stat.halt_successful_poll;
 				if (!vcpu_valid_wakeup(vcpu))
 					++vcpu->stat.halt_poll_invalid;
 				goto out;
 			}
+			/*
+			 * 这里少了一个cpu_relax()
+			 *
+			 * Sean Christopherson suggested as below:
+			 *
+			 * "Rather than disallowing halt-polling entirely, on x86 it should be
+			 * sufficient to simply have the hardware thread yield to its sibling(s)
+			 * via PAUSE.  It probably won't get back all performance, but I would
+			 * expect it to be close.
+			 * This compiles on all KVM architectures, and AFAICT the intended usage
+			 * of cpu_relax() is identical for all architectures."
+			 */
 			poll_end = cur = ktime_get();
 		} while (kvm_vcpu_can_poll(cur, stop));
 	}
@@ -2998,6 +3470,10 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 	for (;;) {
 		set_current_state(TASK_INTERRUPTIBLE);
 
+		/*
+		 * 如果一开始到这里break的话, 上面也算是successful poll
+		 * 因为waited还没设置成true
+		 */
 		if (kvm_vcpu_check_block(vcpu) < 0)
 			break;
 
@@ -3010,6 +3486,13 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 	kvm_arch_vcpu_unblocking(vcpu);
 	block_ns = ktime_to_ns(cur) - ktime_to_ns(start);
 
+	/*
+	 * 后来的dev patch里的注释
+	 * Note, halt-polling is considered successful so long as the vCPU was
+	 * never actually scheduled out, i.e. even if the wake event arrived
+	 * after of the halt-polling loop itself, but before the full wait.
+	 */
+
 	update_halt_poll_stats(
 		vcpu, ktime_to_ns(ktime_sub(poll_end, start)), waited);
 
@@ -3033,10 +3516,24 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 	}
 
 	trace_kvm_vcpu_wakeup(block_ns, waited, vcpu_valid_wakeup(vcpu));
+	/*
+	 * 每个arch都定义, 很多是empty, 只在这里调用
+	 */
 	kvm_arch_vcpu_block_finish(vcpu);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_block);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|282| <<kvm_bg_timer_expire>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/arm64/kvm/psci.c|111| <<kvm_psci_vcpu_on>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1349| <<kvm_s390_vcpu_wakeup>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/x86/kvm/svm/avic.c|117| <<avic_ga_log_notifier>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/x86/kvm/svm/avic.c|312| <<avic_kick_target_vcpus>> kvm_vcpu_wake_up(vcpu);
+ *   - arch/x86/kvm/svm/avic.c|697| <<svm_deliver_avic_intr>> kvm_vcpu_wake_up(vcpu);
+ *   - virt/kvm/kvm_main.c|299| <<kvm_make_vcpus_request_mask>> if (!(req & KVM_REQUEST_NO_WAKEUP) && kvm_vcpu_wake_up(vcpu))
+ *   - virt/kvm/kvm_main.c|3260| <<kvm_vcpu_kick>> if (kvm_vcpu_wake_up(vcpu))
+ */
 bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *waitp;
@@ -3044,6 +3541,13 @@ bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu)
 	waitp = kvm_arch_vcpu_get_wait(vcpu);
 	if (rcuwait_wake_up(waitp)) {
 		WRITE_ONCE(vcpu->ready, true);
+		/*
+		 * x86在以下使用kvm_vcpu_stat->halt_wakeup:
+		 *   - arch/x86/kvm/x86.c|368| <<global>> VCPU_STAT("halt_wakeup", halt_wakeup),
+		 *   - virt/kvm/kvm_main.c|3205| <<kvm_vcpu_wake_up>> ++vcpu->stat.halt_wakeup;
+		 *
+		 * 这里应该是vcpu halt的时候试图进行的wakeup
+		 */
 		++vcpu->stat.halt_wakeup;
 		return true;
 	}
@@ -3056,6 +3560,44 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_wake_up);
 /*
  * Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.
  */
+/*
+ * 在以下使用kvm_vcpu_kick():
+ *   - arch/powerpc/kvm/book3s_pr.c|2071| <<global>> .fast_vcpu_kick = kvm_vcpu_kick,
+ *   - arch/arm64/kvm/arm.c|442| <<vcpu_power_off>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/arm.c|947| <<vcpu_interrupt_line>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|465| <<kvm_pmu_perf_overflow_notify_vcpu>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|502| <<kvm_pmu_perf_overflow>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/psci.c|59| <<kvm_psci_vcpu_off>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|103| <<vgic_v4_doorbell_handler>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|367| <<vgic_queue_irq_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|416| <<vgic_queue_irq_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|711| <<vgic_prune_ap_list>> kvm_vcpu_kick(target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|1003| <<vgic_kick_vcpus>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/include/asm/kvm_ppc.h|582| <<kvmppc_fast_vcpu_kick>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/book3s.c|781| <<kvmppc_decrementer_func>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/booke.c|625| <<kvmppc_watchdog_func>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/booke.c|636| <<kvmppc_watchdog_func>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1828| <<kvmppc_set_tsr_bits>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/e500_emulate.c|78| <<kvmppc_e500_emul_msgsnd>> kvm_vcpu_kick(cvcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1862| <<kvm_vcpu_ioctl_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/hyperv.c|544| <<stimer_mark_pending>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/i8259.c|63| <<pic_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1100| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1108| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1114| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1120| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1129| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1140| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1687| <<apic_timer_expired>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|2078| <<vmx_preemption_timer_fn>> kvm_vcpu_kick(&vmx->vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|234| <<pi_wakeup_handler>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4245| <<vmx_deliver_nested_posted_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4278| <<vmx_deliver_posted_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|3904| <<kvmclock_update_fn>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|6464| <<kvm_arch_sync_dirty_log>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|12261| <<kvm_arch_memslots_updated>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|12750| <<kvm_arch_async_page_present_queued>> kvm_vcpu_kick(vcpu);
+ */
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 {
 	int me;
@@ -3064,7 +3606,17 @@ void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 	if (kvm_vcpu_wake_up(vcpu))
 		return;
 
+	/*
+	 * #define get_cpu() ({ preempt_disable(); __smp_processor_id(); })
+	 */
 	me = get_cpu();
+	/*
+	 * 注释Note, the vCPU could get migrated to a different pCPU at any point
+	 * after kvm_arch_vcpu_should_kick(), which could result in sending an
+	 * IPI to the previous pCPU.  But, that's ok because the purpose of the
+	 * IPI is to force the vCPU to leave IN_GUEST_MODE, and migrating the
+	 * vCPU also requires it to leave IN_GUEST_MODE.
+	 */
 	if (cpu != me && (unsigned)cpu < nr_cpu_ids && cpu_online(cpu))
 		if (kvm_arch_vcpu_should_kick(vcpu))
 			smp_send_reschedule(cpu);
@@ -3073,6 +3625,13 @@ void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_vcpu_kick);
 #endif /* !CONFIG_S390 */
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_hv.c|911| <<kvm_arch_vcpu_yield_to>> return kvm_vcpu_yield_to(target);
+ *   - arch/s390/kvm/diag.c|199| <<__diag_time_slice_end_directed>> if (kvm_vcpu_yield_to(tcpu) <= 0)
+ *   - arch/x86/kvm/x86.c|10308| <<kvm_sched_yield>> if (kvm_vcpu_yield_to(target) <= 0)
+ *   - virt/kvm/kvm_main.c|3685| <<kvm_vcpu_on_spin>> yielded = kvm_vcpu_yield_to(vcpu);
+ */
 int kvm_vcpu_yield_to(struct kvm_vcpu *target)
 {
 	struct pid *pid;
@@ -3115,6 +3674,10 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_yield_to);
  *  locking does not harm. It may result in trying to yield to  same VCPU, fail
  *  and continue with next VCPU and so on.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3682| <<kvm_vcpu_on_spin>> if (!kvm_vcpu_eligible_for_directed_yield(vcpu))
+ */
 static bool kvm_vcpu_eligible_for_directed_yield(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
@@ -3142,6 +3705,9 @@ bool __weak kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)
 	return kvm_arch_vcpu_runnable(vcpu);
 }
 
+/*
+ * dy是direct yiealable
+ */
 static bool vcpu_dy_runnable(struct kvm_vcpu *vcpu)
 {
 	if (kvm_arch_dy_runnable(vcpu))
@@ -3254,6 +3820,10 @@ static vm_fault_t kvm_vcpu_fault(struct vm_fault *vmf)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_vcpu_vm_ops:
+ *   - virt/kvm/kvm_main.c|3754| <<kvm_vcpu_mmap>> vma->vm_ops = &kvm_vcpu_vm_ops;
+ */
 static const struct vm_operations_struct kvm_vcpu_vm_ops = {
 	.fault = kvm_vcpu_fault,
 };
@@ -3280,6 +3850,11 @@ static int kvm_vcpu_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_vcpu_fops:
+ *   - virt/kvm/kvm_main.c|3782| <<create_vcpu_fd>> return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
+ *   - virt/kvm/kvm_main.c|5814| <<kvm_init>> kvm_vcpu_fops.owner = module;
+ */
 static struct file_operations kvm_vcpu_fops = {
 	.release        = kvm_vcpu_release,
 	.unlocked_ioctl = kvm_vcpu_ioctl,
@@ -3299,6 +3874,10 @@ static int create_vcpu_fd(struct kvm_vcpu *vcpu)
 	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3548| <<kvm_vm_ioctl_create_vcpu>> kvm_create_vcpu_debugfs(vcpu);
+ */
 static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
@@ -3329,6 +3908,11 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 		return -EINVAL;
 
 	mutex_lock(&kvm->lock);
+	/*
+	 * x86在以下设置kvm->created_vcpus:
+	 *   - virt/kvm/kvm_main.c|3398| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus++;
+	 *   - virt/kvm/kvm_main.c|3474| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus--;
+	 */
 	if (kvm->created_vcpus == KVM_MAX_VCPUS) {
 		mutex_unlock(&kvm->lock);
 		return -EINVAL;
@@ -3353,6 +3937,10 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 		r = -ENOMEM;
 		goto vcpu_free;
 	}
+	/*
+	 * struct kvm_vcpu *vcpu;
+	 * -> struct kvm_run *run;
+	 */
 	vcpu->run = page_address(page);
 
 	kvm_vcpu_init(vcpu, kvm, id);
@@ -3426,6 +4014,11 @@ static int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)
 	return 0;
 }
 
+/*
+ * struct file_operations kvm_vcpu_fops.unlocked_ioctl = kvm_vcpu_ioctl()
+ *
+ * virt/kvm/kvm_main.c|4259| <<kvm_vcpu_compat_ioctl>> r = kvm_vcpu_ioctl(filp, ioctl, arg);
+ */
 static long kvm_vcpu_ioctl(struct file *filp,
 			   unsigned int ioctl, unsigned long arg)
 {
@@ -3553,6 +4146,19 @@ static long kvm_vcpu_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_TRANSLATE: {
+		/*
+		 * struct kvm_translation {
+		 *     // in
+		 *     __u64 linear_address;
+		 *
+		 *     // out
+		 *     __u64 physical_address;
+		 *     __u8  valid;
+		 *     __u8  writeable;
+		 *     __u8  usermode;
+		 *     __u8  pad[5];
+		 * };
+		 */
 		struct kvm_translation tr;
 
 		r = -EFAULT;
@@ -3741,6 +4347,11 @@ static int kvm_device_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_device_fops:
+ *   - virt/kvm/kvm_main.c|4341| <<kvm_device_from_filp>> if (filp->f_op != &kvm_device_fops)
+ *   - virt/kvm/kvm_main.c|4413| <<kvm_ioctl_create_device>> ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
+ */
 static const struct file_operations kvm_device_fops = {
 	.unlocked_ioctl = kvm_device_ioctl,
 	.release = kvm_device_release,
@@ -3756,6 +4367,17 @@ struct kvm_device *kvm_device_from_filp(struct file *filp)
 	return filp->private_data;
 }
 
+/*
+ * 在以下使用kvm_device_ops_table[KVM_DEV_TYPE_MAX]:
+ *   - virt/kvm/kvm_main.c|4356| <<kvm_register_device_ops>> if (type >= ARRAY_SIZE(kvm_device_ops_table))
+ *   - virt/kvm/kvm_main.c|4359| <<kvm_register_device_ops>> if (kvm_device_ops_table[type] != NULL)
+ *   - virt/kvm/kvm_main.c|4362| <<kvm_register_device_ops>> kvm_device_ops_table[type] = ops;
+ *   - virt/kvm/kvm_main.c|4368| <<kvm_unregister_device_ops>> if (kvm_device_ops_table[type] != NULL)
+ *   - virt/kvm/kvm_main.c|4369| <<kvm_unregister_device_ops>> kvm_device_ops_table[type] = NULL;
+ *   - virt/kvm/kvm_main.c|4381| <<kvm_ioctl_create_device>> if (cd->type >= ARRAY_SIZE(kvm_device_ops_table))
+ *   - virt/kvm/kvm_main.c|4384| <<kvm_ioctl_create_device>> type = array_index_nospec(cd->type, ARRAY_SIZE(kvm_device_ops_table));
+ *   - virt/kvm/kvm_main.c|4385| <<kvm_ioctl_create_device>> ops = kvm_device_ops_table[type];
+ */
 static const struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #ifdef CONFIG_KVM_MPIC
 	[KVM_DEV_TYPE_FSL_MPIC_20]	= &kvm_mpic_ops,
@@ -3763,6 +4385,17 @@ static const struct kvm_device_ops *kvm_device_ops_table[KVM_DEV_TYPE_MAX] = {
 #endif
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2780| <<kvm_vgic_register_its_device>> return kvm_register_device_ops(&kvm_arm_vgic_its_ops,
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|268| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v2_ops,
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|272| <<kvm_register_vgic_device>> ret = kvm_register_device_ops(&kvm_arm_vgic_v3_ops,
+ *   - arch/powerpc/kvm/book3s.c|1048| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xive_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/powerpc/kvm/book3s.c|1051| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xive_native_ops,
+ *   - arch/powerpc/kvm/book3s.c|1056| <<kvmppc_book3s_init>> kvm_register_device_ops(&kvm_xics_ops, KVM_DEV_TYPE_XICS);
+ *   - arch/s390/kvm/kvm-s390.c|491| <<kvm_arch_init>> rc = kvm_register_device_ops(&kvm_flic_ops, KVM_DEV_TYPE_FLIC);
+ *   - virt/kvm/vfio.c|419| <<kvm_vfio_ops_init>> return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
+ */
 int kvm_register_device_ops(const struct kvm_device_ops *ops, u32 type)
 {
 	if (type >= ARRAY_SIZE(kvm_device_ops_table))
@@ -3781,6 +4414,10 @@ void kvm_unregister_device_ops(u32 type)
 		kvm_device_ops_table[type] = NULL;
 }
 
+/*
+ * 处理KVM_CREATE_DEVICE:
+ *   - virt/kvm/kvm_main.c|4740| <<kvm_vm_ioctl(KVM_CREATE_DEVICE)>> r = kvm_ioctl_create_device(kvm, &cd);
+ */
 static int kvm_ioctl_create_device(struct kvm *kvm,
 				   struct kvm_create_device *cd)
 {
@@ -3887,6 +4524,10 @@ static long kvm_vm_ioctl_check_extension_generic(struct kvm *kvm, long arg)
 	return kvm_vm_ioctl_check_extension(kvm, arg);
 }
 
+/*
+ * 处理KVM_CAP_DIRTY_LOG_RING:
+ *   - virt/kvm/kvm_main.c|4569| <<kvm_vm_ioctl_enable_cap_generic>> return kvm_vm_ioctl_enable_dirty_log_ring(kvm, cap->args[0]);
+ */
 static int kvm_vm_ioctl_enable_dirty_log_ring(struct kvm *kvm, u32 size)
 {
 	int r;
@@ -4319,12 +4960,25 @@ static struct file_operations kvm_chardev_ops = {
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * 在以下使用kvm_dev:
+ *   - virt/kvm/kvm_main.c|4951| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|4992| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ *   - virt/kvm/kvm_main.c|5182| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|5223| <<kvm_exit>> misc_deregister(&kvm_dev);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
 	&kvm_chardev_ops,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4608| <<kvm_starting_cpu>> hardware_enable_nolock(NULL);
+ *   - virt/kvm/kvm_main.c|4657| <<hardware_enable_all>> on_each_cpu(hardware_enable_nolock, NULL, 1);
+ *   - virt/kvm/kvm_main.c|5366| <<kvm_resume>> hardware_enable_nolock(NULL);
+ */
 static void hardware_enable_nolock(void *junk)
 {
 	int cpu = raw_smp_processor_id();
@@ -4344,6 +4998,13 @@ static void hardware_enable_nolock(void *junk)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5934| <<kvm_init>> kvm_starting_cpu, kvm_dying_cpu);
+ *
+ * 5937         r = cpuhp_setup_state_nocalls(CPUHP_AP_KVM_STARTING, "kvm/cpu:starting",
+ * 5938                                       kvm_starting_cpu, kvm_dying_cpu);
+ */
 static int kvm_starting_cpu(unsigned int cpu)
 {
 	raw_spin_lock(&kvm_count_lock);
@@ -4388,6 +5049,10 @@ static void hardware_disable_all(void)
 	raw_spin_unlock(&kvm_count_lock);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1223| <<kvm_create_vm>> r = hardware_enable_all();
+ */
 static int hardware_enable_all(void)
 {
 	int r = 0;
@@ -4425,11 +5090,27 @@ static int kvm_reboot(struct notifier_block *notifier, unsigned long val,
 	return NOTIFY_OK;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5944| <<kvm_init>> register_reboot_notifier(&kvm_reboot_notifier);
+ *   - virt/kvm/kvm_main.c|5991| <<kvm_init>> unregister_reboot_notifier(&kvm_reboot_notifier);
+ *   - virt/kvm/kvm_main.c|6013| <<kvm_exit>> unregister_reboot_notifier(&kvm_reboot_notifier);
+ */
 static struct notifier_block kvm_reboot_notifier = {
 	.notifier_call = kvm_reboot,
 	.priority = 0,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1304| <<kvm_destroy_vm>> kvm_io_bus_destroy(bus);
+ *
+ * struct kvm_io_bus {
+ *     int dev_count;
+ *     int ioeventfd_count;
+ *     struct kvm_io_range range[];
+ * };
+ */
 static void kvm_io_bus_destroy(struct kvm_io_bus *bus)
 {
 	int i;
@@ -4445,6 +5126,13 @@ static void kvm_io_bus_destroy(struct kvm_io_bus *bus)
 static inline int kvm_io_bus_cmp(const struct kvm_io_range *r1,
 				 const struct kvm_io_range *r2)
 {
+	/*
+	 * struct kvm_io_range {
+	 *     gpa_t addr;
+	 *     int len;
+	 *     struct kvm_io_device *dev;
+	 * };
+	 */
 	gpa_t addr1 = r1->addr;
 	gpa_t addr2 = r2->addr;
 
@@ -4472,6 +5160,12 @@ static int kvm_io_bus_sort_cmp(const void *p1, const void *p2)
 	return kvm_io_bus_cmp(p1, p2);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5175| <<__kvm_io_bus_write>> idx = kvm_io_bus_get_first_dev(bus, range->addr, range->len);
+ *   - virt/kvm/kvm_main.c|5246| <<__kvm_io_bus_read>> idx = kvm_io_bus_get_first_dev(bus, range->addr, range->len);
+ *   - virt/kvm/kvm_main.c|5384| <<kvm_io_bus_get_dev>> dev_idx = kvm_io_bus_get_first_dev(bus, addr, 1);
+ */
 static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 			     gpa_t addr, int len)
 {
@@ -4517,6 +5211,25 @@ static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 }
 
 /* kvm_io_bus_write - called under kvm->slots_lock */
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmio.c|165| <<io_mem_abort>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
+ *   - arch/mips/kvm/emulate.c|1254| <<kvm_mips_emulate_store>> r = kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/powerpc/kvm/book3s.c|965| <<kvmppc_h_logical_ci_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|434| <<kvmppc_hv_emulate_mmio>> ret = kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, (gpa_t) gpa, 0,
+ *   - arch/powerpc/kvm/powerpc.c|1366| <<kvmppc_handle_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
+ *   - arch/x86/kvm/vmx/vmx.c|5675| <<handle_ept_misconfig>> !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
+ *   - arch/x86/kvm/x86.c|7813| <<vcpu_mmio_write>> && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))
+ *   - arch/x86/kvm/x86.c|8425| <<kernel_pio>> r = kvm_io_bus_write(vcpu, KVM_PIO_BUS,
+ *
+ * enum kvm_bus {
+ *     KVM_MMIO_BUS,
+ *     KVM_PIO_BUS,
+ *     KVM_VIRTIO_CCW_NOTIFY_BUS,
+ *     KVM_FAST_MMIO_BUS,
+ *     KVM_NR_BUSES
+ * };
+ */
 int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val)
 {
@@ -4608,6 +5321,30 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 }
 
 /* Caller must hold slots_lock. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1824| <<vgic_register_its_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|736| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|1081| <<vgic_register_dist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,
+ *   - arch/mips/kvm/loongson_ipi.c|209| <<kvm_init_loongson_ipi>> kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, addr, 0x400, device);
+ *   - arch/powerpc/kvm/mpic.c|1449| <<map_mmio>> kvm_io_bus_register_dev(opp->kvm, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/i8254.c|703| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
+ *   - arch/x86/kvm/i8254.c|710| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
+ *   - arch/x86/kvm/i8259.c|607| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
+ *   - arch/x86/kvm/i8259.c|616| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_eclr);
+ *   - arch/x86/kvm/ioapic.c|700| <<kvm_ioapic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, ioapic->base_address,
+ *   - virt/kvm/coalesced_mmio.c|156| <<kvm_vm_ioctl_register_coalesced_mmio>> ret = kvm_io_bus_register_dev(kvm,
+ *   - virt/kvm/eventfd.c|959| <<kvm_assign_ioeventfd_idx>> ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
+ *
+ * enum kvm_bus {
+ *     KVM_MMIO_BUS,
+ *     KVM_PIO_BUS,
+ *     KVM_VIRTIO_CCW_NOTIFY_BUS,
+ *     KVM_FAST_MMIO_BUS,
+ *     KVM_NR_BUSES
+ * };
+ */
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev)
 {
@@ -4723,6 +5460,10 @@ struct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 }
 EXPORT_SYMBOL_GPL(kvm_io_bus_get_dev);
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5530| <<kvm_stat_data_open>> return kvm_debugfs_open(inode, file, kvm_stat_data_get,
+ */
 static int kvm_debugfs_open(struct inode *inode, struct file *file,
 			   int (*get)(void *, u64 *), int (*set)(void *, u64),
 			   const char *fmt)
@@ -4760,8 +5501,28 @@ static int kvm_debugfs_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4876| <<kvm_stat_data_get>> r = kvm_get_stat_per_vm(stat_data->kvm,
+ *   - virt/kvm/kvm_main.c|4935| <<vm_stat_get>> kvm_get_stat_per_vm(kvm, offset, &tmp_val);
+ */
 static int kvm_get_stat_per_vm(struct kvm *kvm, size_t offset, u64 *val)
 {
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_vm_stat stat;
+	 *    -> ulong mmu_shadow_zapped;
+	 *    -> ulong mmu_pte_write;
+	 *    -> ulong mmu_pde_zapped;
+	 *    -> ulong mmu_flooded;
+	 *    -> ulong mmu_recycled;
+	 *    -> ulong mmu_cache_miss;
+	 *    -> ulong mmu_unsync;
+	 *    -> ulong remote_tlb_flush;
+	 *    -> ulong lpages;
+	 *    -> ulong nx_lpage_splits;
+	 *    -> ulong max_mmu_page_hash_collisions;
+	 */
 	*val = *(ulong *)((void *)kvm + offset);
 
 	return 0;
@@ -4781,6 +5542,10 @@ static int kvm_get_stat_per_vcpu(struct kvm *kvm, size_t offset, u64 *val)
 
 	*val = 0;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_stat stat;
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		*val += *(u64 *)((void *)vcpu + offset);
 
@@ -4846,6 +5611,14 @@ static int kvm_stat_data_open(struct inode *inode, struct file *file)
 				kvm_stat_data_clear, "%llu\n");
 }
 
+/*
+ * 在以下使用stat_fops_per_vm:
+ *   - virt/kvm/kvm_main.c|1148| <<kvm_create_vm_debugfs>> &stat_fops_per_vm);
+ *
+ * 1146                 debugfs_create_file(p->name, KVM_DBGFS_GET_MODE(p),
+ * 1147                                     kvm->debugfs_dentry, stat_data,
+ * 1148                                     &stat_fops_per_vm);
+ */
 static const struct file_operations stat_fops_per_vm = {
 	.owner = THIS_MODULE,
 	.open = kvm_stat_data_open,
@@ -4926,11 +5699,20 @@ static int vcpu_stat_clear(void *_offset, u64 val)
 DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, vcpu_stat_clear,
 			"%llu\n");
 
+/*
+ * 在以下使用stat_fops:
+ *   - virt/kvm/kvm_main.c|5062| <<kvm_init_debug>> stat_fops[p->kind]);
+ */
 static const struct file_operations *stat_fops[] = {
 	[KVM_STAT_VCPU] = &vcpu_stat_fops,
 	[KVM_STAT_VM]   = &vm_stat_fops,
 };
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1291| <<kvm_destroy_vm>> kvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);
+ *   - virt/kvm/kvm_main.c|4907| <<kvm_dev_ioctl_create_vm>> kvm_uevent_notify_change(KVM_EVENT_CREATE_VM, kvm);
+ */
 static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 {
 	struct kobj_uevent_env *env;
@@ -4981,13 +5763,83 @@ static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 	kfree(env);
 }
 
+/*
+ * struct kvm_stats_debugfs_item debugfs_entries[] = {
+ *		VCPU_STAT("pf_fixed", pf_fixed),
+ *		VCPU_STAT("pf_guest", pf_guest),
+ *		VCPU_STAT("tlb_flush", tlb_flush),
+ *		VCPU_STAT("invlpg", invlpg),
+ *		VCPU_STAT("exits", exits),
+ *		VCPU_STAT("io_exits", io_exits),
+ *		VCPU_STAT("mmio_exits", mmio_exits),
+ *		VCPU_STAT("signal_exits", signal_exits),
+ *		VCPU_STAT("irq_window", irq_window_exits),
+ *		VCPU_STAT("nmi_window", nmi_window_exits),
+ *		VCPU_STAT("halt_exits", halt_exits),
+ *		VCPU_STAT("halt_successful_poll", halt_successful_poll),
+ *		VCPU_STAT("halt_attempted_poll", halt_attempted_poll),
+ *		VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
+ *		VCPU_STAT("halt_wakeup", halt_wakeup),
+ *		VCPU_STAT("hypercalls", hypercalls),
+ *		VCPU_STAT("request_irq", request_irq_exits),
+ *		VCPU_STAT("irq_exits", irq_exits),
+ *		VCPU_STAT("host_state_reload", host_state_reload),
+ *		VCPU_STAT("fpu_reload", fpu_reload),
+ *		VCPU_STAT("insn_emulation", insn_emulation),
+ *		VCPU_STAT("insn_emulation_fail", insn_emulation_fail),
+ *		VCPU_STAT("irq_injections", irq_injections),
+ *		VCPU_STAT("nmi_injections", nmi_injections),
+ *		VCPU_STAT("req_event", req_event),
+ *		VCPU_STAT("l1d_flush", l1d_flush),
+ *		VCPU_STAT("halt_poll_success_ns", halt_poll_success_ns),
+ *		VCPU_STAT("halt_poll_fail_ns", halt_poll_fail_ns),
+ *		VCPU_STAT("nested_run", nested_run),
+ *		VCPU_STAT("directed_yield_attempted", directed_yield_attempted),
+ *		VCPU_STAT("directed_yield_successful", directed_yield_successful),
+ *		VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
+ *		VM_STAT("mmu_pte_write", mmu_pte_write),
+ *		VM_STAT("mmu_pde_zapped", mmu_pde_zapped),
+ *		VM_STAT("mmu_flooded", mmu_flooded),
+ *		VM_STAT("mmu_recycled", mmu_recycled),
+ *		VM_STAT("mmu_cache_miss", mmu_cache_miss),
+ *		VM_STAT("mmu_unsync", mmu_unsync),
+ *		VM_STAT("remote_tlb_flush", remote_tlb_flush),
+ *		VM_STAT("largepages", lpages, .mode = 0444),
+ *		VM_STAT("nx_largepages_splitted", nx_lpage_splits, .mode = 0444),
+ *		VM_STAT("max_mmu_page_hash_collisions", max_mmu_page_hash_collisions),
+ *		{ NULL }
+ * };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|5262| <<kvm_init>> kvm_init_debug();
+ */
 static void kvm_init_debug(void)
 {
 	struct kvm_stats_debugfs_item *p;
 
+	/*
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - arch/powerpc/kvm/book3s_hv.c|5072| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|860| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|5069| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|5074| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|5291| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
 
+	/*
+	 * 在以下使用kvm_debugfs_num_entries:
+	 *   - virt/kvm/kvm_main.c|840| <<kvm_destroy_vm_debugfs>> for (i = 0; i < kvm_debugfs_num_entries; i++)
+	 *   - virt/kvm/kvm_main.c|862| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+	 *   - virt/kvm/kvm_main.c|5071| <<kvm_init_debug>> kvm_debugfs_num_entries = 0;
+	 *   - virt/kvm/kvm_main.c|5072| <<kvm_init_debug>> for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
+	 */
 	kvm_debugfs_num_entries = 0;
+	/*
+	 * debugfs_entries在上面注释拷贝了一份
+	 * 都是"struct kvm_stats_debugfs_item"类型
+	 */
 	for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
 		debugfs_create_file(p->name, KVM_DBGFS_GET_MODE(p),
 				    kvm_debugfs_dir, (void *)(long)p->offset,
@@ -5023,6 +5875,20 @@ struct kvm_vcpu *preempt_notifier_to_vcpu(struct preempt_notifier *pn)
 	return container_of(pn, struct kvm_vcpu, preempt_notifier);
 }
 
+/*
+ * kvm_sched_in
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 {
 	struct kvm_vcpu *vcpu = preempt_notifier_to_vcpu(pn);
@@ -5035,6 +5901,22 @@ static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 	kvm_arch_vcpu_load(vcpu, cpu);
 }
 
+/*
+ * kvm_sched_out
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用kvm_sched_out():
+ *   - virt/kvm/kvm_main.c|5562| <<kvm_init>> kvm_preempt_ops.sched_out = kvm_sched_out;
+ */
 static void kvm_sched_out(struct preempt_notifier *pn,
 			  struct task_struct *next)
 {
@@ -5062,6 +5944,15 @@ struct kvm_vcpu *kvm_get_running_vcpu(void)
 	struct kvm_vcpu *vcpu;
 
 	preempt_disable();
+	/*
+	 * 在以下使用kvm_running_vcpu:
+	 *   - virt/kvm/kvm_main.c|230| <<vcpu_load>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|242| <<vcpu_put>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|5345| <<kvm_sched_in>> __this_cpu_write(kvm_running_vcpu, vcpu);
+	 *   - virt/kvm/kvm_main.c|5360| <<kvm_sched_out>> __this_cpu_write(kvm_running_vcpu, NULL);
+	 *   - virt/kvm/kvm_main.c|5377| <<kvm_get_running_vcpu>> vcpu = __this_cpu_read(kvm_running_vcpu);
+	 *   - virt/kvm/kvm_main.c|5389| <<kvm_get_running_vcpus>> return &kvm_running_vcpu;
+	 */
 	vcpu = __this_cpu_read(kvm_running_vcpu);
 	preempt_enable();
 
@@ -5089,6 +5980,17 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2156| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1615| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1037| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5060| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4580| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8230| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
@@ -5221,6 +6123,10 @@ struct kvm_vm_worker_thread_context {
 	int err;
 };
 
+/*
+ * 在以下使用kvm_vm_worker_thread():
+ *   - virt/kvm/kvm_main.c|5605| <<kvm_vm_create_worker_thread>> thread = kthread_run(kvm_vm_worker_thread, &init_context,
+ */
 static int kvm_vm_worker_thread(void *context)
 {
 	/*
@@ -5239,6 +6145,12 @@ static int kvm_vm_worker_thread(void *context)
 	if (err)
 		goto init_complete;
 
+	/*
+	 * 注释
+	 * kvm_vm_worker_thread() creates a kthread VM worker and migrates it
+	 * to the parent cgroup using cgroup_attach_task_all() based on its
+	 * effective cpumask.
+	 */
 	err = cgroup_attach_task_all(init_context->parent, current);
 	if (err) {
 		kvm_err("%s: cgroup_attach_task_all failed with err %d\n",
@@ -5265,6 +6177,10 @@ static int kvm_vm_worker_thread(void *context)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6091| <<kvm_mmu_post_init_vm>> err = kvm_vm_create_worker_thread(kvm, kvm_nx_lpage_recovery_worker, 0,
+ */
 int kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,
 				uintptr_t data, const char *name,
 				struct task_struct **thread_ptr)
diff --git a/virt/kvm/vfio.c b/virt/kvm/vfio.c
index 8fcbc50221c2..11e106e1cd29 100644
--- a/virt/kvm/vfio.c
+++ b/virt/kvm/vfio.c
@@ -329,6 +329,9 @@ static int kvm_vfio_set_group(struct kvm_device *dev, long attr, u64 arg)
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.set_attr = kvm_vio_set_attr()
+ */
 static int kvm_vfio_set_attr(struct kvm_device *dev,
 			     struct kvm_device_attr *attr)
 {
@@ -340,6 +343,9 @@ static int kvm_vfio_set_attr(struct kvm_device *dev,
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.has_attr = kvm_vio_has_attr()
+ */
 static int kvm_vfio_has_attr(struct kvm_device *dev,
 			     struct kvm_device_attr *attr)
 {
@@ -360,6 +366,9 @@ static int kvm_vfio_has_attr(struct kvm_device *dev,
 	return -ENXIO;
 }
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.destroy = kvm_vfio_destroy()
+ */
 static void kvm_vfio_destroy(struct kvm_device *dev)
 {
 	struct kvm_vfio *kv = dev->private;
@@ -392,6 +401,9 @@ static struct kvm_device_ops kvm_vfio_ops = {
 	.has_attr = kvm_vfio_has_attr,
 };
 
+/*
+ * struct kvm_device_ops kvm_vfio_ops.create = kvm_vfio_create()
+ */
 static int kvm_vfio_create(struct kvm_device *dev, u32 type)
 {
 	struct kvm_device *tmp;
@@ -414,6 +426,10 @@ static int kvm_vfio_create(struct kvm_device *dev, u32 type)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6074| <<kvm_init>> r = kvm_vfio_ops_init();
+ */
 int kvm_vfio_ops_init(void)
 {
 	return kvm_register_device_ops(&kvm_vfio_ops, KVM_DEV_TYPE_VFIO);
diff --git a/virt/lib/irqbypass.c b/virt/lib/irqbypass.c
index 28fda42e471b..86f93b5d1a75 100644
--- a/virt/lib/irqbypass.c
+++ b/virt/lib/irqbypass.c
@@ -81,6 +81,11 @@ static void __disconnect(struct irq_bypass_producer *prod,
  * Add the provided IRQ producer to the list of producers and connect
  * with any matching token found on the IRQ consumers list.
  */
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|354| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&vdev->ctx[vector].producer);
+ *   - drivers/vhost/vdpa.c|174| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer);
+ */
 int irq_bypass_register_producer(struct irq_bypass_producer *producer)
 {
 	struct irq_bypass_producer *tmp;
@@ -132,6 +137,12 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_producer);
  * Remove a previously registered IRQ producer from the list of producers
  * and disconnect it from any connected IRQ consumer.
  */
+/*
+ * called by:
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|302| <<vfio_msi_set_vector_signal>> irq_bypass_unregister_producer(&vdev->ctx[vector].producer);
+ *   - drivers/vhost/vdpa.c|168| <<vhost_vdpa_setup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ *   - drivers/vhost/vdpa.c|184| <<vhost_vdpa_unsetup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ */
 void irq_bypass_unregister_producer(struct irq_bypass_producer *producer)
 {
 	struct irq_bypass_producer *tmp;
@@ -176,6 +187,10 @@ EXPORT_SYMBOL_GPL(irq_bypass_unregister_producer);
  * Add the provided IRQ consumer to the list of consumers and connect
  * with any matching token found on the IRQ producer list.
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|479| <<kvm_irqfd_assign>> ret = irq_bypass_register_consumer(&irqfd->consumer);
+ */
 int irq_bypass_register_consumer(struct irq_bypass_consumer *consumer)
 {
 	struct irq_bypass_consumer *tmp;
@@ -228,6 +243,10 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_consumer);
  * Remove a previously registered IRQ consumer from the list of consumers
  * and disconnect it from any connected IRQ producer.
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|152| <<irqfd_shutdown>> irq_bypass_unregister_consumer(&irqfd->consumer);
+ */
 void irq_bypass_unregister_consumer(struct irq_bypass_consumer *consumer)
 {
 	struct irq_bypass_consumer *tmp;
-- 
2.17.1

