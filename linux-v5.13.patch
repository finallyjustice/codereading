From 5903e53862c6392e5215f5b536c045660be5a33d Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 28 Sep 2021 23:14:57 -0700
Subject: [PATCH 1/1] linux v5.13

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/kvm_host.h      |  470 +++++++
 arch/x86/include/asm/msr-index.h     |    8 +
 arch/x86/include/asm/pvclock-abi.h   |   51 +
 arch/x86/include/asm/pvclock.h       |   20 +
 arch/x86/include/asm/xen/interface.h |    7 +
 arch/x86/include/uapi/asm/kvm_para.h |    7 +
 arch/x86/kernel/apic/io_apic.c       |    4 +
 arch/x86/kernel/apic/vector.c        |    9 +
 arch/x86/kernel/crash.c              |    6 +
 arch/x86/kernel/kvm.c                |   27 +
 arch/x86/kernel/kvmclock.c           |   26 +
 arch/x86/kernel/pvclock.c            |   47 +
 arch/x86/kernel/tsc.c                |    6 +
 arch/x86/kvm/cpuid.c                 |    9 +
 arch/x86/kvm/emulate.c               |    4 +
 arch/x86/kvm/hyperv.c                |    4 +
 arch/x86/kvm/lapic.c                 |  376 ++++++
 arch/x86/kvm/lapic.h                 |  173 +++
 arch/x86/kvm/mmu/mmu.c               |   27 +
 arch/x86/kvm/mmu/page_track.c        |   27 +
 arch/x86/kvm/svm/nested.c            |   15 +
 arch/x86/kvm/vmx/posted_intr.c       |   14 +
 arch/x86/kvm/vmx/vmcs.h              |   12 +
 arch/x86/kvm/vmx/vmx.c               |  255 ++++
 arch/x86/kvm/vmx/vmx.h               |   42 +
 arch/x86/kvm/x86.c                   | 1768 ++++++++++++++++++++++++++
 arch/x86/xen/enlighten.c             |   44 +
 arch/x86/xen/time.c                  |   30 +
 drivers/acpi/acpica/nseval.c         |   12 +
 drivers/acpi/acpica/psparse.c        |   93 ++
 drivers/acpi/acpica/psxface.c        |    4 +
 drivers/acpi/osl.c                   |   30 +
 drivers/acpi/scan.c                  |   36 +
 drivers/acpi/utils.c                 |   14 +
 drivers/cpuidle/poll_state.c         |    7 +
 drivers/iommu/intel/dmar.c           |   30 +
 drivers/iommu/intel/irq_remapping.c  |   43 +
 drivers/iommu/iommu.c                |   78 ++
 drivers/misc/pvpanic/pvpanic.c       |   26 +
 drivers/net/virtio_net.c             |   43 +
 drivers/net/xen-netback/rx.c         |   21 +
 drivers/net/xen-netfront.c           |   19 +
 fs/configfs/configfs_internal.h      |    8 +
 include/linux/intel-iommu.h          |    5 +
 include/linux/kvm_host.h             |   81 ++
 include/linux/netdevice.h            |    7 +
 include/linux/timekeeping.h          |    6 +
 include/uapi/linux/kvm.h             |   39 +
 include/xen/interface/vcpu.h         |    5 +
 include/xen/xen-ops.h                |   25 +
 kernel/irq/matrix.c                  |   15 +
 kernel/locking/qspinlock.c           |   64 +
 kernel/locking/qspinlock_paravirt.h  |  152 +++
 kernel/sched/clock.c                 |   15 +
 kernel/time/timekeeping.c            |   11 +
 mm/memory-failure.c                  |   25 +
 net/core/dev.c                       |    8 +
 virt/kvm/kvm_main.c                  |  274 ++++
 58 files changed, 4684 insertions(+)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 9c7ced0e3171..02da764df6f1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -58,6 +58,26 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2831| <<kvm_gen_update_masterclock>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2993| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3084| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3093| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4315| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4952| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6096| <<kvm_arch_vm_ioctl>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - arch/x86/kvm/x86.c|8118| <<kvm_hyperv_tsc_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8179| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9360| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|9645| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10762| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|333| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|350| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * kvm_guest_time_update()
+ * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
@@ -67,11 +87,50 @@
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1270| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2158| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2336| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8243| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9288| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|10748| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 调用kvm_gen_update_masterclock()
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
+/*
+ * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+ *   - arch/x86/kvm/x86.c|2742| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|2767| <<kvm_gen_update_masterclock>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *   - arch/x86/kvm/x86.c|8053| <<kvm_hyperv_tsc_notifier>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ *
+ * 给所有vCPU发送KVM_REQ_MCLOCK_INPROGRESS,将它们踢出Guest模式,该请求没有Handler,因此vCPU无法再进入Guest
+ */
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2179| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4339| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9358| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * kvm_gen_kvmclock_update()
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+ * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -686,6 +745,28 @@ struct kvm_vcpu_arch {
 
 	/* emulate context */
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->emulate_ctxt:
+	 *   - arch/x86/kvm/svm/svm.c|2150| <<svm_instr_opcode>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/trace.h|778| <<static_call>> __entry->len = vcpu->arch.emulate_ctxt->fetch.ptr
+	 *   - arch/x86/kvm/trace.h|779| <<static_call>> - vcpu->arch.emulate_ctxt->fetch.data;
+	 *   - arch/x86/kvm/trace.h|780| <<static_call>> __entry->rip = vcpu->arch.emulate_ctxt->_eip - __entry->len;
+	 *   - arch/x86/kvm/trace.h|782| <<static_call>> vcpu->arch.emulate_ctxt->fetch.data,
+	 *   - arch/x86/kvm/trace.h|784| <<static_call>> __entry->flags = kei_decode_mode(vcpu->arch.emulate_ctxt->mode);
+	 *   - arch/x86/kvm/x86.c|6512| <<emulator_read_write_onepage>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7219| <<inject_emulated_exception>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7243| <<alloc_emulate_ctxt>> vcpu->arch.emulate_ctxt = ctxt;
+	 *   - arch/x86/kvm/x86.c|7250| <<init_emulate_ctxt>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7280| <<kvm_inject_realmode_interrupt>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7601| <<x86_decode_emulated_instruction>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|7627| <<x86_emulate_instruction>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|9789| <<__get_regs>> emulator_writeback_register_cache(vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|9981| <<kvm_task_switch>> struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+	 *   - arch/x86/kvm/x86.c|10417| <<kvm_arch_vcpu_create>> kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|10462| <<kvm_arch_vcpu_destroy>> kmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);
+	 *   - arch/x86/kvm/x86.c|11939| <<kvm_sev_es_outs>> ret = emulator_pio_out_emulated(vcpu->arch.emulate_ctxt, size, port,
+	 *   - arch/x86/kvm/x86.c|11954| <<kvm_sev_es_ins>> ret = emulator_pio_in_emulated(vcpu->arch.emulate_ctxt, size, port,
+	 */
 	struct x86_emulate_ctxt *emulate_ctxt;
 	bool emulate_regs_need_sync_to_vcpu;
 	bool emulate_regs_need_sync_from_vcpu;
@@ -693,10 +774,29 @@ struct kvm_vcpu_arch {
 
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
+	/*
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3506| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3510| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	unsigned int hw_tsc_khz;
 	struct gfn_to_hva_cache pv_time;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	bool pv_time_enabled;
 	/* set guest stopped flag in pvclock flags field */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|2765| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|4767| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	bool pvclock_set_guest_stopped_request;
 
 	struct {
@@ -706,21 +806,162 @@ struct kvm_vcpu_arch {
 		struct gfn_to_pfn_cache cache;
 	} st;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|526| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2357| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2363| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2482| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3289| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3591| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	u64 l1_tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3358| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset += vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4467| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|23| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|791| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|792| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2535| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4503| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/vmx.c|1991| <<vmx_write_l1_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.tsc_offset - g_tsc_offset, offset);
+	 *   - arch/x86/kvm/x86.c|4478| <<kvm_get_msr_common(MSR_IA32_TSC)>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset : vcpu->arch.tsc_offset;
+	 */
 	u64 tsc_offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	u64 last_guest_tsc;
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|5282| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+	 *   - arch/x86/kvm/x86.c|11850| <<kvm_arch_hardware_enable>> vcpu->arch.last_host_tsc = local_tsc;
+	 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|5211| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+	 *   - arch/x86/kvm/x86.c|5212| <<kvm_arch_vcpu_load>> rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|11798| <<kvm_arch_hardware_enable>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+	 *   - arch/x86/kvm/x86.c|11800| <<kvm_arch_hardware_enable>> if (vcpu->arch.last_host_tsc > max_tsc)
+	 *   - arch/x86/kvm/x86.c|11801| <<kvm_arch_hardware_enable>> max_tsc = vcpu->arch.last_host_tsc;
+	 */
 	u64 last_host_tsc;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_offset_adjustment:
+	 *   - arch/x86/kvm/x86.c|5199| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+	 *   - arch/x86/kvm/x86.c|5200| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+	 *   - arch/x86/kvm/x86.c|5201| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_offset_adjustment = 0;
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_hardware_enable>> vcpu->arch.tsc_offset_adjustment += delta_cyc;
+	 */
 	u64 tsc_offset_adjustment;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2319| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2517| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 this_tsc_nsec;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2657| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 this_tsc_write;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|3048| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|3103| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2454| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_shift,
+	 *   - arch/x86/kvm/x86.c|2531| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|361| <<nsec_to_cycles>> vcpu->arch.virtual_tsc_shift);
+	 */
 	s8 virtual_tsc_shift;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_set_tsc_khz>> &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2530| <<compute_guest_tsc>> vcpu->arch.virtual_tsc_mult,
+	 *   - arch/x86/kvm/x86.h|360| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+	 */
 	u32 virtual_tsc_mult;
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/hyperv.c|1552| <<kvm_hv_get_msr>> data = (u64)vcpu->arch.virtual_tsc_khz * 1000;
+	 *   - arch/x86/kvm/lapic.c|1570| <<__wait_lapic_expire>> do_div(delay_ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1590| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1595| <<adjust_lapic_timer_advance>> do_div(ns, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/lapic.c|1697| <<start_sw_tscdeadline>> unsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/vmx/nested.c|2113| <<vmx_start_preemption_timer>> if (vcpu->arch.virtual_tsc_khz == 0)
+	 *   - arch/x86/kvm/vmx/nested.c|2118| <<vmx_start_preemption_timer>> do_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);
+	 *   - arch/x86/kvm/vmx/nested.c|3903| <<vmx_get_preemption_timer_value>> value = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|2434| <<kvm_synchronize_tsc>> if (vcpu->arch.virtual_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2445| <<kvm_synchronize_tsc>> u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
+	 *   - arch/x86/kvm/x86.c|2463| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2496| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|5231| <<kvm_arch_vcpu_ioctl>> r = vcpu->arch.virtual_tsc_khz;
+	 *
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	u32 virtual_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+	 *   - arch/x86/kvm/x86.c|4571| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+	 *   - arch/x86/kvm/x86.c|4574| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+	 *   - arch/x86/kvm/x86.c|4602| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+	 *   - arch/x86/kvm/x86.c|4963| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+	 */
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	u64 tsc_scaling_ratio;
 
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
@@ -731,6 +972,22 @@ struct kvm_vcpu_arch {
 	struct kvm_mtrr mtrr_state;
 	u64 pat;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->switch_db_regs:
+	 *   - arch/x86/kvm/svm/svm.c|1868| <<svm_sync_dirty_debug_regs>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/svm/svm.c|2563| <<dr_interception>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/svm/svm.c|3769| <<svm_vcpu_run>> if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))
+	 *   - arch/x86/kvm/vmx/vmx.c|4400| <<vmx_exec_control>> if (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)
+	 *   - arch/x86/kvm/vmx/vmx.c|5381| <<handle_dr>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|5409| <<vmx_sync_dirty_debug_regs>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;
+	 *   - arch/x86/kvm/x86.c|1154| <<kvm_update_dr0123>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_RELOAD;
+	 *   - arch/x86/kvm/x86.c|1167| <<kvm_update_dr7>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_BP_ENABLED;
+	 *   - arch/x86/kvm/x86.c|1169| <<kvm_update_dr7>> vcpu->arch.switch_db_regs |= KVM_DEBUGREG_BP_ENABLED;
+	 *   - arch/x86/kvm/x86.c|9362| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.switch_db_regs)) {
+	 *   - arch/x86/kvm/x86.c|9369| <<vcpu_enter_guest>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
+	 *   - arch/x86/kvm/x86.c|9392| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)) {
+	 *   - arch/x86/kvm/x86.c|9397| <<vcpu_enter_guest>> vcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;
+	 */
 	unsigned switch_db_regs;
 	unsigned long db[KVM_NR_DB_REGS];
 	unsigned long dr6;
@@ -860,6 +1117,17 @@ struct kvm_lpage_info {
 struct kvm_arch_memory_slot {
 	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
 	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	/*
+	 * 在以下使用kvm_arch_memory_slot->gfn_track:
+	 *   - arch/x86/kvm/mmu/page_track.c|26| <<kvm_page_track_free_memslot>> kvfree(slot->arch.gfn_track[i]);
+	 *   - arch/x86/kvm/mmu/page_track.c|27| <<kvm_page_track_free_memslot>> slot->arch.gfn_track[i] = NULL;
+	 *   - arch/x86/kvm/mmu/page_track.c|37| <<kvm_page_track_create_memslot>> slot->arch.gfn_track[i] =
+	 *   - arch/x86/kvm/mmu/page_track.c|38| <<kvm_page_track_create_memslot>> kvcalloc(npages, sizeof(*slot->arch.gfn_track[i]),
+	 *   - arch/x86/kvm/mmu/page_track.c|40| <<kvm_page_track_create_memslot>> if (!slot->arch.gfn_track[i])
+	 *   - arch/x86/kvm/mmu/page_track.c|66| <<update_gfn_track>> val = slot->arch.gfn_track[mode][index];
+	 *   - arch/x86/kvm/mmu/page_track.c|71| <<update_gfn_track>> slot->arch.gfn_track[mode][index] += count;
+	 *   - arch/x86/kvm/mmu/page_track.c|155| <<kvm_page_track_is_active>> return !!READ_ONCE(slot->arch.gfn_track[mode][index]);
+	 */
 	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
 };
 
@@ -999,12 +1267,42 @@ struct kvm_arch {
 	struct kvm_pit *vpit;
 	atomic_t vapics_in_nmi_mode;
 	struct mutex apic_map_lock;
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|301| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|303| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1032| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1074| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1211| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10053| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|12612| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	struct kvm_apic_map __rcu *apic_map;
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|218| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|226| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|300| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|323| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|334| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|340| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|346| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|362| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|2376| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_t apic_map_dirty;
 
 	bool apic_access_page_done;
 	unsigned long apicv_inhibit_reasons;
 
+	/*
+	 * 似乎在以下使用kvm_arch->wall_clock (gpa_t):
+	 *   - arch/x86/kvm/x86.c|3690| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+	 *   - arch/x86/kvm/x86.c|3697| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+	 *   - arch/x86/kvm/x86.c|4026| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+	 *   - arch/x86/kvm/x86.c|4032| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+	 */
 	gpa_t wall_clock;
 
 	bool mwait_in_guest;
@@ -1013,22 +1311,159 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2542| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *   - arch/x86/kvm/x86.c|2599| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_write = data;
+	 *   - arch/x86/kvm/x86.c|11075| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2562| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2600| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 */
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2889| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2918| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2890| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = data;
+	 *   - arch/x86/kvm/x86.c|2919| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 cur_tsc_write;
+	/*
+	 * cur_tsc_offset = 18443709364331608385,
+	 *
+	 * crash> eval 18443709364331608385
+	 * hexadecimal: fff537f2a987f941
+	 *     decimal: 18443709364331608385  (-3034709377943231)
+	 *       octal: 1777651577125141774501
+	 *      binary: 1111111111110101001101111111001010101001100001111111100101000001
+	 *
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu0/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu1/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu2/tsc-offset
+	 * -3034709377943231
+	 * # cat /sys/kernel/debug/kvm/2967-13/vcpu3/tsc-offset
+	 * -3034709377943231
+	 *
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 */
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 *
+	 * 测试的例子是cur_tsc_generation = 1
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_gtod_sync_lock:
+	 *   - arch/x86/kvm/x86.c|2508| <<kvm_synchronize_tsc>> spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2516| <<kvm_synchronize_tsc>> spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2745| <<kvm_gen_update_masterclock>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2747| <<kvm_gen_update_masterclock>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2765| <<get_kvmclock_ns>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2767| <<get_kvmclock_ns>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2773| <<get_kvmclock_ns>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2877| <<kvm_guest_time_update>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2883| <<kvm_guest_time_update>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|5985| <<kvm_arch_vm_ioctl>> spin_lock_irq(&ka->pvclock_gtod_sync_lock);
+	 *   - arch/x86/kvm/x86.c|5991| <<kvm_arch_vm_ioctl>> spin_unlock_irq(&ka->pvclock_gtod_sync_lock);
+	 *   - arch/x86/kvm/x86.c|8010| <<kvm_hyperv_tsc_notifier>> spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|8012| <<kvm_hyperv_tsc_notifier>> spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+	 *   - arch/x86/kvm/x86.c|10850| <<kvm_arch_init_vm>> spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
+	 */
 	spinlock_t pvclock_gtod_sync_lock;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched 
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|2780| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|2853| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|2984| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6090| <<kvm_arch_vm_ioctl>> now_ns = ka->master_kernel_ns;
+	 *
+	 * 猜测是内核启动了的ns
+	 */
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|2945| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3079| <<get_kvmclock_ns>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3216| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *
+	 * 猜测是内核启动了的cycle
+	 */
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_fn:
+	 *   - arch/x86/kvm/x86.c|3703| <<kvmclock_update_fn>> kvmclock_update_work);
+	 *   - arch/x86/kvm/x86.c|3737| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work,
+	 *   - arch/x86/kvm/x86.c|3764| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|11787| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|11835| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3754| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3765| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11435| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11788| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
@@ -1043,7 +1478,22 @@ struct kvm_arch {
 	int audit_point;
 	#endif
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2950| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|11175| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 */
 	bool backwards_tsc_observed;
+	/*
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 *
+	 * 如果写入的是MSR_KVM_SYSTEM_TIME,表明Guest使用的是旧版kvmclock,不支持Master Clock模式,
+	 * 此时要设置kvm->arch.boot_vcpu_runs_old_kvmclock = 1,并对当前vCPU(即vCPU0)发送一个KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 这最终会导致kvm->arch.use_master_clock = 0
+	 */
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
 
@@ -1178,6 +1628,18 @@ struct kvm_vcpu_stat {
 struct x86_instruction_info;
 
 struct msr_data {
+	/*
+	 * 在以下设置msr_data->host_initiated:
+	 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+	 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+	 *
+	 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+	 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+	 * msr_info->host_initiated就为false.
+	 */
 	bool host_initiated;
 	u32 index;
 	u64 data;
@@ -1550,6 +2012,14 @@ extern u64 kvm_mce_cap_supported;
  */
 #define EMULTYPE_NO_DECODE	    (1 << 0)
 #define EMULTYPE_TRAP_UD	    (1 << 1)
+/*
+ * 在以下使用EMULTYPE_SKIP:
+ *   - arch/x86/kvm/svm/svm.c|347| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/vmx/vmx.c|1816| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+ *   - arch/x86/kvm/x86.c|7315| <<handle_emulation_failure>> if (emulation_type & EMULTYPE_SKIP) {
+ *   - arch/x86/kvm/x86.c|7610| <<x86_decode_emulated_instruction>> if (!(emulation_type & EMULTYPE_SKIP) &&
+ *   - arch/x86/kvm/x86.c|7683| <<x86_emulate_instruction>> if (emulation_type & EMULTYPE_SKIP) {
+ */
 #define EMULTYPE_SKIP		    (1 << 2)
 #define EMULTYPE_ALLOW_RETRY_PF	    (1 << 3)
 #define EMULTYPE_TRAP_UD_FORCED	    (1 << 4)
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 211ba3375ee9..c67004588857 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -126,6 +126,14 @@
 
 #define MSR_IA32_TSX_CTRL		0x00000122
 #define TSX_CTRL_RTM_DISABLE		BIT(0)	/* Disable RTM feature */
+/*
+ * 在以下使用TSX_CTRL_CPUID_CLEAR:
+ *   - arch/x86/kernel/cpu/tsx.c|37| <<tsx_disable>> tsx |= TSX_CTRL_CPUID_CLEAR;
+ *   - arch/x86/kernel/cpu/tsx.c|56| <<tsx_enable>> tsx &= ~TSX_CTRL_CPUID_CLEAR;
+ *   - arch/x86/kvm/cpuid.c|1214| <<kvm_cpuid>> (data & TSX_CTRL_CPUID_CLEAR))
+ *   - arch/x86/kvm/vmx/vmx.c|2344| <<vmx_set_msr>> if (data & ~(TSX_CTRL_RTM_DISABLE | TSX_CTRL_CPUID_CLEAR))
+ *   - arch/x86/kvm/vmx/vmx.c|7110| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].mask = ~(u64)TSX_CTRL_CPUID_CLEAR;
+ */
 #define TSX_CTRL_CPUID_CLEAR		BIT(1)	/* Disable TSX enumeration */
 
 /* SRBDS support */
diff --git a/arch/x86/include/asm/pvclock-abi.h b/arch/x86/include/asm/pvclock-abi.h
index 1436226efe3e..63f1da735cbe 100644
--- a/arch/x86/include/asm/pvclock-abi.h
+++ b/arch/x86/include/asm/pvclock-abi.h
@@ -23,12 +23,54 @@
  * and after reading them.
  */
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ *
+ *
+ * 在kvm vm上测试的时候, 下面的都不变
+ * system_time=23867275
+ * tsc_timestamp=206742471, version=4, tsc_to_system_mul=2532092704, tsc_shift=255
+ * kvm_clock_read()一直在变
+ */
 struct pvclock_vcpu_time_info {
+	/*
+	 * 奇数表示Host正在修改,偶数表示已修改完毕
+	 */
 	u32   version;
 	u32   pad0;
+	/*
+	 * 更新pvti时,vCPU的vTSC 
+	 */
 	u64   tsc_timestamp;
+	/*
+	 * 当dmesg是14.903088的时候,
+	 * kvm_clock_read()返回15169858774 (15.169858774)
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	u64   system_time;
+	/*
+	 * 用于将tsc转换为nsec
+	 */
 	u32   tsc_to_system_mul;
+	/*
+	 * 用于将tsc转换为nsec
+	 */
 	s8    tsc_shift;
 	u8    flags;
 	u8    pad[2];
@@ -41,6 +83,15 @@ struct pvclock_wall_clock {
 } __attribute__((__packed__));
 
 #define PVCLOCK_TSC_STABLE_BIT	(1 << 0)
+/*
+ * 在以下使用PVCLOCK_GUEST_STOPPED:
+ *   - arch/x86/kernel/kvmclock.c|152| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+ *   - arch/x86/kernel/kvmclock.c|153| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kernel/pvclock.c|80| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+ *   - arch/x86/kernel/pvclock.c|81| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+ *   - arch/x86/kvm/x86.c|2769| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
+ *   - arch/x86/kvm/x86.c|2772| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+ */
 #define PVCLOCK_GUEST_STOPPED	(1 << 1)
 /* PVCLOCK_COUNTS_FROM_ZERO broke ABI and can't be used anymore. */
 #define PVCLOCK_COUNTS_FROM_ZERO (1 << 2)
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 19b695ff2c68..269fb08ac2be 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -39,6 +39,12 @@ bool pvclock_read_retry(const struct pvclock_vcpu_time_info *src,
  * Scale a 64-bit delta by scaling and multiplying by a 32-bit fraction,
  * yielding a 64-bit result.
  */
+/*
+ * called by:
+ *   - arch/x86/include/asm/pvclock.h|99| <<__pvclock_read_cycles>> u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
+ *   - arch/x86/kvm/x86.c|2378| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.h|360| <<nsec_to_cycles>> return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
+ */
 static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 {
 	u64 product;
@@ -78,10 +84,24 @@ static inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shift)
 	return product;
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+ *   - arch/x86/kernel/pvclock.c|76| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+ *   - arch/x86/kvm/x86.c|3072| <<get_kvmclock_ns>> ret = __pvclock_read_cycles(&hv_clock, rdtsc());
+ *   - drivers/ptp/ptp_kvm_x86.c|91| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair.tsc);
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
+	/*
+	 * 更新pvti时,vCPU的vTSC
+	 */
 	u64 delta = tsc - src->tsc_timestamp;
+	/*
+	 * src->tsc_to_system_mul和src->tsc_shift:
+	 * 用于将tsc转换为nsec
+	 */
 	u64 offset = pvclock_scale_delta(delta, src->tsc_to_system_mul,
 					     src->tsc_shift);
 	return src->system_time + offset;
diff --git a/arch/x86/include/asm/xen/interface.h b/arch/x86/include/asm/xen/interface.h
index baca0b00ef76..59b4351367ba 100644
--- a/arch/x86/include/asm/xen/interface.h
+++ b/arch/x86/include/asm/xen/interface.h
@@ -104,6 +104,13 @@ DEFINE_GUEST_HANDLE(xen_ulong_t);
 #define MACH2PHYS_NR_ENTRIES  ((MACH2PHYS_VIRT_END-MACH2PHYS_VIRT_START)>>__MACH2PHYS_SHIFT)
 
 /* Maximum number of virtual CPUs in multi-processor guests. */
+/*
+ * 在以下使用MAX_VIRT_CPUS:
+ *   - arch/x86/kvm/xen.h|125| <<global>> struct compat_vcpu_info vcpu_info[MAX_VIRT_CPUS];
+ *   - include/xen/interface/xen.h|560| <<global>> struct vcpu_info vcpu_info[MAX_VIRT_CPUS];
+ *   - arch/x86/xen/enlighten.c|190| <<xen_vcpu_info_reset>> if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS) {
+ *   - arch/x86/xen/smp.c|135| <<xen_smp_cpus_done>> if (xen_vcpu_nr(cpu) < MAX_VIRT_CPUS)
+ */
 #define MAX_VIRT_CPUS 32
 
 /*
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 950afebfba88..ec41631d2082 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -34,6 +34,13 @@
 #define KVM_FEATURE_ASYNC_PF_INT	14
 #define KVM_FEATURE_MSI_EXT_DEST_ID	15
 
+/*
+ * 在以下使用KVM_HINTS_REALTIME:
+ *   - arch/x86/kernel/kvm.c|459| <<pv_tlb_flush_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|471| <<pv_sched_yield_supported>> !kvm_para_has_hint(KVM_HINTS_REALTIME) &&
+ *   - arch/x86/kernel/kvm.c|946| <<kvm_spinlock_init>> if (kvm_para_has_hint(KVM_HINTS_REALTIME)) {
+ *   - drivers/cpuidle/cpuidle-haltpoll.c|99| <<haltpoll_want>> return kvm_para_has_hint(KVM_HINTS_REALTIME) || force;
+ */
 #define KVM_HINTS_REALTIME      0
 
 /* The last 8 bits are used to indicate how to interpret the flags field
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index d5c691a3208b..f3becec4bfdf 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -869,6 +869,10 @@ void ioapic_set_alloc_attr(struct irq_alloc_info *info, int node,
 	info->ioapic.valid = 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|1055| <<mp_map_pin_to_irq>> ioapic_copy_alloc_attr(&tmp, info, gsi, ioapic, pin);
+ */
 static void ioapic_copy_alloc_attr(struct irq_alloc_info *dst,
 				   struct irq_alloc_info *src,
 				   u32 gsi, int ioapic_idx, int pin)
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index fb67ed5e7e6a..278c605e038c 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -218,6 +218,15 @@ static int reserve_irq_vector(struct irq_data *irqd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/vector.c|265| <<assign_irq_vector>> ret = assign_vector_locked(irqd, vector_searchmask);
+ *   - arch/x86/kernel/apic/vector.c|279| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, vector_searchmask))
+ *   - arch/x86/kernel/apic/vector.c|285| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, vector_searchmask))
+ *   - arch/x86/kernel/apic/vector.c|290| <<assign_irq_vector_any_locked>> if (!assign_vector_locked(irqd, cpumask_of_node(node)))
+ *   - arch/x86/kernel/apic/vector.c|295| <<assign_irq_vector_any_locked>> return assign_vector_locked(irqd, cpu_online_mask);
+ *   - arch/x86/kernel/apic/vector.c|868| <<apic_set_affinity>> err = assign_vector_locked(irqd, vector_searchmask);
+ */
 static int
 assign_vector_locked(struct irq_data *irqd, const struct cpumask *dest)
 {
diff --git a/arch/x86/kernel/crash.c b/arch/x86/kernel/crash.c
index 54ce999ed321..2598591a9769 100644
--- a/arch/x86/kernel/crash.c
+++ b/arch/x86/kernel/crash.c
@@ -56,6 +56,12 @@ struct crash_memmap_data {
  *
  * protected by rcu.
  */
+/*
+ * 在以下使用crash_vmclear_loaded_vmcss:
+ *   - arch/x86/kernel/crash.c|67| <<cpu_crash_vmclear_loaded_vmcss>> do_vmclear_operation = rcu_dereference(crash_vmclear_loaded_vmcss);
+ *   - arch/x86/kvm/vmx/vmx.c|8159| <<vmx_exit>> RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+ *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_init>> rcu_assign_pointer(crash_vmclear_loaded_vmcss, crash_vmclear_local_loaded_vmcss);
+ */
 crash_vmclear_fn __rcu *crash_vmclear_loaded_vmcss = NULL;
 EXPORT_SYMBOL_GPL(crash_vmclear_loaded_vmcss);
 
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index a26643dc6bd6..30497a5e2ccb 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -453,6 +453,11 @@ static int kvm_cpu_online(unsigned int cpu)
 
 static DEFINE_PER_CPU(cpumask_var_t, __pv_cpu_mask);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|613| <<kvm_alloc_cpumask>> if (pv_tlb_flush_supported() || pv_ipi_supported())
+ *   - arch/x86/kernel/kvm.c|719| <<kvm_guest_init>> if (pv_tlb_flush_supported()) {
+ */
 static bool pv_tlb_flush_supported(void)
 {
 	return (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &&
@@ -868,6 +873,10 @@ static void kvm_kick_cpu(int cpu)
 
 #include <asm/qspinlock.h>
 
+/*
+ * 在以下使用kvm_wait():
+ *   - arch/x86/kernel/kvm.c|987| <<kvm_spinlock_init>> pv_ops.lock.wait = kvm_wait;
+ */
 static void kvm_wait(u8 *ptr, u8 val)
 {
 	if (in_nmi())
@@ -884,6 +893,9 @@ static void kvm_wait(u8 *ptr, u8 val)
 	} else {
 		local_irq_disable();
 
+		/*
+		 * 缺少的注释: safe_halt() will enable IRQ
+		 */
 		if (READ_ONCE(*ptr) == val)
 			safe_halt();
 
@@ -960,6 +972,21 @@ void __init kvm_spinlock_init(void)
 
 	pr_info("PV spinlocks enabled\n");
 
+
+	/*
+	 * Implement paravirt qspinlocks; the general idea is to halt the vcpus instead
+	 * of spinning them.
+	 *
+	 * This relies on the architecture to provide two paravirt hypercalls:
+	 *
+	 *   pv_wait(u8 *ptr, u8 val) -- suspends the vcpu if *ptr == val
+	 *   pv_kick(cpu)             -- wakes a suspended vcpu
+	 *
+	 * Using these we implement __pv_queued_spin_lock_slowpath() and
+	 * __pv_queued_spin_unlock() to replace native_queued_spin_lock_slowpath() and
+	 * native_queued_spin_unlock().
+	 */
+
 	__pv_init_lock_hash();
 	pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_ops.lock.queued_spin_unlock =
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ad273e5861c1..05b9a7d18e1b 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -67,6 +67,10 @@ static inline struct pvclock_vsyscall_time_info *this_cpu_hvclock(void)
  * have elapsed since the hypervisor wrote the data. So we try to account for
  * that with system time
  */
+/*
+ * 在以下使用kvm_get_wallclock():
+ *   - arch/x86/kernel/kvmclock.c|349| <<kvmclock_init>> x86_platform.get_wallclock = kvm_get_wallclock;
+ */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
@@ -80,6 +84,24 @@ static int kvm_set_wallclock(const struct timespec64 *now)
 	return -ENODEV;
 }
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ */
 static u64 kvm_clock_read(void)
 {
 	u64 ret;
@@ -163,6 +185,10 @@ static int kvm_cs_enable(struct clocksource *cs)
 	return 0;
 }
 
+/*
+ * kvm_clock 作为一个clocksource,其频率为1GHz,实际上其read回调返回的值
+ * 是就是System Time(即Host Boot Time + Kvmclock Offset)的读数,单位为纳秒
+ */
 struct clocksource kvm_clock = {
 	.name	= "kvm-clock",
 	.read	= kvm_clock_get_cycles,
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index eda37df016f0..c892c866d7fb 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -36,6 +36,11 @@ unsigned long pvclock_tsc_khz(struct pvclock_vcpu_time_info *src)
 	return pv_tsc_khz;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|154| <<kvm_check_and_clear_guest_paused>> pvclock_touch_watchdogs();
+ *   - arch/x86/kernel/pvclock.c|82| <<pvclock_clocksource_read>> pvclock_touch_watchdogs();
+ */
 void pvclock_touch_watchdogs(void)
 {
 	touch_softlockup_watchdog_sync();
@@ -44,6 +49,12 @@ void pvclock_touch_watchdogs(void)
 	reset_hung_task_detector();
 }
 
+/*
+ * 在以下使用atomic的last_value:
+ *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+ *   - arch/x86/kernel/pvclock.c|128| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+ *   - arch/x86/kernel/pvclock.c|132| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+ */
 static atomic64_t last_value = ATOMIC64_INIT(0);
 
 void pvclock_resume(void)
@@ -64,6 +75,26 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * - CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock is
+ * affected by NTP adjustments and can jump forward and backward when a system
+ * administrator adjusts system time.
+ * - CLOCK_MONOTONIC clock gives the time since a fixed starting point-usually since
+ * you booted the system. This clock is affected by NTP, but it can't jump backward.
+ * - CLOCK_MONOTONIC_RAW clock gives the same time as CLOCK_MONOTONIC, but this clock
+ * is not affected by NTP adjustments. 
+ * - CLOCK_REALTIME_COARSE and CLOCK_MONOTONIC_COARSE are faster but less-accurate
+ * variants of CLOCK_REALTIME and CLOCK_MONOTONIC.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 当dmesg是14.903088的时候,
+ * kvm_clock_read()返回15169858774 (15.169858774)
+ *
+ * PerCPUTime = ((RDTSC() - tsc_timestamp) >> tsc_shift) * tsc_to_system_mul + system_time
+ */
 u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 {
 	unsigned version;
@@ -100,6 +131,12 @@ u64 pvclock_clocksource_read(struct pvclock_vcpu_time_info *src)
 	 * updating at the same time, and one of them could be slightly behind,
 	 * making the assumption that last_value always go forward fail to hold.
 	 */
+	/*
+	 * 在以下使用atomic的last_value:
+	 *   - arch/x86/kernel/pvclock.c|56| <<pvclock_resume>> atomic64_set(&last_value, 0);
+	 *   - arch/x86/kernel/pvclock.c|128| <<pvclock_clocksource_read>> last = atomic64_read(&last_value);
+	 *   - arch/x86/kernel/pvclock.c|132| <<pvclock_clocksource_read>> last = atomic64_cmpxchg(&last_value, last, ret);
+	 */
 	last = atomic64_read(&last_value);
 	do {
 		if (ret < last)
@@ -143,12 +180,22 @@ void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 	set_normalized_timespec64(ts, now.tv_sec, now.tv_nsec);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|343| <<kvmclock_init>> pvclock_set_pvti_cpu0_va(hv_clock_boot);
+ *   - arch/x86/xen/time.c|486| <<xen_setup_vsyscall_time_info>> pvclock_set_pvti_cpu0_va(xen_clock);
+ */
 void pvclock_set_pvti_cpu0_va(struct pvclock_vsyscall_time_info *pvti)
 {
 	WARN_ON(vclock_was_used(VDSO_CLOCKMODE_PVCLOCK));
 	pvti_cpu0_va = pvti;
 }
 
+/*
+ * called by:
+ *   - arch/x86/entry/vdso/vma.c|206| <<vvar_fault>> pvclock_get_pvti_cpu0_va();
+ *   - drivers/ptp/ptp_kvm_x86.c|31| <<kvm_arch_ptp_init>> hv_clock = pvclock_get_pvti_cpu0_va();
+ */
 struct pvclock_vsyscall_time_info *pvclock_get_pvti_cpu0_va(void)
 {
 	return pvti_cpu0_va;
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 57ec01192180..d2bb06ca9b1e 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -30,9 +30,15 @@
 #include <asm/i8259.h>
 #include <asm/uv/uv.h>
 
+/*
+ * cpu频率
+ */
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
 
+/*
+ * tsc频率
+ */
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index b4da665bb892..a7210d9f985b 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -399,6 +399,11 @@ static __always_inline void kvm_cpu_cap_mask(enum cpuid_leafs leaf, u32 mask)
 	__kvm_cpu_cap_mask(leaf);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|907| <<svm_set_cpu_caps>> kvm_set_cpu_caps();
+ *   - arch/x86/kvm/vmx/vmx.c|7454| <<vmx_set_cpu_caps>> kvm_set_cpu_caps();
+ */
 void kvm_set_cpu_caps(void)
 {
 	unsigned int f_nx = is_efer_nx() ? F(NX) : 0;
@@ -663,6 +668,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1011| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 5e5de05a8fbf..fc629ed18159 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -5481,6 +5481,10 @@ void init_decode_cache(struct x86_emulate_ctxt *ctxt)
 	ctxt->mem_read.end = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7715| <<x86_emulate_instruction>> r = x86_emulate_insn(ctxt);
+ */
 int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index f00830e5202f..9214d1abb66c 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1168,6 +1168,10 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 	mutex_unlock(&hv->hv_lock);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2753| <<kvm_gen_update_masterclock>> kvm_hv_invalidate_tsc_page(kvm);
+ */
 void kvm_hv_invalidate_tsc_page(struct kvm *kvm)
 {
 	struct kvm_hv *hv = to_kvm_hv(kvm);
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 17fa4ab1b834..a4a57dc458e6 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -42,6 +42,26 @@
 #include "cpuid.h"
 #include "hyperv.h"
 
+/*
+ * The x2APIC architecture extends the xAPIC architecture in a backward
+ * compatible manner and provides forward extendability for future Intel
+ * platform innovations. Specifically, the x2APIC architecture does the
+ * following:
+ *
+ * - Retains all key elements of compatibility to the xAPIC architecture:
+ *   1. delivery modes,
+ *   2. interrupt and processor priorities,
+ *   3. interrupt sources,
+ *   4. interrupt destination types;
+ * - Provides extensions to scal processor addressability for both the logical
+ *   and physical destination modes;
+ * - Adds new features to enhance performance of interrupt delivery;
+ * - Reduces complexity of logical destination mode interrupt delivery on link
+ *   based platform architectures/
+ * - Use MSR programming interface to access APIC registers when operating in
+ *   XAPIC mode.
+ */
+
 #ifndef CONFIG_X86_64
 #define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
 #else
@@ -57,9 +77,21 @@
 #define APIC_VERSION			(0x14UL | ((KVM_APIC_LVT_NUM - 1) << 16))
 #define LAPIC_MMIO_LENGTH		(1 << 12)
 /* followed define is not in apicdef.h */
+/*
+ * 在以下使用MAX_APIC_VECTOR:
+ *   - arch/x86/kvm/lapic.c|514| <<find_highest_vector>> for (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;
+ *   - arch/x86/kvm/lapic.c|534| <<count_vectors>> for (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {
+ *   - arch/x86/kvm/lapic.c|649| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+ */
 #define MAX_APIC_VECTOR			256
 #define APIC_VECTORS_PER_REG		32
 
+/*
+ * 在以下使用lapic_timer_advance_dynamic:
+ *   - arch/x86/kvm/lapic.c|1754| <<__kvm_wait_lapic_expire>> if (lapic_timer_advance_dynamic) {
+ *   - arch/x86/kvm/lapic.c|2693| <<kvm_create_lapic>> lapic_timer_advance_dynamic = true;
+ *   - arch/x86/kvm/lapic.c|2696| <<kvm_create_lapic>> lapic_timer_advance_dynamic = false;
+ */
 static bool lapic_timer_advance_dynamic __read_mostly;
 #define LAPIC_TIMER_ADVANCE_ADJUST_MIN	100	/* clock cycles */
 #define LAPIC_TIMER_ADVANCE_ADJUST_MAX	10000	/* clock cycles */
@@ -73,6 +105,12 @@ static inline int apic_test_vector(int vec, void *bitmap)
 	return test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|123| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+ *   - arch/x86/kvm/ioapic.c|195| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/ioapic.c|302| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+ */
 bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -91,7 +129,24 @@ static inline int __apic_test_and_clear_vector(int vec, void *bitmap)
 	return __test_and_clear_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * 在以下使用apic_hw_disabled:
+ *   - arch/x86/kvm/lapic.c|2312| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2387| <<kvm_lapic_set_base>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2391| <<kvm_lapic_set_base>> static_branch_inc(&apic_hw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3046| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.h|236| <<kvm_apic_hw_enabled>> if (static_branch_unlikely(&apic_hw_disabled.key))
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);
+/*
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|327| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|329| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2315| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|2588| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3047| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.h|245| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);
 
 static inline int apic_enabled(struct kvm_lapic *apic)
@@ -106,6 +161,15 @@ static inline int apic_enabled(struct kvm_lapic *apic)
 	(LVT_MASK | APIC_MODE_MASK | APIC_INPUT_POLARITY | \
 	 APIC_LVT_REMOTE_IRR | APIC_LVT_LEVEL_TRIGGER)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|260| <<kvm_recalculate_apic_map>> max_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));
+ *   - arch/x86/kvm/lapic.c|283| <<kvm_recalculate_apic_map>> x2apic_id = kvm_x2apic_id(apic);
+ *   - arch/x86/kvm/lapic.c|822| <<kvm_apic_match_physical_addr>> return mda == kvm_x2apic_id(apic);
+ *   - arch/x86/kvm/lapic.c|830| <<kvm_apic_match_physical_addr>> if (kvm_x2apic_id(apic) > 0xff && mda == kvm_x2apic_id(apic))
+ *
+ * 返回apic->vcpu->vcpu_id
+ */
 static inline u32 kvm_x2apic_id(struct kvm_lapic *apic)
 {
 	return apic->vcpu->vcpu_id;
@@ -113,9 +177,22 @@ static inline u32 kvm_x2apic_id(struct kvm_lapic *apic)
 
 static bool kvm_can_post_timer_interrupt(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用pi_inject_timer:
+	 *   - arch/x86/kvm/x86.c|256| <<global>> module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
+	 *   - arch/x86/kvm/x86.h|352| <<global>> extern int pi_inject_timer
+	 *   - arch/x86/kvm/lapic.c|116| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
+	 *   - arch/x86/kvm/x86.c|9858| <<kvm_arch_init>> if (pi_inject_timer == -1)
+	 *   - arch/x86/kvm/x86.c|9859| <<kvm_arch_init>> pi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);
+	 */
 	return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2026| <<start_hv_timer>> if (!kvm_can_use_hv_timer(vcpu))
+ *   - arch/x86/kvm/x86.c|2078| <<handle_fastpath_set_tscdeadline>> if (!kvm_can_use_hv_timer(vcpu))
+ */
 bool kvm_can_use_hv_timer(struct kvm_vcpu *vcpu)
 {
 	return kvm_x86_ops.set_hv_timer
@@ -124,11 +201,27 @@ bool kvm_can_use_hv_timer(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_can_use_hv_timer);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1733| <<apic_timer_expired>> if (kvm_use_posted_timer_interrupt(apic->vcpu)) {
+ */
 static bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)
 {
 	return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
 }
 
+/*
+ * 为了加速对于目标VCPU的查找,在kvm.arch.apic_map中保存了kvm_apic_map结构体.
+ * phys_map成员和logical_map成员记录了RTE的destination filed同VLAPIC结构体的
+ * 对应的关系,分别对应physical mode和logic mode.在发送中断的时候,如果有该map表,
+ * 且中断不是lowest priority和广播,则通过RTE的destination filed就可以直接找到
+ * 目标VCPU，进行快速的分发.否则需要遍历所有的VCPU,逐一的和RTE的destination
+ * field进行匹配.
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|322| <<kvm_recalculate_apic_map>> if (!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))
+ *   - arch/x86/kvm/lapic.c|1006| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+ */
 static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 		u32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {
 	switch (map->mode) {
@@ -181,6 +274,32 @@ enum {
 	DIRTY
 };
 
+/*
+ * 5.4的例子
+ * kvm_make_vcpus_request_mask
+ * kvm_make_all_cpus_request_except
+ * kvm_make_all_cpus_request
+ * kvm_make_scan_ioapic_request
+ * kvm_recalculate_apic_map
+ * kvm_lapic_reg_write
+ * handle_fastpath_set_msr_irqoff
+ * vmx_vcpu_run
+ * vcpu_enter_guest
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_66
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2192| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2443| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2680| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2686| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/x86.c|531| <<kvm_set_apic_base>> kvm_recalculate_apic_map(vcpu->kvm);
+ */
 void kvm_recalculate_apic_map(struct kvm *kvm)
 {
 	struct kvm_apic_map *new, *old = NULL;
@@ -281,12 +400,25 @@ void kvm_recalculate_apic_map(struct kvm *kvm)
 	kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2164| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_set_spiv(apic, val & mask);
+ *   - arch/x86/kvm/lapic.c|2493| <<kvm_lapic_reset>> apic_set_spiv(apic, 0xff);
+ *   - arch/x86/kvm/lapic.c|2758| <<kvm_apic_set_state>> apic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));
+ */
 static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 {
 	bool enabled = val & APIC_SPIV_APIC_ENABLED;
 
 	kvm_lapic_set_reg(apic, APIC_SPIV, val);
 
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|341| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|342| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2331| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|246| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (enabled != apic->sw_enabled) {
 		apic->sw_enabled = enabled;
 		if (enabled)
@@ -294,6 +426,18 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 		else
 			static_branch_inc(&apic_sw_disabled.key);
 
+		/*
+		 * 在以下使用kvm_arch->apic_map_dirty:
+		 *   - arch/x86/kvm/lapic.c|218| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+		 *   - arch/x86/kvm/lapic.c|226| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|300| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|323| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|334| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|340| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|346| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|362| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|2376| <<kvm_lapic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 */
 		atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 	}
 
@@ -314,6 +458,11 @@ static inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2259| <<kvm_lapic_reg_write(APIC_DFR)>> kvm_apic_set_dfr(apic, val | 0x0FFFFFFF);
+ *   - arch/x86/kvm/lapic.c|2599| <<kvm_lapic_reset>> kvm_apic_set_dfr(apic, 0xffffffffU);
+ */
 static inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)
 {
 	kvm_lapic_set_reg(apic, APIC_DFR, val);
@@ -325,6 +474,10 @@ static inline u32 kvm_apic_calc_x2apic_ldr(u32 id)
 	return ((id >> 4) << 16) | (1 << (id & 0xf));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2437| <<kvm_lapic_set_base>> kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 {
 	u32 ldr = kvm_apic_calc_x2apic_ldr(id);
@@ -351,6 +504,17 @@ static inline int apic_lvtt_period(struct kvm_lapic *apic)
 	return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1526| <<__apic_read>> if (apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|1691| <<apic_update_lvtt>> if (apic_lvtt_tscdeadline(apic) != (timer_mode ==
+ *   - arch/x86/kvm/lapic.c|1815| <<kvm_apic_inject_pending_timer_irqs>> if (apic_lvtt_tscdeadline(apic)) {
+ *   - arch/x86/kvm/lapic.c|1839| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+ *   - arch/x86/kvm/lapic.c|2119| <<start_sw_timer>> else if (apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|2328| <<kvm_lapic_reg_write>> if (apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|2464| <<kvm_get_lapic_tscdeadline_msr>> if (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))
+ *   - arch/x86/kvm/lapic.c|2474| <<kvm_set_lapic_tscdeadline_msr>> if (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))
+ */
 static inline int apic_lvtt_tscdeadline(struct kvm_lapic *apic)
 {
 	return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
@@ -361,6 +525,12 @@ static inline int apic_lvt_nmi_mode(u32 lvt_val)
 	return (lvt_val & (APIC_MODE_MASK | APIC_LVT_MASKED)) == APIC_DM_NMI;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|165| <<kvm_vcpu_after_set_cpuid>> kvm_apic_set_version(vcpu);
+ *   - arch/x86/kvm/lapic.c|2557| <<kvm_lapic_reset>> kvm_apic_set_version(apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|2845| <<kvm_apic_set_state>> kvm_apic_set_version(vcpu);
+ */
 void kvm_apic_set_version(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -390,6 +560,11 @@ static const unsigned int apic_lvt_mask[KVM_APIC_LVT_NUM] = {
 	LVT_MASK		/* LVTERR */
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|524| <<apic_search_irr>> return find_highest_vector(apic->regs + APIC_IRR);
+ *   - arch/x86/kvm/lapic.c|610| <<apic_find_highest_isr>> result = find_highest_vector(apic->regs + APIC_ISR);
+ */
 static int find_highest_vector(void *bitmap)
 {
 	int vec;
@@ -405,6 +580,10 @@ static int find_highest_vector(void *bitmap)
 	return -1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2460| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+ */
 static u8 count_vectors(void *bitmap)
 {
 	int vec;
@@ -419,6 +598,11 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|607| <<kvm_apic_update_irr>> return __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|3705| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+ */
 bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 {
 	u32 i, vec;
@@ -449,6 +633,10 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6576| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -498,12 +686,20 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3531| <<nested_vmx_run>> kvm_apic_clear_irr(vcpu, vmx->nested.posted_intr_nv);
+ */
 void kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)
 {
 	apic_clear_irr(vec, vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_clear_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2705| <<kvm_get_apic_interrupt>> apic_set_isr(vector, apic);
+ */
 static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -576,6 +772,12 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * 在以下使用kvm_lapic_find_highest_irr():
+ *   - arch/x86/kvm/svm/svm.c|4514| <<global>> .sync_pir_to_irr = kvm_lapic_find_highest_irr,
+ *   - arch/x86/kvm/vmx/vmx.c|6593| <<vmx_sync_pir_to_irr>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ *   - arch/x86/kvm/x86.c|10209| <<update_cr8_intercept>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ */
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
 	/* This may race with setting of irr in __apic_accept_irq() and
@@ -651,6 +853,36 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 	return count;
 }
 
+/*
+ * pv-eoi (减少vmexit的数量)
+ *
+ * x86 PC体系架构中的中断控制器,早先是8259A,现在更普遍使用的是APIC,他们处理中断的流程遵循如下流程:
+ *
+ * 1. 外部设备产生一个中断,如果该中断没有被屏蔽掉,中断控制器将IRR寄存器中相应的位置1,表示收到中断,但是还未提交给CPU处理.
+ * 2. 中断控制器将该中断提交给CPU,CPU收到中断请求后,会应答中断控制器.
+ * 3. 中断控制器收到CPU的中断应答后,将IRR寄存器中相应的位清0,并将ISR寄存器相应的位置1,表示CPU正在处理该中断.
+ * 4. 当该中断的处理程序结束以前,需要将中断控制器的EOI寄存器对应的位置1,表示CPU完成了对该中断的处理.
+ * 5. 中断控制器收到EOI后,ISR寄存器中相应的位清0，允许下次中断.
+ * 6. 在虚拟化场景中,该流程至少会导致两次VM Exit: 第一次是VMM截获到设备中断的时候,通知客户机退出,将这个中断注入到客户机中;
+ *    另外一次是当客户机操作系统处理完该中断后,写中断控制器的EOI寄存器,这是个MMIO操作,也会导致客户机退出.
+ *    在一个外部IO比较频繁的场景中,外部中断会导致大量的VM Exit,影响客户机的整体性能.
+ *
+ * PV-EOI其实就是通过半虚拟化的办法来优化上述的VM Exit影响,virtio也是使用这个思想来优化网络和磁盘;就EOI的优化来说,其思想本质上很简单:
+ *
+ * 1. 客户机和VMM协商,首先确定双方是否都能支持PV-EOI特性,如果成功,则进一步协商一块2 bytes的内存区间作为双方处理EOI的共享缓存;
+ * 2. 在VMM向客户机注入中断之前,会把缓存的最低位置1,表示客户机不需要通过写EOI寄存器;
+ * 3. 客户机在写EOI之前,如果发现该位被设置,则将该位清0;VMM轮询这个标志位,当检查到清0后,会更新模拟中断控制器中的EOI寄存器;
+ *    如果客户机发现该位未被设置,则继续使用MMIO或者MSR写EOI寄存器;
+ *
+ * 需要注意的是,为了保证客户机和VMM同时处理共享内存的性能和可靠性,目前KVM的PV-EOF方案采用了如下的优化措施:
+ *
+ * 1. VMM保障仅会在客户机VCPU的上下文中更改共享内存中的最低位,从而避免了客户机采用任何锁机制来与VMM进行同步;
+ * 2. 客户机必须使用原子的test_and_clear操作来更改共享内存中的最低位,这是因为VMM在任何时候都有可能设置或者清除该位;
+ *
+ * https://blog.csdn.net/luo_brian/article/details/8744025?utm_source=tuicool&utm_medium=referral
+ * https://fedoraproject.org/wiki/QA:Testcase_Virtualization_PV_EOI
+ */
+
 static int pv_eoi_put_user(struct kvm_vcpu *vcpu, u8 val)
 {
 
@@ -691,6 +923,10 @@ static void pv_eoi_set_pending(struct kvm_vcpu *vcpu)
 	__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2983| <<apic_sync_pv_eoi_from_guest>> pv_eoi_clr_pending(vcpu);
+ */
 static void pv_eoi_clr_pending(struct kvm_vcpu *vcpu)
 {
 	if (pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0) {
@@ -912,6 +1148,12 @@ static bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,
  * means that the interrupt should be dropped.  In this case, *bitmap would be
  * zero and *dst undefined.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1062| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+ *   - arch/x86/kvm/lapic.c|1104| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+ *   - arch/x86/kvm/lapic.c|1241| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+ */
 static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 		struct kvm_lapic **src, struct kvm_lapic_irq *irq,
 		struct kvm_apic_map *map, struct kvm_lapic ***dst,
@@ -1057,6 +1299,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|703| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+ *   - arch/x86/kvm/lapic.c|2606| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1233,6 +1480,11 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 	kvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2049| <<kvm_lapic_reg_write>> apic_set_eoi(apic);
+ *   - arch/x86/kvm/lapic.c|2736| <<apic_sync_pv_eoi_from_guest>> vector = apic_set_eoi(apic);
+ */
 static int apic_set_eoi(struct kvm_lapic *apic)
 {
 	int vector = apic_find_highest_isr(apic);
@@ -1273,6 +1525,11 @@ void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2095| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/x86.c|2018| <<handle_fastpath_set_x2apic_icr_irqoff>> kvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));
+ */
 void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 {
 	struct kvm_lapic_irq irq;
@@ -1644,6 +1901,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1749| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1857| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1949| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|1998| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2526| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1845,6 +2110,21 @@ static void cancel_hv_timer(struct kvm_lapic *apic)
 	WARN_ON(preemptible());
 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
 	static_call(kvm_x86_cancel_hv_timer)(apic->vcpu);
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	apic->lapic_timer.hv_timer_in_use = false;
 }
 
@@ -1864,6 +2144,21 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	ktimer->hv_timer_in_use = true;
 	hrtimer_cancel(&ktimer->timer);
 
@@ -1996,6 +2291,20 @@ static void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2163| <<kvm_lapic_reg_write>> kvm_lapic_reg_write(apic, APIC_ICR,
+ *   - arch/x86/kvm/lapic.c|2207| <<apic_mmio_write>> kvm_lapic_reg_write(apic, offset & 0xff0, val);
+ *   - arch/x86/kvm/lapic.c|2218| <<kvm_lapic_set_eoi>> kvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);
+ *   - arch/x86/kvm/lapic.c|2237| <<kvm_apic_write_nodecode>> kvm_lapic_reg_write(vcpu->arch.apic, offset, val);
+ *   - arch/x86/kvm/lapic.c|2841| <<kvm_x2apic_msr_write>> kvm_lapic_reg_write(apic, APIC_ICR2, (u32)(data >> 32));
+ *   - arch/x86/kvm/lapic.c|2842| <<kvm_x2apic_msr_write>> return kvm_lapic_reg_write(apic, reg, (u32)data);
+ *   - arch/x86/kvm/lapic.c|2875| <<kvm_hv_vapic_msr_write>> kvm_lapic_reg_write(apic, APIC_ICR2, (u32)(data >> 32));
+ *   - arch/x86/kvm/lapic.c|2876| <<kvm_hv_vapic_msr_write>> return kvm_lapic_reg_write(apic, reg, (u32)data);
+ *   - arch/x86/kvm/svm/avic.c|340| <<avic_incomplete_ipi_interception>> kvm_lapic_reg_write(apic, APIC_ICR2, icrh);
+ *   - arch/x86/kvm/svm/avic.c|341| <<avic_incomplete_ipi_interception>> kvm_lapic_reg_write(apic, APIC_ICR, icrl);
+ *   - arch/x86/kvm/svm/avic.c|507| <<avic_unaccel_trap_write>> kvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));
+ */
 int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 {
 	int ret = 0;
@@ -2174,6 +2483,10 @@ static int apic_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5286| <<handle_apic_access>> kvm_lapic_set_eoi(vcpu);
+ */
 void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 {
 	kvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);
@@ -2181,6 +2494,10 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
 /* emulate APIC access in a trap manner */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5287| <<handle_apic_write>> kvm_apic_write_nodecode(vcpu, offset);
+ */
 void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
@@ -2260,9 +2577,21 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * called:
+ *   - arch/x86/kvm/lapic.c|2437| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu, APIC_DEFAULT_PHYS_BASE |
+ *   - arch/x86/kvm/lapic.c|2472| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu,
+ *   - arch/x86/kvm/lapic.c|2716| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+ *   - arch/x86/kvm/x86.c|544| <<kvm_set_apic_base>> kvm_lapic_set_base(vcpu, msr_info->data);
+ */
 void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
 	u64 old_value = vcpu->arch.apic_base;
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_lapic *apic;
+	 */
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	if (!apic)
@@ -2292,6 +2621,9 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	if (((old_value ^ value) & X2APIC_ENABLE) && (value & X2APIC_ENABLE))
 		kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
 
+	/*
+	 * vmx+x86_set_virtual_apic_mode()
+	 */
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE))
 		static_call(kvm_x86_set_virtual_apic_mode)(vcpu);
 
@@ -2445,6 +2777,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11997| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
@@ -2470,6 +2806,12 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	apic->lapic_timer.timer.function = apic_timer_fn;
 	if (timer_advance_ns == -1) {
 		apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+		/*
+		 * 在以下使用lapic_timer_advance_dynamic:
+		 *   - arch/x86/kvm/lapic.c|1754| <<__kvm_wait_lapic_expire>> if (lapic_timer_advance_dynamic) {
+		 *   - arch/x86/kvm/lapic.c|2693| <<kvm_create_lapic>> lapic_timer_advance_dynamic = true;
+		 *   - arch/x86/kvm/lapic.c|2696| <<kvm_create_lapic>> lapic_timer_advance_dynamic = false;
+		 */
 		lapic_timer_advance_dynamic = true;
 	} else {
 		apic->lapic_timer.timer_advance_ns = timer_advance_ns;
@@ -2527,6 +2869,10 @@ void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|138| <<kvm_cpu_get_interrupt>> return kvm_get_apic_interrupt(v);
+ */
 int kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)
 {
 	int vector = kvm_apic_has_interrupt(vcpu);
@@ -2669,6 +3015,10 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
  * last entry. If yes, set EOI on guests's behalf.
  * Clear PV EOI in guest memory in any case.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2995| <<kvm_lapic_sync_from_vapic>> apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
+ */
 static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
@@ -2799,6 +3149,10 @@ int kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 	return kvm_lapic_reg_write(apic, reg, (u32)data);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5029| <<kvm_get_msr_common(APIC_BASE_MSR)>> return kvm_x2apic_msr_read(vcpu, msr_info->index, &msr_info->data);
+ */
 int kvm_x2apic_msr_read(struct kvm_vcpu *vcpu, u32 msr, u64 *data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2872,6 +3226,13 @@ int kvm_lapic_enable_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10934| <<vcpu_enter_guest>> kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11152| <<vcpu_block>> kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11376| <<kvm_arch_vcpu_ioctl_run>> kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11582| <<kvm_arch_vcpu_ioctl_get_mpstate>> kvm_apic_accept_events(vcpu);
+ */
 void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2925,6 +3286,17 @@ void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		else
 			vcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;
 	}
+	/*
+	 * 在以下使用KVM_APIC_SIPI:
+	 *   - arch/x86/kvm/lapic.c|1341| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3203| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe))
+	 *   - arch/x86/kvm/lapic.c|3204| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3216| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe)) {
+	 *   - arch/x86/kvm/lapic.c|3217| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|3803| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3807| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|11619| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	if (test_bit(KVM_APIC_SIPI, &pe)) {
 		clear_bit(KVM_APIC_SIPI, &apic->pending_events);
 		if (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {
@@ -2937,6 +3309,10 @@ void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9912| <<kvm_arch_exit>> kvm_lapic_exit();
+ */
 void kvm_lapic_exit(void)
 {
 	static_key_deferred_flush(&apic_hw_disabled);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 997c45a5963a..50a6a5bc662b 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -9,6 +9,17 @@
 #include "hyperv.h"
 
 #define KVM_APIC_INIT		0
+/*
+ * 在以下使用KVM_APIC_SIPI:
+ *   - arch/x86/kvm/lapic.c|1341| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/lapic.c|3203| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe))
+ *   - arch/x86/kvm/lapic.c|3204| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/lapic.c|3216| <<kvm_apic_accept_events>> if (test_bit(KVM_APIC_SIPI, &pe)) {
+ *   - arch/x86/kvm/lapic.c|3217| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/vmx/nested.c|3803| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+ *   - arch/x86/kvm/vmx/nested.c|3807| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+ *   - arch/x86/kvm/x86.c|11619| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+ */
 #define KVM_APIC_SIPI		1
 #define KVM_APIC_LVT_NUM	6
 
@@ -40,6 +51,21 @@ struct kvm_timer {
 	u32 timer_advance_ns;
 	s64 advance_expire_delta;
 	atomic_t pending;			/* accumulated triggered timers */
+	/*
+	 * 在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1848| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1867| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 * 在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1504| <<cancel_apic_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1655| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1839| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1846| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1888| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1898| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1929| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1956| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1966| <<kvm_lapic_restart_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 */
 	bool hv_timer_in_use;
 };
 
@@ -49,6 +75,13 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|341| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|342| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2331| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|246| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	bool sw_enabled;
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
@@ -62,8 +95,36 @@ struct kvm_lapic {
 	 * Note: Only one register, the TPR, is used by the microcode.
 	 */
 	void *regs;
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3057| <<kvm_lapic_set_vapic_addr>> int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
+	 *   - arch/x86/kvm/lapic.c|3059| <<kvm_lapic_set_vapic_addr>> if (vapic_addr) {
+	 *   - arch/x86/kvm/lapic.c|3062| <<kvm_lapic_set_vapic_addr>> vapic_addr, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3069| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|6476| <<kvm_arch_vcpu_ioctl>> r = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);
+	 *   - arch/x86/kvm/x86.c|10208| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	gpa_t vapic_addr;
 	struct gfn_to_hva_cache vapic_cache;
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1330| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1341| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3177| <<kvm_apic_accept_events>> pe = smp_load_acquire(&apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3204| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3209| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3217| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.h|354| <<kvm_apic_has_events>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|365| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1098| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3792| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3796| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|3803| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|3807| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6052| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6054| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|11619| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	unsigned long pending_events;
 	unsigned int sipi_vector;
 };
@@ -153,6 +214,43 @@ static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 	apic->irr_pending = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|523| <<kvm_ioapic_update_eoi_one>> kvm_lapic_get_reg(apic, APIC_SPIV) & APIC_SPIV_DIRECTED_EOI)
+ *   - arch/x86/kvm/lapic.c|274| <<kvm_recalculate_apic_map>> ldr = kvm_lapic_get_reg(apic, APIC_LDR);
+ *   - arch/x86/kvm/lapic.c|280| <<kvm_recalculate_apic_map>> if (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)
+ *   - arch/x86/kvm/lapic.c|367| <<apic_lvt_enabled>> return !(kvm_lapic_get_reg(apic, lvt_type) & APIC_LVT_MASKED);
+ *   - arch/x86/kvm/lapic.c|747| <<__apic_update_ppr>> old_ppr = kvm_lapic_get_reg(apic, APIC_PROCPRI);
+ *   - arch/x86/kvm/lapic.c|748| <<__apic_update_ppr>> tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI);
+ *   - arch/x86/kvm/lapic.c|818| <<kvm_apic_match_logical_addr>> logical_id = kvm_lapic_get_reg(apic, APIC_LDR);
+ *   - arch/x86/kvm/lapic.c|826| <<kvm_apic_match_logical_addr>> switch (kvm_lapic_get_reg(apic, APIC_DFR)) {
+ *   - arch/x86/kvm/lapic.c|1342| <<apic_get_tmcct>> if (kvm_lapic_get_reg(apic, APIC_TMICT) == 0 ||
+ *   - arch/x86/kvm/lapic.c|1393| <<__apic_read>> val = kvm_lapic_get_reg(apic, offset);
+ *   - arch/x86/kvm/lapic.c|1399| <<__apic_read>> val = kvm_lapic_get_reg(apic, offset);
+ *   - arch/x86/kvm/lapic.c|1507| <<update_divide_count>> tdcr = kvm_lapic_get_reg(apic, APIC_TDCR);
+ *   - arch/x86/kvm/lapic.c|1547| <<apic_update_lvtt>> u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
+ *   - arch/x86/kvm/lapic.c|1571| <<lapic_timer_int_injected>> u32 reg = kvm_lapic_get_reg(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|1773| <<update_target_expiration>> tmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));
+ *   - arch/x86/kvm/lapic.c|1799| <<set_target_expiration>> tmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));
+ *   - arch/x86/kvm/lapic.c|1812| <<set_target_expiration>> kvm_lapic_get_reg(apic, count_reg));
+ *   - arch/x86/kvm/lapic.c|1822| <<set_target_expiration>> kvm_lapic_get_reg(apic, count_reg),
+ *   - arch/x86/kvm/lapic.c|2126| <<kvm_lapic_reg_write>> if (kvm_lapic_get_reg(apic, APIC_LVR) & APIC_LVR_DIRECTED_EOI)
+ *   - arch/x86/kvm/lapic.c|2134| <<kvm_lapic_reg_write>> lvt_val = kvm_lapic_get_reg(apic,
+ *   - arch/x86/kvm/lapic.c|2148| <<kvm_lapic_reg_write>> kvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));
+ *   - arch/x86/kvm/lapic.c|2347| <<kvm_lapic_set_tpr>> | (kvm_lapic_get_reg(apic, APIC_TASKPRI) & 4));
+ *   - arch/x86/kvm/lapic.c|2354| <<kvm_lapic_get_cr8>> tpr = (u64) kvm_lapic_get_reg(vcpu->arch.apic, APIC_TASKPRI);
+ *   - arch/x86/kvm/lapic.c|2442| <<kvm_lapic_reset>> apic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));
+ *   - arch/x86/kvm/lapic.c|2503| <<kvm_apic_local_deliver>> u32 reg = kvm_lapic_get_reg(apic, lvt_type);
+ *   - arch/x86/kvm/lapic.c|2606| <<kvm_apic_accept_pic_intr>> u32 lvt0 = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LVT0);
+ *   - arch/x86/kvm/lapic.c|2727| <<kvm_apic_set_state>> apic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));
+ *   - arch/x86/kvm/lapic.c|2852| <<kvm_lapic_sync_to_vapic>> tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;
+ *   - arch/x86/kvm/lapic.h|274| <<kvm_xapic_id>> return kvm_lapic_get_reg(apic, APIC_ID) >> 24;
+ *   - arch/x86/kvm/svm/avic.c|399| <<avic_ldr_write>> flat = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR) == APIC_DFR_FLAT;
+ *   - arch/x86/kvm/svm/avic.c|427| <<avic_handle_ldr_update>> u32 ldr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LDR);
+ *   - arch/x86/kvm/svm/avic.c|476| <<avic_handle_dfr_update>> u32 dfr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR);
+ *   - arch/x86/kvm/svm/avic.c|507| <<avic_unaccel_trap_write>> kvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));
+ *   - arch/x86/kvm/vmx/nested.c|3269| <<vmx_has_apicv_interrupt>> u8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);
+ */
 static inline u32 kvm_lapic_get_reg(struct kvm_lapic *apic, int reg_off)
 {
 	return *((u32 *) (apic->regs + reg_off));
@@ -177,19 +275,68 @@ static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * 在以下使用apic_hw_disabled:
+ *   - arch/x86/kvm/lapic.c|2312| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2387| <<kvm_lapic_set_base>> static_branch_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2391| <<kvm_lapic_set_base>> static_branch_inc(&apic_hw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3046| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.h|236| <<kvm_apic_hw_enabled>> if (static_branch_unlikely(&apic_hw_disabled.key))
+ */
 extern struct static_key_false_deferred apic_hw_disabled;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|116| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|1573| <<apic_mmio_read>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|1657| <<lapic_timer_int_injected>> if (kvm_apic_hw_enabled(apic)) {
+ *   - arch/x86/kvm/lapic.c|2326| <<apic_mmio_write>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2602| <<kvm_apic_local_deliver>> if (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {
+ *   - arch/x86/kvm/lapic.c|2704| <<kvm_apic_accept_pic_intr>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ *   - arch/x86/kvm/lapic.h|259| <<kvm_apic_present>> return lapic_in_kernel(vcpu) && kvm_apic_hw_enabled(vcpu->arch.apic);
+ *   - arch/x86/kvm/x86.c|10743| <<vcpu_load_eoi_exitmap>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ */
 static inline int kvm_apic_hw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 猜测如果没有VM disable了APIC, 直接返回MSR_IA32_APICBASE_ENABLE就行
+	 */
 	if (static_branch_unlikely(&apic_hw_disabled.key))
 		return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
 	return MSR_IA32_APICBASE_ENABLE;
 }
 
+/*
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|327| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|329| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2315| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|2588| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3047| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.h|245| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+ *
+ * 这是extern的
+ */
 extern struct static_key_false_deferred apic_sw_disabled;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|76| <<kvm_irq_delivery_to_apic>> } else if (kvm_apic_sw_enabled(vcpu->arch.apic)) {
+ *   - arch/x86/kvm/lapic.c|116| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|319| <<kvm_recalculate_apic_map>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2253| <<kvm_lapic_reg_write>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2264| <<kvm_lapic_reg_write>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.h|264| <<kvm_lapic_enabled>> return kvm_apic_present(vcpu) && kvm_apic_sw_enabled(vcpu->arch.apic);
+ */
 static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|341| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|342| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2331| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|246| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (static_branch_unlikely(&apic_sw_disabled.key))
 		return apic->sw_enabled;
 	return true;
@@ -205,6 +352,32 @@ static inline int kvm_lapic_enabled(struct kvm_vcpu *vcpu)
 	return kvm_apic_present(vcpu) && kvm_apic_sw_enabled(vcpu->arch.apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|269| <<kvm_recalculate_apic_map>> if ((apic_x2apic_mode(apic) || x2apic_id > 0xff) &&
+ *   - arch/x86/kvm/lapic.c|276| <<kvm_recalculate_apic_map>> if (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])
+ *   - arch/x86/kvm/lapic.c|284| <<kvm_recalculate_apic_map>> if (apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|795| <<kvm_apic_broadcast>> return mda == (apic_x2apic_mode(apic) ?
+ *   - arch/x86/kvm/lapic.c|804| <<kvm_apic_match_physical_addr>> if (apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|828| <<kvm_apic_match_logical_addr>> if (apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|867| <<kvm_apic_mda>> !ipi && dest_id == APIC_BROADCAST && apic_x2apic_mode(target))
+ *   - arch/x86/kvm/lapic.c|933| <<kvm_apic_is_broadcast_dest>> bool x2apic_ipi = src && *src && apic_x2apic_mode(*src);
+ *   - arch/x86/kvm/lapic.c|1331| <<kvm_apic_send_ipi>> if (apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|1454| <<kvm_lapic_reg_read>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|1497| <<apic_mmio_read>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2103| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2119| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2126| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2161| <<kvm_lapic_reg_write>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2218| <<kvm_lapic_reg_write>> if (apic_x2apic_mode(apic) && val != 0)
+ *   - arch/x86/kvm/lapic.c|2223| <<kvm_lapic_reg_write>> if (apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2250| <<apic_mmio_write>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2455| <<kvm_lapic_reset>> if (!apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2675| <<kvm_apic_state_fixup>> if (apic_x2apic_mode(vcpu->arch.apic)) {
+ *   - arch/x86/kvm/lapic.c|2894| <<kvm_x2apic_msr_write>> if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/lapic.c|2911| <<kvm_x2apic_msr_read>> if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))
+ *   - arch/x86/kvm/x86.c|2057| <<handle_fastpath_set_x2apic_icr_irqoff>> if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))
+ */
 static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 {
 	return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 8d5876dfc6b7..30bb0878c345 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -1253,6 +1253,11 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1280| <<rmap_write_protect>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|103| <<kvm_slot_page_track_add_page>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn)
 {
@@ -4850,6 +4855,20 @@ kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu)
 	return role.base;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|208| <<kvm_vcpu_after_set_cpuid>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|1230| <<init_vmcb>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4362| <<nested_vmx_restore_host_state>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2942| <<enter_rmode>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|863| <<kvm_post_set_cr0>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1046| <<kvm_post_set_cr4>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|1556| <<set_efer>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|7118| <<emulator_set_hflags>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|7451| <<kvm_smm_changed>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|8970| <<enter_smm>> kvm_mmu_reset_context(vcpu);
+ *   - arch/x86/kvm/x86.c|10057| <<__set_sregs>> kvm_mmu_reset_context(vcpu);
+ */
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_unload(vcpu);
@@ -4882,6 +4901,14 @@ int kvm_mmu_load(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4855| <<kvm_mmu_reset_context>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|5943| <<kvm_mmu_destroy>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|3090| <<kvm_vcpu_flush_tlb_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|9156| <<vcpu_enter_guest>> kvm_mmu_unload(vcpu);
+ *   - arch/x86/kvm/x86.c|10781| <<kvm_unload_vcpu_mmu>> kvm_mmu_unload(vcpu);
+ */
 void kvm_mmu_unload(struct kvm_vcpu *vcpu)
 {
 	kvm_mmu_free_roots(vcpu, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);
diff --git a/arch/x86/kvm/mmu/page_track.c b/arch/x86/kvm/mmu/page_track.c
index 34bb0ec69bd8..4226ce244ddb 100644
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@ -28,6 +28,10 @@ void kvm_page_track_free_memslot(struct kvm_memory_slot *slot)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12690| <<kvm_alloc_memslot_metadata>> if (kvm_page_track_create_memslot(slot, npages))
+ */
 int kvm_page_track_create_memslot(struct kvm_memory_slot *slot,
 				  unsigned long npages)
 {
@@ -83,6 +87,11 @@ static void update_gfn_track(struct kvm_memory_slot *slot, gfn_t gfn,
  * @gfn: the guest page.
  * @mode: tracking mode, currently only write track is supported.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|762| <<account_shadowed>> return kvm_slot_page_track_add_page(kvm, slot, gfn,
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1814| <<kvmgt_page_track_add>> kvm_slot_page_track_add_page(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE);
+ */
 void kvm_slot_page_track_add_page(struct kvm *kvm,
 				  struct kvm_memory_slot *slot, gfn_t gfn,
 				  enum kvm_page_track_mode mode)
@@ -163,6 +172,10 @@ void kvm_page_track_cleanup(struct kvm *kvm)
 	cleanup_srcu_struct(&head->track_srcu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12461| <<kvm_arch_init_vm>> kvm_page_track_init(kvm);
+ */
 void kvm_page_track_init(struct kvm *kvm)
 {
 	struct kvm_page_track_notifier_head *head;
@@ -176,6 +189,11 @@ void kvm_page_track_init(struct kvm *kvm)
  * register the notifier so that event interception for the tracked guest
  * pages can be received.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5520| <<kvm_mmu_init_vm>> kvm_page_track_register_notifier(kvm, node);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1948| <<kvmgt_guest_init>> kvm_page_track_register_notifier(kvm, &info->track_node);
+ */
 void
 kvm_page_track_register_notifier(struct kvm *kvm,
 				 struct kvm_page_track_notifier_node *n)
@@ -216,6 +234,11 @@ EXPORT_SYMBOL_GPL(kvm_page_track_unregister_notifier);
  * The node should figure out if the written page is the one that node is
  * interested in by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7929| <<emulator_write_phys>> kvm_page_track_write(vcpu, gpa, val, bytes);
+ *   - arch/x86/kvm/x86.c|8203| <<emulator_cmpxchg_emulated>> kvm_page_track_write(vcpu, gpa, new, bytes);
+ */
 void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			  int bytes)
 {
@@ -243,6 +266,10 @@ void kvm_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
  * The node should figure out it has any write-protected pages in this slot
  * by itself.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12867| <<kvm_arch_flush_shadow_memslot>> kvm_page_track_flush_slot(kvm, slot);
+ */
 void kvm_page_track_flush_slot(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
 	struct kvm_page_track_notifier_head *head;
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index 5e8d8443154e..2dc8b636c99a 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -114,6 +114,16 @@ static void nested_svm_uninit_mmu_context(struct kvm_vcpu *vcpu)
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|533| <<nested_vmcb02_prepare_control>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|302| <<set_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|317| <<clr_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|327| <<set_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|337| <<clr_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|346| <<svm_set_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|355| <<svm_clr_intercept>> recalc_intercepts(svm);
+ */
 void recalc_intercepts(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *c, *h, *g;
@@ -478,6 +488,11 @@ static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|575| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm);
+ *   - arch/x86/kvm/svm/nested.c|1343| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm);
+ */
 static void nested_vmcb02_prepare_control(struct vcpu_svm *svm)
 {
 	const u32 mask = V_INTR_MASKING_MASK | V_GIF_ENABLE_MASK | V_GIF_MASK;
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index 5f81ef092bd4..f628f22d0eb6 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -13,6 +13,13 @@
  * We maintain a per-CPU linked-list of vCPU, so in wakeup_handler() we
  * can find which vCPU should be waken up.
  */
+/*
+ * 在以下使用percpu的blocked_vcpu_on_cpu:
+ *   - arch/x86/kvm/vmx/posted_intr.c|16| <<global>> static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+ *   - arch/x86/kvm/vmx/posted_intr.c|215| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/posted_intr.c|227| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+ */
 static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
 static DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);
 
@@ -151,6 +158,13 @@ int pi_pre_block(struct kvm_vcpu *vcpu)
 	if (!WARN_ON_ONCE(vcpu->pre_pcpu != -1)) {
 		vcpu->pre_pcpu = vcpu->cpu;
 		spin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));
+		/*
+		 * 在以下使用percpu的blocked_vcpu_on_cpu:
+		 *   - arch/x86/kvm/vmx/posted_intr.c|16| <<global>> static DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);
+		 *   - arch/x86/kvm/vmx/posted_intr.c|155| <<pi_pre_block>> list_add_tail(&vcpu->blocked_vcpu_list, &per_cpu(blocked_vcpu_on_cpu, vcpu->pre_pcpu));
+		 *   - arch/x86/kvm/vmx/posted_intr.c|215| <<pi_wakeup_handler>> list_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/posted_intr.c|227| <<pi_init_cpu>> INIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));
+		 */
 		list_add_tail(&vcpu->blocked_vcpu_list,
 			      &per_cpu(blocked_vcpu_on_cpu,
 				       vcpu->pre_pcpu));
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index 1472c6c376f7..c10002683851 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -64,7 +64,19 @@ struct loaded_vmcs {
 	bool hv_timer_soft_disabled;
 	/* Support for vnmi-less CPUs */
 	int soft_vnmi_blocked;
+	/*
+	 * 在以下使用loaded_vmcs->entry_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|6521| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->entry_time));
+	 *   - arch/x86/kvm/vmx/vmx.c|6689| <<vmx_vcpu_run>> vmx->loaded_vmcs->entry_time = ktime_get();
+	 */
 	ktime_t entry_time;
+	/*
+	 * 在以下使用loaded_vmcs->vnmi_blocked_time:
+	 *   - arch/x86/kvm/vmx/vmx.c|4632| <<vmx_inject_nmi>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|4670| <<vmx_set_nmi_mask>> vmx->loaded_vmcs->vnmi_blocked_time = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6065| <<__vmx_handle_exit>> } else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&
+	 *   - arch/x86/kvm/vmx/vmx.c|6519| <<vmx_recover_nmi_blocking>> vmx->loaded_vmcs->vnmi_blocked_time +=
+	 */
 	s64 vnmi_blocked_time;
 	unsigned long *msr_bitmap;
 	struct list_head loaded_vmcss_on_cpu_link;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index c2a779b688e6..f894dd937a94 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -76,31 +76,95 @@ static const struct x86_cpu_id vmx_cpu_id[] = {
 MODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);
 #endif
 
+/*
+ * 在以下使用enable_vpid:
+ *   - arch/x86/kvm/vmx/vmx.c|80| <<global>> module_param_named(vpid, enable_vpid, bool, 0444);
+ *   - arch/x86/kvm/vmx/nested.c|1177| <<nested_vmx_transition_tlb_flush>> if (!enable_vpid)
+ *   - arch/x86/kvm/vmx/nested.c|2198| <<prepare_vmcs02_early_rare>> if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/nested.c|6526| <<nested_vmx_setup_ctls_msrs>> if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/vmx.c|3146| <<vmx_flush_tlb_all>> } else if (enable_vpid) {
+ *   - arch/x86/kvm/vmx/vmx.c|3879| <<allocate_vpid>> if (!enable_vpid)
+ *   - arch/x86/kvm/vmx/vmx.c|3893| <<free_vpid>> if (!enable_vpid || vpid == 0)
+ *   - arch/x86/kvm/vmx/vmx.c|7955| <<hardware_setup>> enable_vpid = 0;
+ *
+ * The original architecture for VMX operation required VMX transitions to flush the TLBs and paging-structure caches.
+ * This ensured that translations cached for the old linear-address space would not be used after the transition.
+ * Virtual-processor identifiers (VPIDs) introduce to VMX operation a facility by which a logical processor may cache
+ * information for multiple linear-address spaces. When VPIDs are used, VMX transitions may retain cached informa-
+ * tion and the logical processor switches to a different linear-address space.
+ */
 bool __read_mostly enable_vpid = 1;
 module_param_named(vpid, enable_vpid, bool, 0444);
 
 static bool __read_mostly enable_vnmi = 1;
 module_param_named(vnmi, enable_vnmi, bool, S_IRUGO);
 
+/*
+ * 在以下使用flexpriority_enabled:
+ *   - arch/x86/kvm/vmx/vmx.c|86| <<global>> module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6537| <<nested_vmx_setup_ctls_msrs>> if (flexpriority_enabled)
+ *   - arch/x86/kvm/vmx/vmx.c|731| <<cpu_need_virtualize_apic_accesses>> return flexpriority_enabled && lapic_in_kernel(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|736| <<report_flexpriority>> return flexpriority_enabled;
+ *   - arch/x86/kvm/vmx/vmx.c|6346| <<vmx_set_virtual_apic_mode>> if (!flexpriority_enabled &&
+ *   - arch/x86/kvm/vmx/vmx.c|6367| <<vmx_set_virtual_apic_mode>> if (flexpriority_enabled) {
+ *   - arch/x86/kvm/vmx/vmx.c|7936| <<hardware_setup>> flexpriority_enabled = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7946| <<hardware_setup>> if (!flexpriority_enabled)
+ *
+ * 参考cpu_has_vmx_flexpriority()
+ */
 bool __read_mostly flexpriority_enabled = 1;
 module_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);
 
 bool __read_mostly enable_ept = 1;
 module_param_named(ept, enable_ept, bool, S_IRUGO);
 
+/*
+ * This control determines whether guest software may run
+ * in unpaged protected mode or in real-address mode.
+ */
 bool __read_mostly enable_unrestricted_guest = 1;
 module_param_named(unrestricted_guest,
 			enable_unrestricted_guest, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_ept_ad_bits:
+ *   - arch/x86/kvm/vmx/vmx.c|96| <<global>> module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6501| <<nested_vmx_setup_ctls_msrs>> if (enable_ept_ad_bits) {
+ *   - arch/x86/kvm/vmx/vmx.c|3269| <<construct_eptp>> if (enable_ept_ad_bits &&
+ *   - arch/x86/kvm/vmx/vmx.c|7930| <<hardware_setup>> enable_ept_ad_bits = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7985| <<hardware_setup>> kvm_mmu_set_ept_masks(enable_ept_ad_bits,
+ *   - arch/x86/kvm/vmx/vmx.c|8002| <<hardware_setup>> if (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())
+ */
 bool __read_mostly enable_ept_ad_bits = 1;
 module_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);
 
+/*
+ * 在以下使用emulate_invalid_guest_state:
+ *   - arch/x86/kvm/vmx/vmx.c|99| <<global>> module_param(emulate_invalid_guest_state, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|1521| <<emulation_required>> return emulate_invalid_guest_state && !vmx_guest_state_valid(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2914| <<fix_pmode_seg>> if (!emulate_invalid_guest_state) {
+ *   - arch/x86/kvm/vmx/vmx.c|2977| <<fix_rmode_seg>> if (!emulate_invalid_guest_state) {
+ *   - arch/x86/kvm/vmx/vmx.c|6586| <<vmx_has_emulated_msr>> return enable_unrestricted_guest || emulate_invalid_guest_state;
+ */
 static bool __read_mostly emulate_invalid_guest_state = true;
 module_param(emulate_invalid_guest_state, bool, S_IRUGO);
 
+/*
+ * 在以下使用fasteoi:
+ *   - arch/x86/kvm/vmx/vmx.c|102| <<global>> module_param(fasteoi, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|5273| <<handle_apic_access>> if (likely(fasteoi)) {
+ */
 static bool __read_mostly fasteoi = 1;
 module_param(fasteoi, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_apicv:
+ *   - arch/x86/kvm/vmx/vmx.c|105| <<global>> module_param(enable_apicv, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|6378| <<nested_vmx_setup_ctls_msrs>> (enable_apicv ? PIN_BASED_POSTED_INTR : 0);
+ *   - arch/x86/kvm/vmx/vmx.c|3901| <<vmx_msr_bitmap_mode>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7026| <<vmx_vm_init>> kvm_apicv_init(kvm, enable_apicv);
+ *   - arch/x86/kvm/vmx/vmx.c|7879| <<hardware_setup>> enable_apicv = 0;
+ */
 bool __read_mostly enable_apicv = 1;
 module_param(enable_apicv, bool, S_IRUGO);
 
@@ -112,9 +176,25 @@ module_param(enable_apicv, bool, S_IRUGO);
 static bool __read_mostly nested = 1;
 module_param(nested, bool, S_IRUGO);
 
+/*
+ * 在以下使用enable_pml (Page Modification Logging):
+ *   - arch/x86/kvm/vmx/vmx.c|116| <<global>> module_param_named(pml, enable_pml, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|2171| <<prepare_vmcs02_constant_state>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|4478| <<init_vmcs>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|5996| <<__vmx_handle_exit>> if (enable_pml && !is_guest_mode(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|6866| <<vmx_free_vcpu>> if (enable_pml)
+ *   - arch/x86/kvm/vmx/vmx.c|6892| <<vmx_create_vcpu>> if (enable_pml) {
+ *   - arch/x86/kvm/vmx/vmx.c|7912| <<hardware_setup>> enable_pml = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|7914| <<hardware_setup>> if (!enable_pml)
+ */
 bool __read_mostly enable_pml = 1;
 module_param_named(pml, enable_pml, bool, S_IRUGO);
 
+/*
+ * 在以下使用dump_invalid_vmcs:
+ *   - arch/x86/kvm/vmx/vmx.c|119| <<global>> module_param(dump_invalid_vmcs, bool, 0644);
+ *   - arch/x86/kvm/vmx/vmx.c|5821| <<dump_vmcs>> if (!dump_invalid_vmcs) {
+ */
 static bool __read_mostly dump_invalid_vmcs = 0;
 module_param(dump_invalid_vmcs, bool, 0644);
 
@@ -124,12 +204,40 @@ module_param(dump_invalid_vmcs, bool, 0644);
 #define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL
 
 /* Guest_tsc -> host_tsc conversion requires 64-bit division.  */
+/*
+ * 在以下使用cpu_preemption_timer_multi:
+ *   - arch/x86/kvm/vmx/vmx.c|6649| <<vmx_update_hv_timer>> cpu_preemption_timer_multi);
+ *   - arch/x86/kvm/vmx/vmx.c|7490| <<vmx_set_hv_timer>> if (delta_tsc >> (cpu_preemption_timer_multi + 32))
+ *   - arch/x86/kvm/vmx/vmx.c|7925| <<hardware_setup>> cpu_preemption_timer_multi =
+ *   - arch/x86/kvm/vmx/vmx.c|7930| <<hardware_setup>> use_timer_freq >>= cpu_preemption_timer_multi;
+ */
 static int __read_mostly cpu_preemption_timer_multi;
+/*
+ * 在以下使用enable_preemption_timer:
+ *   - arch/x86/kvm/vmx/vmx.c|130| <<global>> module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/vmx.c|4199| <<vmx_pin_based_exec_ctrl>> if (!enable_preemption_timer)
+ *   - arch/x86/kvm/vmx/vmx.c|6764| <<vmx_vcpu_run>> if (enable_preemption_timer)
+ *   - arch/x86/kvm/vmx/vmx.c|7918| <<hardware_setup>> enable_preemption_timer = false;
+ *   - arch/x86/kvm/vmx/vmx.c|7920| <<hardware_setup>> if (enable_preemption_timer) {
+ *   - arch/x86/kvm/vmx/vmx.c|7938| <<hardware_setup>> enable_preemption_timer = false;
+ *   - arch/x86/kvm/vmx/vmx.c|7941| <<hardware_setup>> if (!enable_preemption_timer) {
+ */
 static bool __read_mostly enable_preemption_timer = 1;
 #ifdef CONFIG_X86_64
 module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
 #endif
 
+/*
+ * 在以下使用allow_smaller_maxphyaddr:
+ *   - arch/x86/kvm/vmx/vmx.c|134| <<global>> module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|209| <<global>> bool __read_mostly allow_smaller_maxphyaddr = 0;
+ *   - arch/x86/kvm/svm/svm.c|1057| <<svm_hardware_setup>> allow_smaller_maxphyaddr = !npt_enabled;
+ *   - arch/x86/kvm/vmx/vmx.c|4923| <<handle_exception_nmi>> WARN_ON_ONCE(!allow_smaller_maxphyaddr);
+ *   - arch/x86/kvm/vmx/vmx.c|5420| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))
+ *   - arch/x86/kvm/vmx/vmx.c|8104| <<vmx_init>> allow_smaller_maxphyaddr = true;
+ *   - arch/x86/kvm/vmx/vmx.h|578| <<vmx_need_pf_intercept>> return allow_smaller_maxphyaddr && cpuid_maxphyaddr(vcpu) < boot_cpu_data.x86_phys_bits;
+ *   - arch/x86/kvm/x86.c|3992| <<kvm_vm_ioctl_check_extension>> r = (int ) allow_smaller_maxphyaddr;
+ */
 extern bool __read_mostly allow_smaller_maxphyaddr;
 module_param(allow_smaller_maxphyaddr, bool, S_IRUGO);
 
@@ -190,26 +298,77 @@ static unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;
 module_param(ple_window, uint, 0444);
 
 /* Default doubles per-vcpu window every exit. */
+/*
+ * 在以下使用ple_window_grow:
+ *   - arch/x86/kvm/vmx/vmx.c|194| <<global>> module_param(ple_window_grow, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5508| <<grow_ple_window>> ple_window_grow,
+ *   - arch/x86/kvm/vmx/vmx.c|7873| <<hardware_setup>> ple_window_grow = 0;
+ */
 static unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;
 module_param(ple_window_grow, uint, 0444);
 
 /* Default resets per-vcpu window every exit to ple_window. */
+/*
+ * 在以下使用ple_window_shrink:
+ *   - arch/x86/kvm/vmx/vmx.c|198| <<global>> module_param(ple_window_shrink, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5524| <<shrink_ple_window>> ple_window_shrink,
+ *   - arch/x86/kvm/vmx/vmx.c|7875| <<hardware_setup>> ple_window_shrink = 0;
+ */
 static unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;
 module_param(ple_window_shrink, uint, 0444);
 
 /* Default is to compute the maximum so we can never overflow. */
+/*
+ * 在以下使用ple_window_max:
+ *   - arch/x86/kvm/vmx/vmx.c|202| <<global>> module_param(ple_window_max, uint, 0444);
+ *   - arch/x86/kvm/vmx/vmx.c|5509| <<grow_ple_window>> ple_window_max);
+ *   - arch/x86/kvm/vmx/vmx.c|7874| <<hardware_setup>> ple_window_max = 0;
+ */
 static unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
 module_param(ple_window_max, uint, 0444);
 
 /* Default is SYSTEM mode, 1 for host-guest mode */
+/*
+ * Processor Trace virtualization can be work in one of 3 possible
+ * modes by set new option "pt_mode". Default value is system mode.
+ * a. system-wide: trace both host/guest and output to host buffer;
+ * b. host-only: only trace host and output to host buffer;
+ * c. host-guest: trace host/guest simultaneous and output to their respective buffer.
+ *
+ * KVM currently only supports (a) and (c).
+ *
+ * 在以下使用pt_mode:
+ *   - arch/x86/kvm/vmx/vmx.c|206| <<global>> module_param(pt_mode, int , S_IRUGO);
+ *   - arch/x86/kvm/vmx/capabilities.h|373| <<vmx_pt_mode_is_system>> return pt_mode == PT_MODE_SYSTEM;
+ *   - arch/x86/kvm/vmx/capabilities.h|377| <<vmx_pt_mode_is_host_guest>> return pt_mode == PT_MODE_HOST_GUEST;
+ *   - arch/x86/kvm/vmx/vmx.c|7951| <<hardware_setup>> if (pt_mode != PT_MODE_SYSTEM && pt_mode != PT_MODE_HOST_GUEST)
+ *   - arch/x86/kvm/vmx/vmx.c|7954| <<hardware_setup>> pt_mode = PT_MODE_SYSTEM;
+ */
 int __read_mostly pt_mode = PT_MODE_SYSTEM;
 module_param(pt_mode, int, S_IRUGO);
 
+/*
+ * 在以下使用vmx_l1d_should_flush:
+ *   - arch/x86/kvm/vmx/vmx.c|300| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|302| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_should_flush);
+ *   - arch/x86/kvm/vmx/vmx.c|6665| <<vmx_vcpu_enter_exit>> if (static_branch_unlikely(&vmx_l1d_should_flush))
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+/*
+ * 在以下使用vmx_l1d_flush_cond:
+ *   - arch/x86/kvm/vmx/vmx.c|453| <<vmx_setup_l1d_flush>> static_branch_enable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|455| <<vmx_setup_l1d_flush>> static_branch_disable(&vmx_l1d_flush_cond);
+ *   - arch/x86/kvm/vmx/vmx.c|6321| <<vmx_l1d_flush>> if (static_branch_likely(&vmx_l1d_flush_cond)) {
+ */
 static DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);
 static DEFINE_MUTEX(vmx_l1d_flush_mutex);
 
 /* Storage for pre module init parameter parsing */
+/*
+ * 在以下使用vmentry_l1d_flush_param:
+ *   - arch/x86/kvm/vmx/vmx.c|507| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+ *   - arch/x86/kvm/vmx/vmx.c|8242| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ */
 static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
 
 static const struct {
@@ -225,6 +384,16 @@ static const struct {
 };
 
 #define L1D_CACHE_ORDER 4
+/*
+ * 在以下使用vmx_l1d_flush_pages:
+ *   - arch/x86/kvm/vmx/vmx.c|423| <<vmx_setup_l1d_flush>> if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ *   - arch/x86/kvm/vmx/vmx.c|432| <<vmx_setup_l1d_flush>> vmx_l1d_flush_pages = page_address(page);
+ *   - arch/x86/kvm/vmx/vmx.c|440| <<vmx_setup_l1d_flush>> memset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1,
+ *   - arch/x86/kvm/vmx/vmx.c|6368| <<vmx_l1d_flush>> :: [flush_pages] "r" (vmx_l1d_flush_pages),
+ *   - arch/x86/kvm/vmx/vmx.c|8128| <<vmx_cleanup_l1d_flush>> if (vmx_l1d_flush_pages) {
+ *   - arch/x86/kvm/vmx/vmx.c|8129| <<vmx_cleanup_l1d_flush>> free_pages((unsigned long )vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ *   - arch/x86/kvm/vmx/vmx.c|8130| <<vmx_cleanup_l1d_flush>> vmx_l1d_flush_pages = NULL;
+ */
 static void *vmx_l1d_flush_pages;
 
 static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
@@ -411,6 +580,14 @@ noinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)
 }
 
 static DEFINE_PER_CPU(struct vmcs *, vmxarea);
+/*
+ * 在以下使用loaded_vmcss_on_cpu:
+ *   - arch/x86/kvm/vmx/vmx.c|583| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|883| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|1516| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+ *   - arch/x86/kvm/vmx/vmx.c|2607| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+ *   - arch/x86/kvm/vmx/vmx.c|8249| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+ */
 DEFINE_PER_CPU(struct vmcs *, current_vmcs);
 /*
  * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed
@@ -682,6 +859,16 @@ static bool is_valid_passthrough_msr(u32 msr)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|4307| <<nested_vmx_get_vmcs01_guest_efer>> efer_msr = vmx_find_uret_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|1923| <<vmx_setup_uret_msr>> uret_msr = vmx_find_uret_msr(vmx, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2175| <<vmx_get_msr>> msr = vmx_find_uret_msr(vmx, msr_info->index);
+ *   - arch/x86/kvm/vmx/vmx.c|2503| <<vmx_set_msr>> msr = vmx_find_uret_msr(vmx, msr_index);
+ *   - arch/x86/kvm/vmx/vmx.c|3133| <<vmx_set_efer>> struct vmx_uret_msr *msr = vmx_find_uret_msr(vmx, MSR_EFER);
+ *   - arch/x86/kvm/vmx/vmx.c|7098| <<vmx_create_vcpu>> tsx_ctrl = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7474| <<vmx_vcpu_after_set_cpuid>> msr = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);
+ */
 struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)
 {
 	int i;
@@ -1680,6 +1867,14 @@ static int vmx_skip_emulated_instruction(struct kvm_vcpu *vcpu)
 	return skip_emulated_instruction(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|1730| <<vmx_queue_exception>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4571| <<vmx_vcpu_reset>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4615| <<vmx_inject_irq>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4646| <<vmx_inject_nmi>> vmx_clear_hlt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_enter_smm>> vmx_clear_hlt(vcpu);
+ */
 static void vmx_clear_hlt(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -3050,6 +3245,11 @@ static void vmx_flush_tlb_guest(struct kvm_vcpu *vcpu)
 	 * disabled (VM-Enter with vpid enabled and vpid==0 is disallowed),
 	 * i.e. no explicit INVVPID is necessary.
 	 */
+	/*
+	 * struct vcpu_vmx:
+	 * -> struct kvm_vcpu vcpu;
+	 * -> int vpid;
+	 */
 	vpid_sync_context(to_vmx(vcpu)->vpid);
 }
 
@@ -4643,6 +4843,14 @@ static void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/vmx/vmx.c|1730| <<vmx_queue_exception>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4571| <<vmx_vcpu_reset>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4615| <<vmx_inject_irq>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|4646| <<vmx_inject_nmi>> vmx_clear_hlt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7555| <<vmx_pre_enter_smm>> vmx_clear_hlt(vcpu);
+	 */
 	vmx_clear_hlt(vcpu);
 }
 
@@ -5432,6 +5640,10 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * calld by:
+ *   - arch/x86/kvm/vmx/vmx.c|6180| <<__vmx_handle_exit>> return handle_invalid_guest_state(vcpu);
+ */
 static int handle_invalid_guest_state(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6146,6 +6358,10 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
  * information but as all relevant affected CPUs have 32KiB L1D cache size
  * there is no point in doing so.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6830| <<vmx_vcpu_enter_exit>> vmx_l1d_flush(vcpu);
+ */
 static noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)
 {
 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
@@ -6848,6 +7064,12 @@ static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 	free_loaded_vmcs(vmx->loaded_vmcs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11852| <<kvm_arch_vcpu_create>> r = static_call(kvm_x86_vcpu_create)(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.vcpu_create = vmx_create_vcpu()
+ */
 static int vmx_create_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vmx_uret_msr *tsx_ctrl;
@@ -7745,6 +7967,10 @@ static struct kvm_x86_ops vmx_x86_ops __initdata = {
 	.vcpu_deliver_sipi_vector = kvm_vcpu_deliver_sipi_vector,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|8001| <<hardware_setup>> vmx_setup_user_return_msrs();
+ */
 static __init void vmx_setup_user_return_msrs(void)
 {
 
@@ -7771,6 +7997,12 @@ static __init void vmx_setup_user_return_msrs(void)
 		kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10652| <<kvm_arch_hardware_setup>> r = ops->hardware_setup();
+ *
+ * struct kvm_x86_init_ops.hardware_setup = hardware_setup()
+ */
 static __init int hardware_setup(void)
 {
 	unsigned long host_bndcfgs;
@@ -7999,6 +8231,10 @@ static void vmx_exit(void)
 }
 module_exit(vmx_exit);
 
+/*
+ * 在以下使用vmx_init():
+ *   - arch/x86/kvm/vmx/vmx.c|8270| <<global>> module_init(vmx_init);
+ */
 static int __init vmx_init(void)
 {
 	int r, cpu;
@@ -8049,6 +8285,11 @@ static int __init vmx_init(void)
 	 * contain 'auto' which will be turned into the default 'cond'
 	 * mitigation mode.
 	 */
+	/*
+	 * 在以下使用vmentry_l1d_flush_param:
+	 *   - arch/x86/kvm/vmx/vmx.c|507| <<vmentry_l1d_flush_set>> vmentry_l1d_flush_param = l1tf;
+	 *   - arch/x86/kvm/vmx/vmx.c|8242| <<vmx_init>> r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+	 */
 	r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
 	if (r) {
 		vmx_exit();
@@ -8056,12 +8297,26 @@ static int __init vmx_init(void)
 	}
 
 	for_each_possible_cpu(cpu) {
+		/*
+		 * 在以下使用loaded_vmcss_on_cpu:
+		 *   - arch/x86/kvm/vmx/vmx.c|583| <<global>> static DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|883| <<crash_vmclear_local_loaded_vmcss>> list_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/vmx.c|1516| <<vmx_vcpu_load_vmcs>> &per_cpu(loaded_vmcss_on_cpu, cpu));
+		 *   - arch/x86/kvm/vmx/vmx.c|2607| <<vmclear_local_loaded_vmcss>> list_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),
+		 *   - arch/x86/kvm/vmx/vmx.c|8249| <<vmx_init>> INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
+		 */
 		INIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));
 
 		pi_init_cpu(cpu);
 	}
 
 #ifdef CONFIG_KEXEC_CORE
+	/*
+	 * 在以下使用crash_vmclear_loaded_vmcss:
+	 *   - arch/x86/kernel/crash.c|67| <<cpu_crash_vmclear_loaded_vmcss>> do_vmclear_operation = rcu_dereference(crash_vmclear_loaded_vmcss);
+	 *   - arch/x86/kvm/vmx/vmx.c|8159| <<vmx_exit>> RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_init>> rcu_assign_pointer(crash_vmclear_loaded_vmcss, crash_vmclear_local_loaded_vmcss);
+	 */
 	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
 			   crash_vmclear_local_loaded_vmcss);
 #endif
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 16e4e457ba23..0eab2b3d7507 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -242,6 +242,27 @@ struct vcpu_vmx {
 
 	unsigned long         exit_qualification;
 	u32                   exit_intr_info;
+	/*
+	 * 在以下使用vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|4871| <<handle_exception_nmi>> vect_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<handle_task_switch>> idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5323| <<handle_task_switch>> idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5324| <<handle_task_switch>> type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5340| <<handle_task_switch>> if (vmx->idt_vectoring_info &
+	 *   - arch/x86/kvm/vmx/vmx.c|5384| <<handle_ept_violation>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5604| <<handle_pml_full>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5735| <<vmx_get_exit_info>> *info2 = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5985| <<__vmx_handle_exit>> u32 vectoring_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6513| <<vmx_recover_nmi_blocking>> idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6547| <<__vmx_complete_interrupts>> u32 idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|6555| <<__vmx_complete_interrupts>> idtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6566| <<__vmx_complete_interrupts>> vector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6567| <<__vmx_complete_interrupts>> type = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|6583| <<__vmx_complete_interrupts>> if (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6602| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|6832| <<vmx_vcpu_run>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|6844| <<vmx_vcpu_run>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -254,6 +275,19 @@ struct vcpu_vmx {
 	 * into hardware when running the guest.  guest_uret_msrs[] is resorted
 	 * whenever the number of "active" uret MSRs is modified.
 	 */
+	/*
+	 * 在以下使用vcpu_vmx->guest_uret_msrs:
+	 *   - arch/x86/kvm/vmx/vmx.c|878| <<vmx_find_uret_msr>> return &vmx->guest_uret_msrs[i];
+	 *   - arch/x86/kvm/vmx/vmx.c|885| <<vmx_set_guest_uret_msr>> unsigned int slot = msr - vmx->guest_uret_msrs;
+	 *   - arch/x86/kvm/vmx/vmx.c|1253| <<update_transition_efer>> vmx->guest_uret_msrs[i].data = guest_efer;
+	 *   - arch/x86/kvm/vmx/vmx.c|1254| <<update_transition_efer>> vmx->guest_uret_msrs[i].mask = ~ignore_bits;
+	 *   - arch/x86/kvm/vmx/vmx.c|1407| <<vmx_prepare_switch_to_guest>> if (!vmx->guest_uret_msrs[i].load_into_hardware)
+	 *   - arch/x86/kvm/vmx/vmx.c|1411| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].data,
+	 *   - arch/x86/kvm/vmx/vmx.c|1412| <<vmx_prepare_switch_to_guest>> vmx->guest_uret_msrs[i].mask);
+	 *   - arch/x86/kvm/vmx/vmx.c|7099| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].data = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7100| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].mask = -1ull;
+	 *   - arch/x86/kvm/vmx/vmx.c|7110| <<vmx_create_vcpu>> vmx->guest_uret_msrs[i].mask = ~(u64)TSX_CTRL_CPUID_CLEAR;
+	 */
 	struct vmx_uret_msr   guest_uret_msrs[MAX_NR_USER_RETURN_MSRS];
 	int                   nr_active_uret_msrs;
 	bool                  guest_uret_msrs_loaded;
@@ -311,6 +345,14 @@ struct vcpu_vmx {
 
 	/* Dynamic PLE window. */
 	unsigned int ple_window;
+	/*
+	 * 在以下使用vcpu_vmx->ple_window_dirty:
+	 *   - arch/x86/kvm/vmx/vmx.c|4423| <<init_vmcs>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|5490| <<grow_ple_window>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|5506| <<shrink_ple_window>> vmx->ple_window_dirty = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6698| <<vmx_vcpu_run>> if (vmx->ple_window_dirty) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6699| <<vmx_vcpu_run>> vmx->ple_window_dirty = false;
+	 */
 	bool ple_window_dirty;
 
 	bool req_immediate_exit;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e0f4a46649d7..c4ff03dd13e8 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -132,26 +132,99 @@ bool __read_mostly report_ignored_msrs = true;
 module_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);
 EXPORT_SYMBOL_GPL(report_ignored_msrs);
 
+/*
+ * 在以下使用min_timer_period_us:
+ *   - arch/x86/kvm/x86.c|136| <<global>> module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.h|348| <<global>> extern unsigned int min_timer_period_us;
+ *   - arch/x86/kvm/i8254.c|351| <<create_pit_timer>> s64 min_period = min_timer_period_us * 1000LL;
+ *   - arch/x86/kvm/lapic.c|1490| <<limit_periodic_timer_frequency>> s64 min_period = min_timer_period_us * 1000LL;
+ */
 unsigned int min_timer_period_us = 200;
 module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用kvmclock_periodic_sync:
+ *   - arch/x86/kvm/x86.c|139| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|3206| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+ *   - arch/x86/kvm/x86.c|10792| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+ */
 static bool __read_mostly kvmclock_periodic_sync = true;
 module_param(kvmclock_periodic_sync, bool, S_IRUGO);
 
+/*
+ * 在以下设置kvm_has_tsc_control:
+ *   - arch/x86/kvm/svm/svm.c|962| <<svm_hardware_setup>> kvm_has_tsc_control = true;
+ *   - arch/x86/kvm/vmx/vmx.c|8063| <<hardware_setup>> kvm_has_tsc_control = true;
+ * 在以下使用kvm_has_tsc_control:
+ *   - arch/x86/kvm/debugfs.c|56| <<kvm_arch_create_vcpu_debugfs>> if (kvm_has_tsc_control) {
+ *   - arch/x86/kvm/vmx/nested.c|2537| <<prepare_vmcs02>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/vmx/nested.c|4507| <<nested_vmx_vmexit>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/vmx/vmx.c|1572| <<vmx_vcpu_load_vmcs>> if (kvm_has_tsc_control &&
+ *   - arch/x86/kvm/x86.c|2244| <<set_tsc_khz>> if (!kvm_has_tsc_control) {
+ *   - arch/x86/kvm/x86.c|2975| <<kvm_guest_time_update>> if (kvm_has_tsc_control)
+ *   - arch/x86/kvm/x86.c|4099| <<kvm_vm_ioctl_check_extension>> r = kvm_has_tsc_control;
+ *   - arch/x86/kvm/x86.c|5218| <<kvm_arch_vcpu_ioctl>> if (kvm_has_tsc_control &&
+ *   - arch/x86/kvm/x86.c|10813| <<kvm_arch_hardware_setup>> if (kvm_has_tsc_control) {
+ *
+ * 旧的机器是false
+ * crash> kvm_has_tsc_control
+ * kvm_has_tsc_control = $1 = false
+ */
 bool __read_mostly kvm_has_tsc_control;
 EXPORT_SYMBOL_GPL(kvm_has_tsc_control);
+/*
+ * 在以下使用kvm_max_guest_tsc_khz:
+ *   - arch/x86/kvm/x86.c|6236| <<kvm_arch_vcpu_ioctl>> user_tsc_khz >= kvm_max_guest_tsc_khz)
+ *   - arch/x86/kvm/x86.c|9085| <<kvm_hyperv_tsc_notifier>> kvm_max_guest_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|11982| <<kvm_arch_hardware_setup>> kvm_max_guest_tsc_khz = max;
+ *
+ * 旧的机器是0
+ */
 u32  __read_mostly kvm_max_guest_tsc_khz;
 EXPORT_SYMBOL_GPL(kvm_max_guest_tsc_khz);
+/*
+ * 在以下设置kvm_tsc_scaling_ratio_frac_bits:
+ *   - arch/x86/kvm/svm/svm.c|964| <<svm_hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 32;
+ *   - arch/x86/kvm/vmx/vmx.c|8065| <<hardware_setup>> kvm_tsc_scaling_ratio_frac_bits = 48;
+ * 在以下使用kvm_tsc_scaling_ratio_frac_bits:
+ *   - arch/x86/kvm/x86.c|172| <<global>> EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
+ *   - arch/x86/kvm/debugfs.c|40| <<vcpu_get_tsc_scaling_frac_bits>> *val = kvm_tsc_scaling_ratio_frac_bits;
+ *   - arch/x86/kvm/vmx/vmx.c|7659| <<vmx_set_hv_timer>> kvm_tsc_scaling_ratio_frac_bits,
+ *   - arch/x86/kvm/x86.c|2501| <<set_tsc_khz>> ratio = mul_u64_u32_div(1ULL << kvm_tsc_scaling_ratio_frac_bits,
+ *   - arch/x86/kvm/x86.c|2719| <<__scale_tsc>> return mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);
+ *   - arch/x86/kvm/x86.c|11984| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+ *
+ * 4.14简单测试
+ * crash> kvm_tsc_scaling_ratio_frac_bits
+ * kvm_tsc_scaling_ratio_frac_bits = $6 = 0 '\000'
+ */
 u8   __read_mostly kvm_tsc_scaling_ratio_frac_bits;
 EXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);
 u64  __read_mostly kvm_max_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_max_tsc_scaling_ratio);
+/*
+ * 在以下设置kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/x86.c|11984| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+ * 在以下使用kvm_default_tsc_scaling_ratio:
+ *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+ *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+ *   - arch/x86/kvm/x86.c|2474| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/x86.c|2545| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+ *   - arch/x86/kvm/x86.c|2743| <<kvm_scale_tsc>> if (ratio != kvm_default_tsc_scaling_ratio)
+ *   - arch/x86/kvm/x86.c|3216| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+ */
 u64 __read_mostly kvm_default_tsc_scaling_ratio;
 EXPORT_SYMBOL_GPL(kvm_default_tsc_scaling_ratio);
 bool __read_mostly kvm_has_bus_lock_exit;
 EXPORT_SYMBOL_GPL(kvm_has_bus_lock_exit);
 
 /* tsc tolerance in parts per million - default to 1/2 of the NTP threshold */
+/*
+ * 在以下使用tsc_tolerance_ppm:
+ *   - arch/x86/kvm/x86.c|182| <<global>> module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.c|2578| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2579| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 __read_mostly tsc_tolerance_ppm = 250;
 module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
 
@@ -161,6 +234,11 @@ module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
  * advancement entirely.  Any other value is used as-is and disables adaptive
  * tuning, i.e. allows privileged userspace to set an exact advancement time.
  */
+/*
+ * 在以下使用lapic_timer_advance_ns:
+ *   - arch/x86/kvm/x86.c|191| <<global>> module_param(lapic_timer_advance_ns, int , S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.c|11627| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 static int __read_mostly lapic_timer_advance_ns = -1;
 module_param(lapic_timer_advance_ns, int, S_IRUGO | S_IWUSR);
 
@@ -174,6 +252,14 @@ EXPORT_SYMBOL_GPL(enable_vmware_backdoor);
 static bool __read_mostly force_emulation_prefix = false;
 module_param(force_emulation_prefix, bool, S_IRUGO);
 
+/*
+ * 在以下使用pi_inject_timer:
+ *   - arch/x86/kvm/x86.c|256| <<global>> module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.h|352| <<global>> extern int pi_inject_timer
+ *   - arch/x86/kvm/lapic.c|116| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
+ *   - arch/x86/kvm/x86.c|9858| <<kvm_arch_init>> if (pi_inject_timer == -1)
+ *   - arch/x86/kvm/x86.c|9859| <<kvm_arch_init>> pi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);
+ */
 int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
 
@@ -193,8 +279,58 @@ struct kvm_user_return_msrs {
 	} values[KVM_MAX_NR_USER_RETURN_MSRS];
 };
 
+/*
+ * commit 7e34fbd05c6350b30161be84d4f8954c9d321292
+ * Author: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Date:   Wed Sep 23 11:03:55 2020 -0700
+ *
+ * KVM: x86: Rename "shared_msrs" to "user_return_msrs"
+ *
+ * Rename the "shared_msrs" mechanism, which is used to defer restoring
+ * MSRs that are only consumed when running in userspace, to a more banal
+ * but less likely to be confusing "user_return_msrs".
+ *
+ * The "shared" nomenclature is confusing as it's not obvious who is
+ * sharing what, e.g. reasonable interpretations are that the guest value
+ * is shared by vCPUs in a VM, or that the MSR value is shared/common to
+ * guest and host, both of which are wrong.
+ *
+ * "shared" is also misleading as the MSR value (in hardware) is not
+ * guaranteed to be shared/reused between VMs (if that's indeed the correct
+ * interpretation of the name), as the ability to share values between VMs
+ * is simply a side effect (albiet a very nice side effect) of deferring
+ * restoration of the host value until returning from userspace.
+ *
+ * "user_return" avoids the above confusion by describing the mechanism
+ * itself instead of its effects.
+ *
+ * Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
+ * Message-Id: <20200923180409.32255-2-sean.j.christopherson@intel.com>
+ * Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
+ */
+
+/*
+ * 在以下使用kvm_nr_uret_msrs:
+ *   - arch/x86/kvm/vmx/vmx.c|1396| <<vmx_prepare_switch_to_guest>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/vmx/vmx.c|7088| <<vmx_create_vcpu>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|416| <<kvm_on_user_return>> for (slot = 0; slot < kvm_nr_uret_msrs; ++slot) {
+ *   - arch/x86/kvm/x86.c|442| <<kvm_add_user_return_msr>> BUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);
+ *   - arch/x86/kvm/x86.c|447| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|448| <<kvm_add_user_return_msr>> return kvm_nr_uret_msrs++;
+ *   - arch/x86/kvm/x86.c|456| <<kvm_find_user_return_msr>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|471| <<kvm_user_return_msr_cpu_online>> for (i = 0; i < kvm_nr_uret_msrs; ++i) {
+ *   - arch/x86/kvm/x86.c|9870| <<kvm_arch_init>> kvm_nr_uret_msrs = 0;
+ */
 u32 __read_mostly kvm_nr_uret_msrs;
 EXPORT_SYMBOL_GPL(kvm_nr_uret_msrs);
+/*
+ * 在以下使用kvm_uret_msrs_list:
+ *   - arch/x86/kvm/x86.c|461| <<kvm_on_user_return>> wrmsrl(kvm_uret_msrs_list[slot], values->host);
+ *   - arch/x86/kvm/x86.c|494| <<kvm_add_user_return_msr>> kvm_uret_msrs_list[kvm_nr_uret_msrs] = msr;
+ *   - arch/x86/kvm/x86.c|504| <<kvm_find_user_return_msr>> if (kvm_uret_msrs_list[i] == msr)
+ *   - arch/x86/kvm/x86.c|519| <<kvm_user_return_msr_cpu_online>> rdmsrl_safe(kvm_uret_msrs_list[i], &value);
+ *   - arch/x86/kvm/x86.c|534| <<kvm_set_user_return_msr>> err = wrmsrl_safe(kvm_uret_msrs_list[slot], value);
+ */
 static u32 __read_mostly kvm_uret_msrs_list[KVM_MAX_NR_USER_RETURN_MSRS];
 static struct kvm_user_return_msrs __percpu *user_return_msrs;
 
@@ -351,6 +487,11 @@ static int kvm_probe_user_return_msr(u32 msr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|967| <<svm_hardware_setup>> tsc_aux_uret_slot = kvm_add_user_return_msr(MSR_TSC_AUX);
+ *   - arch/x86/kvm/vmx/vmx.c|7983| <<vmx_setup_user_return_msrs>> kvm_add_user_return_msr(vmx_uret_msrs_list[i]);
+ */
 int kvm_add_user_return_msr(u32 msr)
 {
 	BUG_ON(kvm_nr_uret_msrs >= KVM_MAX_NR_USER_RETURN_MSRS);
@@ -363,6 +504,12 @@ int kvm_add_user_return_msr(u32 msr)
 }
 EXPORT_SYMBOL_GPL(kvm_add_user_return_msr);
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/kvm_host.h|2258| <<kvm_is_supported_user_return_msr>> return kvm_find_user_return_msr(msr) >= 0;
+ *   - arch/x86/kvm/vmx/vmx.c|876| <<vmx_find_uret_msr>> i = kvm_find_user_return_msr(msr);
+ *   - arch/x86/kvm/vmx/vmx.c|1244| <<update_transition_efer>> i = kvm_find_user_return_msr(MSR_EFER);
+ */
 int kvm_find_user_return_msr(u32 msr)
 {
 	int i;
@@ -433,6 +580,12 @@ enum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_get_apic_mode);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|4685| <<vmx_vcpu_reset>> kvm_set_apic_base(vcpu, &apic_base_msr);
+ *   - arch/x86/kvm/x86.c|4453| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_set_apic_base(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|11491| <<__set_sregs>> if (kvm_set_apic_base(vcpu, &apic_base_msr))
+ */
 int kvm_set_apic_base(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	enum lapic_mode old_mode = kvm_get_apic_mode(vcpu);
@@ -1300,6 +1453,11 @@ static const u32 msrs_to_save_all[] = {
 	MSR_ARCH_PERFMON_EVENTSEL0 + 16, MSR_ARCH_PERFMON_EVENTSEL0 + 17,
 };
 
+/*
+ * 在以下使用msrs_to_save:
+ *   - arch/x86/kvm/x86.c|4034| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+ *   - arch/x86/kvm/x86.c|6062| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];
 static unsigned num_msrs_to_save;
 
@@ -1613,6 +1771,10 @@ EXPORT_SYMBOL_GPL(kvm_msr_allowed);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1773| <<kvm_set_msr_ignored_check>> int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
+ */
 static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 			 bool host_initiated)
 {
@@ -1678,9 +1840,26 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	return static_call(kvm_x86_set_msr)(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1841| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2094| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
+	/*
+	 * 在以下设置msr_data->host_initiated:
+	 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+	 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+	 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+	 *
+	 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+	 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+	 * msr_info->host_initiated就为false.
+	 */
 	int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
 
 	if (ret == KVM_MSR_RET_INVALID)
@@ -1747,6 +1926,15 @@ int kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data)
 }
 EXPORT_SYMBOL_GPL(kvm_get_msr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|912| <<nested_vmx_load_msr>> if (kvm_set_msr(vcpu, e.index, e.value)) {
+ *   - arch/x86/kvm/vmx/nested.c|2601| <<prepare_vmcs02>> WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,
+ *   - arch/x86/kvm/vmx/nested.c|4231| <<load_vmcs12_host_state>> WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,
+ *   - arch/x86/kvm/vmx/nested.c|4407| <<nested_vmx_restore_host_state>> if (kvm_set_msr(vcpu, h.index, h.value)) {
+ *   - arch/x86/kvm/x86.c|1940| <<kvm_emulate_wrmsr>> r = kvm_set_msr(vcpu, ecx, data);
+ *   - arch/x86/kvm/x86.c|8390| <<emulator_set_msr>> r = kvm_set_msr(vcpu, msr_index, data);
+ */
 int kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data)
 {
 	return kvm_set_msr_ignored_check(vcpu, index, data, false);
@@ -1902,6 +2090,11 @@ int kvm_emulate_monitor(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_monitor);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10574| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+ */
 static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
 {
 	xfer_to_guest_mode_prepare();
@@ -1916,6 +2109,10 @@ static inline bool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)
  * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the
  * other cases which must be called after interrupts are enabled on the host.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2046| <<handle_fastpath_set_msr_irqoff>> if (!handle_fastpath_set_x2apic_icr_irqoff(vcpu, data)) {
+ */
 static int handle_fastpath_set_x2apic_icr_irqoff(struct kvm_vcpu *vcpu, u64 data)
 {
 	if (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))
@@ -1946,6 +2143,11 @@ static int handle_fastpath_set_tscdeadline(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3696| <<svm_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6852| <<vmx_exit_handlers_fastpath>> return handle_fastpath_set_msr_irqoff(vcpu);
+ */
 fastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu)
 {
 	u32 msr = kvm_rcx_read(vcpu);
@@ -1986,6 +2188,10 @@ static int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 	return kvm_get_msr_ignored_check(vcpu, index, data, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6297| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+ */
 static int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
 	return kvm_set_msr_ignored_check(vcpu, index, *data, true);
@@ -1998,6 +2204,13 @@ struct pvclock_clock {
 	u64 mask;
 	u32 mult;
 	u32 shift;
+	/*
+	 * 在以下使用pvclock_clock->base_cycles:
+	 *   - arch/x86/kvm/x86.c|2224| <<update_pvclock_gtod>> vdata->clock.base_cycles = tk->tkr_mono.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|2232| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|3361| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+	 *   - arch/x86/kvm/x86.c|3385| <<do_realtime>> ns = gtod->clock.base_cycles;
+	 */
 	u64 base_cycles;
 	u64 offset;
 };
@@ -2005,6 +2218,13 @@ struct pvclock_clock {
 struct pvclock_gtod_data {
 	seqcount_t	seq;
 
+	/*
+	 * CLOCK_MONOTONIC和CLOCK_MONOTONIC_RAW
+	 *
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 */
 	struct pvclock_clock clock; /* extract of a clocksource struct */
 	struct pvclock_clock raw_clock; /* extract of a clocksource struct */
 
@@ -2012,15 +2232,144 @@ struct pvclock_gtod_data {
 	u64		wall_time_sec;
 };
 
+/*
+ * 这是4.14的例子
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $1 = {
+ *   seq = {
+ *     sequence = 507970306
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 10566905113766286,
+ *     mask = 18446744073709551615,
+ *     mult = 4945956,
+ *     shift = 24
+ *   },
+ *   boot_ns = 2220701952811493,
+ *   nsec_base = 334196164124419,
+ *   wall_time_sec = 1630097708
+ * }
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $4 = {
+ *   seq = {
+ *     sequence = 508028248
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 10567925239710761,
+ *     mask = 18446744073709551615,
+ *     mult = 4945954,
+ *     shift = 24
+ *   },
+ *   boot_ns = 2221001952811493,
+ *   nsec_base = 12666403808125002,
+ *   wall_time_sec = 1630098008
+ * }
+ *
+ * 这是5.4的例子
+ *
+ * crash> pvclock_gtod_data
+ * pvclock_gtod_data = $3 = {
+ *   seq = {
+ *     sequence = 43526056
+ *   },
+ *   clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 11962450506088969,
+ *     mask = 18446744073709551615,
+ *     mult = 4945955,
+ *     shift = 24,
+ *     base_cycles = 6161127504543786,
+ *     offset = 240303143085771
+ *   },
+ *   raw_clock = {
+ *     vclock_mode = 1,
+ *     cycle_last = 11962450506088969,
+ *     mask = 18446744073709551615,
+ *     mult = 4945492,
+ *     shift = 24,
+ *     base_cycles = 14693326820319724,
+ *     offset = 240280000000000
+ *   },
+ *   offs_boot = 0,
+ *   wall_time_sec = 1630509117
+ * }
+ *
+ * 在以下使用pvclock_gtod_data:
+ *   - arch/x86/kvm/x86.c|2024| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2055| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - arch/x86/kvm/x86.c|2275| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2353| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+ *   - arch/x86/kvm/x86.c|2478| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+ *   - arch/x86/kvm/x86.c|2533| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2552| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ *   - arch/x86/kvm/x86.c|2575| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2587| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2660| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+ *   - arch/x86/kvm/x86.c|8172| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+ */
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * 4.14上的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|8175| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
 
+	/*
+	 * start a seqcount_t write side critical section
+	 */
 	write_seqcount_begin(&vdata->seq);
 
 	/* copy pvclock gtod data */
+	/*
+	 * struct tk_read_base - base structure for timekeeping readout
+	 * @clock:      Current clocksource used for timekeeping.
+	 * @mask:       Bitmask for two's complement subtraction of non 64bit clocks
+	 * @cycle_last: @clock cycle value at last update
+	 * @mult:       (NTP adjusted) multiplier for scaled math conversion
+	 * @shift:      Shift value for scaled math conversion
+	 * @xtime_nsec: Shifted (fractional) nano seconds offset for readout
+	 * @base:       ktime_t (nanoseconds) base time for readout
+	 * @base_real:  Nanoseconds base value for clock REALTIME readout
+	 *
+	 * struct timekeeper *tk:
+	 * -> struct tk_read_base tkr_mono; --> The readout base structure for CLOCK_MONOTONIC
+	 *    -> struct clocksource *clock;
+	 *       -> u64     mask;
+	 *       -> u64     cycle_last;
+	 *       -> u32     mult;
+	 *       -> u32     shift;
+	 *       -> u64     xtime_nsec;
+	 *       -> ktime_t base;
+	 *       -> u64     base_real;
+	 * -> struct tk_read_base tkr_raw; --> The readout base structure for CLOCK_MONOTONIC_RAW
+	 */
 	vdata->clock.vclock_mode	= tk->tkr_mono.clock->vdso_clock_mode;
 	vdata->clock.cycle_last		= tk->tkr_mono.cycle_last;
 	vdata->clock.mask		= tk->tkr_mono.mask;
@@ -2037,6 +2386,17 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	vdata->raw_clock.base_cycles	= tk->tkr_raw.xtime_nsec;
 	vdata->raw_clock.offset		= tk->tkr_raw.base;
 
+	/*
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 *
+	 * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+	 * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+	 * 每来一个timer interrupt,也需要去更新xtime.
+	 *
+	 * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+	 */
 	vdata->wall_time_sec            = tk->xtime_sec;
 
 	vdata->offs_boot		= tk->offs_boot;
@@ -2044,6 +2404,18 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	write_seqcount_end(&vdata->seq);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2079| <<get_kvmclock_base_ns>> static s64 get_kvmclock_base_ns(void )
+ *   - arch/x86/kvm/x86.c|2391| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|2728| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2744| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|2855| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|5949| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|10812| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+ *
+ * 应该是返回启动以后的ns
+ */
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Count up from boot time, but with the frequency of the raw clock.  */
@@ -2053,10 +2425,21 @@ static s64 get_kvmclock_base_ns(void)
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Master clock not used, so we can just use CLOCK_BOOTTIME.  */
+	/*
+	 * monotonic time since boot in ns format
+	 */
 	return ktime_get_boottime_ns();
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3317| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|3324| <<kvm_set_msr_common>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/xen.c|58| <<kvm_xen_shared_info_init>> kvm_write_wall_clock(kvm, gpa + wc_ofs, sec_hi_ofs - wc_ofs);
+ *
+ * 根据分析, 这里分享给VM的wall clock是不变的 (就是启动的时间)
+ */
 void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2085,6 +2468,22 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+	 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+	 * jiffies一定是单调递增的.
+	 *
+	 * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+	 * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+	 * 每来一个timer interrupt,也需要去更新xtime.
+	 *
+	 * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+	 *
+	 * ktime_get_real_ns(): get the real (wall-) time in ns
+	 *
+	 * 这里的wall_nsec指的应该是VM启动的时候的时间
+	 * 所以可以用当前的KVM的ns减去VM已经运行了的ns
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -2103,26 +2502,77 @@ void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3473| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|3479| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ *
+ * MSR_KVM_SYSTEM_TIME_NEW: old_msr是false
+ * MSR_KVM_SYSTEM_TIME    : old_msr是true
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->boot_vcpu_runs_old_kvmclock:
+		 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+		 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+		 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+		 *
+		 * 如果运行的是vcpu0,且是否使用旧的kvmclock msr与当前的
+		 * boot_vcpu_runs_old_kvmclock标志不一致,那么一定是出了
+		 * 一些什么问题,需要校准MASTERCLOCK,发出KVM_REQ_MASTERCLOCK_UPDATE
+		 * 请求
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
 		ka->boot_vcpu_runs_old_kvmclock = old_msr;
 	}
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> gpa_t time;
+	 *    -> struct gfn_to_hva_cache pv_time;
+	 */
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2179| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4339| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9358| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * kvm_gen_kvmclock_update()
+	 * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+	 * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 */
 	vcpu->arch.pv_time_enabled = false;
+	/*
+	 * 在guest的kvm_register_clock()会在最后一位设置1
+	 */
 	if (!(system_time & 1))
 		return;
 
+	/*
+	 * 会把传过来的地址参数data记录到pv_time中.
+	 * 这样子就可以通过pv_time来直接修改Guest中的pv time
+	 */
 	if (!kvm_gfn_to_hva_cache_init(vcpu->kvm,
 				       &vcpu->arch.pv_time, system_time & ~1ULL,
 				       sizeof(struct pvclock_vcpu_time_info)))
@@ -2131,12 +2581,24 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2351| <<kvm_get_time_scale>> *pmultiplier = div_frac(scaled64, tps32);
+ */
 static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 {
 	do_shl32_div32(dividend, divisor);
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2415| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3515| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ *
+ * 这里应该是要计算pshift和pmultiplier来让base_hz能转换成scaled_hz
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2166,12 +2628,44 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|3308| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+ *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+ *   - arch/x86/kvm/x86.c|3453| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|8696| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|8711| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|8730| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ *
+ * 在4.14上是3392425, 对应dmesg
+ * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+/*
+ * 在以下使用max_tsc_khz:
+ *   - arch/x86/kvm/x86.c|8927| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|8938| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+ *   - arch/x86/kvm/x86.c|11328| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ *
+ * crash> max_tsc_khz
+ * max_tsc_khz = $4 = 3392426
+ */
 static unsigned long max_tsc_khz;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2446| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2447| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 {
 	u64 v = (u64)khz * (1000000 + ppm);
@@ -2179,12 +2673,34 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 	return v;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2452| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
 
 	/* Guest TSC same frequency as host TSC? */
 	if (!scale) {
+		/*
+		 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+		 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+		 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+		 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+		 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+		 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+		 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+		 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+		 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+		 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+		 *
+		 * 在普通测试上是0 (可能没有migration吧)
+		 */
 		vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
 		return 0;
 	}
@@ -2192,7 +2708,22 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	/* TSC scaling supported? */
 	if (!kvm_has_tsc_control) {
 		if (user_tsc_khz > tsc_khz) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+			 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+			 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+			 *
+			 * 如果在KVM_SET_TSC_KHZ时设置的vTSCfreq > pTSCfreq,且Host不支持TSC Scaling,
+			 * 便会设置vcpu->arch.always_catchup = 1,每次VMExit就会发送KVM_REQ_CLOCK_UPDATE,
+			 * 从而保证TSC Offset不断增加,令Guest TSC不断catchup设定的vTSCfreq下的理论值.
+			 */
 			vcpu->arch.tsc_catchup = 1;
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+			 *   - arch/x86/kvm/x86.c|2454| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+			 */
 			vcpu->arch.tsc_always_catchup = 1;
 			return 0;
 		} else {
@@ -2211,10 +2742,33 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 		return -1;
 	}
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 * 在以下使用kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/debugfs.c|32| <<vcpu_get_tsc_scaling_ratio>> *val = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/svm.c|1455| <<svm_prepare_guest_switch>> u64 tsc_ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1573| <<vmx_vcpu_load_vmcs>> vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/vmx/vmx.c|7660| <<vmx_set_hv_timer>> vcpu->arch.tsc_scaling_ratio, &delta_tsc))
+	 *   - arch/x86/kvm/vmx/vmx.h|563| <<decache_tsc_multiplier>> vmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2362| <<kvm_scale_tsc>> u64 ratio = vcpu->arch.tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2528| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	vcpu->arch.tsc_scaling_ratio = ratio;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6044| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|11412| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, max_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2228,9 +2782,26 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	}
 
 	/* Compute a scale to convert nanoseconds in TSC cycles */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2415| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3515| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+	 *
+	 * 这里应该是要计算pshift和pmultiplier来让base_hz能转换成scaled_hz
+	 */
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2248,20 +2819,62 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3625| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ *
+ * 这个函数只在vcpu->tsc_catchup的情况下调用
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2319| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2517| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 注意: 这里是kernel_ns - vcpu->arch.this_tsc_nsec (中间有减号)
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2657| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	tsc += vcpu->arch.this_tsc_write;
 	return tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2382| <<kvm_track_tsc_matching>> (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
+ *   - arch/x86/kvm/x86.c|2727| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2730| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ *   - arch/x86/kvm/x86.c|2739| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|2742| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ *   - arch/x86/kvm/x86.c|8556| <<pvclock_gtod_notify>> if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
+ */
 static inline int gtod_is_based_on_tsc(int mode)
 {
+	/*
+	 * 在以下使用VDSO_CLOCKMODE_HVCLOCK:
+	 *   - drivers/clocksource/hyperv_timer.c|440| <<global>> .vdso_clock_mode = VDSO_CLOCKMODE_HVCLOCK,
+	 *   - arch/x86/entry/vdso/vma.c|215| <<vvar_fault>> if (tsc_pg && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))
+	 *   - arch/x86/include/asm/vdso/clocksource.h|8| <<VDSO_ARCH_CLOCKMODES>> VDSO_CLOCKMODE_HVCLOCK
+	 *   - arch/x86/include/asm/vdso/gettimeofday.h|263| <<__arch_get_hw_counter>> if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
+	 *   - arch/x86/kvm/x86.c|2396| <<gtod_is_based_on_tsc>> return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
+	 *   - arch/x86/kvm/x86.c|2878| <<vgettsc>> case VDSO_CLOCKMODE_HVCLOCK:
+	 *   - arch/x86/kvm/x86.c|2883| <<vgettsc>> *mode = VDSO_CLOCKMODE_HVCLOCK;
+	 *   - drivers/clocksource/hyperv_timer.c|425| <<hv_cs_enable>> vclocks_set_used(VDSO_CLOCKMODE_HVCLOCK);
+	 */
 	return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2552| <<kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2269,6 +2882,16 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *
+	 * 在4.14上8个vcpu的VM nr_vcpus_matched_tsc是7
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			 atomic_read(&vcpu->kvm->online_vcpus));
 
@@ -2280,6 +2903,27 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 调用kvm_gen_update_masterclock()
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+	 * 似乎用来填充"struct pvclock_vcpu_time_info"
+	 *
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 这里其实是两个条件 ('or'不是'and'):
+	 * - ka->use_master_clock: 是不是要再evaluate一次???
+	 * - (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched)
+	 *   其实这里有隐藏条件就是!ka->use_master_clock
+	 *   也就是说没用master clock但是符合用master clock的条件
+	 *   这个时候可以再次evaluate一次!
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2300,16 +2944,55 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
  *
  * N equals to kvm_tsc_scaling_ratio_frac_bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2661| <<kvm_scale_tsc>> _tsc = __scale_tsc(ratio, tsc);
+ *   - arch/x86/kvm/x86.c|11773| <<kvm_arch_hardware_setup>> __scale_tsc(kvm_max_tsc_scaling_ratio, tsc_khz));
+ *
+ * Multiply tsc by a fixed point number represented by ratio.
+ */
 static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 {
+	/*
+	 * 4.14简单测试
+	 * crash> kvm_tsc_scaling_ratio_frac_bits
+	 * kvm_tsc_scaling_ratio_frac_bits = $6 = 0 '\000'
+	 */
 	return mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2484| <<kvm_compute_tsc_offset>> tsc = kvm_scale_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/x86.c|2506| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+ *   - arch/x86/kvm/x86.c|2718| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc(vcpu, (u64) adjustment);
+ *   - arch/x86/kvm/x86.c|3281| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
+ *   - arch/x86/kvm/x86.c|4067| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(vcpu, rdtsc()) + tsc_offset;
+ */
 u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 {
 	u64 _tsc = tsc;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|2239| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2265| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = ratio;
+	 *   - arch/x86/kvm/x86.c|2277| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *
+	 * 在普通测试上是0 (可能没有migration吧)
+	 */
 	u64 ratio = vcpu->arch.tsc_scaling_ratio;
 
+	/*
+	 * 在以下设置kvm_default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/x86.c|11984| <<kvm_arch_hardware_setup>> kvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;
+	 * 在以下使用kvm_default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1565| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/vmx/vmx.c|7657| <<vmx_set_hv_timer>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2474| <<set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2545| <<kvm_set_tsc_khz>> vcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_scale_tsc>> if (ratio != kvm_default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3216| <<adjust_tsc_offset_host>> if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
+	 */
 	if (ratio != kvm_default_tsc_scaling_ratio)
 		_tsc = __scale_tsc(ratio, tsc);
 
@@ -2317,6 +3000,13 @@ u64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)
 }
 EXPORT_SYMBOL_GPL(kvm_scale_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2570| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2649| <<kvm_synchronize_tsc>> offset = kvm_compute_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3755| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4681| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu,
+ */
 static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2326,18 +3016,109 @@ static u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|530| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1611| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1622| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1707| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1791| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|1815| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|943| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2088| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|7646| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|2858| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|8386| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|9451| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->l1_tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|526| <<nested_vmcb02_prepare_control>> svm->vcpu.arch.l1_tsc_offset + svm->nested.ctl.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2357| <<kvm_read_l1_tsc>> return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
+	 *   - arch/x86/kvm/x86.c|2363| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.l1_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|2482| <<adjust_tsc_offset_guest>> u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3289| <<kvm_set_msr_common>> u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|3591| <<kvm_get_msr_common>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
+	 */
 	return vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
+/*
+ * cur_tsc_offset = 18443709364331608385,
+ *
+ * crash> eval 18443709364331608385
+ * hexadecimal: fff537f2a987f941
+ *     decimal: 18443709364331608385  (-3034709377943231)
+ *       octal: 1777651577125141774501
+ *      binary: 1111111111110101001101111111001010101001100001111111100101000001
+ *
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu0/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu1/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu2/tsc-offset
+ * -3034709377943231
+ * # cat /sys/kernel/debug/kvm/2967-13/vcpu3/tsc-offset
+ * -3034709377943231
+ *
+ * 
+ * 但是当5.4 kvm上notsc的时候, tsc_offset一直在变化:
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu0/tsc-offset 
+ * -3672408965251004
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu1/tsc-offset 
+ * -3672408670022485
+ * ... ...
+ * ... ...
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu0/tsc-offset 
+ * -3672408820963291
+ * # cat /sys/kernel/debug/kvm/5623-14/vcpu1/tsc-offset 
+ * -3672408669450647
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)
 {
 	vcpu->arch.l1_tsc_offset = offset;
+	/*
+	 * 在以下设置kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/svm/nested.c|790| <<nested_svm_vmexit>> svm->vcpu.arch.tsc_offset = svm->vcpu.arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3358| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset += vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|3429| <<nested_vmx_enter_non_root_mode>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|4467| <<nested_vmx_vmexit>> vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	 *   - arch/x86/kvm/x86.c|2743| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
+	 * 在以下使用kvm_vcpu_arch->tsc_offset:
+	 *   - arch/x86/kvm/debugfs.c|23| <<vcpu_get_tsc_offset>> *val = vcpu->arch.tsc_offset;
+	 *   - arch/x86/kvm/svm/nested.c|791| <<nested_svm_vmexit>> if (svm->vmcb->control.tsc_offset != svm->vcpu.arch.tsc_offset) {
+	 *   - arch/x86/kvm/svm/nested.c|792| <<nested_svm_vmexit>> svm->vmcb->control.tsc_offset = svm->vcpu.arch.tsc_offset;
+	 *   - arch/x86/kvm/vmx/nested.c|2535| <<prepare_vmcs02>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/nested.c|4503| <<nested_vmx_vmexit>> vmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);
+	 *   - arch/x86/kvm/vmx/vmx.c|1991| <<vmx_write_l1_tsc_offset>> trace_kvm_write_tsc_offset(vcpu->vcpu_id, vcpu->arch.tsc_offset - g_tsc_offset, offset);
+	 *   - arch/x86/kvm/x86.c|4478| <<kvm_get_msr_common(MSR_IA32_TSC)>> u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset : vcpu->arch.tsc_offset;
+	 *
+	 * vmx_write_l1_tsc_offset()
+	 */
 	vcpu->arch.tsc_offset = static_call(kvm_x86_write_l1_tsc_offset)(vcpu, offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2923| <<kvm_synchronize_tsc>> if (!kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5087| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5093| <<kvm_arch_vcpu_load>> if (kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|11395| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)
+ *   - arch/x86/kvm/x86.c|11660| <<kvm_arch_hardware_enable>> stable = !kvm_check_tsc_unstable();
+ *
+ * 返回true说明unstable
+ * 返回false说明stable
+ */
 static inline bool kvm_check_tsc_unstable(void)
 {
 #ifdef CONFIG_X86_64
@@ -2345,12 +3126,68 @@ static inline bool kvm_check_tsc_unstable(void)
 	 * TSC is marked unstable when we're running on Hyper-V,
 	 * 'TSC page' clocksource is good.
 	 */
+	/*
+	 * 在以下使用VDSO_CLOCKMODE_HVCLOCK:
+	 *   - drivers/clocksource/hyperv_timer.c|440| <<global>> .vdso_clock_mode = VDSO_CLOCKMODE_HVCLOCK,
+	 *   - arch/x86/entry/vdso/vma.c|215| <<vvar_fault>> if (tsc_pg && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))
+	 *   - arch/x86/include/asm/vdso/clocksource.h|8| <<VDSO_ARCH_CLOCKMODES>> VDSO_CLOCKMODE_HVCLOCK
+	 *   - arch/x86/include/asm/vdso/gettimeofday.h|263| <<__arch_get_hw_counter>> if (clock_mode == VDSO_CLOCKMODE_HVCLOCK) {
+	 *   - arch/x86/kvm/x86.c|2396| <<gtod_is_based_on_tsc>> return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
+	 *   - arch/x86/kvm/x86.c|2878| <<vgettsc>> case VDSO_CLOCKMODE_HVCLOCK:
+	 *   - arch/x86/kvm/x86.c|2883| <<vgettsc>> *mode = VDSO_CLOCKMODE_HVCLOCK;
+	 *   - drivers/clocksource/hyperv_timer.c|425| <<hv_cs_enable>> vclocks_set_used(VDSO_CLOCKMODE_HVCLOCK);
+	 */
 	if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
 		return false;
 #endif
 	return check_tsc_unstable();
 }
 
+/*
+ * 在4.14上, 2个vcpu调用6次, 4个vcpu调用12次
+ * /usr/share/bcc/tools/trace -t -C 'kvm_write_tsc'
+ * TIME     CPU PID     TID     COMM            FUNC
+ * 3.001404 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.002039 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.002591 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.003144 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.018026 0   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.018118 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.018204 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.018262 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ * 3.031890 1   26732   26737   CPU 0/KVM       kvm_write_tsc
+ * 3.031936 16  26732   26738   CPU 1/KVM       kvm_write_tsc
+ * 3.031979 18  26732   26739   CPU 2/KVM       kvm_write_tsc
+ * 3.032021 11  26732   26740   CPU 3/KVM       kvm_write_tsc
+ *
+ * 根据分析, 应该是QEMU来的 (不是VM kernel写的MSR TSC)
+ *
+ *
+ * KVM希望各个vCPU的TSC处在同步状态,称之为Master Clock模式,每当vTSC被修改,就有两种可能:
+ *
+ * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+ * 此时TSC进入下一代,体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+ * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offset,
+ * 其中offset即上述L1 TSC Offset.重新同步后，可以以此为基点导出任意vCPU的TSC值.
+ *
+ * vCPU在尝试重新同步TSC，此时不能将L1 offset设置为offset
+ * 此时会设置kvm->arch.nr_vcpus_matched_tsc++,一旦所有vCPU都处于matched状态,就可以重新回到Master Clock模式
+ * 在满足vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz的前提下(vTSCfreq相同是TSC能同步的前提),以下情形视为尝试同步TSC:
+ *
+ * Host Initiated且写入值为0,视为正在初始化
+ * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+ * 按照Host TSC是否同步(即是否stable),会为L1 TSC offset设置不同的值
+ *
+ * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+ * 对于Unstable Host,则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+ * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差值
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|3395| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|10581| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ *
+ * 4.14测试kvm_write_tsc的时候data=0
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2360,11 +3197,53 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool already_matched;
 	bool synchronizing = false;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * offset = vTSC - (pTSC * scale)
+	 * 这里是根据tsc计算的
+	 *
+	 * 根据写入的guest tsc和host的tsc计算scaling的offset
+	 */
 	offset = kvm_compute_tsc_offset(vcpu, data);
+	/*
+	 * monotonic time since boot in ns format
+	 *
+	 * 应该是返回启动以后的ns
+	 */
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+	 *
+	 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 在以下尝试同步TSC:
+	 *   - Host Initiated且写入值为0,视为正在初始化
+	 *   - 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+	 *
+	 * vcpu->arch.virtual_tsc_khz为vCPU的vTSCfreq
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 */
 	if (vcpu->arch.virtual_tsc_khz) {
 		if (data == 0) {
 			/*
@@ -2374,6 +3253,12 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 */
 			synchronizing = true;
 		} else {
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2542| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *   - arch/x86/kvm/x86.c|2599| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_write = data;
+			 *   - arch/x86/kvm/x86.c|11075| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_write = 0;
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2382,6 +3267,9 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 * of virtual cycle time against real time is
 			 * interpreted as an attempt to synchronize the CPU.
 			 */
+			/*
+			 * 写入的TSC值与kvm->arch.last_tsc_write偏差在1秒内
+			 */
 			synchronizing = data < tsc_exp + tsc_hz &&
 					data + tsc_hz > tsc_exp;
 		}
@@ -2393,11 +3281,59 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	 * compensation code attempt to catch up if we fall behind, but
 	 * it's better to try to match offsets from the beginning.
          */
+	/*
+	 * 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2285| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 *
+	 * 4.14在简单测试的时候
+	 * crash> cpu_khz
+	 * cpu_khz = $4 = 3392635
+	 * crash> tsc_khz
+	 * tsc_khz = $5 = 3392425
+	 *
+	 * virtual_tsc_khz也是3392425
+	 *
+	 * 在以下使用kvm_vcpu_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2562| <<kvm_synchronize_tsc>> vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|2600| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *
+	 * !!!! 第一次进来elapsed非常大, 但是不满足"vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz"
+	 */
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+		/*
+		 * 对于Stable Host,L1 TSC Offset设置为kvm->arch.cur_tsc_offset
+		 * 对于Unstable Host, 则将L1 TSC Offset设置为(vTSC + nsec_to_tsc(boot_time - kvm->arch.last_tsc_nsec)) - (pTSC * scale),
+		 * 即假设本次和上次写入的TSC值相同,然后补偿上Host Boot Time的差
+		 */
+
+		/*
+		 * kvm_check_tsc_unstable():
+		 * 返回true说明unstable
+		 * 返回false说明stable
+		 */
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 如果是第一次进来, offset其实就是offset = kvm_compute_tsc_offset(vcpu, data);
+			 *
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *
+			 * 如果tsc是stable的话, 所有的用一个就可以
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_tsc_nsec:
+			 *   - arch/x86/kvm/x86.c|2447| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+			 *   - arch/x86/kvm/x86.c|2509| <<kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+			 *   - arch/x86/kvm/x86.c|10790| <<kvm_arch_hardware_enable>> kvm->arch.last_tsc_nsec = 0;
+			 *
+			 * 表示写入时刻的Host Boot Time (monotonic time since boot in ns format)
+			 *
+			 * elapsed = ns - kvm->arch.last_tsc_nsec;
+			 */
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
 			data += delta;
 			offset = kvm_compute_tsc_offset(vcpu, data);
@@ -2405,6 +3341,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		matched = true;
 		already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
 	} else {
+		/*
+		 * 破坏了已同步的TSC,此时将L1 TSC offset设置为offset即可
+		 * 此时TSC进入下一代m体现为kvm->arch.cur_tsc_generation++且kvm->arch.nr_vcpus_matched_tsc = 0
+		 * 此时还会记录kvm->arch.cur_tsc_nsec,kvm->arch.cur_tsc_write,kvm->arch.cur_tsc_offsetm
+		 * 其中offset即上述L1 TSC Offset.重新同步后, 可以以此为基点导出任意vCPU的TSC值.
+		 */
+
 		/*
 		 * We split periods of matched TSC writes into generations.
 		 * For each generation, we track the original measured
@@ -2414,9 +3357,25 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+		 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 *
+		 * 测试的例子是cur_tsc_generation = 1
+		 */
 		kvm->arch.cur_tsc_generation++;
+		/*
+		 * 这个是根据get_kvmclock_base_ns()得来的, 返回的是host启动以后经历的时间
+		 */
 		kvm->arch.cur_tsc_nsec = ns;
 		kvm->arch.cur_tsc_write = data;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2702| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+		 *   - arch/x86/kvm/x86.c|2736| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+		 */
 		kvm->arch.cur_tsc_offset = offset;
 		matched = false;
 	}
@@ -2425,22 +3384,59 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	 * We also track th most recent recorded KHZ, write and time to
 	 * allow the matching interval to be extended at each write.
 	 */
+	/*
+	 * 这个是根据get_kvmclock_base_ns()得来的, 返回的是host启动以后经历的时间
+	 */
 	kvm->arch.last_tsc_nsec = ns;
 	kvm->arch.last_tsc_write = data;
 	kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 */
 	vcpu->arch.last_guest_tsc = data;
 
 	/* Keep track of which generation this VCPU has synchronized to */
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2699| <<kvm_synchronize_tsc>> already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
+	 *   - arch/x86/kvm/x86.c|2710| <<kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|2614| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|2762| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|11513| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 */
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
 	spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
 	if (!matched) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+		 */
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (!already_matched) {
 		kvm->arch.nr_vcpus_matched_tsc++;
@@ -2450,13 +3446,32 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3035| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3627| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|4140| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|4169| <<kvm_set_msr_common>> adjust_tsc_offset_guest(vcpu, adj);
+ *
+ * 核心思想是在kvm_vcpu_write_tsc_offset()加上一个adjustment
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
 	u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5082| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+ */
 static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 {
 	if (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)
@@ -2467,6 +3482,10 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2578| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2487,6 +3506,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2602| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2622| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2523,6 +3547,24 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+ * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+ * jiffies一定是单调递增的.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+ *
+ * CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock
+ * is affected by NTP adjustments and can jump forward and backward when a
+ * system administrator adjusts system time.
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2962| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2532,6 +3574,13 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 
 	do {
 		seq = read_seqcount_begin(&gtod->seq);
+		/*
+		 * CLOCK_MONOTONIC和CLOCK_MONOTONIC_RAW
+		 *
+		 * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+		 * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+		 * jiffies一定是单调递增的.
+		 */
 		ns = gtod->raw_clock.base_cycles;
 		ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
 		ns >>= gtod->raw_clock.shift;
@@ -2542,6 +3591,24 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * monotonic time字面意思是单调时间,实际上它指的是系统启动以后流逝的时间,这是由变量jiffies来记录的.
+ * 系统每次启动时jiffies初始化为0,每来一个timer interrupt,jiffies加1,也就是说它代表系统启动后流逝的tick数.
+ * jiffies一定是单调递增的.
+ *
+ * wall time字面意思是挂钟时间,实际上就是指的是现实的时间,这是由变量xtime来记录的.
+ * 系统每次启动时将CMOS上的RTC时间读入xtime,这个值是"自1970-01-01起经历的秒数,本秒中经历的纳秒数",
+ * 每来一个timer interrupt,也需要去更新xtime.
+ *
+ * 相比monotonic time, wall time未必是单调递增的, 因为这个时间(比如RTC)是可以修改的.
+ *
+ * CLOCK_REALTIME clock gives the time passed since January 1, 1970. This clock
+ * is affected by NTP adjustments and can jump forward and backward when a
+ * system administrator adjusts system time.
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|2651| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2564,6 +3631,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2932| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
@@ -2575,6 +3646,10 @@ static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 }
 
 /* returns true if host is using TSC based clocksource */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9670| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+ */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
 {
@@ -2627,6 +3702,20 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2759| <<kvm_gen_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|8046| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|10888| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ *
+ * GTOD: Generic time of Day
+ *
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2634,6 +3723,14 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	int vclock_mode;
 	bool host_tsc_clocksource, vcpus_matched;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2338| <<kvm_track_tsc_matching>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2547| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2549| <<kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|2759| <<pvclock_update_vm_gtod_copy>> vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
@@ -2641,14 +3738,46 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
 	 */
+	/*
+	 * 若Host Clocksource为TSC, 则读取当前时刻的TSC值, 记为pTSC1,
+	 * pvclock中记录的TSC值记为pTSC0, 我们据此设置Master Clock:
+	 *
+	 * kvm->arch.master_cycle_now设置为pTSC1, 即该时刻的Host TSC值
+	 * kvm->arch.master_kernel_ns设置为nsec_base + tsc_to_nsec(pTSC1 - pTSC0) + boot_ns 即该时刻的Host Boot Time
+	 *
+	 * kvm_get_time_and_clockread()的返回值: returns true if host is using TSC based clocksource
+	 */
 	host_tsc_clocksource = kvm_get_time_and_clockread(
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 在以下使用kvm_arch->backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|2950| <<pvclock_update_vm_gtod_copy>> && !ka->backwards_tsc_observed
+	 *   - arch/x86/kvm/x86.c|11175| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 *
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 *
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2183| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2186| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|2821| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 *
+	 * 如果写入的是MSR_KVM_SYSTEM_TIME,表明Guest使用的是旧版kvmclock,不支持Master Clock模式,
+	 * 此时要设置kvm->arch.boot_vcpu_runs_old_kvmclock = 1,并对当前vCPU(即vCPU0)发送一个KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 这最终会导致kvm->arch.use_master_clock = 0
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2658,11 +3787,48 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3606| <<kvm_gen_update_masterclock>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9130| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * 调用的次数非常少, 就是qemu创建和VM kernel启动的时候, 下面是4.14的例子
+ * kvm_gen_update_masterclock
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1270| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2158| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2336| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|8243| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9288| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|10748| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|59| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|6011| <<kvm_arch_vm_ioctl>> kvm_gen_update_masterclock(kvm);
+ *   - arch/x86/kvm/x86.c|9289| <<vcpu_enter_guest>> kvm_gen_update_masterclock(vcpu->kvm);
+ *
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE
+ * 核心思想是为整个kvm更新:
+ *   - ka->use_master_clock
+ *   - ka->master_kernel_ns
+ *   - ka->master_cycle_now
+ *   - kvm_guest_has_master_clock
+ * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+ * 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvm_gen_update_masterclock(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2671,15 +3837,35 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 	struct kvm_arch *ka = &kvm->arch;
 	unsigned long flags;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2753| <<kvm_gen_update_masterclock>> kvm_hv_invalidate_tsc_page(kvm);
+	 */
 	kvm_hv_invalidate_tsc_page(kvm);
 
+	/*
+	 * 申请kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *
+	 * 给所有vCPU发送KVM_REQ_MCLOCK_INPROGRESS,将它们踢出Guest模式
+	 * 该请求没有Handler,因此vCPU无法再进入Guest
+	 */
 	kvm_make_mclock_inprogress_request(kvm);
 
 	/* no guest entries from this point */
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	/*
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 	spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
 
+	/*
+	 * KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -2689,6 +3875,15 @@ static void kvm_gen_update_masterclock(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|527| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2164| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/x86.c|6578| <<kvm_arch_vm_ioctl>> now_ns = get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|69| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|405| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|446| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -2699,6 +3894,9 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
 	if (!ka->use_master_clock) {
 		spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
+		/*
+		 * get_kvmclock_base_ns()应该是返回启动以后的ns
+		 */
 		return get_kvmclock_base_ns() + ka->kvmclock_offset;
 	}
 
@@ -2709,10 +3907,21 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
 	get_cpu();
 
+	/*
+	 * 在4.14上是3392425, 对应dmesg
+	 * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+	 */
 	if (__this_cpu_read(cpu_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
+		/*
+		 * called by:
+		 *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+		 *   - arch/x86/kernel/pvclock.c|76| <<pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+		 *   - arch/x86/kvm/x86.c|3072| <<get_kvmclock_ns>> ret = __pvclock_read_cycles(&hv_clock, rdtsc());
+		 *   - drivers/ptp/ptp_kvm_x86.c|91| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair.tsc);
+		 */
 		ret = __pvclock_read_cycles(&hv_clock, rdtsc());
 	} else
 		ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
@@ -2722,6 +3931,16 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2868| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|2870| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|2873| <<kvm_guest_time_update>> kvm_setup_pvclock_page(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ *
+ * vcpu_enter_guest()
+ * -> kvm_guest_time_update()
+ *    -> kvm_setup_pvclock_page()
+ */
 static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				   struct gfn_to_hva_cache *cache,
 				   unsigned int offset)
@@ -2752,6 +3971,9 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	if (guest_hv_clock.version & 1)
 		++guest_hv_clock.version;  /* first time write, random junk */
 
+	/* struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.version = guest_hv_clock.version + 1;
 	kvm_write_guest_offset_cached(v->kvm, cache,
 				      &vcpu->hv_clock, offset,
@@ -2760,8 +3982,23 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 	smp_wmb();
 
 	/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
+	/*
+	 * 在以下使用PVCLOCK_GUEST_STOPPED:
+	 *   - arch/x86/kernel/kvmclock.c|152| <<kvm_check_and_clear_guest_paused>> if ((src->pvti.flags & PVCLOCK_GUEST_STOPPED) != 0) {
+	 *   - arch/x86/kernel/kvmclock.c|153| <<kvm_check_and_clear_guest_paused>> src->pvti.flags &= ~PVCLOCK_GUEST_STOPPED;
+	 *   - arch/x86/kernel/pvclock.c|80| <<pvclock_clocksource_read>> if (unlikely((flags & PVCLOCK_GUEST_STOPPED) != 0)) {
+	 *   - arch/x86/kernel/pvclock.c|81| <<pvclock_clocksource_read>> src->flags &= ~PVCLOCK_GUEST_STOPPED;
+	 *   - arch/x86/kvm/x86.c|2769| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
+	 *   - arch/x86/kvm/x86.c|2772| <<kvm_setup_pvclock_page>> vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
+	 */
 	vcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pvclock_set_guest_stopped_request:
+	 *   - arch/x86/kvm/x86.c|2765| <<kvm_setup_pvclock_page>> if (vcpu->pvclock_set_guest_stopped_request) {
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_setup_pvclock_page>> vcpu->pvclock_set_guest_stopped_request = false;
+	 *   - arch/x86/kvm/x86.c|4767| <<kvm_set_guest_paused>> vcpu->arch.pvclock_set_guest_stopped_request = true;
+	 */
 	if (vcpu->pvclock_set_guest_stopped_request) {
 		vcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
 		vcpu->pvclock_set_guest_stopped_request = false;
@@ -2781,6 +4018,12 @@ static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
 				     sizeof(vcpu->hv_clock.version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9174| <<vcpu_enter_guest>> r = kvm_guest_time_update(vcpu);
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -2799,8 +4042,20 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 * to the guest.
 	 */
 	spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2783| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 */
 	use_master_clock = ka->use_master_clock;
 	if (use_master_clock) {
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|2945| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3079| <<get_kvmclock_ns>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3216| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *
+		 * live crash了好多次, 这两个都不变
+		 */
 		host_tsc = ka->master_cycle_now;
 		kernel_ns = ka->master_kernel_ns;
 	}
@@ -2808,14 +4063,33 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* Keep irq disabled to prevent changes to the clock */
 	local_irq_save(flags);
+	/*
+	 * 在以下使用cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3308| <<get_kvmclock_ns>> if (__this_cpu_read(cpu_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3309| <<get_kvmclock_ns>> kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3453| <<kvm_guest_time_update>> tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|8696| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|8711| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|8730| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 *
+	 * 在4.14上是3392425, 对应dmesg
+	 * [    5.536054] tsc: Refined TSC clocksource calibration: 3392.425 MHz
+	 */
 	tgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);
 	if (unlikely(tgt_tsc_khz == 0)) {
 		local_irq_restore(flags);
+		/*
+		 * kvm_guest_time_update()
+		 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 		return 1;
 	}
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
+		/*
+		 * kernel启动后的时间ns
+		 */
 		kernel_ns = get_kvmclock_base_ns();
 	}
 
@@ -2831,6 +4105,14 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2320| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3246| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|4631| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 *
+	 * 测试的一个例子是vcpu->tsc_catchup=false
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -2843,9 +4125,22 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	/* With all the info we got, fill in the values */
 
+	/*
+	 * 在以下设置kvm_has_tsc_control:
+	 *   - arch/x86/kvm/svm/svm.c|962| <<svm_hardware_setup>> kvm_has_tsc_control = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|8063| <<hardware_setup>> kvm_has_tsc_control = true;
+	 */
 	if (kvm_has_tsc_control)
 		tgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);
 
+	/*
+	 * 将1KHZ TSC转换成guest TSC
+	 * 如果当前guest时钟(kvmclock)的频率不同，则更新转换比例
+	 *
+	 * 在以下使用kvm_vcpu_arch->hw_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|3506| <<kvm_guest_time_update>> if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
+	 *   - arch/x86/kvm/x86.c|3510| <<kvm_guest_time_update>> vcpu->hw_tsc_khz = tgt_tsc_khz;
+	 */
 	if (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {
 		kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
 				   &vcpu->hv_clock.tsc_shift,
@@ -2853,8 +4148,46 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		vcpu->hw_tsc_khz = tgt_tsc_khz;
 	}
 
+	/*
+	 * 在kvm vm上测试的时候, 下面的都不变
+	 * system_time=23867275
+	 * tsc_timestamp=206742471, version=4, tsc_to_system_mul=2532092704, tsc_shift=255
+	 * kvm_clock_read()一直在变
+	 *
+	 * 把hypervisor clock换成hpet就开始增长了!!!!!
+	 *
+	 *
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 *    -> u64 tsc_timestamp;
+	 *    -> u64 system_time;
+	 *
+	 * 更新pvti时,vCPU的vTSC
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 *
+	 * 当dmesg是14.903088的时候,
+	 * kvm_clock_read()返回15169858774 (15.169858774)
+	 *
+	 * 更新pvti时,Guest的虚拟时间,单位为纳秒
+	 */
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+	 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	vcpu->last_guest_tsc = tsc_timestamp;
 
 	/* If the host uses TSC clocksource, then it is stable */
@@ -2864,6 +4197,35 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	vcpu->hv_clock.flags = pvclock_flags;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time_enabled:
+	 *   - arch/x86/kvm/x86.c|2210| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|2217| <<kvm_write_system_time>> vcpu->arch.pv_time_enabled = true;
+	 *   - arch/x86/kvm/x86.c|3240| <<kvm_guest_time_update>> if (vcpu->pv_time_enabled)
+	 *   - arch/x86/kvm/x86.c|3472| <<kvmclock_reset>> vcpu->arch.pv_time_enabled = false;
+	 *   - arch/x86/kvm/x86.c|5192| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time_enabled)
+	 *
+	 * pv_time = {
+         *   generation = 182, 
+	 *   gpa = 15401488384, 
+	 *   hva = 140509217558528, 
+	 *   len = 32, 
+	 *   memslot = 0xffff8c237bd80008
+	 * }, 
+	 * pv_time_enabled = true, 
+	 * pvclock_set_guest_stopped_request = false, 
+	 * st = {
+	 *   preempted = 1 '\001', 
+	 *   msr_val = 17979068481, 
+	 *   last_steal = 81611316161, 
+	 *   cache = {
+	 *     generation = 182, 
+	 *     gfn = 4389421, 
+	 *     pfn = 28505645, 
+	 *     dirty = true
+	 *   }
+	 * },
+	 */
 	if (vcpu->pv_time_enabled)
 		kvm_setup_pvclock_page(v, &vcpu->pv_time, 0);
 	if (vcpu->xen.vcpu_info_set)
@@ -2892,6 +4254,13 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvmclock_update_fn():
+ *   - arch/x86/kvm/x86.c|11121| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *
+ * 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	int i;
@@ -2902,33 +4271,81 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9359| <<vcpu_enter_guest>> kvm_gen_kvmclock_update(vcpu);
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE
+ * 核心思想是为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+ * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 调用kvm_guest_time_update()
+	 * 处理KVM_REQ_CLOCK_UPDATE, 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 调用kvmclock_update_fn(), 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
+/*
+ * 在以下使用KVMCLOCK_SYNC_PERIOD:
+ *   - arch/x86/kvm/x86.c|3281| <<kvmclock_sync_fn>> KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|10875| <<kvm_arch_vcpu_postcreate>> KVMCLOCK_SYNC_PERIOD);
+ *
+ * 300秒
+ */
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
+	/*
+	 * struct kvm_arch:
+	 * -> struct delayed_work kvmclock_sync_work;
+	 */
 	struct kvm_arch *ka = container_of(dwork, struct kvm_arch,
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|139| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|3206| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|10792| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (!kvmclock_periodic_sync)
 		return;
 
+	/*
+	 * 调用kvmclock_update_fn(), 为kvm的每一个vcpu触发KVM_REQ_CLOCK_UPDATE
+	 * 调用kvm_guest_time_update(), 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3754| <<kvmclock_sync_fn>> kvmclock_sync_work);
+	 *   - arch/x86/kvm/x86.c|3765| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11435| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
+	 *   - arch/x86/kvm/x86.c|11788| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|11834| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -3057,6 +4474,11 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11798| <<kvm_arch_vcpu_destroy>> kvmclock_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|11847| <<kvm_vcpu_reset>> kvmclock_reset(vcpu);
+ */
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.pv_time_enabled = false;
@@ -3133,8 +4555,27 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 
 	smp_wmb();
 
+	/*
+	 * struct kvm_steal_time {
+	 * -> __u64 steal;
+	 * -> __u32 version;
+	 * -> __u32 flags;
+	 * -> __u8  preempted;
+	 * -> __u8  u8_pad[3];
+	 * -> __u32 pad[11];
+	 * };
+	 */
 	st->steal += current->sched_info.run_delay -
 		vcpu->arch.st.last_steal;
+	/*
+	 * struct kvm_vcpu_arch:
+	 * -> struct {
+	 *        -> u8 preempted;
+	 *        -> u64 msr_val;
+	 *        -> u64 last_steal;
+	 *        -> struct gfn_to_pfn_cache cache;
+	 *    } st;
+	 */
 	vcpu->arch.st.last_steal = current->sched_info.run_delay;
 
 	smp_wmb();
@@ -3147,6 +4588,12 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
+	/*
+	 * struct msr_data:
+	 * -> bool host_initiated;
+	 * -> u32 index; 
+	 * -> u64 data;
+	 */
 	u32 msr = msr_info->index;
 	u64 data = msr_info->data;
 
@@ -3219,11 +4666,24 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		kvm_set_lapic_tscdeadline_msr(vcpu, data);
 		break;
 	case MSR_IA32_TSC_ADJUST:
+		/*
+		 * 关于msr_tsc_adjust
+		 * 如果想要修改tsc的counter, 可以直接写入msr_tsc.
+		 * 然而这样很难保证所有的CPU是同步的 (因为每个CPU都要往msr_tsc写一个absolute value)
+		 * 使用msr_tsc_adjust就可以解决这个问题, 因为为每个CPU写入的是offset, 不是absolute valu
+		 */
 		if (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {
 			if (!msr_info->host_initiated) {
 				s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
 				adjust_tsc_offset_guest(vcpu, adj);
 			}
+			/*
+			 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+			 *   - arch/x86/kvm/x86.c|4571| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+			 *   - arch/x86/kvm/x86.c|4574| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+			 *   - arch/x86/kvm/x86.c|4602| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+			 *   - arch/x86/kvm/x86.c|4963| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+			 */
 			vcpu->arch.ia32_tsc_adjust_msr = data;
 		}
 		break;
@@ -3247,11 +4707,30 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		vcpu->arch.msr_ia32_power_ctl = data;
 		break;
 	case MSR_IA32_TSC:
+		/*
+		 * 在以下设置msr_data->host_initiated:
+		 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+		 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+		 *
+		 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+		 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+		 * msr_info->host_initiated就为false.
+		 */
 		if (msr_info->host_initiated) {
 			kvm_synchronize_tsc(vcpu, data);
 		} else {
 			u64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
 			adjust_tsc_offset_guest(vcpu, adj);
+			/*
+			 * 在以下使用kvm_vcpu_arch->ia32_tsc_adjust_msr:
+			 *   - arch/x86/kvm/x86.c|4571| <<kvm_set_msr_common>> s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+			 *   - arch/x86/kvm/x86.c|4574| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr = data;
+			 *   - arch/x86/kvm/x86.c|4602| <<kvm_set_msr_common>> vcpu->arch.ia32_tsc_adjust_msr += adj;
+			 *   - arch/x86/kvm/x86.c|4963| <<kvm_get_msr_common>> msr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;
+			 */
 			vcpu->arch.ia32_tsc_adjust_msr += adj;
 		}
 		break;
@@ -3284,6 +4763,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))
 			return 1;
 
+		/*
+		 * 似乎在以下使用kvm_vcpu_arch->wall_clock (gpa_t):
+		 *   - arch/x86/kvm/x86.c|3690| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+		 *   - arch/x86/kvm/x86.c|3697| <<kvm_set_msr_common>> vcpu->kvm->arch.wall_clock = data;
+		 *   - arch/x86/kvm/x86.c|4026| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+		 *   - arch/x86/kvm/x86.c|4032| <<kvm_get_msr_common>> msr_info->data = vcpu->kvm->arch.wall_clock;
+		 */
 		vcpu->kvm->arch.wall_clock = data;
 		kvm_write_wall_clock(vcpu->kvm, data, 0);
 		break;
@@ -3476,6 +4962,13 @@ static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2725| <<svm_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/svm/svm.c|2738| <<svm_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2055| <<vmx_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2180| <<vmx_get_msr>> return kvm_get_msr_common(vcpu, msr_info);
+ */
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	switch (msr_info->index) {
@@ -3552,6 +5045,18 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		 * return L1's TSC value to ensure backwards-compatible
 		 * behavior for migration.
 		 */
+		/*
+		 * 在以下设置msr_data->host_initiated:
+		 *   - arch/x86/kvm/svm/svm.c|2610| <<efer_trap>> msr_info.host_initiated = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4689| <<vmx_vcpu_reset>> apic_base_msr.host_initiated = true;
+		 *   - arch/x86/kvm/x86.c|1765| <<__kvm_set_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|1810| <<__kvm_get_msr>> msr.host_initiated = host_initiated;
+		 *   - arch/x86/kvm/x86.c|11540| <<__set_sregs>> apic_base_msr.host_initiated = true;
+		 *
+		 * msr_info->host_initiated用于区分此次读MSR内容的动作是由qemu发起的,还是由guest自己发起的.
+		 * 如果是qemu发起的,msr_info->host_initiated就为true,如果是guest自己发起的,
+		 * msr_info->host_initiated就为false.
+		 */
 		u64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :
 							    vcpu->arch.tsc_offset;
 
@@ -4018,6 +5523,11 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		if (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))
 			goto out;
 		n = msr_list.nmsrs;
+		/*
+		 * 在以下使用msrs_to_save:
+		 *   - arch/x86/kvm/x86.c|4034| <<kvm_arch_dev_ioctl>> if (copy_to_user(user_msr_list->indices, &msrs_to_save,
+		 *   - arch/x86/kvm/x86.c|6062| <<kvm_init_msr_list>> msrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];
+		 */
 		msr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;
 		if (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))
 			goto out;
@@ -4108,6 +5618,32 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * 4.14的例子.
+ * kvm_arch_vcpu_load
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_arch_vcpu_load
+ * finish_task_switch
+ * __schedule
+ * schedule
+ * kvm_vcpu_block
+ * kvm_arch_vcpu_ioctl_run
+ * __dta_kvm_vcpu_ioctl_657
+ * do_vfs_ioctl
+ * sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|211| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5058| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -4132,14 +5668,39 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	}
 
 	if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+		/*
+		 * 在以下设置kvm_vcpu_arch->last_host_tsc:
+		 *   - arch/x86/kvm/x86.c|5282| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+		 *   - arch/x86/kvm/x86.c|11850| <<kvm_arch_hardware_enable>> vcpu->arch.last_host_tsc = local_tsc;
+		 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+		 *   - arch/x86/kvm/x86.c|5211| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+		 *   - arch/x86/kvm/x86.c|5212| <<kvm_arch_vcpu_load>> rdtsc() - vcpu->arch.last_host_tsc;
+		 *   - arch/x86/kvm/x86.c|11798| <<kvm_arch_hardware_enable>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+		 *   - arch/x86/kvm/x86.c|11800| <<kvm_arch_hardware_enable>> if (vcpu->arch.last_host_tsc > max_tsc)
+		 *   - arch/x86/kvm/x86.c|11801| <<kvm_arch_hardware_enable>> max_tsc = vcpu->arch.last_host_tsc;
+		 */
 		s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
 				rdtsc() - vcpu->arch.last_host_tsc;
 		if (tsc_delta < 0)
 			mark_tsc_unstable("KVM discovered backwards TSC");
 
 		if (kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下设置kvm_vcpu_arch->last_guest_tsc:
+			 *   - arch/x86/kvm/x86.c|2675| <<kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = data;
+			 *   - arch/x86/kvm/x86.c|3270| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+			 *   - arch/x86/kvm/x86.c|10014| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+			 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+			 *   - arch/x86/kvm/x86.c|4629| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+			 */
 			u64 offset = kvm_compute_tsc_offset(vcpu,
 						vcpu->arch.last_guest_tsc);
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/x86.c|2898| <<kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 *   - arch/x86/kvm/x86.c|2930| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+			 *   - arch/x86/kvm/x86.c|4991| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 */
 			kvm_vcpu_write_tsc_offset(vcpu, offset);
 			vcpu->arch.tsc_catchup = 1;
 		}
@@ -4161,6 +5722,10 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5405| <<kvm_arch_vcpu_put>> kvm_steal_time_set_preempted(vcpu);
+ */
 static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 {
 	struct kvm_host_map map;
@@ -4184,6 +5749,11 @@ static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 	kvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, true);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|219| <<vcpu_put>> kvm_arch_vcpu_put(vcpu);
+ *   - virt/kvm/kvm_main.c|5115| <<kvm_sched_out>> kvm_arch_vcpu_put(vcpu);
+ */
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	int idx;
@@ -4212,6 +5782,10 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	set_debugreg(0, 6);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6051| <<kvm_arch_vcpu_ioctl(KVM_GET_LAPIC)>> r = kvm_vcpu_ioctl_get_lapic(vcpu, u.lapic);
+ */
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
@@ -5906,6 +7480,16 @@ long kvm_arch_vm_ioctl(struct file *filp,
 			now_ns = ka->master_kernel_ns;
 		else
 			now_ns = get_kvmclock_base_ns();
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+		 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		ka->kvmclock_offset = user_ns.clock - now_ns;
 		spin_unlock_irq(&ka->pvclock_gtod_sync_lock);
 
@@ -7277,6 +8861,11 @@ void kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip)
 }
 EXPORT_SYMBOL_GPL(kvm_inject_realmode_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7663| <<x86_emulate_instruction>> return handle_emulation_failure(vcpu, emulation_type);
+ *   - arch/x86/kvm/x86.c|7720| <<x86_emulate_instruction>> return handle_emulation_failure(vcpu, emulation_type);
+ */
 static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 {
 	++vcpu->stat.insn_emulation_fail;
@@ -7287,6 +8876,14 @@ static int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用EMULTYPE_SKIP:
+	 *   - arch/x86/kvm/svm/svm.c|347| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+	 *   - arch/x86/kvm/vmx/vmx.c|1816| <<skip_emulated_instruction>> if (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))
+	 *   - arch/x86/kvm/x86.c|7315| <<handle_emulation_failure>> if (emulation_type & EMULTYPE_SKIP) {
+	 *   - arch/x86/kvm/x86.c|7610| <<x86_decode_emulated_instruction>> if (!(emulation_type & EMULTYPE_SKIP) &&
+	 *   - arch/x86/kvm/x86.c|7683| <<x86_emulate_instruction>> if (emulation_type & EMULTYPE_SKIP) {
+	 */
 	if (emulation_type & EMULTYPE_SKIP) {
 		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
 		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
@@ -7595,6 +9192,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5149| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|7783| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|7790| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -7865,6 +9468,10 @@ int kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in)
 }
 EXPORT_SYMBOL_GPL(kvm_fast_pio);
 
+/*
+ * 在以下使用kvmclock_cpu_down_prep():
+ *   - arch/x86/kvm/x86.c|9384| <<kvm_timer_init>> cpuhp_setup_state(CPUHP_AP_X86_KVM_CLK_ONLINE, "x86/kvm/clk:online", kvmclock_cpu_online, kvmclock_cpu_down_prep);
+ */
 static int kvmclock_cpu_down_prep(unsigned int cpu)
 {
 	__this_cpu_write(cpu_tsc_khz, 0);
@@ -8095,6 +9702,15 @@ static struct perf_guest_info_callbacks kvm_guest_cbs = {
 };
 
 #ifdef CONFIG_X86_64
+/*
+ * 在一下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|8628| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|8637| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|8829| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+ *
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|8385| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
@@ -8103,9 +9719,25 @@ static void pvclock_gtod_update_fn(struct work_struct *work)
 	int i;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 * 然后为每个vcpu触发KVM_REQ_CLOCK_UPDATE调用kvm_guest_time_update(),
+	 * 似乎用来填充"struct pvclock_vcpu_time_info"
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
@@ -8119,20 +9751,88 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
+	/*
+	 * 在一下使用pvclock_gtod_work:
+	 *   - arch/x86/kvm/x86.c|8628| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+	 *   - arch/x86/kvm/x86.c|8637| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+	 *   - arch/x86/kvm/x86.c|8829| <<kvm_arch_exit>> cancel_work_sync(&pvclock_gtod_work);
+	 *
+	 * 在以下使用pvclock_gtod_update_fn():
+	 *   - arch/x86/kvm/x86.c|8385| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+	 */
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|8640| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|8710| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|8828| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * 4.14上的例子
+ * pvclock_gtod_notify
+ * raw_notifier_call_chain
+ * timekeeping_update
+ * update_wall_time
+ * tick_do_update_jiffies64.part.13
+ * tick_sched_do_timer
+ * tick_sched_timer
+ * __hrtimer_run_queues
+ * hrtimer_interrupt
+ * smp_apic_timer_interrupt
+ * apic_timer_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * call_cpuidle
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * secondary_startup_64
+ *
+ * kvm通过pvclock_gtod_register_notifier向timekeeper层注册了一个回调pvclock_gtod_notify,
+ * 每当Host Kernel时钟更新时(即timekeeping_update被调用时),就会调用pvclock_gtod_notify.
+ *
+ * 参数unused不使用
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
+	/*
+	 * struct pvclock_gtod_data {
+	 *     seqcount_t      seq;
+	 *
+	 *     struct pvclock_clock clock; // extract of a clocksource struct
+	 *     struct pvclock_clock raw_clock; // extract of a clocksource struct
+	 *
+	 *     ktime_t         offs_boot;
+	 *     u64             wall_time_sec;
+	 * };
+	 *
+	 * 在以下使用pvclock_gtod_data:
+	 *   - arch/x86/kvm/x86.c|2024| <<update_pvclock_gtod>> struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2055| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+	 *   - arch/x86/kvm/x86.c|2275| <<kvm_track_tsc_matching>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2353| <<kvm_check_tsc_unstable>> if (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)
+	 *   - arch/x86/kvm/x86.c|2478| <<read_tsc>> u64 last = pvclock_gtod_data.clock.cycle_last;
+	 *   - arch/x86/kvm/x86.c|2533| <<do_monotonic_raw>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2552| <<do_realtime>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 *   - arch/x86/kvm/x86.c|2575| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+	 *   - arch/x86/kvm/x86.c|2587| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+	 *   - arch/x86/kvm/x86.c|2660| <<pvclock_update_vm_gtod_copy>> vclock_mode = pvclock_gtod_data.clock.vclock_mode;
+	 *   - arch/x86/kvm/x86.c|8172| <<pvclock_gtod_notify>> struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
+	 */
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 	struct timekeeper *tk = priv;
 
+	/*
+	 * 只在此处调用
+	 */
 	update_pvclock_gtod(tk);
 
 	/*
@@ -8140,12 +9840,36 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|8640| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|8710| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|8828| <<kvm_arch_exit>> irq_work_sync(&pvclock_irq_work);
+	 *
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2679| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|8169| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|8204| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 *
+	 * 根据注释,若clocksource不是TSC,但全局变量kvm_guest_has_master_clock非零,
+	 * 说明clocksource从TSC变为了非TSC,此时向所有vCPU发送KVM_REQ_MASTER_CLOCK_UPDATE,
+	 * 然后令kvm_guest_has_master_clock = 0.
+	 *
+	 * 也就是说要同时满足两个条件
+	 * 1. !gtod_is_based_on_tsc(gtod->clock.vclock_mode) --> host的clock不是tsc
+	 * 2. atomic_read(&kvm_guest_has_master_clock) != 0  --> kvm_guest_has_master_clock不是0
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
 	return 0;
 }
 
+/*
+ * 在以下使用pvclock_gtod_notifier:
+ *   - arch/x86/kvm/x86.c|8518| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|8550| <<kvm_arch_exit>> pvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);
+ */
 static struct notifier_block pvclock_gtod_notifier = {
 	.notifier_call = pvclock_gtod_notify,
 };
@@ -8307,6 +10031,10 @@ int kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9909| <<kvm_emulate_hypercall(KVM_HC_CLOCK_PAIRING)>> ret = kvm_pv_clock_pairing(vcpu, a0, a1);
+ */
 static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 			        unsigned long clock_type)
 {
@@ -9320,6 +11048,11 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (kvm_lapic_enabled(vcpu) && vcpu->arch.apicv_active)
 		static_call(kvm_x86_sync_pir_to_irr)(vcpu);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10574| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+	 */
 	if (kvm_vcpu_exit_request(vcpu)) {
 		vcpu->mode = OUTSIDE_GUEST_MODE;
 		smp_wmb();
@@ -9354,6 +11087,11 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (likely(exit_fastpath != EXIT_FASTPATH_REENTER_GUEST))
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|10540| <<vcpu_enter_guest>> if (kvm_vcpu_exit_request(vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10574| <<vcpu_enter_guest>> if (unlikely(kvm_vcpu_exit_request(vcpu))) {
+		 */
                 if (unlikely(kvm_vcpu_exit_request(vcpu))) {
 			exit_fastpath = EXIT_FASTPATH_EXIT_HANDLED;
 			break;
@@ -10403,6 +12141,12 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3414| <<kvm_vm_ioctl_create_vcpu>> kvm_arch_vcpu_postcreate(vcpu);
+ *
+ * 大概是创建vcpu到最后的时候调用
+ */
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -10749,7 +12493,24 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	mutex_init(&kvm->arch.apic_map_lock);
 	spin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|348| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3076| <<get_kvmclock_ns>> return get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3080| <<get_kvmclock_ns>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3092| <<get_kvmclock_ns>> ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3294| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|6436| <<kvm_arch_vm_ioctl>> ka->kvmclock_offset = user_ns.clock - now_ns;
+	 *   - arch/x86/kvm/x86.c|11421| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	/*
+	 * 核心思想是为整个kvm更新:
+	 *   - ka->use_master_clock
+	 *   - ka->master_kernel_ns
+	 *   - ka->master_cycle_now
+	 *   - kvm_guest_has_master_clock
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 
 	kvm->arch.guest_can_read_msr_platform_info = true;
@@ -11248,6 +13009,13 @@ bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
 	return vcpu->arch.preempted_in_kernel;
 }
 
+/*
+ * 注释Note, the vCPU could get migrated to a different pCPU at any point
+ * after kvm_arch_vcpu_should_kick(), which could result in sending an
+ * IPI to the previous pCPU.  But, that's ok because the purpose of the
+ * IPI is to force the vCPU to leave IN_GUEST_MODE, and migrating the
+ * vCPU also requires it to leave IN_GUEST_MODE.
+ */
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c
index aa9f50fccc5d..773eeb1f002f 100644
--- a/arch/x86/xen/enlighten.c
+++ b/arch/x86/xen/enlighten.c
@@ -38,6 +38,31 @@ EXPORT_SYMBOL_GPL(hypercall_page);
  * hypercall.
  *
  */
+/*
+ * 在以下使用xen_vcpu:
+ *   - arch/x86/xen/enlighten.c|41| <<global>> DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - include/xen/xen-ops.h|12| <<global>> DECLARE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - rch/x86/xen/enlighten.c|198| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
+ *   - arch/x86/xen/enlighten.c|202| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = NULL;
+ *   - arch/x86/xen/enlighten.c|233| <<xen_vcpu_setup>> if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+ *   - arch/x86/xen/enlighten.c|264| <<xen_vcpu_setup>> per_cpu(xen_vcpu, cpu) = vcpup;
+ *   - arch/x86/xen/enlighten.c|271| <<xen_vcpu_setup>> return ((per_cpu(xen_vcpu, cpu) == NULL) ? -ENODEV : 0);
+ *   - arch/x86/xen/enlighten_pv.c|1436| <<xen_cpu_up_prepare_pv>> if (per_cpu(xen_vcpu, cpu) == NULL)
+ *   - arch/x86/xen/irq.c|32| <<xen_save_fl>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/irq.c|51| <<xen_irq_disable>> this_cpu_read(xen_vcpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/irq.c|67| <<xen_irq_enable>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/mmu_pv.c|1209| <<xen_write_cr2>> this_cpu_read(xen_vcpu)->arch.cr2 = cr2;
+ *   - arch/x86/xen/smp_pv.c|365| <<xen_pv_cpu_up>> per_cpu(xen_vcpu, cpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/time.c|52| <<xen_clocksource_read>> src = &__this_cpu_read(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|74| <<xen_read_wallclock>> vcpu_time = &get_cpu_var(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|76| <<xen_read_wallclock>> put_cpu_var(xen_vcpu);
+ *   - arch/x86/xen/time.c|507| <<xen_time_init>> pvti = &__this_cpu_read(xen_vcpu)->time;
+ *   - drivers/xen/events/events_2l.c|123| <<evtchn_2l_unmask>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|173| <<evtchn_2l_handle_events>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|280| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, i);
+ *   - drivers/xen/events/events_2l.c|289| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, cpu);
+ *   - drivers/xen/events/events_base.c|1699| <<__xen_evtchn_do_upcall>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ */
 DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
 
 /*
@@ -45,6 +70,13 @@ DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
  * hypercall. This can be used both in PV and PVHVM mode. The structure
  * overrides the default per_cpu(xen_vcpu, cpu) value.
  */
+/*
+ * 在以下使用xen_vcpu_info:
+ *   - arch/x86/xen/enlighten.c|48| <<global>> DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
+ *   - arch/x86/xen/xen-ops.h|24| <<global>> DECLARE_PER_CPU(struct vcpu_info, xen_vcpu_info);
+ *   - arch/x86/xen/enlighten.c|219| <<xen_vcpu_setup>> if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+ *   - arch/x86/xen/enlighten.c|224| <<xen_vcpu_setup>> vcpup = &per_cpu(xen_vcpu_info, cpu);
+ */
 DEFINE_PER_CPU(struct vcpu_info, xen_vcpu_info);
 
 /* Linux <-> Xen vCPU id mapping */
@@ -196,6 +228,13 @@ void xen_vcpu_info_reset(int cpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/enlighten.c|136| <<xen_vcpu_setup_restore>> rc = xen_vcpu_setup(cpu);
+ *   - arch/x86/xen/enlighten_hvm.c|166| <<xen_cpu_up_prepare_hvm>> rc = xen_vcpu_setup(cpu);
+ *   - arch/x86/xen/enlighten_pv.c|1031| <<xen_setup_vcpu_info_placement>> (void ) xen_vcpu_setup(cpu);
+ *   - arch/x86/xen/smp_hvm.c|20| <<xen_hvm_smp_prepare_boot_cpu>> xen_vcpu_setup(0);
+ */
 int xen_vcpu_setup(int cpu)
 {
 	struct vcpu_register_vcpu_info info;
@@ -235,6 +274,11 @@ int xen_vcpu_setup(int cpu)
 		 * hypercall does not allow to over-write info.mfn and
 		 * info.offset.
 		 */
+		/*
+		 * 在以下使用VCPUOP_register_vcpu_info:
+		 *   - arch/arm/xen/enlighten.c|156| <<xen_starting_cpu>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
+		 *   - arch/x86/xen/enlighten.c|252| <<xen_vcpu_setup>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
+		 */
 		err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
 					 xen_vcpu_nr(cpu), &info);
 
diff --git a/arch/x86/xen/time.c b/arch/x86/xen/time.c
index d9c945ee1100..2f8ad086520c 100644
--- a/arch/x86/xen/time.c
+++ b/arch/x86/xen/time.c
@@ -31,6 +31,13 @@
 /* Minimum amount of time until next clock event fires */
 #define TIMER_SLOP	100000
 
+/*
+ * 在以下使用xen_sched_clock_offset:
+ *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+ *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+ */
 static u64 xen_sched_clock_offset __read_mostly;
 
 /* Get the TSC speed from Xen */
@@ -62,6 +69,13 @@ static u64 xen_clocksource_get_cycles(struct clocksource *cs)
 
 static u64 xen_sched_clock(void)
 {
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 */
 	return xen_clocksource_read() - xen_sched_clock_offset;
 }
 
@@ -520,8 +534,20 @@ static void __init xen_time_init(void)
 		pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/time.c|556| <<xen_init_time_ops>> xen_init_time_common();
+ *   - arch/x86/xen/time.c|599| <<xen_hvm_init_time_ops>> xen_init_time_common();
+ */
 static void __init xen_init_time_common(void)
 {
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|65| <<xen_sched_clock>> return xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|390| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|433| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|525| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 */ 
 	xen_sched_clock_offset = xen_clocksource_read();
 	static_call_update(pv_steal_clock, xen_steal_clock);
 	paravirt_set_sched_clock(xen_sched_clock);
@@ -556,6 +582,10 @@ static void xen_hvm_setup_cpu_clockevents(void)
 	xen_setup_cpu_clockevents();
 }
 
+/*
+ * called by:
+ *   - arch/x86/xen/enlighten_hvm.c|219| <<xen_hvm_guest_init>> xen_hvm_init_time_ops();
+ */
 void __init xen_hvm_init_time_ops(void)
 {
 	/*
diff --git a/drivers/acpi/acpica/nseval.c b/drivers/acpi/acpica/nseval.c
index 63748ac699f7..fe26edf54aab 100644
--- a/drivers/acpi/acpica/nseval.c
+++ b/drivers/acpi/acpica/nseval.c
@@ -39,6 +39,18 @@ ACPI_MODULE_NAME("nseval")
  * MUTEX:       Locks interpreter
  *
  ******************************************************************************/
+/*
+ * called by:
+ *   - drivers/acpi/acpica/evgpe.c|506| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/evregion.c|630| <<acpi_ev_execute_reg_method>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/hwxface.c|362| <<acpi_get_sleep_type_data>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/nsinit.c|156| <<acpi_ns_initialize_devices>> status = acpi_ns_evaluate(info.evaluate_info);
+ *   - drivers/acpi/acpica/nsinit.c|176| <<acpi_ns_initialize_devices>> status = acpi_ns_evaluate(info.evaluate_info);
+ *   - drivers/acpi/acpica/nsinit.c|645| <<acpi_ns_init_one_device>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/nsxfeval.c|354| <<acpi_evaluate_object>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/rsutils.c|746| <<acpi_rs_set_srs_method_data>> status = acpi_ns_evaluate(info);
+ *   - drivers/acpi/acpica/uteval.c|60| <<acpi_ut_evaluate_object>> status = acpi_ns_evaluate(info);
+ */
 acpi_status acpi_ns_evaluate(struct acpi_evaluate_info *info)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/psparse.c b/drivers/acpi/acpica/psparse.c
index 7eb7a81619a3..ef8e5d37ae0d 100644
--- a/drivers/acpi/acpica/psparse.c
+++ b/drivers/acpi/acpica/psparse.c
@@ -405,6 +405,99 @@ acpi_ps_next_parse_state(struct acpi_walk_state *walk_state,
  *
  ******************************************************************************/
 
+/*
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ut_evaluate_object
+ * [0] acpi_rs_get_prt_method_data
+ * [0] acpi_get_irq_routing_table
+ * [0] acpi_pci_irq_find_prt_entry
+ * [0] acpi_pci_irq_lookup
+ * [0] acpi_pci_irq_enable
+ * [0] do_pci_enable_device
+ * [0] pci_enable_device_flags
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_ev_asynch_execute_gpe_method
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_bus_get_status
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_bus_get_status
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_integer
+ * [0] acpi_processor_add
+ * [0] acpi_bus_attach
+ * [0] acpi_bus_scan
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] acpi_ps_parse_aml
+ * [0] acpi_ps_execute_method
+ * [0] acpi_ns_evaluate
+ * [0] acpi_evaluate_object
+ * [0] acpi_evaluate_ost
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/acpica/dbmethod.c|325| <<acpi_db_disassemble_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dbutils.c|366| <<acpi_db_second_pass_parse>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsargs.c|86| <<acpi_ds_execute_arguments>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsargs.c|125| <<acpi_ds_execute_arguments>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/dsmethod.c|100| <<acpi_ds_auto_serialize_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/nsparse.c|228| <<acpi_ns_one_complete_parse>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/psxface.c|190| <<acpi_ps_execute_method>> status = acpi_ps_parse_aml(walk_state);
+ *   - drivers/acpi/acpica/psxface.c|295| <<acpi_ps_execute_table>> status = acpi_ps_parse_aml(walk_state);
+ */
 acpi_status acpi_ps_parse_aml(struct acpi_walk_state *walk_state)
 {
 	acpi_status status;
diff --git a/drivers/acpi/acpica/psxface.c b/drivers/acpi/acpica/psxface.c
index fd0f28c7af1e..3cc77b619493 100644
--- a/drivers/acpi/acpica/psxface.c
+++ b/drivers/acpi/acpica/psxface.c
@@ -81,6 +81,10 @@ acpi_debug_trace(const char *name, u32 debug_level, u32 debug_layer, u32 flags)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/nseval.c|205| <<acpi_ns_evaluate>> status = acpi_ps_execute_method(info);
+ */
 acpi_status acpi_ps_execute_method(struct acpi_evaluate_info *info)
 {
 	acpi_status status;
diff --git a/drivers/acpi/osl.c b/drivers/acpi/osl.c
index 327e1b4eb6b0..4a24d38dae0d 100644
--- a/drivers/acpi/osl.c
+++ b/drivers/acpi/osl.c
@@ -1058,6 +1058,18 @@ int __init acpi_debugger_init(void)
  *
  ******************************************************************************/
 
+/*
+ * called by:
+ *   - drivers/acpi/acpica/dbexec.c|695| <<acpi_db_create_execution_thread>> status = acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbexec.c|849| <<acpi_db_create_execution_threads>> acpi_os_execute(OSL_DEBUGGER_EXEC_THREAD,
+ *   - drivers/acpi/acpica/dbxface.c|448| <<acpi_initialize_debugger>> status = acpi_os_execute(OSL_DEBUGGER_MAIN_THREAD,
+ *   - drivers/acpi/acpica/evgpe.c|526| <<acpi_ev_asynch_execute_gpe_method>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/acpica/evgpe.c|823| <<acpi_ev_gpe_dispatch>> status = acpi_os_execute(OSL_GPE_HANDLER,
+ *   - drivers/acpi/acpica/evmisc.c|139| <<acpi_ev_queue_notify_request>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/acpi/bus.c|516| <<acpi_device_fixed_event>> acpi_os_execute(OSL_NOTIFY_HANDLER, acpi_device_notify_fixed, data);
+ *   - drivers/acpi/sbshc.c|232| <<smbus_alarm>> acpi_os_execute(OSL_NOTIFY_HANDLER,
+ *   - drivers/platform/x86/dell/dell-rbtn.c|278| <<rbtn_resume>> status = acpi_os_execute(OSL_NOTIFY_HANDLER,
+ */
 acpi_status acpi_os_execute(acpi_execute_type type,
 			    acpi_osd_exec_callback function, void *context)
 {
@@ -1152,6 +1164,10 @@ struct acpi_hp_work {
 	u32 src;
 };
 
+/*
+ * 在以下使用acpi_hotplug_work_fn():
+ *   - drivers/acpi/osl.c|1176| <<acpi_hotplug_schedule>> INIT_WORK(&hpw->work, acpi_hotplug_work_fn);
+ */
 static void acpi_hotplug_work_fn(struct work_struct *work)
 {
 	struct acpi_hp_work *hpw = container_of(work, struct acpi_hp_work, work);
@@ -1161,6 +1177,20 @@ static void acpi_hotplug_work_fn(struct work_struct *work)
 	kfree(hpw);
 }
 
+/*
+ * [0] acpi_hotplug_schedule
+ * [0] acpi_bus_notify
+ * [0] acpi_ev_notify_dispatch
+ * [0] acpi_os_execute_deferred
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/acpi/bus.c|490| <<acpi_bus_notify>> if (ACPI_SUCCESS(acpi_hotplug_schedule(adev, type)))
+ *   - drivers/acpi/device_sysfs.c|381| <<eject_store>> status = acpi_hotplug_schedule(acpi_device, ACPI_OST_EC_OSPM_EJECT);
+ */
 acpi_status acpi_hotplug_schedule(struct acpi_device *adev, u32 src)
 {
 	struct acpi_hp_work *hpw;
diff --git a/drivers/acpi/scan.c b/drivers/acpi/scan.c
index e10d38ac7cf2..f8da0cdcc45a 100644
--- a/drivers/acpi/scan.c
+++ b/drivers/acpi/scan.c
@@ -321,6 +321,12 @@ static int acpi_scan_device_check(struct acpi_device *adev)
 			dev_warn(&adev->dev, "Already enumerated\n");
 			return -EALREADY;
 		}
+		/*
+		 * cpu hotplug会跑到这里
+		 * 在这里加一个while(1)就不会触发下面的event了
+		 * {"timestamp": {"seconds": 1628873592, "microseconds": 941033}, "event": "ACPI_DEVICE_OST",
+		 * "data": {"info": {"device": "core1", "source": 1, "status": 0, "slot": "2", "slot-type": "CPU"}}}
+		 */
 		error = acpi_bus_scan(adev->handle);
 		if (error) {
 			dev_warn(&adev->dev, "Namespace scan failure\n");
@@ -363,6 +369,10 @@ static int acpi_scan_bus_check(struct acpi_device *adev)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/scan.c|419| <<acpi_device_hotplug>> error = acpi_generic_hotplug_event(adev, src);
+ */
 static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 {
 	switch (type) {
@@ -383,6 +393,32 @@ static int acpi_generic_hotplug_event(struct acpi_device *adev, u32 type)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/acpi/osl.c|1160| <<acpi_hotplug_work_fn>> acpi_device_hotplug(hpw->adev, hpw->src);
+ *
+ * (qemu) device_add host-x86_64-cpu,id=core1,socket-id=1,core-id=0,thread-id=0
+ *
+ * Comm: kworker/u8:0
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn0
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * (qemu) device_del core1
+ *
+ * Comm: kworker/u8:3
+ * Workqueue: kacpi_hotplug acpi_hotplug_work_fn
+ * [0] acpi_device_hotplug
+ * [0] acpi_hotplug_work_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 void acpi_device_hotplug(struct acpi_device *adev, u32 src)
 {
 	u32 ost_code = ACPI_OST_SC_NON_SPECIFIC_FAILURE;
diff --git a/drivers/acpi/utils.c b/drivers/acpi/utils.c
index 3b54b8fd7396..64ef25517fb3 100644
--- a/drivers/acpi/utils.c
+++ b/drivers/acpi/utils.c
@@ -404,6 +404,20 @@ EXPORT_SYMBOL(acpi_get_physical_device_location);
  * must call this function when evaluating _OST for hotplug operations.
  * When the platform does not support _OST, this function has no effect.
  */
+/*
+ * https://zhuanlan.zhihu.com/p/113296734
+ *
+ * called by:
+ *   - drivers/acpi/acpi_pad.c|404| <<acpi_pad_handle_notify>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_AGGREGATOR_NOTIFY, 0, &param);
+ *   - drivers/acpi/bus.c|496| <<acpi_bus_notify>> acpi_evaluate_ost(handle, type, ost_code, NULL);
+ *   - drivers/acpi/bus.c|577| <<sb_notify_work>> acpi_evaluate_ost(sb_handle, ACPI_OST_EC_OSPM_SHUTDOWN,
+ *   - drivers/acpi/device_sysfs.c|386| <<eject_store>> acpi_evaluate_ost(acpi_device->handle, ACPI_OST_EC_OSPM_EJECT,
+ *   - drivers/acpi/processor_perflib.c|105| <<acpi_processor_ppc_ost>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_NOTIFY_PERFORMANCE,
+ *   - drivers/acpi/scan.c|389| <<acpi_generic_hotplug_event>> acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_EJECT_REQUEST,
+ *   - drivers/acpi/scan.c|455| <<acpi_device_hotplug>> acpi_evaluate_ost(adev->handle, src, ost_code, NULL);
+ *   - drivers/pci/pcie/edr.c|137| <<acpi_send_edr_status>> status = acpi_evaluate_ost(adev->handle, ACPI_NOTIFY_DISCONNECT_RECOVER,
+ *   - drivers/xen/xen-acpi-pad.c|92| <<acpi_pad_handle_notify>> acpi_evaluate_ost(handle, ACPI_PROCESSOR_AGGREGATOR_NOTIFY,
+ */
 acpi_status
 acpi_evaluate_ost(acpi_handle handle, u32 source_event, u32 status_code,
 		  struct acpi_buffer *status_buf)
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index f7e83613ae94..57637c9de6cb 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -10,6 +10,13 @@
 
 #define POLL_IDLE_RELAX_COUNT	200
 
+/*
+ * called by:
+ *   - drivers/cpuidle/cpuidle.c|237| <<cpuidle_enter_state>> entered_state = target_state->enter(dev, drv, index);
+ *
+ * 在以下使用poll_idle():
+ *   - drivers/cpuidle/poll_state.c|55| <<cpuidle_poll_state_init>> state->enter = poll_idle;
+ */
 static int __cpuidle poll_idle(struct cpuidle_device *dev,
 			       struct cpuidle_driver *drv, int index)
 {
diff --git a/drivers/iommu/intel/dmar.c b/drivers/iommu/intel/dmar.c
index 84057cb9596c..eed94b655907 100644
--- a/drivers/iommu/intel/dmar.c
+++ b/drivers/iommu/intel/dmar.c
@@ -56,6 +56,21 @@ struct dmar_res_callback {
  * 2) Use RCU in interrupt context
  */
 DECLARE_RWSEM(dmar_global_lock);
+/*
+ * 在以下使用dmar_drhd_units:
+ *   - drivers/iommu/intel/cap_audit.c|149| <<cap_audit_static>> if (list_empty(&dmar_drhd_units))
+ *   - drivers/iommu/intel/dmar.c|77| <<dmar_register_drhd_unit>> list_add_tail_rcu(&drhd->list, &dmar_drhd_units);
+ *   - drivers/iommu/intel/dmar.c|79| <<dmar_register_drhd_unit>> list_add_rcu(&drhd->list, &dmar_drhd_units);
+ *   - drivers/iommu/intel/dmar.c|397| <<dmar_find_dmaru>> list_for_each_entry_rcu(dmaru, &dmar_drhd_units, list,
+ *   - drivers/iommu/intel/dmar.c|812| <<dmar_dev_scope_init>> if (list_empty(&dmar_drhd_units)) {
+ *   - drivers/iommu/intel/dmar.c|853| <<dmar_table_init>> } else if (list_empty(&dmar_drhd_units)) {
+ *   - drivers/iommu/intel/dmar.c|2127| <<dmar_free_unused_resources>> if (dmar_dev_scope_status != 1 && !list_empty(&dmar_drhd_units))
+ *   - drivers/iommu/intel/dmar.c|2131| <<dmar_free_unused_resources>> list_for_each_entry_safe(dmaru, dmaru_n, &dmar_drhd_units, list) {
+ *   - include/linux/dmar.h|74| <<for_each_drhd_unit>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|78| <<for_each_active_drhd_unit>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|83| <<for_each_active_iommu>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ *   - include/linux/dmar.h|88| <<for_each_iommu>> list_for_each_entry_rcu(drhd, &dmar_drhd_units, list, \
+ */
 LIST_HEAD(dmar_drhd_units);
 
 struct acpi_table_header * __initdata dmar_tbl;
@@ -1819,6 +1834,10 @@ static const char *irq_remap_fault_reasons[] =
 	"Blocked an interrupt request due to source-id verification failure",
 };
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/dmar.c|1913| <<dmar_fault_do_one>> reason = dmar_get_fault_reason(fault_reason, &fault_type);
+ */
 static const char *dmar_get_fault_reason(u8 fault_reason, int *fault_type)
 {
 	if (fault_reason >= 0x20 && (fault_reason - 0x20 <
@@ -1903,6 +1922,10 @@ void dmar_msi_read(int irq, struct msi_msg *msg)
 	raw_spin_unlock_irqrestore(&iommu->register_lock, flag);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/dmar.c|1991| <<dmar_fault>> dmar_fault_do_one(iommu, type, fault_reason,
+ */
 static int dmar_fault_do_one(struct intel_iommu *iommu, int type,
 		u8 fault_reason, u32 pasid, u16 source_id,
 		unsigned long long addr)
@@ -1927,6 +1950,13 @@ static int dmar_fault_do_one(struct intel_iommu *iommu, int type,
 }
 
 #define PRIMARY_FAULT_REG_LEN (16)
+/*
+ * 在以下使用dmar_fault():
+ *   - drivers/iommu/intel/dmar.c|2027| <<dmar_set_interrupt>> ret = request_irq(irq, dmar_fault, IRQF_NO_THREAD, iommu->name, iommu);
+ *   - drivers/iommu/intel/dmar.c|2054| <<enable_drhd_fault_handling>> dmar_fault(iommu->irq, iommu);
+ *   - drivers/iommu/intel/iommu.c|2905| <<intel_iommu_init_qi>> dmar_fault(-1, iommu);
+ *   - drivers/iommu/intel/irq_remapping.c|593| <<intel_setup_irq_remapping>> dmar_fault(-1, iommu);
+ */
 irqreturn_t dmar_fault(int irq, void *dev_id)
 {
 	struct intel_iommu *iommu = dev_id;
diff --git a/drivers/iommu/intel/irq_remapping.c b/drivers/iommu/intel/irq_remapping.c
index f912fe45bea2..8c37bc33d00e 100644
--- a/drivers/iommu/intel/irq_remapping.c
+++ b/drivers/iommu/intel/irq_remapping.c
@@ -470,6 +470,11 @@ static int iommu_load_old_irte(struct intel_iommu *iommu)
 }
 
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|618| <<intel_setup_irq_remapping>> iommu_set_irq_remapping(iommu, eim_mode);
+ *   - drivers/iommu/intel/irq_remapping.c|1072| <<reenable_irq_remapping>> iommu_set_irq_remapping(iommu, eim);
+ */
 static void iommu_set_irq_remapping(struct intel_iommu *iommu, int mode)
 {
 	unsigned long flags;
@@ -1104,6 +1109,10 @@ void intel_irq_remap_add_device(struct dmar_pci_notify_info *info)
 	dev_set_msi_domain(&info->dev->dev, map_dev_to_ir(info->dev));
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1285| <<intel_irq_remapping_prepare_irte>> prepare_irte(irte, irq_cfg->vector, irq_cfg->dest_apicid);
+ */
 static void prepare_irte(struct irte *irte, int vector, unsigned int dest)
 {
 	memset(irte, 0, sizeof(*irte));
@@ -1187,6 +1196,23 @@ intel_ir_set_affinity(struct irq_data *data, const struct cpumask *mask,
 	return IRQ_SET_MASK_OK_DONE;
 }
 
+/*
+ * [0] intel_ir_compose_msi_msg
+ * [0] irq_chip_compose_msi_msg
+ * [0] msi_domain_activate
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] irq_startup
+ * [0] __setup_irq
+ * [0] request_threaded_irq
+ * [0] pci_request_irq
+ * [0] queue_request_irq
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void intel_ir_compose_msi_msg(struct irq_data *irq_data,
 				     struct msi_msg *msg)
 {
@@ -1254,6 +1280,23 @@ static void fill_msi_msg(struct msi_msg *msg, u32 index, u32 subhandle)
 	msg->arch_data.dmar_subhandle = subhandle;
 }
 
+/*
+ * [0] intel_irq_remapping_alloc
+ * [0] intel_irq_remapping_alloc
+ * [0] msi_domain_alloc
+ * [0] __irq_domain_alloc_irqs
+ * [0] __msi_domain_alloc_irqs
+ * [0] __pci_enable_msix_range
+ * [0] pci_alloc_irq_vectors_affinity
+ * [0] nvme_reset_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/iommu/intel/irq_remapping.c|1381| <<intel_irq_remapping_alloc>> intel_irq_remapping_prepare_irte(ird, irq_cfg, info, index, i);
+ */
 static void intel_irq_remapping_prepare_irte(struct intel_ir_data *data,
 					     struct irq_cfg *irq_cfg,
 					     struct irq_alloc_info *info,
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index 808ab70d5df5..96a182d0556a 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -26,6 +26,12 @@
 #include <trace/events/iommu.h>
 
 static struct kset *iommu_group_kset;
+/*
+ * 在以下使用iommu_group_ida:
+ *   - drivers/iommu/iommu.c|597| <<iommu_group_release>> ida_simple_remove(&iommu_group_ida, group->id);
+ *   - drivers/iommu/iommu.c|653| <<iommu_group_alloc>> ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
+ *   - drivers/iommu/iommu.c|663| <<iommu_group_alloc>> ida_simple_remove(&iommu_group_ida, group->id);
+ */
 static DEFINE_IDA(iommu_group_ida);
 
 static unsigned int iommu_def_domain_type __read_mostly;
@@ -150,6 +156,29 @@ subsys_initcall(iommu_subsys_init);
  *
  * Return: 0 on success, or an error.
  */
+/*
+ * called by:
+ *   - drivers/iommu/amd/init.c|1891| <<iommu_init_pci>> iommu_device_register(&iommu->iommu, &amd_iommu_ops, NULL);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c|3669| <<arm_smmu_device_probe>> ret = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);
+ *   - drivers/iommu/arm/arm-smmu/arm-smmu.c|2164| <<arm_smmu_device_probe>> err = iommu_device_register(&smmu->iommu, &arm_smmu_ops, dev);
+ *   - drivers/iommu/arm/arm-smmu/qcom_iommu.c|850| <<qcom_iommu_device_probe>> ret = iommu_device_register(&qcom_iommu->iommu, &qcom_iommu_ops, dev);
+ *   - drivers/iommu/exynos-iommu.c|633| <<exynos_sysmmu_probe>> ret = iommu_device_register(&data->iommu, &exynos_iommu_ops, dev);
+ *   - drivers/iommu/fsl_pamu_domain.c|477| <<pamu_domain_init>> ret = iommu_device_register(&pamu_iommu, &fsl_pamu_ops, NULL);
+ *   - drivers/iommu/intel/dmar.c|1158| <<alloc_iommu>> err = iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);
+ *   - drivers/iommu/intel/iommu.c|4402| <<intel_iommu_init>> iommu_device_register(&iommu->iommu, &intel_iommu_ops, NULL);
+ *   - drivers/iommu/ipmmu-vmsa.c|1079| <<ipmmu_probe>> ret = iommu_device_register(&mmu->iommu, &ipmmu_ops, &pdev->dev);
+ *   - drivers/iommu/msm_iommu.c|795| <<msm_iommu_probe>> ret = iommu_device_register(&iommu->iommu, &msm_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/mtk_iommu.c|895| <<mtk_iommu_probe>> ret = iommu_device_register(&data->iommu, &mtk_iommu_ops, dev);
+ *   - drivers/iommu/mtk_iommu_v1.c|619| <<mtk_iommu_probe>> ret = iommu_device_register(&data->iommu, &mtk_iommu_ops, dev);
+ *   - drivers/iommu/omap-iommu.c|1238| <<omap_iommu_probe>> err = iommu_device_register(&obj->iommu, &omap_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/rockchip-iommu.c|1199| <<rk_iommu_probe>> err = iommu_device_register(&iommu->iommu, &rk_iommu_ops, dev);
+ *   - drivers/iommu/s390-iommu.c|336| <<zpci_init_iommu>> rc = iommu_device_register(&zdev->iommu_dev, &s390_iommu_ops, NULL);
+ *   - drivers/iommu/sprd-iommu.c|511| <<sprd_iommu_probe>> ret = iommu_device_register(&sdev->iommu, &sprd_iommu_ops, dev);
+ *   - drivers/iommu/sun50i-iommu.c|971| <<sun50i_iommu_probe>> ret = iommu_device_register(&iommu->iommu, &sun50i_iommu_ops, &pdev->dev);
+ *   - drivers/iommu/tegra-gart.c|356| <<tegra_gart_probe>> err = iommu_device_register(&gart->iommu, &gart_iommu_ops, dev);
+ *   - drivers/iommu/tegra-smmu.c|1148| <<tegra_smmu_probe>> err = iommu_device_register(&smmu->iommu, &tegra_smmu_ops, dev);
+ *   - drivers/iommu/virtio-iommu.c|1069| <<viommu_probe>> iommu_device_register(&viommu->iommu, &viommu_ops, parent_dev);
+ */
 int iommu_device_register(struct iommu_device *iommu,
 			  const struct iommu_ops *ops, struct device *hwdev)
 {
@@ -596,6 +625,22 @@ static struct kobj_type iommu_group_ktype = {
  * group to be automatically reclaimed once it has no devices or external
  * references.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|985| <<iommu_register_group>> grp = iommu_group_alloc();
+ *   - drivers/iommu/fsl_pamu_domain.c|342| <<get_device_iommu_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1462| <<generic_device_group>> return iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1540| <<pci_device_group>> return iommu_group_alloc();
+ *   - drivers/iommu/iommu.c|1552| <<fsl_mc_device_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/ipmmu-vmsa.c|879| <<ipmmu_find_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/mtk_iommu.c|609| <<mtk_iommu_device_group>> group = iommu_group_alloc();
+ *   - drivers/iommu/omap-iommu.c|1229| <<omap_iommu_probe>> obj->group = iommu_group_alloc();
+ *   - drivers/iommu/rockchip-iommu.c|1189| <<rk_iommu_probe>> iommu->group = iommu_group_alloc();
+ *   - drivers/iommu/sprd-iommu.c|501| <<sprd_iommu_probe>> sdev->group = iommu_group_alloc();
+ *   - drivers/iommu/sun50i-iommu.c|934| <<sun50i_iommu_probe>> iommu->group = iommu_group_alloc();
+ *   - drivers/vfio/mdev/mdev_driver.c|21| <<mdev_attach_iommu>> group = iommu_group_alloc();
+ *   - drivers/vfio/vfio.c|124| <<vfio_iommu_group_get>> group = iommu_group_alloc();
+ */
 struct iommu_group *iommu_group_alloc(void)
 {
 	struct iommu_group *group;
@@ -611,6 +656,12 @@ struct iommu_group *iommu_group_alloc(void)
 	INIT_LIST_HEAD(&group->entry);
 	BLOCKING_INIT_NOTIFIER_HEAD(&group->notifier);
 
+	/*
+	 * 在以下使用iommu_group_ida:
+	 *   - drivers/iommu/iommu.c|597| <<iommu_group_release>> ida_simple_remove(&iommu_group_ida, group->id);
+	 *   - drivers/iommu/iommu.c|653| <<iommu_group_alloc>> ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
+	 *   - drivers/iommu/iommu.c|663| <<iommu_group_alloc>> ida_simple_remove(&iommu_group_ida, group->id);
+	 */
 	ret = ida_simple_get(&iommu_group_ida, 0, 0, GFP_KERNEL);
 	if (ret < 0) {
 		kfree(group);
@@ -834,6 +885,13 @@ static bool iommu_is_attach_deferred(struct iommu_domain *domain,
  * This function is called by an iommu driver to add a device into a
  * group.  Adding a device increments the group reference count.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kernel/iommu.c|1161| <<iommu_add_device>> return iommu_group_add_device(table_group->group, dev);
+ *   - drivers/iommu/iommu.c|1609| <<iommu_group_get_for_dev>> ret = iommu_group_add_device(group, dev);
+ *   - drivers/vfio/mdev/mdev_driver.c|25| <<mdev_attach_iommu>> ret = iommu_group_add_device(group, &mdev->dev);
+ *   - drivers/vfio/vfio.c|130| <<vfio_iommu_group_get>> ret = iommu_group_add_device(group, dev);
+ */
 int iommu_group_add_device(struct iommu_group *group, struct device *dev)
 {
 	int ret, i = 0;
@@ -890,6 +948,16 @@ int iommu_group_add_device(struct iommu_group *group, struct device *dev)
 
 	trace_add_device_to_group(group->id, dev);
 
+	/*
+	 * [    0.316738] iommu: Default domain type: Translated
+	 * [    0.498945] pci 0000:00:00.0: Adding to iommu group 0
+	 * [    0.499739] pci 0000:00:01.0: Adding to iommu group 1
+	 * [    0.500488] pci 0000:00:02.0: Adding to iommu group 2
+	 * [    0.501261] pci 0000:00:03.0: Adding to iommu group 3
+	 * [    0.502012] pci 0000:00:1f.0: Adding to iommu group 4
+	 * [    0.502787] pci 0000:00:1f.2: Adding to iommu group 4
+	 * [    0.503506] pci 0000:00:1f.3: Adding to iommu group 4
+	 */
 	dev_info(dev, "Adding to iommu group %d\n", group->id);
 
 	return 0;
@@ -1427,6 +1495,16 @@ EXPORT_SYMBOL_GPL(generic_device_group);
  * Use standard PCI bus topology, isolation features, and DMA alias quirks
  * to find or create an IOMMU group for a device.
  */
+/*
+ * called by:
+ *   - drivers/iommu/amd/iommu.c|1738| <<amd_iommu_device_group>> return pci_device_group(dev);
+ *   - drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c|2546| <<arm_smmu_device_group>> group = pci_device_group(dev);
+ *   - drivers/iommu/arm/arm-smmu/arm-smmu.c|1473| <<arm_smmu_device_group>> group = pci_device_group(dev);
+ *   - drivers/iommu/fsl_pamu_domain.c|394| <<get_pci_device_group>> group = pci_device_group(&pdev->dev);
+ *   - drivers/iommu/intel/iommu.c|5290| <<intel_iommu_device_group>> return pci_device_group(dev);
+ *   - drivers/iommu/tegra-smmu.c|929| <<tegra_smmu_device_group>> group->group = pci_device_group(dev);
+ *   - drivers/iommu/virtio-iommu.c|924| <<viommu_device_group>> return pci_device_group(dev);
+ */
 struct iommu_group *pci_device_group(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
diff --git a/drivers/misc/pvpanic/pvpanic.c b/drivers/misc/pvpanic/pvpanic.c
index 65f70a4da8c0..3e299e3f2945 100644
--- a/drivers/misc/pvpanic/pvpanic.c
+++ b/drivers/misc/pvpanic/pvpanic.c
@@ -25,6 +25,13 @@ MODULE_AUTHOR("Mihai Carabas <mihai.carabas@oracle.com>");
 MODULE_DESCRIPTION("pvpanic device driver ");
 MODULE_LICENSE("GPL");
 
+/*
+ * 在以下使用pvpanic_list:
+ *   - drivers/misc/pvpanic/pvpanic.c|37| <<pvpanic_send_event>> list_for_each_entry(pi_cur, &pvpanic_list, list) {
+ *   - drivers/misc/pvpanic/pvpanic.c|69| <<pvpanic_probe>> list_add(&pi->list, &pvpanic_list);
+ *   - drivers/misc/pvpanic/pvpanic.c|84| <<pvpanic_remove>> list_for_each_entry_safe(pi_cur, pi_next, &pvpanic_list, list) {
+ *   - drivers/misc/pvpanic/pvpanic.c|96| <<pvpanic_init>> INIT_LIST_HEAD(&pvpanic_list);
+ */
 static struct list_head pvpanic_list;
 static spinlock_t pvpanic_lock;
 
@@ -60,6 +67,25 @@ static struct notifier_block pvpanic_panic_nb = {
 	.priority = 1, /* let this called before broken drm_fb_helper */
 };
 
+/*
+ * [0] pvpanic_probe
+ * [0] platform_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/misc/pvpanic/pvpanic-mmio.c|109| <<pvpanic_mmio_probe>> return pvpanic_probe(pi);
+ *   - drivers/misc/pvpanic/pvpanic-pci.c|102| <<pvpanic_pci_probe>> return pvpanic_probe(pi);
+ */
 int pvpanic_probe(struct pvpanic_instance *pi)
 {
 	if (!pi || !pi->base)
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 78a01c71a17c..7a4ffd356bb5 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -214,6 +214,19 @@ struct virtnet_info {
 	u8 hdr_len;
 
 	/* Work struct for refilling if we run low on memory. */
+	/*
+	 * 在以下使用virtnet_info->refill:
+	 *   - drivers/net/virtio_net.c|1383| <<refill_work>> container_of(work, struct virtnet_info, refill.work);
+	 *   - drivers/net/virtio_net.c|1398| <<refill_work>> schedule_delayed_work(&vi->refill, HZ/2);
+	 *   - drivers/net/virtio_net.c|1429| <<virtnet_receive>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1552| <<virtnet_open>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1877| <<_virtnet_set_queues>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|1899| <<virtnet_close>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2375| <<virtnet_freeze_down>> cancel_delayed_work_sync(&vi->refill);
+	 *   - drivers/net/virtio_net.c|2401| <<virtnet_restore_up>> schedule_delayed_work(&vi->refill, 0);
+	 *   - drivers/net/virtio_net.c|2883| <<virtnet_alloc_queues>> INIT_DELAYED_WORK(&vi->refill, refill_work);
+	 *   - drivers/net/virtio_net.c|3234| <<virtnet_probe>> cancel_delayed_work_sync(&vi->refill);
+	 */
 	struct delayed_work refill;
 
 	/* Work struct for config space updates */
@@ -319,6 +332,13 @@ static struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)
 	return p;
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|339| <<virtqueue_napi_complete>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|354| <<skb_xmit_done>> virtqueue_napi_schedule(napi, vq);
+ *   - drivers/net/virtio_net.c|1340| <<skb_recv_done>> virtqueue_napi_schedule(&rq->napi, rvq);
+ *   - drivers/net/virtio_net.c|1352| <<virtnet_napi_enable>> virtqueue_napi_schedule(napi, vq);
+ */
 static void virtqueue_napi_schedule(struct napi_struct *napi,
 				    struct virtqueue *vq)
 {
@@ -1340,6 +1360,15 @@ static void skb_recv_done(struct virtqueue *rvq)
 	virtqueue_napi_schedule(&rq->napi, rvq);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1371| <<virtnet_napi_tx_enable>> return virtnet_napi_enable(vq, napi);
+ *   - drivers/net/virtio_net.c|1392| <<refill_work>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|1565| <<virtnet_open>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2404| <<virtnet_restore_up>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2538| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|2555| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ */
 static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 {
 	napi_enable(napi);
@@ -1854,6 +1883,12 @@ static void virtnet_ack_link_announce(struct virtnet_info *vi)
 	rtnl_unlock();
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1888| <<virtnet_set_queues>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2161| <<virtnet_set_channels>> err = _virtnet_set_queues(vi, queue_pairs);
+ *   - drivers/net/virtio_net.c|2517| <<virtnet_xdp_set>> err = _virtnet_set_queues(vi, curr_qp + xdp_qp);
+ */
 static int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)
 {
 	struct scatterlist sg;
@@ -2690,6 +2725,10 @@ static void virtnet_free_queues(struct virtnet_info *vi)
 	kfree(vi->ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2712| <<free_receive_bufs>> _free_receive_bufs(vi);
+ */
 static void _free_receive_bufs(struct virtnet_info *vi)
 {
 	struct bpf_prog *old_prog;
@@ -2706,6 +2745,10 @@ static void _free_receive_bufs(struct virtnet_info *vi)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3249| <<remove_vq_common>> free_receive_bufs(vi);
+ */
 static void free_receive_bufs(struct virtnet_info *vi)
 {
 	rtnl_lock();
diff --git a/drivers/net/xen-netback/rx.c b/drivers/net/xen-netback/rx.c
index accc991d153f..02a4c84c60f1 100644
--- a/drivers/net/xen-netback/rx.c
+++ b/drivers/net/xen-netback/rx.c
@@ -137,6 +137,11 @@ static void xenvif_rx_queue_drop_expired(struct xenvif_queue *queue)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|183| <<xenvif_rx_copy_add>> xenvif_rx_copy_flush(queue);
+ *   - drivers/net/xen-netback/rx.c|487| <<xenvif_rx_action>> xenvif_rx_copy_flush(queue);
+ */
 static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 {
 	unsigned int i;
@@ -171,6 +176,10 @@ static void xenvif_rx_copy_flush(struct xenvif_queue *queue)
 	__skb_queue_purge(queue->rx_copy.completed);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|387| <<xenvif_rx_data_slot>> xenvif_rx_copy_add(queue, req, offset, data, len);
+ */
 static void xenvif_rx_copy_add(struct xenvif_queue *queue,
 			       struct xen_netif_rx_request *req,
 			       unsigned int offset, void *data, size_t len)
@@ -371,6 +380,10 @@ static void xenvif_rx_next_chunk(struct xenvif_queue *queue,
 	*len = chunk_len;
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|461| <<xenvif_rx_skb>> xenvif_rx_data_slot(queue, &pkt, req, rsp);
+ */
 static void xenvif_rx_data_slot(struct xenvif_queue *queue,
 				struct xenvif_pkt_state *pkt,
 				struct xen_netif_rx_request *req,
@@ -439,6 +452,10 @@ static void xenvif_rx_extra_slot(struct xenvif_queue *queue,
 	BUG();
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|482| <<xenvif_rx_action>> xenvif_rx_skb(queue);
+ */
 static void xenvif_rx_skb(struct xenvif_queue *queue)
 {
 	struct xenvif_pkt_state pkt;
@@ -469,6 +486,10 @@ static void xenvif_rx_skb(struct xenvif_queue *queue)
 
 #define RX_BATCH_SIZE 64
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|646| <<xenvif_kthread_guest_rx>> xenvif_rx_action(queue);
+ */
 void xenvif_rx_action(struct xenvif_queue *queue)
 {
 	struct sk_buff_head completed_skbs;
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 44275908d61a..de188737d9c2 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -356,6 +356,25 @@ static void xennet_alloc_rx_buffers(struct netfront_queue *queue)
 		notify_remote_via_irq(queue->rx_irq);
 }
 
+/*
+ * [0] xennet_open
+ * [0] __dev_open
+ * [0] __dev_change_flags
+ * [0] dev_change_flags
+ * [0] do_setlink
+ * [0] __rtnl_newlink
+ * [0] rtnl_newlink
+ * [0] rtnetlink_rcv_msg
+ * [0] netlink_rcv_skb
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int xennet_open(struct net_device *dev)
 {
 	struct netfront_info *np = netdev_priv(dev);
diff --git a/fs/configfs/configfs_internal.h b/fs/configfs/configfs_internal.h
index c0395363eab9..8a0e92d83854 100644
--- a/fs/configfs/configfs_internal.h
+++ b/fs/configfs/configfs_internal.h
@@ -28,6 +28,14 @@ void put_fragment(struct configfs_fragment *);
 struct configfs_fragment *get_fragment(struct configfs_fragment *);
 
 struct configfs_dirent {
+	/*
+	 * 在以下使用configfs_dirent->s_count:
+	 *   - fs/configfs/configfs_internal.h|147| <<configfs_get>> WARN_ON(!atomic_read(&sd->s_count));
+	 *   - fs/configfs/configfs_internal.h|148| <<configfs_get>> atomic_inc(&sd->s_count);
+	 *   - fs/configfs/configfs_internal.h|155| <<configfs_put>> WARN_ON(!atomic_read(&sd->s_count));
+	 *   - fs/configfs/configfs_internal.h|156| <<configfs_put>> if (atomic_dec_and_test(&sd->s_count))
+	 *   - fs/configfs/dir.c|190| <<configfs_new_dirent>> atomic_set(&sd->s_count, 1);
+	 */
 	atomic_t		s_count;
 	int			s_dependent_count;
 	struct list_head	s_sibling;
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index 03faf20a6817..0a0c23d021f8 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -82,6 +82,11 @@
 #define DMAR_IQA_REG	0x90	/* Invalidation queue addr register */
 #define DMAR_ICS_REG	0x9c	/* Invalidation complete status register */
 #define DMAR_IQER_REG	0xb0	/* Invalidation queue error record register */
+/*
+ * 在以下使用DMAR_IRTA_REG:
+ *   - drivers/iommu/intel/irq_remapping.c|440| <<iommu_load_old_irte>> irta = dmar_readq(iommu->reg + DMAR_IRTA_REG);
+ *   - drivers/iommu/intel/irq_remapping.c|483| <<iommu_set_irq_remapping>> dmar_writeq(iommu->reg + DMAR_IRTA_REG,
+ */
 #define DMAR_IRTA_REG	0xb8    /* Interrupt remapping table addr register */
 #define DMAR_PQH_REG	0xc0	/* Page request queue head register */
 #define DMAR_PQT_REG	0xc8	/* Page request queue tail register */
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 8583ed3ff344..ca46ecf86b0e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -236,6 +236,11 @@ bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range);
 enum {
 	OUTSIDE_GUEST_MODE,
 	IN_GUEST_MODE,
+	/*
+	 * 在以下使用EXITING_GUEST_MODE:
+	 *   - arch/x86/kvm/x86.c|1939| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+	 *   - include/linux/kvm_host.h|400| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 */
 	EXITING_GUEST_MODE,
 	READING_SHADOW_PAGE_TABLES,
 };
@@ -397,6 +402,15 @@ static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
 	 * memory barrier following the write of vcpu->mode in VCPU RUN.
 	 */
 	smp_mb__before_atomic();
+	/*
+	 * 在以下使用EXITING_GUEST_MODE:
+	 *   - arch/x86/kvm/x86.c|1939| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+	 *   - include/linux/kvm_host.h|400| <<kvm_vcpu_exiting_guest_mode>> return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
+	 *
+	 * cmpxchg(void* ptr, int old, int new)
+	 * 如果ptr和old的值一样,则把new写到ptr内存,
+	 * 否则返回ptr的值,整个操作是原子的.
+	 */
 	return cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);
 }
 
@@ -534,7 +548,28 @@ struct kvm {
 	 * and is accessed atomically.
 	 */
 	atomic_t online_vcpus;
+	/*
+	 * x86在以下设置kvm->created_vcpus:
+	 *   - virt/kvm/kvm_main.c|3398| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus++;
+	 *   - virt/kvm/kvm_main.c|3474| <<kvm_vm_ioctl_create_vcpu>> kvm->created_vcpus--;
+	 * x86在以下使用kvm->created_vcpus:
+	 *   - arch/x86/kvm/svm/sev.c|236| <<sev_guest_init>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|6743| <<kvm_vm_ioctl_enable_cap>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|6973| <<kvm_arch_vm_ioctl>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|6997| <<kvm_arch_vm_ioctl>> if (kvm->created_vcpus)
+	 *   - arch/x86/kvm/x86.c|7154| <<kvm_arch_vm_ioctl>> if (kvm->created_vcpus)
+	 *   - virt/kvm/kvm_main.c|3393| <<kvm_vm_ioctl_create_vcpu>> if (kvm->created_vcpus == KVM_MAX_VCPUS) {
+	 *   - virt/kvm/kvm_main.c|3977| <<kvm_vm_ioctl_enable_dirty_log_ring>> if (kvm->created_vcpus) {
+	 */
 	int created_vcpus;
+	/*
+	 * 在以下使用kvm->last_boosted_vcpu:
+	 *   - virt/kvm/kvm_main.c|3228| <<kvm_vcpu_on_spin>> int last_boosted_vcpu = me->kvm->last_boosted_vcpu;
+	 *   - virt/kvm/kvm_main.c|3244| <<kvm_vcpu_on_spin>> if (!pass && i <= last_boosted_vcpu) {
+	 *   - virt/kvm/kvm_main.c|3245| <<kvm_vcpu_on_spin>> i = last_boosted_vcpu;
+	 *   - virt/kvm/kvm_main.c|3247| <<kvm_vcpu_on_spin>> } else if (pass && i > last_boosted_vcpu)
+	 *   - virt/kvm/kvm_main.c|3265| <<kvm_vcpu_on_spin>> kvm->last_boosted_vcpu = i;
+	 */
 	int last_boosted_vcpu;
 	struct list_head vm_list;
 	struct mutex lock;
@@ -579,6 +614,16 @@ struct kvm {
 	struct list_head devices;
 	u64 manual_dirty_log_protect;
 	struct dentry *debugfs_dentry;
+	/*
+	 * 在以下使用kvm->debugfs_stat_data:
+	 *   - virt/kvm/kvm_main.c|839| <<kvm_destroy_vm_debugfs>> if (kvm->debugfs_stat_data) {
+	 *   - virt/kvm/kvm_main.c|841| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data[i]);
+	 *   - virt/kvm/kvm_main.c|842| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data);
+	 *   - virt/kvm/kvm_main.c|858| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+	 *   - virt/kvm/kvm_main.c|859| <<kvm_create_vm_debugfs>> sizeof(*kvm->debugfs_stat_data),
+	 *   - virt/kvm/kvm_main.c|861| <<kvm_create_vm_debugfs>> if (!kvm->debugfs_stat_data)
+	 *   - virt/kvm/kvm_main.c|871| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data[p - debugfs_entries] = stat_data;
+	 */
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
@@ -1176,6 +1221,24 @@ search_memslots(struct kvm_memslots *slots, gfn_t gfn)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1256| <<resize_hpt_rehash_hpte>> __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|120| <<kvmppc_set_dirty_from_hpte>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|136| <<revmap_for_hpte>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|221| <<kvmppc_do_h_enter>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/powerpc/kvm/book3s_hv_rm_mmu.c|881| <<kvmppc_get_hpa>> memslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|758| <<account_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|788| <<unaccount_shadowed>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|946| <<gfn_to_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|139| <<inspect_spte_has_rmap>> slot = __gfn_to_memslot(slots, gfn);
+ *   - arch/x86/kvm/mmu/mmu_audit.c|202| <<audit_write_protection>> slot = __gfn_to_memslot(slots, sp->gfn);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|247| <<handle_changed_spte_dirty_log>> slot = __gfn_to_memslot(__kvm_memslots(kvm, as_id), gfn);
+ *   - virt/kvm/kvm_main.c|1843| <<gfn_to_memslot>> return __gfn_to_memslot(kvm_memslots(kvm), gfn);
+ *   - virt/kvm/kvm_main.c|1849| <<kvm_vcpu_gfn_to_memslot>> return __gfn_to_memslot(kvm_vcpu_memslots(vcpu), gfn);
+ *   - virt/kvm/kvm_main.c|2336| <<__kvm_map_gfn>> struct kvm_memory_slot *slot = __gfn_to_memslot(slots, gfn);
+ *   - virt/kvm/kvm_main.c|2721| <<__kvm_gfn_to_hva_cache_init>> ghc->memslot = __gfn_to_memslot(slots, start_gfn);
+ */
 static inline struct kvm_memory_slot *
 __gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)
 {
@@ -1257,8 +1320,14 @@ struct kvm_stats_debugfs_item {
 #define KVM_DBGFS_GET_MODE(dbgfs_item)                                         \
 	((dbgfs_item)->mode ? (dbgfs_item)->mode : 0644)
 
+/*
+ * 这里的base是kvm, 不是kvm_stat
+ */
 #define VM_STAT(n, x, ...) 							\
 	{ n, offsetof(struct kvm, stat.x), KVM_STAT_VM, ## __VA_ARGS__ }
+/*
+ * 这里的base是kvm_vcpu, 不是kvm_vcpu_state
+ */
 #define VCPU_STAT(n, x, ...)							\
 	{ n, offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU, ## __VA_ARGS__ }
 
@@ -1383,6 +1452,18 @@ static inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	set_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|669| <<check_vcpu_requests>> if (kvm_request_pending(vcpu)) {
+ *   - arch/arm64/kvm/arm.c|790| <<kvm_arch_vcpu_ioctl_run>> kvm_request_pending(vcpu)) {
+ *   - arch/mips/kvm/vz.c|2434| <<kvm_vz_check_requests>> if (!kvm_request_pending(vcpu))
+ *   - arch/powerpc/kvm/booke.c|689| <<kvmppc_core_prepare_to_enter>> if (kvm_request_pending(vcpu)) {
+ *   - arch/powerpc/kvm/powerpc.c|51| <<kvm_arch_vcpu_runnable>> return !!(v->arch.pending_exceptions) || kvm_request_pending(v);
+ *   - arch/powerpc/kvm/powerpc.c|113| <<kvmppc_prepare_to_enter>> if (kvm_request_pending(vcpu)) {
+ *   - arch/s390/kvm/kvm-s390.c|3793| <<kvm_s390_handle_requests>> if (!kvm_request_pending(vcpu))
+ *   - arch/x86/kvm/x86.c|1944| <<kvm_vcpu_exit_request>> return vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||
+ *   - arch/x86/kvm/x86.c|10364| <<vcpu_enter_guest>> if (kvm_request_pending(vcpu)) {
+ */
 static inline bool kvm_request_pending(struct kvm_vcpu *vcpu)
 {
 	return READ_ONCE(vcpu->requests);
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 5cbc950b34df..ed90c68355c0 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -2156,6 +2156,13 @@ struct net_device {
 #ifdef CONFIG_PCPU_DEV_REFCNT
 	int __percpu		*pcpu_refcnt;
 #else
+	/*
+	 * 在以下使用net_device->dev_refcnt:
+	 *   - include/linux/netdevice.h|4142| <<dev_put>> refcount_dec(&dev->dev_refcnt);
+	 *   - include/linux/netdevice.h|4157| <<dev_hold>> refcount_inc(&dev->dev_refcnt);
+	 *   - net/core/dev.c|10420| <<netdev_refcnt_read>> return refcount_read(&dev->dev_refcnt);
+	 *   - net/core/dev.c|10789| <<alloc_netdev_mqs>> refcount_set(&dev->dev_refcnt, 1);
+	 */
 	refcount_t		dev_refcnt;
 #endif
 
diff --git a/include/linux/timekeeping.h b/include/linux/timekeeping.h
index 78a98bdff76d..b6e384cd9e36 100644
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@ -161,6 +161,12 @@ static inline u64 ktime_get_real_ns(void)
 
 static inline u64 ktime_get_boottime_ns(void)
 {
+	/*
+	 * ktime_get_boottime - Returns monotonic time since boot in ktime_t format
+	 *
+	 * This is similar to CLOCK_MONTONIC/ktime_get, but also includes the
+	 * time spent in suspend.
+	 */
 	return ktime_to_ns(ktime_get_boottime());
 }
 
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 79d9c44d1ad7..25850504ea8f 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -251,6 +251,25 @@ struct kvm_xen_exit {
 #define KVM_EXIT_S390_RESET       14
 #define KVM_EXIT_DCR              15 /* deprecated */
 #define KVM_EXIT_NMI              16
+/*
+ * x86在以下使用KVM_EXIT_INTERNAL_ERROR:
+ *   - arch/x86/kvm/svm/nested.c|1366| <<svm_get_nested_state_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/svm/sev.c|2199| <<sev_es_validate_vmgexit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/svm/svm.c|3226| <<svm_handle_invalid_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/nested.c|3135| <<nested_get_vmcs12_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/nested.c|3193| <<vmx_get_nested_state_pages>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|56| <<sgx_handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|115| <<sgx_inject_fault>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/sgx.c|158| <<__handle_encls_ecreate>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|5077| <<handle_exception_nmi>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|5649| <<handle_invalid_guest_state>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|6240| <<__vmx_handle_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/vmx/vmx.c|6304| <<__vmx_handle_exit>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|7311| <<handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|7320| <<handle_emulation_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|9984| <<kvm_task_switch>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ *   - arch/x86/kvm/x86.c|11712| <<kvm_handle_memory_failure>> vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+ */
 #define KVM_EXIT_INTERNAL_ERROR   17
 #define KVM_EXIT_OSI              18
 #define KVM_EXIT_PAPR_HCALL	  19
@@ -272,12 +291,32 @@ struct kvm_xen_exit {
 
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
+/*
+ * x86在以下使用KVM_INTERNAL_ERROR_EMULATION:
+ *   - arch/x86/kvm/svm/nested.c|1368| <<svm_get_nested_state_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/nested.c|3137| <<nested_get_vmcs12_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/nested.c|3195| <<vmx_get_nested_state_pages>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|57| <<sgx_handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|116| <<sgx_inject_fault>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/sgx.c|159| <<__handle_encls_ecreate>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/vmx/vmx.c|5651| <<handle_invalid_guest_state>> KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|7312| <<handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|7321| <<handle_emulation_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|9985| <<kvm_task_switch>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ *   - arch/x86/kvm/x86.c|11713| <<kvm_handle_memory_failure>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+ */
 #define KVM_INTERNAL_ERROR_EMULATION	1
 /* Encounter unexpected simultaneous exceptions. */
 #define KVM_INTERNAL_ERROR_SIMUL_EX	2
 /* Encounter unexpected vm-exit due to delivery event. */
 #define KVM_INTERNAL_ERROR_DELIVERY_EV	3
 /* Encounter unexpected vm-exit reason */
+/*
+ * 在以下使用KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON:
+ *   - arch/x86/kvm/svm/sev.c|2200| <<sev_es_validate_vmgexit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ *   - arch/x86/kvm/svm/svm.c|3227| <<svm_handle_invalid_exit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ *   - arch/x86/kvm/vmx/vmx.c|6306| <<__vmx_handle_exit>> vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;
+ */
 #define KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON	4
 
 /* for KVM_RUN, returned by mmap(vcpu_fd, offset=0) */
diff --git a/include/xen/interface/vcpu.h b/include/xen/interface/vcpu.h
index 504c71601511..68edf0a7f19c 100644
--- a/include/xen/interface/vcpu.h
+++ b/include/xen/interface/vcpu.h
@@ -168,6 +168,11 @@ DEFINE_GUEST_HANDLE_STRUCT(vcpu_set_singleshot_timer);
  * The pointer need not be page aligned, but the structure must not
  * cross a page boundary.
  */
+/*
+ * 在以下使用VCPUOP_register_vcpu_info:
+ *   - arch/arm/xen/enlighten.c|156| <<xen_starting_cpu>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info, xen_vcpu_nr(cpu),
+ *   - arch/x86/xen/enlighten.c|252| <<xen_vcpu_setup>> err = HYPERVISOR_vcpu_op(VCPUOP_register_vcpu_info,
+ */
 #define VCPUOP_register_vcpu_info   10  /* arg == struct vcpu_info */
 struct vcpu_register_vcpu_info {
     uint64_t mfn;    /* mfn of page to place vcpu_info */
diff --git a/include/xen/xen-ops.h b/include/xen/xen-ops.h
index 39a5580f8feb..dc30dcf43e1f 100644
--- a/include/xen/xen-ops.h
+++ b/include/xen/xen-ops.h
@@ -9,6 +9,31 @@
 #include <asm/xen/interface.h>
 #include <xen/interface/vcpu.h>
 
+/*
+ * 在以下使用xen_vcpu:
+ *   - arch/x86/xen/enlighten.c|41| <<global>> DEFINE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - include/xen/xen-ops.h|12| <<global>> DECLARE_PER_CPU(struct vcpu_info *, xen_vcpu);
+ *   - rch/x86/xen/enlighten.c|198| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = &HYPERVISOR_shared_info->vcpu_info[xen_vcpu_nr(cpu)];
+ *   - arch/x86/xen/enlighten.c|202| <<xen_vcpu_info_reset>> per_cpu(xen_vcpu, cpu) = NULL;
+ *   - arch/x86/xen/enlighten.c|233| <<xen_vcpu_setup>> if (per_cpu(xen_vcpu, cpu) == &per_cpu(xen_vcpu_info, cpu))
+ *   - arch/x86/xen/enlighten.c|264| <<xen_vcpu_setup>> per_cpu(xen_vcpu, cpu) = vcpup;
+ *   - arch/x86/xen/enlighten.c|271| <<xen_vcpu_setup>> return ((per_cpu(xen_vcpu, cpu) == NULL) ? -ENODEV : 0);
+ *   - arch/x86/xen/enlighten_pv.c|1436| <<xen_cpu_up_prepare_pv>> if (per_cpu(xen_vcpu, cpu) == NULL)
+ *   - arch/x86/xen/irq.c|32| <<xen_save_fl>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/irq.c|51| <<xen_irq_disable>> this_cpu_read(xen_vcpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/irq.c|67| <<xen_irq_enable>> vcpu = this_cpu_read(xen_vcpu);
+ *   - arch/x86/xen/mmu_pv.c|1209| <<xen_write_cr2>> this_cpu_read(xen_vcpu)->arch.cr2 = cr2;
+ *   - arch/x86/xen/smp_pv.c|365| <<xen_pv_cpu_up>> per_cpu(xen_vcpu, cpu)->evtchn_upcall_mask = 1;
+ *   - arch/x86/xen/time.c|52| <<xen_clocksource_read>> src = &__this_cpu_read(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|74| <<xen_read_wallclock>> vcpu_time = &get_cpu_var(xen_vcpu)->time;
+ *   - arch/x86/xen/time.c|76| <<xen_read_wallclock>> put_cpu_var(xen_vcpu);
+ *   - arch/x86/xen/time.c|507| <<xen_time_init>> pvti = &__this_cpu_read(xen_vcpu)->time;
+ *   - drivers/xen/events/events_2l.c|123| <<evtchn_2l_unmask>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|173| <<evtchn_2l_handle_events>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ *   - drivers/xen/events/events_2l.c|280| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, i);
+ *   - drivers/xen/events/events_2l.c|289| <<xen_debug_interrupt>> v = per_cpu(xen_vcpu, cpu);
+ *   - drivers/xen/events/events_base.c|1699| <<__xen_evtchn_do_upcall>> struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
+ */
 DECLARE_PER_CPU(struct vcpu_info *, xen_vcpu);
 
 DECLARE_PER_CPU(uint32_t, xen_vcpu_id);
diff --git a/kernel/irq/matrix.c b/kernel/irq/matrix.c
index 578596e41cb6..f5dccc6c7075 100644
--- a/kernel/irq/matrix.c
+++ b/kernel/irq/matrix.c
@@ -486,6 +486,21 @@ unsigned int irq_matrix_allocated(struct irq_matrix *m)
  *
  * Note, this is a lockless snapshot.
  */
+/*
+ * cat /sys/kernel/debug/irq/domains/VECTOR
+ * name:   VECTOR
+ *  size:   0
+ *  mapped: 26
+ *  flags:  0x00000003
+ * Online bitmaps:        2
+ * Global available:    387
+ * Global reserved:      11
+ * Total allocated:      15
+ * System: 38: 0-19,32,50,128,236,240-242,244,246-255
+ *  | CPU | avl | man | mac | act | vectors
+ *      0   190     1     1   11  33-42,48
+ *      1   197     1     1    4  33-34,40-41
+ */
 void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind)
 {
 	unsigned int nsys = bitmap_weight(m->system_map, m->matrix_bits);
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index cbff6ba53d56..816ec8e6b420 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -105,6 +105,13 @@ struct qnode {
  *
  * PV doubles the storage and uses the second cacheline for PV state.
  */
+/*
+ * 在以下使用percpu的qnodes[]:
+ *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+ *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+ *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+ *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+ */
 static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
 
 /*
@@ -122,11 +129,22 @@ static inline __pure u32 encode_tail(int cpu, int idx)
 	return tail;
 }
 
+/*
+ * called by:
+ *   - kernel/locking/qspinlock.c|508| <<queued_spin_lock_slowpath>> prev = decode_tail(old);
+ */
 static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 {
 	int cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;
 	int idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
 
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 */
 	return per_cpu_ptr(&qnodes[idx].mcs, cpu);
 }
 
@@ -312,6 +330,22 @@ static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,
  * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
  *   queue               :         ^--'                             :
  */
+/*
+ * 在以下使用queued_spin_lock_slowpath():
+ *   - arch/x86/kernel/paravirt.c|362| <<global>> .lock.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
+ *   - include/asm-generic/qspinlock.h|71| <<global>> extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);
+ *   - kernel/locking/qspinlock.c|289| <<global>> #define queued_spin_lock_slowpath native_queued_spin_lock_slowpath
+ *   - kernel/locking/qspinlock.c|595| <<global>> #undef queued_spin_lock_slowpath
+ *   - kernel/locking/qspinlock.c|596| <<global>> #define queued_spin_lock_slowpath __pv_queued_spin_lock_slowpath
+ *   - arch/powerpc/include/asm/qspinlock.h|15| <<queued_spin_lock_slowpath>> static __always_inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+ *   - arch/powerpc/include/asm/qspinlock.h|43| <<queued_spin_lock>> queued_spin_lock_slowpath(lock, val);
+ *   - arch/x86/hyperv/hv_spinlock.c|80| <<hv_init_spinlocks>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - arch/x86/include/asm/paravirt.h|585| <<pv_queued_spin_lock_slowpath>> PVOP_VCALL2(lock.queued_spin_lock_slowpath, lock, val);
+ *   - arch/x86/include/asm/qspinlock.h|49| <<queued_spin_lock_slowpath>> static inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+ *   - arch/x86/kernel/kvm.c|979| <<kvm_spinlock_init>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - arch/x86/xen/spinlock.c|139| <<xen_init_spinlocks>> pv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
+ *   - include/asm-generic/qspinlock.h|85| <<queued_spin_lock>> queued_spin_lock_slowpath(lock, val);
+ */
 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 {
 	struct mcs_spinlock *prev, *next, *node;
@@ -397,6 +431,15 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 queue:
 	lockevent_inc(lock_slowpath);
 pv_queue:
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 *
+	 * struct mcs_spinlock *prev, *next, *node;
+	 */
 	node = this_cpu_ptr(&qnodes[0].mcs);
 	idx = node->count++;
 	tail = encode_tail(smp_processor_id(), idx);
@@ -412,6 +455,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 */
 	if (unlikely(idx >= MAX_NODES)) {
 		lockevent_inc(lock_no_node);
+		/*
+		 * called by:
+		 *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+		 *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+		 * 重新定义的queued_spin_trylock():
+		 * #define queued_spin_trylock(l)       pv_hybrid_queued_unfair_trylock(l)
+		 */
 		while (!queued_spin_trylock(lock))
 			cpu_relax();
 		goto release;
@@ -440,6 +490,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 * attempt the trylock once more in the hope someone let go while we
 	 * weren't watching.
 	 */
+	/*
+	 * called by:
+	 *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+	 *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+	 * 重新定义的queued_spin_trylock():
+	 * #define queued_spin_trylock(l)       pv_hybrid_queued_unfair_trylock(l)
+	 */
 	if (queued_spin_trylock(lock))
 		goto release;
 
@@ -557,6 +614,13 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	/*
 	 * release the node
 	 */
+	/*
+	 * 在以下使用percpu的qnodes[]:
+	 *   - kernel/locking/qspinlock.c|108| <<global>> static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
+	 *   - kernel/locking/qspinlock.c|130| <<decode_tail>> return per_cpu_ptr(&qnodes[idx].mcs, cpu);
+	 *   - kernel/locking/qspinlock.c|419| <<queued_spin_lock_slowpath>> node = this_cpu_ptr(&qnodes[0].mcs);
+	 *   - kernel/locking/qspinlock.c|593| <<queued_spin_lock_slowpath>> __this_cpu_dec(qnodes[0].mcs.count);
+	 */
 	__this_cpu_dec(qnodes[0].mcs.count);
 }
 EXPORT_SYMBOL(queued_spin_lock_slowpath);
diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h
index e84d21aa0722..12bbb17628de 100644
--- a/kernel/locking/qspinlock_paravirt.h
+++ b/kernel/locking/qspinlock_paravirt.h
@@ -21,6 +21,13 @@
  * native_queued_spin_unlock().
  */
 
+/*
+ * 在以下使用_Q_SLOW_VAL:
+ *   - kernel/locking/qspinlock_paravirt.h|391| <<pv_kick_node>> WRITE_ONCE(lock->locked, _Q_SLOW_VAL);
+ *   - kernel/locking/qspinlock_paravirt.h|456| <<pv_wait_head_or_lock>> if (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {
+ *   - kernel/locking/qspinlock_paravirt.h|470| <<pv_wait_head_or_lock>> pv_wait(&lock->locked, _Q_SLOW_VAL);
+ *   - kernel/locking/qspinlock_paravirt.h|501| <<__pv_queued_spin_unlock_slowpath>> if (unlikely(locked != _Q_SLOW_VAL)) {
+ */
 #define _Q_SLOW_VAL	(3U << _Q_LOCKED_OFFSET)
 
 /*
@@ -35,12 +42,24 @@
  * controlled by PV_PREV_CHECK_MASK. This is to ensure that we won't
  * pound on the cacheline of the previous node too heavily.
  */
+/*
+ * 只在以下使用PV_PREV_CHECK_MASK:
+ *   - kernel/locking/qspinlock_paravirt.h|269| <<pv_wait_early>> if ((loop & PV_PREV_CHECK_MASK) != 0)
+ */
 #define PV_PREV_CHECK_MASK	0xff
 
 /*
  * Queue node uses: vcpu_running & vcpu_halted.
  * Queue head uses: vcpu_running & vcpu_hashed.
  */
+/*
+ * 在以下使用vcpu_halted:
+ *   - kernel/locking/qspinlock_paravirt.h|320| <<pv_wait_node>> smp_store_mb(pn->state, vcpu_halted);
+ *   - kernel/locking/qspinlock_paravirt.h|325| <<pv_wait_node>> pv_wait(&pn->state, vcpu_halted);
+ *   - kernel/locking/qspinlock_paravirt.h|333| <<pv_wait_node>> cmpxchg(&pn->state, vcpu_halted, vcpu_running);
+ *   - kernel/locking/qspinlock_paravirt.h|380| <<pv_kick_node>> if (cmpxchg_relaxed(&pn->state, vcpu_halted, vcpu_hashed)
+ *   - kernel/locking/qspinlock_paravirt.h|381| <<pv_kick_node>> != vcpu_halted)
+ */
 enum vcpu_state {
 	vcpu_running = 0,
 	vcpu_halted,		/* Used only in pv_wait_node */
@@ -77,6 +96,13 @@ struct pv_node {
  * queued lock (no lock starvation) and an unfair lock (good performance
  * on not heavily contended locks).
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock.c|418| <<queued_spin_lock_slowpath>> while (!queued_spin_trylock(lock))
+ *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> if (queued_spin_trylock(lock))
+ * 重新定义的queued_spin_trylock():
+ * #define queued_spin_trylock(l)	pv_hybrid_queued_unfair_trylock(l)
+ */
 #define queued_spin_trylock(l)	pv_hybrid_queued_unfair_trylock(l)
 static inline bool pv_hybrid_queued_unfair_trylock(struct qspinlock *lock)
 {
@@ -173,10 +199,34 @@ struct pv_hash_entry {
 	struct pv_node   *node;
 };
 
+/*
+ * 在以下使用PV_HE_PER_LINE:
+ *   - kernel/locking/qspinlock_paravirt.h|230| <<__pv_init_lock_hash>> int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);
+ *   - kernel/locking/qspinlock_paravirt.h|248| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+ */
 #define PV_HE_PER_LINE	(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))
+/*
+ * 在以下使用PV_HE_MIN:
+ *   - kernel/locking/qspinlock_paravirt.h|232| <<__pv_init_lock_hash>> if (pv_hash_size < PV_HE_MIN)
+ *   - kernel/locking/qspinlock_paravirt.h|233| <<__pv_init_lock_hash>> pv_hash_size = PV_HE_MIN;
+ */
 #define PV_HE_MIN	(PAGE_SIZE / sizeof(struct pv_hash_entry))
 
+/*
+ * 在以下使用pv_lock_hash:
+ *   - kernel/locking/qspinlock_paravirt.h|199| <<__pv_init_lock_hash>> pv_lock_hash = alloc_large_system_hash("PV qspinlock",
+ *   - kernel/locking/qspinlock_paravirt.h|208| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+ *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+ */
 static struct pv_hash_entry *pv_lock_hash;
+/*
+ * 在以下使用pv_lock_hash_bits:
+ *   - kernel/locking/qspinlock_paravirt.h|203| <<__pv_init_lock_hash>> &pv_lock_hash_bits, NULL,
+ *   - kernel/locking/qspinlock_paravirt.h|209| <<for_each_hash_entry>> offset < (1 << pv_lock_hash_bits); \
+ *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+ *   - kernel/locking/qspinlock_paravirt.h|214| <<pv_hash>> unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
+ *   - kernel/locking/qspinlock_paravirt.h|241| <<pv_unhash>> unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
+ */
 static unsigned int pv_lock_hash_bits __read_mostly;
 
 /*
@@ -185,6 +235,13 @@ static unsigned int pv_lock_hash_bits __read_mostly;
  * This function should be called from the paravirt spinlock initialization
  * routine.
  */
+/*
+ * called by:
+ *   - arch/powerpc/include/asm/qspinlock.h|70| <<pv_spinlocks_init>> __pv_init_lock_hash();
+ *   - arch/x86/hyperv/hv_spinlock.c|79| <<hv_init_spinlocks>> __pv_init_lock_hash();
+ *   - arch/x86/kernel/kvm.c|978| <<kvm_spinlock_init>> __pv_init_lock_hash();
+ *   - arch/x86/xen/spinlock.c|138| <<xen_init_spinlocks>> __pv_init_lock_hash();
+ */
 void __init __pv_init_lock_hash(void)
 {
 	int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);
@@ -196,6 +253,12 @@ void __init __pv_init_lock_hash(void)
 	 * Allocate space from bootmem which should be page-size aligned
 	 * and hence cacheline aligned.
 	 */
+	/*
+	 * 在以下使用pv_lock_hash:
+	 *   - kernel/locking/qspinlock_paravirt.h|199| <<__pv_init_lock_hash>> pv_lock_hash = alloc_large_system_hash("PV qspinlock",
+	 *   - kernel/locking/qspinlock_paravirt.h|208| <<for_each_hash_entry>> for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0; \
+	 *   - kernel/locking/qspinlock_paravirt.h|210| <<for_each_hash_entry>> offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
+	 */
 	pv_lock_hash = alloc_large_system_hash("PV qspinlock",
 					       sizeof(struct pv_hash_entry),
 					       pv_hash_size, 0,
@@ -204,11 +267,21 @@ void __init __pv_init_lock_hash(void)
 					       pv_hash_size, pv_hash_size);
 }
 
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|268| <<pv_hash>> for_each_hash_entry(he, offset, hash) {
+ *   - kernel/locking/qspinlock_paravirt.h|295| <<pv_unhash>> for_each_hash_entry(he, offset, hash) {
+ */
 #define for_each_hash_entry(he, offset, hash)						\
 	for (hash &= ~(PV_HE_PER_LINE - 1), he = &pv_lock_hash[hash], offset = 0;	\
 	     offset < (1 << pv_lock_hash_bits);						\
 	     offset++, he = &pv_lock_hash[(hash + offset) & ((1 << pv_lock_hash_bits) - 1)])
 
+/*
+ * 在以下使用pv_hash():
+ *   - kernel/locking/qspinlock_paravirt.h|480| <<pv_kick_node>> (void )pv_hash(lock, pn);
+ *   - kernel/locking/qspinlock_paravirt.h|531| <<pv_wait_head_or_lock>> lp = pv_hash(lock, pn);
+ */
 static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)
 {
 	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
@@ -236,6 +309,10 @@ static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)
 	BUG();
 }
 
+/*
+ * 在以下使用pv_unhash():
+ *   - kernel/locking/qspinlock_paravirt.h|609| <<__pv_queued_spin_unlock_slowpath>> node = pv_unhash(lock);
+ */
 static struct pv_node *pv_unhash(struct qspinlock *lock)
 {
 	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);
@@ -245,6 +322,12 @@ static struct pv_node *pv_unhash(struct qspinlock *lock)
 	for_each_hash_entry(he, offset, hash) {
 		if (READ_ONCE(he->lock) == lock) {
 			node = READ_ONCE(he->node);
+			/*
+			 * truct pv_hash_entry {
+			 *     struct qspinlock *lock;
+			 *     struct pv_node   *node;
+			 * };
+			 */
 			WRITE_ONCE(he->lock, NULL);
 			return node;
 		}
@@ -263,9 +346,29 @@ static struct pv_node *pv_unhash(struct qspinlock *lock)
  * Return true if when it is time to check the previous node which is not
  * in a running state.
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|304| <<pv_wait_node>> if (pv_wait_early(pp, loop)) {
+ *   - kernel/locking/qspinlock_paravirt.h|324| <<pv_wait_node>> lockevent_cond_inc(pv_wait_early, wait_early);
+ */
 static inline bool
 pv_wait_early(struct pv_node *prev, int loop)
 {
+	/*
+	 * Queue Node Adaptive Spinning
+	 *
+	 * A queue node vCPU will stop spinning if the vCPU in the previous node is
+	 * not running. The one lock stealing attempt allowed at slowpath entry
+	 * mitigates the slight slowdown for non-overcommitted guest with this
+	 * aggressive wait-early mechanism.
+	 *      
+	 * The status of the previous node will be checked at fixed interval
+	 * controlled by PV_PREV_CHECK_MASK. This is to ensure that we won't
+	 * pound on the cacheline of the previous node too heavily.
+	 *
+	 * 只在以下使用PV_PREV_CHECK_MASK:
+	 *   - kernel/locking/qspinlock_paravirt.h|269| <<pv_wait_early>> if ((loop & PV_PREV_CHECK_MASK) != 0)
+	 */
 	if ((loop & PV_PREV_CHECK_MASK) != 0)
 		return false;
 
@@ -275,6 +378,12 @@ pv_wait_early(struct pv_node *prev, int loop)
 /*
  * Initialize the PV part of the mcs_spinlock node.
  */
+/*
+ * 在以下使用pv_init_node():
+ *   - kernel/locking/qspinlock.c|283| <<global>> #define pv_init_node __pv_init_node
+ *   - kernel/locking/qspinlock.c|590| <<global>> #undef pv_init_node
+ *   - kernel/locking/qspinlock.c|446| <<queued_spin_lock_slowpath>> pv_init_node(node);
+ */
 static void pv_init_node(struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -290,6 +399,14 @@ static void pv_init_node(struct mcs_spinlock *node)
  * pv_kick_node() is used to set _Q_SLOW_VAL and fill in hash table on its
  * behalf.
  */
+/*
+ * 在以下使用pv_wait_node():
+ *   - kernel/locking/qspinlock.c|284| <<global>> #define pv_wait_node __pv_wait_node
+ *   - kernel/locking/qspinlock.c|591| <<global>> #undef pv_wait_node
+ *   - kernel/locking/lock_events_list.h|35| <<LOCK_EVENT>> LOCK_EVENT(pv_wait_node)
+ *   - kernel/locking/qspinlock.c|490| <<queued_spin_lock_slowpath>> pv_wait_node(node, prev);
+ *   - kernel/locking/qspinlock_paravirt.h|411| <<pv_wait_node>> lockevent_inc(pv_wait_node);
+ */
 static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -357,6 +474,12 @@ static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
  * such that they're waiting in pv_wait_head_or_lock(), this avoids a
  * wake/sleep cycle.
  */
+/*
+ * 在以下使用pv_kick_node():
+ *   - kernel/locking/qspinlock.c|285| <<global>> #define pv_kick_node __pv_kick_node
+ *   - kernel/locking/qspinlock.c|592| <<global>> #undef pv_kick_node
+ *   - kernel/locking/qspinlock.c|571| <<queued_spin_lock_slowpath>> pv_kick_node(lock, next);
+ */
 static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
@@ -399,6 +522,12 @@ static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
  *
  * The current value of the lock will be returned for additional processing.
  */
+/*
+ * 在以下使用pv_wait_head_or_lock():
+ *   - kernel/locking/qspinlock.c|286| <<global>> #define pv_wait_head_or_lock __pv_wait_head_or_lock
+ *   - kernel/locking/qspinlock.c|593| <<global>> #undef pv_wait_head_or_lock
+ *   - kernel/locking/qspinlock.c|525| <<queued_spin_lock_slowpath>> if ((val = pv_wait_head_or_lock(lock, node)))
+ */
 static u32
 pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
 {
@@ -489,6 +618,10 @@ pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
  * PV versions of the unlock fastpath and slowpath functions to be used
  * instead of queued_spin_unlock().
  */
+/*
+ * called by:
+ *   - kernel/locking/qspinlock_paravirt.h|560| <<__pv_queued_spin_unlock>> __pv_queued_spin_unlock_slowpath(lock, locked);
+ */
 __visible void
 __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 {
@@ -544,6 +677,14 @@ __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 #include <asm/qspinlock_paravirt.h>
 
 #ifndef __pv_queued_spin_unlock
+/*
+ * 在以下使用__pv_queued_spin_unlock():
+ *   - arch/x86/include/asm/qspinlock_paravirt.h|66| <<global>> PV_CALLEE_SAVE_REGS_THUNK(__pv_queued_spin_unlock);
+ *   - arch/powerpc/include/asm/qspinlock.h|29| <<queued_spin_unlock>> __pv_queued_spin_unlock(lock);
+ *   - arch/x86/hyperv/hv_spinlock.c|81| <<hv_init_spinlocks>> pv_ops.lock.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ *   - arch/x86/kernel/kvm.c|981| <<kvm_spinlock_init>> PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ *   - arch/x86/xen/spinlock.c|141| <<xen_init_spinlocks>> PV_CALLEE_SAVE(__pv_queued_spin_unlock);
+ */
 __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 {
 	u8 locked;
@@ -553,7 +694,18 @@ __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 	 * unhash. Otherwise it would be possible to have multiple @lock
 	 * entries, which would be BAD.
 	 */
+	/*
+	 * cmpxchg(void *ptr, unsigned long old, unsigned long new);
+	 * 函数完成的功能是: 将old和ptr指向的内容比较,如果相等,
+	 * 则将new写入到ptr中,返回old,如果不相等,则返回ptr指向的内容.
+	 *
+	 * struct qspinlock *lock:
+	 * -> u8 locked;
+	 */
 	locked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);
+	/*
+	 * 如果slow的话最后两位可能都是11
+	 */
 	if (likely(locked == _Q_LOCKED_VAL))
 		return;
 
diff --git a/kernel/sched/clock.c b/kernel/sched/clock.c
index c2b2859ddd82..4f264e32e6fb 100644
--- a/kernel/sched/clock.c
+++ b/kernel/sched/clock.c
@@ -78,12 +78,27 @@ static DEFINE_STATIC_KEY_FALSE(sched_clock_running);
  * Similarly we start with __sched_clock_stable_early, thereby assuming we
  * will become stable, such that there's only a single 1 -> 0 transition.
  */
+/*
+ * 在以下使用__sched_clock_stable:
+ *   - kernel/sched/clock.c|110| <<sched_clock_stable>> return static_branch_likely(&__sched_clock_stable);
+ *   - kernel/sched/clock.c|139| <<__set_sched_clock_stable>> static_branch_enable(&__sched_clock_stable);
+ *   - kernel/sched/clock.c|175| <<__sched_clock_work>> static_branch_disable(&__sched_clock_stable);
+ */
 static DEFINE_STATIC_KEY_FALSE(__sched_clock_stable);
 static int __sched_clock_stable_early = 1;
 
 /*
  * We want: ktime_get_ns() + __gtod_offset == sched_clock() + __sched_clock_offset
  */
+/*
+ * 在以下使用__sched_clock_offset:
+ *   - arch/x86/events/core.c|2703| <<arch_perf_update_userpage>> offset = data.cyc2ns_offset + __sched_clock_offset;
+ *   - kernel/sched/clock.c|132| <<__set_sched_clock_stable>> __sched_clock_offset = (scd->tick_gtod + __gtod_offset) - (scd->tick_raw);
+ *   - kernel/sched/clock.c|137| <<__set_sched_clock_stable>> scd->tick_raw, __sched_clock_offset);
+ *   - kernel/sched/clock.c|173| <<__sched_clock_work>> scd->tick_raw, __sched_clock_offset);
+ *   - kernel/sched/clock.c|204| <<__sched_clock_gtod_offset>> __gtod_offset = (scd->tick_raw + __sched_clock_offset) - scd->tick_gtod;
+ *   - kernel/sched/clock.c|371| <<sched_clock_cpu>> return sched_clock() + __sched_clock_offset;
+ */
 __read_mostly u64 __sched_clock_offset;
 static __read_mostly u64 __gtod_offset;
 
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 8a364aa9881a..7d359593b30a 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -657,6 +657,12 @@ static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
  * pvclock_gtod_register_notifier - register a pvclock timedata update listener
  * @nb: Pointer to the notifier block to register
  */
+/*
+ * called by:
+ *   - arch/arm/xen/enlighten.c|393| <<xen_guest_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|8528| <<kvm_arch_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/xen/time.c|520| <<xen_time_init>> pvclock_gtod_register_notifier(&xen_pvclock_gtod_notifier);
+ */
 int pvclock_gtod_register_notifier(struct notifier_block *nb)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
@@ -1754,6 +1760,11 @@ void timekeeping_inject_sleeptime64(const struct timespec64 *delta)
 /**
  * timekeeping_resume - Resumes the generic timekeeping subsystem.
  */
+/*
+ * 在以下使用timekeeping_resume():
+ *   - struct syscore_ops timekeeping_syscore_ops.resume = timekeeping_resume()
+ *   - kernel/time/tick-common.c|549| <<tick_unfreeze>> timekeeping_resume();
+ */
 void timekeeping_resume(void)
 {
 	struct timekeeper *tk = &tk_core.timekeeper;
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index 6f5f78885ab4..cc364cb43033 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -59,8 +59,20 @@
 #include "internal.h"
 #include "ras/ras_event.h"
 
+/*
+ * 在以下使用sysctl_memory_failure_early_kill:
+ *   - kernel/sysctl.c|3072| <<global>> .data = &sysctl_memory_failure_early_kill,
+ *   - kernel/sysctl.c|3073| <<global>> .maxlen = sizeof(sysctl_memory_failure_early_kill),
+ *   - mm/memory-failure.c|436| <<find_early_kill_thread>> if (sysctl_memory_failure_early_kill)
+ */
 int sysctl_memory_failure_early_kill __read_mostly = 0;
 
+/*
+ * 在以下使用sysctl_memory_failure_recovery:
+ *   - kernel/sysctl.c|3081| <<global>> .data = &sysctl_memory_failure_recovery,
+ *   - kernel/sysctl.c|3082| <<global>> .maxlen = sizeof(sysctl_memory_failure_recovery),
+ *   - mm/memory-failure.c|1451| <<memory_failure>> if (!sysctl_memory_failure_recovery)
+ */
 int sysctl_memory_failure_recovery __read_mostly = 1;
 
 atomic_long_t num_poisoned_pages __read_mostly = ATOMIC_LONG_INIT(0);
@@ -1437,6 +1449,19 @@ static int memory_failure_dev_pagemap(unsigned long pfn, int flags,
  * Must run in process context (e.g. a work queue) with interrupts
  * enabled and no spinlocks hold.
  */
+/*
+ * called by:
+ *   - arch/parisc/kernel/pdt.c|331| <<pdt_mainloop>> memory_failure(pde >> PAGE_SHIFT, 0);
+ *   - arch/powerpc/kernel/mce.c|313| <<machine_process_ue_event>> memory_failure(pfn, 0);
+ *   - arch/powerpc/platforms/powernv/opal-memory-errors.c|50| <<handle_memory_error_event>> memory_failure(paddr_start >> PAGE_SHIFT, 0);
+ *   - arch/x86/kernel/cpu/mce/core.c|649| <<uc_decode_notifier>> if (!memory_failure(pfn, 0)) {
+ *   - arch/x86/kernel/cpu/mce/core.c|1266| <<kill_me_maybe>> if (!memory_failure(p->mce_addr >> PAGE_SHIFT, flags) &&
+ *   - arch/x86/kernel/cpu/mce/core.c|1455| <<memory_failure>> int memory_failure(unsigned long pfn, int flags)
+ *   - drivers/base/memory.c|549| <<hard_offline_page_store>> ret = memory_failure(pfn, 0);
+ *   - mm/hwpoison-inject.c|51| <<hwpoison_inject>> return memory_failure(pfn, 0);
+ *   - mm/madvise.c|911| <<madvise_inject_error>> ret = memory_failure(pfn, MF_COUNT_INCREASED);
+ *   - mm/memory-failure.c|1694| <<memory_failure_work_func>> memory_failure(entry.pfn, entry.flags);
+ */
 int memory_failure(unsigned long pfn, int flags)
 {
 	struct page *p;
diff --git a/net/core/dev.c b/net/core/dev.c
index ef8cf7619baf..08e6abc40bb3 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -10438,6 +10438,10 @@ int netdev_unregister_timeout_secs __read_mostly = 10;
  * We can get stuck here if buggy protocols don't correctly
  * call dev_put.
  */
+/*
+ * called by:
+ *   - net/core/dev.c|10563| <<netdev_run_todo>> netdev_wait_allrefs(dev);
+ */
 static void netdev_wait_allrefs(struct net_device *dev)
 {
 	unsigned long rebroadcast_time, warning_time;
@@ -10519,6 +10523,10 @@ static void netdev_wait_allrefs(struct net_device *dev)
  * We must not return until all unregister events added during
  * the interval the lock was held have been completed.
  */
+/*
+ * called by:
+ *   - net/core/rtnetlink.c|112| <<rtnl_unlock>> netdev_run_todo();
+ */
 void netdev_run_todo(void)
 {
 	struct list_head list;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 46fb042837d2..990fa82a57a1 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -111,9 +111,25 @@ static struct kmem_cache *kvm_vcpu_cache;
 static __read_mostly struct preempt_ops kvm_preempt_ops;
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
+/*
+ * 在以下使用kvm_debugfs_dir:
+ *   - arch/powerpc/kvm/book3s_hv.c|5072| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+ *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+ *   - virt/kvm/kvm_main.c|860| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+ *   - virt/kvm/kvm_main.c|5069| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+ *   - virt/kvm/kvm_main.c|5074| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+ *   - virt/kvm/kvm_main.c|5291| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+ */
 struct dentry *kvm_debugfs_dir;
 EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
 
+/*
+ * 在以下使用kvm_debugfs_num_entries:
+ *   - virt/kvm/kvm_main.c|840| <<kvm_destroy_vm_debugfs>> for (i = 0; i < kvm_debugfs_num_entries; i++)
+ *   - virt/kvm/kvm_main.c|862| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+ *   - virt/kvm/kvm_main.c|5071| <<kvm_init_debug>> kvm_debugfs_num_entries = 0;
+ *   - virt/kvm/kvm_main.c|5072| <<kvm_init_debug>> for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
+ */
 static int kvm_debugfs_num_entries;
 static const struct file_operations stat_fops_per_vm;
 
@@ -146,6 +162,11 @@ static void hardware_disable_all(void);
 
 static void kvm_io_bus_destroy(struct kvm_io_bus *bus);
 
+/*
+ * 在以下使用kvm_rebooting:
+ *   - arch/x86/kvm/x86.c|539| <<kvm_spurious_fault>> BUG_ON(!kvm_rebooting);
+ *   - virt/kvm/kvm_main.c|4583| <<kvm_reboot>> kvm_rebooting = true;
+ */
 __visible bool kvm_rebooting;
 EXPORT_SYMBOL_GPL(kvm_rebooting);
 
@@ -843,6 +864,56 @@ static void kvm_destroy_vm_debugfs(struct kvm *kvm)
 	}
 }
 
+/*
+ * struct kvm_stats_debugfs_item debugfs_entries[] = {
+ *		VCPU_STAT("pf_fixed", pf_fixed),
+ *		VCPU_STAT("pf_guest", pf_guest),
+ *		VCPU_STAT("tlb_flush", tlb_flush),
+ *		VCPU_STAT("invlpg", invlpg),
+ *		VCPU_STAT("exits", exits),
+ *		VCPU_STAT("io_exits", io_exits),
+ *		VCPU_STAT("mmio_exits", mmio_exits),
+ *		VCPU_STAT("signal_exits", signal_exits),
+ *		VCPU_STAT("irq_window", irq_window_exits),
+ *		VCPU_STAT("nmi_window", nmi_window_exits),
+ *		VCPU_STAT("halt_exits", halt_exits),
+ *		VCPU_STAT("halt_successful_poll", halt_successful_poll),
+ *		VCPU_STAT("halt_attempted_poll", halt_attempted_poll),
+ *		VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
+ *		VCPU_STAT("halt_wakeup", halt_wakeup),
+ *		VCPU_STAT("hypercalls", hypercalls),
+ *		VCPU_STAT("request_irq", request_irq_exits),
+ *		VCPU_STAT("irq_exits", irq_exits),
+ *		VCPU_STAT("host_state_reload", host_state_reload),
+ *		VCPU_STAT("fpu_reload", fpu_reload),
+ *		VCPU_STAT("insn_emulation", insn_emulation),
+ *		VCPU_STAT("insn_emulation_fail", insn_emulation_fail),
+ *		VCPU_STAT("irq_injections", irq_injections),
+ *		VCPU_STAT("nmi_injections", nmi_injections),
+ *		VCPU_STAT("req_event", req_event),
+ *		VCPU_STAT("l1d_flush", l1d_flush),
+ *		VCPU_STAT("halt_poll_success_ns", halt_poll_success_ns),
+ *		VCPU_STAT("halt_poll_fail_ns", halt_poll_fail_ns),
+ *		VCPU_STAT("nested_run", nested_run),
+ *		VCPU_STAT("directed_yield_attempted", directed_yield_attempted),
+ *		VCPU_STAT("directed_yield_successful", directed_yield_successful),
+ *		VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
+ *		VM_STAT("mmu_pte_write", mmu_pte_write),
+ *		VM_STAT("mmu_pde_zapped", mmu_pde_zapped),
+ *		VM_STAT("mmu_flooded", mmu_flooded),
+ *		VM_STAT("mmu_recycled", mmu_recycled),
+ *		VM_STAT("mmu_cache_miss", mmu_cache_miss),
+ *		VM_STAT("mmu_unsync", mmu_unsync),
+ *		VM_STAT("remote_tlb_flush", remote_tlb_flush),
+ *		VM_STAT("largepages", lpages, .mode = 0444),
+ *		VM_STAT("nx_largepages_splitted", nx_lpage_splits, .mode = 0444),
+ *		VM_STAT("max_mmu_page_hash_collisions", max_mmu_page_hash_collisions),
+ *		{ NULL }
+ * };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|4322| <<kvm_dev_ioctl_create_vm>> if (kvm_create_vm_debugfs(kvm, r) < 0) {
+ */
 static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 {
 	char dir_name[ITOA_MAX_LEN * 2];
@@ -855,6 +926,13 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 	snprintf(dir_name, sizeof(dir_name), "%d-%d", task_pid_nr(current), fd);
 	kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
 
+	/*
+	 * 在以下使用kvm_debugfs_num_entries:
+	 *   - virt/kvm/kvm_main.c|840| <<kvm_destroy_vm_debugfs>> for (i = 0; i < kvm_debugfs_num_entries; i++)
+	 *   - virt/kvm/kvm_main.c|862| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+	 *   - virt/kvm/kvm_main.c|5071| <<kvm_init_debug>> kvm_debugfs_num_entries = 0;
+	 *   - virt/kvm/kvm_main.c|5072| <<kvm_init_debug>> for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
+	 */
 	kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
 					 sizeof(*kvm->debugfs_stat_data),
 					 GFP_KERNEL_ACCOUNT);
@@ -862,12 +940,27 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 		return -ENOMEM;
 
 	for (p = debugfs_entries; p->name; p++) {
+		/*
+		 * struct kvm_stat_data *stat_data;
+		 * -> struct kvm *kvm;
+		 * -> struct kvm_stats_debugfs_item *dbgfs_item;
+		 */
 		stat_data = kzalloc(sizeof(*stat_data), GFP_KERNEL_ACCOUNT);
 		if (!stat_data)
 			return -ENOMEM;
 
 		stat_data->kvm = kvm;
 		stat_data->dbgfs_item = p;
+		/*
+		 * 在以下使用kvm->debugfs_stat_data:
+		 *   - virt/kvm/kvm_main.c|839| <<kvm_destroy_vm_debugfs>> if (kvm->debugfs_stat_data) {
+		 *   - virt/kvm/kvm_main.c|841| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data[i]);
+		 *   - virt/kvm/kvm_main.c|842| <<kvm_destroy_vm_debugfs>> kfree(kvm->debugfs_stat_data);
+		 *   - virt/kvm/kvm_main.c|858| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+		 *   - virt/kvm/kvm_main.c|859| <<kvm_create_vm_debugfs>> sizeof(*kvm->debugfs_stat_data),
+		 *   - virt/kvm/kvm_main.c|861| <<kvm_create_vm_debugfs>> if (!kvm->debugfs_stat_data)
+		 *   - virt/kvm/kvm_main.c|871| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data[p - debugfs_entries] = stat_data;
+		 */
 		kvm->debugfs_stat_data[p - debugfs_entries] = stat_data;
 		debugfs_create_file(p->name, KVM_DBGFS_GET_MODE(p),
 				    kvm->debugfs_dentry, stat_data,
@@ -1638,6 +1731,10 @@ EXPORT_SYMBOL_GPL(kvm_get_dirty_log);
  * exiting to userspace will be logged for the next call.
  *
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1739| <<kvm_vm_ioctl_get_dirty_log>> r = kvm_get_dirty_log_protect(kvm, log);
+ */
 static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 {
 	struct kvm_memslots *slots;
@@ -2990,6 +3087,18 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 					++vcpu->stat.halt_poll_invalid;
 				goto out;
 			}
+			/*
+			 * 这里少了一个cpu_relax()
+			 *
+			 * Sean Christopherson suggested as below:
+			 *
+			 * "Rather than disallowing halt-polling entirely, on x86 it should be
+			 * sufficient to simply have the hardware thread yield to its sibling(s)
+			 * via PAUSE.  It probably won't get back all performance, but I would
+			 * expect it to be close.
+			 * This compiles on all KVM architectures, and AFAICT the intended usage
+			 * of cpu_relax() is identical for all architectures."
+			 */
 			poll_end = cur = ktime_get();
 		} while (kvm_vcpu_can_poll(cur, stop));
 	}
@@ -3056,6 +3165,44 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_wake_up);
 /*
  * Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.
  */
+/*
+ * 在以下使用kvm_vcpu_kick():
+ *   - arch/powerpc/kvm/book3s_pr.c|2071| <<global>> .fast_vcpu_kick = kvm_vcpu_kick,
+ *   - arch/arm64/kvm/arm.c|442| <<vcpu_power_off>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/arm.c|947| <<vcpu_interrupt_line>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|465| <<kvm_pmu_perf_overflow_notify_vcpu>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|502| <<kvm_pmu_perf_overflow>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/psci.c|59| <<kvm_psci_vcpu_off>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|103| <<vgic_v4_doorbell_handler>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|367| <<vgic_queue_irq_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|416| <<vgic_queue_irq_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|711| <<vgic_prune_ap_list>> kvm_vcpu_kick(target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|1003| <<vgic_kick_vcpus>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/include/asm/kvm_ppc.h|582| <<kvmppc_fast_vcpu_kick>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/book3s.c|781| <<kvmppc_decrementer_func>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/booke.c|625| <<kvmppc_watchdog_func>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/booke.c|636| <<kvmppc_watchdog_func>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/booke.c|1828| <<kvmppc_set_tsr_bits>> kvm_vcpu_kick(vcpu);
+ *   - arch/powerpc/kvm/e500_emulate.c|78| <<kvmppc_e500_emul_msgsnd>> kvm_vcpu_kick(cvcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1862| <<kvm_vcpu_ioctl_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/hyperv.c|544| <<stimer_mark_pending>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/i8259.c|63| <<pic_unlock>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1100| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1108| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1114| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1120| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1129| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1140| <<__apic_accept_irq>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/lapic.c|1687| <<apic_timer_expired>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|2078| <<vmx_preemption_timer_fn>> kvm_vcpu_kick(&vmx->vcpu);
+ *   - arch/x86/kvm/vmx/posted_intr.c|234| <<pi_wakeup_handler>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4245| <<vmx_deliver_nested_posted_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|4278| <<vmx_deliver_posted_interrupt>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|3904| <<kvmclock_update_fn>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|6464| <<kvm_arch_sync_dirty_log>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|12261| <<kvm_arch_memslots_updated>> kvm_vcpu_kick(vcpu);
+ *   - arch/x86/kvm/x86.c|12750| <<kvm_arch_async_page_present_queued>> kvm_vcpu_kick(vcpu);
+ */
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 {
 	int me;
@@ -3065,6 +3212,13 @@ void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 		return;
 
 	me = get_cpu();
+	/*
+	 * 注释Note, the vCPU could get migrated to a different pCPU at any point
+	 * after kvm_arch_vcpu_should_kick(), which could result in sending an
+	 * IPI to the previous pCPU.  But, that's ok because the purpose of the
+	 * IPI is to force the vCPU to leave IN_GUEST_MODE, and migrating the
+	 * vCPU also requires it to leave IN_GUEST_MODE.
+	 */
 	if (cpu != me && (unsigned)cpu < nr_cpu_ids && cpu_online(cpu))
 		if (kvm_arch_vcpu_should_kick(vcpu))
 			smp_send_reschedule(cpu);
@@ -3299,6 +3453,10 @@ static int create_vcpu_fd(struct kvm_vcpu *vcpu)
 	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3548| <<kvm_vm_ioctl_create_vcpu>> kvm_create_vcpu_debugfs(vcpu);
+ */
 static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
@@ -4319,6 +4477,13 @@ static struct file_operations kvm_chardev_ops = {
 	KVM_COMPAT(kvm_dev_ioctl),
 };
 
+/*
+ * 在以下使用kvm_dev:
+ *   - virt/kvm/kvm_main.c|4951| <<kvm_uevent_notify_change>> if (!kvm_dev.this_device || !kvm)
+ *   - virt/kvm/kvm_main.c|4992| <<kvm_uevent_notify_change>> kobject_uevent_env(&kvm_dev.this_device->kobj, KOBJ_CHANGE, env->envp);
+ *   - virt/kvm/kvm_main.c|5182| <<kvm_init>> r = misc_register(&kvm_dev);
+ *   - virt/kvm/kvm_main.c|5223| <<kvm_exit>> misc_deregister(&kvm_dev);
+ */
 static struct miscdevice kvm_dev = {
 	KVM_MINOR,
 	"kvm",
@@ -4760,8 +4925,28 @@ static int kvm_debugfs_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4876| <<kvm_stat_data_get>> r = kvm_get_stat_per_vm(stat_data->kvm,
+ *   - virt/kvm/kvm_main.c|4935| <<vm_stat_get>> kvm_get_stat_per_vm(kvm, offset, &tmp_val);
+ */
 static int kvm_get_stat_per_vm(struct kvm *kvm, size_t offset, u64 *val)
 {
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_vm_stat stat;
+	 *    -> ulong mmu_shadow_zapped;
+	 *    -> ulong mmu_pte_write;
+	 *    -> ulong mmu_pde_zapped;
+	 *    -> ulong mmu_flooded;
+	 *    -> ulong mmu_recycled;
+	 *    -> ulong mmu_cache_miss;
+	 *    -> ulong mmu_unsync;
+	 *    -> ulong remote_tlb_flush;
+	 *    -> ulong lpages;
+	 *    -> ulong nx_lpage_splits;
+	 *    -> ulong max_mmu_page_hash_collisions;
+	 */
 	*val = *(ulong *)((void *)kvm + offset);
 
 	return 0;
@@ -4781,6 +4966,10 @@ static int kvm_get_stat_per_vcpu(struct kvm *kvm, size_t offset, u64 *val)
 
 	*val = 0;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_stat stat;
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		*val += *(u64 *)((void *)vcpu + offset);
 
@@ -4926,6 +5115,10 @@ static int vcpu_stat_clear(void *_offset, u64 val)
 DEFINE_SIMPLE_ATTRIBUTE(vcpu_stat_fops, vcpu_stat_get, vcpu_stat_clear,
 			"%llu\n");
 
+/*
+ * 在以下使用stat_fops:
+ *   - virt/kvm/kvm_main.c|5062| <<kvm_init_debug>> stat_fops[p->kind]);
+ */
 static const struct file_operations *stat_fops[] = {
 	[KVM_STAT_VCPU] = &vcpu_stat_fops,
 	[KVM_STAT_VM]   = &vm_stat_fops,
@@ -4981,13 +5174,83 @@ static void kvm_uevent_notify_change(unsigned int type, struct kvm *kvm)
 	kfree(env);
 }
 
+/*
+ * struct kvm_stats_debugfs_item debugfs_entries[] = {
+ *		VCPU_STAT("pf_fixed", pf_fixed),
+ *		VCPU_STAT("pf_guest", pf_guest),
+ *		VCPU_STAT("tlb_flush", tlb_flush),
+ *		VCPU_STAT("invlpg", invlpg),
+ *		VCPU_STAT("exits", exits),
+ *		VCPU_STAT("io_exits", io_exits),
+ *		VCPU_STAT("mmio_exits", mmio_exits),
+ *		VCPU_STAT("signal_exits", signal_exits),
+ *		VCPU_STAT("irq_window", irq_window_exits),
+ *		VCPU_STAT("nmi_window", nmi_window_exits),
+ *		VCPU_STAT("halt_exits", halt_exits),
+ *		VCPU_STAT("halt_successful_poll", halt_successful_poll),
+ *		VCPU_STAT("halt_attempted_poll", halt_attempted_poll),
+ *		VCPU_STAT("halt_poll_invalid", halt_poll_invalid),
+ *		VCPU_STAT("halt_wakeup", halt_wakeup),
+ *		VCPU_STAT("hypercalls", hypercalls),
+ *		VCPU_STAT("request_irq", request_irq_exits),
+ *		VCPU_STAT("irq_exits", irq_exits),
+ *		VCPU_STAT("host_state_reload", host_state_reload),
+ *		VCPU_STAT("fpu_reload", fpu_reload),
+ *		VCPU_STAT("insn_emulation", insn_emulation),
+ *		VCPU_STAT("insn_emulation_fail", insn_emulation_fail),
+ *		VCPU_STAT("irq_injections", irq_injections),
+ *		VCPU_STAT("nmi_injections", nmi_injections),
+ *		VCPU_STAT("req_event", req_event),
+ *		VCPU_STAT("l1d_flush", l1d_flush),
+ *		VCPU_STAT("halt_poll_success_ns", halt_poll_success_ns),
+ *		VCPU_STAT("halt_poll_fail_ns", halt_poll_fail_ns),
+ *		VCPU_STAT("nested_run", nested_run),
+ *		VCPU_STAT("directed_yield_attempted", directed_yield_attempted),
+ *		VCPU_STAT("directed_yield_successful", directed_yield_successful),
+ *		VM_STAT("mmu_shadow_zapped", mmu_shadow_zapped),
+ *		VM_STAT("mmu_pte_write", mmu_pte_write),
+ *		VM_STAT("mmu_pde_zapped", mmu_pde_zapped),
+ *		VM_STAT("mmu_flooded", mmu_flooded),
+ *		VM_STAT("mmu_recycled", mmu_recycled),
+ *		VM_STAT("mmu_cache_miss", mmu_cache_miss),
+ *		VM_STAT("mmu_unsync", mmu_unsync),
+ *		VM_STAT("remote_tlb_flush", remote_tlb_flush),
+ *		VM_STAT("largepages", lpages, .mode = 0444),
+ *		VM_STAT("nx_largepages_splitted", nx_lpage_splits, .mode = 0444),
+ *		VM_STAT("max_mmu_page_hash_collisions", max_mmu_page_hash_collisions),
+ *		{ NULL }
+ * };
+ *
+ * called by:
+ *   - virt/kvm/kvm_main.c|5262| <<kvm_init>> kvm_init_debug();
+ */
 static void kvm_init_debug(void)
 {
 	struct kvm_stats_debugfs_item *p;
 
+	/*
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - arch/powerpc/kvm/book3s_hv.c|5072| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|860| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|5069| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|5074| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|5291| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
 
+	/*
+	 * 在以下使用kvm_debugfs_num_entries:
+	 *   - virt/kvm/kvm_main.c|840| <<kvm_destroy_vm_debugfs>> for (i = 0; i < kvm_debugfs_num_entries; i++)
+	 *   - virt/kvm/kvm_main.c|862| <<kvm_create_vm_debugfs>> kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
+	 *   - virt/kvm/kvm_main.c|5071| <<kvm_init_debug>> kvm_debugfs_num_entries = 0;
+	 *   - virt/kvm/kvm_main.c|5072| <<kvm_init_debug>> for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
+	 */
 	kvm_debugfs_num_entries = 0;
+	/*
+	 * debugfs_entries在上面注释拷贝了一份
+	 * 都是"struct kvm_stats_debugfs_item"类型
+	 */
 	for (p = debugfs_entries; p->name; ++p, kvm_debugfs_num_entries++) {
 		debugfs_create_file(p->name, KVM_DBGFS_GET_MODE(p),
 				    kvm_debugfs_dir, (void *)(long)p->offset,
@@ -5089,6 +5352,17 @@ static void check_processor_compat(void *data)
 	*c->ret = kvm_arch_check_processor_compat(c->opaque);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2156| <<arm_init>> int rc = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/mips/kvm/mips.c|1615| <<kvm_mips_init>> ret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/book3s.c|1037| <<kvmppc_book3s_init>> r = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500.c|533| <<kvmppc_e500_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/powerpc/kvm/e500mc.c|403| <<kvmppc_e500mc_init>> r = kvm_init(NULL, sizeof(struct kvmppc_vcpu_e500), 0, THIS_MODULE);
+ *   - arch/s390/kvm/kvm-s390.c|5060| <<kvm_s390_init>> return kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);
+ *   - arch/x86/kvm/svm/svm.c|4580| <<svm_init>> return kvm_init(&svm_init_ops, sizeof(struct vcpu_svm),
+ *   - arch/x86/kvm/vmx/vmx.c|8230| <<vmx_init>> r = kvm_init(&vmx_init_ops, sizeof(struct vcpu_vmx),
+ */
 int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		  struct module *module)
 {
-- 
2.17.1

