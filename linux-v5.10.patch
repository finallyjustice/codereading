From ba59c197a7472497d3a030886562deaa10e159ac Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 14 Feb 2021 18:20:51 -0800
Subject: [PATCH 1/1] linux v5.10

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/apic.h        |   3 +
 arch/x86/kernel/apic/apic.c        |  18 ++
 arch/x86/kernel/apic/x2apic_phys.c |   7 +
 arch/x86/kernel/pci-swiotlb.c      |  43 +++
 arch/x86/kvm/lapic.c               |  10 +
 arch/x86/mm/mem_encrypt.c          |   8 +
 arch/x86/xen/mmu_pv.c              |   5 +
 arch/x86/xen/pci-swiotlb-xen.c     |   7 +
 block/blk-core.c                   |  24 ++
 block/blk-mq-debugfs.c             |   3 +
 block/blk-mq-tag.c                 | 137 +++++++++
 block/blk-mq-tag.h                 | 107 +++++++
 block/blk-mq.c                     | 171 +++++++++++
 block/blk-mq.h                     |  49 ++++
 block/blk-stat.c                   | 147 ++++++++++
 block/blk-stat.h                   |  24 ++
 block/blk-wbt.c                    |   5 +
 block/genhd.c                      |   4 +
 block/partitions/core.c            |  70 +++++
 drivers/block/null_blk_main.c      |  13 +
 drivers/block/xen-blkback/common.h |   7 +
 drivers/block/xen-blkback/xenbus.c |  23 ++
 drivers/nvme/host/trace.c          |   4 +
 drivers/scsi/scsi_lib.c            |  13 +
 drivers/vhost/scsi.c               |  27 ++
 drivers/xen/mcelog.c               |  15 +
 drivers/xen/swiotlb-xen.c          | 147 ++++++++++
 include/linux/blk-mq.h             |  75 +++++
 include/linux/blkdev.h             |  19 ++
 include/linux/dma-direct.h         |  10 +
 include/linux/dma-map-ops.h        |  13 +
 include/linux/iommu-helper.h       |  14 +
 include/linux/swiotlb.h            |  42 +++
 kernel/dma/direct.c                |   4 +
 kernel/dma/direct.h                |  22 ++
 kernel/dma/mapping.c               |  43 +++
 kernel/dma/swiotlb.c               | 447 +++++++++++++++++++++++++++++
 kernel/smp.c                       |   5 +
 kernel/time/tick-oneshot.c         |  12 +
 mm/internal.h                      |   9 +
 mm/page_alloc.c                    |  31 ++
 mm/util.c                          |   6 +
 mm/vmalloc.c                       |  18 ++
 43 files changed, 1861 insertions(+)

diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 4e3099d9ae62..f8a003e03b41 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -405,6 +405,9 @@ static inline void apic_write(u32 reg, u32 val)
 
 static inline void apic_eoi(void)
 {
+	/*
+	 * apic_x2apic_phys用的应该比较多
+	 */
 	apic->eoi_write(APIC_EOI, APIC_EOI_ACK);
 }
 
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index b3eef1d5c903..23dab6e904a3 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -604,6 +604,13 @@ static __init bool apic_validate_deadline_timer(void)
  * Setup the local APIC timer for this CPU. Copy the initialized values
  * of the boot CPU and register the clock event in the framework.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/apic.c|1019| <<setup_boot_APIC_clock>> setup_APIC_timer();
+ *   - arch/x86/kernel/apic/apic.c|1027| <<setup_boot_APIC_clock>> setup_APIC_timer();
+ *   - arch/x86/kernel/apic/apic.c|1039| <<setup_boot_APIC_clock>> setup_APIC_timer();
+ *   - arch/x86/kernel/apic/apic.c|1045| <<setup_secondary_APIC_clock>> setup_APIC_timer();
+ */
 static void setup_APIC_timer(void)
 {
 	struct clock_event_device *levt = this_cpu_ptr(&lapic_events);
@@ -1049,8 +1056,19 @@ void setup_secondary_APIC_clock(void)
 /*
  * The guts of the apic timer interrupt
  */
+/*
+ * [0] local_apic_timer_interrupt
+ * [0] __sysvec_apic_timer_interrupt
+ * [0] sysvec_apic_timer_interrupt
+ * [0] asm_sysvec_apic_timer_interrupt
+ */
 static void local_apic_timer_interrupt(void)
 {
+	/*
+	 * clock_event_device.set_next_event = lapic_next_deadline()用的比较多
+	 * clock_event_device.name = "lapic-deadline"
+	 * clock_event_device.state_use_accessors = CLOCK_EVT_STATE_ONESHOT
+	 */
 	struct clock_event_device *evt = this_cpu_ptr(&lapic_events);
 
 	/*
diff --git a/arch/x86/kernel/apic/x2apic_phys.c b/arch/x86/kernel/apic/x2apic_phys.c
index bc9693841353..c088d41242cd 100644
--- a/arch/x86/kernel/apic/x2apic_phys.c
+++ b/arch/x86/kernel/apic/x2apic_phys.c
@@ -140,6 +140,13 @@ void x2apic_send_IPI_self(int vector)
 	apic_write(APIC_SELF_IPI, vector);
 }
 
+/*
+ * 在以下使用apic_x2apic_phys:
+ *   - arch/x86/kernel/apic/x2apic_phys.c|190| <<global>> apic_driver(apic_x2apic_phys);
+ *   - arch/x86/kernel/apic/x2apic_phys.c|95| <<x2apic_phys_probe>> return apic == &apic_x2apic_phys;
+ *
+ * 这个用的比较多
+ */
 static struct apic apic_x2apic_phys __ro_after_init = {
 
 	.name				= "physical x2apic",
diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c
index c2cfa5e7c152..3d488ee53159 100644
--- a/arch/x86/kernel/pci-swiotlb.c
+++ b/arch/x86/kernel/pci-swiotlb.c
@@ -14,6 +14,21 @@
 #include <asm/xen/swiotlb-xen.h>
 #include <asm/iommu_table.h>
 
+/*
+ * 在以下使用swiotlb:
+ *   - arch/x86/kernel/amd_gart_64.c|811| <<gart_iommu_init>> swiotlb = 0;
+ *   - arch/x86/kernel/pci-dma.c|105| <<iommu_setup>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|28| <<pci_swiotlb_detect_override>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|30| <<pci_swiotlb_detect_override>> return swiotlb;
+ *   - arch/x86/kernel/pci-swiotlb.c|45| <<pci_swiotlb_detect_4gb>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|53| <<pci_swiotlb_detect_4gb>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|55| <<pci_swiotlb_detect_4gb>> return swiotlb;
+ *   - arch/x86/kernel/pci-swiotlb.c|64| <<pci_swiotlb_init>> if (swiotlb)
+ *   - arch/x86/kernel/pci-swiotlb.c|71| <<pci_swiotlb_late_init>> if (!swiotlb)
+ *   - arch/x86/xen/pci-swiotlb-xen.c|39| <<pci_xen_swiotlb_detect>> if (xen_initial_domain() || swiotlb || swiotlb_force == SWIOTLB_FORCE)
+ *   - arch/x86/xen/pci-swiotlb-xen.c|45| <<pci_xen_swiotlb_detect>> swiotlb = 0;
+ *   - drivers/iommu/amd/iommu.c|2364| <<amd_iommu_init_dma_ops>> swiotlb = (iommu_default_passthrough() || sme_me_mask) ? 1 : 0;
+ */
 int swiotlb __read_mostly;
 
 /*
@@ -22,6 +37,13 @@ int swiotlb __read_mostly;
  * This returns non-zero if we are forced to use swiotlb (by the boot
  * option).
  */
+/*
+ * [0] pci_swiotlb_detect_override
+ * [0] pci_iommu_alloc
+ * [0] mem_init
+ * [0] start_kernel
+ * [0] secondary_startup_64_no_verify
+ */
 int __init pci_swiotlb_detect_override(void)
 {
 	if (swiotlb_force == SWIOTLB_FORCE)
@@ -59,18 +81,39 @@ IOMMU_INIT(pci_swiotlb_detect_4gb,
 	   pci_swiotlb_init,
 	   pci_swiotlb_late_init);
 
+/*
+ * [0] pci_swiotlb_init
+ * [0] pci_iommu_alloc
+ * [0] mem_init
+ * [0] start_kernel
+ * [0] secondary_startup_64_no_verify
+ */
 void __init pci_swiotlb_init(void)
 {
 	if (swiotlb)
 		swiotlb_init(0);
 }
 
+/*
+ * [0] pci_swiotlb_late_init
+ * [0] pci_iommu_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * 被上面IOMMU_INIT_FINISH()和IOMMU_INIT()使用
+ */
 void __init pci_swiotlb_late_init(void)
 {
 	/* An IOMMU turned us off. */
 	if (!swiotlb)
 		swiotlb_exit();
 	else {
+		/*
+		 * [    0.587457] PCI-DMA: Using software bounce buffering for IO (SWIOTLB)
+		 * [    0.588574] software IO TLB: mapped [mem 0x00000000bbfdd000-0x00000000bffdd000] (64MB)
+		 */
 		printk(KERN_INFO "PCI-DMA: "
 		       "Using software bounce buffering for IO (SWIOTLB)\n");
 		swiotlb_print_info();
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 86c33d53c90a..2e8ca333c28c 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1610,6 +1610,12 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1639| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|1645| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2497| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -2413,6 +2419,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9862| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c
index bc0833713be9..0e0a1a0f860c 100644
--- a/arch/x86/mm/mem_encrypt.c
+++ b/arch/x86/mm/mem_encrypt.c
@@ -179,6 +179,10 @@ void __init sme_map_bootdata(char *real_mode_data)
 	__sme_early_map_unmap_mem(__va(cmdline_paddr), COMMAND_LINE_SIZE, true);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/head64.c|491| <<x86_64_start_kernel>> sme_early_init();
+ */
 void __init sme_early_init(void)
 {
 	unsigned int i;
@@ -434,6 +438,10 @@ static void print_mem_encrypt_feature_info(void)
 }
 
 /* Architecture __weak replacement functions */
+/*
+ * called by:
+ *   - init/main.c|1008| <<start_kernel>> mem_encrypt_init();
+ */
 void __init mem_encrypt_init(void)
 {
 	if (!sme_me_mask)
diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c
index cf2ade864c30..799f8b07949a 100644
--- a/arch/x86/xen/mmu_pv.c
+++ b/arch/x86/xen/mmu_pv.c
@@ -2265,6 +2265,11 @@ static int xen_exchange_memory(unsigned long extents_in, unsigned int order_in,
 	return success;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|218| <<xen_swiotlb_fixup>> rc = xen_create_contiguous_region(
+ *   - drivers/xen/swiotlb-xen.c|415| <<xen_swiotlb_alloc_coherent>> if (xen_create_contiguous_region(phys, order,
+ */
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
 				 unsigned int address_bits,
 				 dma_addr_t *dma_handle)
diff --git a/arch/x86/xen/pci-swiotlb-xen.c b/arch/x86/xen/pci-swiotlb-xen.c
index 19ae3e4fe4e9..ee57436bda52 100644
--- a/arch/x86/xen/pci-swiotlb-xen.c
+++ b/arch/x86/xen/pci-swiotlb-xen.c
@@ -18,6 +18,13 @@
 #endif
 #include <linux/export.h>
 
+/*
+ * 在以下使用xen_swiotlb:
+ *   - arch/x86/xen/pci-swiotlb-xen.c|40| <<pci_xen_swiotlb_detect>> xen_swiotlb = 1;
+ *   - arch/x86/xen/pci-swiotlb-xen.c|56| <<pci_xen_swiotlb_detect>> return xen_swiotlb;
+ *   - arch/x86/xen/pci-swiotlb-xen.c|61| <<pci_xen_swiotlb_init>> if (xen_swiotlb) {
+ *   - arch/x86/xen/pci-swiotlb-xen.c|76| <<pci_xen_swiotlb_init_late>> if (xen_swiotlb)
+ */
 int xen_swiotlb __read_mostly;
 
 /*
diff --git a/block/blk-core.c b/block/blk-core.c
index 2db8bda43b6e..9fb9b06920ba 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -514,6 +514,30 @@ static void blk_timeout_work(struct work_struct *work)
 {
 }
 
+/*
+ * called by:
+ *   - arch/m68k/emu/nfblock.c|122| <<nfhd_init_one>> dev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - arch/xtensa/platforms/iss/simdisk.c|269| <<simdisk_setup>> dev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - block/blk-mq.c|3138| <<blk_mq_init_queue_data>> uninit_q = blk_alloc_queue(set->numa_node);
+ *   - drivers/block/brd.c|385| <<brd_alloc>> brd->brd_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/drbd/drbd_main.c|2746| <<drbd_create_device>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1845| <<null_add_dev>> nullb->q = blk_alloc_queue(dev->home_node);
+ *   - drivers/block/pktcdvd.c|2687| <<pkt_setup_dev>> disk->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/ps3vram.c|739| <<ps3vram_probe>> queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/rsxx/dev.c|239| <<rsxx_setup_dev>> card->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/umem.c|890| <<mm_pci_probe>> card->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/zram/zram_drv.c|1906| <<zram_add>> queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|379| <<nvm_create_tgt>> tqueue = blk_alloc_queue(dev->q->node);
+ *   - drivers/md/bcache/super.c|929| <<bcache_device_init>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/md/dm.c|1813| <<alloc_dev>> md->queue = blk_alloc_queue(numa_node_id);
+ *   - drivers/md/md.c|5707| <<md_alloc>> mddev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/blk.c|253| <<nsblk_attach_disk>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/btt.c|1524| <<btt_blk_init>> btt->btt_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/pmem.c|424| <<pmem_attach_disk>> q = blk_alloc_queue(dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|377| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue(ctrl->numa_node);
+ *   - drivers/s390/block/dcssblk.c|654| <<dcssblk_add_store>> dev_info->dcssblk_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/s390/block/xpram.c|347| <<xpram_setup_blkdev>> xpram_queues[i] = blk_alloc_queue(NUMA_NO_NODE);
+ */
 struct request_queue *blk_alloc_queue(int node_id)
 {
 	struct request_queue *q;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index 3094542e12ae..7886fbfb2fe0 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -164,6 +164,9 @@ static ssize_t queue_state_write(void *data, const char __user *buf,
 	if (copy_from_user(opbuf, buf, count))
 		return -EFAULT;
 	op = strstrip(opbuf);
+	/*
+	 * echo "kick" > /sys/kernel/debug/block/sda/state
+	 */
 	if (strcmp(op, "run") == 0) {
 		blk_mq_run_hw_queues(q, true);
 	} else if (strcmp(op, "start") == 0) {
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 9c92053e704d..a8e655eefb15 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,12 +21,22 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|132| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (blk_mq_is_sbitmap_shared(hctx->flags)) {
 		struct request_queue *q = hctx->queue;
 		struct blk_mq_tag_set *set = q->tag_set;
 
+		/*
+		 * 在以下使用QUEUE_FLAG_HCTX_ACTIVE:
+		 *   - block/blk-mq-tag.c|30| <<__blk_mq_tag_busy>> if (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) &&
+		 *   - block/blk-mq-tag.c|31| <<__blk_mq_tag_busy>> !test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))
+		 *   - block/blk-mq-tag.c|63| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(QUEUE_FLAG_HCTX_ACTIVE,
+		 */
 		if (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) &&
 		    !test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))
 			atomic_inc(&set->active_queues_shared_sbitmap);
@@ -42,6 +52,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|73| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|263| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(tags->bitmap_tags);
@@ -53,6 +68,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|163| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -171,6 +190,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	 * Give up this allocation if the hctx is inactive.  The caller will
 	 * retry on an active hctx.
 	 */
+	/*
+	 * 在以下使用BLK_MQ_S_INACTIVE:
+	 *   - block/blk-mq-tag.c|193| <<blk_mq_get_tag>> if (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {
+	 *   - block/blk-mq.c|2507| <<blk_mq_hctx_notify_offline>> set_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 *   - block/blk-mq.c|2530| <<blk_mq_hctx_notify_online>> clear_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 */
 	if (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {
 		blk_mq_put_tag(tags, data->ctx, tag + tag_offset);
 		return BLK_MQ_NO_TAG;
@@ -178,6 +203,13 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by;
+ *   - block/blk-mq-tag.c|194| <<blk_mq_get_tag>> blk_mq_put_tag(tags, data->ctx, tag + tag_offset);
+ *   - block/blk-mq.c|497| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|499| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|252| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,
 		    unsigned int tag)
 {
@@ -199,6 +231,10 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|266| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_iter_data *iter_data = data;
@@ -234,6 +270,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|471| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|472| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -258,6 +299,10 @@ struct bt_tags_iter_data {
 #define BT_TAG_ITER_STARTED		(1 << 1)
 #define BT_TAG_ITER_STATIC_RQS		(1 << 2)
 
+/*
+ * 在以下使用bt_tags_iter():
+ *   - block/blk-mq-tag.c|346| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -296,6 +341,11 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @data:	Will be passed as second argument to @fn.
  * @flags:	BT_TAG_ITER_*
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|364| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, tags->breserved_tags, fn, priv,
+ *   - block/blk-mq-tag.c|366| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, tags->bitmap_tags, fn, priv, flags);
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, unsigned int flags)
 {
@@ -310,6 +360,11 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|384| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+ *   - block/blk-mq-tag.c|404| <<blk_mq_tagset_busy_iter>> __blk_mq_all_tag_iter(tagset->tags[i], fn, priv,
+ */
 static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv, unsigned int flags)
 {
@@ -333,6 +388,10 @@ static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
  *
  * Caller has to pass the tag map from which requests are allocated.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2481| <<blk_mq_hctx_has_requests>> blk_mq_all_tag_iter(tags, blk_mq_has_request, &data);
+ */
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv)
 {
@@ -349,6 +408,25 @@ void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * 部分调用blk_mq_tagset_busy_iter()的例子:
+ *   - block/blk-mq-debugfs.c|418| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - block/blk-mq-tag.c|445| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset,
+ *   - drivers/block/nbd.c|827| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/nvme/host/fc.c|2466| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|2489| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|2462| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2463| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1016| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|1034| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1892| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1912| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|410| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|420| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/scsi/hosts.c|581| <<scsi_host_busy>> blk_mq_tagset_busy_iter(&shost->tag_set,
+ *   - drivers/scsi/hosts.c|680| <<scsi_host_complete_all_commands>> blk_mq_tagset_busy_iter(&shost->tag_set, complete_all_cmds_iter,
+ *   - drivers/scsi/hosts.c|717| <<bool>> blk_mq_tagset_busy_iter(&shost->tag_set, __scsi_host_busy_iter_fn,
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -362,6 +440,10 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 }
 EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
 
+/*
+ * 在以下使用blk_mq_tagset_count_completed_rqs():
+ *   - block/blk-mq-tag.c|446| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset, blk_mq_tagset_count_completed_rqs, &count);
+ */
 static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
 		void *data, bool reserved)
 {
@@ -379,6 +461,19 @@ static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
  *
  * Note: This function has to be run after all IO queues are shutdown
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2468| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|2491| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|2464| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->tagset);
+ *   - drivers/nvme/host/pci.c|2465| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1018| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1036| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/tcp.c|1894| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1914| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->tagset);
+ *   - drivers/nvme/target/loop.c|412| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/target/loop.c|422| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ */
 void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)
 {
 	while (true) {
@@ -407,6 +502,13 @@ EXPORT_SYMBOL(blk_mq_tagset_wait_completed_request);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|118| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|128| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|894| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|999| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -422,6 +524,12 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		return;
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 在以下设置hctx->tags:
+		 *   - block/blk-mq.c|2644| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+		 *   - block/blk-mq.c|2874| <<blk_mq_map_swqueue>> hctx->tags = NULL;
+		 *   - block/blk-mq.c|2878| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+		 */
 		struct blk_mq_tags *tags = hctx->tags;
 
 		/*
@@ -438,6 +546,13 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|497| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->__bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|499| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->__breserved_tags, tags->nr_reserved_tags,
+ *   - block/blk-mq-tag.c|523| <<blk_mq_init_shared_sbitmap>> if (bt_alloc(&set->__bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|525| <<blk_mq_init_shared_sbitmap>> if (bt_alloc(&set->__breserved_tags, set->reserved_tags,
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -445,6 +560,13 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * 在以下使用blk_mq_init_bitmap_tags():
+ *   - block/blk-mq-tag.c|523| <<blk_mq_init_tags>> if (blk_mq_init_bitmap_tags(tags, node, alloc_policy) < 0) {
+ *
+ * 核心思想是分配blk_mq_tags->__bitmap_tags
+ * 然后让blk_mq_tags->bitmap_tags(指针)指向blk_mq_tags->__bitmap_tags
+ */
 static int blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 				   int node, int alloc_policy)
 {
@@ -466,6 +588,10 @@ static int blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3507| <<blk_mq_alloc_tag_set>> if (blk_mq_init_shared_sbitmap(set, set->flags)) {
+ */
 int blk_mq_init_shared_sbitmap(struct blk_mq_tag_set *set, unsigned int flags)
 {
 	unsigned int depth = set->queue_depth - set->reserved_tags;
@@ -498,6 +624,10 @@ void blk_mq_exit_shared_sbitmap(struct blk_mq_tag_set *set)
 	sbitmap_queue_free(&set->__breserved_tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2327| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node, flags);
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, unsigned int flags)
@@ -517,9 +647,16 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	tags->nr_tags = total_tags;
 	tags->nr_reserved_tags = reserved_tags;
 
+	/*
+	 * 这里可能直接退出
+	 */
 	if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
 		return tags;
 
+	/*
+	 * 核心思想是分配blk_mq_tags->__bitmap_tags
+	 * 然后让blk_mq_tags->bitmap_tags(指针)指向blk_mq_tags->__bitmap_tags
+	 */
 	if (blk_mq_init_bitmap_tags(tags, node, alloc_policy) < 0) {
 		kfree(tags);
 		return NULL;
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 7d3e6b333a4a..01b1edeced9c 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -6,14 +6,63 @@
  * Tag address space map.
  */
 struct blk_mq_tags {
+	/*
+	 * 在以下修改blk_mq_tags->nr_tags:
+	 *   - block/blk-mq-tag.c|537| <<blk_mq_init_tags>> tags->nr_tags = total_tags;
+	 * 在以下使用blk_mq_tags->nr_tags:
+	 *   - block/blk-mq-debugfs.c|450| <<blk_mq_debugfs_tags_show>> seq_printf(m, "nr_tags=%u\n", tags->nr_tags);
+	 *   - block/blk-mq-sysfs.c|148| <<blk_mq_hw_sysfs_nr_tags_show>> return sprintf(page, "%u\n", hctx->tags->nr_tags);
+	 *   - block/blk-mq-tag.c|192| <<blk_mq_put_tag>> BUG_ON(real_tag >= tags->nr_tags);
+	 *   - block/blk-mq-tag.c|463| <<blk_mq_init_bitmap_tags>> unsigned int depth = tags->nr_tags - tags->nr_reserved_tags;
+	 *   - block/blk-mq-tag.c|579| <<blk_mq_tag_update_depth>> if (tdepth > tags->nr_tags) {
+	 *   - block/blk-mq.c|860| <<blk_mq_tag_to_rq>> if (tag < tags->nr_tags) {
+	 *   - block/blk-mq.c|2282| <<blk_mq_free_rqs>> for (i = 0; i < tags->nr_tags; i++) {
+	 */
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用blk_mq_tags->active_queues:
+	 *   - block/blk-mq-debugfs.c|453| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|36| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|70| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq.h|312| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
+	/*
+	 * 在以下使用blk_mq_tags->bitmap_tags:
+	 *   - block/bfq-iosched.c|6368| <<bfq_depth_updated>> min_shallow = bfq_update_depths(bfqd, tags->bitmap_tags);
+	 *   - block/bfq-iosched.c|6369| <<bfq_depth_updated>> sbitmap_queue_min_shallow_depth(tags->bitmap_tags, min_shallow);
+	 *   - block/blk-mq-debugfs.c|456| <<blk_mq_debugfs_tags_show>> sbitmap_queue_show(tags->bitmap_tags, m);
+	 *   - block/blk-mq-debugfs.c|491| <<hctx_tags_bitmap_show>> sbitmap_bitmap_show(&hctx->tags->bitmap_tags->sb, m);
+	 *   - block/blk-mq-debugfs.c|525| <<hctx_sched_tags_bitmap_show>> sbitmap_bitmap_show(&hctx->sched_tags->bitmap_tags->sb, m);
+	 *   - block/blk-mq-tag.c|47| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(tags->bitmap_tags);
+	 *   - block/blk-mq-tag.c|106| <<blk_mq_get_tag>> bt = tags->bitmap_tags;
+	 *   - block/blk-mq-tag.c|154| <<blk_mq_get_tag>> bt = tags->bitmap_tags;
+	 *   - block/blk-mq-tag.c|188| <<blk_mq_put_tag>> sbitmap_queue_clear(tags->bitmap_tags, real_tag, ctx->cpu);
+	 *   - block/blk-mq-tag.c|321| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, tags->bitmap_tags, fn, priv, flags);
+	 *   - block/blk-mq-tag.c|436| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, tags->bitmap_tags, fn, priv, false);
+	 *   - block/blk-mq-tag.c|460| <<blk_mq_init_bitmap_tags>> tags->bitmap_tags = &tags->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|485| <<blk_mq_init_shared_sbitmap>> tags->bitmap_tags = &set->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|533| <<blk_mq_free_tags>> sbitmap_queue_free(tags->bitmap_tags);
+	 *   - block/blk-mq-tag.c|587| <<blk_mq_tag_update_depth>> sbitmap_queue_resize(tags->bitmap_tags,
+	 *   - block/blk-mq.c|1099| <<__blk_mq_get_driver_tag>> struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
+	 *   - block/blk-mq.c|1149| <<blk_mq_dispatch_wake>> sbq = hctx->tags->bitmap_tags;
+	 *   - block/blk-mq.c|1167| <<blk_mq_mark_tag_wait>> struct sbitmap_queue *sbq = hctx->tags->bitmap_tags;
+	 *   - block/kyber-iosched.c|362| <<kyber_sched_tags_shift>> return q->queue_hw_ctx[0]->sched_tags->bitmap_tags->sb.shift;
+	 *   - block/kyber-iosched.c|505| <<kyber_init_hctx>> sbitmap_queue_min_shallow_depth(hctx->sched_tags->bitmap_tags,
+	 */
 	struct sbitmap_queue *bitmap_tags;
 	struct sbitmap_queue *breserved_tags;
 
+	/*
+	 * 在以下使用blk_mq_tags->__bitmap_tags:
+	 *   - block/blk-mq-tag.c|454| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->__bitmap_tags, depth, round_robin, node))
+	 *   - block/blk-mq-tag.c|460| <<blk_mq_init_bitmap_tags>> tags->bitmap_tags = &tags->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|465| <<blk_mq_init_bitmap_tags>> sbitmap_queue_free(&tags->__bitmap_tags);
+	 *   - block/blk-mq-tag.c|485| <<blk_mq_init_shared_sbitmap>> tags->bitmap_tags = &set->__bitmap_tags;
+	 */
 	struct sbitmap_queue __bitmap_tags;
 	struct sbitmap_queue __breserved_tags;
 
@@ -46,6 +95,12 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|122| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|169| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1190| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -63,22 +118,74 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|376| <<__blk_mq_alloc_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|472| <<blk_mq_alloc_request_hctx>> blk_mq_tag_busy(data.hctx);
+ *   - block/blk-mq.c|1103| <<__blk_mq_get_driver_tag>> blk_mq_tag_busy(rq->mq_hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
 		return false;
 
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1009| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2580| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
 		return;
 
 	__blk_mq_tag_idle(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|184| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1105| <<__blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 55bcee5dc032..9f850b3fb7f1 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -253,6 +253,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|354| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -539,6 +543,12 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 	if (blk_mq_need_time_stamp(rq))
 		now = ktime_get_ns();
 
+	/*
+	 * 在以下使用RQF_STATS:
+	 *   - block/blk-mq.c|276| <<blk_mq_need_time_stamp>> return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
+	 *   - block/blk-mq.c|546| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+	 *   - block/blk-mq.c|743| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+	 */
 	if (rq->rq_flags & RQF_STATS) {
 		blk_mq_poll_stats_start(rq->q);
 		blk_stat_add(rq, now);
@@ -587,6 +597,11 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|649| <<__blk_mq_complete_request_remote>> blk_mq_trigger_softirq(rq);
+ *   - block/blk-mq.c|691| <<blk_mq_complete_request_remote>> blk_mq_trigger_softirq(rq);
+ */
 static void blk_mq_trigger_softirq(struct request *rq)
 {
 	struct list_head *list;
@@ -733,9 +748,22 @@ void blk_mq_start_request(struct request *rq)
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|746| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|188| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|200| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|234| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
 		rq->io_start_time_ns = ktime_get_ns();
 		rq->stats_sectors = blk_rq_sectors(rq);
+		/*
+		 * 在以下使用RQF_STATS:
+		 *   - block/blk-mq.c|276| <<blk_mq_need_time_stamp>> return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
+		 *   - block/blk-mq.c|546| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+		 *   - block/blk-mq.c|743| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+		 */
 		rq->rq_flags |= RQF_STATS;
 		rq_qos_issue(q, rq);
 	}
@@ -841,6 +869,14 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 		blk_mq_kick_requeue_list(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|172| <<queue_state_write>> blk_mq_kick_requeue_list(q);
+ *   - block/blk-mq.c|845| <<blk_mq_add_to_requeue_list>> blk_mq_kick_requeue_list(q);
+ *   - drivers/block/xen-blkfront.c|2067| <<blkif_recover>> blk_mq_kick_requeue_list(info->rq);
+ *   - drivers/md/dm-rq.c|68| <<dm_start_queue>> blk_mq_kick_requeue_list(q);
+ *   - drivers/s390/block/scm_blk.c|247| <<scm_request_requeue>> blk_mq_kick_requeue_list(bdev->rq);
+ */
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
 	kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
@@ -1094,6 +1130,10 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1129| <<blk_mq_get_driver_tag>> if (rq->tag == BLK_MQ_NO_TAG && !__blk_mq_get_driver_tag(rq))
+ */
 static bool __blk_mq_get_driver_tag(struct request *rq)
 {
 	struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
@@ -1118,6 +1158,14 @@ static bool __blk_mq_get_driver_tag(struct request *rq)
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1187| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1213| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1309| <<blk_mq_prep_dispatch_rq>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1383| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|2066| <<__blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ */
 static bool blk_mq_get_driver_tag(struct request *rq)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1640,6 +1688,24 @@ EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
  * pending requests to be sent. If this is true, run the queue to send requests
  * to hardware.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|343| <<blk_mq_sched_dispatch_requests>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|467| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|501| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|145| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1158| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1479| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1684| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1768| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1788| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1867| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2209| <<blk_mq_submit_bio>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2571| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - drivers/block/rnbd/rnbd-clt.c|166| <<rnbd_clt_dev_requeue>> blk_mq_run_hw_queue(q->hctx, true);
+ */
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1668,6 +1734,20 @@ EXPORT_SYMBOL(blk_mq_run_hw_queue);
  * @q: Pointer to the request queue to run.
  * @async: If we want to run the queue asynchronously.
  */
+/*
+ * 部分调用blk_mq_run_hw_queues()的例子:
+ *   - block/bfq-iosched.c|426| <<bfq_schedule_dispatch>> blk_mq_run_hw_queues(bfqd->queue, true);
+ *   - block/blk-mq-debugfs.c|168| <<queue_state_write>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|140| <<blk_freeze_queue_start>> blk_mq_run_hw_queues(q, false);
+ *   - block/blk-mq.c|252| <<blk_mq_unquiesce_queue>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|820| <<blk_mq_requeue_work>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/md/dm-table.c|2015| <<dm_table_run_md_queue_async>> blk_mq_run_hw_queues(t->md->queue, true);
+ *   - drivers/scsi/scsi_lib.c|335| <<scsi_kick_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|487| <<scsi_run_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|557| <<scsi_run_queue_async>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_sysfs.c|814| <<store_state_field>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_transport_fc.c|3676| <<fc_bsg_goose_queue>> blk_mq_run_hw_queues(q, true);
+ */
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1785,6 +1865,14 @@ void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL_GPL(blk_mq_start_stopped_hw_queue);
 
+/*
+ * 部分调用blk_mq_start_stopped_hw_queues()的例子:
+ *   - block/blk-mq-debugfs.c|170| <<queue_state_write>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/null_blk_main.c|1227| <<null_restart_queue_async>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/virtio_blk.c|199| <<virtblk_done>> blk_mq_start_stopped_hw_queues(vblk->disk->queue, true);
+ *   - drivers/block/xen-blkfront.c|1231| <<kick_pending_request_queues_locked>> blk_mq_start_stopped_hw_queues(rinfo->dev_info->rq, true);
+ *   - drivers/block/xen-blkfront.c|2066| <<blkif_recover>> blk_mq_start_stopped_hw_queues(info->rq, true);
+ */
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2311,6 +2399,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags, unsigned int flags)
 	blk_mq_free_tags(tags, flags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|528| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|569| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2757| <<__blk_mq_alloc_map_and_request>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2887,6 +2981,13 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2912| <<blk_mq_update_tag_set_shared>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2948| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ *
+ * 对于request_queue的每一个hctx->flags, 设置或者取消BLK_MQ_F_TAG_QUEUE_SHARED
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2900,6 +3001,14 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2932| <<blk_mq_del_queue_tag_set>> blk_mq_update_tag_set_shared(set, false);
+ *   - block/blk-mq.c|2950| <<blk_mq_add_queue_tag_set>> blk_mq_update_tag_set_shared(set, true);
+ *
+ * 对于tagset的每一个request_queue的每一个hctx->flags
+ * 根据参数shared"设置"或者"取消"BLK_MQ_F_TAG_QUEUE_SHARED
+ */
 static void blk_mq_update_tag_set_shared(struct blk_mq_tag_set *set,
 					 bool shared)
 {
@@ -2909,6 +3018,9 @@ static void blk_mq_update_tag_set_shared(struct blk_mq_tag_set *set,
 
 	list_for_each_entry(q, &set->tag_list, tag_set_list) {
 		blk_mq_freeze_queue(q);
+		/*
+		 * 对于request_queue的每一个hctx->flags, 设置或者取消BLK_MQ_F_TAG_QUEUE_SHARED
+		 */
 		queue_set_hctx_shared(q, shared);
 		blk_mq_unfreeze_queue(q);
 	}
@@ -2924,12 +3036,20 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 		/* just transitioned to unshared */
 		set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
 		/* update existing queue */
+		/*
+		 * 对于tagset的每一个request_queue的每一个hctx->flags
+		 * 根据参数shared"设置"或者"取消"BLK_MQ_F_TAG_QUEUE_SHARED
+		 */
 		blk_mq_update_tag_set_shared(set, false);
 	}
 	mutex_unlock(&set->tag_list_lock);
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3239| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2938,12 +3058,38 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 	/*
 	 * Check to see if we're transitioning to shared (from 1 to 2 queues).
 	 */
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	if (!list_empty(&set->tag_list) &&
 	    !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
 		set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
 		/* update existing queue */
+		/*
+		 * 对于tagset的每一个request_queue的每一个hctx->flags
+		 * 根据参数shared"设置"或者"取消"BLK_MQ_F_TAG_QUEUE_SHARED
+		 */
 		blk_mq_update_tag_set_shared(set, true);
 	}
+	/*
+	 * queue_set_hctx_shared():
+	 * 对于request_queue的每一个hctx->flags, 设置或者取消BLK_MQ_F_TAG_QUEUE_SHARED
+	 */
 	if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
 		queue_set_hctx_shared(q, true);
 	list_add_tail(&q->tag_set_list, &set->tag_list);
@@ -3181,6 +3327,11 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3147| <<blk_mq_init_queue_data>> q = blk_mq_init_allocated_queue(set, uninit_q, false);
+ *   - drivers/md/dm-rq.c|560| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue, true);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q,
 						  bool elevator_init)
@@ -3452,6 +3603,9 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (blk_mq_is_sbitmap_shared(set->flags)) {
 		atomic_set(&set->active_queues_shared_sbitmap, 0);
 
+		/*
+		 * 只在此处调用blk_mq_init_shared_sbitmap()
+		 */
 		if (blk_mq_init_shared_sbitmap(set, set->flags)) {
 			ret = -ENOMEM;
 			goto out_free_mq_rq_maps;
@@ -3689,6 +3843,10 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3871| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
@@ -3698,6 +3856,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|553| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3722,6 +3884,10 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3913| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, rq);
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct request *rq)
 {
@@ -3910,6 +4076,11 @@ static int __init blk_mq_init(void)
 
 	for_each_possible_cpu(i)
 		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+	/*
+	 * 在以下使用BLOCK_SOFTIRQ:
+	 *   - block/blk-mq.c|615| <<blk_mq_trigger_softirq>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-mq.c|628| <<blk_softirq_cpu_dead>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 */
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
diff --git a/block/blk-mq.h b/block/blk-mq.h
index a52703c98b77..224e9b306513 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -151,6 +151,14 @@ struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
 	blk_mq_req_flags_t flags;
+	/*
+	 * 在以下使用blk_mq_alloc_data->shallow_depth:
+	 *   - lock/bfq-iosched.c|542| <<bfq_limit_depth>> data->shallow_depth =
+	 *   - block/bfq-iosched.c|547| <<bfq_limit_depth>> data->shallow_depth);
+	 *   - block/blk-mq-tag.c|102| <<__blk_mq_get_tag>> if (data->shallow_depth)
+	 *   - block/blk-mq-tag.c|103| <<__blk_mq_get_tag>> return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
+	 *   - block/kyber-iosched.c|561| <<kyber_limit_depth>> data->shallow_depth = kqd->async_depth;
+	 */
 	unsigned int shallow_depth;
 	unsigned int cmd_flags;
 
@@ -159,8 +167,33 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+ *   - block/blk-mq-tag.c|62| <<__blk_mq_tag_idle>> if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+ *   - block/blk-mq.c|3471| <<blk_mq_alloc_tag_set>> if (blk_mq_is_sbitmap_shared(set->flags)) {
+ *   - block/blk-mq.c|3506| <<blk_mq_free_tag_set>> if (blk_mq_is_sbitmap_shared(set->flags))
+ *   - block/blk-mq.c|3545| <<blk_mq_update_nr_requests>> if (!ret && blk_mq_is_sbitmap_shared(set->flags))
+ *   - block/blk-mq.h|204| <<__blk_mq_inc_active_requests>> if (blk_mq_is_sbitmap_shared(hctx->flags))
+ *   - block/blk-mq.h|212| <<__blk_mq_dec_active_requests>> if (blk_mq_is_sbitmap_shared(hctx->flags))
+ *   - block/blk-mq.h|220| <<__blk_mq_active_requests>> if (blk_mq_is_sbitmap_shared(hctx->flags))
+ *   - block/blk-mq.h|302| <<hctx_may_queue>> if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+ */
 static inline bool blk_mq_is_sbitmap_shared(unsigned int flags)
 {
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	return flags & BLK_MQ_F_TAG_HCTX_SHARED;
 }
 
@@ -303,10 +336,26 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 		struct request_queue *q = hctx->queue;
 		struct blk_mq_tag_set *set = q->tag_set;
 
+		/*
+		 * 在以下使用BLK_MQ_S_TAG_ACTIVE:
+		 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+		 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq-tag.c|68| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq.h|331| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
+		 *   - block/blk-mq.h|335| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 */
 		if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
 			return true;
 		users = atomic_read(&set->active_queues_shared_sbitmap);
 	} else {
+		/*
+		 * 在以下使用BLK_MQ_S_TAG_ACTIVE:
+		 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+		 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq-tag.c|68| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq.h|331| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
+		 *   - block/blk-mq.h|335| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 */
 		if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 			return true;
 		users = atomic_read(&hctx->tags->active_queues);
diff --git a/block/blk-stat.c b/block/blk-stat.c
index ae3dd1fb8e61..f0966c00ffe4 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -15,9 +15,24 @@
 struct blk_queue_stats {
 	struct list_head callbacks;
 	spinlock_t lock;
+	/*
+	 * 在以下使用blk_queue_stats->enable_accounting:
+	 *   - block/blk-stat.c|164| <<blk_stat_remove_callback>> if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
+	 *   - block/blk-stat.c|192| <<blk_stat_enable_accounting>> q->stats->enable_accounting = true;
+	 *   - block/blk-stat.c|208| <<blk_alloc_queue_stats>> stats->enable_accounting = false;
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|199| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|87| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|95| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|148| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *
+ * 初始化blk_rq_stat的field为0或者初始值
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -26,6 +41,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|210| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|94| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -40,6 +60,11 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|222| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|74| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -48,6 +73,21 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * 猜测是因为wbt
+ * [0] blk_stat_add
+ * [0] __blk_mq_end_request
+ * [0] scsi_end_request
+ * [0] scsi_io_completion
+ * [0] blk_done_softirq
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ *
+ * called by:
+ *   - block/blk-mq.c|548| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ *
+ * 对request_queue的每一个callback(blk_stat_callback)都调用blk_rq_stat_add()
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -62,10 +102,23 @@ void blk_stat_add(struct request *rq, u64 now)
 
 	rcu_read_lock();
 	cpu = get_cpu();
+	/*
+	 * struct request_queue *q:
+	 * -> struct blk_queue_stats *stats;
+	 *    -> struct list_head callbacks;
+	 */
 	list_for_each_entry_rcu(cb, &q->stats->callbacks, list) {
 		if (!blk_stat_is_active(cb))
 			continue;
 
+		/*
+		 * 在以下设置blk_stat_callback->bucket_fn():
+		 *   - block/blk-stat.c|201| <<blk_stat_alloc_callback>> cb->bucket_fn = bucket_fn;
+		 * 在以下调用blk_stat_callback->bucket_fn():
+		 *   - block/blk-stat.c|108| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+		 *
+		 * 计算出一个bucket
+		 */
 		bucket = cb->bucket_fn(rq);
 		if (bucket < 0)
 			continue;
@@ -77,18 +130,31 @@ void blk_stat_add(struct request *rq, u64 now)
 	rcu_read_unlock();
 }
 
+/*
+ * 在以下使用blk_stat_timer_fn():
+ *   - block/blk-stat.c|161| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
 	unsigned int bucket;
 	int cpu;
 
+	/*
+	 * blk_rq_stat_init()
+	 * 初始化blk_rq_stat的field为0或者初始值
+	 */
 	for (bucket = 0; bucket < cb->buckets; bucket++)
 		blk_rq_stat_init(&cb->stat[bucket]);
 
 	for_each_online_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
+		/*
+		 * struct blk_stat_callback *cb:
+		 * -> struct blk_rq_stat __percpu *cpu_stat;
+		 * -> struct blk_rq_stat *stat;
+		 */
 		cpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);
 		for (bucket = 0; bucket < cb->buckets; bucket++) {
 			blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
@@ -99,6 +165,36 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * [0] blk_stat_alloc_callback
+ * [0] blk_mq_init_allocated_queue
+ * [0] blk_mq_init_queue_data
+ * [0] scsi_mq_alloc_queue
+ * [0] scsi_alloc_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by;
+ *   - block/blk-mq.c|3324| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|821| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -127,12 +223,48 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	cb->timer_fn = timer_fn;
 	cb->bucket_fn = bucket_fn;
 	cb->data = data;
+	/*
+	 * 在以下设置blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|201| <<blk_stat_alloc_callback>> cb->bucket_fn = bucket_fn;
+	 * 在以下调用blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|108| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 */
 	cb->buckets = buckets;
 	timer_setup(&cb->timer, blk_stat_timer_fn, 0);
 
 	return cb;
 }
 
+/*
+ * [0] blk_stat_add_callback
+ * [0] wbt_init
+ * [0] blk_register_queue
+ * [0] __device_add_disk
+ * [0] sd_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] __driver_attach_async_helper
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] blk_stat_add_callback
+ * [0] wbt_init
+ * [0] blk_register_queue
+ * [0] __device_add_disk
+ * [0] loop_add
+ * [0] loop_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-mq.c|3827| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|844| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -184,6 +316,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-iocost.c|3216| <<ioc_qos_write>> blk_stat_enable_accounting(ioc->rqos.q);
+ *   - block/blk-throttle.c|2456| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	unsigned long flags;
@@ -195,6 +333,10 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|541| <<blk_alloc_queue>> q->stats = blk_alloc_queue_stats();
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -210,6 +352,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|589| <<blk_alloc_queue>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|791| <<blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86eefb..21146b055add 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -37,6 +37,12 @@ struct blk_stat_callback {
 	 * should be accounted under. Return -1 for no bucket for this
 	 * request.
 	 */
+	/*
+	 * 在以下设置blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|201| <<blk_stat_alloc_callback>> cb->bucket_fn = bucket_fn;
+	 * 在以下调用blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|108| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 */
 	int (*bucket_fn)(const struct request *);
 
 	/**
@@ -126,6 +132,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3833| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+ *   - block/blk-stat.c|66| <<blk_stat_add>> if (!blk_stat_is_active(cb))
+ *   - block/blk-wbt.c|585| <<wbt_wait>> if (!blk_stat_is_active(rwb->cb))
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
 	return timer_pending(&cb->timer);
@@ -139,12 +151,20 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-wbt.c|349| <<rwb_arm_timer>> blk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
 	mod_timer(&cb->timer, jiffies + nsecs_to_jiffies(nsecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|704| <<wbt_disable_default>> blk_stat_deactivate(rwb->cb);
+ */
 static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
 {
 	del_timer_sync(&cb->timer);
@@ -158,6 +178,10 @@ static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3836| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index fd410086fe1d..a8d6e59f3a4a 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -325,6 +325,11 @@ static void scale_down(struct rq_wb *rwb, bool hard_throttle)
 	rwb_trace_step(rwb, tracepoint_string("scale down"));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|405| <<wb_timer_fn>> rwb_arm_timer(rwb);
+ *   - block/blk-wbt.c|586| <<wbt_wait>> rwb_arm_timer(rwb);
+ */
 static void rwb_arm_timer(struct rq_wb *rwb)
 {
 	struct rq_depth *rqd = &rwb->rq_depth;
diff --git a/block/genhd.c b/block/genhd.c
index 9387f050c248..da893c9dd4ff 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -676,6 +676,10 @@ static int exact_lock(dev_t devt, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|737| <<register_disk>> disk_scan_partitions(disk);
+ */
 static void disk_scan_partitions(struct gendisk *disk)
 {
 	struct block_device *bdev;
diff --git a/block/partitions/core.c b/block/partitions/core.c
index a02e22411594..1732f125ac7f 100644
--- a/block/partitions/core.c
+++ b/block/partitions/core.c
@@ -137,6 +137,9 @@ static struct parsed_partitions *check_partition(struct gendisk *hd,
 	i = res = err = 0;
 	while (!res && check_part[i]) {
 		memset(state->parts, 0, state->limit * sizeof(state->parts[0]));
+		/*
+		 * 应该是msdos_partition()
+		 */
 		res = check_part[i++](state);
 		if (res < 0) {
 			/*
@@ -149,6 +152,9 @@ static struct parsed_partitions *check_partition(struct gendisk *hd,
 
 	}
 	if (res > 0) {
+		/*
+		 * [    0.856686]  sda: sda1 sda2
+		 */
 		printk(KERN_INFO "%s", state->pp_buf);
 
 		free_page((unsigned long)state->pp_buf);
@@ -324,6 +330,12 @@ int hd_ref_init(struct hd_struct *part)
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - block/genhd.c|915| <<del_gendisk>> delete_partition(part);
+ *   - block/partitions/core.c|616| <<bdev_del_partition>> delete_partition(part);
+ *   - block/partitions/core.c|696| <<blk_drop_partitions>> delete_partition(part);
+ */
 void delete_partition(struct hd_struct *part)
 {
 	struct gendisk *disk = part_to_disk(part);
@@ -360,6 +372,52 @@ static DEVICE_ATTR(whole_disk, 0444, whole_disk_show, NULL);
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * 启动的时候检测partition
+ * [0] add_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] __blkdev_get
+ * [0] blkdev_get_by_dev
+ * [0] __device_add_disk
+ * [0] sd_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] __driver_attach_async_helper
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 用fdisk创建的时候'w'的时候生成. 先是fdisk创建的:
+ *
+ * [0] add_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] blkdev_common_ioctl
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * 然后是systemd-udevd调用的:
+ *
+ * [0] add_partition
+ * [0] blk_add_partitions
+ * [0] bdev_disk_changed
+ * [0] blkdev_common_ioctl
+ * [0] blkdev_ioctl
+ * [0] block_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - block/partitions/core.c|571| <<bdev_add_partition>> part = add_partition(bdev->bd_disk, partno, start, length,
+ *   - block/partitions/core.c|727| <<blk_add_partition>> part = add_partition(disk, p, from, size, state->parts[p].flags,
+ */
 static struct hd_struct *add_partition(struct gendisk *disk, int partno,
 				sector_t start, sector_t len, int flags,
 				struct partition_meta_info *info)
@@ -515,6 +573,10 @@ static bool partition_overlaps(struct gendisk *disk, sector_t start,
 	return overlap;
 }
 
+/*
+ * called by:
+ *   - block/ioctl.c|52| <<blkpg_do_ioctl>> return bdev_add_partition(bdev, p.pno, start, length);
+ */
 int bdev_add_partition(struct block_device *bdev, int partno,
 		sector_t start, sector_t length)
 {
@@ -647,6 +709,10 @@ int blk_drop_partitions(struct block_device *bdev)
 EXPORT_SYMBOL_GPL(blk_drop_partitions);
 #endif
 
+/*
+ * called by:
+ *   - block/partitions/core.c|815| <<blk_add_partitions>> if (!blk_add_partition(disk, bdev, state, p))
+ */
 static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 		struct parsed_partitions *state, int p)
 {
@@ -697,6 +763,10 @@ static bool blk_add_partition(struct gendisk *disk, struct block_device *bdev,
 	return true;
 }
 
+/*
+ * called by:
+ *   - fs/block_dev.c|1417| <<bdev_disk_changed>> ret = blk_add_partitions(disk, bdev);
+ */
 int blk_add_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct parsed_partitions *state;
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index 4685ea401d5b..f9720127bb7e 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -1710,6 +1710,19 @@ static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 	set->flags = BLK_MQ_F_SHOULD_MERGE;
 	if (g_no_sched)
 		set->flags |= BLK_MQ_F_NO_SCHED;
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	if (g_shared_tag_bitmap)
 		set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
 	set->driver_data = NULL;
diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index a1b9df2c4ef1..d28a8ffe8318 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -312,6 +312,13 @@ struct xen_blkif {
 	atomic_t		drain;
 
 	struct work_struct	free_work;
+	/*
+	 * 在以下使用xen_blkif->nr_ring_pages:
+	 *   - drivers/block/xen-blkback/xenbus.c|321| <<xen_blkif_disconnect>> WARN_ON(i != (XEN_BLKIF_REQS_PER_PAGE * blkif->nr_ring_pages));
+	 *   - drivers/block/xen-blkback/xenbus.c|327| <<xen_blkif_disconnect>> blkif->nr_ring_pages = 0;
+	 *   - drivers/block/xen-blkback/xenbus.c|989| <<read_per_ring_refs>> nr_grefs = blkif->nr_ring_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|1142| <<connect_ring>> blkif->nr_ring_pages = 1 << ring_page_order;
+	 */
 	unsigned int 		nr_ring_pages;
 	/* All rings for this device. */
 	struct xen_blkif_ring	*rings;
diff --git a/drivers/block/xen-blkback/xenbus.c b/drivers/block/xen-blkback/xenbus.c
index 76912c584a76..eb62c437d662 100644
--- a/drivers/block/xen-blkback/xenbus.c
+++ b/drivers/block/xen-blkback/xenbus.c
@@ -969,6 +969,11 @@ static void connect(struct backend_info *be)
 /*
  * Each ring may have multi pages, depends on "ring-page-order".
  */
+/*
+ * called by:
+ *   - drivers/block/xen-blkback/xenbus.c|1145| <<connect_ring>> return read_per_ring_refs(&blkif->rings[0], dev->otherend);
+ *   - drivers/block/xen-blkback/xenbus.c|1157| <<connect_ring>> err = read_per_ring_refs(&blkif->rings[i], xspath);
+ */
 static int read_per_ring_refs(struct xen_blkif_ring *ring, const char *dir)
 {
 	unsigned int ring_ref[XENBUS_MAX_RING_GRANTS];
@@ -986,6 +991,13 @@ static int read_per_ring_refs(struct xen_blkif_ring *ring, const char *dir)
 		return err;
 	}
 
+	/*
+	 * 在以下使用xen_blkif->nr_ring_pages:
+	 *   - drivers/block/xen-blkback/xenbus.c|321| <<xen_blkif_disconnect>> WARN_ON(i != (XEN_BLKIF_REQS_PER_PAGE * blkif->nr_ring_pages));
+	 *   - drivers/block/xen-blkback/xenbus.c|327| <<xen_blkif_disconnect>> blkif->nr_ring_pages = 0;
+	 *   - drivers/block/xen-blkback/xenbus.c|989| <<read_per_ring_refs>> nr_grefs = blkif->nr_ring_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|1142| <<connect_ring>> blkif->nr_ring_pages = 1 << ring_page_order;
+	 */
 	nr_grefs = blkif->nr_ring_pages;
 
 	if (unlikely(!nr_grefs)) {
@@ -1069,6 +1081,10 @@ static int read_per_ring_refs(struct xen_blkif_ring *ring, const char *dir)
 	return err;
 }
 
+/*
+ * 处理XenbusStateConnected:
+ *   - drivers/block/xen-blkback/xenbus.c|830| <<frontend_changed>> err = connect_ring(be);
+ */
 static int connect_ring(struct backend_info *be)
 {
 	struct xenbus_device *dev = be->dev;
@@ -1139,6 +1155,13 @@ static int connect_ring(struct backend_info *be)
 		return err;
 	}
 
+	/*
+	 * 在以下使用xen_blkif->nr_ring_pages:
+	 *   - drivers/block/xen-blkback/xenbus.c|321| <<xen_blkif_disconnect>> WARN_ON(i != (XEN_BLKIF_REQS_PER_PAGE * blkif->nr_ring_pages));
+	 *   - drivers/block/xen-blkback/xenbus.c|327| <<xen_blkif_disconnect>> blkif->nr_ring_pages = 0;
+	 *   - drivers/block/xen-blkback/xenbus.c|989| <<read_per_ring_refs>> nr_grefs = blkif->nr_ring_pages;
+	 *   - drivers/block/xen-blkback/xenbus.c|1142| <<connect_ring>> blkif->nr_ring_pages = 1 << ring_page_order;
+	 */
 	blkif->nr_ring_pages = 1 << ring_page_order;
 
 	if (blkif->nr_rings == 1)
diff --git a/drivers/nvme/host/trace.c b/drivers/nvme/host/trace.c
index 5c3cb6928f3c..c9305390e41f 100644
--- a/drivers/nvme/host/trace.c
+++ b/drivers/nvme/host/trace.c
@@ -141,6 +141,10 @@ static const char *nvme_trace_common(struct trace_seq *p, u8 *cdw10)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/trace.h|31| <<parse_nvme_cmd>> nvme_trace_parse_admin_cmd(p, opcode, cdw10)))
+ */
 const char *nvme_trace_parse_admin_cmd(struct trace_seq *p,
 				       u8 opcode, u8 *cdw10)
 {
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 03c6d0620bfd..42ba00e821c0 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -1911,6 +1911,19 @@ int scsi_mq_setup_tags(struct Scsi_Host *shost)
 	tag_set->flags |=
 		BLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);
 	tag_set->driver_data = shost;
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	if (shost->host_tagset)
 		tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
 
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 6ff8a5096691..77d6d9c2db4b 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -1807,12 +1807,39 @@ static int vhost_scsi_set_features(struct vhost_scsi *vs, u64 features)
 	return 0;
 }
 
+/*
+ * [0] vhost_scsi_open
+ * [0] misc_open
+ * [0] chrdev_open
+ * [0] do_dentry_open
+ * [0] path_openat
+ * [0] do_filp_open
+ * [0] do_sys_openat2
+ * [0] do_sys_open
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int vhost_scsi_open(struct inode *inode, struct file *f)
 {
 	struct vhost_scsi *vs;
 	struct vhost_virtqueue **vqs;
 	int r = -ENOMEM, i;
 
+	/*
+	 * #define __GFP_IO        ((__force gfp_t)___GFP_IO)
+	 * #define __GFP_FS        ((__force gfp_t)___GFP_FS)
+	 * #define __GFP_DIRECT_RECLAIM    ((__force gfp_t)___GFP_DIRECT_RECLAIM) / Caller can reclaim
+	 * #define __GFP_KSWAPD_RECLAIM    ((__force gfp_t)___GFP_KSWAPD_RECLAIM) // kswapd can wake
+	 * #define __GFP_RECLAIM ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))
+	 * #define __GFP_RETRY_MAYFAIL     ((__force gfp_t)___GFP_RETRY_MAYFAIL)
+	 * #define __GFP_NOFAIL    ((__force gfp_t)___GFP_NOFAIL)
+	 * #define __GFP_NORETRY   ((__force gfp_t)___GFP_NORETRY)
+	 *
+	 * #define GFP_ATOMIC      (__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)
+	 * #define GFP_KERNEL      (__GFP_RECLAIM | __GFP_IO | __GFP_FS)
+	 * #define GFP_NOWAIT      (__GFP_KSWAPD_RECLAIM)
+	 * #define GFP_NOIO        (__GFP_RECLAIM)
+	 */
 	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);
 	if (!vs) {
 		vs = vzalloc(sizeof(*vs));
diff --git a/drivers/xen/mcelog.c b/drivers/xen/mcelog.c
index e9ac3b8c4167..1afc5d4a31ed 100644
--- a/drivers/xen/mcelog.c
+++ b/drivers/xen/mcelog.c
@@ -292,6 +292,11 @@ static int convert_log(struct mc_info *mi)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/mcelog.c|339| <<xen_mce_work_fn>> err = mc_queue_handle(XEN_MC_URGENT);
+ *   - drivers/xen/mcelog.c|344| <<xen_mce_work_fn>> err = mc_queue_handle(XEN_MC_NONURGENT);
+ */
 static int mc_queue_handle(uint32_t flags)
 {
 	struct xen_mc mc_op;
@@ -329,6 +334,13 @@ static int mc_queue_handle(uint32_t flags)
 }
 
 /* virq handler for machine check error info*/
+/*
+ * 在以下调用xen_mce_work=xen_mce_work_fn():
+ *   - drivers/xen/mcelog.c|357| <<xen_mce_interrupt>> schedule_work(&xen_mce_work);
+ *
+ * 在以下使用xen_mce_work_fn():
+ *   - drivers/xen/mcelog.c|353| <<global>> static DECLARE_WORK(xen_mce_work, xen_mce_work_fn);
+ */
 static void xen_mce_work_fn(struct work_struct *work)
 {
 	int err;
@@ -354,6 +366,9 @@ static DECLARE_WORK(xen_mce_work, xen_mce_work_fn);
 
 static irqreturn_t xen_mce_interrupt(int irq, void *dev_id)
 {
+	/*
+	 * xen_mce_work_fn()
+	 */
 	schedule_work(&xen_mce_work);
 	return IRQ_HANDLED;
 }
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 2b385c1b4a99..b162473cd254 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -39,6 +39,10 @@
 #include <asm/xen/page-coherent.h>
 
 #include <trace/events/swiotlb.h>
+/*
+ * 在以下使用MAX_DMA_BITS:
+ *   - drivers/xen/swiotlb-xen.c|144| <<xen_swiotlb_fixup>> } while (rc && dma_bits++ < MAX_DMA_BITS);
+ */
 #define MAX_DMA_BITS 32
 /*
  * Used to do a quick range check in swiotlb_tbl_unmap_single and
@@ -46,12 +50,52 @@
  * API.
  */
 
+/*
+ * 在以下使用xen_io_tlb_start:
+ *   - drivers/xen/swiotlb-xen.c|119| <<is_xen_swiotlb_buffer>> return paddr >= virt_to_phys(xen_io_tlb_start) &&
+ *   - drivers/xen/swiotlb-xen.c|210| <<xen_swiotlb_init>> xen_io_tlb_start = phys_to_virt(io_tlb_start);
+ *   - drivers/xen/swiotlb-xen.c|218| <<xen_swiotlb_init>> xen_io_tlb_start = memblock_alloc(PAGE_ALIGN(bytes),
+ *   - drivers/xen/swiotlb-xen.c|220| <<xen_swiotlb_init>> if (!xen_io_tlb_start)
+ *   - drivers/xen/swiotlb-xen.c|227| <<xen_swiotlb_init>> xen_io_tlb_start = (void *)xen_get_swiotlb_free_pages(order);
+ *   - drivers/xen/swiotlb-xen.c|228| <<xen_swiotlb_init>> if (xen_io_tlb_start)
+ *   - drivers/xen/swiotlb-xen.c|239| <<xen_swiotlb_init>> if (!xen_io_tlb_start) {
+ *   - drivers/xen/swiotlb-xen.c|246| <<xen_swiotlb_init>> rc = xen_swiotlb_fixup(xen_io_tlb_start,
+ *   - drivers/xen/swiotlb-xen.c|251| <<xen_swiotlb_init>> memblock_free(__pa(xen_io_tlb_start),
+ *   - drivers/xen/swiotlb-xen.c|254| <<xen_swiotlb_init>> free_pages((unsigned long )xen_io_tlb_start, order);
+ *   - drivers/xen/swiotlb-xen.c|255| <<xen_swiotlb_init>> xen_io_tlb_start = NULL;
+ *   - drivers/xen/swiotlb-xen.c|261| <<xen_swiotlb_init>> if (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs,
+ *   - drivers/xen/swiotlb-xen.c|266| <<xen_swiotlb_init>> rc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);
+ *   - drivers/xen/swiotlb-xen.c|269| <<xen_swiotlb_init>> xen_io_tlb_end = xen_io_tlb_start + bytes;
+ *   - drivers/xen/swiotlb-xen.c|286| <<xen_swiotlb_init>> free_pages((unsigned long )xen_io_tlb_start, order);
+ */
 static char *xen_io_tlb_start, *xen_io_tlb_end;
+/*
+ * 在以下使用xen_io_tlb_nslabs:
+ *   - drivers/xen/swiotlb-xen.c|159| <<xen_set_nslabs>> xen_io_tlb_nslabs = (64 * 1024 * 1024 >> IO_TLB_SHIFT);
+ *   - drivers/xen/swiotlb-xen.c|160| <<xen_set_nslabs>> xen_io_tlb_nslabs = ALIGN(xen_io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - drivers/xen/swiotlb-xen.c|162| <<xen_set_nslabs>> xen_io_tlb_nslabs = nr_tbl;
+ *   - drivers/xen/swiotlb-xen.c|164| <<xen_set_nslabs>> return xen_io_tlb_nslabs << IO_TLB_SHIFT;
+ *   - drivers/xen/swiotlb-xen.c|201| <<xen_swiotlb_init>> xen_io_tlb_nslabs = swiotlb_nr_tbl();
+ *   - drivers/xen/swiotlb-xen.c|203| <<xen_swiotlb_init>> bytes = xen_set_nslabs(xen_io_tlb_nslabs);
+ *   - drivers/xen/swiotlb-xen.c|204| <<xen_swiotlb_init>> order = get_order(xen_io_tlb_nslabs << IO_TLB_SHIFT);
+ *   - drivers/xen/swiotlb-xen.c|235| <<xen_swiotlb_init>> xen_io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - drivers/xen/swiotlb-xen.c|236| <<xen_swiotlb_init>> bytes = xen_io_tlb_nslabs << IO_TLB_SHIFT;
+ *   - drivers/xen/swiotlb-xen.c|248| <<xen_swiotlb_init>> xen_io_tlb_nslabs);
+ *   - drivers/xen/swiotlb-xen.c|261| <<xen_swiotlb_init>> if (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs,
+ *   - drivers/xen/swiotlb-xen.c|266| <<xen_swiotlb_init>> rc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);
+ *   - drivers/xen/swiotlb-xen.c|276| <<xen_swiotlb_init>> xen_io_tlb_nslabs = max(1024UL,
+ *   - drivers/xen/swiotlb-xen.c|277| <<xen_swiotlb_init>> (xen_io_tlb_nslabs >> 1));
+ *   - drivers/xen/swiotlb-xen.c|279| <<xen_swiotlb_init>> (xen_io_tlb_nslabs << IO_TLB_SHIFT) >> 20);
+ */
 static unsigned long xen_io_tlb_nslabs;
 /*
  * Quick lookup value of the bus address of the IOTLB.
  */
 
+/*
+ * called by only:
+ *   - drivers/xen/swiotlb-xen.c|70| <<xen_phys_to_dma>> return phys_to_dma(dev, xen_phys_to_bus(dev, paddr));
+ */
 static inline phys_addr_t xen_phys_to_bus(struct device *dev, phys_addr_t paddr)
 {
 	unsigned long bfn = pfn_to_bfn(XEN_PFN_DOWN(paddr));
@@ -61,11 +105,22 @@ static inline phys_addr_t xen_phys_to_bus(struct device *dev, phys_addr_t paddr)
 	return baddr;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|91| <<xen_virt_to_bus>> return xen_phys_to_dma(dev, virt_to_phys(address));
+ *   - drivers/xen/swiotlb-xen.c|333| <<xen_swiotlb_alloc_coherent>> dev_addr = xen_phys_to_dma(hwdev, phys);
+ *   - drivers/xen/swiotlb-xen.c|402| <<xen_swiotlb_map_page>> dma_addr_t dev_addr = xen_phys_to_dma(dev, phys);
+ *   - drivers/xen/swiotlb-xen.c|432| <<xen_swiotlb_map_page>> dev_addr = xen_phys_to_dma(dev, map);
+ */
 static inline dma_addr_t xen_phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
 	return phys_to_dma(dev, xen_phys_to_bus(dev, paddr));
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|133| <<xen_dma_to_phys>> return xen_bus_to_phys(dev, dma_to_phys(dev, dma_addr));
+ */
 static inline phys_addr_t xen_bus_to_phys(struct device *dev,
 					  phys_addr_t baddr)
 {
@@ -76,17 +131,34 @@ static inline phys_addr_t xen_bus_to_phys(struct device *dev,
 	return paddr;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|420| <<xen_swiotlb_free_coherent>> phys = xen_dma_to_phys(hwdev, dev_addr);
+ *   - drivers/xen/swiotlb-xen.c|520| <<xen_swiotlb_unmap_page>> phys_addr_t paddr = xen_dma_to_phys(hwdev, dev_addr);
+ *   - drivers/xen/swiotlb-xen.c|543| <<xen_swiotlb_sync_single_for_cpu>> phys_addr_t paddr = xen_dma_to_phys(dev, dma_addr);
+ *   - drivers/xen/swiotlb-xen.c|563| <<xen_swiotlb_sync_single_for_device>> phys_addr_t paddr = xen_dma_to_phys(dev, dma_addr);
+ */
 static inline phys_addr_t xen_dma_to_phys(struct device *dev,
 					  dma_addr_t dma_addr)
 {
 	return xen_bus_to_phys(dev, dma_to_phys(dev, dma_addr));
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|669| <<xen_swiotlb_dma_supported>> return xen_virt_to_bus(hwdev, xen_io_tlb_end - 1) <= mask;
+ */
 static inline dma_addr_t xen_virt_to_bus(struct device *dev, void *address)
 {
 	return xen_phys_to_dma(dev, virt_to_phys(address));
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|388| <<xen_swiotlb_alloc_coherent>> !range_straddles_page_boundary(phys, size))
+ *   - drivers/xen/swiotlb-xen.c|431| <<xen_swiotlb_free_coherent>> range_straddles_page_boundary(phys, size)) &&
+ *   - drivers/xen/swiotlb-xen.c|464| <<xen_swiotlb_map_page>> !range_straddles_page_boundary(phys, size) &&
+ */
 static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 {
 	unsigned long next_bfn, xen_pfn = XEN_PFN_DOWN(p);
@@ -101,6 +173,12 @@ static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|479| <<xen_swiotlb_unmap_page>> if (is_xen_swiotlb_buffer(hwdev, dev_addr))
+ *   - drivers/xen/swiotlb-xen.c|499| <<xen_swiotlb_sync_single_for_cpu>> if (is_xen_swiotlb_buffer(dev, dma_addr))
+ *   - drivers/xen/swiotlb-xen.c|512| <<xen_swiotlb_sync_single_for_device>> if (is_xen_swiotlb_buffer(dev, dma_addr))
+ */
 static int is_xen_swiotlb_buffer(struct device *dev, dma_addr_t dma_addr)
 {
 	unsigned long bfn = XEN_PFN_DOWN(dma_to_phys(dev, dma_addr));
@@ -118,6 +196,12 @@ static int is_xen_swiotlb_buffer(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|232| <<xen_swiotlb_init>> rc = xen_swiotlb_fixup(xen_io_tlb_start,
+ *
+ * 这里应该加type, MAX_DMA_BITS是32或者64
+ */
 static int
 xen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)
 {
@@ -145,6 +229,12 @@ xen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)
 	} while (i < nslabs);
 	return 0;
 }
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|277| <<xen_swiotlb_init>> bytes = xen_set_nslabs(xen_io_tlb_nslabs);
+ *
+ * 需要type
+ */
 static unsigned long xen_set_nslabs(unsigned long nr_tbl)
 {
 	if (!nr_tbl) {
@@ -177,6 +267,12 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	}
 	return "";
 }
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|143| <<xen_mm_init>> xen_swiotlb_init(1, false);
+ *   - arch/x86/xen/pci-swiotlb-xen.c|62| <<pci_xen_swiotlb_init>> xen_swiotlb_init(1, true );
+ *   - arch/x86/xen/pci-swiotlb-xen.c|79| <<pci_xen_swiotlb_init_late>> rc = xen_swiotlb_init(1, false );
+ */
 int __ref xen_swiotlb_init(int verbose, bool early)
 {
 	unsigned long bytes, order;
@@ -201,6 +297,9 @@ int __ref xen_swiotlb_init(int verbose, bool early)
 	 * Get IO TLB memory from any location.
 	 */
 	if (early) {
+		/*
+		 * 这里是分配bytes的内存!!!
+		 */
 		xen_io_tlb_start = memblock_alloc(PAGE_ALIGN(bytes),
 						  PAGE_SIZE);
 		if (!xen_io_tlb_start)
@@ -273,6 +372,9 @@ int __ref xen_swiotlb_init(int verbose, bool early)
 	return rc;
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.alloc = xen_swiotlb_alloc_coherent()
+ */
 static void *
 xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 			   dma_addr_t *dma_handle, gfp_t flags,
@@ -330,6 +432,9 @@ xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 	return ret;
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.free = xen_swiotlb_free_coherent()
+ */
 static void
 xen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
 			  dma_addr_t dev_addr, unsigned long attrs)
@@ -370,6 +475,9 @@ xen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
  * Once the device is given the dma address, the device owns this memory until
  * either xen_swiotlb_unmap_page or xen_swiotlb_dma_sync_single is performed.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.map_page = xen_swiotlb_map_page()
+ */
 static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 				unsigned long offset, size_t size,
 				enum dma_data_direction dir,
@@ -395,6 +503,12 @@ static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 	 */
 	trace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);
 
+	/*
+	 * called by:
+	 *   - drivers/iommu/intel/iommu.c|3821| <<bounce_map_single>> tlb_addr = swiotlb_tbl_map_single(dev, paddr, size,
+	 *   - drivers/xen/swiotlb-xen.c|398| <<xen_swiotlb_map_page>> map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
+	 *   - kernel/dma/swiotlb.c|705| <<swiotlb_map>> swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, dir,
+	 */
 	map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
 	if (map == (phys_addr_t)DMA_MAPPING_ERROR)
 		return DMA_MAPPING_ERROR;
@@ -429,6 +543,9 @@ static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
  * After this call, reads by the cpu to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.unmap_page = xen_swiotlb_unmap_page()
+ */
 static void xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 		size_t size, enum dma_data_direction dir, unsigned long attrs)
 {
@@ -448,6 +565,9 @@ static void xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 		swiotlb_tbl_unmap_single(hwdev, paddr, size, size, dir, attrs);
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu()
+ */
 static void
 xen_swiotlb_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr,
 		size_t size, enum dma_data_direction dir)
@@ -465,6 +585,11 @@ xen_swiotlb_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr,
 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
 }
 
+/*
+ * 在以下使用xen_swiotlb_sync_single_for_device():
+ *   - struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_device = xen_swiotlb_sync_single_for_device()
+ *   - drivers/xen/swiotlb-xen.c|676| <<xen_swiotlb_sync_sg_for_device>> xen_swiotlb_sync_single_for_device(dev, sg->dma_address,
+ */
 static void
 xen_swiotlb_sync_single_for_device(struct device *dev, dma_addr_t dma_addr,
 		size_t size, enum dma_data_direction dir)
@@ -486,6 +611,9 @@ xen_swiotlb_sync_single_for_device(struct device *dev, dma_addr_t dma_addr,
  * Unmap a set of streaming mode DMA translations.  Again, cpu read rules
  * concerning calls here are the same as for swiotlb_unmap_page() above.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.unmap_sg = xen_swiotlb_unmap_sg()
+ */
 static void
 xen_swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -501,6 +629,9 @@ xen_swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.map_sg = xen_swiotlb_map_sg()
+ */
 static int
 xen_swiotlb_map_sg(struct device *dev, struct scatterlist *sgl, int nelems,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -525,6 +656,9 @@ xen_swiotlb_map_sg(struct device *dev, struct scatterlist *sgl, int nelems,
 	return 0;
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_sg_for_cpu = xen_swiotlb_sync_sg_for_cpu()
+ */
 static void
 xen_swiotlb_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,
 			    int nelems, enum dma_data_direction dir)
@@ -538,6 +672,9 @@ xen_swiotlb_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,
 	}
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_sg_for_device = xen_swiotlb_sync_sg_for_device()
+ */
 static void
 xen_swiotlb_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,
 			       int nelems, enum dma_data_direction dir)
@@ -557,12 +694,22 @@ xen_swiotlb_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,
  * during bus mastering, then you would pass 0x00ffffff as the mask to
  * this function.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.dma_supported = xen_swiotlb_dma_supported()
+ */
 static int
 xen_swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
 	return xen_virt_to_bus(hwdev, xen_io_tlb_end - 1) <= mask;
 }
 
+/*
+ * 在以下使用xen_swiotlb_dma_ops:
+ *   - arch/arm/mm/dma-mapping.c|2284| <<arch_setup_dma_ops>> dev->dma_ops = &xen_swiotlb_dma_ops;
+ *   - arch/arm64/mm/dma-mapping.c|57| <<arch_setup_dma_ops>> dev->dma_ops = &xen_swiotlb_dma_ops;
+ *   - arch/x86/xen/pci-swiotlb-xen.c|63| <<pci_xen_swiotlb_init>> dma_ops = &xen_swiotlb_dma_ops;
+ *   - arch/x86/xen/pci-swiotlb-xen.c|83| <<pci_xen_swiotlb_init_late>> dma_ops = &xen_swiotlb_dma_ops;
+ */
 const struct dma_map_ops xen_swiotlb_dma_ops = {
 	.alloc = xen_swiotlb_alloc_coherent,
 	.free = xen_swiotlb_free_coherent,
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 794b2a33a2c3..09ac938b4fff 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -74,6 +74,21 @@ struct blk_mq_hw_ctx {
 	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
 	 * pending request in that software queue.
 	 */
+	/*
+	 * 在以下使用blk_mq_hw_ctx->ctx_map:
+	 *   - block/blk-mq-debugfs.c|443| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|238| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq-sysfs.c|42| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|73| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|85| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|86| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|94| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|1049| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1087| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2740| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2760| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2926| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	struct sbitmap		ctx_map;
 
 	/**
@@ -258,8 +273,23 @@ struct blk_mq_tag_set {
 	unsigned int		timeout;
 	unsigned int		flags;
 	void			*driver_data;
+	/*
+	 * 在以下使用blk_mq_tag_set->active_queues_shared_sbitmap:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> atomic_inc(&set->active_queues_shared_sbitmap);
+	 *   - block/blk-mq-tag.c|66| <<__blk_mq_tag_idle>> atomic_dec(&set->active_queues_shared_sbitmap);
+	 *   - block/blk-mq.c|3511| <<blk_mq_alloc_tag_set>> atomic_set(&set->active_queues_shared_sbitmap, 0);
+	 *   - block/blk-mq.h|341| <<hctx_may_queue>> users = atomic_read(&set->active_queues_shared_sbitmap);
+	 */
 	atomic_t		active_queues_shared_sbitmap;
 
+	/*
+	 * 在以下使用blk_mq_tag_set->__bitmap_tags:
+	 *   - block/blk-mq-tag.c|476| <<blk_mq_init_shared_sbitmap>> if (bt_alloc(&set->__bitmap_tags, depth, round_robin, node))
+	 *   - block/blk-mq-tag.c|485| <<blk_mq_init_shared_sbitmap>> tags->bitmap_tags = &set->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|491| <<blk_mq_init_shared_sbitmap>> sbitmap_queue_free(&set->__bitmap_tags);
+	 *   - block/blk-mq-tag.c|497| <<blk_mq_exit_shared_sbitmap>> sbitmap_queue_free(&set->__bitmap_tags);
+	 *   - block/blk-mq-tag.c|596| <<blk_mq_tag_resize_shared_sbitmap>> sbitmap_queue_resize(&set->__bitmap_tags, size - set->reserved_tags);
+	 */
 	struct sbitmap_queue	__bitmap_tags;
 	struct sbitmap_queue	__breserved_tags;
 	struct blk_mq_tags	**tags;
@@ -390,12 +420,43 @@ struct blk_mq_ops {
 
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	BLK_MQ_F_TAG_QUEUE_SHARED = 1 << 1,
 	/*
 	 * Set when this device requires underlying blk-mq device for
 	 * completing IO:
 	 */
 	BLK_MQ_F_STACKING	= 1 << 2,
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	BLK_MQ_F_TAG_HCTX_SHARED = 1 << 3,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
@@ -403,10 +464,24 @@ enum {
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 在以下使用BLK_MQ_S_TAG_ACTIVE:
+	 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|68| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq.h|331| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
+	 *   - block/blk-mq.h|335| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	/* hw queue is inactive after all its CPUs become offline */
+	/*
+	 * 在以下使用BLK_MQ_S_INACTIVE:
+	 *   - block/blk-mq-tag.c|193| <<blk_mq_get_tag>> if (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {
+	 *   - block/blk-mq.c|2507| <<blk_mq_hctx_notify_offline>> set_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 *   - block/blk-mq.c|2530| <<blk_mq_hctx_notify_online>> clear_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 */
 	BLK_MQ_S_INACTIVE	= 3,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 033eb5f73b65..fddcbe8b71a6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -97,6 +97,12 @@ typedef __u32 __bitwise req_flags_t;
 /* on IO scheduler merge hash */
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
 /* track IO completion time */
+/*
+ * 在以下使用RQF_STATS:
+ *   - block/blk-mq.c|276| <<blk_mq_need_time_stamp>> return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
+ *   - block/blk-mq.c|546| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+ *   - block/blk-mq.c|743| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+ */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
@@ -615,6 +621,13 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|746| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|188| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|200| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|234| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
@@ -623,6 +636,12 @@ struct request_queue {
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 #define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
 #define QUEUE_FLAG_RQ_ALLOC_TIME 27	/* record rq->alloc_time_ns */
+/*
+ * 在以下使用QUEUE_FLAG_HCTX_ACTIVE:
+ *   - block/blk-mq-tag.c|30| <<__blk_mq_tag_busy>> if (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) &&
+ *   - block/blk-mq-tag.c|31| <<__blk_mq_tag_busy>> !test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))
+ *   - block/blk-mq-tag.c|63| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(QUEUE_FLAG_HCTX_ACTIVE,
+ */
 #define QUEUE_FLAG_HCTX_ACTIVE	28	/* at least one blk-mq hctx is active */
 #define QUEUE_FLAG_NOWAIT       29	/* device supports NOWAIT */
 
diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 18aade195884..49532e1cd7e6 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -96,6 +96,16 @@ static inline bool force_dma_unencrypted(struct device *dev)
 }
 #endif /* CONFIG_ARCH_HAS_FORCE_DMA_UNENCRYPTED */
 
+/*
+ * 调用dma_capable()的地方:
+ *   - arch/x86/kernel/amd_gart_64.c|187| <<need_iommu>> return force_iommu || !dma_capable(dev, addr, size, true);
+ *   - arch/x86/kernel/amd_gart_64.c|193| <<nonforced_iommu>> return !dma_capable(dev, addr, size, true);
+ *   - drivers/xen/swiotlb-xen.c|487| <<xen_swiotlb_map_page>> if (dma_capable(dev, dev_addr, size, true) &&
+ *   - drivers/xen/swiotlb-xen.c|514| <<xen_swiotlb_map_page>> if (unlikely(!dma_capable(dev, dev_addr, size, true))) {
+ *   - kernel/dma/direct.c|426| <<dma_direct_map_resource>> if (unlikely(!dma_capable(dev, dma_addr, size, false))) {
+ *   - kernel/dma/direct.h|98| <<dma_direct_map_page>> if (unlikely(!dma_capable(dev, dma_addr, size, true))) {
+ *   - kernel/dma/swiotlb.c|1048| <<swiotlb_map>> if (unlikely(!dma_capable(dev, dma_addr, size, true))) {
+ */
 static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size,
 		bool is_ram)
 {
diff --git a/include/linux/dma-map-ops.h b/include/linux/dma-map-ops.h
index a5f89fc4d6df..074a5056865c 100644
--- a/include/linux/dma-map-ops.h
+++ b/include/linux/dma-map-ops.h
@@ -76,8 +76,21 @@ struct dma_map_ops {
 
 static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 {
+	/*
+	 * 部分设置dma_ops和dev->dma_ops的例子:
+	 *   - drivers/acpi/scan.c|1468| <<acpi_dma_configure_id>> set_dma_ops(dev, &dma_dummy_ops);
+	 *   - drivers/iommu/intel/iommu.c|5748| <<intel_iommu_release_device>> set_dma_ops(dev, NULL);
+	 *   - drivers/iommu/intel/iommu.c|5757| <<intel_iommu_probe_finalize>> set_dma_ops(dev, &bounce_dma_ops);
+	 *   - drivers/iommu/intel/iommu.c|5759| <<intel_iommu_probe_finalize>> set_dma_ops(dev, &intel_dma_ops);
+	 *   - drivers/iommu/intel/iommu.c|5761| <<intel_iommu_probe_finalize>> set_dma_ops(dev, NULL);
+	 *   - arch/x86/xen/pci-swiotlb-xen.c|70| <<pci_xen_swiotlb_init>> dma_ops = &xen_swiotlb_dma_ops;
+	 *   - arch/x86/xen/pci-swiotlb-xen.c|90| <<pci_xen_swiotlb_init_late>> dma_ops = &xen_swiotlb_dma_ops;
+	 */
 	if (dev->dma_ops)
 		return dev->dma_ops;
+	/*
+	 * 比如返回dma_ops
+	 */
 	return get_arch_dma_ops(dev->bus);
 }
 
diff --git a/include/linux/iommu-helper.h b/include/linux/iommu-helper.h
index 70d01edcbf8b..c82fd7290643 100644
--- a/include/linux/iommu-helper.h
+++ b/include/linux/iommu-helper.h
@@ -15,6 +15,20 @@ static inline unsigned long iommu_device_max_index(unsigned long size,
 		return size;
 }
 
+/*
+ * called by:
+ *   - arch/alpha/kernel/pci_iommu.c|154| <<iommu_arena_find_pages>> if (!i && iommu_is_span_boundary(p, n, base, boundary_size)) {
+ *   - arch/ia64/hp/common/sba_iommu.c|547| <<sba_search_bitmap>> ret = iommu_is_span_boundary(tpide, bits_wanted,
+ *   - arch/ia64/hp/common/sba_iommu.c|574| <<sba_search_bitmap>> ret = iommu_is_span_boundary(tpide, bits_wanted,
+ *   - arch/sparc/kernel/iommu_common.h|48| <<is_span_boundary>> return iommu_is_span_boundary(entry, nr, shift, boundary_size);
+ *   - drivers/parisc/ccio-dma.c|300| <<CCIO_SEARCH_LOOP>> ret = iommu_is_span_boundary(idx << 3, pages_needed, 0, boundary_size);\
+ *   - drivers/parisc/sba_iommu.c|359| <<sba_search_bitmap>> ret = iommu_is_span_boundary(tpide, bits_wanted,
+ *   - drivers/parisc/sba_iommu.c|394| <<sba_search_bitmap>> ret = iommu_is_span_boundary(tpide, bits_wanted,
+ *   - kernel/dma/swiotlb.c|865| <<swiotlb_tbl_map_single>> while (iommu_is_span_boundary(index, nslots, offset_slots,
+ *   - lib/iommu-helper.c|21| <<iommu_area_alloc>> if (iommu_is_span_boundary(index, nr, shift, boundary_size)) {
+ *
+ * 检查地址是否符合alignment的要求
+ */
 static inline int iommu_is_span_boundary(unsigned int index, unsigned int nr,
 		unsigned long shift, unsigned long boundary_size)
 {
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index fbdc65782195..608c3f94d662 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -22,12 +22,33 @@ enum swiotlb_force {
  * must be a power of 2.  What is the appropriate value ?
  * The complexity of {map,unmap}_single is linearly dependent on this value.
  */
+/*
+ * 在以下使用IO_TLB_SEGSIZE:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|240| <<plat_swiotlb_setup>> swiotlb_nslabs = ALIGN(swiotlb_nslabs, IO_TLB_SEGSIZE);
+ *   - arch/powerpc/platforms/pseries/svm.c|50| <<svm_swiotlb_init>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - drivers/mmc/host/sdhci.c|4587| <<sdhci_setup_host>> IO_TLB_SEGSIZE;
+ *   - drivers/xen/swiotlb-xen.c|211| <<xen_swiotlb_fixup>> dma_bits = get_order(IO_TLB_SEGSIZE << IO_TLB_SHIFT) + PAGE_SHIFT;
+ *   - drivers/xen/swiotlb-xen.c|215| <<xen_swiotlb_fixup>> int slabs = min(nslabs - i, (unsigned long )IO_TLB_SEGSIZE);
+ *   - drivers/xen/swiotlb-xen.c|234| <<xen_set_nslabs>> xen_io_tlb_nslabs = ALIGN(xen_io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|228| <<setup_io_tlb_npages>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|430| <<swiotlb_init_with_tbl>> io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|486| <<swiotlb_init>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|532| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|616| <<swiotlb_late_init_with_tbl>> io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|838| <<swiotlb_tbl_map_single>> for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)
+ *   - kernel/dma/swiotlb.c|919| <<swiotlb_tbl_unmap_single>> count = ((index + nslots) < ALIGN(index + 1, IO_TLB_SEGSIZE) ?
+ *   - kernel/dma/swiotlb.c|933| <<swiotlb_tbl_unmap_single>> for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE -1) && io_tlb_list[i]; i--)
+ *   - kernel/dma/swiotlb.c|1027| <<swiotlb_max_mapping_size>> return ((size_t)1 << IO_TLB_SHIFT) * IO_TLB_SEGSIZE;
+ */
 #define IO_TLB_SEGSIZE	128
 
 /*
  * log of the size of each IO TLB slab.  The number of slabs is command line
  * controllable.
  */
+/*
+ * 1 << 11 = 2048 (2k)
+ */
 #define IO_TLB_SHIFT 11
 
 extern void swiotlb_init(int verbose);
@@ -42,6 +63,15 @@ extern void __init swiotlb_update_mem_attributes(void);
  * Enumeration for sync targets
  */
 enum dma_sync_target {
+	/*
+	 * 在以下使用SYNC_FOR_CPU:
+	 *   - drivers/iommu/intel/iommu.c|3960| <<bounce_sync_single_for_cpu>> bounce_sync_single(dev, addr, size, dir, SYNC_FOR_CPU);
+	 *   - drivers/iommu/intel/iommu.c|3979| <<bounce_sync_sg_for_cpu>> sg_dma_len(sg), dir, SYNC_FOR_CPU);
+	 *   - drivers/xen/swiotlb-xen.c|574| <<xen_swiotlb_sync_single_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+	 *   - kernel/dma/direct.c|374| <<dma_direct_sync_sg_for_cpu>> SYNC_FOR_CPU);
+	 *   - kernel/dma/direct.h|77| <<dma_direct_sync_single_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+	 *   - kernel/dma/swiotlb.c|963| <<swiotlb_tbl_sync_single>> case SYNC_FOR_CPU:
+	 */
 	SYNC_FOR_CPU = 0,
 	SYNC_FOR_DEVICE = 1,
 };
@@ -69,6 +99,18 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t phys,
 extern enum swiotlb_force swiotlb_force;
 extern phys_addr_t io_tlb_start, io_tlb_end;
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3770| <<bounce_sync_single>> if (is_swiotlb_buffer(tlb_addr))
+ *   - drivers/iommu/intel/iommu.c|3853| <<bounce_map_single>> if (is_swiotlb_buffer(tlb_addr))
+ *   - drivers/iommu/intel/iommu.c|3881| <<bounce_unmap_single>> if (is_swiotlb_buffer(tlb_addr))
+ *   - kernel/dma/direct.c|346| <<dma_direct_sync_sg_for_device>> if (unlikely(is_swiotlb_buffer(paddr)))
+ *   - kernel/dma/direct.c|372| <<dma_direct_sync_sg_for_cpu>> if (unlikely(is_swiotlb_buffer(paddr)))
+ *   - kernel/dma/direct.c|507| <<dma_direct_need_sync>> is_swiotlb_buffer(dma_to_phys(dev, dma_addr));
+ *   - kernel/dma/direct.h|59| <<dma_direct_sync_single_for_device>> if (unlikely(is_swiotlb_buffer(paddr)))
+ *   - kernel/dma/direct.h|76| <<dma_direct_sync_single_for_cpu>> if (unlikely(is_swiotlb_buffer(paddr)))
+ *   - kernel/dma/direct.h|121| <<dma_direct_unmap_page>> if (unlikely(is_swiotlb_buffer(phys)))
+ */
 static inline bool is_swiotlb_buffer(phys_addr_t paddr)
 {
 	return paddr >= io_tlb_start && paddr < io_tlb_end;
diff --git a/kernel/dma/direct.c b/kernel/dma/direct.c
index 06c111544f61..f3650bf6ef69 100644
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@ -393,6 +393,10 @@ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
 }
 #endif
 
+/*
+ * called by:
+ *   - kernel/dma/mapping.c|216| <<dma_map_sg_attrs>> ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
+ */
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs)
 {
diff --git a/kernel/dma/direct.h b/kernel/dma/direct.h
index b98615578737..aece3ba7591d 100644
--- a/kernel/dma/direct.h
+++ b/kernel/dma/direct.h
@@ -80,6 +80,11 @@ static inline void dma_direct_sync_single_for_cpu(struct device *dev,
 		arch_dma_mark_clean(paddr, size);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|403| <<dma_direct_map_sg>> sg->dma_address = dma_direct_map_page(dev, sg_page(sg),
+ *   - kernel/dma/mapping.c|153| <<dma_map_page_attrs>> addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
+ */
 static inline dma_addr_t dma_direct_map_page(struct device *dev,
 		struct page *page, unsigned long offset, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -87,9 +92,26 @@ static inline dma_addr_t dma_direct_map_page(struct device *dev,
 	phys_addr_t phys = page_to_phys(page) + offset;
 	dma_addr_t dma_addr = phys_to_dma(dev, phys);
 
+	/*
+	 * 在以下设置swiotlb_force:
+	 *   - arch/arm64/mm/init.c|509| <<mem_init>> swiotlb_force = SWIOTLB_NO_FORCE;
+	 *   - arch/powerpc/platforms/pseries/svm.c|30| <<init_svm>> swiotlb_force = SWIOTLB_FORCE;
+	 *   - arch/s390/mm/init.c|185| <<pv_init>> swiotlb_force = SWIOTLB_FORCE;
+	 *   - arch/x86/mm/mem_encrypt.c|198| <<sme_early_init>> swiotlb_force = SWIOTLB_FORCE;
+	 *   - kernel/dma/swiotlb.c|162| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_FORCE;
+	 *   - kernel/dma/swiotlb.c|164| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_NO_FORCE;
+	 *
+	 * sme_early_init()
+	 * -> sme_early_init()
+	 *    -> if (sev_active())
+	 *           swiotlb_force = SWIOTLB_FORCE;
+	 */
 	if (unlikely(swiotlb_force == SWIOTLB_FORCE))
 		return swiotlb_map(dev, phys, size, dir, attrs);
 
+	/*
+	 * 核心思想是判断dma的地址是否满足dev的需求
+	 */
 	if (unlikely(!dma_capable(dev, dma_addr, size, true))) {
 		if (swiotlb_force != SWIOTLB_NO_FORCE)
 			return swiotlb_map(dev, phys, size, dir, attrs);
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index 51bb8fa8eb89..f380d5574b6c 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -137,10 +137,43 @@ static inline bool dma_map_direct(struct device *dev,
 	return dma_go_direct(dev, *dev->dma_mask, ops);
 }
 
+/*
+ * called by:
+ *   - drivers/dma/bcm2835-dma.c|937| <<bcm2835_dma_probe>> od->zero_page = dma_map_page_attrs(od->ddev.dev, ZERO_PAGE(0), 0,
+ *   - drivers/net/ethernet/broadcom/bnxt/bnxt.c|696| <<__bnxt_alloc_rx_page>> *mapping = dma_map_page_attrs(dev, page, 0, PAGE_SIZE, bp->rx_dir,
+ *   - drivers/net/ethernet/broadcom/bnxt/bnxt.c|822| <<bnxt_alloc_rx_page>> mapping = dma_map_page_attrs(&pdev->dev, page, offset,
+ *   - drivers/net/ethernet/cavium/thunder/nicvf_queues.c|213| <<nicvf_alloc_rcv_buffer>> *rbuf = (u64)dma_map_page_attrs(&nic->pdev->dev, nic->rb_page,
+ *   - drivers/net/ethernet/cavium/thunder/nicvf_queues.c|1576| <<nicvf_sq_append_skb>> dma_addr = dma_map_page_attrs(&nic->pdev->dev, virt_to_page(skb->data),
+ *   - drivers/net/ethernet/cavium/thunder/nicvf_queues.c|1595| <<nicvf_sq_append_skb>> dma_addr = dma_map_page_attrs(&nic->pdev->dev,
+ *   - drivers/net/ethernet/intel/i40e/i40e_txrx.c|1525| <<i40e_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0,
+ *   - drivers/net/ethernet/intel/iavf/iavf_txrx.c|830| <<iavf_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0,
+ *   - drivers/net/ethernet/intel/ice/ice_txrx.c|645| <<ice_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0, ice_rx_pg_size(rx_ring),
+ *   - drivers/net/ethernet/intel/igb/igb_main.c|8828| <<igb_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0,
+ *   - drivers/net/ethernet/intel/igc/igc_main.c|1805| <<igc_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0,
+ *   - drivers/net/ethernet/intel/ixgbe/ixgbe_main.c|1546| <<ixgbe_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0,
+ *   - drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c|622| <<ixgbevf_alloc_mapped_page>> dma = dma_map_page_attrs(rx_ring->dev, page, 0,
+ *   - drivers/net/ethernet/marvell/octeontx2/nic/otx2_common.h|563| <<otx2_dma_map_page>> iova = dma_map_page_attrs(pfvf->dev, page,
+ *   - drivers/spi/spi-bcm2835.c|934| <<bcm2835_dma_init>> bs->fill_tx_addr = dma_map_page_attrs(ctlr->dma_tx->device->dev,
+ *   - include/linux/blkdev.h|710| <<dma_map_bvec>> dma_map_page_attrs(dev, (bv)->bv_page, (bv)->bv_offset, (bv)->bv_len, \
+ *   - include/linux/dma-mapping.h|279| <<dma_map_single_attrs>> return dma_map_page_attrs(dev, virt_to_page(ptr), offset_in_page(ptr),
+ *   - include/linux/dma-mapping.h|387| <<dma_map_page>> #define dma_map_page(d, p, o, s, r) dma_map_page_attrs(d, p, o, s, r, 0)
+ *   - net/core/page_pool.c|220| <<__page_pool_alloc_pages_slow>> dma = dma_map_page_attrs(pool->p.dev, page, 0,
+ *   - net/xdp/xsk_buff_pool.c|404| <<xp_dma_map>> dma = dma_map_page_attrs(dev, pages[i], 0, PAGE_SIZE,
+ */
 dma_addr_t dma_map_page_attrs(struct device *dev, struct page *page,
 		size_t offset, size_t size, enum dma_data_direction dir,
 		unsigned long attrs)
 {
+	/*
+	 * 部分设置dma_ops和dev->dma_ops的例子:
+	 *   - drivers/acpi/scan.c|1468| <<acpi_dma_configure_id>> set_dma_ops(dev, &dma_dummy_ops);
+	 *   - drivers/iommu/intel/iommu.c|5748| <<intel_iommu_release_device>> set_dma_ops(dev, NULL);
+	 *   - drivers/iommu/intel/iommu.c|5757| <<intel_iommu_probe_finalize>> set_dma_ops(dev, &bounce_dma_ops);
+	 *   - drivers/iommu/intel/iommu.c|5759| <<intel_iommu_probe_finalize>> set_dma_ops(dev, &intel_dma_ops);
+	 *   - drivers/iommu/intel/iommu.c|5761| <<intel_iommu_probe_finalize>> set_dma_ops(dev, NULL);
+	 *   - arch/x86/xen/pci-swiotlb-xen.c|70| <<pci_xen_swiotlb_init>> dma_ops = &xen_swiotlb_dma_ops;
+	 *   - arch/x86/xen/pci-swiotlb-xen.c|90| <<pci_xen_swiotlb_init_late>> dma_ops = &xen_swiotlb_dma_ops;
+	 */
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 	dma_addr_t addr;
 
@@ -177,6 +210,16 @@ EXPORT_SYMBOL(dma_unmap_page_attrs);
  * dma_maps_sg_attrs returns 0 on error and > 0 on success.
  * It should never return a value < 0.
  */
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/i915_gem_gtt.c|31| <<i915_gem_gtt_prepare_pages>> if (dma_map_sg_attrs(&obj->base.dev->pdev->dev,
+ *   - drivers/nvme/host/pci.c|850| <<nvme_map_data>> nr_mapped = dma_map_sg_attrs(dev->dev, iod->sg, iod->nents,
+ *   - drivers/pci/p2pdma.c|888| <<pci_p2pdma_map_sg_attrs>> return dma_map_sg_attrs(dev, sg, nents, dir, attrs);
+ *   - drivers/vdpa/mlx5/core/mr.c|279| <<map_direct_mr>> err = dma_map_sg_attrs(dma, mr->sg_head.sgl, mr->nsg, DMA_BIDIRECTIONAL, 0);
+ *   - include/linux/dma-mapping.h|324| <<dma_map_sgtable>> nents = dma_map_sg_attrs(dev, sgt->sgl, sgt->orig_nents, dir, attrs);
+ *   - include/linux/dma-mapping.h|385| <<dma_map_sg>> #define dma_map_sg(d, s, n, r) dma_map_sg_attrs(d, s, n, r, 0)
+ *   - include/rdma/ib_verbs.h|4048| <<ib_dma_map_sg_attrs>> return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
+ */
 int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg, int nents,
 		enum dma_data_direction dir, unsigned long attrs)
 {
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 781b9dca197c..156387a08bf8 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -50,9 +50,42 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/swiotlb.h>
 
+/*
+ * 设置dma_ops和dev->dma_ops的地方:
+ *   - drivers/acpi/scan.c|1468| <<acpi_dma_configure_id>> set_dma_ops(dev, &dma_dummy_ops);
+ *   - drivers/iommu/intel/iommu.c|5748| <<intel_iommu_release_device>> set_dma_ops(dev, NULL);
+ *   - drivers/iommu/intel/iommu.c|5757| <<intel_iommu_probe_finalize>> set_dma_ops(dev, &bounce_dma_ops);
+ *   - drivers/iommu/intel/iommu.c|5759| <<intel_iommu_probe_finalize>> set_dma_ops(dev, &intel_dma_ops);
+ *   - drivers/iommu/intel/iommu.c|5761| <<intel_iommu_probe_finalize>> set_dma_ops(dev, NULL);
+ *   - arch/x86/xen/pci-swiotlb-xen.c|70| <<pci_xen_swiotlb_init>> dma_ops = &xen_swiotlb_dma_ops;
+ *   - arch/x86/xen/pci-swiotlb-xen.c|90| <<pci_xen_swiotlb_init_late>> dma_ops = &xen_swiotlb_dma_ops;
+ */
+
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|288| <<swiotlb_init_with_tbl>> io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|444| <<swiotlb_late_init_with_tbl>> io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|634| <<swiotlb_tbl_map_single>> for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)
+ *   - kernel/dma/swiotlb.c|720| <<swiotlb_tbl_unmap_single>> for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE -1) && io_tlb_list[i]; i--)
+ *
+ * 返回的结果永远是0-127
+ */
 #define OFFSET(val,align) ((unsigned long)	\
 	                   ( (val) & ( (align) - 1)))
 
+/*
+ * 在以下使用SLABS_PER_PAGE:
+ *   - drivers/xen/swiotlb-xen.c|210| <<xen_swiotlb_init>> #define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))
+ *   - drivers/xen/swiotlb-xen.c|212| <<xen_swiotlb_init>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ *   - drivers/xen/swiotlb-xen.c|221| <<xen_swiotlb_init>> xen_io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|295| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|298| <<swiotlb_late_init_with_default_size>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ *   - kernel/dma/swiotlb.c|313| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *
+ * #define IO_TLB_SHIFT 11 -->  1 << 11 = 2048 (2k)
+ *
+ * 理论上SLABS_PER_PAGE=2
+ */
 #define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))
 
 /*
@@ -60,8 +93,26 @@
  * 64bit capable cards will only lightly use the swiotlb.  If we can't
  * allocate a contiguous 1MB, we're probably in trouble anyway.
  */
+/*
+ * 在以下使用IO_TLB_MIN_SLABS:
+ *   - kernel/dma/swiotlb.c|380| <<swiotlb_late_init_with_default_size>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ */
 #define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)
 
+/*
+ * 在以下设置swiotlb_force:
+ *   - arch/arm64/mm/init.c|509| <<mem_init>> swiotlb_force = SWIOTLB_NO_FORCE;
+ *   - arch/powerpc/platforms/pseries/svm.c|30| <<init_svm>> swiotlb_force = SWIOTLB_FORCE;
+ *   - arch/s390/mm/init.c|185| <<pv_init>> swiotlb_force = SWIOTLB_FORCE;
+ *   - arch/x86/mm/mem_encrypt.c|198| <<sme_early_init>> swiotlb_force = SWIOTLB_FORCE;
+ *   - kernel/dma/swiotlb.c|162| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_FORCE;
+ *   - kernel/dma/swiotlb.c|164| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_NO_FORCE;
+ *
+ * sme_early_init()
+ * -> sme_early_init()
+ *    -> if (sev_active())
+ *           swiotlb_force = SWIOTLB_FORCE;
+ */
 enum swiotlb_force swiotlb_force;
 
 /*
@@ -69,17 +120,67 @@ enum swiotlb_force swiotlb_force;
  * swiotlb_tbl_sync_single_*, to see if the memory was in fact allocated by this
  * API.
  */
+/*
+ * 在以下使用io_tlb_start:
+ *   - arch/powerpc/platforms/pseries/svm.c|58| <<svm_swiotlb_init>> if (io_tlb_start)
+ *   - arch/powerpc/platforms/pseries/svm.c|59| <<svm_swiotlb_init>> memblock_free_early(io_tlb_start,
+ *   - drivers/xen/swiotlb-xen.c|195| <<xen_swiotlb_init>> if (io_tlb_start != 0) {
+ *   - drivers/xen/swiotlb-xen.c|196| <<xen_swiotlb_init>> xen_io_tlb_start = phys_to_virt(io_tlb_start);
+ *   - include/linux/swiotlb.h|74| <<is_swiotlb_buffer>> return paddr >= io_tlb_start && paddr < io_tlb_end;
+ *   - kernel/dma/swiotlb.c|190| <<swiotlb_print_info>> pr_info("mapped [mem %pa-%pa] (%luMB)\n", &io_tlb_start, &io_tlb_end,
+ *   - kernel/dma/swiotlb.c|208| <<swiotlb_update_mem_attributes>> vaddr = phys_to_virt(io_tlb_start);
+ *   - kernel/dma/swiotlb.c|229| <<swiotlb_init_with_tbl>> io_tlb_start = __pa(tlb);
+ *   - kernel/dma/swiotlb.c|230| <<swiotlb_init_with_tbl>> io_tlb_end = io_tlb_start + bytes;
+ *   - kernel/dma/swiotlb.c|286| <<swiotlb_init>> if (io_tlb_start) {
+ *   - kernel/dma/swiotlb.c|287| <<swiotlb_init>> memblock_free_early(io_tlb_start,
+ *   - kernel/dma/swiotlb.c|289| <<swiotlb_init>> io_tlb_start = 0;
+ *   - kernel/dma/swiotlb.c|347| <<swiotlb_cleanup>> io_tlb_start = 0;
+ *   - kernel/dma/swiotlb.c|360| <<swiotlb_late_init_with_tbl>> io_tlb_start = virt_to_phys(tlb);
+ *   - kernel/dma/swiotlb.c|361| <<swiotlb_late_init_with_tbl>> io_tlb_end = io_tlb_start + bytes;
+ *   - kernel/dma/swiotlb.c|417| <<swiotlb_exit>> free_pages((unsigned long )phys_to_virt(io_tlb_start),
+ *   - kernel/dma/swiotlb.c|424| <<swiotlb_exit>> memblock_free_late(io_tlb_start,
+ *   - kernel/dma/swiotlb.c|481| <<swiotlb_tbl_map_single>> dma_addr_t tbl_dma_addr = phys_to_dma_unencrypted(hwdev, io_tlb_start);
+ *   - kernel/dma/swiotlb.c|564| <<swiotlb_tbl_map_single>> tlb_addr = io_tlb_start + (index << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|615| <<swiotlb_tbl_unmap_single>> int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|660| <<swiotlb_tbl_sync_single>> int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
+ */
 phys_addr_t io_tlb_start, io_tlb_end;
 
 /*
  * The number of IO TLB blocks (in groups of 64) between io_tlb_start and
  * io_tlb_end.  This is command line adjustable via setup_io_tlb_npages.
  */
+/*
+ * 在以下设置io_tlb_nslabs:
+ *   - arch/powerpc/platforms/pseries/svm.c|49| <<svm_swiotlb_init>> io_tlb_nslabs = (swiotlb_size_or_default() >> IO_TLB_SHIFT);
+ *   - arch/powerpc/platforms/pseries/svm.c|50| <<svm_swiotlb_init>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|155| <<setup_io_tlb_npages>> io_tlb_nslabs = simple_strtoul(str, &str, 0);
+ *   - kernel/dma/swiotlb.c|157| <<setup_io_tlb_npages>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|165| <<setup_io_tlb_npages>> io_tlb_nslabs = 1;
+ *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> io_tlb_nslabs = nslabs;
+ *   - kernel/dma/swiotlb.c|329| <<swiotlb_init>> io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|330| <<swiotlb_init>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|369| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|377| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|389| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = req_nslabs;
+ *   - kernel/dma/swiotlb.c|395| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|408| <<swiotlb_cleanup>> io_tlb_nslabs = 0;
+ *   - kernel/dma/swiotlb.c|419| <<swiotlb_late_init_with_tbl>> io_tlb_nslabs = nslabs;
+ */
 static unsigned long io_tlb_nslabs;
 
 /*
  * The number of used IO TLB block
  */
+/*
+ * 在以下使用io_tlb_used:
+ *   - kernel/dma/swiotlb.c|606| <<swiotlb_tbl_map_single>> if (unlikely(nslots > io_tlb_nslabs - io_tlb_used))
+ *   - kernel/dma/swiotlb.c|653| <<swiotlb_tbl_map_single>> tmp_io_tlb_used = io_tlb_used;
+ *   - kernel/dma/swiotlb.c|661| <<swiotlb_tbl_map_single>> io_tlb_used += nslots;
+ *   - kernel/dma/swiotlb.c|723| <<swiotlb_tbl_unmap_single>> io_tlb_used -= nslots;
+ *   - kernel/dma/swiotlb.c|820| <<swiotlb_create_debugfs>> debugfs_create_ulong("io_tlb_used", 0400, root, &io_tlb_used);
+ */
 static unsigned long io_tlb_used;
 
 /*
@@ -87,12 +188,26 @@ static unsigned long io_tlb_used;
  * each index
  */
 static unsigned int *io_tlb_list;
+/*
+ * 在以下使用io_tlb_index:
+ *   - kernel/dma/swiotlb.c|356| <<swiotlb_init_with_tbl>> io_tlb_index = 0;
+ *   - kernel/dma/swiotlb.c|512| <<swiotlb_late_init_with_tbl>> io_tlb_index = 0;
+ *   - kernel/dma/swiotlb.c|674| <<swiotlb_tbl_map_single>> index = ALIGN(io_tlb_index, stride);
+ *   - kernel/dma/swiotlb.c|707| <<swiotlb_tbl_map_single>> io_tlb_index = ((index + nslots) < io_tlb_nslabs
+ */
 static unsigned int io_tlb_index;
 
 /*
  * Max segment that we can provide which (if pages are contingous) will
  * not be bounced (unless SWIOTLB_FORCE is set).
  */
+/*
+ * 在以下使用max_segment:
+ *   - kernel/dma/swiotlb.c|182| <<swiotlb_max_segment>> return unlikely(no_iotlb_memory) ? 0 : max_segment;
+ *   - kernel/dma/swiotlb.c|189| <<swiotlb_set_max_segment>> max_segment = 1;
+ *   - kernel/dma/swiotlb.c|191| <<swiotlb_set_max_segment>> max_segment = rounddown(val, PAGE_SIZE);
+ *   - kernel/dma/swiotlb.c|409| <<swiotlb_cleanup>> max_segment = 0;
+ */
 static unsigned int max_segment;
 
 /*
@@ -100,13 +215,36 @@ static unsigned int max_segment;
  * for the sync operations.
  */
 #define INVALID_PHYS_ADDR (~(phys_addr_t)0)
+/*
+ * 在以下分配io_tlb_orig_addr:
+ *   - kernel/dma/swiotlb.c|347| <<swiotlb_init_with_tbl>> io_tlb_orig_addr = memblock_alloc(alloc_size, PAGE_SIZE);
+ *   - kernel/dma/swiotlb.c|501| <<swiotlb_late_init_with_tbl>> io_tlb_orig_addr = (phys_addr_t *)
+ */
 static phys_addr_t *io_tlb_orig_addr;
 
 /*
  * Protect the above data structures in the map and unmap calls
  */
+/*
+ * 在以下使用io_tlb_lock:
+ *   - kernel/dma/swiotlb.c|826| <<swiotlb_tbl_map_single>> spin_lock_irqsave(&io_tlb_lock, flags);
+ *   - kernel/dma/swiotlb.c|877| <<swiotlb_tbl_map_single>> spin_unlock_irqrestore(&io_tlb_lock, flags);
+ *   - kernel/dma/swiotlb.c|884| <<swiotlb_tbl_map_single>> spin_unlock_irqrestore(&io_tlb_lock, flags);
+ *   - kernel/dma/swiotlb.c|935| <<swiotlb_tbl_unmap_single>> spin_lock_irqsave(&io_tlb_lock, flags);
+ *   - kernel/dma/swiotlb.c|956| <<swiotlb_tbl_unmap_single>> spin_unlock_irqrestore(&io_tlb_lock, flags)
+ */
 static DEFINE_SPINLOCK(io_tlb_lock);
 
+/*
+ * 在以下使用late_alloc:
+ *   - kernel/dma/swiotlb.c|190| <<swiotlb_update_mem_attributes>> if (no_iotlb_memory || late_alloc)
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> late_alloc = 1;
+ *   - kernel/dma/swiotlb.c|390| <<swiotlb_exit>> if (late_alloc) {
+ *
+ * sta2x11_new_instance()
+ * -> swiotlb_late_init_with_default_size()
+ *    -> swiotlb_late_init_with_tbl()
+ */
 static int late_alloc;
 
 static int __init
@@ -122,6 +260,15 @@ setup_io_tlb_npages(char *str)
 	if (!strcmp(str, "force")) {
 		swiotlb_force = SWIOTLB_FORCE;
 	} else if (!strcmp(str, "noforce")) {
+		/*
+		 * 在以下设置swiotlb_force:
+		 *   - arch/arm64/mm/init.c|509| <<mem_init>> swiotlb_force = SWIOTLB_NO_FORCE;
+		 *   - arch/powerpc/platforms/pseries/svm.c|30| <<init_svm>> swiotlb_force = SWIOTLB_FORCE;
+		 *   - arch/s390/mm/init.c|185| <<pv_init>> swiotlb_force = SWIOTLB_FORCE;
+		 *   - arch/x86/mm/mem_encrypt.c|198| <<sme_early_init>> swiotlb_force = SWIOTLB_FORCE;
+		 *   - kernel/dma/swiotlb.c|162| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_FORCE;
+		 *   - kernel/dma/swiotlb.c|164| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_NO_FORCE;
+		 */
 		swiotlb_force = SWIOTLB_NO_FORCE;
 		io_tlb_nslabs = 1;
 	}
@@ -130,30 +277,106 @@ setup_io_tlb_npages(char *str)
 }
 early_param("swiotlb", setup_io_tlb_npages);
 
+/*
+ * 在以下使用no_iotlb_memory:
+ *   - kernel/dma/swiotlb.c|235| <<swiotlb_nr_tbl>> return unlikely(no_iotlb_memory) ? 0 : io_tlb_nslabs;
+ *   - kernel/dma/swiotlb.c|241| <<swiotlb_max_segment>> return unlikely(no_iotlb_memory) ? 0 : max_segment;
+ *   - kernel/dma/swiotlb.c|284| <<swiotlb_print_info>> if (no_iotlb_memory) {
+ *   - kernel/dma/swiotlb.c|308| <<swiotlb_update_mem_attributes>> if (no_iotlb_memory || late_alloc)
+ *   - kernel/dma/swiotlb.c|357| <<swiotlb_init_with_tbl>> no_iotlb_memory = false;
+ *   - kernel/dma/swiotlb.c|417| <<swiotlb_init>> no_iotlb_memory = true;
+ *   - kernel/dma/swiotlb.c|513| <<swiotlb_late_init_with_tbl>> no_iotlb_memory = false;
+ *   - kernel/dma/swiotlb.c|628| <<swiotlb_tbl_map_single>> if (no_iotlb_memory)
+ */
 static bool no_iotlb_memory;
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c|1360| <<amdgpu_ttm_tt_populate>> if (adev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c|1403| <<amdgpu_ttm_tt_unpopulate>> if (adev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c|2591| <<amdgpu_ttm_debugfs_init>> if (!(adev->need_swiotlb && swiotlb_nr_tbl()))
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|45| <<i915_gem_object_get_pages_internal>> if (swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/nouveau/nouveau_bo.c|1331| <<nouveau_ttm_tt_populate>> if (swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/nouveau/nouveau_bo.c|1361| <<nouveau_ttm_tt_unpopulate>> if (swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|660| <<radeon_ttm_tt_populate>> if (rdev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|691| <<radeon_ttm_tt_unpopulate>> if (rdev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|1102| <<radeon_ttm_debugfs_init>> if (!(rdev->need_swiotlb && swiotlb_nr_tbl()))
+ *   - drivers/pci/xen-pcifront.c|696| <<pcifront_connect_and_init_dma>> if (!err && !swiotlb_nr_tbl()) {
+ *   - drivers/xen/swiotlb-xen.c|197| <<xen_swiotlb_init>> xen_io_tlb_nslabs = swiotlb_nr_tbl();
+ *
+ * 需要参数type
+ */
 unsigned long swiotlb_nr_tbl(void)
 {
 	return unlikely(no_iotlb_memory) ? 0 : io_tlb_nslabs;
 }
 EXPORT_SYMBOL_GPL(swiotlb_nr_tbl);
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|48| <<i915_gem_object_get_pages_internal>> max_segment = swiotlb_max_segment();
+ *   - drivers/gpu/drm/i915/i915_scatterlist.h|112| <<i915_sg_segment_size>> unsigned int size = swiotlb_max_segment();
+ *   - drivers/mmc/host/sdhci.c|4585| <<sdhci_setup_host>> if (swiotlb_max_segment()) {
+ *
+ * 需要参数type
+ */
 unsigned int swiotlb_max_segment(void)
 {
+	/*
+	 * 在以下使用max_segment:
+	 *   - kernel/dma/swiotlb.c|182| <<swiotlb_max_segment>> return unlikely(no_iotlb_memory) ? 0 : max_segment;
+	 *   - kernel/dma/swiotlb.c|189| <<swiotlb_set_max_segment>> max_segment = 1;
+	 *   - kernel/dma/swiotlb.c|191| <<swiotlb_set_max_segment>> max_segment = rounddown(val, PAGE_SIZE);
+	 *   - kernel/dma/swiotlb.c|409| <<swiotlb_cleanup>> max_segment = 0;
+	 */
 	return unlikely(no_iotlb_memory) ? 0 : max_segment;
 }
 EXPORT_SYMBOL_GPL(swiotlb_max_segment);
 
+/*
+ * [0] swiotlb_set_max_segment
+ * [0] swiotlb_init_with_tbl
+ * [0] swiotlb_init
+ * [0] pci_iommu_alloc
+ * [0] mem_init
+ * [0] start_kernel
+ * [0] secondary_startup_64_no_verify
+ *
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|267| <<xen_swiotlb_init>> swiotlb_set_max_segment(PAGE_SIZE);
+ *   - kernel/dma/swiotlb.c|297| <<swiotlb_init_with_tbl>> swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|454| <<swiotlb_late_init_with_tbl>> swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
+ *
+ * 需要参数type
+ */
 void swiotlb_set_max_segment(unsigned int val)
 {
+	/*
+	 * max_segment默认67108864(64MB)
+	 */
 	if (swiotlb_force == SWIOTLB_FORCE)
 		max_segment = 1;
 	else
 		max_segment = rounddown(val, PAGE_SIZE);
+	/*
+	 * rounddown() = round down to next specified multiple
+	 */
 }
 
 /* default to 64MB */
+/*
+ * 在以下使用IO_TLB_DEFAULT_SIZE:
+ *   - kernel/dma/swiotlb.c|270| <<swiotlb_size_or_default>> return size ? size : (IO_TLB_DEFAULT_SIZE);
+ *   - kernel/dma/swiotlb.c|386| <<swiotlb_init>> size_t default_size = IO_TLB_DEFAULT_SIZE;
+ */
 #define IO_TLB_DEFAULT_SIZE (64UL<<20)
+/*
+ * called by:
+ *   - arch/powerpc/platforms/pseries/svm.c|49| <<svm_swiotlb_init>> io_tlb_nslabs = (swiotlb_size_or_default() >> IO_TLB_SHIFT);
+ *   - arch/x86/kernel/setup.c|443| <<reserve_crashkernel_low>> low_size = max(swiotlb_size_or_default() + (8UL << 20), 256UL << 20);
+ *
+ * 默认使用LO
+ */
 unsigned long swiotlb_size_or_default(void)
 {
 	unsigned long size;
@@ -163,6 +386,13 @@ unsigned long swiotlb_size_or_default(void)
 	return size ? size : (IO_TLB_DEFAULT_SIZE);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|23| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-swiotlb.c|76| <<pci_swiotlb_late_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|257| <<swiotlb_init_with_tbl>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|390| <<swiotlb_late_init_with_tbl>> swiotlb_print_info();
+ */
 void swiotlb_print_info(void)
 {
 	unsigned long bytes = io_tlb_nslabs << IO_TLB_SHIFT;
@@ -172,6 +402,10 @@ void swiotlb_print_info(void)
 		return;
 	}
 
+	/*
+	 * [    0.587457] PCI-DMA: Using software bounce buffering for IO (SWIOTLB)
+	 * [    0.588574] software IO TLB: mapped [mem 0x00000000bbfdd000-0x00000000bffdd000] (64MB)
+	 */
 	pr_info("mapped [mem %pa-%pa] (%luMB)\n", &io_tlb_start, &io_tlb_end,
 	       bytes >> 20);
 }
@@ -182,6 +416,12 @@ void swiotlb_print_info(void)
  * call SWIOTLB when the operations are possible.  It needs to be called
  * before the SWIOTLB memory is used.
  */
+/*
+ * called by:
+ *   - arch/powerpc/platforms/pseries/svm.c|33| <<init_svm>> swiotlb_update_mem_attributes();
+ *   - arch/s390/mm/init.c|184| <<pv_init>> swiotlb_update_mem_attributes();
+ *   - arch/x86/mm/mem_encrypt.c|443| <<mem_encrypt_init>> swiotlb_update_mem_attributes();
+ */
 void __init swiotlb_update_mem_attributes(void)
 {
 	void *vaddr;
@@ -196,6 +436,13 @@ void __init swiotlb_update_mem_attributes(void)
 	memset(vaddr, 0, bytes);
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|248| <<plat_swiotlb_setup>> if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
+ *   - arch/powerpc/platforms/pseries/svm.c|55| <<svm_swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, false))
+ *   - drivers/xen/swiotlb-xen.c|247| <<xen_swiotlb_init>> if (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs,
+ *   - kernel/dma/swiotlb.c|276| <<swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, verbose))
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	unsigned long i, bytes;
@@ -228,6 +475,13 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 		io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
 		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 	}
+	/*
+	 * 在以下使用io_tlb_index:
+	 *   - kernel/dma/swiotlb.c|356| <<swiotlb_init_with_tbl>> io_tlb_index = 0;
+	 *   - kernel/dma/swiotlb.c|512| <<swiotlb_late_init_with_tbl>> io_tlb_index = 0;
+	 *   - kernel/dma/swiotlb.c|674| <<swiotlb_tbl_map_single>> index = ALIGN(io_tlb_index, stride);
+	 *   - kernel/dma/swiotlb.c|707| <<swiotlb_tbl_map_single>> io_tlb_index = ((index + nslots) < io_tlb_nslabs
+	 */
 	io_tlb_index = 0;
 	no_iotlb_memory = false;
 
@@ -242,21 +496,52 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * [0] swiotlb_init_with_tbl
+ * [0] swiotlb_init
+ * [0] pci_iommu_alloc
+ * [0] mem_init
+ * [0] start_kernel
+ * [0] secondary_startup_64_no_verify
+ *
+ * called by:
+ *   - arch/arm/mm/init.c|382| <<mem_init>> swiotlb_init(1);
+ *   - arch/arm64/mm/init.c|507| <<mem_init>> swiotlb_init(1);
+ *   - arch/ia64/mm/init.c|663| <<mem_init>> swiotlb_init(1);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/powerpc/mm/mem.c|291| <<mem_init>> swiotlb_init(0);
+ *   - arch/riscv/kernel/setup.c|94| <<setup_arch>> swiotlb_init(1);
+ *   - arch/s390/mm/init.c|183| <<pv_init>> swiotlb_init(1);
+ *   - arch/x86/kernel/pci-swiotlb.c|94| <<pci_swiotlb_init>> swiotlb_init(0);
+ */
 void  __init
 swiotlb_init(int verbose)
 {
+	/* 默认64MB */
 	size_t default_size = IO_TLB_DEFAULT_SIZE;
 	unsigned char *vstart;
 	unsigned long bytes;
 
 	if (!io_tlb_nslabs) {
+		/*
+		 * 32768
+		 */
 		io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
 		io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
 	}
 
+	/*
+	 * 1 << 11 = 2048 (2k) --> 每个slot是2K
+	 */
 	bytes = io_tlb_nslabs << IO_TLB_SHIFT;
 
 	/* Get IO TLB memory from the low pages */
+	/*
+	 * 应该是从4G以下分配??? 分配MEMBLOCK_LOW_LIMIT-->ARCH_LOW_ADDRESS_LIMIT
+	 *
+	 * 如果不是为了bit, 用普通的memblock_alloc(alloc_size, PAGE_SIZE);
+	 */
 	vstart = memblock_alloc_low(PAGE_ALIGN(bytes), PAGE_SIZE);
 	if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, verbose))
 		return;
@@ -275,6 +560,10 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/x86/pci/sta2x11-fixup.c|59| <<sta2x11_new_instance>> if (swiotlb_late_init_with_default_size(size))
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -319,6 +608,11 @@ swiotlb_late_init_with_default_size(size_t default_size)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|594| <<swiotlb_late_init_with_tbl>> swiotlb_cleanup();
+ *   - kernel/dma/swiotlb.c|624| <<swiotlb_exit>> swiotlb_cleanup();
+ */
 static void swiotlb_cleanup(void)
 {
 	io_tlb_end = 0;
@@ -327,6 +621,15 @@ static void swiotlb_cleanup(void)
 	max_segment = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|262| <<xen_swiotlb_init>> rc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);
+ *   - kernel/dma/swiotlb.c|528| <<swiotlb_late_init_with_default_size>> rc = swiotlb_late_init_with_tbl(vstart, io_tlb_nslabs);
+ *
+ * sta2x11_new_instance()
+ * -> swiotlb_late_init_with_default_size()
+ *    -> swiotlb_late_init_with_tbl()
+ */
 int
 swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 {
@@ -351,6 +654,11 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 	if (!io_tlb_list)
 		goto cleanup3;
 
+	/*
+	 * 在以下分配io_tlb_orig_addr:
+	 *   - kernel/dma/swiotlb.c|347| <<swiotlb_init_with_tbl>> io_tlb_orig_addr = memblock_alloc(alloc_size, PAGE_SIZE);
+	 *   - kernel/dma/swiotlb.c|501| <<swiotlb_late_init_with_tbl>> io_tlb_orig_addr = (phys_addr_t *)
+	 */
 	io_tlb_orig_addr = (phys_addr_t *)
 		__get_free_pages(GFP_KERNEL,
 				 get_order(io_tlb_nslabs *
@@ -367,6 +675,12 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 
 	swiotlb_print_info();
 
+	/*
+	 * 在以下使用late_alloc:
+	 *   - kernel/dma/swiotlb.c|190| <<swiotlb_update_mem_attributes>> if (no_iotlb_memory || late_alloc)
+	 *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> late_alloc = 1;
+	 *   - kernel/dma/swiotlb.c|390| <<swiotlb_exit>> if (late_alloc) {
+	 */
 	late_alloc = 1;
 
 	swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
@@ -382,6 +696,12 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/ia64/hp/common/sba_iommu.c|2106| <<sba_init>> swiotlb_exit();
+ *   - arch/powerpc/kernel/dma-swiotlb.c|25| <<check_swiotlb_enabled>> swiotlb_exit();
+ *   - arch/x86/kernel/pci-swiotlb.c|111| <<pci_swiotlb_late_init>> swiotlb_exit();
+ */
 void __init swiotlb_exit(void)
 {
 	if (!io_tlb_orig_addr)
@@ -408,6 +728,15 @@ void __init swiotlb_exit(void)
 /*
  * Bounce: copy the swiotlb buffer from or back to the original dma location
  */
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|572| <<swiotlb_tbl_map_single>> swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_TO_DEVICE);
+ *   - kernel/dma/swiotlb.c|595| <<swiotlb_tbl_unmap_single>> swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_FROM_DEVICE);
+ *   - kernel/dma/swiotlb.c|641| <<swiotlb_tbl_sync_single>> swiotlb_bounce(orig_addr, tlb_addr,
+ *   - kernel/dma/swiotlb.c|648| <<swiotlb_tbl_sync_single>> swiotlb_bounce(orig_addr, tlb_addr,
+ *
+ * Bounce: copy the swiotlb buffer from or back to the original dma location
+ */
 static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 			   size_t size, enum dma_data_direction dir)
 {
@@ -445,6 +774,36 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 	}
 }
 
+/*
+ * [0] swiotlb_tbl_map_single
+ * [0] swiotlb_map
+ * [0] dma_map_page_attrs
+ * [0] e1000_xmit_frame
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] __ip_queue_xmit
+ * [0] __tcp_transmit_skb
+ * [0] tcp_write_xmit
+ * [0] __tcp_push_pending_frames
+ * [0] tcp_sendmsg_locked
+ * [0] tcp_sendmsg
+ * [0] sock_sendmsg
+ * [0] sock_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3821| <<bounce_map_single>> tlb_addr = swiotlb_tbl_map_single(dev, paddr, size,
+ *   - drivers/xen/swiotlb-xen.c|398| <<xen_swiotlb_map_page>> map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
+ *   - kernel/dma/swiotlb.c|705| <<swiotlb_map>> swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, dir,
+ */
 phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 		size_t mapping_size, size_t alloc_size,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -471,15 +830,28 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 		return (phys_addr_t)DMA_MAPPING_ERROR;
 	}
 
+	/*
+	 * 这个用来boundary, 不是min/max的mask
+	 * 下面的max_slots的变量名不对
+	 */
 	mask = dma_get_seg_boundary(hwdev);
 
 	tbl_dma_addr &= mask;
 
+	/*
+	 * offset_slots只在下面的某一处使用
+	 */
 	offset_slots = ALIGN(tbl_dma_addr, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
 
 	/*
 	 * Carefully handle integer overflow which can occur when mask == ~0UL.
 	 */
+	/*
+	 * mask和max_slots都是unsigned long, max_slots只在下面的某一处使用
+	 *
+	 * 如果mask是0xffffffff, mask+1就是0x100000000
+	 * 如果mask是0xffffffffffffffff, mask+1就是0x0
+	 */
 	max_slots = mask + 1
 		    ? ALIGN(mask + 1, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT
 		    : 1UL << (BITS_PER_LONG - IO_TLB_SHIFT);
@@ -489,6 +861,9 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 	 * (and hence alignment) to a page size.
 	 */
 	nslots = ALIGN(alloc_size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
+	/*
+	 * x86下stride是2或者1
+	 */
 	if (alloc_size >= PAGE_SIZE)
 		stride = (1 << (PAGE_SHIFT - IO_TLB_SHIFT));
 	else
@@ -502,15 +877,29 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 	 */
 	spin_lock_irqsave(&io_tlb_lock, flags);
 
+	/*
+	 * 寻找min_not_zero(*dev->dma_mask, dev->bus_dma_limit);
+	 */
+
 	if (unlikely(nslots > io_tlb_nslabs - io_tlb_used))
 		goto not_found;
 
+	/*
+	 * 在以下使用io_tlb_index:
+	 *   - kernel/dma/swiotlb.c|356| <<swiotlb_init_with_tbl>> io_tlb_index = 0;
+	 *   - kernel/dma/swiotlb.c|512| <<swiotlb_late_init_with_tbl>> io_tlb_index = 0;
+	 *   - kernel/dma/swiotlb.c|674| <<swiotlb_tbl_map_single>> index = ALIGN(io_tlb_index, stride);
+	 *   - kernel/dma/swiotlb.c|707| <<swiotlb_tbl_map_single>> io_tlb_index = ((index + nslots) < io_tlb_nslabs
+	 */
 	index = ALIGN(io_tlb_index, stride);
 	if (index >= io_tlb_nslabs)
 		index = 0;
 	wrap = index;
 
 	do {
+		/*
+		 * 检查地址是否符合alignment的要求
+		 */
 		while (iommu_is_span_boundary(index, nslots, offset_slots,
 					      max_slots)) {
 			index += stride;
@@ -577,6 +966,17 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 /*
  * tlb_addr is the physical address of the bounce buffer to unmap.
  */
+/*
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3854| <<bounce_map_single>> swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ *   - drivers/iommu/intel/iommu.c|3882| <<bounce_unmap_single>> swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ *   - drivers/xen/swiotlb-xen.c|425| <<xen_swiotlb_map_page>> swiotlb_tbl_unmap_single(dev, map, size, size, dir,
+ *   - drivers/xen/swiotlb-xen.c|464| <<xen_swiotlb_unmap_page>> swiotlb_tbl_unmap_single(hwdev, paddr, size, size, dir, attrs);
+ *   - kernel/dma/direct.h|122| <<dma_direct_unmap_page>> swiotlb_tbl_unmap_single(dev, phys, size, size, dir, attrs);
+ *   - kernel/dma/swiotlb.c|938| <<swiotlb_map>> swiotlb_tbl_unmap_single(dev, swiotlb_addr, size, size, dir,
+ *
+ * 需要
+ */
 void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 			      size_t mapping_size, size_t alloc_size,
 			      enum dma_data_direction dir, unsigned long attrs)
@@ -624,6 +1024,16 @@ void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 	spin_unlock_irqrestore(&io_tlb_lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3771| <<bounce_sync_single>> swiotlb_tbl_sync_single(dev, tlb_addr, size, dir, target);
+ *   - drivers/xen/swiotlb-xen.c|481| <<xen_swiotlb_sync_single_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+ *   - drivers/xen/swiotlb-xen.c|491| <<xen_swiotlb_sync_single_for_device>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
+ *   - kernel/dma/direct.c|347| <<dma_direct_sync_sg_for_device>> swiotlb_tbl_sync_single(dev, paddr, sg->length,
+ *   - kernel/dma/direct.c|373| <<dma_direct_sync_sg_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, sg->length, dir,
+ *   - kernel/dma/direct.h|60| <<dma_direct_sync_single_for_device>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
+ *   - kernel/dma/direct.h|77| <<dma_direct_sync_single_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+ */
 void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 			     size_t size, enum dma_data_direction dir,
 			     enum dma_sync_target target)
@@ -659,6 +1069,13 @@ void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
  * Create a swiotlb mapping for the buffer at @paddr, and in case of DMAing
  * to the device copy the data into it as well.
  */
+/*
+ * called by:
+ *   - kernel/dma/direct.h|91| <<dma_direct_map_page>> return swiotlb_map(dev, phys, size, dir, attrs);
+ *   - kernel/dma/direct.h|95| <<dma_direct_map_page>> return swiotlb_map(dev, phys, size, dir, attrs);
+ *
+ * amd sev的入口函数
+ */
 dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
 {
@@ -668,6 +1085,12 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 	trace_swiotlb_bounced(dev, phys_to_dma(dev, paddr), size,
 			      swiotlb_force);
 
+	/*
+	 * 在以下调用swiotlb_tbl_map_single():
+	 *   - drivers/iommu/intel/iommu.c|3821| <<bounce_map_single>> tlb_addr = swiotlb_tbl_map_single(dev, paddr, size,
+	 *   - drivers/xen/swiotlb-xen.c|398| <<xen_swiotlb_map_page>> map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
+	 *   - kernel/dma/swiotlb.c|705| <<swiotlb_map>> swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, dir,
+	 */
 	swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, dir,
 			attrs);
 	if (swiotlb_addr == (phys_addr_t)DMA_MAPPING_ERROR)
@@ -676,6 +1099,15 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 	/* Ensure that the address returned is DMA'ble */
 	dma_addr = phys_to_dma_unencrypted(dev, swiotlb_addr);
 	if (unlikely(!dma_capable(dev, dma_addr, size, true))) {
+		/*
+		 * 在以下调用swiotlb_tbl_unmap_single():
+		 *   - drivers/iommu/intel/iommu.c|3854| <<bounce_map_single>> swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+		 *   - drivers/iommu/intel/iommu.c|3882| <<bounce_unmap_single>> swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+		 *   - drivers/xen/swiotlb-xen.c|425| <<xen_swiotlb_map_page>> swiotlb_tbl_unmap_single(dev, map, size, size, dir,
+		 *   - drivers/xen/swiotlb-xen.c|464| <<xen_swiotlb_unmap_page>> swiotlb_tbl_unmap_single(hwdev, paddr, size, size, dir, attrs);
+		 *   - kernel/dma/direct.h|122| <<dma_direct_unmap_page>> swiotlb_tbl_unmap_single(dev, phys, size, size, dir, attrs);
+		 *   - kernel/dma/swiotlb.c|938| <<swiotlb_map>> swiotlb_tbl_unmap_single(dev, swiotlb_addr, size, size, dir,
+		 */
 		swiotlb_tbl_unmap_single(dev, swiotlb_addr, size, size, dir,
 			attrs | DMA_ATTR_SKIP_CPU_SYNC);
 		dev_WARN_ONCE(dev, 1,
@@ -689,11 +1121,23 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 	return dma_addr;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|500| <<dma_direct_max_mapping_size>> return swiotlb_max_mapping_size(dev);
+ *
+ * 不需要
+ */
 size_t swiotlb_max_mapping_size(struct device *dev)
 {
 	return ((size_t)1 << IO_TLB_SHIFT) * IO_TLB_SEGSIZE;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|498| <<dma_direct_max_mapping_size>> if (is_swiotlb_active() &&
+ *
+ * 不需要
+ */
 bool is_swiotlb_active(void)
 {
 	/*
@@ -705,6 +1149,9 @@ bool is_swiotlb_active(void)
 
 #ifdef CONFIG_DEBUG_FS
 
+/*
+ * 需要
+ */
 static int __init swiotlb_create_debugfs(void)
 {
 	struct dentry *root;
diff --git a/kernel/smp.c b/kernel/smp.c
index 4d17501433be..59f8b18aa236 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -440,6 +440,11 @@ static void flush_smp_call_function_queue(bool warn_cpu_offline)
 		sched_ttwu_pending(entry);
 }
 
+/*
+ * called by:
+ *   - kernel/sched/core.c|1801| <<migration_cpu_stop>> flush_smp_call_function_from_idle();
+ *   - kernel/sched/idle.c|326| <<do_idle>> flush_smp_call_function_from_idle();
+ */
 void flush_smp_call_function_from_idle(void)
 {
 	unsigned long flags;
diff --git a/kernel/time/tick-oneshot.c b/kernel/time/tick-oneshot.c
index f9745d47425a..db7e204e781d 100644
--- a/kernel/time/tick-oneshot.c
+++ b/kernel/time/tick-oneshot.c
@@ -20,6 +20,18 @@
 /**
  * tick_program_event
  */
+/*
+ * called by:
+ *   - ernel/time/hrtimer.c|674| <<hrtimer_force_reprogram>> tick_program_event(cpu_base->expires_next, 1);
+ *   - kernel/time/hrtimer.c|853| <<hrtimer_reprogram>> tick_program_event(expires, 1);
+ *   - kernel/time/hrtimer.c|1658| <<hrtimer_interrupt>> if (!tick_program_event(expires_next, 0)) {
+ *   - kernel/time/hrtimer.c|1702| <<hrtimer_interrupt>> tick_program_event(expires_next, 1);
+ *   - kernel/time/tick-broadcast.c|848| <<__tick_broadcast_oneshot_control>> tick_program_event(dev->next_event, 1);
+ *   - kernel/time/tick-sched.c|670| <<tick_nohz_restart>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ *   - kernel/time/tick-sched.c|840| <<tick_nohz_stop_tick>> tick_program_event(tick, 1);
+ *   - kernel/time/tick-sched.c|1234| <<tick_nohz_handler>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ *   - kernel/time/tick-sched.c|1271| <<tick_nohz_switch_to_nohz>> tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
+ */
 int tick_program_event(ktime_t expires, int force)
 {
 	struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);
diff --git a/mm/internal.h b/mm/internal.h
index c43ccdddb0f6..e00892b938de 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -568,6 +568,15 @@ unsigned int reclaim_clean_pages_from_list(struct zone *zone,
 #else
 #define ALLOC_NOFRAGMENT	  0x0
 #endif
+/*
+ * #define ___GFP_KSWAPD_RECLAIM   0x800u
+ *
+ * 在以下使用ALLOC_KSWAPD:
+ *   - mm/page_alloc.c|2544| <<steal_suitable_fallback>> if (alloc_flags & ALLOC_KSWAPD)
+ *   - mm/page_alloc.c|4404| <<gfp_to_alloc_flags>> BUILD_BUG_ON(__GFP_KSWAPD_RECLAIM != (__force gfp_t) ALLOC_KSWAPD);
+ *   - mm/page_alloc.c|4678| <<__alloc_pages_slowpath>> if (alloc_flags & ALLOC_KSWAPD)
+ *   - mm/page_alloc.c|4746| <<__alloc_pages_slowpath>> if (alloc_flags & ALLOC_KSWAPD)
+ */
 #define ALLOC_KSWAPD		0x800 /* allow waking of kswapd, __GFP_KSWAPD_RECLAIM set */
 
 enum ttu_flags;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index eaa227a479e4..4efcc80095bf 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -4386,6 +4386,10 @@ static void wake_all_kswapds(unsigned int order, gfp_t gfp_mask,
 	}
 }
 
+/*
+ * called by:
+ *   - mm/page_alloc.c|4642| <<__alloc_pages_slowpath>> alloc_flags = gfp_to_alloc_flags(gfp_mask);
+ */
 static inline unsigned int
 gfp_to_alloc_flags(gfp_t gfp_mask)
 {
@@ -4604,10 +4608,29 @@ check_retry_cpuset(int cpuset_mems_cookie, struct alloc_context *ac)
 	return false;
 }
 
+/*
+ * called by:
+ *   - mm/page_alloc.c|4965| <<__alloc_pages_nodemask>> page = __alloc_pages_slowpath(alloc_mask, order, &ac);
+ */
 static inline struct page *
 __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 						struct alloc_context *ac)
 {
+	/*
+	 * #define __GFP_IO        ((__force gfp_t)___GFP_IO)
+	 * #define __GFP_FS        ((__force gfp_t)___GFP_FS)
+	 * #define __GFP_DIRECT_RECLAIM    ((__force gfp_t)___GFP_DIRECT_RECLAIM) / Caller can reclaim
+	 * #define __GFP_KSWAPD_RECLAIM    ((__force gfp_t)___GFP_KSWAPD_RECLAIM) // kswapd can wake
+	 * #define __GFP_RECLAIM ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))
+	 * #define __GFP_RETRY_MAYFAIL     ((__force gfp_t)___GFP_RETRY_MAYFAIL)
+	 * #define __GFP_NOFAIL    ((__force gfp_t)___GFP_NOFAIL)
+	 * #define __GFP_NORETRY   ((__force gfp_t)___GFP_NORETRY)
+	 *
+	 * #define GFP_ATOMIC      (__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)
+	 * #define GFP_KERNEL      (__GFP_RECLAIM | __GFP_IO | __GFP_FS)
+	 * #define GFP_NOWAIT      (__GFP_KSWAPD_RECLAIM)
+	 * #define GFP_NOIO        (__GFP_RECLAIM)
+	 */
 	bool can_direct_reclaim = gfp_mask & __GFP_DIRECT_RECLAIM;
 	const bool costly_order = order > PAGE_ALLOC_COSTLY_ORDER;
 	struct page *page = NULL;
@@ -4913,6 +4936,14 @@ static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,
 /*
  * This is the 'heart' of the zoned buddy allocator.
  */
+/*
+ * called by:
+ *   - include/linux/gfp.h|511| <<__alloc_pages>> return __alloc_pages_nodemask(gfp_mask, order, preferred_nid, NULL);
+ *   - mm/hugetlb.c|1630| <<alloc_buddy_huge_page>> page = __alloc_pages_nodemask(gfp_mask, order, nid, nmask);
+ *   - mm/mempolicy.c|2234| <<alloc_pages_vma>> page = __alloc_pages_nodemask(gfp, order, preferred_nid, nmask);
+ *   - mm/mempolicy.c|2271| <<alloc_pages_current>> page = __alloc_pages_nodemask(gfp, order,
+ *   - mm/migrate.c|1575| <<alloc_migration_target>> new_page = __alloc_pages_nodemask(gfp_mask, order, nid, mtc->nmask);
+ */
 struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 							nodemask_t *nodemask)
diff --git a/mm/util.c b/mm/util.c
index 4ddb6e186dd5..626f7a244800 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -555,6 +555,9 @@ void *kvmalloc_node(size_t size, gfp_t flags, int node)
 	 * vmalloc uses GFP_KERNEL for some internal allocations (e.g page tables)
 	 * so the given set of flags has to be compatible.
 	 */
+	/*
+	 * 如果没设置GFP_KERNEL就只能用kmalloc
+	 */
 	if ((flags & GFP_KERNEL) != GFP_KERNEL)
 		return kmalloc_node(size, flags, node);
 
@@ -568,6 +571,9 @@ void *kvmalloc_node(size_t size, gfp_t flags, int node)
 	if (size > PAGE_SIZE) {
 		kmalloc_flags |= __GFP_NOWARN;
 
+		/*
+		 * 如果没有设置__GFP_RETRY_MAYFAIL ...
+		 */
 		if (!(kmalloc_flags & __GFP_RETRY_MAYFAIL))
 			kmalloc_flags |= __GFP_NORETRY;
 	}
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 6ae491a8b210..aaec0e3996e7 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -2595,6 +2595,24 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
  *
  * Return: pointer to the allocated memory or %NULL on error
  */
+/*
+ * called by:
+ *   - arch/arm64/include/asm/vmap_stack.h|22| <<arch_alloc_vmap_stack>> return __vmalloc_node(stack_size, THREAD_ALIGN, THREADINFO_GFP, node,
+ *   - arch/powerpc/kernel/irq.c|739| <<alloc_vm_stack>> return __vmalloc_node(THREAD_SIZE, THREAD_ALIGN, THREADINFO_GFP,
+ *   - arch/s390/kernel/setup.c|314| <<stack_alloc>> return (unsigned long )__vmalloc_node(THREAD_SIZE, THREAD_SIZE,
+ *   - lib/test_vmalloc.c|100| <<random_size_align_alloc_test>> ptr = __vmalloc_node(size, align, GFP_KERNEL | __GFP_ZERO, 0,
+ *   - lib/test_vmalloc.c|123| <<align_shift_alloc_test>> ptr = __vmalloc_node(PAGE_SIZE, align, GFP_KERNEL|__GFP_ZERO, 0,
+ *   - lib/test_vmalloc.c|140| <<fix_align_alloc_test>> ptr = __vmalloc_node(5 * PAGE_SIZE, THREAD_ALIGN << 1,
+ *   - mm/nommu.c|161| <<__vmalloc_node>> void *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,
+ *   - mm/util.c|584| <<kvmalloc_node>> return __vmalloc_node(size, 1, flags, node,
+ *   - mm/vmalloc.c|2473| <<__vmalloc_area_node>> pages = __vmalloc_node(array_size, 1, nested_gfp, node,
+ *   - mm/vmalloc.c|2615| <<__vmalloc>> return __vmalloc_node(size, 1, gfp_mask, NUMA_NO_NODE,
+ *   - mm/vmalloc.c|2634| <<vmalloc>> return __vmalloc_node(size, 1, GFP_KERNEL, NUMA_NO_NODE,
+ *   - mm/vmalloc.c|2654| <<vzalloc>> return __vmalloc_node(size, 1, GFP_KERNEL | __GFP_ZERO, NUMA_NO_NODE,
+ *   - mm/vmalloc.c|2692| <<vmalloc_node>> return __vmalloc_node(size, 1, GFP_KERNEL, node,
+ *   - mm/vmalloc.c|2710| <<vzalloc_node>> return __vmalloc_node(size, 1, GFP_KERNEL | __GFP_ZERO, node,
+ *   - mm/vmalloc.c|2738| <<vmalloc_32>> return __vmalloc_node(size, 1, GFP_VMALLOC32, NUMA_NO_NODE,
+ */
 void *__vmalloc_node(unsigned long size, unsigned long align,
 			    gfp_t gfp_mask, int node, const void *caller)
 {
-- 
2.17.1

