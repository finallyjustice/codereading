From db731a554ec9d2c60ae2fd5f20665063a9ff9bfa Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 11 Jan 2021 07:56:36 -0800
Subject: [PATCH 1/1] linux v5.10

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/kernel/pci-swiotlb.c |  43 +++++
 arch/x86/kvm/lapic.c          |  10 ++
 block/blk-core.c              |  24 +++
 block/blk-mq-tag.c            | 137 +++++++++++++++
 block/blk-mq-tag.h            | 107 ++++++++++++
 block/blk-mq.c                | 171 +++++++++++++++++++
 block/blk-mq.h                |  49 ++++++
 block/blk-stat.c              | 147 ++++++++++++++++
 block/blk-stat.h              |  24 +++
 block/blk-wbt.c               |   5 +
 drivers/block/null_blk_main.c |  13 ++
 drivers/scsi/scsi_lib.c       |  13 ++
 drivers/xen/mcelog.c          |  15 ++
 drivers/xen/swiotlb-xen.c     |  20 +++
 include/linux/blk-mq.h        |  75 ++++++++
 include/linux/blkdev.h        |  19 +++
 include/linux/swiotlb.h       |   3 +
 kernel/dma/direct.h           |   5 +
 kernel/dma/swiotlb.c          | 310 ++++++++++++++++++++++++++++++++++
 19 files changed, 1190 insertions(+)

diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c
index c2cfa5e7c152..3d488ee53159 100644
--- a/arch/x86/kernel/pci-swiotlb.c
+++ b/arch/x86/kernel/pci-swiotlb.c
@@ -14,6 +14,21 @@
 #include <asm/xen/swiotlb-xen.h>
 #include <asm/iommu_table.h>
 
+/*
+ * 在以下使用swiotlb:
+ *   - arch/x86/kernel/amd_gart_64.c|811| <<gart_iommu_init>> swiotlb = 0;
+ *   - arch/x86/kernel/pci-dma.c|105| <<iommu_setup>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|28| <<pci_swiotlb_detect_override>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|30| <<pci_swiotlb_detect_override>> return swiotlb;
+ *   - arch/x86/kernel/pci-swiotlb.c|45| <<pci_swiotlb_detect_4gb>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|53| <<pci_swiotlb_detect_4gb>> swiotlb = 1;
+ *   - arch/x86/kernel/pci-swiotlb.c|55| <<pci_swiotlb_detect_4gb>> return swiotlb;
+ *   - arch/x86/kernel/pci-swiotlb.c|64| <<pci_swiotlb_init>> if (swiotlb)
+ *   - arch/x86/kernel/pci-swiotlb.c|71| <<pci_swiotlb_late_init>> if (!swiotlb)
+ *   - arch/x86/xen/pci-swiotlb-xen.c|39| <<pci_xen_swiotlb_detect>> if (xen_initial_domain() || swiotlb || swiotlb_force == SWIOTLB_FORCE)
+ *   - arch/x86/xen/pci-swiotlb-xen.c|45| <<pci_xen_swiotlb_detect>> swiotlb = 0;
+ *   - drivers/iommu/amd/iommu.c|2364| <<amd_iommu_init_dma_ops>> swiotlb = (iommu_default_passthrough() || sme_me_mask) ? 1 : 0;
+ */
 int swiotlb __read_mostly;
 
 /*
@@ -22,6 +37,13 @@ int swiotlb __read_mostly;
  * This returns non-zero if we are forced to use swiotlb (by the boot
  * option).
  */
+/*
+ * [0] pci_swiotlb_detect_override
+ * [0] pci_iommu_alloc
+ * [0] mem_init
+ * [0] start_kernel
+ * [0] secondary_startup_64_no_verify
+ */
 int __init pci_swiotlb_detect_override(void)
 {
 	if (swiotlb_force == SWIOTLB_FORCE)
@@ -59,18 +81,39 @@ IOMMU_INIT(pci_swiotlb_detect_4gb,
 	   pci_swiotlb_init,
 	   pci_swiotlb_late_init);
 
+/*
+ * [0] pci_swiotlb_init
+ * [0] pci_iommu_alloc
+ * [0] mem_init
+ * [0] start_kernel
+ * [0] secondary_startup_64_no_verify
+ */
 void __init pci_swiotlb_init(void)
 {
 	if (swiotlb)
 		swiotlb_init(0);
 }
 
+/*
+ * [0] pci_swiotlb_late_init
+ * [0] pci_iommu_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * 被上面IOMMU_INIT_FINISH()和IOMMU_INIT()使用
+ */
 void __init pci_swiotlb_late_init(void)
 {
 	/* An IOMMU turned us off. */
 	if (!swiotlb)
 		swiotlb_exit();
 	else {
+		/*
+		 * [    0.587457] PCI-DMA: Using software bounce buffering for IO (SWIOTLB)
+		 * [    0.588574] software IO TLB: mapped [mem 0x00000000bbfdd000-0x00000000bffdd000] (64MB)
+		 */
 		printk(KERN_INFO "PCI-DMA: "
 		       "Using software bounce buffering for IO (SWIOTLB)\n");
 		swiotlb_print_info();
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 86c33d53c90a..2e8ca333c28c 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1610,6 +1610,12 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1639| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|1645| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2497| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -2413,6 +2419,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9862| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
diff --git a/block/blk-core.c b/block/blk-core.c
index 2db8bda43b6e..9fb9b06920ba 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -514,6 +514,30 @@ static void blk_timeout_work(struct work_struct *work)
 {
 }
 
+/*
+ * called by:
+ *   - arch/m68k/emu/nfblock.c|122| <<nfhd_init_one>> dev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - arch/xtensa/platforms/iss/simdisk.c|269| <<simdisk_setup>> dev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - block/blk-mq.c|3138| <<blk_mq_init_queue_data>> uninit_q = blk_alloc_queue(set->numa_node);
+ *   - drivers/block/brd.c|385| <<brd_alloc>> brd->brd_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/drbd/drbd_main.c|2746| <<drbd_create_device>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1845| <<null_add_dev>> nullb->q = blk_alloc_queue(dev->home_node);
+ *   - drivers/block/pktcdvd.c|2687| <<pkt_setup_dev>> disk->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/ps3vram.c|739| <<ps3vram_probe>> queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/rsxx/dev.c|239| <<rsxx_setup_dev>> card->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/umem.c|890| <<mm_pci_probe>> card->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/zram/zram_drv.c|1906| <<zram_add>> queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|379| <<nvm_create_tgt>> tqueue = blk_alloc_queue(dev->q->node);
+ *   - drivers/md/bcache/super.c|929| <<bcache_device_init>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/md/dm.c|1813| <<alloc_dev>> md->queue = blk_alloc_queue(numa_node_id);
+ *   - drivers/md/md.c|5707| <<md_alloc>> mddev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/blk.c|253| <<nsblk_attach_disk>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/btt.c|1524| <<btt_blk_init>> btt->btt_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/pmem.c|424| <<pmem_attach_disk>> q = blk_alloc_queue(dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|377| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue(ctrl->numa_node);
+ *   - drivers/s390/block/dcssblk.c|654| <<dcssblk_add_store>> dev_info->dcssblk_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/s390/block/xpram.c|347| <<xpram_setup_blkdev>> xpram_queues[i] = blk_alloc_queue(NUMA_NO_NODE);
+ */
 struct request_queue *blk_alloc_queue(int node_id)
 {
 	struct request_queue *q;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 9c92053e704d..a8e655eefb15 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,12 +21,22 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|132| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (blk_mq_is_sbitmap_shared(hctx->flags)) {
 		struct request_queue *q = hctx->queue;
 		struct blk_mq_tag_set *set = q->tag_set;
 
+		/*
+		 * 在以下使用QUEUE_FLAG_HCTX_ACTIVE:
+		 *   - block/blk-mq-tag.c|30| <<__blk_mq_tag_busy>> if (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) &&
+		 *   - block/blk-mq-tag.c|31| <<__blk_mq_tag_busy>> !test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))
+		 *   - block/blk-mq-tag.c|63| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(QUEUE_FLAG_HCTX_ACTIVE,
+		 */
 		if (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) &&
 		    !test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))
 			atomic_inc(&set->active_queues_shared_sbitmap);
@@ -42,6 +52,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|73| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|263| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(tags->bitmap_tags);
@@ -53,6 +68,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|163| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -171,6 +190,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	 * Give up this allocation if the hctx is inactive.  The caller will
 	 * retry on an active hctx.
 	 */
+	/*
+	 * 在以下使用BLK_MQ_S_INACTIVE:
+	 *   - block/blk-mq-tag.c|193| <<blk_mq_get_tag>> if (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {
+	 *   - block/blk-mq.c|2507| <<blk_mq_hctx_notify_offline>> set_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 *   - block/blk-mq.c|2530| <<blk_mq_hctx_notify_online>> clear_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 */
 	if (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {
 		blk_mq_put_tag(tags, data->ctx, tag + tag_offset);
 		return BLK_MQ_NO_TAG;
@@ -178,6 +203,13 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by;
+ *   - block/blk-mq-tag.c|194| <<blk_mq_get_tag>> blk_mq_put_tag(tags, data->ctx, tag + tag_offset);
+ *   - block/blk-mq.c|497| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|499| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|252| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,
 		    unsigned int tag)
 {
@@ -199,6 +231,10 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|266| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_iter_data *iter_data = data;
@@ -234,6 +270,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|471| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|472| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -258,6 +299,10 @@ struct bt_tags_iter_data {
 #define BT_TAG_ITER_STARTED		(1 << 1)
 #define BT_TAG_ITER_STATIC_RQS		(1 << 2)
 
+/*
+ * 在以下使用bt_tags_iter():
+ *   - block/blk-mq-tag.c|346| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -296,6 +341,11 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @data:	Will be passed as second argument to @fn.
  * @flags:	BT_TAG_ITER_*
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|364| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, tags->breserved_tags, fn, priv,
+ *   - block/blk-mq-tag.c|366| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, tags->bitmap_tags, fn, priv, flags);
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, unsigned int flags)
 {
@@ -310,6 +360,11 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|384| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+ *   - block/blk-mq-tag.c|404| <<blk_mq_tagset_busy_iter>> __blk_mq_all_tag_iter(tagset->tags[i], fn, priv,
+ */
 static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv, unsigned int flags)
 {
@@ -333,6 +388,10 @@ static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
  *
  * Caller has to pass the tag map from which requests are allocated.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2481| <<blk_mq_hctx_has_requests>> blk_mq_all_tag_iter(tags, blk_mq_has_request, &data);
+ */
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv)
 {
@@ -349,6 +408,25 @@ void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * 部分调用blk_mq_tagset_busy_iter()的例子:
+ *   - block/blk-mq-debugfs.c|418| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - block/blk-mq-tag.c|445| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset,
+ *   - drivers/block/nbd.c|827| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/nvme/host/fc.c|2466| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|2489| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|2462| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2463| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1016| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|1034| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1892| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1912| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|410| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|420| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/scsi/hosts.c|581| <<scsi_host_busy>> blk_mq_tagset_busy_iter(&shost->tag_set,
+ *   - drivers/scsi/hosts.c|680| <<scsi_host_complete_all_commands>> blk_mq_tagset_busy_iter(&shost->tag_set, complete_all_cmds_iter,
+ *   - drivers/scsi/hosts.c|717| <<bool>> blk_mq_tagset_busy_iter(&shost->tag_set, __scsi_host_busy_iter_fn,
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -362,6 +440,10 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 }
 EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
 
+/*
+ * 在以下使用blk_mq_tagset_count_completed_rqs():
+ *   - block/blk-mq-tag.c|446| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset, blk_mq_tagset_count_completed_rqs, &count);
+ */
 static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
 		void *data, bool reserved)
 {
@@ -379,6 +461,19 @@ static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
  *
  * Note: This function has to be run after all IO queues are shutdown
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2468| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|2491| <<__nvme_fc_abort_outstanding_ios>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|2464| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->tagset);
+ *   - drivers/nvme/host/pci.c|2465| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1018| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1036| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/tcp.c|1894| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1914| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->tagset);
+ *   - drivers/nvme/target/loop.c|412| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/target/loop.c|422| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ */
 void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)
 {
 	while (true) {
@@ -407,6 +502,13 @@ EXPORT_SYMBOL(blk_mq_tagset_wait_completed_request);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|118| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|128| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|894| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|999| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -422,6 +524,12 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		return;
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 在以下设置hctx->tags:
+		 *   - block/blk-mq.c|2644| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+		 *   - block/blk-mq.c|2874| <<blk_mq_map_swqueue>> hctx->tags = NULL;
+		 *   - block/blk-mq.c|2878| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+		 */
 		struct blk_mq_tags *tags = hctx->tags;
 
 		/*
@@ -438,6 +546,13 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|497| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->__bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|499| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->__breserved_tags, tags->nr_reserved_tags,
+ *   - block/blk-mq-tag.c|523| <<blk_mq_init_shared_sbitmap>> if (bt_alloc(&set->__bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|525| <<blk_mq_init_shared_sbitmap>> if (bt_alloc(&set->__breserved_tags, set->reserved_tags,
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -445,6 +560,13 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * 在以下使用blk_mq_init_bitmap_tags():
+ *   - block/blk-mq-tag.c|523| <<blk_mq_init_tags>> if (blk_mq_init_bitmap_tags(tags, node, alloc_policy) < 0) {
+ *
+ * 核心思想是分配blk_mq_tags->__bitmap_tags
+ * 然后让blk_mq_tags->bitmap_tags(指针)指向blk_mq_tags->__bitmap_tags
+ */
 static int blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 				   int node, int alloc_policy)
 {
@@ -466,6 +588,10 @@ static int blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3507| <<blk_mq_alloc_tag_set>> if (blk_mq_init_shared_sbitmap(set, set->flags)) {
+ */
 int blk_mq_init_shared_sbitmap(struct blk_mq_tag_set *set, unsigned int flags)
 {
 	unsigned int depth = set->queue_depth - set->reserved_tags;
@@ -498,6 +624,10 @@ void blk_mq_exit_shared_sbitmap(struct blk_mq_tag_set *set)
 	sbitmap_queue_free(&set->__breserved_tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2327| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node, flags);
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, unsigned int flags)
@@ -517,9 +647,16 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	tags->nr_tags = total_tags;
 	tags->nr_reserved_tags = reserved_tags;
 
+	/*
+	 * 这里可能直接退出
+	 */
 	if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
 		return tags;
 
+	/*
+	 * 核心思想是分配blk_mq_tags->__bitmap_tags
+	 * 然后让blk_mq_tags->bitmap_tags(指针)指向blk_mq_tags->__bitmap_tags
+	 */
 	if (blk_mq_init_bitmap_tags(tags, node, alloc_policy) < 0) {
 		kfree(tags);
 		return NULL;
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 7d3e6b333a4a..01b1edeced9c 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -6,14 +6,63 @@
  * Tag address space map.
  */
 struct blk_mq_tags {
+	/*
+	 * 在以下修改blk_mq_tags->nr_tags:
+	 *   - block/blk-mq-tag.c|537| <<blk_mq_init_tags>> tags->nr_tags = total_tags;
+	 * 在以下使用blk_mq_tags->nr_tags:
+	 *   - block/blk-mq-debugfs.c|450| <<blk_mq_debugfs_tags_show>> seq_printf(m, "nr_tags=%u\n", tags->nr_tags);
+	 *   - block/blk-mq-sysfs.c|148| <<blk_mq_hw_sysfs_nr_tags_show>> return sprintf(page, "%u\n", hctx->tags->nr_tags);
+	 *   - block/blk-mq-tag.c|192| <<blk_mq_put_tag>> BUG_ON(real_tag >= tags->nr_tags);
+	 *   - block/blk-mq-tag.c|463| <<blk_mq_init_bitmap_tags>> unsigned int depth = tags->nr_tags - tags->nr_reserved_tags;
+	 *   - block/blk-mq-tag.c|579| <<blk_mq_tag_update_depth>> if (tdepth > tags->nr_tags) {
+	 *   - block/blk-mq.c|860| <<blk_mq_tag_to_rq>> if (tag < tags->nr_tags) {
+	 *   - block/blk-mq.c|2282| <<blk_mq_free_rqs>> for (i = 0; i < tags->nr_tags; i++) {
+	 */
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用blk_mq_tags->active_queues:
+	 *   - block/blk-mq-debugfs.c|453| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|36| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|70| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq.h|312| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
+	/*
+	 * 在以下使用blk_mq_tags->bitmap_tags:
+	 *   - block/bfq-iosched.c|6368| <<bfq_depth_updated>> min_shallow = bfq_update_depths(bfqd, tags->bitmap_tags);
+	 *   - block/bfq-iosched.c|6369| <<bfq_depth_updated>> sbitmap_queue_min_shallow_depth(tags->bitmap_tags, min_shallow);
+	 *   - block/blk-mq-debugfs.c|456| <<blk_mq_debugfs_tags_show>> sbitmap_queue_show(tags->bitmap_tags, m);
+	 *   - block/blk-mq-debugfs.c|491| <<hctx_tags_bitmap_show>> sbitmap_bitmap_show(&hctx->tags->bitmap_tags->sb, m);
+	 *   - block/blk-mq-debugfs.c|525| <<hctx_sched_tags_bitmap_show>> sbitmap_bitmap_show(&hctx->sched_tags->bitmap_tags->sb, m);
+	 *   - block/blk-mq-tag.c|47| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(tags->bitmap_tags);
+	 *   - block/blk-mq-tag.c|106| <<blk_mq_get_tag>> bt = tags->bitmap_tags;
+	 *   - block/blk-mq-tag.c|154| <<blk_mq_get_tag>> bt = tags->bitmap_tags;
+	 *   - block/blk-mq-tag.c|188| <<blk_mq_put_tag>> sbitmap_queue_clear(tags->bitmap_tags, real_tag, ctx->cpu);
+	 *   - block/blk-mq-tag.c|321| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, tags->bitmap_tags, fn, priv, flags);
+	 *   - block/blk-mq-tag.c|436| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, tags->bitmap_tags, fn, priv, false);
+	 *   - block/blk-mq-tag.c|460| <<blk_mq_init_bitmap_tags>> tags->bitmap_tags = &tags->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|485| <<blk_mq_init_shared_sbitmap>> tags->bitmap_tags = &set->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|533| <<blk_mq_free_tags>> sbitmap_queue_free(tags->bitmap_tags);
+	 *   - block/blk-mq-tag.c|587| <<blk_mq_tag_update_depth>> sbitmap_queue_resize(tags->bitmap_tags,
+	 *   - block/blk-mq.c|1099| <<__blk_mq_get_driver_tag>> struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
+	 *   - block/blk-mq.c|1149| <<blk_mq_dispatch_wake>> sbq = hctx->tags->bitmap_tags;
+	 *   - block/blk-mq.c|1167| <<blk_mq_mark_tag_wait>> struct sbitmap_queue *sbq = hctx->tags->bitmap_tags;
+	 *   - block/kyber-iosched.c|362| <<kyber_sched_tags_shift>> return q->queue_hw_ctx[0]->sched_tags->bitmap_tags->sb.shift;
+	 *   - block/kyber-iosched.c|505| <<kyber_init_hctx>> sbitmap_queue_min_shallow_depth(hctx->sched_tags->bitmap_tags,
+	 */
 	struct sbitmap_queue *bitmap_tags;
 	struct sbitmap_queue *breserved_tags;
 
+	/*
+	 * 在以下使用blk_mq_tags->__bitmap_tags:
+	 *   - block/blk-mq-tag.c|454| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->__bitmap_tags, depth, round_robin, node))
+	 *   - block/blk-mq-tag.c|460| <<blk_mq_init_bitmap_tags>> tags->bitmap_tags = &tags->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|465| <<blk_mq_init_bitmap_tags>> sbitmap_queue_free(&tags->__bitmap_tags);
+	 *   - block/blk-mq-tag.c|485| <<blk_mq_init_shared_sbitmap>> tags->bitmap_tags = &set->__bitmap_tags;
+	 */
 	struct sbitmap_queue __bitmap_tags;
 	struct sbitmap_queue __breserved_tags;
 
@@ -46,6 +95,12 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|122| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|169| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1190| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -63,22 +118,74 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|376| <<__blk_mq_alloc_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|472| <<blk_mq_alloc_request_hctx>> blk_mq_tag_busy(data.hctx);
+ *   - block/blk-mq.c|1103| <<__blk_mq_get_driver_tag>> blk_mq_tag_busy(rq->mq_hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
 		return false;
 
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1009| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2580| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
 		return;
 
 	__blk_mq_tag_idle(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|184| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1105| <<__blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 55bcee5dc032..9f850b3fb7f1 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -253,6 +253,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|354| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -539,6 +543,12 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 	if (blk_mq_need_time_stamp(rq))
 		now = ktime_get_ns();
 
+	/*
+	 * 在以下使用RQF_STATS:
+	 *   - block/blk-mq.c|276| <<blk_mq_need_time_stamp>> return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
+	 *   - block/blk-mq.c|546| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+	 *   - block/blk-mq.c|743| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+	 */
 	if (rq->rq_flags & RQF_STATS) {
 		blk_mq_poll_stats_start(rq->q);
 		blk_stat_add(rq, now);
@@ -587,6 +597,11 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|649| <<__blk_mq_complete_request_remote>> blk_mq_trigger_softirq(rq);
+ *   - block/blk-mq.c|691| <<blk_mq_complete_request_remote>> blk_mq_trigger_softirq(rq);
+ */
 static void blk_mq_trigger_softirq(struct request *rq)
 {
 	struct list_head *list;
@@ -733,9 +748,22 @@ void blk_mq_start_request(struct request *rq)
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * 在以下使用QUEUE_FLAG_STATS:
+	 *   - block/blk-mq.c|746| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+	 *   - block/blk-stat.c|188| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|200| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+	 *   - block/blk-stat.c|234| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
 		rq->io_start_time_ns = ktime_get_ns();
 		rq->stats_sectors = blk_rq_sectors(rq);
+		/*
+		 * 在以下使用RQF_STATS:
+		 *   - block/blk-mq.c|276| <<blk_mq_need_time_stamp>> return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
+		 *   - block/blk-mq.c|546| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+		 *   - block/blk-mq.c|743| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+		 */
 		rq->rq_flags |= RQF_STATS;
 		rq_qos_issue(q, rq);
 	}
@@ -841,6 +869,14 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 		blk_mq_kick_requeue_list(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|172| <<queue_state_write>> blk_mq_kick_requeue_list(q);
+ *   - block/blk-mq.c|845| <<blk_mq_add_to_requeue_list>> blk_mq_kick_requeue_list(q);
+ *   - drivers/block/xen-blkfront.c|2067| <<blkif_recover>> blk_mq_kick_requeue_list(info->rq);
+ *   - drivers/md/dm-rq.c|68| <<dm_start_queue>> blk_mq_kick_requeue_list(q);
+ *   - drivers/s390/block/scm_blk.c|247| <<scm_request_requeue>> blk_mq_kick_requeue_list(bdev->rq);
+ */
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
 	kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
@@ -1094,6 +1130,10 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1129| <<blk_mq_get_driver_tag>> if (rq->tag == BLK_MQ_NO_TAG && !__blk_mq_get_driver_tag(rq))
+ */
 static bool __blk_mq_get_driver_tag(struct request *rq)
 {
 	struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
@@ -1118,6 +1158,14 @@ static bool __blk_mq_get_driver_tag(struct request *rq)
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1187| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1213| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1309| <<blk_mq_prep_dispatch_rq>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1383| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|2066| <<__blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ */
 static bool blk_mq_get_driver_tag(struct request *rq)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1640,6 +1688,24 @@ EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
  * pending requests to be sent. If this is true, run the queue to send requests
  * to hardware.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|343| <<blk_mq_sched_dispatch_requests>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|467| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|501| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|145| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1158| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1479| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1684| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1768| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1788| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1867| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2209| <<blk_mq_submit_bio>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2571| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - drivers/block/rnbd/rnbd-clt.c|166| <<rnbd_clt_dev_requeue>> blk_mq_run_hw_queue(q->hctx, true);
+ */
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1668,6 +1734,20 @@ EXPORT_SYMBOL(blk_mq_run_hw_queue);
  * @q: Pointer to the request queue to run.
  * @async: If we want to run the queue asynchronously.
  */
+/*
+ * 部分调用blk_mq_run_hw_queues()的例子:
+ *   - block/bfq-iosched.c|426| <<bfq_schedule_dispatch>> blk_mq_run_hw_queues(bfqd->queue, true);
+ *   - block/blk-mq-debugfs.c|168| <<queue_state_write>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|140| <<blk_freeze_queue_start>> blk_mq_run_hw_queues(q, false);
+ *   - block/blk-mq.c|252| <<blk_mq_unquiesce_queue>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|820| <<blk_mq_requeue_work>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/md/dm-table.c|2015| <<dm_table_run_md_queue_async>> blk_mq_run_hw_queues(t->md->queue, true);
+ *   - drivers/scsi/scsi_lib.c|335| <<scsi_kick_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|487| <<scsi_run_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|557| <<scsi_run_queue_async>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_sysfs.c|814| <<store_state_field>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_transport_fc.c|3676| <<fc_bsg_goose_queue>> blk_mq_run_hw_queues(q, true);
+ */
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1785,6 +1865,14 @@ void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL_GPL(blk_mq_start_stopped_hw_queue);
 
+/*
+ * 部分调用blk_mq_start_stopped_hw_queues()的例子:
+ *   - block/blk-mq-debugfs.c|170| <<queue_state_write>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/null_blk_main.c|1227| <<null_restart_queue_async>> blk_mq_start_stopped_hw_queues(q, true);
+ *   - drivers/block/virtio_blk.c|199| <<virtblk_done>> blk_mq_start_stopped_hw_queues(vblk->disk->queue, true);
+ *   - drivers/block/xen-blkfront.c|1231| <<kick_pending_request_queues_locked>> blk_mq_start_stopped_hw_queues(rinfo->dev_info->rq, true);
+ *   - drivers/block/xen-blkfront.c|2066| <<blkif_recover>> blk_mq_start_stopped_hw_queues(info->rq, true);
+ */
 void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2311,6 +2399,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags, unsigned int flags)
 	blk_mq_free_tags(tags, flags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|528| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|569| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2757| <<__blk_mq_alloc_map_and_request>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2887,6 +2981,13 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2912| <<blk_mq_update_tag_set_shared>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2948| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ *
+ * 对于request_queue的每一个hctx->flags, 设置或者取消BLK_MQ_F_TAG_QUEUE_SHARED
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2900,6 +3001,14 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2932| <<blk_mq_del_queue_tag_set>> blk_mq_update_tag_set_shared(set, false);
+ *   - block/blk-mq.c|2950| <<blk_mq_add_queue_tag_set>> blk_mq_update_tag_set_shared(set, true);
+ *
+ * 对于tagset的每一个request_queue的每一个hctx->flags
+ * 根据参数shared"设置"或者"取消"BLK_MQ_F_TAG_QUEUE_SHARED
+ */
 static void blk_mq_update_tag_set_shared(struct blk_mq_tag_set *set,
 					 bool shared)
 {
@@ -2909,6 +3018,9 @@ static void blk_mq_update_tag_set_shared(struct blk_mq_tag_set *set,
 
 	list_for_each_entry(q, &set->tag_list, tag_set_list) {
 		blk_mq_freeze_queue(q);
+		/*
+		 * 对于request_queue的每一个hctx->flags, 设置或者取消BLK_MQ_F_TAG_QUEUE_SHARED
+		 */
 		queue_set_hctx_shared(q, shared);
 		blk_mq_unfreeze_queue(q);
 	}
@@ -2924,12 +3036,20 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 		/* just transitioned to unshared */
 		set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
 		/* update existing queue */
+		/*
+		 * 对于tagset的每一个request_queue的每一个hctx->flags
+		 * 根据参数shared"设置"或者"取消"BLK_MQ_F_TAG_QUEUE_SHARED
+		 */
 		blk_mq_update_tag_set_shared(set, false);
 	}
 	mutex_unlock(&set->tag_list_lock);
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3239| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2938,12 +3058,38 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 	/*
 	 * Check to see if we're transitioning to shared (from 1 to 2 queues).
 	 */
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	if (!list_empty(&set->tag_list) &&
 	    !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
 		set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
 		/* update existing queue */
+		/*
+		 * 对于tagset的每一个request_queue的每一个hctx->flags
+		 * 根据参数shared"设置"或者"取消"BLK_MQ_F_TAG_QUEUE_SHARED
+		 */
 		blk_mq_update_tag_set_shared(set, true);
 	}
+	/*
+	 * queue_set_hctx_shared():
+	 * 对于request_queue的每一个hctx->flags, 设置或者取消BLK_MQ_F_TAG_QUEUE_SHARED
+	 */
 	if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
 		queue_set_hctx_shared(q, true);
 	list_add_tail(&q->tag_set_list, &set->tag_list);
@@ -3181,6 +3327,11 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3147| <<blk_mq_init_queue_data>> q = blk_mq_init_allocated_queue(set, uninit_q, false);
+ *   - drivers/md/dm-rq.c|560| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue, true);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q,
 						  bool elevator_init)
@@ -3452,6 +3603,9 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (blk_mq_is_sbitmap_shared(set->flags)) {
 		atomic_set(&set->active_queues_shared_sbitmap, 0);
 
+		/*
+		 * 只在此处调用blk_mq_init_shared_sbitmap()
+		 */
 		if (blk_mq_init_shared_sbitmap(set, set->flags)) {
 			ret = -ENOMEM;
 			goto out_free_mq_rq_maps;
@@ -3689,6 +3843,10 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3871| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
@@ -3698,6 +3856,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|553| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3722,6 +3884,10 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3913| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, rq);
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct request *rq)
 {
@@ -3910,6 +4076,11 @@ static int __init blk_mq_init(void)
 
 	for_each_possible_cpu(i)
 		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+	/*
+	 * 在以下使用BLOCK_SOFTIRQ:
+	 *   - block/blk-mq.c|615| <<blk_mq_trigger_softirq>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-mq.c|628| <<blk_softirq_cpu_dead>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 */
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
diff --git a/block/blk-mq.h b/block/blk-mq.h
index a52703c98b77..224e9b306513 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -151,6 +151,14 @@ struct blk_mq_alloc_data {
 	/* input parameter */
 	struct request_queue *q;
 	blk_mq_req_flags_t flags;
+	/*
+	 * 在以下使用blk_mq_alloc_data->shallow_depth:
+	 *   - lock/bfq-iosched.c|542| <<bfq_limit_depth>> data->shallow_depth =
+	 *   - block/bfq-iosched.c|547| <<bfq_limit_depth>> data->shallow_depth);
+	 *   - block/blk-mq-tag.c|102| <<__blk_mq_get_tag>> if (data->shallow_depth)
+	 *   - block/blk-mq-tag.c|103| <<__blk_mq_get_tag>> return __sbitmap_queue_get_shallow(bt, data->shallow_depth);
+	 *   - block/kyber-iosched.c|561| <<kyber_limit_depth>> data->shallow_depth = kqd->async_depth;
+	 */
 	unsigned int shallow_depth;
 	unsigned int cmd_flags;
 
@@ -159,8 +167,33 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|26| <<__blk_mq_tag_busy>> if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+ *   - block/blk-mq-tag.c|62| <<__blk_mq_tag_idle>> if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+ *   - block/blk-mq.c|3471| <<blk_mq_alloc_tag_set>> if (blk_mq_is_sbitmap_shared(set->flags)) {
+ *   - block/blk-mq.c|3506| <<blk_mq_free_tag_set>> if (blk_mq_is_sbitmap_shared(set->flags))
+ *   - block/blk-mq.c|3545| <<blk_mq_update_nr_requests>> if (!ret && blk_mq_is_sbitmap_shared(set->flags))
+ *   - block/blk-mq.h|204| <<__blk_mq_inc_active_requests>> if (blk_mq_is_sbitmap_shared(hctx->flags))
+ *   - block/blk-mq.h|212| <<__blk_mq_dec_active_requests>> if (blk_mq_is_sbitmap_shared(hctx->flags))
+ *   - block/blk-mq.h|220| <<__blk_mq_active_requests>> if (blk_mq_is_sbitmap_shared(hctx->flags))
+ *   - block/blk-mq.h|302| <<hctx_may_queue>> if (blk_mq_is_sbitmap_shared(hctx->flags)) {
+ */
 static inline bool blk_mq_is_sbitmap_shared(unsigned int flags)
 {
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	return flags & BLK_MQ_F_TAG_HCTX_SHARED;
 }
 
@@ -303,10 +336,26 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 		struct request_queue *q = hctx->queue;
 		struct blk_mq_tag_set *set = q->tag_set;
 
+		/*
+		 * 在以下使用BLK_MQ_S_TAG_ACTIVE:
+		 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+		 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq-tag.c|68| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq.h|331| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
+		 *   - block/blk-mq.h|335| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 */
 		if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
 			return true;
 		users = atomic_read(&set->active_queues_shared_sbitmap);
 	} else {
+		/*
+		 * 在以下使用BLK_MQ_S_TAG_ACTIVE:
+		 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+		 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq-tag.c|68| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 *   - block/blk-mq.h|331| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
+		 *   - block/blk-mq.h|335| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+		 */
 		if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 			return true;
 		users = atomic_read(&hctx->tags->active_queues);
diff --git a/block/blk-stat.c b/block/blk-stat.c
index ae3dd1fb8e61..f0966c00ffe4 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -15,9 +15,24 @@
 struct blk_queue_stats {
 	struct list_head callbacks;
 	spinlock_t lock;
+	/*
+	 * 在以下使用blk_queue_stats->enable_accounting:
+	 *   - block/blk-stat.c|164| <<blk_stat_remove_callback>> if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
+	 *   - block/blk-stat.c|192| <<blk_stat_enable_accounting>> q->stats->enable_accounting = true;
+	 *   - block/blk-stat.c|208| <<blk_alloc_queue_stats>> stats->enable_accounting = false;
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|199| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|87| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|95| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|148| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *
+ * 初始化blk_rq_stat的field为0或者初始值
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -26,6 +41,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|210| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|94| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -40,6 +60,11 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|222| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|74| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -48,6 +73,21 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * 猜测是因为wbt
+ * [0] blk_stat_add
+ * [0] __blk_mq_end_request
+ * [0] scsi_end_request
+ * [0] scsi_io_completion
+ * [0] blk_done_softirq
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ *
+ * called by:
+ *   - block/blk-mq.c|548| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ *
+ * 对request_queue的每一个callback(blk_stat_callback)都调用blk_rq_stat_add()
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -62,10 +102,23 @@ void blk_stat_add(struct request *rq, u64 now)
 
 	rcu_read_lock();
 	cpu = get_cpu();
+	/*
+	 * struct request_queue *q:
+	 * -> struct blk_queue_stats *stats;
+	 *    -> struct list_head callbacks;
+	 */
 	list_for_each_entry_rcu(cb, &q->stats->callbacks, list) {
 		if (!blk_stat_is_active(cb))
 			continue;
 
+		/*
+		 * 在以下设置blk_stat_callback->bucket_fn():
+		 *   - block/blk-stat.c|201| <<blk_stat_alloc_callback>> cb->bucket_fn = bucket_fn;
+		 * 在以下调用blk_stat_callback->bucket_fn():
+		 *   - block/blk-stat.c|108| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+		 *
+		 * 计算出一个bucket
+		 */
 		bucket = cb->bucket_fn(rq);
 		if (bucket < 0)
 			continue;
@@ -77,18 +130,31 @@ void blk_stat_add(struct request *rq, u64 now)
 	rcu_read_unlock();
 }
 
+/*
+ * 在以下使用blk_stat_timer_fn():
+ *   - block/blk-stat.c|161| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
 	unsigned int bucket;
 	int cpu;
 
+	/*
+	 * blk_rq_stat_init()
+	 * 初始化blk_rq_stat的field为0或者初始值
+	 */
 	for (bucket = 0; bucket < cb->buckets; bucket++)
 		blk_rq_stat_init(&cb->stat[bucket]);
 
 	for_each_online_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
+		/*
+		 * struct blk_stat_callback *cb:
+		 * -> struct blk_rq_stat __percpu *cpu_stat;
+		 * -> struct blk_rq_stat *stat;
+		 */
 		cpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);
 		for (bucket = 0; bucket < cb->buckets; bucket++) {
 			blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
@@ -99,6 +165,36 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * [0] blk_stat_alloc_callback
+ * [0] blk_mq_init_allocated_queue
+ * [0] blk_mq_init_queue_data
+ * [0] scsi_mq_alloc_queue
+ * [0] scsi_alloc_sdev
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by;
+ *   - block/blk-mq.c|3324| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|821| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -127,12 +223,48 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	cb->timer_fn = timer_fn;
 	cb->bucket_fn = bucket_fn;
 	cb->data = data;
+	/*
+	 * 在以下设置blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|201| <<blk_stat_alloc_callback>> cb->bucket_fn = bucket_fn;
+	 * 在以下调用blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|108| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 */
 	cb->buckets = buckets;
 	timer_setup(&cb->timer, blk_stat_timer_fn, 0);
 
 	return cb;
 }
 
+/*
+ * [0] blk_stat_add_callback
+ * [0] wbt_init
+ * [0] blk_register_queue
+ * [0] __device_add_disk
+ * [0] sd_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] __driver_attach_async_helper
+ * [0] async_run_entry_fn
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] blk_stat_add_callback
+ * [0] wbt_init
+ * [0] blk_register_queue
+ * [0] __device_add_disk
+ * [0] loop_add
+ * [0] loop_init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-mq.c|3827| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|844| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -184,6 +316,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-iocost.c|3216| <<ioc_qos_write>> blk_stat_enable_accounting(ioc->rqos.q);
+ *   - block/blk-throttle.c|2456| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	unsigned long flags;
@@ -195,6 +333,10 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|541| <<blk_alloc_queue>> q->stats = blk_alloc_queue_stats();
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -210,6 +352,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|589| <<blk_alloc_queue>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|791| <<blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 17b47a86eefb..21146b055add 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -37,6 +37,12 @@ struct blk_stat_callback {
 	 * should be accounted under. Return -1 for no bucket for this
 	 * request.
 	 */
+	/*
+	 * 在以下设置blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|201| <<blk_stat_alloc_callback>> cb->bucket_fn = bucket_fn;
+	 * 在以下调用blk_stat_callback->bucket_fn():
+	 *   - block/blk-stat.c|108| <<blk_stat_add>> bucket = cb->bucket_fn(rq);
+	 */
 	int (*bucket_fn)(const struct request *);
 
 	/**
@@ -126,6 +132,12 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3833| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+ *   - block/blk-stat.c|66| <<blk_stat_add>> if (!blk_stat_is_active(cb))
+ *   - block/blk-wbt.c|585| <<wbt_wait>> if (!blk_stat_is_active(rwb->cb))
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
 	return timer_pending(&cb->timer);
@@ -139,12 +151,20 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-wbt.c|349| <<rwb_arm_timer>> blk_stat_activate_nsecs(rwb->cb, rwb->cur_win_nsec);
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
 	mod_timer(&cb->timer, jiffies + nsecs_to_jiffies(nsecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|704| <<wbt_disable_default>> blk_stat_deactivate(rwb->cb);
+ */
 static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
 {
 	del_timer_sync(&cb->timer);
@@ -158,6 +178,10 @@ static inline void blk_stat_deactivate(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3836| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index fd410086fe1d..a8d6e59f3a4a 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -325,6 +325,11 @@ static void scale_down(struct rq_wb *rwb, bool hard_throttle)
 	rwb_trace_step(rwb, tracepoint_string("scale down"));
 }
 
+/*
+ * called by:
+ *   - block/blk-wbt.c|405| <<wb_timer_fn>> rwb_arm_timer(rwb);
+ *   - block/blk-wbt.c|586| <<wbt_wait>> rwb_arm_timer(rwb);
+ */
 static void rwb_arm_timer(struct rq_wb *rwb)
 {
 	struct rq_depth *rqd = &rwb->rq_depth;
diff --git a/drivers/block/null_blk_main.c b/drivers/block/null_blk_main.c
index 4685ea401d5b..f9720127bb7e 100644
--- a/drivers/block/null_blk_main.c
+++ b/drivers/block/null_blk_main.c
@@ -1710,6 +1710,19 @@ static int null_init_tag_set(struct nullb *nullb, struct blk_mq_tag_set *set)
 	set->flags = BLK_MQ_F_SHOULD_MERGE;
 	if (g_no_sched)
 		set->flags |= BLK_MQ_F_NO_SCHED;
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	if (g_shared_tag_bitmap)
 		set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
 	set->driver_data = NULL;
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 03c6d0620bfd..42ba00e821c0 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -1911,6 +1911,19 @@ int scsi_mq_setup_tags(struct Scsi_Host *shost)
 	tag_set->flags |=
 		BLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);
 	tag_set->driver_data = shost;
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	if (shost->host_tagset)
 		tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
 
diff --git a/drivers/xen/mcelog.c b/drivers/xen/mcelog.c
index e9ac3b8c4167..1afc5d4a31ed 100644
--- a/drivers/xen/mcelog.c
+++ b/drivers/xen/mcelog.c
@@ -292,6 +292,11 @@ static int convert_log(struct mc_info *mi)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/mcelog.c|339| <<xen_mce_work_fn>> err = mc_queue_handle(XEN_MC_URGENT);
+ *   - drivers/xen/mcelog.c|344| <<xen_mce_work_fn>> err = mc_queue_handle(XEN_MC_NONURGENT);
+ */
 static int mc_queue_handle(uint32_t flags)
 {
 	struct xen_mc mc_op;
@@ -329,6 +334,13 @@ static int mc_queue_handle(uint32_t flags)
 }
 
 /* virq handler for machine check error info*/
+/*
+ * 在以下调用xen_mce_work=xen_mce_work_fn():
+ *   - drivers/xen/mcelog.c|357| <<xen_mce_interrupt>> schedule_work(&xen_mce_work);
+ *
+ * 在以下使用xen_mce_work_fn():
+ *   - drivers/xen/mcelog.c|353| <<global>> static DECLARE_WORK(xen_mce_work, xen_mce_work_fn);
+ */
 static void xen_mce_work_fn(struct work_struct *work)
 {
 	int err;
@@ -354,6 +366,9 @@ static DECLARE_WORK(xen_mce_work, xen_mce_work_fn);
 
 static irqreturn_t xen_mce_interrupt(int irq, void *dev_id)
 {
+	/*
+	 * xen_mce_work_fn()
+	 */
 	schedule_work(&xen_mce_work);
 	return IRQ_HANDLED;
 }
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 2b385c1b4a99..781db044d657 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -39,6 +39,10 @@
 #include <asm/xen/page-coherent.h>
 
 #include <trace/events/swiotlb.h>
+/*
+ * 在以下使用MAX_DMA_BITS:
+ *   - drivers/xen/swiotlb-xen.c|144| <<xen_swiotlb_fixup>> } while (rc && dma_bits++ < MAX_DMA_BITS);
+ */
 #define MAX_DMA_BITS 32
 /*
  * Used to do a quick range check in swiotlb_tbl_unmap_single and
@@ -118,6 +122,10 @@ static int is_xen_swiotlb_buffer(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|232| <<xen_swiotlb_init>> rc = xen_swiotlb_fixup(xen_io_tlb_start,
+ */
 static int
 xen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)
 {
@@ -177,6 +185,12 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	}
 	return "";
 }
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|143| <<xen_mm_init>> xen_swiotlb_init(1, false);
+ *   - arch/x86/xen/pci-swiotlb-xen.c|62| <<pci_xen_swiotlb_init>> xen_swiotlb_init(1, true );
+ *   - arch/x86/xen/pci-swiotlb-xen.c|79| <<pci_xen_swiotlb_init_late>> rc = xen_swiotlb_init(1, false );
+ */
 int __ref xen_swiotlb_init(int verbose, bool early)
 {
 	unsigned long bytes, order;
@@ -395,6 +409,12 @@ static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 	 */
 	trace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);
 
+	/*
+	 * called by:
+	 *   - drivers/iommu/intel/iommu.c|3821| <<bounce_map_single>> tlb_addr = swiotlb_tbl_map_single(dev, paddr, size,
+	 *   - drivers/xen/swiotlb-xen.c|398| <<xen_swiotlb_map_page>> map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
+	 *   - kernel/dma/swiotlb.c|705| <<swiotlb_map>> swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, dir,
+	 */
 	map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
 	if (map == (phys_addr_t)DMA_MAPPING_ERROR)
 		return DMA_MAPPING_ERROR;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 794b2a33a2c3..09ac938b4fff 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -74,6 +74,21 @@ struct blk_mq_hw_ctx {
 	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
 	 * pending request in that software queue.
 	 */
+	/*
+	 * 在以下使用blk_mq_hw_ctx->ctx_map:
+	 *   - block/blk-mq-debugfs.c|443| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|238| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq-sysfs.c|42| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|73| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|85| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|86| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|94| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|1049| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1087| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2740| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2760| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2926| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	struct sbitmap		ctx_map;
 
 	/**
@@ -258,8 +273,23 @@ struct blk_mq_tag_set {
 	unsigned int		timeout;
 	unsigned int		flags;
 	void			*driver_data;
+	/*
+	 * 在以下使用blk_mq_tag_set->active_queues_shared_sbitmap:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> atomic_inc(&set->active_queues_shared_sbitmap);
+	 *   - block/blk-mq-tag.c|66| <<__blk_mq_tag_idle>> atomic_dec(&set->active_queues_shared_sbitmap);
+	 *   - block/blk-mq.c|3511| <<blk_mq_alloc_tag_set>> atomic_set(&set->active_queues_shared_sbitmap, 0);
+	 *   - block/blk-mq.h|341| <<hctx_may_queue>> users = atomic_read(&set->active_queues_shared_sbitmap);
+	 */
 	atomic_t		active_queues_shared_sbitmap;
 
+	/*
+	 * 在以下使用blk_mq_tag_set->__bitmap_tags:
+	 *   - block/blk-mq-tag.c|476| <<blk_mq_init_shared_sbitmap>> if (bt_alloc(&set->__bitmap_tags, depth, round_robin, node))
+	 *   - block/blk-mq-tag.c|485| <<blk_mq_init_shared_sbitmap>> tags->bitmap_tags = &set->__bitmap_tags;
+	 *   - block/blk-mq-tag.c|491| <<blk_mq_init_shared_sbitmap>> sbitmap_queue_free(&set->__bitmap_tags);
+	 *   - block/blk-mq-tag.c|497| <<blk_mq_exit_shared_sbitmap>> sbitmap_queue_free(&set->__bitmap_tags);
+	 *   - block/blk-mq-tag.c|596| <<blk_mq_tag_resize_shared_sbitmap>> sbitmap_queue_resize(&set->__bitmap_tags, size - set->reserved_tags);
+	 */
 	struct sbitmap_queue	__bitmap_tags;
 	struct sbitmap_queue	__breserved_tags;
 	struct blk_mq_tags	**tags;
@@ -390,12 +420,43 @@ struct blk_mq_ops {
 
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq.c|2677| <<blk_mq_alloc_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2897| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2899| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2925| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;
+	 *   - block/blk-mq.c|2943| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_QUEUE_SHARED:
+	 *   - block/blk-mq-tag.h|81| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq-tag.h|94| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - block/blk-mq.c|1128| <<blk_mq_get_driver_tag>> if ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&
+	 *   - block/blk-mq.c|1172| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|1429| <<blk_mq_dispatch_rq_list>> (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED);
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {
+	 *   - block/blk-mq.c|2947| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)
+	 *   - block/blk-mq.h|293| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))
+	 *   - drivers/block/rnbd/rnbd-clt.c|1172| <<setup_mq_tags>> BLK_MQ_F_TAG_QUEUE_SHARED;
+	 */
 	BLK_MQ_F_TAG_QUEUE_SHARED = 1 << 1,
 	/*
 	 * Set when this device requires underlying blk-mq device for
 	 * completing IO:
 	 */
 	BLK_MQ_F_STACKING	= 1 << 2,
+	/*
+	 * 在以下设置BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-tag.c|555| <<blk_mq_tag_update_depth>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/block/null_blk_main.c|1714| <<null_init_tag_set>> set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - drivers/scsi/scsi_lib.c|1915| <<scsi_mq_setup_tags>> tag_set->flags |= BLK_MQ_F_TAG_HCTX_SHARED;
+	 * 在以下使用BLK_MQ_F_TAG_HCTX_SHARED:
+	 *   - block/blk-mq-sched.c|510| <<blk_mq_sched_free_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|525| <<blk_mq_sched_alloc_tags>> unsigned int flags = set->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-sched.c|548| <<blk_mq_sched_tags_teardown>> unsigned int flags = hctx->flags & ~BLK_MQ_F_TAG_HCTX_SHARED;
+	 *   - block/blk-mq-tag.c|520| <<blk_mq_init_tags>> if (flags & BLK_MQ_F_TAG_HCTX_SHARED)
+	 *   - block/blk-mq-tag.c|532| <<blk_mq_free_tags>> if (!(flags & BLK_MQ_F_TAG_HCTX_SHARED)) {
+	 *   - block/blk-mq.h|164| <<blk_mq_is_sbitmap_shared>> return flags & BLK_MQ_F_TAG_HCTX_SHARED;
+	 */
 	BLK_MQ_F_TAG_HCTX_SHARED = 1 << 3,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
@@ -403,10 +464,24 @@ enum {
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 在以下使用BLK_MQ_S_TAG_ACTIVE:
+	 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|35| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|68| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq.h|331| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &q->queue_flags))
+	 *   - block/blk-mq.h|335| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	/* hw queue is inactive after all its CPUs become offline */
+	/*
+	 * 在以下使用BLK_MQ_S_INACTIVE:
+	 *   - block/blk-mq-tag.c|193| <<blk_mq_get_tag>> if (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {
+	 *   - block/blk-mq.c|2507| <<blk_mq_hctx_notify_offline>> set_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 *   - block/blk-mq.c|2530| <<blk_mq_hctx_notify_online>> clear_bit(BLK_MQ_S_INACTIVE, &hctx->state);
+	 */
 	BLK_MQ_S_INACTIVE	= 3,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 033eb5f73b65..fddcbe8b71a6 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -97,6 +97,12 @@ typedef __u32 __bitwise req_flags_t;
 /* on IO scheduler merge hash */
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
 /* track IO completion time */
+/*
+ * 在以下使用RQF_STATS:
+ *   - block/blk-mq.c|276| <<blk_mq_need_time_stamp>> return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
+ *   - block/blk-mq.c|546| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+ *   - block/blk-mq.c|743| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+ */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
@@ -615,6 +621,13 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|746| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|188| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|200| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|234| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
@@ -623,6 +636,12 @@ struct request_queue {
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 #define QUEUE_FLAG_ZONE_RESETALL 26	/* supports Zone Reset All */
 #define QUEUE_FLAG_RQ_ALLOC_TIME 27	/* record rq->alloc_time_ns */
+/*
+ * 在以下使用QUEUE_FLAG_HCTX_ACTIVE:
+ *   - block/blk-mq-tag.c|30| <<__blk_mq_tag_busy>> if (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) &&
+ *   - block/blk-mq-tag.c|31| <<__blk_mq_tag_busy>> !test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))
+ *   - block/blk-mq-tag.c|63| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(QUEUE_FLAG_HCTX_ACTIVE,
+ */
 #define QUEUE_FLAG_HCTX_ACTIVE	28	/* at least one blk-mq hctx is active */
 #define QUEUE_FLAG_NOWAIT       29	/* device supports NOWAIT */
 
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index fbdc65782195..a9a3557b81e5 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -28,6 +28,9 @@ enum swiotlb_force {
  * log of the size of each IO TLB slab.  The number of slabs is command line
  * controllable.
  */
+/*
+ * 1 << 11 = 2048 (2k)
+ */
 #define IO_TLB_SHIFT 11
 
 extern void swiotlb_init(int verbose);
diff --git a/kernel/dma/direct.h b/kernel/dma/direct.h
index b98615578737..199338d84d66 100644
--- a/kernel/dma/direct.h
+++ b/kernel/dma/direct.h
@@ -80,6 +80,11 @@ static inline void dma_direct_sync_single_for_cpu(struct device *dev,
 		arch_dma_mark_clean(paddr, size);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|403| <<dma_direct_map_sg>> sg->dma_address = dma_direct_map_page(dev, sg_page(sg),
+ *   - kernel/dma/mapping.c|153| <<dma_map_page_attrs>> addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
+ */
 static inline dma_addr_t dma_direct_map_page(struct device *dev,
 		struct page *page, unsigned long offset, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index 781b9dca197c..7ec12c8082b2 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -50,9 +50,31 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/swiotlb.h>
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|288| <<swiotlb_init_with_tbl>> io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|444| <<swiotlb_late_init_with_tbl>> io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|634| <<swiotlb_tbl_map_single>> for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)
+ *   - kernel/dma/swiotlb.c|720| <<swiotlb_tbl_unmap_single>> for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE -1) && io_tlb_list[i]; i--)
+ *
+ * 返回的结果永远是0-127
+ */
 #define OFFSET(val,align) ((unsigned long)	\
 	                   ( (val) & ( (align) - 1)))
 
+/*
+ * 在以下使用SLABS_PER_PAGE:
+ *   - drivers/xen/swiotlb-xen.c|210| <<xen_swiotlb_init>> #define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))
+ *   - drivers/xen/swiotlb-xen.c|212| <<xen_swiotlb_init>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ *   - drivers/xen/swiotlb-xen.c|221| <<xen_swiotlb_init>> xen_io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|295| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|298| <<swiotlb_late_init_with_default_size>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ *   - kernel/dma/swiotlb.c|313| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *
+ * #define IO_TLB_SHIFT 11 -->  1 << 11 = 2048 (2k)
+ *
+ * 理论上SLABS_PER_PAGE=2
+ */
 #define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))
 
 /*
@@ -60,8 +82,21 @@
  * 64bit capable cards will only lightly use the swiotlb.  If we can't
  * allocate a contiguous 1MB, we're probably in trouble anyway.
  */
+/*
+ * 在以下使用IO_TLB_MIN_SLABS:
+ *   - kernel/dma/swiotlb.c|380| <<swiotlb_late_init_with_default_size>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ */
 #define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)
 
+/*
+ * 在以下设置swiotlb_force:
+ *   - arch/arm64/mm/init.c|509| <<mem_init>> swiotlb_force = SWIOTLB_NO_FORCE;
+ *   - arch/powerpc/platforms/pseries/svm.c|30| <<init_svm>> swiotlb_force = SWIOTLB_FORCE;
+ *   - arch/s390/mm/init.c|185| <<pv_init>> swiotlb_force = SWIOTLB_FORCE;
+ *   - arch/x86/mm/mem_encrypt.c|198| <<sme_early_init>> swiotlb_force = SWIOTLB_FORCE;
+ *   - kernel/dma/swiotlb.c|162| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_FORCE;
+ *   - kernel/dma/swiotlb.c|164| <<setup_io_tlb_npages>> swiotlb_force = SWIOTLB_NO_FORCE;
+ */
 enum swiotlb_force swiotlb_force;
 
 /*
@@ -69,17 +104,67 @@ enum swiotlb_force swiotlb_force;
  * swiotlb_tbl_sync_single_*, to see if the memory was in fact allocated by this
  * API.
  */
+/*
+ * 在以下使用io_tlb_start:
+ *   - arch/powerpc/platforms/pseries/svm.c|58| <<svm_swiotlb_init>> if (io_tlb_start)
+ *   - arch/powerpc/platforms/pseries/svm.c|59| <<svm_swiotlb_init>> memblock_free_early(io_tlb_start,
+ *   - drivers/xen/swiotlb-xen.c|195| <<xen_swiotlb_init>> if (io_tlb_start != 0) {
+ *   - drivers/xen/swiotlb-xen.c|196| <<xen_swiotlb_init>> xen_io_tlb_start = phys_to_virt(io_tlb_start);
+ *   - include/linux/swiotlb.h|74| <<is_swiotlb_buffer>> return paddr >= io_tlb_start && paddr < io_tlb_end;
+ *   - kernel/dma/swiotlb.c|190| <<swiotlb_print_info>> pr_info("mapped [mem %pa-%pa] (%luMB)\n", &io_tlb_start, &io_tlb_end,
+ *   - kernel/dma/swiotlb.c|208| <<swiotlb_update_mem_attributes>> vaddr = phys_to_virt(io_tlb_start);
+ *   - kernel/dma/swiotlb.c|229| <<swiotlb_init_with_tbl>> io_tlb_start = __pa(tlb);
+ *   - kernel/dma/swiotlb.c|230| <<swiotlb_init_with_tbl>> io_tlb_end = io_tlb_start + bytes;
+ *   - kernel/dma/swiotlb.c|286| <<swiotlb_init>> if (io_tlb_start) {
+ *   - kernel/dma/swiotlb.c|287| <<swiotlb_init>> memblock_free_early(io_tlb_start,
+ *   - kernel/dma/swiotlb.c|289| <<swiotlb_init>> io_tlb_start = 0;
+ *   - kernel/dma/swiotlb.c|347| <<swiotlb_cleanup>> io_tlb_start = 0;
+ *   - kernel/dma/swiotlb.c|360| <<swiotlb_late_init_with_tbl>> io_tlb_start = virt_to_phys(tlb);
+ *   - kernel/dma/swiotlb.c|361| <<swiotlb_late_init_with_tbl>> io_tlb_end = io_tlb_start + bytes;
+ *   - kernel/dma/swiotlb.c|417| <<swiotlb_exit>> free_pages((unsigned long )phys_to_virt(io_tlb_start),
+ *   - kernel/dma/swiotlb.c|424| <<swiotlb_exit>> memblock_free_late(io_tlb_start,
+ *   - kernel/dma/swiotlb.c|481| <<swiotlb_tbl_map_single>> dma_addr_t tbl_dma_addr = phys_to_dma_unencrypted(hwdev, io_tlb_start);
+ *   - kernel/dma/swiotlb.c|564| <<swiotlb_tbl_map_single>> tlb_addr = io_tlb_start + (index << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|615| <<swiotlb_tbl_unmap_single>> int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|660| <<swiotlb_tbl_sync_single>> int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
+ */
 phys_addr_t io_tlb_start, io_tlb_end;
 
 /*
  * The number of IO TLB blocks (in groups of 64) between io_tlb_start and
  * io_tlb_end.  This is command line adjustable via setup_io_tlb_npages.
  */
+/*
+ * 在以下设置io_tlb_nslabs:
+ *   - arch/powerpc/platforms/pseries/svm.c|49| <<svm_swiotlb_init>> io_tlb_nslabs = (swiotlb_size_or_default() >> IO_TLB_SHIFT);
+ *   - arch/powerpc/platforms/pseries/svm.c|50| <<svm_swiotlb_init>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|155| <<setup_io_tlb_npages>> io_tlb_nslabs = simple_strtoul(str, &str, 0);
+ *   - kernel/dma/swiotlb.c|157| <<setup_io_tlb_npages>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|165| <<setup_io_tlb_npages>> io_tlb_nslabs = 1;
+ *   - kernel/dma/swiotlb.c|266| <<swiotlb_init_with_tbl>> io_tlb_nslabs = nslabs;
+ *   - kernel/dma/swiotlb.c|329| <<swiotlb_init>> io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|330| <<swiotlb_init>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|369| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|377| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|389| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = req_nslabs;
+ *   - kernel/dma/swiotlb.c|395| <<swiotlb_late_init_with_default_size>> io_tlb_nslabs = SLABS_PER_PAGE << order;
+ *   - kernel/dma/swiotlb.c|408| <<swiotlb_cleanup>> io_tlb_nslabs = 0;
+ *   - kernel/dma/swiotlb.c|419| <<swiotlb_late_init_with_tbl>> io_tlb_nslabs = nslabs;
+ */
 static unsigned long io_tlb_nslabs;
 
 /*
  * The number of used IO TLB block
  */
+/*
+ * 在以下使用io_tlb_used:
+ *   - kernel/dma/swiotlb.c|606| <<swiotlb_tbl_map_single>> if (unlikely(nslots > io_tlb_nslabs - io_tlb_used))
+  3 kernel/dma/swiotlb.c|653| <<swiotlb_tbl_map_single>> tmp_io_tlb_used = io_tlb_used;
+  4 kernel/dma/swiotlb.c|661| <<swiotlb_tbl_map_single>> io_tlb_used += nslots;
+  5 kernel/dma/swiotlb.c|723| <<swiotlb_tbl_unmap_single>> io_tlb_used -= nslots;
+  6 kernel/dma/swiotlb.c|820| <<swiotlb_create_debugfs>> debugfs_create_ulong("io_tlb_used", 0400, root, &io_tlb_used);
+ */
 static unsigned long io_tlb_used;
 
 /*
@@ -87,12 +172,26 @@ static unsigned long io_tlb_used;
  * each index
  */
 static unsigned int *io_tlb_list;
+/*
+ * 在以下使用io_tlb_index:
+ *   - kernel/dma/swiotlb.c|356| <<swiotlb_init_with_tbl>> io_tlb_index = 0;
+ *   - kernel/dma/swiotlb.c|512| <<swiotlb_late_init_with_tbl>> io_tlb_index = 0;
+ *   - kernel/dma/swiotlb.c|674| <<swiotlb_tbl_map_single>> index = ALIGN(io_tlb_index, stride);
+ *   - kernel/dma/swiotlb.c|707| <<swiotlb_tbl_map_single>> io_tlb_index = ((index + nslots) < io_tlb_nslabs
+ */
 static unsigned int io_tlb_index;
 
 /*
  * Max segment that we can provide which (if pages are contingous) will
  * not be bounced (unless SWIOTLB_FORCE is set).
  */
+/*
+ * 在以下使用max_segment:
+ *   - kernel/dma/swiotlb.c|182| <<swiotlb_max_segment>> return unlikely(no_iotlb_memory) ? 0 : max_segment;
+ *   - kernel/dma/swiotlb.c|189| <<swiotlb_set_max_segment>> max_segment = 1;
+ *   - kernel/dma/swiotlb.c|191| <<swiotlb_set_max_segment>> max_segment = rounddown(val, PAGE_SIZE);
+ *   - kernel/dma/swiotlb.c|409| <<swiotlb_cleanup>> max_segment = 0;
+ */
 static unsigned int max_segment;
 
 /*
@@ -100,6 +199,11 @@ static unsigned int max_segment;
  * for the sync operations.
  */
 #define INVALID_PHYS_ADDR (~(phys_addr_t)0)
+/*
+ * 在以下分配io_tlb_orig_addr:
+ *   - kernel/dma/swiotlb.c|347| <<swiotlb_init_with_tbl>> io_tlb_orig_addr = memblock_alloc(alloc_size, PAGE_SIZE);
+ *   - kernel/dma/swiotlb.c|501| <<swiotlb_late_init_with_tbl>> io_tlb_orig_addr = (phys_addr_t *)
+ */
 static phys_addr_t *io_tlb_orig_addr;
 
 /*
@@ -107,6 +211,12 @@ static phys_addr_t *io_tlb_orig_addr;
  */
 static DEFINE_SPINLOCK(io_tlb_lock);
 
+/*
+ * 在以下使用late_alloc:
+ *   - kernel/dma/swiotlb.c|190| <<swiotlb_update_mem_attributes>> if (no_iotlb_memory || late_alloc)
+ *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> late_alloc = 1;
+ *   - kernel/dma/swiotlb.c|390| <<swiotlb_exit>> if (late_alloc) {
+ */
 static int late_alloc;
 
 static int __init
@@ -130,30 +240,80 @@ setup_io_tlb_npages(char *str)
 }
 early_param("swiotlb", setup_io_tlb_npages);
 
+/*
+ * 在以下使用no_iotlb_memory:
+ *   - kernel/dma/swiotlb.c|235| <<swiotlb_nr_tbl>> return unlikely(no_iotlb_memory) ? 0 : io_tlb_nslabs;
+ *   - kernel/dma/swiotlb.c|241| <<swiotlb_max_segment>> return unlikely(no_iotlb_memory) ? 0 : max_segment;
+ *   - kernel/dma/swiotlb.c|284| <<swiotlb_print_info>> if (no_iotlb_memory) {
+ *   - kernel/dma/swiotlb.c|308| <<swiotlb_update_mem_attributes>> if (no_iotlb_memory || late_alloc)
+ *   - kernel/dma/swiotlb.c|357| <<swiotlb_init_with_tbl>> no_iotlb_memory = false;
+ *   - kernel/dma/swiotlb.c|417| <<swiotlb_init>> no_iotlb_memory = true;
+ *   - kernel/dma/swiotlb.c|513| <<swiotlb_late_init_with_tbl>> no_iotlb_memory = false;
+ *   - kernel/dma/swiotlb.c|628| <<swiotlb_tbl_map_single>> if (no_iotlb_memory)
+ */
 static bool no_iotlb_memory;
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c|1360| <<amdgpu_ttm_tt_populate>> if (adev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c|1403| <<amdgpu_ttm_tt_unpopulate>> if (adev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c|2591| <<amdgpu_ttm_debugfs_init>> if (!(adev->need_swiotlb && swiotlb_nr_tbl()))
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|45| <<i915_gem_object_get_pages_internal>> if (swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/nouveau/nouveau_bo.c|1331| <<nouveau_ttm_tt_populate>> if (swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/nouveau/nouveau_bo.c|1361| <<nouveau_ttm_tt_unpopulate>> if (swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|660| <<radeon_ttm_tt_populate>> if (rdev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|691| <<radeon_ttm_tt_unpopulate>> if (rdev->need_swiotlb && swiotlb_nr_tbl()) {
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|1102| <<radeon_ttm_debugfs_init>> if (!(rdev->need_swiotlb && swiotlb_nr_tbl()))
+ *   - drivers/pci/xen-pcifront.c|696| <<pcifront_connect_and_init_dma>> if (!err && !swiotlb_nr_tbl()) {
+ *   - drivers/xen/swiotlb-xen.c|197| <<xen_swiotlb_init>> xen_io_tlb_nslabs = swiotlb_nr_tbl();
+ */
 unsigned long swiotlb_nr_tbl(void)
 {
 	return unlikely(no_iotlb_memory) ? 0 : io_tlb_nslabs;
 }
 EXPORT_SYMBOL_GPL(swiotlb_nr_tbl);
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|48| <<i915_gem_object_get_pages_internal>> max_segment = swiotlb_max_segment();
+ *   - drivers/gpu/drm/i915/i915_scatterlist.h|112| <<i915_sg_segment_size>> unsigned int size = swiotlb_max_segment();
+ *   - drivers/mmc/host/sdhci.c|4585| <<sdhci_setup_host>> if (swiotlb_max_segment()) {
+ */
 unsigned int swiotlb_max_segment(void)
 {
 	return unlikely(no_iotlb_memory) ? 0 : max_segment;
 }
 EXPORT_SYMBOL_GPL(swiotlb_max_segment);
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|267| <<xen_swiotlb_init>> swiotlb_set_max_segment(PAGE_SIZE);
+ *   - kernel/dma/swiotlb.c|297| <<swiotlb_init_with_tbl>> swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
+ *   - kernel/dma/swiotlb.c|454| <<swiotlb_late_init_with_tbl>> swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
+ */
 void swiotlb_set_max_segment(unsigned int val)
 {
 	if (swiotlb_force == SWIOTLB_FORCE)
 		max_segment = 1;
 	else
 		max_segment = rounddown(val, PAGE_SIZE);
+	/*
+	 * rounddown() = round down to next specified multiple
+	 */
 }
 
 /* default to 64MB */
+/*
+ * 在以下使用IO_TLB_DEFAULT_SIZE:
+ *   - kernel/dma/swiotlb.c|270| <<swiotlb_size_or_default>> return size ? size : (IO_TLB_DEFAULT_SIZE);
+ *   - kernel/dma/swiotlb.c|386| <<swiotlb_init>> size_t default_size = IO_TLB_DEFAULT_SIZE;
+ */
 #define IO_TLB_DEFAULT_SIZE (64UL<<20)
+/*
+ * called by:
+ *   - arch/powerpc/platforms/pseries/svm.c|49| <<svm_swiotlb_init>> io_tlb_nslabs = (swiotlb_size_or_default() >> IO_TLB_SHIFT);
+ *   - arch/x86/kernel/setup.c|443| <<reserve_crashkernel_low>> low_size = max(swiotlb_size_or_default() + (8UL << 20), 256UL << 20);
+ */
 unsigned long swiotlb_size_or_default(void)
 {
 	unsigned long size;
@@ -163,6 +323,13 @@ unsigned long swiotlb_size_or_default(void)
 	return size ? size : (IO_TLB_DEFAULT_SIZE);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|23| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-swiotlb.c|76| <<pci_swiotlb_late_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|257| <<swiotlb_init_with_tbl>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|390| <<swiotlb_late_init_with_tbl>> swiotlb_print_info();
+ */
 void swiotlb_print_info(void)
 {
 	unsigned long bytes = io_tlb_nslabs << IO_TLB_SHIFT;
@@ -172,6 +339,10 @@ void swiotlb_print_info(void)
 		return;
 	}
 
+	/*
+	 * [    0.587457] PCI-DMA: Using software bounce buffering for IO (SWIOTLB)
+	 * [    0.588574] software IO TLB: mapped [mem 0x00000000bbfdd000-0x00000000bffdd000] (64MB)
+	 */
 	pr_info("mapped [mem %pa-%pa] (%luMB)\n", &io_tlb_start, &io_tlb_end,
 	       bytes >> 20);
 }
@@ -182,6 +353,12 @@ void swiotlb_print_info(void)
  * call SWIOTLB when the operations are possible.  It needs to be called
  * before the SWIOTLB memory is used.
  */
+/*
+ * called by:
+ *   - arch/powerpc/platforms/pseries/svm.c|33| <<init_svm>> swiotlb_update_mem_attributes();
+ *   - arch/s390/mm/init.c|184| <<pv_init>> swiotlb_update_mem_attributes();
+ *   - arch/x86/mm/mem_encrypt.c|443| <<mem_encrypt_init>> swiotlb_update_mem_attributes();
+ */
 void __init swiotlb_update_mem_attributes(void)
 {
 	void *vaddr;
@@ -196,6 +373,13 @@ void __init swiotlb_update_mem_attributes(void)
 	memset(vaddr, 0, bytes);
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|248| <<plat_swiotlb_setup>> if (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)
+ *   - arch/powerpc/platforms/pseries/svm.c|55| <<svm_swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, false))
+ *   - drivers/xen/swiotlb-xen.c|247| <<xen_swiotlb_init>> if (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs,
+ *   - kernel/dma/swiotlb.c|276| <<swiotlb_init>> if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, verbose))
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	unsigned long i, bytes;
@@ -228,6 +412,13 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 		io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
 		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 	}
+	/*
+	 * 在以下使用io_tlb_index:
+	 *   - kernel/dma/swiotlb.c|356| <<swiotlb_init_with_tbl>> io_tlb_index = 0;
+	 *   - kernel/dma/swiotlb.c|512| <<swiotlb_late_init_with_tbl>> io_tlb_index = 0;
+	 *   - kernel/dma/swiotlb.c|674| <<swiotlb_tbl_map_single>> index = ALIGN(io_tlb_index, stride);
+	 *   - kernel/dma/swiotlb.c|707| <<swiotlb_tbl_map_single>> io_tlb_index = ((index + nslots) < io_tlb_nslabs
+	 */
 	io_tlb_index = 0;
 	no_iotlb_memory = false;
 
@@ -242,21 +433,43 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * called by:
+ *   - arch/arm/mm/init.c|382| <<mem_init>> swiotlb_init(1);
+ *   - arch/arm64/mm/init.c|507| <<mem_init>> swiotlb_init(1);
+ *   - arch/ia64/mm/init.c|663| <<mem_init>> swiotlb_init(1);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(1);
+ *   - arch/powerpc/mm/mem.c|291| <<mem_init>> swiotlb_init(0);
+ *   - arch/riscv/kernel/setup.c|94| <<setup_arch>> swiotlb_init(1);
+ *   - arch/s390/mm/init.c|183| <<pv_init>> swiotlb_init(1);
+ *   - arch/x86/kernel/pci-swiotlb.c|94| <<pci_swiotlb_init>> swiotlb_init(0);
+ */
 void  __init
 swiotlb_init(int verbose)
 {
+	/* 默认64MB */
 	size_t default_size = IO_TLB_DEFAULT_SIZE;
 	unsigned char *vstart;
 	unsigned long bytes;
 
 	if (!io_tlb_nslabs) {
+		/*
+		 * 32768
+		 */
 		io_tlb_nslabs = (default_size >> IO_TLB_SHIFT);
 		io_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);
 	}
 
+	/*
+	 * 1 << 11 = 2048 (2k) --> 每个slot是2K
+	 */
 	bytes = io_tlb_nslabs << IO_TLB_SHIFT;
 
 	/* Get IO TLB memory from the low pages */
+	/*
+	 * 应该是从4G以下分配??? 分配MEMBLOCK_LOW_LIMIT-->ARCH_LOW_ADDRESS_LIMIT
+	 */
 	vstart = memblock_alloc_low(PAGE_ALIGN(bytes), PAGE_SIZE);
 	if (vstart && !swiotlb_init_with_tbl(vstart, io_tlb_nslabs, verbose))
 		return;
@@ -275,6 +488,10 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/x86/pci/sta2x11-fixup.c|59| <<sta2x11_new_instance>> if (swiotlb_late_init_with_default_size(size))
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -319,6 +536,11 @@ swiotlb_late_init_with_default_size(size_t default_size)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|594| <<swiotlb_late_init_with_tbl>> swiotlb_cleanup();
+ *   - kernel/dma/swiotlb.c|624| <<swiotlb_exit>> swiotlb_cleanup();
+ */
 static void swiotlb_cleanup(void)
 {
 	io_tlb_end = 0;
@@ -327,6 +549,11 @@ static void swiotlb_cleanup(void)
 	max_segment = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|262| <<xen_swiotlb_init>> rc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);
+ *   - kernel/dma/swiotlb.c|528| <<swiotlb_late_init_with_default_size>> rc = swiotlb_late_init_with_tbl(vstart, io_tlb_nslabs);
+ */
 int
 swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 {
@@ -367,6 +594,12 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 
 	swiotlb_print_info();
 
+	/*
+	 * 在以下使用late_alloc:
+	 *   - kernel/dma/swiotlb.c|190| <<swiotlb_update_mem_attributes>> if (no_iotlb_memory || late_alloc)
+	 *   - kernel/dma/swiotlb.c|370| <<swiotlb_late_init_with_tbl>> late_alloc = 1;
+	 *   - kernel/dma/swiotlb.c|390| <<swiotlb_exit>> if (late_alloc) {
+	 */
 	late_alloc = 1;
 
 	swiotlb_set_max_segment(io_tlb_nslabs << IO_TLB_SHIFT);
@@ -382,6 +615,12 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/ia64/hp/common/sba_iommu.c|2106| <<sba_init>> swiotlb_exit();
+ *   - arch/powerpc/kernel/dma-swiotlb.c|25| <<check_swiotlb_enabled>> swiotlb_exit();
+ *   - arch/x86/kernel/pci-swiotlb.c|111| <<pci_swiotlb_late_init>> swiotlb_exit();
+ */
 void __init swiotlb_exit(void)
 {
 	if (!io_tlb_orig_addr)
@@ -408,6 +647,15 @@ void __init swiotlb_exit(void)
 /*
  * Bounce: copy the swiotlb buffer from or back to the original dma location
  */
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|572| <<swiotlb_tbl_map_single>> swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_TO_DEVICE);
+ *   - kernel/dma/swiotlb.c|595| <<swiotlb_tbl_unmap_single>> swiotlb_bounce(orig_addr, tlb_addr, mapping_size, DMA_FROM_DEVICE);
+ *   - kernel/dma/swiotlb.c|641| <<swiotlb_tbl_sync_single>> swiotlb_bounce(orig_addr, tlb_addr,
+ *   - kernel/dma/swiotlb.c|648| <<swiotlb_tbl_sync_single>> swiotlb_bounce(orig_addr, tlb_addr,
+ *
+ * Bounce: copy the swiotlb buffer from or back to the original dma location
+ */
 static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 			   size_t size, enum dma_data_direction dir)
 {
@@ -445,6 +693,36 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 	}
 }
 
+/*
+ * [0] swiotlb_tbl_map_single
+ * [0] swiotlb_map
+ * [0] dma_map_page_attrs
+ * [0] e1000_xmit_frame
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] __ip_queue_xmit
+ * [0] __tcp_transmit_skb
+ * [0] tcp_write_xmit
+ * [0] __tcp_push_pending_frames
+ * [0] tcp_sendmsg_locked
+ * [0] tcp_sendmsg
+ * [0] sock_sendmsg
+ * [0] sock_write_iter
+ * [0] new_sync_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3821| <<bounce_map_single>> tlb_addr = swiotlb_tbl_map_single(dev, paddr, size,
+ *   - drivers/xen/swiotlb-xen.c|398| <<xen_swiotlb_map_page>> map = swiotlb_tbl_map_single(dev, phys, size, size, dir, attrs);
+ *   - kernel/dma/swiotlb.c|705| <<swiotlb_map>> swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, dir,
+ */
 phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 		size_t mapping_size, size_t alloc_size,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -577,6 +855,15 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t orig_addr,
 /*
  * tlb_addr is the physical address of the bounce buffer to unmap.
  */
+/*
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3854| <<bounce_map_single>> swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ *   - drivers/iommu/intel/iommu.c|3882| <<bounce_unmap_single>> swiotlb_tbl_unmap_single(dev, tlb_addr, size,
+ *   - drivers/xen/swiotlb-xen.c|425| <<xen_swiotlb_map_page>> swiotlb_tbl_unmap_single(dev, map, size, size, dir,
+ *   - drivers/xen/swiotlb-xen.c|464| <<xen_swiotlb_unmap_page>> swiotlb_tbl_unmap_single(hwdev, paddr, size, size, dir, attrs);
+ *   - kernel/dma/direct.h|122| <<dma_direct_unmap_page>> swiotlb_tbl_unmap_single(dev, phys, size, size, dir, attrs);
+ *   - kernel/dma/swiotlb.c|938| <<swiotlb_map>> swiotlb_tbl_unmap_single(dev, swiotlb_addr, size, size, dir,
+ */
 void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 			      size_t mapping_size, size_t alloc_size,
 			      enum dma_data_direction dir, unsigned long attrs)
@@ -624,6 +911,16 @@ void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 	spin_unlock_irqrestore(&io_tlb_lock, flags);
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/intel/iommu.c|3771| <<bounce_sync_single>> swiotlb_tbl_sync_single(dev, tlb_addr, size, dir, target);
+ *   - drivers/xen/swiotlb-xen.c|481| <<xen_swiotlb_sync_single_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+ *   - drivers/xen/swiotlb-xen.c|491| <<xen_swiotlb_sync_single_for_device>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
+ *   - kernel/dma/direct.c|347| <<dma_direct_sync_sg_for_device>> swiotlb_tbl_sync_single(dev, paddr, sg->length,
+ *   - kernel/dma/direct.c|373| <<dma_direct_sync_sg_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, sg->length, dir,
+ *   - kernel/dma/direct.h|60| <<dma_direct_sync_single_for_device>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_DEVICE);
+ *   - kernel/dma/direct.h|77| <<dma_direct_sync_single_for_cpu>> swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
+ */
 void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 			     size_t size, enum dma_data_direction dir,
 			     enum dma_sync_target target)
@@ -659,6 +956,11 @@ void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
  * Create a swiotlb mapping for the buffer at @paddr, and in case of DMAing
  * to the device copy the data into it as well.
  */
+/*
+ * called by:
+ *   - kernel/dma/direct.h|91| <<dma_direct_map_page>> return swiotlb_map(dev, phys, size, dir, attrs);
+ *   - kernel/dma/direct.h|95| <<dma_direct_map_page>> return swiotlb_map(dev, phys, size, dir, attrs);
+ */
 dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
 {
@@ -689,11 +991,19 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 	return dma_addr;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|500| <<dma_direct_max_mapping_size>> return swiotlb_max_mapping_size(dev);
+ */
 size_t swiotlb_max_mapping_size(struct device *dev)
 {
 	return ((size_t)1 << IO_TLB_SHIFT) * IO_TLB_SEGSIZE;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|498| <<dma_direct_max_mapping_size>> if (is_swiotlb_active() &&
+ */
 bool is_swiotlb_active(void)
 {
 	/*
-- 
2.17.1

