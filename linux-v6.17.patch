From 8b2fa082dae7a772beaf4cff6a24f5ba4f644308 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 1 Dec 2025 10:05:38 -0800
Subject: [PATCH 1/1] linux-v6.17

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/apic.h            |    5 +
 arch/x86/include/asm/kvm_host.h        |  345 ++++
 arch/x86/include/asm/spec-ctrl.h       |    4 +
 arch/x86/include/asm/vmx.h             |    9 +
 arch/x86/kernel/kvm.c                  |    3 +
 arch/x86/kvm/cpuid.c                   |   37 +
 arch/x86/kvm/debugfs.c                 |   19 +
 arch/x86/kvm/hyperv.c                  |   74 +
 arch/x86/kvm/i8254.c                   |   21 +
 arch/x86/kvm/i8259.c                   |    9 +
 arch/x86/kvm/ioapic.c                  |   79 +
 arch/x86/kvm/irq.c                     |  450 +++++
 arch/x86/kvm/irq.h                     |   78 +
 arch/x86/kvm/kvm_cache_regs.h          |   37 +
 arch/x86/kvm/lapic.c                   | 2173 ++++++++++++++++++++++++
 arch/x86/kvm/lapic.h                   |  255 +++
 arch/x86/kvm/mmu/mmu.c                 |   91 +
 arch/x86/kvm/pmu.c                     |   14 +
 arch/x86/kvm/smm.c                     |    6 +
 arch/x86/kvm/svm/avic.c                |  119 ++
 arch/x86/kvm/svm/hyperv.c              |    9 +
 arch/x86/kvm/svm/nested.c              |  368 ++++
 arch/x86/kvm/svm/sev.c                 |   27 +
 arch/x86/kvm/svm/svm.c                 |  349 ++++
 arch/x86/kvm/svm/svm.h                 |    9 +
 arch/x86/kvm/vmx/capabilities.h        |   36 +
 arch/x86/kvm/vmx/common.h              |   16 +
 arch/x86/kvm/vmx/main.c                |   68 +
 arch/x86/kvm/vmx/nested.c              | 1745 +++++++++++++++++++
 arch/x86/kvm/vmx/nested.h              |  124 ++
 arch/x86/kvm/vmx/pmu_intel.c           |   26 +
 arch/x86/kvm/vmx/tdx.c                 |  109 ++
 arch/x86/kvm/vmx/vmcs.h                |    7 +
 arch/x86/kvm/vmx/vmx.c                 | 1025 +++++++++++
 arch/x86/kvm/vmx/vmx.h                 |  185 ++
 arch/x86/kvm/vmx/vmx_ops.h             |   17 +
 arch/x86/kvm/x86.c                     | 1308 ++++++++++++++
 arch/x86/kvm/x86.h                     |   63 +
 arch/x86/kvm/xen.c                     |   10 +
 drivers/block/zram/zcomp.c             |    4 +
 drivers/net/virtio_net.c               |   15 +
 drivers/target/target_core_device.c    |   13 +
 drivers/target/target_core_sbc.c       |   14 +
 drivers/target/target_core_transport.c |    8 +
 drivers/vhost/net.c                    |   54 +
 drivers/vhost/scsi.c                   |  176 ++
 drivers/vhost/test.c                   |   22 +
 drivers/vhost/vdpa.c                   |   30 +
 drivers/vhost/vhost.c                  | 1440 ++++++++++++++++
 drivers/vhost/vhost.h                  |   75 +
 drivers/vhost/vsock.c                  |   48 +
 drivers/virtio/virtio_balloon.c        |  351 ++++
 fs/eventfd.c                           |    3 +
 include/kvm/iodev.h                    |   24 +
 include/linux/balloon_compaction.h     |   16 +
 include/linux/kvm_host.h               |   48 +
 include/linux/poll.h                   |   26 +
 include/uapi/linux/kvm.h               |    7 +
 kernel/entry/kvm.c                     |   11 +
 kernel/kthread.c                       |   21 +
 kernel/vhost_task.c                    |   90 +
 mm/huge_memory.c                       |   12 +
 mm/page_alloc.c                        |    6 +
 mm/page_reporting.c                    |   34 +
 mm/rmap.c                              |   12 +
 virt/kvm/kvm_main.c                    |  103 ++
 66 files changed, 11992 insertions(+)

diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 07ba4935e..23fbbee85 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -511,6 +511,11 @@ static inline bool is_vector_pending(unsigned int vector)
  * 16 bytes aligned. The status of each vector is kept in a single
  * bit.
  */
+/*
+ * 在以下使用apic_find_highest_vector():
+ *   - arch/x86/kvm/lapic.c|814| <<apic_search_irr>> return apic_find_highest_vector(apic->regs + APIC_IRR);
+ *   - arch/x86/kvm/lapic.c|1010| <<apic_find_highest_isr>> result = apic_find_highest_vector(apic->regs + APIC_ISR);
+ */
 static inline int apic_find_highest_vector(void *bitmap)
 {
 	int vec;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f19a76d3c..4c5d15c58 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -105,6 +105,17 @@
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
+/*
+ * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+ *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+ *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+ *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+ *
+ * 处理的函数: kvm_vcpu_reload_apic_access_page()
+ */
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_HV_CRASH		KVM_ARCH_REQ(18)
@@ -114,6 +125,18 @@
 #define KVM_REQ_HV_STIMER		KVM_ARCH_REQ(22)
 #define KVM_REQ_LOAD_EOI_EXITMAP	KVM_ARCH_REQ(23)
 #define KVM_REQ_GET_NESTED_STATE_PAGES	KVM_ARCH_REQ(24)
+/*
+ * 在以下使用KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+ *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+ *
+ * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+ */
 #define KVM_REQ_APICV_UPDATE \
 	KVM_ARCH_REQ_FLAGS(25, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
@@ -783,6 +806,15 @@ struct kvm_vcpu_arch {
 	 */
 	unsigned long regs[NR_VCPU_REGS];
 	u32 regs_avail;
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	u32 regs_dirty;
 
 	unsigned long cr0;
@@ -798,16 +830,74 @@ struct kvm_vcpu_arch {
 	u32 hflags;
 	u64 efer;
 	u64 host_debugctl;
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	bool load_eoi_exitmap_pending;
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 */
 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
 	int mp_state;
 	u64 ia32_misc_enable_msr;
 	u64 smbase;
 	u64 smi_count;
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	bool at_instruction_boundary;
 	bool tpr_access_reporting;
 	bool xfd_no_write_intercept;
@@ -1042,6 +1132,15 @@ struct kvm_vcpu_arch {
 	} pv;
 
 	int pending_ioapic_eoi;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	int pending_external_vector;
 	int highest_stale_pending_ioapic_eoi;
 
@@ -1395,15 +1494,91 @@ struct kvm_arch {
 	struct kvm_pit *vpit;
 #endif
 	atomic_t vapics_in_nmi_mode;
+	/*
+	 * 在以下使用kvm_arch->apic_map_lock:
+	 *   - arch/x86/kvm/lapic.c|412| <<kvm_recalculate_apic_map>> mutex_lock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|425| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|500| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/x86.c|13686| <<kvm_arch_init_vm>> mutex_init(&kvm->arch.apic_map_lock);
+	 */
 	struct mutex apic_map_lock;
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	struct kvm_apic_map __rcu *apic_map;
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_t apic_map_dirty;
 
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 */
 	bool apic_access_memslot_enabled;
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_inhibited:
+	 *   - arch/x86/kvm/lapic.c|2668| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *                    kvm->arch.apic_access_memslot_inhibited)
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_inhibited = true;
+	 */
 	bool apic_access_memslot_inhibited;
 
 	/* Protects apicv_inhibit_reasons */
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	struct rw_semaphore apicv_update_lock;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10175| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10181| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10215| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|11153| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|11183| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|11192| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
@@ -1420,6 +1595,13 @@ struct kvm_arch {
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2686| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|5802| <<kvm_arch_tsc_set_attr>> matched = (vcpu->arch.virtual_tsc_khz &&
+	 *                                   kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *                                   kvm->arch.last_tsc_offset == offset)
+	 */
 	u64 last_tsc_offset;
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
@@ -1452,11 +1634,28 @@ struct kvm_arch {
 
 	u64 disabled_quirks;
 
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	enum kvm_irqchip_mode irqchip_mode;
 	u8 nr_reserved_ioapic_pins;
 
 	bool disabled_lapic_found;
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	bool x2apic_format;
 	bool x2apic_broadcast_quirk_disabled;
 
@@ -1701,6 +1900,12 @@ struct kvm_x86_ops {
 
 	void (*hardware_unsetup)(void);
 	bool (*has_emulated_msr)(struct kvm *kvm, u32 index);
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+	 *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+	 *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+	 *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+	 */
 	void (*vcpu_after_set_cpuid)(struct kvm_vcpu *vcpu);
 
 	unsigned int vm_size;
@@ -1712,8 +1917,20 @@ struct kvm_x86_ops {
 	int (*vcpu_precreate)(struct kvm *kvm);
 	int (*vcpu_create)(struct kvm_vcpu *vcpu);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_reset:
+	 *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+	 *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+	 *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+	 */
 	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
 
+	/*
+	 * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+	 *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+	 *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+	 *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+	 */
 	void (*prepare_switch_to_guest)(struct kvm_vcpu *vcpu);
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
@@ -1777,6 +1994,12 @@ struct kvm_x86_ops {
 	int (*vcpu_pre_run)(struct kvm_vcpu *vcpu);
 	enum exit_fastpath_completion (*vcpu_run)(struct kvm_vcpu *vcpu,
 						  u64 run_flags);
+	/*
+	 * 在以下使用kvm_x86_ops->handle_exit:
+	 *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+	 *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+	 *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+	 */
 	int (*handle_exit)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion exit_fastpath);
 	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
@@ -1801,16 +2024,49 @@ struct kvm_x86_ops {
 	 */
 	bool (*set_vnmi_pending)(struct kvm_vcpu *vcpu);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_ops->enable_irq_window:
+	 *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+	 *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+	 *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+	 */
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
 
 	const bool x2apic_icr_is_split;
 	const unsigned long required_apicv_inhibits;
+	/*
+	 * 在以下使用kvm_x86_ops->allow_apicv_in_x2apic_without_x2apic_virtualization:
+	 *   - arch/x86/kvm/svm/svm.c|5734| <<svm_hardware_setup>> svm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization = true;
+	 *   - arch/x86/kvm/x86.c|11103| <<kvm_vcpu_update_apicv>> if (... kvm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization)
+	 *
+	 * 只有amd svm使用
+	 */
 	bool allow_apicv_in_x2apic_without_x2apic_virtualization;
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);
+	/*
+	 * 在以下使用kvm_x86_ops->load_eoi_exitmap:
+	 *   - arch/x86/kvm/vmx/main.c|976| <<global>> .load_eoi_exitmap = vt_op(load_eoi_exitmap),
+	 *   - arch/x86/kvm/x86.c|11144| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+	 *   - arch/x86/kvm/x86.c|11165| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(
+	 */
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
+	/*
+	 * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+	 *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+	 *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+	 *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+	 */
 	void (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_ops->set_apic_access_page_addr:
+	 *   - arch/x86/kvm/vmx/main.c|992| <<global>> .set_apic_access_page_addr = vt_op(set_apic_access_page_addr),
+	 *   - arch/x86/kvm/mmu/mmu.c|1675| <<kvm_unmap_gfn_range>> if (kvm_x86_ops.set_apic_access_page_addr &&
+	 *   - arch/x86/kvm/vmx/vmx.c|9261| <<vmx_hardware_setup>> vt_x86_ops.set_apic_access_page_addr = NULL;
+	 *   - arch/x86/kvm/x86.c|11306| <<kvm_vcpu_reload_apic_access_page>> kvm_x86_call(set_apic_access_page_addr)(vcpu);
+	 */
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu);
 	void (*deliver_interrupt)(struct kvm_lapic *apic, int delivery_mode,
 				  int trig_mode, int vector);
@@ -1855,6 +2111,12 @@ struct kvm_x86_ops {
 	void (*get_entry_info)(struct kvm_vcpu *vcpu,
 			       u32 *intr_info, u32 *error_code);
 
+	/*
+	 * 在以下使用kvm_x86_ops->check_intercept:
+	 *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+	 *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+	 *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+	 */
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage,
@@ -1916,9 +2178,21 @@ struct kvm_x86_ops {
 	/*
 	 * Returns vCPU specific APICv inhibit reasons
 	 */
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_get_apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/svm/svm.c|5354| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+	 *   - arch/x86/kvm/svm/svm.c|5594| <<svm_hardware_setup>> svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
+	 *   - arch/x86/kvm/x86.c|10158| <<kvm_vcpu_apicv_activated>> kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
+	 */
 	unsigned long (*vcpu_get_apicv_inhibit_reasons)(struct kvm_vcpu *vcpu);
 
 	gva_t (*get_untagged_addr)(struct kvm_vcpu *vcpu, gva_t gva, unsigned int flags);
+	/*
+	 * 在以下使用kvm_x86_ops->alloc_apic_backing_page:
+	 *   - arch/x86/kvm/svm/svm.c|5481| <<global>> .alloc_apic_backing_page = svm_alloc_apic_backing_page,
+	 *   - arch/x86/kvm/lapic.c|4386| <<kvm_create_lapic>> if (kvm_x86_ops.alloc_apic_backing_page)
+	 *   - arch/x86/kvm/lapic.c|4387| <<kvm_create_lapic>> apic->regs = kvm_x86_call(alloc_apic_backing_page)(vcpu);
+	 */
 	void *(*alloc_apic_backing_page)(struct kvm_vcpu *vcpu);
 	int (*gmem_prepare)(struct kvm *kvm, kvm_pfn_t pfn, gfn_t gfn, int max_order);
 	void (*gmem_invalidate)(kvm_pfn_t start, kvm_pfn_t end);
@@ -1926,19 +2200,59 @@ struct kvm_x86_ops {
 };
 
 struct kvm_x86_nested_ops {
+	/*
+	 * 在以下使用kvm_x86_nested_ops->leave_nested:
+	 *   - arch/x86/kvm/svm/nested.c|2093| <<global>> .leave_nested = svm_leave_nested,
+	 *   - arch/x86/kvm/vmx/nested.c|8987| <<global>> .leave_nested = vmx_leave_nested,
+	 *   - arch/x86/kvm/x86.h|143| <<kvm_leave_nested>> kvm_x86_ops.nested_ops->leave_nested(vcpu);
+	 */
 	void (*leave_nested)(struct kvm_vcpu *vcpu);
 	bool (*is_exception_vmexit)(struct kvm_vcpu *vcpu, u8 vector,
 				    u32 error_code);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->check_events:
+	 *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+	 *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+	 *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+	 */
 	int (*check_events)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->has_events:
+	 *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+	 *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+	 *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+	 */
 	bool (*has_events)(struct kvm_vcpu *vcpu, bool for_injection);
 	void (*triple_fault)(struct kvm_vcpu *vcpu);
 	int (*get_state)(struct kvm_vcpu *vcpu,
 			 struct kvm_nested_state __user *user_kvm_nested_state,
 			 unsigned user_data_size);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->set_state:
+	 *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+	 *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+	 *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+	 *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+	 *					user_kvm_nested_state, &kvm_state);
+	 */
 	int (*set_state)(struct kvm_vcpu *vcpu,
 			 struct kvm_nested_state __user *user_kvm_nested_state,
 			 struct kvm_nested_state *kvm_state);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->get_nested_state_pages:
+	 *   - arch/x86/kvm/svm/nested.c|2230| <<global>> .get_nested_state_pages = svm_get_nested_state_pages,
+	 *   - arch/x86/kvm/vmx/nested.c|8994| <<global>> .get_nested_state_pages = vmx_get_nested_state_pages,
+	 *   - arch/x86/kvm/x86.c|11305| <<vcpu_enter_guest(KVM_REQ_GET_NESTED_STATE_PAGES)>>
+	 *                             if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
+	 */
 	bool (*get_nested_state_pages)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->write_log_dirty:
+	 *   - arch/x86/kvm/vmx/nested.c|8462| <<global>> .write_log_dirty = nested_vmx_write_pml_buffer,
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|225| <<FNAME(update_accessed_dirty_bits)>> if (kvm_x86_ops.nested_ops->write_log_dirty(vcpu, addr))
+	 */
 	int (*write_log_dirty)(struct kvm_vcpu *vcpu, gpa_t l2_gpa);
 
 	int (*enable_evmcs)(struct kvm_vcpu *vcpu,
@@ -2251,15 +2565,46 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set);
 
+/*
+ * 在以下使用kvm_set_apicv_inhibit():
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+ *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+ *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+ *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ */
 static inline void kvm_set_apicv_inhibit(struct kvm *kvm,
 					 enum kvm_apicv_inhibit reason)
 {
+	/*
+	 * 在以下使用kvm_set_or_clear_apicv_inhibit():
+	 *   - arch/x86/include/asm/kvm_host.h|2560| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+	 *   - arch/x86/include/asm/kvm_host.h|2566| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+	 */
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
 }
 
+/*
+ * 在以下使用kvm_clear_apicv_inhibit():
+ *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/lapic.c|602| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|612| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|623| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/svm/svm.c|3238| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/x86.c|6688| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|7245| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ */
 static inline void kvm_clear_apicv_inhibit(struct kvm *kvm,
 					   enum kvm_apicv_inhibit reason)
 {
+	/*
+	 * 在以下使用kvm_set_or_clear_apicv_inhibit():
+	 *   - arch/x86/include/asm/kvm_host.h|2560| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+	 *   - arch/x86/include/asm/kvm_host.h|2566| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+	 */
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
 }
 
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 00b7e0398..ee2154712 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -24,6 +24,10 @@ extern void x86_virt_spec_ctrl(u64 guest_virt_spec_ctrl, bool guest);
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * 在以下使用x86_spec_ctrl_set_guest():
+ *   - arch/x86/kvm/svm/svm.c|4491| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index cca7d6641..443b998ea 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -492,6 +492,15 @@ enum vmcs_field {
 #define VMX_AR_RESERVD_MASK 0xfffe0f00
 
 #define TSS_PRIVATE_MEMSLOT			(KVM_USER_MEM_SLOTS + 0)
+/*
+ * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+ *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+ *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+ *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+ *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ */
 #define APIC_ACCESS_PAGE_PRIVATE_MEMSLOT	(KVM_USER_MEM_SLOTS + 1)
 #define IDENTITY_PAGETABLE_PRIVATE_MEMSLOT	(KVM_USER_MEM_SLOTS + 2)
 
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 8ae750cde..5c7270a88 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -344,6 +344,9 @@ static notrace __maybe_unused void kvm_guest_apic_eoi_write(void)
 	 * there's no need for lock or memory barriers.
 	 * An optimization barrier is implied in apic write.
 	 */
+	/*
+	 * 如果还有
+	 */
 	if (__test_and_clear_bit(KVM_PV_EOI_BIT, this_cpu_ptr(&kvm_apic_eoi)))
 		return;
 	apic_native_eoi();
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index e2836a255..20195b7aa 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -283,6 +283,27 @@ static void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 		kvm_update_feature_runtime(vcpu, best, X86_FEATURE_OSXSAVE,
 					   kvm_is_cr4_bit_set(vcpu, X86_CR4_OSXSAVE));
 
+		/*
+		 * 在以下设置kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+		 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+		 * 在以下使用kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+		 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+		 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+		 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+		 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+		 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+		 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+		 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+		 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+		 */
 		kvm_update_feature_runtime(vcpu, best, X86_FEATURE_APIC,
 					   vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
 
@@ -357,6 +378,11 @@ static u32 cpuid_get_reg_unsafe(struct kvm_cpuid_entry2 *entry, u32 reg)
 static int cpuid_func_emulated(struct kvm_cpuid_entry2 *entry, u32 func,
 			       bool include_partially_emulated);
 
+/*
+ * 在以下使用kvm_vcpu_after_set_cpuid():
+ *   - arch/x86/kvm/cpuid.c|568| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ *   - arch/x86/kvm/x86.c|13087| <<kvm_arch_vcpu_create>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -440,6 +466,12 @@ void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 
 	kvm_hv_set_cpuid(vcpu, kvm_cpuid_has_hyperv(vcpu));
 
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+	 *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+	 *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+	 *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+	 */
 	/* Invoke the vendor callback only after the above state is updated. */
 	kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
 
@@ -488,6 +520,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * 在以下使用kvm_set_cpuid():
+ *   - arch/x86/kvm/cpuid.c|617| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|643| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
diff --git a/arch/x86/kvm/debugfs.c b/arch/x86/kvm/debugfs.c
index 999227fc7..4da670d50 100644
--- a/arch/x86/kvm/debugfs.c
+++ b/arch/x86/kvm/debugfs.c
@@ -15,6 +15,25 @@
 static int vcpu_get_timer_advance_ns(void *data, u64 *val)
 {
 	struct kvm_vcpu *vcpu = (struct kvm_vcpu *) data;
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	*val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
 	return 0;
 }
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 72b19a88a..c90eef13c 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -134,6 +134,23 @@ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	down_write(&vcpu->kvm->arch.apicv_update_lock);
 
 	if (auto_eoi_new)
@@ -141,6 +158,13 @@ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
 	else
 		hv->synic_auto_eoi_used--;
 
+	/*
+	 * 在以下使用__kvm_set_or_clear_apicv_inhibit():
+	 *   - arch/x86/kvm/hyperv.c|161| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+	 *   - arch/x86/kvm/x86.c|11293| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+	 *   - arch/x86/kvm/x86.c|13156| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+	 *                                                             APICV_INHIBIT_REASON_BLOCKIRQ, set);
+	 */
 	/*
 	 * Inhibit APICv if any vCPU is using SynIC's AutoEOI, which relies on
 	 * the hypervisor to manually inject IRQs.
@@ -492,6 +516,16 @@ static int synic_set_irq(struct kvm_vcpu_hv_synic *synic, u32 sint)
 	irq.vector = vector;
 	irq.level = 1;
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
 	trace_kvm_hv_synic_set_irq(vcpu->vcpu_id, sint, irq.vector, ret);
 	return ret;
@@ -841,6 +875,19 @@ static int stimer_notify_direct(struct kvm_vcpu_hv_stimer *stimer)
 		.vector = stimer->config.apic_vector
 	};
 
+	/*
+	 * 在以下使用kvm_apic_set_irq():
+	 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *
+	 * 调用__apic_accept_irq()
+	 */
 	if (lapic_in_kernel(vcpu))
 		return !kvm_apic_set_irq(vcpu, &irq, NULL);
 	return 0;
@@ -1556,6 +1603,13 @@ static int kvm_hv_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data, bool host)
 
 		if (!(data & HV_X64_MSR_VP_ASSIST_PAGE_ENABLE)) {
 			hv_vcpu->hv_vapic = data;
+			/*
+			 * 在以下使用kvm_lapic_set_pv_eoi():
+			 *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+			 *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+			 *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+			 *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+			 */
 			if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
 				return 1;
 			break;
@@ -1574,6 +1628,13 @@ static int kvm_hv_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data, bool host)
 			return 1;
 		hv_vcpu->hv_vapic = data;
 		kvm_vcpu_mark_page_dirty(vcpu, gfn);
+		/*
+		 * 在以下使用kvm_lapic_set_pv_eoi():
+		 *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+		 *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+		 *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+		 *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+		 */
 		if (kvm_lapic_set_pv_eoi(vcpu,
 					    gfn_to_gpa(gfn) | KVM_MSR_ENABLED,
 					    sizeof(struct hv_vp_assist_page)))
@@ -2216,6 +2277,19 @@ static void kvm_hv_send_ipi_to_many(struct kvm *kvm, u32 vector,
 					    valid_bank_mask, sparse_banks))
 			continue;
 
+		/*
+		 * 在以下使用kvm_apic_set_irq():
+		 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *
+		 * 调用__apic_accept_irq()
+		 */
 		/* We fail only when APIC is disabled */
 		kvm_apic_set_irq(vcpu, &irq, NULL);
 	}
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index 850972dea..f3301dec6 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -305,12 +305,33 @@ static void kvm_pit_set_reinject(struct kvm_pit *pit, bool reinject)
 	 * So, deactivate APICv when PIT is in reinject mode.
 	 */
 	if (reinject) {
+		/*
+		 * 在以下使用kvm_set_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+		 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+		 *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+		 *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 */
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
 		/* The initial state is preserved while ps->reinject == 0. */
 		kvm_pit_reset_reinject(pit);
 		kvm_register_irq_ack_notifier(kvm, &ps->irq_ack_notifier);
 		kvm_register_irq_mask_notifier(kvm, 0, &pit->mask_notifier);
 	} else {
+		/*
+		 * 在以下使用kvm_clear_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|602| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|612| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|623| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/svm/svm.c|3238| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 *   - arch/x86/kvm/x86.c|6688| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+		 *   - arch/x86/kvm/x86.c|7245| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+		 */
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
 		kvm_unregister_irq_ack_notifier(kvm, &ps->irq_ack_notifier);
 		kvm_unregister_irq_mask_notifier(kvm, 0, &pit->mask_notifier);
diff --git a/arch/x86/kvm/i8259.c b/arch/x86/kvm/i8259.c
index 2ac7f1678..59db46ea0 100644
--- a/arch/x86/kvm/i8259.c
+++ b/arch/x86/kvm/i8259.c
@@ -211,6 +211,11 @@ int kvm_pic_set_irq(struct kvm_kernel_irq_routing_entry *e, struct kvm *kvm,
 /*
  * acknowledge interrupt 'irq'
  */
+/*
+ * 在以下使用pic_intack():
+ *   - arch/x86/kvm/i8259.c|241| <<kvm_pic_read_irq>> pic_intack(&s->pics[0], irq);
+ *   - arch/x86/kvm/i8259.c|245| <<kvm_pic_read_irq>> pic_intack(&s->pics[1], irq2);
+ */
 static inline void pic_intack(struct kvm_kpic_state *s, int irq)
 {
 	s->isr |= 1 << irq;
@@ -582,6 +587,10 @@ static const struct kvm_io_device_ops picdev_elcr_ops = {
 	.write    = picdev_elcr_write,
 };
 
+/*
+ * 在以下使用kvm_pic_init():
+ *   - arch/x86/kvm/x86.c|7022| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> r = kvm_pic_init(kvm);
+ */
 int kvm_pic_init(struct kvm *kvm)
 {
 	struct kvm_pic *s;
diff --git a/arch/x86/kvm/ioapic.c b/arch/x86/kvm/ioapic.c
index 2b5d389bc..2f6b65178 100644
--- a/arch/x86/kvm/ioapic.c
+++ b/arch/x86/kvm/ioapic.c
@@ -114,6 +114,24 @@ static void __rtc_irq_eoi_tracking_restore_one(struct kvm_vcpu *vcpu)
 	union kvm_ioapic_redirect_entry *e;
 
 	e = &ioapic->redirtbl[RTC_GSI];
+	/*
+	 * 在以下使用kvm_apic_match_dest():
+	 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+	 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+	 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+	 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+	 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+	 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+	 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+	 */
 	if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 				 e->fields.dest_id,
 				 kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
@@ -188,6 +206,24 @@ static void ioapic_lazy_update_eoi(struct kvm_ioapic *ioapic, int irq)
 	union kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];
 
 	kvm_for_each_vcpu(i, vcpu, ioapic->kvm) {
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 					 entry->fields.dest_id,
 					 entry->fields.dest_mode) ||
@@ -226,6 +262,15 @@ static int ioapic_set_irq(struct kvm_ioapic *ioapic, unsigned int irq,
 	 * to receive the EOI.  In this case, we do a lazy update of the
 	 * pending EOI when trying to set IOAPIC irq.
 	 */
+	/*
+	 * 在以下使用kvm_apicv_activated():
+	 *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+	 *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+	 *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+	 */
 	if (edge && kvm_apicv_activated(ioapic->kvm))
 		ioapic_lazy_update_eoi(ioapic, irq);
 
@@ -307,6 +352,14 @@ void kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm)
 {
 	if (!ioapic_in_kernel(kvm))
 		return;
+	/*
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 }
 
@@ -466,6 +519,14 @@ static void ioapic_write_indirect(struct kvm_ioapic *ioapic, u32 val)
 			kvm_make_scan_ioapic_request_mask(ioapic->kvm,
 							  vcpu_bitmap);
 		} else {
+			/*
+			 * 在以下使用kvm_make_scan_ioapic_request():
+			 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+			 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+			 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+			 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+			 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+			 */
 			kvm_make_scan_ioapic_request(ioapic->kvm);
 		}
 		break;
@@ -503,6 +564,16 @@ static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)
 		 * if rtc_irq_check_coalesced returns false).
 		 */
 		BUG_ON(ioapic->rtc_status.pending_eoi != 0);
+		/*
+		 * 在以下使用kvm_irq_delivery_to_apic():
+		 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+		 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+		 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+		 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+		 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+		 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+		 */
 		ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
 					       &ioapic->rtc_status.dest_map);
 		ioapic->rtc_status.pending_eoi = (ret < 0 ? 0 : ret);
@@ -796,6 +867,14 @@ void kvm_set_ioapic(struct kvm *kvm, struct kvm_ioapic_state *state)
 	memcpy(ioapic, state, sizeof(struct kvm_ioapic_state));
 	ioapic->irr = 0;
 	ioapic->irr_delivered = 0;
+	/*
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 	kvm_ioapic_inject_all(ioapic, state->irr);
 	spin_unlock(&ioapic->lock);
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 16da89259..1a169c1ae 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -24,6 +24,11 @@
  * check if there are pending timer events
  * to be processed.
  */
+/*
+ * 在以下使用kvm_cpu_has_pending_timer():
+ *   - arch/x86/kvm/x86.c|11346| <<vcpu_run>> if (kvm_cpu_has_pending_timer(vcpu))
+ *   - virt/kvm/kvm_main.c|3621| <<kvm_vcpu_check_block>> if (kvm_cpu_has_pending_timer(vcpu))
+ */
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	int r = 0;
@@ -39,13 +44,39 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 /*
  * check if there is a pending userspace external interrupt
  */
+/*
+ * 在以下使用pending_userspace_extint():
+ *   - arch/x86/kvm/irq.c|110| <<kvm_cpu_has_extint>> return pending_userspace_extint(v);
+ */
 static int pending_userspace_extint(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	return v->arch.pending_external_vector != -1;
 }
 
+/*
+ * 在以下使用get_userspace_extint():
+ *   - arch/x86/kvm/irq.c|172| <<kvm_cpu_get_extint>> return get_userspace_extint(v);
+ */
 static int get_userspace_extint(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	int vector = vcpu->arch.pending_external_vector;
 
 	vcpu->arch.pending_external_vector = -1;
@@ -56,6 +87,13 @@ static int get_userspace_extint(struct kvm_vcpu *vcpu)
  * check if there is pending interrupt from
  * non-APIC source without intack.
  */
+/*
+ * 在以下使用kvm_cpu_has_extint():
+ *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+ *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+ *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+ *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+ */
 int kvm_cpu_has_extint(struct kvm_vcpu *v)
 {
 	/*
@@ -79,10 +117,35 @@ int kvm_cpu_has_extint(struct kvm_vcpu *v)
 		return 0;
 
 #ifdef CONFIG_KVM_IOAPIC
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 *
+	 * struct kvm_arch:
+	 * -> struct kvm_pic *vpic;
+	 *    -> int output;
+	 */
 	if (pic_in_kernel(v->kvm))
 		return v->kvm->arch.vpic->output;
 #endif
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	WARN_ON_ONCE(!irqchip_split(v->kvm));
 	return pending_userspace_extint(v);
 }
@@ -93,14 +156,42 @@ int kvm_cpu_has_extint(struct kvm_vcpu *v)
  * interrupt from apic will handled by hardware,
  * we don't need to check it here.
  */
+/*
+ * 在以下使用kvm_cpu_has_injectable_intr():
+ *   - arch/x86/kvm/svm/svm.c|2327| <<svm_set_gif>> if (... kvm_cpu_has_injectable_intr(&svm->vcpu) ||
+ *   - arch/x86/kvm/vmx/nested.c|5116| <<__nested_vmx_vmexit>> if (kvm_cpu_has_injectable_intr(vcpu) || vcpu->arch.nmi_pending)
+ *   - arch/x86/kvm/x86.c|10465| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10479| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu))
+ */
 int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 */
 	if (kvm_cpu_has_extint(v))
 		return 1;
 
 	if (!is_guest_mode(v) && kvm_vcpu_apicv_active(v))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_apic_has_interrupt():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+	 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+	 *
+	 * 核心思想:
+	 * 1. 判断kvm_apic_present()
+	 * 2. 根据TPR和ISR更新PPR
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 */
 	return kvm_apic_has_interrupt(v) != -1; /* LAPIC */
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_has_injectable_intr);
@@ -109,14 +200,63 @@ EXPORT_SYMBOL_GPL(kvm_cpu_has_injectable_intr);
  * check if there is pending interrupt without
  * intack.
  */
+/*
+ * 在以下使用kvm_cpu_has_interrupt():
+ *   - arch/x86/kvm/svm/nested.c|1594| <<svm_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !svm_interrupt_blocked(vcpu)) {
+ *   - arch/x86/kvm/vmx/nested.c|4348| <<vmx_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !vmx_interrupt_blocked(vcpu)) {
+ *   - arch/x86/kvm/x86.c|11338| <<kvm_vcpu_has_events>> if (kvm_arch_interrupt_allowed(vcpu) && kvm_cpu_has_interrupt(vcpu))
+ *
+ * 先判断有没有PIC或者userspace的interrupt
+ * 然后:
+ *     1. 判断kvm_apic_present()
+ *     2. 根据TPR和ISR更新PPR
+ *        TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ *     3. 选出符合当前PPR的最高的irr, 没有就返回-1
+ *
+ * check if there is pending interrupt without
+ * intack.
+ */
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 */
 	if (kvm_cpu_has_extint(v))
 		return 1;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
 		return kvm_x86_call(protected_apic_has_interrupt)(v);
 
+	/*
+	 * 在以下使用kvm_apic_has_interrupt():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+	 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+	 *
+	 * 核心思想:
+	 * 1. 判断kvm_apic_present()
+	 * 2. 根据TPR和ISR更新PPR
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 */
 	return kvm_apic_has_interrupt(v) != -1;	/* LAPIC */
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_has_interrupt);
@@ -125,8 +265,24 @@ EXPORT_SYMBOL_GPL(kvm_cpu_has_interrupt);
  * Read pending interrupt(from non-APIC source)
  * vector and intack.
  */
+/*
+ * 在以下使用kvm_cpu_get_extint():
+ *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_get_interrupt>> int vector = kvm_cpu_get_extint(v);
+ *   - arch/x86/kvm/vmx/nested.c|4366| <<vmx_check_nested_events>> irq = kvm_cpu_get_extint(vcpu);
+ *
+ * 注释:
+ * Read pending interrupt(from non-APIC source)
+ * vector and intack.
+ */
 int kvm_cpu_get_extint(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 */
 	if (!kvm_cpu_has_extint(v)) {
 		WARN_ON(!lapic_in_kernel(v));
 		return -1;
@@ -141,10 +297,31 @@ int kvm_cpu_get_extint(struct kvm_vcpu *v)
 #endif
 
 #ifdef CONFIG_KVM_IOAPIC
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 */
 	if (pic_in_kernel(v->kvm))
 		return kvm_pic_read_irq(v->kvm); /* PIC */
 #endif
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	WARN_ON_ONCE(!irqchip_split(v->kvm));
 	return get_userspace_extint(v);
 }
@@ -153,13 +330,47 @@ EXPORT_SYMBOL_GPL(kvm_cpu_get_extint);
 /*
  * Read pending interrupt vector and intack.
  */
+/*
+ * 在以下使用kvm_cpu_get_interrupt():
+ *   - arch/x86/kvm/x86.c|10521| <<kvm_check_and_inject_events>> int irq = kvm_cpu_get_interrupt(vcpu);
+ *
+ * Read pending interrupt vector and intack.
+ */
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_get_extint():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_get_interrupt>> int vector = kvm_cpu_get_extint(v);
+	 *   - arch/x86/kvm/vmx/nested.c|4366| <<vmx_check_nested_events>> irq = kvm_cpu_get_extint(vcpu);
+	 *
+	 * 注释:
+	 * Read pending interrupt(from non-APIC source)
+	 * vector and intack.
+	 */
 	int vector = kvm_cpu_get_extint(v);
 	if (vector != -1)
 		return vector;			/* PIC */
 
+	/*
+	 * 在以下使用kvm_apic_has_interrupt():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+	 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+	 *
+	 * 核心思想:
+	 * 1. 判断kvm_apic_present()
+	 * 2. 根据TPR和ISR更新PPR
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 */
 	vector = kvm_apic_has_interrupt(v);	/* APIC */
+	/*
+	 * 在以下使用kvm_apic_ack_interrupt():
+	 *   - arch/x86/kvm/irq.c|187| <<kvm_cpu_get_interrupt>> kvm_apic_ack_interrupt(v, vector);
+	 *   - arch/x86/kvm/vmx/nested.c|4413| <<vmx_check_nested_events>> kvm_apic_ack_interrupt(vcpu, irq);
+	 */
 	if (vector != -1)
 		kvm_apic_ack_interrupt(v, vector);
 
@@ -187,6 +398,13 @@ bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	bool resample = args->flags & KVM_IRQFD_FLAG_RESAMPLE;
 
+	/*
+	 * 在以下使用irqchip_full():
+	 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+	 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+	 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 */
 	return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
 }
 
@@ -195,6 +413,16 @@ bool kvm_arch_irqchip_in_kernel(struct kvm *kvm)
 	return irqchip_in_kernel(kvm);
 }
 
+/*
+ * 在以下使用kvm_irq_delivery_to_apic():
+ *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+ *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 			     struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -203,9 +431,31 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	unsigned long i, dest_vcpu_bitmap[BITS_TO_LONGS(KVM_MAX_VCPUS)];
 	unsigned int dest_vcpus = 0;
 
+	/*
+	 * struct kvm_lapic_irq {
+	 *     u32 vector;
+	 *     u16 delivery_mode;
+	 *     u16 dest_mode;
+	 *     bool level;
+	 *     u16 trig_mode;
+	 *     u32 shorthand;
+	 *     u32 dest_id;          
+	 *     bool msi_redir_hint;
+	 * };
+	 *
+	 * 在以下使用kvm_irq_delivery_to_apic_fast():
+	 *   - arch/x86/kvm/irq.c|424| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+	 */
 	if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
 		return r;
 
+	/*
+	 * 在以下使用kvm_lowest_prio_delivery():
+	 *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+	 */
 	if (irq->dest_mode == APIC_DEST_PHYSICAL &&
 	    irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
 		pr_info("apic: phys broadcast and lowest prio\n");
@@ -218,13 +468,50 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		if (!kvm_apic_present(vcpu))
 			continue;
 
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (!kvm_apic_match_dest(vcpu, src, irq->shorthand,
 					irq->dest_id, irq->dest_mode))
 			continue;
 
+		/*
+		 * 在以下使用kvm_lowest_prio_delivery():
+		 *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+		 *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+		 *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+		 */
 		if (!kvm_lowest_prio_delivery(irq)) {
 			if (r < 0)
 				r = 0;
+			/*
+			 * 在以下使用kvm_apic_set_irq():
+			 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+			 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *
+			 * 调用__apic_accept_irq()
+			 */
 			r += kvm_apic_set_irq(vcpu, irq, dest_map);
 		} else if (kvm_apic_sw_enabled(vcpu->arch.apic)) {
 			if (!kvm_vector_hashing_enabled()) {
@@ -246,12 +533,32 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		lowest = kvm_get_vcpu(kvm, idx);
 	}
 
+	/*
+	 * 在以下使用kvm_apic_set_irq():
+	 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *
+	 * 调用__apic_accept_irq()
+	 */
 	if (lowest)
 		r = kvm_apic_set_irq(lowest, irq, dest_map);
 
 	return r;
 }
 
+/*
+ * 在以下使用kvm_msi_to_lapic_irq():
+ *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+ */
 static void kvm_msi_to_lapic_irq(struct kvm *kvm,
 				 struct kvm_kernel_irq_routing_entry *e,
 				 struct kvm_lapic_irq *irq)
@@ -263,6 +570,15 @@ static void kvm_msi_to_lapic_irq(struct kvm *kvm,
 	trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
 			      (u64)msg.address_hi << 32 : 0), msg.data);
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
 	irq->vector = msg.arch_data.vector;
 	irq->dest_mode = kvm_lapic_irq_dest_mode(msg.arch_addr_lo.dest_mode_logical);
@@ -276,12 +592,33 @@ static void kvm_msi_to_lapic_irq(struct kvm *kvm,
 static inline bool kvm_msi_route_invalid(struct kvm *kvm,
 		struct kvm_kernel_irq_routing_entry *e)
 {
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
 }
 
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id, int level, bool line_status)
 {
+	/*
+	 * struct kvm_lapic_irq {
+	 *     u32 vector;
+	 *     u16 delivery_mode;
+	 *     u16 dest_mode;
+	 *     bool level;
+	 *     u16 trig_mode;
+	 *     u32 shorthand;
+	 *     u32 dest_id;
+	 *     bool msi_redir_hint;
+	 * };
+	 */
 	struct kvm_lapic_irq irq;
 
 	if (kvm_msi_route_invalid(kvm, e))
@@ -290,8 +627,25 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 	if (!level)
 		return -1;
 
+	/*
+	 * 在以下使用kvm_msi_to_lapic_irq():
+	 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+	 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+	 */
 	kvm_msi_to_lapic_irq(kvm, e, &irq);
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
 }
 
@@ -313,8 +667,20 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (kvm_msi_route_invalid(kvm, e))
 			return -EINVAL;
 
+		/*
+		 * 在以下使用kvm_msi_to_lapic_irq():
+		 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+		 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+		 */
 		kvm_msi_to_lapic_irq(kvm, e, &irq);
 
+		/*
+		 * 在以下使用kvm_irq_delivery_to_apic_fast():
+		 *   - arch/x86/kvm/irq.c|424| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+		 *   - arch/x86/kvm/irq.c|554| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+		 */
 		if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
 			return r;
 		break;
@@ -361,6 +727,16 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	switch (ue->type) {
 #ifdef CONFIG_KVM_IOAPIC
 	case KVM_IRQ_ROUTING_IRQCHIP:
+		/*
+		 * 在以下使用irqchip_split():
+		 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+		 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+		 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+		 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+		 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+		 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+		 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+		 */
 		if (irqchip_split(kvm))
 			return -EINVAL;
 		e->irqchip.pin = ue->u.irqchip.pin;
@@ -411,6 +787,10 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_intr_is_single_vcpu():
+ *   - arch/x86/kvm/irq.c|801| <<kvm_pi_update_irte>> if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
+ */
 bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			     struct kvm_vcpu **dest_vcpu)
 {
@@ -418,6 +798,9 @@ bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 	unsigned long i;
 	struct kvm_vcpu *vcpu;
 
+	/*
+	 * 只在这里调用
+	 */
 	if (kvm_intr_is_single_vcpu_fast(kvm, irq, dest_vcpu))
 		return true;
 
@@ -425,6 +808,24 @@ bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 		if (!kvm_apic_present(vcpu))
 			continue;
 
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (!kvm_apic_match_dest(vcpu, NULL, irq->shorthand,
 					irq->dest_id, irq->dest_mode))
 			continue;
@@ -442,6 +843,24 @@ EXPORT_SYMBOL_GPL(kvm_intr_is_single_vcpu);
 void kvm_scan_ioapic_irq(struct kvm_vcpu *vcpu, u32 dest_id, u16 dest_mode,
 			 u8 vector, unsigned long *ioapic_handled_vectors)
 {
+	/*
+	 * 在以下使用kvm_apic_match_dest():
+	 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+	 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+	 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+	 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+	 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+	 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+	 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+	 */
 	/*
 	 * Intercept EOI if the vCPU is the target of the new IRQ routing, or
 	 * the vCPU has a pending IRQ from the old routing, i.e. if the vCPU
@@ -486,6 +905,13 @@ void kvm_scan_ioapic_routes(struct kvm_vcpu *vcpu,
 			if (entry->type != KVM_IRQ_ROUTING_MSI)
 				continue;
 
+			/*
+			 * 在以下使用kvm_msi_to_lapic_irq():
+			 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+			 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+			 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+			 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+			 */
 			kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
 
 			if (!irq.trig_mode)
@@ -504,6 +930,23 @@ void kvm_arch_irq_routing_update(struct kvm *kvm)
 	kvm_hv_irq_routing_update(kvm);
 #endif
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 *
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	if (irqchip_split(kvm))
 		kvm_make_scan_ioapic_request(kvm);
 }
@@ -521,6 +964,13 @@ static int kvm_pi_update_irte(struct kvm_kernel_irqfd *irqfd,
 		return -EINVAL;
 
 	if (entry && entry->type == KVM_IRQ_ROUTING_MSI) {
+		/*
+		 * 在以下使用kvm_msi_to_lapic_irq():
+		 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+		 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+		 */
 		kvm_msi_to_lapic_irq(kvm, entry, &irq);
 
 		/*
diff --git a/arch/x86/kvm/irq.h b/arch/x86/kvm/irq.h
index 5e62c1f79..c0813335a 100644
--- a/arch/x86/kvm/irq.h
+++ b/arch/x86/kvm/irq.h
@@ -73,8 +73,23 @@ int kvm_setup_default_ioapic_and_pic_routing(struct kvm *kvm);
 int kvm_vm_ioctl_get_irqchip(struct kvm *kvm, struct kvm_irqchip *chip);
 int kvm_vm_ioctl_set_irqchip(struct kvm *kvm, struct kvm_irqchip *chip);
 
+/*
+ * 在以下使用irqchip_full():
+ *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+ *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+ *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+ */
 static inline int irqchip_full(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
@@ -88,14 +103,50 @@ static __always_inline int irqchip_full(struct kvm *kvm)
 }
 #endif
 
+/*
+ * 在以下使用pic_in_kernel():
+ *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+ *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+ *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+ *                                        likely(!pic_in_kernel(vcpu->kvm));
+ *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+ *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+ */
 static inline int pic_in_kernel(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用irqchip_full():
+	 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+	 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+	 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 */
 	return irqchip_full(kvm);
 }
 
 
+/*
+ * 在以下使用irqchip_split():
+ *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+ *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+ *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+ *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+ *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+ *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+ */
 static inline int irqchip_split(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
@@ -103,8 +154,35 @@ static inline int irqchip_split(struct kvm *kvm)
 	return mode == KVM_IRQCHIP_SPLIT;
 }
 
+/*
+ * 在以下使用irqchip_in_kernel():
+ *   - arch/x86/kvm/irq.c|220| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.c|225| <<kvm_arch_irqchip_in_kernel>> return irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.c|369| <<kvm_vm_ioctl_irq_line>> if (!irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/irq.c|380| <<kvm_arch_can_set_irq_routing>> return irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.c|550| <<kvm_pi_update_irte>> if (WARN_ON_ONCE(!irqchip_in_kernel(kvm) || !kvm_arch_has_irq_bypass()))
+ *   - arch/x86/kvm/lapic.c|357| <<kvm_recalculate_apic_map>> WARN_ONCE(!irqchip_in_kernel(kvm),
+ *   - arch/x86/kvm/lapic.c|2956| <<kvm_create_lapic>> if (!irqchip_in_kernel(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/avic.c|714| <<avic_init_vcpu>> if (!enable_apicv || !irqchip_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/vmx/posted_intr.c|154| <<vmx_can_use_vtd_pi>> return irqchip_in_kernel(kvm) && kvm_arch_has_irq_bypass() &&
+ *   - arch/x86/kvm/vmx/vmx.c|4591| <<vmx_alloc_ipiv_pid_table>> if (!irqchip_in_kernel(kvm) || !enable_ipiv)
+ *   - arch/x86/kvm/x86.c|5252| <<kvm_vcpu_ioctl_interrupt>> if (!irqchip_in_kernel(vcpu->kvm)) {
+ *   - arch/x86/kvm/x86.c|5902| <<kvm_vcpu_ioctl_enable_cap>> if (!irqchip_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|6493| <<kvm_vm_ioctl_enable_cap>> if (irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|6717| <<kvm_vm_ioctl_enable_cap>> if (!irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|7023| <<kvm_arch_vm_ioctl>> if (irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|12423| <<kvm_arch_vcpu_create>> if (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu)
+ */
 static inline int irqchip_in_kernel(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 36a8786db..5db4ae307 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -69,10 +69,26 @@ static inline bool kvm_register_is_available(struct kvm_vcpu *vcpu,
 	return test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
 }
 
+/*
+ * 在以下使用kvm_register_is_dirty():
+ *   - arch/x86/kvm/vmx/vmx.c|3237| <<vmx_ept_load_pdptrs>> if (!kvm_register_is_dirty(vcpu, VCPU_EXREG_PDPTR))
+ *   - arch/x86/kvm/vmx/vmx.c|3402| <<vmx_load_mmu_pgd>> else if (kvm_register_is_dirty(vcpu, VCPU_EXREG_CR3))
+ *   - arch/x86/kvm/vmx/vmx.c|7533| <<vmx_vcpu_run>> if (kvm_register_is_dirty(vcpu, VCPU_REGS_RSP))
+ *   - arch/x86/kvm/vmx/vmx.c|7535| <<vmx_vcpu_run>> if (kvm_register_is_dirty(vcpu, VCPU_REGS_RIP))
+ */
 static inline bool kvm_register_is_dirty(struct kvm_vcpu *vcpu,
 					 enum kvm_reg reg)
 {
 	kvm_assert_register_caching_allowed(vcpu);
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
 }
 
@@ -88,6 +104,15 @@ static inline void kvm_register_mark_dirty(struct kvm_vcpu *vcpu,
 {
 	kvm_assert_register_caching_allowed(vcpu);
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
 }
 
@@ -222,12 +247,24 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);
 }
 
+/*
+ * 在以下使用enter_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * 在以下使用leave_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 5fc437341..f97b6cdda 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -53,6 +53,28 @@
 #define mod_64(x, y) ((x) % (y))
 #endif
 
+/*
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ *
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ */
+
 /* 14 is the version for Xeon and Pentium 8.4.8*/
 #define APIC_VERSION			0x14UL
 #define LAPIC_MMIO_LENGTH		(1 << 12)
@@ -101,9 +123,30 @@ bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 		apic_test_vector(vector, apic->regs + APIC_IRR);
 }
 
+/*
+ * 在以下使用kvm_has_noapic_vcpu:
+ *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+ *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+ */
 __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 EXPORT_SYMBOL_GPL(kvm_has_noapic_vcpu);
 
+/*
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|139| <<global>> __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);
+ *   - arch/x86/kvm/lapic.h|335| <<global>> extern struct static_key_false_deferred apic_sw_disabled;
+ *   - arch/x86/kvm/lapic.c|702| <<apic_set_spiv>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|704| <<apic_set_spiv>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|3657| <<kvm_free_lapic>> static_branch_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|4430| <<kvm_create_lapic>> static_branch_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|5305| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|5306| <<kvm_lapic_exit>> WARN_ON(static_branch_unlikely(&apic_sw_disabled.key));
+ *   - arch/x86/kvm/lapic.h|346| <<kvm_apic_sw_enabled>> if (static_branch_unlikely(&apic_sw_disabled.key))
+ */
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);
 __read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);
 
@@ -142,11 +185,28 @@ static bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)
 	return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
 }
 
+/*
+ * 在以下使用kvm_apic_calc_x2apic_ldr():
+ *   - arch/x86/kvm/lapic.c|698| <<kvm_apic_set_x2apic_id>> u32 ldr = kvm_apic_calc_x2apic_ldr(id);
+ *   - arch/x86/kvm/lapic.c|4331| <<kvm_apic_state_fixup>> *ldr = kvm_apic_calc_x2apic_ldr(x2apic_id);
+ *
+ * 在x2apic下.
+ * LDR[31:16] = Cluster ID
+ * LDR[15:0]  = Logical ID Mask
+ *
+ * kvm_apic_calc_x2apic_ldr()根据给定的x2APIC ID,
+ * 计算出该处理器应有的 LDR 值.
+ */
 static inline u32 kvm_apic_calc_x2apic_ldr(u32 id)
 {
 	return ((id >> 4) << 16) | (1 << (id & 0xf));
 }
 
+/*
+ * 在以下使用kvm_apic_map_get_logical_dest():
+ *   - arch/x86/kvm/lapic.c|347| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
+ *   - arch/x86/kvm/lapic.c|1785| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+ */
 static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 		u32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {
 	switch (map->logical_mode) {
@@ -187,12 +247,19 @@ static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 	}
 }
 
+/*
+ * 在以下使用kvm_recalculate_phys_map():
+ *   - arch/x86/kvm/lapic.c|496| <<kvm_recalculate_apic_map>> r = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);
+ */
 static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 				    struct kvm_vcpu *vcpu,
 				    bool *xapic_id_mismatch)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u32 x2apic_id = kvm_x2apic_id(apic);
+	/*
+	 * 返回: kvm_lapic_get_reg(apic, APIC_ID) >> 24;
+	 */
 	u32 xapic_id = kvm_xapic_id(apic);
 	u32 physical_id;
 
@@ -236,6 +303,15 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 	 * manually modified its xAPIC IDs, events targeting that ID are
 	 * supposed to be recognized by all vCPUs with said ID.
 	 */
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	if (vcpu->kvm->arch.x2apic_format) {
 		/* See also kvm_apic_match_physical_addr(). */
 		if (apic_x2apic_mode(apic) || x2apic_id > 0xff)
@@ -263,6 +339,10 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_recalculate_logical_map():
+ *   - arch/x86/kvm/lapic.c|508| <<kvm_recalculate_apic_map>> kvm_recalculate_logical_map(new, vcpu);
+ */
 static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 					struct kvm_vcpu *vcpu)
 {
@@ -285,6 +365,11 @@ static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 	if (apic_x2apic_mode(apic)) {
 		logical_mode = KVM_APIC_MODE_X2APIC;
 	} else {
+		/*
+		 * 代码:
+		 * #define         GET_APIC_LOGICAL_ID(x)  (((x) >> 24) & 0xFFu)
+		 * #define         SET_APIC_LOGICAL_ID(x)  (((x) << 24))
+		 */
 		ldr = GET_APIC_LOGICAL_ID(ldr);
 		if (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)
 			logical_mode = KVM_APIC_MODE_XAPIC_FLAT;
@@ -310,9 +395,18 @@ static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 	 * reversing the LDR calculation to get cluster of APICs, i.e. no
 	 * additional work is required.
 	 */
+	/*
+	 * 在kvm_apic_map_get_logical_dest()中
+	 * 如果是KVM_APIC_MODE_X2APIC直接使用phys_map[]
+	 */
 	if (apic_x2apic_mode(apic))
 		return;
 
+	/*
+	 * 在以下使用kvm_apic_map_get_logical_dest():
+	 *   - arch/x86/kvm/lapic.c|347| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
+	 *   - arch/x86/kvm/lapic.c|1785| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+	 */
 	if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
 							&cluster, &mask))) {
 		new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
@@ -341,6 +435,14 @@ enum {
 	DIRTY
 };
 
+/*
+ * 在以下使用kvm_recalculate_apic_map():
+ *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ */
 static void kvm_recalculate_apic_map(struct kvm *kvm)
 {
 	struct kvm_apic_map *new, *old = NULL;
@@ -350,6 +452,19 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	bool xapic_id_mismatch;
 	int r;
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	/* Read kvm->arch.apic_map_dirty before kvm->arch.apic_map.  */
 	if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
 		return;
@@ -357,9 +472,49 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	WARN_ONCE(!irqchip_in_kernel(kvm),
 		  "Dirty APIC map without an in-kernel local APIC");
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_lock:
+	 *   - arch/x86/kvm/lapic.c|412| <<kvm_recalculate_apic_map>> mutex_lock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|425| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|500| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/x86.c|13686| <<kvm_arch_init_vm>> mutex_init(&kvm->arch.apic_map_lock);
+	 */
 	mutex_lock(&kvm->arch.apic_map_lock);
 
 retry:
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *
+	 *
+	 * 1200  * atomic_cmpxchg_acquire() - atomic compare and exchange with acquire ordering
+	 * 1201  * @v: pointer to atomic_t
+	 * 1202  * @old: int value to compare with
+	 * 1203  * @new: int value to assign
+	 * 1204  *
+	 * 1205  * If (@v == @old), atomically updates @v to @new with acquire ordering.
+	 * 1206  * Otherwise, @v is not modified and relaxed ordering is provided.
+	 * 1207  *
+	 * 1208  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_acquire() there.
+	 * 1209  *
+	 * 1210  * Return: The original value of @v.
+	 * ... ...
+	 * 1212 static __always_inline int
+	 * 1213 atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
+	 *
+	 * 只有kvm->arch.apic_map_dirty是DIRTY的时候才改成UPDATE_IN_PROGRESS
+	 * 返回旧的value
+	 */
 	/*
 	 * Read kvm->arch.apic_map_dirty before kvm->arch.apic_map (if clean)
 	 * or the APIC registers (if dirty).  Note, on retry the map may have
@@ -383,6 +538,9 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	 */
 	xapic_id_mismatch = false;
 
+	/*
+	 * kvm_x2apic_id()返回vcpu_id
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		if (kvm_apic_present(vcpu))
 			max_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));
@@ -394,6 +552,18 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	if (!new)
 		goto out;
 
+	/*
+	 * 1198 struct kvm_apic_map {
+	 * 1199         struct rcu_head rcu;
+	 * 1200         enum kvm_apic_logical_mode logical_mode;
+	 * 1201         u32 max_apic_id;
+	 * 1202         union {
+	 * 1203                 struct kvm_lapic *xapic_flat_map[8];
+	 * 1204                 struct kvm_lapic *xapic_cluster_map[16][4];
+	 * 1205         };
+	 * 1206         struct kvm_lapic *phys_map[];
+	 * 1207 };
+	 */
 	new->max_apic_id = max_id;
 	new->logical_mode = KVM_APIC_MODE_SW_DISABLED;
 
@@ -416,29 +586,107 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 		kvm_recalculate_logical_map(new, vcpu);
 	}
 out:
+	/*
+	 * 在以下使用kvm_set_apicv_inhibit():
+	 *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+	 *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+	 *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+	 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+	 *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+	 *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+	 *
+	 * 在以下使用kvm_clear_apicv_inhibit():
+	 *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+	 *   - arch/x86/kvm/lapic.c|602| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|612| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|623| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+	 *   - arch/x86/kvm/svm/svm.c|3238| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+	 *   - arch/x86/kvm/x86.c|6688| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+	 *   - arch/x86/kvm/x86.c|7245| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+	 */
 	/*
 	 * The optimized map is effectively KVM's internal version of APICv,
 	 * and all unwanted aliasing that results in disabling the optimized
 	 * map also applies to APICv.
 	 */
+	/*
+	 * 注释:
+	 * APICv is disabled because not all vCPUs have a 1:1 mapping between
+	 * APIC ID and vCPU, _and_ KVM is not applying its x2APIC hotplug hack.
+	 */
 	if (!new)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
 
+	/*
+	 * 注释:
+	 * AVIC is disabled because not all vCPUs with a valid LDR have a 1:1
+	 * mapping between logical ID and vCPU.
+	 */
 	if (!new || new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
 
+	/*
+	 * 注释:
+	 * For simplicity, the APIC acceleration is inhibited
+	 * first time either APIC ID or APIC base are changed by the guest
+	 * from their reset values.
+	 */
 	if (xapic_id_mismatch)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
 
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	old = rcu_dereference_protected(kvm->arch.apic_map,
 			lockdep_is_held(&kvm->arch.apic_map_lock));
 	rcu_assign_pointer(kvm->arch.apic_map, new);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *
+	 *
+	 * 1220  * atomic_cmpxchg_release() - atomic compare and exchange with release ordering
+	 * 1221  * @v: pointer to atomic_t
+	 * 1222  * @old: int value to compare with
+	 * 1223  * @new: int value to assign
+	 * 1224  *
+	 * 1225  * If (@v == @old), atomically updates @v to @new with release ordering.
+	 * 1226  * Otherwise, @v is not modified and relaxed ordering is provided.
+	 * 1227  *
+	 * 1228  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_release() there.
+	 * 1229  *
+	 * 1230  * Return: The original value of @v.
+	 * ... ...
+	 * 1232 static __always_inline int
+	 * 1233 atomic_cmpxchg_release(atomic_t *v, int old, int new)
+	 *
+	 * 只有kvm->arch.apic_map_dirty是UPDATE_IN_PROGRESS的时候才改成CLEAN
+	 */
 	/*
 	 * Write kvm->arch.apic_map before clearing apic->apic_map_dirty.
 	 * If another update has come in, leave it DIRTY.
@@ -450,15 +698,36 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	if (old)
 		kvfree_rcu(old, rcu);
 
+	/*
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * 在以下使用apic_set_spiv():
+ *   - arch/x86/kvm/lapic.c|2713| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_set_spiv(apic, val & mask);
+ *   - arch/x86/kvm/lapic.c|3299| <<kvm_lapic_reset>> apic_set_spiv(apic, 0xff);
+ *   - arch/x86/kvm/lapic.c|3734| <<kvm_apic_set_state>> apic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));
+ */
 static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 {
 	bool enabled = val & APIC_SPIV_APIC_ENABLED;
 
 	kvm_lapic_set_reg(apic, APIC_SPIV, val);
 
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (enabled != apic->sw_enabled) {
 		apic->sw_enabled = enabled;
 		if (enabled)
@@ -466,6 +735,19 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 		else
 			static_branch_inc(&apic_sw_disabled.key);
 
+		/*
+		 * 在以下使用kvm_arch->apic_map_dirty:
+		 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+		 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 */
 		atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 	}
 
@@ -476,24 +758,74 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_set_xapic_id():
+ *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+ *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_xapic_id(struct kvm_lapic *apic, u8 id)
 {
 	kvm_lapic_set_reg(apic, APIC_ID, id << 24);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
 static inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)
 {
 	kvm_lapic_set_reg(apic, APIC_LDR, id);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
 static inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)
 {
 	kvm_lapic_set_reg(apic, APIC_DFR, val);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
+/*
+ * 在以下使用kvm_apic_set_x2apic_id():
+ *   - arch/x86/kvm/lapic.c|3266| <<__kvm_apic_set_base>> kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 {
 	u32 ldr = kvm_apic_calc_x2apic_ldr(id);
@@ -502,6 +834,19 @@ static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 
 	kvm_lapic_set_reg(apic, APIC_ID, id);
 	kvm_lapic_set_reg(apic, APIC_LDR, ldr);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
@@ -606,6 +951,11 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * 在以下使用__kvm_apic_update_irr():
+ *   - arch/x86/kvm/lapic.c|791| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|4381| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+ */
 bool __kvm_apic_update_irr(unsigned long *pir, void *regs, int *max_irr)
 {
 	unsigned long pir_vals[NR_PIR_WORDS];
@@ -647,23 +997,73 @@ EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, unsigned long *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * 在以下使用__kvm_apic_update_irr():
+	 *   - arch/x86/kvm/lapic.c|791| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+	 *   - arch/x86/kvm/vmx/nested.c|4381| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+	 */
 	bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	if (unlikely(!apic->apicv_active && irr_updated))
 		apic->irr_pending = true;
 	return irr_updated;
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_irr);
 
+/*
+ * 在以下使用apic_search_irr():
+ *   - arch/x86/kvm/lapic.c|702| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+ *   - arch/x86/kvm/lapic.c|720| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+ */
 static inline int apic_search_irr(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用apic_find_highest_vector():
+	 *   - arch/x86/kvm/lapic.c|814| <<apic_search_irr>> return apic_find_highest_vector(apic->regs + APIC_IRR);
+	 *   - arch/x86/kvm/lapic.c|1010| <<apic_find_highest_isr>> result = apic_find_highest_vector(apic->regs + APIC_ISR);
+	 */
 	return apic_find_highest_vector(apic->regs + APIC_IRR);
 }
 
+/*
+ * 在以下使用apic_find_highest_irr():
+ *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+ *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+ */
 static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 {
 	int result;
 
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	/*
 	 * Note that irr_pending is just a hint. It will be always
 	 * true with virtual interrupt delivery enabled.
@@ -671,41 +1071,106 @@ static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 	if (!apic->irr_pending)
 		return -1;
 
+	/*
+	 * 在以下使用apic_search_irr():
+	 *   - arch/x86/kvm/lapic.c|702| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|720| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+	 */
 	result = apic_search_irr(apic);
 	ASSERT(result == -1 || result >= 16);
 
 	return result;
 }
 
+/*
+ * 在以下使用apic_clear_irr():
+ *   - arch/x86/kvm/lapic.c|722| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|3273| <<kvm_apic_ack_interrupt>> apic_clear_irr(vector, apic);
+ */
 static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (unlikely(apic->apicv_active)) {
 		apic_clear_vector(vec, apic->regs + APIC_IRR);
 	} else {
+		/*
+		 * 在以下使用kvm_lapic->irr_pending:
+		 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+		 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+		 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+		 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+		 *                                               vcpu->arch.apic->irr_pending = true;
+		 */
 		apic->irr_pending = false;
 		apic_clear_vector(vec, apic->regs + APIC_IRR);
+		/*
+		 * 在以下使用apic_search_irr():
+		 *   - arch/x86/kvm/lapic.c|702| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+		 *   - arch/x86/kvm/lapic.c|720| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+		 */
 		if (apic_search_irr(apic) != -1)
 			apic->irr_pending = true;
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_clear_irr():
+ *   - arch/x86/kvm/vmx/nested.c|4412| <<vmx_check_nested_events>> kvm_apic_clear_irr(vcpu, irq);
+ */
 void kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)
 {
+	/*
+	 * 在以下使用apic_clear_irr():
+	 *   - arch/x86/kvm/lapic.c|722| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|3273| <<kvm_apic_ack_interrupt>> apic_clear_irr(vector, apic);
+	 */
 	apic_clear_irr(vec, vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_clear_irr);
 
+/*
+ * 在以下使用apic_vector_to_isr():
+ *   - arch/x86/kvm/lapic.c|803| <<apic_set_isr>> apic_vector_to_isr(vec, apic)))
+ *   - arch/x86/kvm/lapic.c|867| <<apic_clear_isr>> apic_vector_to_isr(vec, apic)))
+ */
 static void *apic_vector_to_isr(int vec, struct kvm_lapic *apic)
 {
 	return apic->regs + APIC_ISR + APIC_VECTOR_TO_REG_OFFSET(vec);
 }
 
+/*
+ * 在以下使用apic_set_isr():
+ *   - arch/x86/kvm/lapic.c|3288| <<kvm_apic_ack_interrupt>> apic_set_isr(vector, apic);
+ */
 static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 {
 	if (__test_and_set_bit(APIC_VECTOR_TO_BIT_NUMBER(vec),
 			       apic_vector_to_isr(vec, apic)))
 		return;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 */
 	/*
 	 * With APIC virtualization enabled, all caching is disabled
 	 * because the processor can modify ISR under the hood.  Instead
@@ -714,8 +1179,28 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	if (unlikely(apic->apicv_active))
 		kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
 	else {
+		/*
+		 * 在以下使用kvm_lapic->isr_count:
+		 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+		 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+		 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+		 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+		 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+		 */
 		++apic->isr_count;
 		BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+		/*
+		 * 在以下使用kvm_lapic->highest_isr_cache:
+		 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+		 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+		 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+		 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+		 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+		 */
 		/*
 		 * ISR (in service register) bit is set when injecting an interrupt.
 		 * The highest vector is injected. Thus the latest bit set matches
@@ -725,25 +1210,63 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * 在以下使用apic_find_highest_isr():
+ *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+ *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+ *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+ */
 static inline int apic_find_highest_isr(struct kvm_lapic *apic)
 {
 	int result;
 
+	/*
+	 * 在以下使用kvm_lapic->isr_count:
+	 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+	 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+	 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+	 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+	 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+	 */
 	/*
 	 * Note that isr_count is always 1, and highest_isr_cache
 	 * is always -1, with APIC virtualization enabled.
 	 */
 	if (!apic->isr_count)
 		return -1;
+	/*
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	if (likely(apic->highest_isr_cache != -1))
 		return apic->highest_isr_cache;
 
+	/*
+	 * 在以下使用apic_find_highest_vector():
+	 *   - arch/x86/kvm/lapic.c|814| <<apic_search_irr>> return apic_find_highest_vector(apic->regs + APIC_IRR);
+	 *   - arch/x86/kvm/lapic.c|1010| <<apic_find_highest_isr>> result = apic_find_highest_vector(apic->regs + APIC_ISR);
+	 */
 	result = apic_find_highest_vector(apic->regs + APIC_ISR);
 	ASSERT(result == -1 || result >= 16);
 
 	return result;
 }
 
+/*
+ * 在以下使用apic_set_eoi():
+ *   - arch/x86/kvm/lapic.c|2065| <<apic_set_eoi>> apic_clear_isr(vector, apic);
+ */
 static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 {
 	if (!__test_and_clear_bit(APIC_VECTOR_TO_BIT_NUMBER(vec),
@@ -757,28 +1280,106 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	 * on the other hand isr_count and highest_isr_cache are unused
 	 * and must be left alone.
 	 */
+	/*
+	 * 在以下使用apic_find_highest_isr():
+	 *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+	 *
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 */
 	if (unlikely(apic->apicv_active))
 		kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
 	else {
+		/*
+		 * 在以下使用kvm_lapic->isr_count:
+		 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+		 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+		 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+		 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+		 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+		 */
 		--apic->isr_count;
 		BUG_ON(apic->isr_count < 0);
+		/*
+		 * 在以下使用kvm_lapic->highest_isr_cache:
+		 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+		 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+		 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+		 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+		 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+		 */
 		apic->highest_isr_cache = -1;
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_update_hwapic_isr():
+ *   - arch/x86/kvm/vmx/nested.c|5121| <<__nested_vmx_vmexit>> kvm_apic_update_hwapic_isr(vcpu);
+ */
 void kvm_apic_update_hwapic_isr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (WARN_ON_ONCE(!lapic_in_kernel(vcpu)) || !apic->apicv_active)
 		return;
 
+	/*
+	 * 在以下使用apic_find_highest_isr():
+	 *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+	 *
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *
+	 * vt_hwapic_isr_update()
+	 * vmx_hwapic_isr_update()
+	 */
 	kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_hwapic_isr);
 
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用apic_find_highest_irr():
+	 *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	/* This may race with setting of irr in __apic_accept_irq() and
 	 * value returned may be wrong, but kvm_vcpu_kick() in __apic_accept_irq
 	 * will cause vmexit immediately and the value will be recalculated
@@ -792,6 +1393,19 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * 在以下使用kvm_apic_set_irq():
+ *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *
+ * 调用__apic_accept_irq()
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -816,6 +1430,19 @@ static int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,
 		min((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {
 		if (map->phys_map[min + i]) {
 			vcpu = map->phys_map[min + i]->vcpu;
+			/*
+			 * 在以下使用kvm_apic_set_irq():
+			 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+			 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *
+			 * 调用__apic_accept_irq()
+			 */
 			count += kvm_apic_set_irq(vcpu, irq, NULL);
 		}
 	}
@@ -823,6 +1450,10 @@ static int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,
 	return count;
 }
 
+/*
+ * 在以下使用kvm_pv_send_ipi():
+ *   - arch/x86/kvm/x86.c|10321| <<____kvm_emulate_hypercall(KVM_HC_SEND_IPI)>> ret = kvm_pv_send_ipi(vcpu->kvm, a0, a1, a2, a3, op_64_bit);
+ */
 int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 		    unsigned long ipi_bitmap_high, u32 min,
 		    unsigned long icr, int op_64_bit)
@@ -841,6 +1472,17 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 	irq.trig_mode = icr & APIC_INT_LEVELTRIG;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
 	count = -EOPNOTSUPP;
@@ -873,14 +1515,35 @@ static inline bool pv_eoi_enabled(struct kvm_vcpu *vcpu)
 	return vcpu->arch.pv_eoi.msr_val & KVM_MSR_ENABLED;
 }
 
+/*
+ * 在以下使用pv_eoi_set_pending():
+ *   - arch/x86/kvm/lapic.c|3979| <<apic_sync_pv_eoi_to_guest>> pv_eoi_set_pending(apic->vcpu);
+ */
 static void pv_eoi_set_pending(struct kvm_vcpu *vcpu)
 {
 	if (pv_eoi_put_user(vcpu, KVM_PV_EOI_ENABLED) < 0)
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
 }
 
+/*
+ * 在以下使用pv_eoi_test_and_clr_pending():
+ *   - arch/x86/kvm/lapic.c|4012| <<apic_sync_pv_eoi_from_guest>> if (pv_eoi_test_and_clr_pending(vcpu))
+ */
 static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 {
 	u8 val;
@@ -893,6 +1556,19 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 	if (val && pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0)
 		return false;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	/*
 	 * Clear pending bit in any case: it will be set again on vmentry.
 	 * While this might not be ideal from performance point of view,
@@ -903,18 +1579,64 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * 在以下使用apic_has_interrupt_for_ppr():
+ *   - arch/x86/kvm/lapic.c|945| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+ *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+ *
+ * 选出最高的irr, 不修改任何数据
+ *
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ * 
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ */
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
+	/*
+	 * 在以下使用apic_find_highest_irr():
+	 *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	if (kvm_x86_ops.sync_pir_to_irr)
 		highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
 	else
 		highest_irr = apic_find_highest_irr(apic);
+	/*
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 0xFF = 255
+	 */
 	if (highest_irr == -1 || (highest_irr & 0xF0) <= ppr)
 		return -1;
 	return highest_irr;
 }
 
+/*
+ * 在以下使用__apic_update_ppr():
+ *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+ *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+ *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+ *
+ * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 根据TPR和ISR更新PPR
+ */
 static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 {
 	u32 tpr, isrv, ppr, old_ppr;
@@ -922,6 +1644,15 @@ static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 
 	old_ppr = kvm_lapic_get_reg(apic, APIC_PROCPRI);
 	tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI);
+	/*
+	 * 在以下使用apic_find_highest_isr():
+	 *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+	 */
 	isr = apic_find_highest_isr(apic);
 	isrv = (isr != -1) ? isr : 0;
 
@@ -937,24 +1668,141 @@ static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 	return ppr < old_ppr;
 }
 
+/*
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ * 
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ *
+ *
+ * 在以下使用apic_update_ppr():
+ *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+ *
+ * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 根据TPR和ISR更新PPR
+ * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+ * 如果有, 则设置KVM_REQ_EVENT
+ */
 static void apic_update_ppr(struct kvm_lapic *apic)
 {
 	u32 ppr;
 
+	/*
+	 * 在以下使用apic_has_interrupt_for_ppr():
+	 *   - arch/x86/kvm/lapic.c|945| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+	 *
+	 * 选出最高的irr, 不修改任何数据
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 *
+	 *
+	 * 在以下使用__apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+	 *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 */
 	if (__apic_update_ppr(apic, &ppr) &&
 	    apic_has_interrupt_for_ppr(apic, ppr) != -1)
 		kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 }
 
+/*
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ *
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ */
+
+/*
+ * 在以下使用kvm_apic_update_ppr():
+ *   - arch/x86/kvm/vmx/vmx.c|5546| <<handle_tpr_below_threshold>> kvm_apic_update_ppr(vcpu);
+ */
 void kvm_apic_update_ppr(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_ppr);
 
+/*
+ * 在以下使用apic_set_tpr():
+ *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+ *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+ *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+ */
 static void apic_set_tpr(struct kvm_lapic *apic, u32 tpr)
 {
 	kvm_lapic_set_reg(apic, APIC_TASKPRI, tpr);
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
 }
 
@@ -964,6 +1812,10 @@ static bool kvm_apic_broadcast(struct kvm_lapic *apic, u32 mda)
 			X2APIC_BROADCAST : APIC_BROADCAST);
 }
 
+/*
+ * 在以下使用kvm_apic_match_physical_addr():
+ *   - arch/x86/kvm/lapic.c|1813| <<kvm_apic_match_dest>> return kvm_apic_match_physical_addr(target, mda);
+ */
 static bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)
 {
 	if (kvm_apic_broadcast(apic, mda))
@@ -984,6 +1836,10 @@ static bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)
 	return mda == kvm_xapic_id(apic);
 }
 
+/*
+ * 在以下使用kvm_apic_match_logical_addr():
+ *   - arch/x86/kvm/lapic.c|1815| <<kvm_apic_match_dest>> return kvm_apic_match_logical_addr(target, mda);
+ */
 static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
 {
 	u32 logical_id;
@@ -993,6 +1849,9 @@ static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
 
 	logical_id = kvm_lapic_get_reg(apic, APIC_LDR);
 
+	/*
+	 * 如果是x2apic这里就退出了
+	 */
 	if (apic_x2apic_mode(apic))
 		return ((logical_id >> 16) == (mda >> 16))
 		       && (logical_id & mda & 0xffff) != 0;
@@ -1026,6 +1885,10 @@ static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
  * important when userspace wants to use x2APIC-format MSIs, because
  * APIC_BROADCAST (0xff) is a legal route for "cluster 0, CPUs 0-7".
  */
+/*
+ * 在以下使用kvm_apic_mda():
+ *   - arch/x86/kvm/lapic.c|1807| <<kvm_apic_match_dest>> u32 mda = kvm_apic_mda(vcpu, dest, source, target);
+ */
 static u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,
 		struct kvm_lapic *source, struct kvm_lapic *target)
 {
@@ -1038,6 +1901,24 @@ static u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,
 	return dest_id;
 }
 
+/*
+ * 在以下使用kvm_apic_match_dest():
+ *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+ *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id, 
+ *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+ *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+ *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+ *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+ *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+ */
 bool kvm_apic_match_dest(struct kvm_vcpu *vcpu, struct kvm_lapic *source,
 			   int shorthand, unsigned int dest, int dest_mode)
 {
@@ -1113,6 +1994,12 @@ static bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,
  * means that the interrupt should be dropped.  In this case, *bitmap would be
  * zero and *dst undefined.
  */
+/*
+ * 在以下使用kvm_apic_map_get_dest_lapic():
+ *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+ *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+ *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+ */
 static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 		struct kvm_lapic **src, struct kvm_lapic_irq *irq,
 		struct kvm_apic_map *map, struct kvm_lapic ***dst,
@@ -1138,14 +2025,28 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 			*dst = &map->phys_map[dest_id];
 			*bitmap = 1;
 		}
+		/*
+		 * 如果mode是APIC_DEST_PHYSICAL一定在这里退出
+		 */
 		return true;
 	}
 
 	*bitmap = 0;
+	/*
+	 * 在以下使用kvm_apic_map_get_logical_dest():
+	 *   - arch/x86/kvm/lapic.c|347| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
+	 *   - arch/x86/kvm/lapic.c|1785| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+	 */
 	if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
 				(u16 *)bitmap))
 		return false;
 
+	/*
+	 * 在以下使用kvm_lowest_prio_delivery():
+	 *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+	 */
 	if (!kvm_lowest_prio_delivery(irq))
 		return true;
 
@@ -1179,6 +2080,11 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * 在以下使用kvm_irq_delivery_to_apic_fast():
+ *   - arch/x86/kvm/irq.c|424| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq.c|554| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -1195,19 +2101,62 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 			*r = 0;
 			return true;
 		}
+		/*
+		 * 在以下使用kvm_apic_set_irq():
+		 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *
+		 * 调用__apic_accept_irq()
+		 */
 		*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
 		return true;
 	}
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 在以下使用kvm_apic_map_get_dest_lapic():
+	 *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+	 *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+	 *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+	 */
 	ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
 	if (ret) {
 		*r = 0;
 		for_each_set_bit(i, &bitmap, 16) {
 			if (!dst[i])
 				continue;
+			/*
+			 * 在以下使用kvm_apic_set_irq():
+			 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+			 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *
+			 * 调用__apic_accept_irq()
+			 */
 			*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
 		}
 	}
@@ -1230,6 +2179,10 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
  *	   interrupt.
  * - Otherwise, use remapped mode to inject the interrupt.
  */
+/*
+ * 在以下使用kvm_intr_is_single_vcpu_fast():
+ *   - arch/x86/kvm/irq.c|667| <<kvm_intr_is_single_vcpu>> if (kvm_intr_is_single_vcpu_fast(kvm, irq, dest_vcpu))
+ */
 bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			struct kvm_vcpu **dest_vcpu)
 {
@@ -1242,8 +2195,25 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
 		return false;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 在以下使用kvm_apic_map_get_dest_lapic():
+	 *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+	 *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+	 *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+	 */
 	if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
 			hweight16(bitmap) == 1) {
 		unsigned long i = find_first_bit(&bitmap, 16);
@@ -1297,6 +2267,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 				apic_clear_vector(vector, apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vt_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		kvm_x86_call(deliver_interrupt)(apic, delivery_mode,
 						trig_mode, vector);
 		break;
@@ -1333,6 +2307,14 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 
 	case APIC_DM_STARTUP:
 		result = 1;
+		/*
+		 * 在以下使用kvm_lapic->sipi_vector:
+		 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+		 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+		 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+		 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+		 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+		 */
 		apic->sipi_vector = vector;
 		/* make sure sipi_vector is visible for the receiver */
 		smp_wmb();
@@ -1363,6 +2345,11 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
  * out the destination vcpus array and set the bitmap or it traverses to
  * each available vcpu to identify the same.
  */
+/*
+ * 在以下使用kvm_bitmap_or_dest_vcpus:
+ *   - arch/x86/kvm/ioapic.c|458| <<ioapic_write_indirect>> kvm_bitmap_or_dest_vcpus(ioapic->kvm, &irq,
+ *   - arch/x86/kvm/ioapic.c|471| <<ioapic_write_indirect>> kvm_bitmap_or_dest_vcpus(ioapic->kvm, &irq,
+ */
 void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			      unsigned long *vcpu_bitmap)
 {
@@ -1375,8 +2362,25 @@ void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 	bool ret;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 在以下使用kvm_apic_map_get_dest_lapic():
+	 *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+	 *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+	 *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+	 */
 	ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
 					  &bitmap);
 	if (ret) {
@@ -1390,6 +2394,24 @@ void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			if (!kvm_apic_present(vcpu))
 				continue;
+			/*
+			 * 在以下使用kvm_apic_match_dest():
+			 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+			 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+			 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+			 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+			 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+			 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+			 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+			 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+			 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+			 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+			 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+			 */
 			if (!kvm_apic_match_dest(vcpu, NULL,
 						 irq->shorthand,
 						 irq->dest_id,
@@ -1406,15 +2428,47 @@ int kvm_apic_compare_prio(struct kvm_vcpu *vcpu1, struct kvm_vcpu *vcpu2)
 	return vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;
 }
 
+/*
+ * 在以下使用kvm_ioapic_handles_vector():
+ *   - arch/x86/kvm/lapic.c|1804| <<kvm_ioapic_send_eoi>> if (!kvm_ioapic_handles_vector(apic, vector))
+ *   - arch/x86/kvm/lapic.c|3988| <<apic_sync_pv_eoi_to_guest>> if (... kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+ */
 static bool kvm_ioapic_handles_vector(struct kvm_lapic *apic, int vector)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 */
 	return test_bit(vector, apic->vcpu->arch.ioapic_handled_vectors);
 }
 
+/*
+ * 在以下使用kvm_ioapic_send_eoi():
+ *   - arch/x86/kvm/lapic.c|1876| <<apic_set_eoi>> kvm_ioapic_send_eoi(apic, vector);
+ *   - arch/x86/kvm/lapic.c|1891| <<kvm_apic_set_eoi_accelerated>> kvm_ioapic_send_eoi(apic, vector);
+ */
 static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 {
 	int __maybe_unused trigger_mode;
 
+	/*
+	 * 在以下使用kvm_ioapic_handles_vector():
+	 *   - arch/x86/kvm/lapic.c|1804| <<kvm_ioapic_send_eoi>> if (!kvm_ioapic_handles_vector(apic, vector))
+	 *   - arch/x86/kvm/lapic.c|3988| <<apic_sync_pv_eoi_to_guest>> if (... kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	/* Eoi the ioapic only if the ioapic doesn't own the vector. */
 	if (!kvm_ioapic_handles_vector(apic, vector))
 		return;
@@ -1427,6 +2481,16 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 	if (apic->vcpu->arch.highest_stale_pending_ioapic_eoi == vector)
 		kvm_make_request(KVM_REQ_SCAN_IOAPIC, apic->vcpu);
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	/* Request a KVM exit to inform the userspace IOAPIC. */
 	if (irqchip_split(apic->vcpu->kvm)) {
 		apic->vcpu->arch.pending_ioapic_eoi = vector;
@@ -1444,8 +2508,22 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 #endif
 }
 
+/*
+ * 在以下使用apic_set_eoi():
+ *   - arch/x86/kvm/lapic.c|2960| <<kvm_lapic_reg_write(APIC_EOI)>> apic_set_eoi(apic);
+ *   - arch/x86/kvm/lapic.c|4447| <<apic_sync_pv_eoi_from_guest>> vector = apic_set_eoi(apic);
+ */
 static int apic_set_eoi(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用apic_find_highest_isr():
+	 *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+	 */
 	int vector = apic_find_highest_isr(apic);
 
 	trace_kvm_eoi(apic, vector);
@@ -1458,11 +2536,31 @@ static int apic_set_eoi(struct kvm_lapic *apic)
 		return vector;
 
 	apic_clear_isr(vector, apic);
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
 
 	if (kvm_hv_synic_has_vector(apic->vcpu, vector))
 		kvm_hv_synic_send_eoi(apic->vcpu, vector);
 
+	/*
+	 * 在以下使用kvm_ioapic_send_eoi():
+	 *   - arch/x86/kvm/lapic.c|1876| <<apic_set_eoi>> kvm_ioapic_send_eoi(apic, vector);
+	 *   - arch/x86/kvm/lapic.c|1891| <<kvm_apic_set_eoi_accelerated>> kvm_ioapic_send_eoi(apic, vector);
+	 */
 	kvm_ioapic_send_eoi(apic, vector);
 	kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 	return vector;
@@ -1472,12 +2570,21 @@ static int apic_set_eoi(struct kvm_lapic *apic)
  * this interface assumes a trap-like exit, which has already finished
  * desired side effect including vISR and vPPR update.
  */
+/*
+ * 在以下使用kvm_apic_set_eoi_accelerated():
+ *   - arch/x86/kvm/vmx/vmx.c|5632| <<handle_apic_eoi_induced>> kvm_apic_set_eoi_accelerated(vcpu, vector);
+ */
 void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	trace_kvm_eoi(apic, vector);
 
+	/*
+	 * 在以下使用kvm_ioapic_send_eoi():
+	 *   - arch/x86/kvm/lapic.c|1876| <<apic_set_eoi>> kvm_ioapic_send_eoi(apic, vector);
+	 *   - arch/x86/kvm/lapic.c|1891| <<kvm_apic_set_eoi_accelerated>> kvm_ioapic_send_eoi(apic, vector);
+	 */
 	kvm_ioapic_send_eoi(apic, vector);
 	kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 }
@@ -1504,6 +2611,16 @@ void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 
 	trace_kvm_apic_ipi(icr_low, irq.dest_id);
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_send_ipi);
@@ -1564,6 +2681,21 @@ static u32 __apic_read(struct kvm_lapic *apic, unsigned int offset)
 		val = apic_get_tmcct(apic);
 		break;
 	case APIC_PROCPRI:
+		/*
+		 * 在以下使用apic_update_ppr():
+		 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+		 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+		 *
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 根据TPR和ISR更新PPR
+		 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+		 * 如果有, 则设置KVM_REQ_EVENT
+		 */
 		apic_update_ppr(apic);
 		val = kvm_lapic_get_reg(apic, offset);
 		break;
@@ -1762,6 +2894,18 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u32 reg;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	/*
 	 * Assume a timer IRQ was "injected" if the APIC is protected.  KVM's
 	 * copy of the vIRR is bogus, it's the responsibility of the caller to
@@ -1775,6 +2919,12 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 		int vec = reg & APIC_VECTOR_MASK;
 		void *bitmap = apic->regs + APIC_ISR;
 
+		/*
+		 * 在以下修改kvm_lapic->apicv_active:
+		 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+		 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+		 */
 		if (apic->apicv_active)
 			bitmap = apic->regs + APIC_IRR;
 
@@ -1786,6 +2936,25 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 
 static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
 
 	/*
@@ -1808,6 +2977,25 @@ static inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,
 					      s64 advance_expire_delta)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
 	u64 ns;
 
@@ -1858,6 +3046,25 @@ static void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 
 void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	if (lapic_in_kernel(vcpu) &&
 	    vcpu->arch.apic->lapic_timer.expired_tscdeadline &&
 	    vcpu->arch.apic->lapic_timer.timer_advance_ns &&
@@ -1870,6 +3077,13 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
 
+	/*
+	 * 在以下使用kvm_apic_local_deliver():
+	 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+	 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+	 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+	 */
 	kvm_apic_local_deliver(apic, APIC_LVTT);
 	if (apic_lvtt_tscdeadline(apic)) {
 		ktimer->tscdeadline = 0;
@@ -1890,6 +3104,12 @@ static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 	if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
 		ktimer->expired_tscdeadline = ktimer->tscdeadline;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (!from_timer_fn && apic->apicv_active) {
 		WARN_ON(kvm_get_running_vcpu() != vcpu);
 		kvm_apic_inject_pending_timer_irqs(apic);
@@ -1897,6 +3117,25 @@ static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 	}
 
 	if (kvm_use_posted_timer_interrupt(apic->vcpu)) {
+		/*
+		 * 在以下使用kvm_timer->timer_advance_ns:
+		 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+		 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+		 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+		 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+		 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+		 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+		 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+		 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+		 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+		 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+		 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+		 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+		 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+		 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+		 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+		 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+		 */
 		/*
 		 * Ensure the guest's timer has truly expired before posting an
 		 * interrupt.  Open code the relevant checks to avoid querying
@@ -1939,6 +3178,25 @@ static void start_sw_tscdeadline(struct kvm_lapic *apic)
 	ns = (tscdeadline - guest_tsc) * 1000000ULL;
 	do_div(ns, this_tsc_khz);
 
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	if (likely(tscdeadline > guest_tsc) &&
 	    likely(ns > apic->lapic_timer.timer_advance_ns)) {
 		expire = ktime_add_ns(now, ns);
@@ -2252,6 +3510,13 @@ static int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 	switch (reg) {
 	case APIC_ID:		/* Local APIC ID */
 		if (!apic_x2apic_mode(apic)) {
+			/*
+			 * 在以下使用kvm_apic_set_xapic_id():
+			 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+			 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 */
 			kvm_apic_set_xapic_id(apic, val >> 24);
 		} else {
 			ret = 1;
@@ -2260,6 +3525,12 @@ static int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 
 	case APIC_TASKPRI:
 		report_tpr_access(apic, true);
+		/*
+		 * 在以下使用apic_set_tpr():
+		 *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+		 *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+		 *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+		 */
 		apic_set_tpr(apic, val & 0xff);
 		break;
 
@@ -2384,6 +3655,14 @@ static int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 		break;
 	}
 
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	/*
 	 * Recalculate APIC maps if necessary, e.g. if the software enable bit
 	 * was toggled, the APIC ID changed, etc...   The maps are marked dirty
@@ -2498,15 +3777,52 @@ void kvm_free_lapic(struct kvm_vcpu *vcpu)
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	if (!vcpu->arch.apic) {
+		/*
+		 * 在以下使用kvm_has_noapic_vcpu:
+		 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+		 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+		 */
 		static_branch_dec(&kvm_has_noapic_vcpu);
 		return;
 	}
 
 	hrtimer_cancel(&apic->lapic_timer.timer);
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
 		static_branch_slow_dec_deferred(&apic_hw_disabled);
 
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (!apic->sw_enabled)
 		static_branch_slow_dec_deferred(&apic_sw_disabled);
 
@@ -2545,6 +3861,12 @@ void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)
 
 void kvm_lapic_set_tpr(struct kvm_vcpu *vcpu, unsigned long cr8)
 {
+	/*
+	 * 在以下使用apic_set_tpr():
+	 *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+	 *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+	 *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+	 */
 	apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
 }
 
@@ -2557,8 +3879,34 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * 在以下使用__kvm_apic_set_base():
+ *   - arch/x86/kvm/lapic.c|3286| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+ *   - arch/x86/kvm/lapic.c|3516| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+ */
 static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	u64 old_value = vcpu->arch.apic_base;
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
@@ -2573,17 +3921,61 @@ static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	/* update jump label if enable bit changes */
 	if ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE) {
 		if (value & MSR_IA32_APICBASE_ENABLE) {
+			/*
+			 * 在以下使用kvm_apic_set_xapic_id():
+			 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+			 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 */
 			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
 			static_branch_slow_dec_deferred(&apic_hw_disabled);
 			/* Check if there are APF page ready requests pending */
 			kvm_make_request(KVM_REQ_APF_READY, vcpu);
 		} else {
 			static_branch_inc(&apic_hw_disabled.key);
+			/*
+			 * 在以下使用kvm_arch->apic_map_dirty:
+			 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+			 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+			 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+			 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 */
 			atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 		}
 	}
 
+	/*
+	 * 这个patch引入的代码.
+	 *
+	 * KVM: x86: Reinitialize xAPIC ID when userspace forces x2APIC => xAPIC
+	 * https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=052c3b99cbc8d227f8cb8edf1519197808d1d653
+	 *
+	 * -	if (((old_value ^ value) & X2APIC_ENABLE) && (value & X2APIC_ENABLE))
+	 * -		kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+	 * +	if ((old_value ^ value) & X2APIC_ENABLE) {
+	 * +		if (value & X2APIC_ENABLE)
+	 * +			kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+	 * +		else if (value & MSR_IA32_APICBASE_ENABLE)
+	 * +			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 * +	}
+	 *
+	 * 在以下使用kvm_apic_set_xapic_id():
+	 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+	 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 */
 	if ((old_value ^ value) & X2APIC_ENABLE) {
+		/*
+		 * 只在这一个地方调用kvm_apic_set_x2apic_id()
+		 */
 		if (value & X2APIC_ENABLE)
 			kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
 		else if (value & MSR_IA32_APICBASE_ENABLE)
@@ -2591,25 +3983,106 @@ static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	}
 
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE)) {
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+		 *
+		 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+		 */
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		/*
+		 * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+		 *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+		 *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+		 *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+		 *
+		 * vt_set_virtual_apic_mode()
+		 * vmx_set_virtual_apic_mode()
+		 */
 		kvm_x86_call(set_virtual_apic_mode)(vcpu);
 	}
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	apic->base_address = apic->vcpu->arch.apic_base &
 			     MSR_IA32_APICBASE_BASE;
 
 	if ((value & MSR_IA32_APICBASE_ENABLE) &&
 	     apic->base_address != APIC_DEFAULT_PHYS_BASE) {
+		/*
+		 * 在以下使用kvm_set_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+		 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+		 *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+		 *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 */
 		kvm_set_apicv_inhibit(apic->vcpu->kvm,
 				      APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_set_base():
+ *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+ *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+ *                         msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+ */
 int kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value, bool host_initiated)
 {
 	enum lapic_mode old_mode = kvm_get_apic_mode(vcpu);
 	enum lapic_mode new_mode = kvm_apic_mode(value);
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (vcpu->arch.apic_base == value)
 		return 0;
 
@@ -2625,12 +4098,36 @@ int kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value, bool host_initiated)
 			return 1;
 	}
 
+	/*
+	 * 在以下使用__kvm_apic_set_base():
+	 *   - arch/x86/kvm/lapic.c|3286| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+	 *   - arch/x86/kvm/lapic.c|3516| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+	 */
 	__kvm_apic_set_base(vcpu, value);
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	kvm_recalculate_apic_map(vcpu->kvm);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_base);
 
+/*
+ * 在以下使用kvm_apic_update_apicv():
+ *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+ *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+ *
+ * 就是修改三个field:
+ * 1. apic->irr_pending
+ * 2. apic->isr_count
+ * 3. apic->highest_isr_cache
+ */
 void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2648,26 +4145,90 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 	 *        state prior to updating KVM's metadata caches, so that KVM
 	 *        can safely search the IRR and set irr_pending accordingly.
 	 */
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	apic->irr_pending = true;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用kvm_lapic->isr_count:
+	 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+	 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+	 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+	 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+	 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+	 */
 	if (apic->apicv_active)
 		apic->isr_count = 1;
 	else
 		apic->isr_count = count_vectors(apic->regs + APIC_ISR);
 
+	/*
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	apic->highest_isr_cache = -1;
 }
 
+/*
+ * 在以下使用kvm_alloc_apic_access_page():
+ *   - arch/x86/kvm/svm/avic.c|287| <<avic_init_backing_page>> ret = kvm_alloc_apic_access_page(vcpu->kvm);
+ *   - arch/x86/kvm/vmx/vmx.c|7480| <<vmx_vcpu_create>> err = kvm_alloc_apic_access_page(vcpu->kvm);
+ */
 int kvm_alloc_apic_access_page(struct kvm *kvm)
 {
 	void __user *hva;
 	int ret = 0;
 
 	mutex_lock(&kvm->slots_lock);
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 *
+	 * 在以下使用kvm_arch->apic_access_memslot_inhibited:
+	 *   - arch/x86/kvm/lapic.c|2668| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *                    kvm->arch.apic_access_memslot_inhibited)
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_inhibited = true;
+	 */
 	if (kvm->arch.apic_access_memslot_enabled ||
 	    kvm->arch.apic_access_memslot_inhibited)
 		goto out;
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
 				      APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);
 	if (IS_ERR(hva)) {
@@ -2682,10 +4243,22 @@ int kvm_alloc_apic_access_page(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_alloc_apic_access_page);
 
+/*
+ * 在以下使用kvm_inhibit_apic_access_page():
+ *   - arch/x86/kvm/x86.c|10561| <<kvm_vcpu_update_apicv>> kvm_inhibit_apic_access_page(vcpu);
+ */
 void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
 
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 */
 	if (!kvm->arch.apic_access_memslot_enabled)
 		return;
 
@@ -2693,7 +4266,24 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 
 	mutex_lock(&kvm->slots_lock);
 
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 */
 	if (kvm->arch.apic_access_memslot_enabled) {
+		/*
+		 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+		 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+		 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+		 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+		 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+		 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 */
 		__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
 		/*
 		 * Clear "enabled" after the memslot is deleted so that a
@@ -2705,6 +4295,12 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 		 */
 		kvm->arch.apic_access_memslot_enabled = false;
 
+		/*
+		 * 在以下使用kvm_arch->apic_access_memslot_inhibited:
+		 *   - arch/x86/kvm/lapic.c|2668| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+		 *                    kvm->arch.apic_access_memslot_inhibited)
+		 *   - arch/x86/kvm/lapic.c|2712| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_inhibited = true;
+		 */
 		/*
 		 * Mark the memslot as inhibited to prevent reallocating the
 		 * memslot during vCPU creation, e.g. if a vCPU is hotplugged.
@@ -2717,12 +4313,22 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 	kvm_vcpu_srcu_read_lock(vcpu);
 }
 
+/*
+ * 在以下使用kvm_lapic_reset():
+ *   - arch/x86/kvm/x86.c|13265| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u64 msr_val;
 	int i;
 
+	/*
+	 * 只有pi_apicv_pre_state_restore
+	 * 在以下调用:
+	 *   - arch/x86/kvm/lapic.c|3262| <<kvm_lapic_reset>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3731| <<kvm_apic_set_state>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 */
 	kvm_x86_call(apicv_pre_state_restore)(vcpu);
 
 	if (!init_event) {
@@ -2736,6 +4342,11 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 		 * The recalculation needed for this vCPU will be done after
 		 * all APIC state has been initialized (see below).
 		 */
+		/*
+		 * 在以下使用__kvm_apic_set_base():
+		 *   - arch/x86/kvm/lapic.c|3286| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+		 *   - arch/x86/kvm/lapic.c|3516| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+		 */
 		__kvm_apic_set_base(vcpu, msr_val);
 	}
 
@@ -2745,6 +4356,13 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 	/* Stop the timer in case it's a reset to an active apic */
 	hrtimer_cancel(&apic->lapic_timer.timer);
 
+	/*
+	 * 在以下使用kvm_apic_set_xapic_id():
+	 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+	 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 */
 	/* The xAPIC ID is set at RESET even if the APIC was already enabled. */
 	if (!init_event)
 		kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
@@ -2778,20 +4396,84 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 		kvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);
 		kvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);
 	}
+	/*
+	 * 在以下使用kvm_apic_update_apicv():
+	 *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+	 *
+	 * 就是修改三个field:
+	 * 1. apic->irr_pending
+	 * 2. apic->isr_count
+	 * 3. apic->highest_isr_cache
+	 */
 	kvm_apic_update_apicv(vcpu);
 	update_divide_count(apic);
 	atomic_set(&apic->lapic_timer.pending, 0);
 
 	vcpu->arch.pv_eoi.msr_val = 0;
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active) {
 		kvm_x86_call(apicv_post_state_restore)(vcpu);
+		/*
+		 * 在以下使用vt_hwapic_isr_update():
+		 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+		 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+		 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+		 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *
+		 * vmx_hwapic_isr_update()
+		 */
 		kvm_x86_call(hwapic_isr_update)(vcpu, -1);
 	}
 
 	vcpu->arch.apic_arb_prio = 0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	vcpu->arch.apic_attention = 0;
 
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	kvm_recalculate_apic_map(vcpu->kvm);
 }
 
@@ -2816,6 +4498,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_apic_local_deliver():
+ *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2840,6 +4529,13 @@ void kvm_apic_nmi_wd_deliver(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 在以下使用kvm_apic_local_deliver():
+	 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+	 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+	 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+	 */
 	if (apic)
 		kvm_apic_local_deliver(apic, APIC_LVT0);
 }
@@ -2864,6 +4560,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * 在以下使用kvm_create_lapic():
+ *   - arch/x86/kvm/x86.c|13239| <<kvm_arch_vcpu_create>> r = kvm_create_lapic(vcpu);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic;
@@ -2871,6 +4571,15 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	ASSERT(vcpu != NULL);
 
 	if (!irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用kvm_has_noapic_vcpu:
+		 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+		 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+		 */
 		static_branch_inc(&kvm_has_noapic_vcpu);
 		return 0;
 	}
@@ -2881,6 +4590,12 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.apic = apic;
 
+	/*
+	 * 在以下使用kvm_x86_ops->alloc_apic_backing_page:
+	 *   - arch/x86/kvm/svm/svm.c|5481| <<global>> .alloc_apic_backing_page = svm_alloc_apic_backing_page,
+	 *   - arch/x86/kvm/lapic.c|4386| <<kvm_create_lapic>> if (kvm_x86_ops.alloc_apic_backing_page)
+	 *   - arch/x86/kvm/lapic.c|4387| <<kvm_create_lapic>> apic->regs = kvm_x86_call(alloc_apic_backing_page)(vcpu);
+	 */
 	if (kvm_x86_ops.alloc_apic_backing_page)
 		apic->regs = kvm_x86_call(alloc_apic_backing_page)(vcpu);
 	else
@@ -2894,11 +4609,51 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 
 	apic->nr_lvt_entries = kvm_apic_calc_nr_lvt_entries(vcpu);
 
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	hrtimer_setup(&apic->lapic_timer.timer, apic_timer_fn, CLOCK_MONOTONIC,
 		      HRTIMER_MODE_ABS_HARD);
 	if (lapic_timer_advance)
 		apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	/*
 	 * Stuff the APIC ENABLE bit in lieu of temporarily incrementing
 	 * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().
@@ -2918,7 +4673,25 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	 * the request will ensure the vCPU gets the correct state before VM-Entry.
 	 */
 	if (enable_apicv) {
+		/*
+		 * 在以下修改kvm_lapic->apicv_active:
+		 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+		 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+		 */
 		apic->apicv_active = true;
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+		 *
+		 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+		 */
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
 
@@ -2930,6 +4703,20 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	return -ENOMEM;
 }
 
+/*
+ * 在以下使用kvm_apic_has_interrupt():
+ *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+ *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+ *
+ * 核心思想:
+ * 1. 判断kvm_apic_present()
+ * 2. 根据TPR和ISR更新PPR
+ * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+ */
 int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2938,10 +4725,41 @@ int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 	if (!kvm_apic_present(vcpu))
 		return -1;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (apic->guest_apic_protected)
 		return -1;
 
+	/*
+	 * 在以下使用__apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+	 *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 */
 	__apic_update_ppr(apic, &ppr);
+	/*
+	 * 在以下使用apic_has_interrupt_for_ppr():
+	 *   - arch/x86/kvm/lapic.c|945| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+	 *
+	 * 选出最高的irr, 不修改任何数据
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 */
 	return apic_has_interrupt_for_ppr(apic, ppr);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_has_interrupt);
@@ -2968,6 +4786,11 @@ void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_ack_interrupt():
+ *   - arch/x86/kvm/irq.c|187| <<kvm_cpu_get_interrupt>> kvm_apic_ack_interrupt(v, vector);
+ *   - arch/x86/kvm/vmx/nested.c|4413| <<vmx_check_nested_events>> kvm_apic_ack_interrupt(vcpu, irq);
+ */
 void kvm_apic_ack_interrupt(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2983,8 +4806,28 @@ void kvm_apic_ack_interrupt(struct kvm_vcpu *vcpu, int vector)
 	 * because the process would deliver it through the IDT.
 	 */
 
+	/*
+	 * 在以下使用apic_clear_irr():
+	 *   - arch/x86/kvm/lapic.c|722| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|3273| <<kvm_apic_ack_interrupt>> apic_clear_irr(vector, apic);
+	 */
 	apic_clear_irr(vector, apic);
 	if (kvm_hv_synic_auto_eoi_set(vcpu, vector)) {
+		/*
+		 * 在以下使用apic_update_ppr():
+		 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+		 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+		 *
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 根据TPR和ISR更新PPR
+		 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+		 * 如果有, 则设置KVM_REQ_EVENT
+		 */
 		/*
 		 * For auto-EOI interrupts, there might be another pending
 		 * interrupt above PPR, so check whether to raise another
@@ -2998,7 +4841,19 @@ void kvm_apic_ack_interrupt(struct kvm_vcpu *vcpu, int vector)
 		 * a concurrent interrupt injection, but that would have
 		 * triggered KVM_REQ_EVENT already.
 		 */
+		/*
+		 * 只在这里调用
+		 */
 		apic_set_isr(vector, apic);
+		/*
+		 * 在以下使用__apic_update_ppr():
+		 *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+		 *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+		 *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+		 *
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 根据TPR和ISR更新PPR
+		 */
 		__apic_update_ppr(apic, &ppr);
 	}
 
@@ -3014,6 +4869,15 @@ static int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,
 		u32 *ldr = (u32 *)(s->regs + APIC_LDR);
 		u64 icr;
 
+		/*
+		 * 在以下使用kvm_arch->x2apic_format:
+		 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+		 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+		 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+		 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+		 */
 		if (vcpu->kvm->arch.x2apic_format) {
 			if (*id != x2apic_id)
 				return -EINVAL;
@@ -3057,6 +4921,15 @@ static int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_apic_get_state():
+ *   - arch/x86/kvm/x86.c|5203| <<kvm_vcpu_ioctl_get_lapic>> return kvm_apic_get_state(vcpu, s);
+ *
+ * 122 #define KVM_APIC_REG_SIZE 0x400
+ * 123 struct kvm_lapic_state {
+ * 124         char regs[KVM_APIC_REG_SIZE];
+ * 125 };
+ */
 int kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 {
 	memcpy(s->regs, vcpu->arch.apic->regs, sizeof(*s));
@@ -3070,11 +4943,28 @@ int kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	return kvm_apic_state_fixup(vcpu, s, false);
 }
 
+/*
+ * 在以下使用kvm_apic_set_state():
+ *   - arch/x86/kvm/x86.c|5214| <<kvm_vcpu_ioctl_set_lapic>> r = kvm_apic_set_state(vcpu, s);
+ *
+ * 122 #define KVM_APIC_REG_SIZE 0x400
+ * 123 struct kvm_lapic_state {
+ * 124         char regs[KVM_APIC_REG_SIZE];
+ * 125 };
+ */
 int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	int r;
 
+	/*
+	 * 只有pi_apicv_pre_state_restore
+	 * 在以下调用:
+	 *   - arch/x86/kvm/lapic.c|3262| <<kvm_lapic_reset>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3731| <<kvm_apic_set_state>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 *
+	 * 似乎是清零一些intel posted interrupt的数据
+	 */
 	kvm_x86_call(apicv_pre_state_restore)(vcpu);
 
 	/* set SPIV separately to get count of SW disabled APICs right */
@@ -3082,15 +4972,59 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 
 	r = kvm_apic_state_fixup(vcpu, s, true);
 	if (r) {
+		/*
+		 * 在以下使用kvm_recalculate_apic_map():
+		 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+		 */
 		kvm_recalculate_apic_map(vcpu->kvm);
 		return r;
 	}
 	memcpy(vcpu->arch.apic->regs, s->regs, sizeof(*s));
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	kvm_recalculate_apic_map(vcpu->kvm);
 	kvm_apic_set_version(vcpu);
 
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
 	cancel_apic_timer(apic);
 	apic->lapic_timer.expired_tscdeadline = 0;
@@ -3099,11 +5033,50 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	update_divide_count(apic);
 	__start_apic_timer(apic, APIC_TMCCT);
 	kvm_lapic_set_reg(apic, APIC_TMCCT, 0);
+	/*
+	 * 在以下使用kvm_apic_update_apicv():
+	 *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+	 *
+	 * 就是修改三个field:
+	 * 1. apic->irr_pending
+	 * 2. apic->isr_count
+	 * 3. apic->highest_isr_cache
+	 */
 	kvm_apic_update_apicv(vcpu);
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active) {
 		kvm_x86_call(apicv_post_state_restore)(vcpu);
+		/*
+		 * 在以下使用apic_find_highest_isr():
+		 *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+		 *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+		 *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+		 *
+		 * 在以下使用vt_hwapic_isr_update():
+		 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+		 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+		 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+		 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *
+		 * vmx_hwapic_isr_update()
+		 */
 		kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
 	}
+	/*
+	 * 这里是unconditionally!!
+	 */
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 
 #ifdef CONFIG_KVM_IOAPIC
@@ -3136,6 +5109,10 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
  * last entry. If yes, set EOI on guests's behalf.
  * Clear PV EOI in guest memory in any case.
  */
+/*
+ * 在以下使用apic_sync_pv_eoi_from_guest():
+ *   - arch/x86/kvm/lapic.c|4045| <<kvm_lapic_sync_from_vapic>> apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
+ */
 static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
@@ -3153,26 +5130,73 @@ static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 	 */
 	BUG_ON(!pv_eoi_enabled(vcpu));
 
+	/*
+	 * 只在这里调用
+	 */
 	if (pv_eoi_test_and_clr_pending(vcpu))
 		return;
 	vector = apic_set_eoi(apic);
 	trace_kvm_pv_eoi(apic, vector);
 }
 
+/*
+ * 在以下使用kvm_lapic_sync_from_vapic():
+ *   - arch/x86/kvm/x86.c|11450| <<vcpu_enter_guest>> kvm_lapic_sync_from_vapic(vcpu);
+ *   - arch/x86/kvm/x86.c|11463| <<vcpu_enter_guest>> kvm_lapic_sync_from_vapic(vcpu);
+ *
+ * KVM 暴露一个"PV EOI page"给 Guest
+ * Guest写入EOI时更新对应位
+ * Host 基于该位确认中断优先级恢复状态
+ */
 void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
 {
 	u32 data;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
 		apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
 
 	if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 		return;
 
+	/*
+	 *  在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+	 */
 	if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				  sizeof(u32)))
 		return;
 
+	/*
+	 * 在以下使用apic_set_tpr():
+	 *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+	 *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+	 *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+	 *
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 *
+	 * 关于TPR:
+	 * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+	 * CPU.低于或等于门槛的中断会被屏蔽或延后.
+	 * 举个简单数字例子:
+	 * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+	 */
 	apic_set_tpr(vcpu->arch.apic, data & 0xff);
 }
 
@@ -3182,9 +5206,40 @@ void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
  * Detect whether it's safe to enable PV EOI and
  * if yes do so.
  */
+/*
+ * 在以下使用apic_sync_pv_eoi_to_guest():
+ *   - arch/x86/kvm/lapic.c|3988| <<kvm_lapic_sync_to_vapic>> apic_sync_pv_eoi_to_guest(vcpu, apic);
+ */
 static void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 *
+	 *
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 *
+	 *
+	 * 在以下使用kvm_ioapic_handles_vector():
+	 *   - arch/x86/kvm/lapic.c|1804| <<kvm_ioapic_send_eoi>> if (!kvm_ioapic_handles_vector(apic, vector))
+	 *   - arch/x86/kvm/lapic.c|3988| <<apic_sync_pv_eoi_to_guest>> if (... kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	if (!pv_eoi_enabled(vcpu) ||
 	    /* IRR set or many bits in ISR: could be nested. */
 	    apic->irr_pending ||
@@ -3202,42 +5257,123 @@ static void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,
 	pv_eoi_set_pending(apic->vcpu);
 }
 
+/*
+ * 在以下使用kvm_lapic_sync_to_vapic():
+ *   - arch/x86/kvm/x86.c|11208| <<vcpu_enter_guest>> kvm_lapic_sync_to_vapic(vcpu);
+ */
 void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 {
 	u32 data, tpr;
 	int max_irr, max_isr;
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 只在这里调用:
+	 * Detect whether it's safe to enable PV EOI and
+	 * if yes do so.
+	 */
 	apic_sync_pv_eoi_to_guest(vcpu, apic);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 		return;
 
+	/*
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 *
+	 * 关于TPR:
+	 * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+	 * CPU.低于或等于门槛的中断会被屏蔽或延后.
+	 * 举个简单数字例子:
+	 * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+	 */
 	tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;
+	/*
+	 * 在以下使用apic_find_highest_irr():
+	 *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	max_irr = apic_find_highest_irr(apic);
 	if (max_irr < 0)
 		max_irr = 0;
+	/*
+	 * 在以下使用apic_find_highest_isr():
+	 *   - arch/x86/kvm/lapic.c|1269| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1326| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1602| <<__apic_update_ppr>> isr = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|2464| <<apic_set_eoi>> int vector = apic_find_highest_isr(apic);
+	 *   - arch/x86/kvm/lapic.c|4954| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|5193| <<kvm_lapic_sync_to_vapic>> max_isr = apic_find_highest_isr(apic);
+	 */
 	max_isr = apic_find_highest_isr(apic);
 	if (max_isr < 0)
 		max_isr = 0;
 	data = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);
 
+	/* 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+	 */
 	kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				sizeof(u32));
 }
 
+/*
+ * 处理KVM_SET_VAPIC_ADDR:
+ *   - arch/x86/kvm/x86.c|6127| <<kvm_arch_vcpu_ioctl>> r = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);
+ */
 int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
 {
 	if (vapic_addr) {
+		/*
+		 *  在以下使用kvm_lapic->vapic_cache:
+		 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+		 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+		 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+		 */
 		if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
 					&vcpu->arch.apic->vapic_cache,
 					vapic_addr, sizeof(u32)))
 			return -EINVAL;
+		/*
+		 * 在以下使用kvm_vcpu_arch->apic_attention:
+		 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+		 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+		 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+		 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+		 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+		 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+		 */
 		__set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
 	} else {
 		__clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
 	}
 
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3955| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|10328| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	vcpu->arch.apic->vapic_addr = vapic_addr;
 	return 0;
 }
@@ -3314,6 +5450,13 @@ int kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)
 	return kvm_lapic_msr_read(vcpu->arch.apic, reg, data);
 }
 
+/*
+ * 在以下使用kvm_lapic_set_pv_eoi():
+ *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+ *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+ *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+ *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+ */
 int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 {
 	u64 addr = data & ~KVM_MSR_ENABLED;
@@ -3340,6 +5483,15 @@ int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_apic_accept_events():
+ *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+ *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+ *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+ *
+ * 主要针对INIT和SIPI
+ */
 int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -3350,6 +5502,12 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		return 0;
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用kvm_check_nested_events():
+		 *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+		 */
 		r = kvm_check_nested_events(vcpu);
 		if (r < 0)
 			return r == -EBUSY ? 0 : r;
@@ -3372,6 +5530,13 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 	}
 
 	if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+		/*
+		 * 在以下使用kvm_vcpu_reset():
+		 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+		 */
 		kvm_vcpu_reset(vcpu, true);
 		if (kvm_vcpu_is_bsp(apic->vcpu))
 			kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
@@ -3382,6 +5547,14 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		if (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {
 			/* evaluate pending_events before reading the vector */
 			smp_rmb();
+			/*
+			 * 在以下使用kvm_lapic->sipi_vector:
+			 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+			 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+			 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+			 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+			 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+			 */
 			sipi_vector = apic->sipi_vector;
 			kvm_x86_call(vcpu_deliver_sipi_vector)(vcpu,
 							       sipi_vector);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 72de14527..229f41b41 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -54,6 +54,25 @@ struct kvm_timer {
 	u32 timer_mode_mask;
 	u64 tscdeadline;
 	u64 expired_tscdeadline;
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns); 
+	 */
 	u32 timer_advance_ns;
 	atomic_t pending;			/* accumulated triggered timers */
 	bool hv_timer_in_use;
@@ -65,14 +84,71 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	bool apicv_active;
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	bool sw_enabled;
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	/* Select registers in the vAPIC cannot be read/written. */
 	bool guest_apic_protected;
+	/*
+	 * 在以下使用kvm_lapic->isr_count:
+	 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+	 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+	 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+	 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+	 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+	 */
 	/* Number of bits set in ISR. */
 	s16 isr_count;
+	/*
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	/* The highest vector set in ISR; if -1 - invalid, must scan ISR. */
 	int highest_isr_cache;
 	/**
@@ -81,9 +157,46 @@ struct kvm_lapic {
 	 * Note: Only one register, the TPR, is used by the microcode.
 	 */
 	void *regs;
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3955| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|10328| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	gpa_t vapic_addr;
+	/*
+	 * 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+	 */
 	struct gfn_to_hva_cache vapic_cache;
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1688| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1699| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|4093| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|4097| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.c|4104| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.h|272| <<kvm_apic_has_pending_init_or_sipi>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|289| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1552| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4242| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4246| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|4256| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4260| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|5654| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|5656| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|12255| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	unsigned long pending_events;
+	/*
+	 * 在以下使用kvm_lapic->sipi_vector:
+	 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+	 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+	 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+	 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+	 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+	 */
 	unsigned int sipi_vector;
 	int nr_lvt_entries;
 };
@@ -149,9 +262,26 @@ void kvm_lapic_exit(void);
 
 u64 kvm_lapic_readable_reg_mask(struct kvm_lapic *apic);
 
+/*
+ * 在以下使用kvm_lapic_set_irr():
+ *   - arch/x86/kvm/svm/svm.c|3844| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	apic_set_vector(vec, apic->regs + APIC_IRR);
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	/*
 	 * irr_pending must be true if any interrupt is pending; set it after
 	 * APIC_IRR to avoid race with apic_clear_irr
@@ -164,10 +294,28 @@ static inline u32 kvm_lapic_get_reg(struct kvm_lapic *apic, int reg_off)
 	return apic_get_reg(apic->regs, reg_off);
 }
 
+/*
+ * 在以下使用kvm_has_noapic_vcpu:
+ *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+ *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+ */
 DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 
 static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_has_noapic_vcpu:
+	 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+	 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+	 */
 	if (static_branch_unlikely(&kvm_has_noapic_vcpu))
 		return vcpu->arch.apic;
 	return true;
@@ -177,6 +325,27 @@ extern struct static_key_false_deferred apic_hw_disabled;
 
 static inline bool kvm_apic_hw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (static_branch_unlikely(&apic_hw_disabled.key))
 		return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
 	return true;
@@ -186,6 +355,13 @@ extern struct static_key_false_deferred apic_sw_disabled;
 
 static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (static_branch_unlikely(&apic_sw_disabled.key))
 		return apic->sw_enabled;
 	return true;
@@ -203,11 +379,63 @@ static inline int kvm_lapic_enabled(struct kvm_vcpu *vcpu)
 
 static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
 }
 
+/*
+ * 在以下使用kvm_vcpu_apicv_active():
+ *   - arch/x86/kvm/irq.c|178| <<kvm_cpu_has_injectable_intr>> if (!is_guest_mode(v) && kvm_vcpu_apicv_active(v))
+ *   - arch/x86/kvm/lapic.c|160| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu) &&
+ *   - arch/x86/kvm/svm/avic.c|788| <<avic_pi_update_irte>> .is_guest_mode = kvm_vcpu_apicv_active(vcpu),
+ *   - arch/x86/kvm/svm/avic.c|1050| <<avic_refresh_virtual_apic_mode>> if (kvm_vcpu_apicv_active(vcpu)) {
+ *   - arch/x86/kvm/svm/avic.c|1082| <<avic_refresh_apicv_exec_ctrl>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/avic.c|1090| <<avic_vcpu_blocking>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/avic.c|1116| <<avic_vcpu_unblocking>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/nested.c|932| <<enter_svm_guest_mode>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1099| <<init_vmcb>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1208| <<init_vmcb>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1453| <<svm_vcpu_load>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1459| <<svm_vcpu_put>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/main.c|742| <<vt_refresh_apicv_exec_ctrl>> KVM_BUG_ON(!kvm_vcpu_apicv_active(vcpu), vcpu->kvm);
+ *   - arch/x86/kvm/vmx/vmx.c|4058| <<vmx_update_msr_bitmap_x2apic>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_pin_based_exec_ctrl>> if (!kvm_vcpu_apicv_active(&vmx->vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4428| <<vmx_refresh_apicv_exec_ctrl>> if (kvm_vcpu_apicv_active(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4497| <<vmx_tertiary_exec_control>> if (!enable_ipiv || !kvm_vcpu_apicv_active(&vmx->vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4588| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7064| <<vmx_sync_pir_to_irr>> if (!is_guest_mode(vcpu) && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7074| <<vmx_load_eoi_exitmap>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/x86.c|11530| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+ *   - arch/x86/kvm/x86.c|11991| <<kvm_arch_dy_has_pending_interrupt>> return kvm_vcpu_apicv_active(vcpu) &&
+ */
 static inline bool kvm_vcpu_apicv_active(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	return lapic_in_kernel(vcpu) && vcpu->arch.apic->apicv_active;
 }
 
@@ -222,6 +450,12 @@ static inline bool kvm_apic_init_sipi_allowed(struct kvm_vcpu *vcpu)
 	       !kvm_x86_call(apic_init_signal_blocked)(vcpu);
 }
 
+/*
+ * 在以下使用kvm_lowest_prio_delivery():
+ *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+ *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+ *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+ */
 static inline bool kvm_lowest_prio_delivery(struct kvm_lapic_irq *irq)
 {
 	return (irq->delivery_mode == APIC_DM_LOWEST ||
@@ -258,6 +492,27 @@ static inline enum lapic_mode kvm_apic_mode(u64 apic_base)
 
 static inline enum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	return kvm_apic_mode(vcpu->arch.apic_base);
 }
 
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 6e838cb6c..4f963e1fc 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -1663,6 +1663,34 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	if (tdp_mmu_enabled)
 		flush = kvm_tdp_mmu_unmap_gfn_range(kvm, range, flush);
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *
+	 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+	 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+	 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+	 *                       
+	 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+	 *
+	 * 在以下使用kvm_x86_ops->set_apic_access_page_addr:
+	 *   - arch/x86/kvm/vmx/main.c|992| <<global>> .set_apic_access_page_addr = vt_op(set_apic_access_page_addr),
+	 *   - arch/x86/kvm/mmu/mmu.c|1675| <<kvm_unmap_gfn_range>> if (kvm_x86_ops.set_apic_access_page_addr &&
+	 *   - arch/x86/kvm/vmx/vmx.c|9261| <<vmx_hardware_setup>> vt_x86_ops.set_apic_access_page_addr = NULL;
+	 *   - arch/x86/kvm/x86.c|11306| <<kvm_vcpu_reload_apic_access_page>> kvm_x86_call(set_apic_access_page_addr)(vcpu);
+	 *
+	 * vt_set_apic_access_page_addr()
+	 * vmx_set_apic_access_page_addr()
+	 */
 	if (kvm_x86_ops.set_apic_access_page_addr &&
 	    range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
@@ -4382,6 +4410,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * 在以下使用handle_mmio_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|6339| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4654,6 +4686,15 @@ static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 	if (slot->flags & KVM_MEMSLOT_INVALID)
 		return RET_PF_RETRY;
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
 		/*
 		 * Don't map L1's APIC access page into L2, KVM doesn't support
@@ -4669,6 +4710,15 @@ static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 		if (is_guest_mode(vcpu))
 			return kvm_handle_noslot_fault(vcpu, fault, access);
 
+		/*
+		 * 在以下使用kvm_apicv_activated():
+		 *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+		 *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+		 *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+		 *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+		 *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+		 *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+		 */
 		/*
 		 * If the APIC access page exists but is disabled, go directly
 		 * to emulation without caching the MMIO access or creating a
@@ -4839,6 +4889,13 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 	if (!flags) {
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
 
+		/*
+		 * 在以下使用kvm_mmu_page_fault():
+		 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+		 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+		 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+		 */
 		r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
 				insn_len);
 	} else if (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {
@@ -6290,6 +6347,13 @@ static int kvm_mmu_write_protect_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return RET_PF_EMULATE;
 }
 
+/*
+ * 在以下使用kvm_mmu_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -6318,6 +6382,9 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
 			return -EFAULT;
 
+		/*
+		 * 只在这里调用handle_mmio_page_fault()
+		 */
 		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
 		if (r == RET_PF_EMULATE)
 			goto emulate;
@@ -7375,6 +7442,12 @@ static void kvm_wake_nx_recovery_thread(struct kvm *kvm)
 	 */
 	struct vhost_task *nx_thread = READ_ONCE(kvm->arch.nx_huge_page_recovery_thread);
 
+	/*
+	 * 在以下使用vhost_task_wake():
+	 *   - arch/x86/kvm/mmu/mmu.c|7446| <<kvm_wake_nx_recovery_thread>> vhost_task_wake(nx_thread);
+	 *   - drivers/vhost/vhost.c|1538| <<vhost_task_wakeup>> return vhost_task_wake(worker->vtsk);
+	 *   - kernel/vhost_task.c|108| <<vhost_task_stop>> vhost_task_wake(vtsk);
+	 */
 	if (nx_thread)
 		vhost_task_wake(nx_thread);
 }
@@ -7713,6 +7786,14 @@ static int kvm_mmu_start_lpage_recovery(struct once *once)
 	struct vhost_task *nx_thread;
 
 	kvm->arch.nx_huge_page_last = get_jiffies_64();
+	/*
+	 * 在以下使用vhost_task_create():
+	 *   - arch/x86/kvm/mmu/mmu.c|7783| <<kvm_mmu_start_lpage_recovery>> nx_thread = vhost_task_create(
+	 *       kvm_nx_huge_page_recovery_worker, kvm_nx_huge_page_recovery_worker_kill,
+	 *       kvm, "kvm-nx-lpage-recovery");
+	 *   - drivers/vhost/vhost.c|957| <<vhost_task_worker_create>> vtsk = vhost_task_create(
+	 *       vhost_run_work_list, vhost_worker_killed, worker, name);
+	 */
 	nx_thread = vhost_task_create(kvm_nx_huge_page_recovery_worker,
 				      kvm_nx_huge_page_recovery_worker_kill,
 				      kvm, "kvm-nx-lpage-recovery");
@@ -7720,6 +7801,11 @@ static int kvm_mmu_start_lpage_recovery(struct once *once)
 	if (IS_ERR(nx_thread))
 		return PTR_ERR(nx_thread);
 
+	/*
+	 * 在以下使用vhost_task_start():
+	 *   - arch/x86/kvm/mmu/mmu.c|7790| <<kvm_mmu_start_lpage_recovery>> vhost_task_start(nx_thread);
+	 *   - drivers/vhost/vhost.c|963| <<vhost_task_worker_create>> vhost_task_start(vtsk);
+	 */
 	vhost_task_start(nx_thread);
 
 	/* Make the task visible only once it is fully started. */
@@ -7737,6 +7823,11 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 
 void kvm_mmu_pre_destroy_vm(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用vhost_task_stop():
+	 *   - arch/x86/kvm/mmu/mmu.c|7821| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - drivers/vhost/vhost.c|1592| <<vhost_task_do_stop>> return vhost_task_stop(worker->vtsk);
+	 */
 	if (kvm->arch.nx_huge_page_recovery_thread)
 		vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
 }
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 75e9cfc68..6e01c9fc3 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -608,6 +608,13 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
 		kvm_pmu_call(deliver_pmi)(vcpu);
+		/*
+		 * 在以下使用kvm_apic_local_deliver():
+		 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+		 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+		 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+		 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+		 */
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
@@ -860,6 +867,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 							 select_user;
 }
 
+/*
+ * 在以下使用kvm_pmu_trigger_event():
+ *   - arch/x86/kvm/vmx/nested.c|3714| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9106| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9445| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9447| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 eventsel)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
diff --git a/arch/x86/kvm/smm.c b/arch/x86/kvm/smm.c
index 9864c0571..c0cd2bf76 100644
--- a/arch/x86/kvm/smm.c
+++ b/arch/x86/kvm/smm.c
@@ -641,6 +641,12 @@ int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 #endif
 		ret = rsm_load_state_32(ctxt, &smram.smram32);
 
+	/*
+	 * 在以下使用kvm_leave_nested():
+	 *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+	 */
 	/*
 	 * If RSM fails and triggers shutdown, architecturally the shutdown
 	 * occurs *before* the transition to guest mode.  But due to KVM's
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index a34c5c3b1..bce741a8f 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -79,6 +79,11 @@ static bool next_vm_id_wrapped = 0;
 static DEFINE_SPINLOCK(svm_vm_data_hash_lock);
 bool x2avic_enabled;
 
+/*
+ * 在以下使用avic_activate_vmcb():
+ *   - arch/x86/kvm/svm/avic.c|246| <<avic_init_vmcb>> avic_activate_vmcb(svm);
+ *   - arch/x86/kvm/svm/avic.c|1096| <<avic_refresh_virtual_apic_mode>> avic_activate_vmcb(svm);
+ */
 static void avic_activate_vmcb(struct vcpu_svm *svm)
 {
 	struct vmcb *vmcb = svm->vmcb01.ptr;
@@ -242,6 +247,19 @@ void avic_init_vmcb(struct vcpu_svm *svm, struct vmcb *vmcb)
 	vmcb->control.avic_physical_id = __sme_set(__pa(kvm_svm->avic_physical_id_table));
 	vmcb->control.avic_vapic_bar = APIC_DEFAULT_PHYS_BASE;
 
+	/*
+	 * 在以下使用kvm_apicv_activated():
+	 *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+	 *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+	 *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+	 *
+	 * 在以下使用avic_activate_vmcb():
+	 *   - arch/x86/kvm/svm/avic.c|246| <<avic_init_vmcb>> avic_activate_vmcb(svm);
+	 *   - arch/x86/kvm/svm/avic.c|1096| <<avic_refresh_virtual_apic_mode>> avic_activate_vmcb(svm);
+	 */
 	if (kvm_apicv_activated(svm->vcpu.kvm))
 		avic_activate_vmcb(svm);
 	else
@@ -264,7 +282,24 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	 */
 	if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
 	    (id > X2AVIC_MAX_PHYSICAL_ID)) {
+		/*
+		 * 在以下使用kvm_set_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+		 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+		 *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+		 *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 */
 		kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+		/*
+		 * 在以下修改kvm_lapic->apicv_active:
+		 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+		 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+		 */
 		vcpu->arch.apic->apicv_active = false;
 		return 0;
 	}
@@ -275,6 +310,15 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	if (WARN_ON_ONCE(!vcpu->arch.apic->regs))
 		return -EINVAL;
 
+	/*
+	 * 在以下使用kvm_apicv_activated():
+	 *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+	 *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+	 *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+	 */
 	if (kvm_apicv_activated(vcpu->kvm)) {
 		int ret;
 
@@ -308,6 +352,10 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * 在以下使用avic_ring_doorbell():
+ *   - arch/x86/kvm/svm/svm.c|3831| <<svm_complete_interrupt_delivery>> avic_ring_doorbell(vcpu);
+ */
 void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -329,7 +377,26 @@ void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 
 static void avic_kick_vcpu(struct kvm_vcpu *vcpu, u32 icrl)
 {
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	vcpu->arch.apic->irr_pending = true;
+	/*
+	 * 在以下使用svm_complete_interrupt_delivery():
+	 *   - arch/x86/kvm/svm/avic.c|351| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu,
+	 *              icrl & APIC_MODE_MASK, icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|3854| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu,
+	 *              delivery_mode, trig_mode, vector);
+	 */
 	svm_complete_interrupt_delivery(vcpu,
 					icrl & APIC_MODE_MASK,
 					icrl & APIC_INT_LEVELTRIG,
@@ -471,6 +538,24 @@ static void avic_kick_target_vcpus(struct kvm *kvm, struct kvm_lapic *source,
 	 * since entered the guest will have processed pending IRQs at VMRUN.
 	 */
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (kvm_apic_match_dest(vcpu, source, icrl & APIC_SHORT_MASK,
 					dest, icrl & APIC_DEST_MASK))
 			avic_kick_vcpu(vcpu, icrl);
@@ -529,6 +614,14 @@ int avic_incomplete_ipi_interception(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_get_apicv_inhibit_reasons:
+ *   - arch/x86/kvm/svm/svm.c|5354| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+ *   - arch/x86/kvm/svm/svm.c|5594| <<svm_hardware_setup>> svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
+ *   - arch/x86/kvm/x86.c|10158| <<kvm_vcpu_apicv_activated>> kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
+ *
+ * struct kvm_x86_ops svm_x86_ops.vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+ */
 unsigned long avic_vcpu_get_apicv_inhibit_reasons(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu))
@@ -723,6 +816,11 @@ int avic_init_vcpu(struct vcpu_svm *svm)
 	return ret;
 }
 
+/*
+ * 在以下使用avic_apicv_post_state_restore():
+ *   - arch/x86/kvm/svm/svm.c|5146| <<global>> .apicv_post_state_restore = avic_apicv_post_state_restore,
+ *   - arch/x86/kvm/svm/avic.c|1053| <<avic_refresh_virtual_apic_mode>> avic_apicv_post_state_restore(vcpu);
+ */
 void avic_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 {
 	avic_handle_dfr_update(vcpu);
@@ -1016,6 +1114,16 @@ void avic_vcpu_put(struct kvm_vcpu *vcpu)
 							   AVIC_STOP_RUNNING);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+ *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+ *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+ *
+ * 在以下使用avic_refresh_virtual_apic_mode():
+ *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/svm/avic.c|1117| <<avic_refresh_apicv_exec_ctrl>> avic_refresh_virtual_apic_mode(vcpu);
+ */
 void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1033,6 +1141,11 @@ void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 		 * accordingly before re-activating.
 		 */
 		avic_apicv_post_state_restore(vcpu);
+		/*
+		 * 在以下使用avic_activate_vmcb():
+		 *   - arch/x86/kvm/svm/avic.c|246| <<avic_init_vmcb>> avic_activate_vmcb(svm);
+		 *   - arch/x86/kvm/svm/avic.c|1096| <<avic_refresh_virtual_apic_mode>> avic_activate_vmcb(svm);
+		 */
 		avic_activate_vmcb(svm);
 	} else {
 		avic_deactivate_vmcb(svm);
@@ -1040,6 +1153,12 @@ void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	vmcb_mark_dirty(vmcb, VMCB_AVIC);
 }
 
+/*
+ * 在以下使用refresh_apicv_exec_ctrl:
+ *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+ *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+ */
 void avic_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	if (!enable_apicv)
diff --git a/arch/x86/kvm/svm/hyperv.c b/arch/x86/kvm/svm/hyperv.c
index 088f6429b..8d1fdac78 100644
--- a/arch/x86/kvm/svm/hyperv.c
+++ b/arch/x86/kvm/svm/hyperv.c
@@ -14,5 +14,14 @@ void svm_hv_inject_synthetic_vmexit_post_tlb_flush(struct kvm_vcpu *vcpu)
 	svm->vmcb->control.exit_code_hi = 0;
 	svm->vmcb->control.exit_info_1 = HV_SVM_ENL_EXITCODE_TRAP_AFTER_FLUSH;
 	svm->vmcb->control.exit_info_2 = 0;
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index b7fd2e869..6d5c8dfd0 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -53,6 +53,15 @@ static void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,
 	vmcb->control.exit_info_1 &= ~0xffffffffULL;
 	vmcb->control.exit_info_1 |= fault->error_code;
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
 
@@ -123,6 +132,17 @@ static bool nested_vmcb_needs_vls_intercept(struct vcpu_svm *svm)
 	return false;
 }
 
+/*
+ * 在以下使用recalc_intercepts():
+ *   - arch/x86/kvm/svm/nested.c|873| <<nested_vmcb02_prepare_control>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/sev.c|4524| <<sev_es_init_vmcb>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.c|654| <<set_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.c|663| <<clr_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|466| <<set_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|476| <<clr_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|485| <<svm_set_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|494| <<svm_clr_intercept>> recalc_intercepts(svm);
+ */
 void recalc_intercepts(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *c, *h;
@@ -395,6 +415,11 @@ static bool nested_vmcb_check_save(struct kvm_vcpu *vcpu)
 	return __nested_vmcb_check_save(vcpu, save);
 }
 
+/*
+ * 在以下使用nested_vmcb_check_controls():
+ *   - arch/x86/kvm/svm/nested.c|986| <<nested_svm_vmrun>>
+ *        if (!nested_vmcb_check_save(vcpu) || !nested_vmcb_check_controls(vcpu)) {
+ */
 static bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -403,6 +428,11 @@ static bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu)
 	return __nested_vmcb_check_controls(vcpu, ctl);
 }
 
+/*
+ * 在以下使用__nested_copy_vmcb_control_to_cache():
+ *   - arch/x86/kvm/svm/nested.c|456| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu, &svm->nested.ctl, control);
+ *   - arch/x86/kvm/svm/nested.c|1852| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+ */
 static
 void __nested_copy_vmcb_control_to_cache(struct kvm_vcpu *vcpu,
 					 struct vmcb_ctrl_area_cached *to,
@@ -453,6 +483,12 @@ void __nested_copy_vmcb_control_to_cache(struct kvm_vcpu *vcpu,
 void nested_copy_vmcb_control_to_cache(struct vcpu_svm *svm,
 				       struct vmcb_control_area *control)
 {
+	/*
+	 * 在以下使用__nested_copy_vmcb_control_to_cache():
+	 *   - arch/x86/kvm/svm/nested.c|456| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu,
+	 *                                  &svm->nested.ctl, control);
+	 *   - arch/x86/kvm/svm/nested.c|1852| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+	 */
 	__nested_copy_vmcb_control_to_cache(&svm->vcpu, &svm->nested.ctl, control);
 }
 
@@ -482,6 +518,10 @@ void nested_copy_vmcb_save_to_cache(struct vcpu_svm *svm,
  * Synchronize fields that are written by the processor, so that
  * they can be copied back into the vmcb12.
  */
+/*
+ * 在以下使用nested_sync_control_from_vmcb02():
+ *   - arch/x86/kvm/svm/svm.c|4464| <<svm_vcpu_run>> nested_sync_control_from_vmcb02(svm);
+ */
 void nested_sync_control_from_vmcb02(struct vcpu_svm *svm)
 {
 	u32 mask;
@@ -516,6 +556,14 @@ void nested_sync_control_from_vmcb02(struct vcpu_svm *svm)
  * Transfer any event that L0 or L1 wanted to inject into L2 to
  * EXIT_INT_INFO.
  */
+/*
+ * 在以下使用nested_save_pending_event_to_vmcb12():
+ *   - arch/x86/kvm/svm/nested.c|1133| <<nested_svm_vmexit>> nested_save_pending_event_to_vmcb12(svm, vmcb12);
+ *
+ * 注释:
+ * Transfer any event that L0 or L1 wanted to inject into L2 to
+ * EXIT_INT_INFO.
+ */
 static void nested_save_pending_event_to_vmcb12(struct vcpu_svm *svm,
 						struct vmcb *vmcb12)
 {
@@ -575,6 +623,19 @@ static void nested_svm_transition_tlb_flush(struct kvm_vcpu *vcpu)
  * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
  * if we are emulating VM-Entry into a guest with NPT enabled.
  */
+/*
+ * 在以下使用nested_svm_load_cr3():
+ *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+ *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+ *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+ *              vmcb01->save.cr3, false, true);
+ *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+ *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+ *
+ * 注释:
+ * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+ * if we are emulating VM-Entry into a guest with NPT enabled.
+ */
 static int nested_svm_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,
 			       bool nested_npt, bool reload_pdptrs)
 {
@@ -605,6 +666,10 @@ void nested_vmcb02_compute_g_pat(struct vcpu_svm *svm)
 	svm->nested.vmcb02.ptr->save.g_pat = svm->vmcb01.ptr->save.g_pat;
 }
 
+/*
+ * 在以下使用nested_vmcb02_prepare_save():
+ *   - arch/x86/kvm/svm/nested.c|920| <<enter_svm_guest_mode>> nested_vmcb02_prepare_save(svm, vmcb12);
+ */
 static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12)
 {
 	bool new_vmcb12 = false;
@@ -700,6 +765,11 @@ static bool is_evtinj_nmi(u32 evtinj)
 	return type == SVM_EVTINJ_TYPE_NMI;
 }
 
+/*
+ * 在以下使用nested_vmcb02_prepare_control():
+ *   - arch/x86/kvm/svm/nested.c|919| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+ *   - arch/x86/kvm/svm/nested.c|1899| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+ */
 static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 					  unsigned long vmcb12_rip,
 					  unsigned long vmcb12_csbase)
@@ -715,6 +785,11 @@ static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 
 	nested_svm_transition_tlb_flush(vcpu);
 
+	/*
+	 * 在以下使用enter_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 */
 	/* Enter Guest-Mode */
 	enter_guest_mode(vcpu);
 
@@ -788,6 +863,11 @@ static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 
 	vmcb02->control.tsc_offset = vcpu->arch.tsc_offset;
 
+	/*
+	 * 在以下使用nested_svm_update_tsc_ratio_msr():
+	 *   - arch/x86/kvm/svm/nested.c|852| <<nested_vmcb02_prepare_control>> nested_svm_update_tsc_ratio_msr(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2994| <<svm_set_msr(MSR_AMD64_TSC_RATIO)>> nested_svm_update_tsc_ratio_msr(vcpu);
+	 */
 	if (guest_cpu_cap_has(vcpu, X86_FEATURE_TSCRATEMSR) &&
 	    svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
 		nested_svm_update_tsc_ratio_msr(vcpu);
@@ -880,6 +960,11 @@ static void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to
 	to_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;
 }
 
+/*
+ * 在以下使用enter_svm_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|1009| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+ *   - arch/x86/kvm/svm/svm.c|4939| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+ */
 int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 			 struct vmcb *vmcb12, bool from_vmrun)
 {
@@ -910,10 +995,37 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 
 	nested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	/*
+	 * 在以下使用nested_vmcb02_prepare_control():
+	 *   - arch/x86/kvm/svm/nested.c|919| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	 *   - arch/x86/kvm/svm/nested.c|1899| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+	 */
 	nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
 	nested_vmcb02_prepare_save(svm, vmcb12);
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+	 *              vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 *
+	 * 注释:
+	 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+	 * if we are emulating VM-Entry into a guest with NPT enabled.
+	 */
 	ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3,
 				  nested_npt_enabled(svm), from_vmrun);
 	if (ret)
@@ -924,6 +1036,18 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 
 	svm_set_gif(svm, true);
 
+	/*
+	 * 在以下使用KVM_REQ_APICV_UPDATE:
+	 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+	 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+	 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+	 *
+	 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+	 */
 	if (kvm_vcpu_apicv_active(vcpu))
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 
@@ -932,6 +1056,10 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 	return 0;
 }
 
+/*
+ * 在以下使用nested_svm_vmrun():
+ *   - arch/x86/kvm/svm/svm.c|2242| <<vmrun_interception>> return nested_svm_vmrun(vcpu);
+ */
 int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1001,6 +1129,13 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 
 	svm->nested.nested_run_pending = 1;
 
+	/*
+	 * 在以下使用enter_svm_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1009| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+	 *   - arch/x86/kvm/svm/svm.c|4939| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+	 *
+	 * 这里进入guest mode!
+	 */
 	if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
 		goto out_exit_err;
 
@@ -1017,6 +1152,15 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 	svm->vmcb->control.exit_info_1  = 0;
 	svm->vmcb->control.exit_info_2  = 0;
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 
 out:
@@ -1062,6 +1206,15 @@ void svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb)
 	to_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;
 }
 
+/*
+ * 在以下使用nested_svm_vmexit():
+ *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+ */
 int nested_svm_vmexit(struct vcpu_svm *svm)
 {
 	struct kvm_vcpu *vcpu = &svm->vcpu;
@@ -1081,6 +1234,13 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	vmcb12 = map.hva;
 
 	/* Exit Guest-Mode */
+	/*
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	leave_guest_mode(vcpu);
 	svm->nested.vmcb12_gpa = 0;
 	WARN_ON_ONCE(svm->nested.nested_run_pending);
@@ -1117,6 +1277,14 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	vmcb12->control.exit_info_1       = vmcb02->control.exit_info_1;
 	vmcb12->control.exit_info_2       = vmcb02->control.exit_info_2;
 
+	/*
+	 * 在以下使用nested_save_pending_event_to_vmcb12():
+	 *   - arch/x86/kvm/svm/nested.c|1133| <<nested_svm_vmexit>> nested_save_pending_event_to_vmcb12(svm, vmcb12);
+	 *
+	 * 注释:
+	 * Transfer any event that L0 or L1 wanted to inject into L2 to
+	 * EXIT_INT_INFO.
+	 */
 	if (vmcb12->control.exit_code != SVM_EXIT_ERR)
 		nested_save_pending_event_to_vmcb12(svm, vmcb12);
 
@@ -1144,6 +1312,15 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 
 	kvm_nested_vmexit_handle_ibrs(vcpu);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	/*
@@ -1239,6 +1416,19 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 
 	nested_svm_uninit_mmu_context(vcpu);
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+	 *              vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 *
+	 * 注释:
+	 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+	 * if we are emulating VM-Entry into a guest with NPT enabled.
+	 */
 	rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
 	if (rc)
 		return 1;
@@ -1264,6 +1454,19 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	 * Un-inhibit the AVIC right away, so that other vCPUs can start
 	 * to benefit from it right away.
 	 */
+	/*
+	 * 在以下使用kvm_apicv_activated():
+	 *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+	 *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+	 *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+	 *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+	 *
+	 * 在以下使用__kvm_vcpu_update_apicv():
+	 *   - arch/x86/kvm/svm/nested.c|1268| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10558| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+	 */
 	if (kvm_apicv_activated(vcpu->kvm))
 		__kvm_vcpu_update_apicv(vcpu);
 
@@ -1281,6 +1484,11 @@ static void nested_svm_triple_fault(struct kvm_vcpu *vcpu)
 	nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
 }
 
+/*
+ * 在以下使用svm_allocate_nested():
+ *   - arch/x86/kvm/svm/svm.c|240| <<svm_set_efer>> int ret = svm_allocate_nested(svm);
+ *   - arch/x86/kvm/svm/svm.c|4956| <<svm_leave_smm>> if (svm_allocate_nested(svm))
+ */
 int svm_allocate_nested(struct vcpu_svm *svm)
 {
 	struct page *vmcb02_page;
@@ -1311,6 +1519,15 @@ void svm_free_nested(struct vcpu_svm *svm)
 	if (!svm->nested.initialized)
 		return;
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	if (WARN_ON_ONCE(svm->vmcb != svm->vmcb01.ptr))
 		svm_switch_vmcb(svm, &svm->vmcb01);
 
@@ -1332,6 +1549,24 @@ void svm_free_nested(struct vcpu_svm *svm)
 	svm->nested.initialized = false;
 }
 
+/*
+ * 在以下使用kvm_leave_nested():
+ *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+ *
+ * 在以下使用kvm_x86_nested_ops->leave_nested:
+ *   - arch/x86/kvm/svm/nested.c|2093| <<global>> .leave_nested = svm_leave_nested,
+ *   - arch/x86/kvm/vmx/nested.c|8987| <<global>> .leave_nested = vmx_leave_nested,
+ *   - arch/x86/kvm/x86.h|143| <<kvm_leave_nested>> kvm_x86_ops.nested_ops->leave_nested(vcpu);
+ *
+ * 在以下使用svm_leave_nested():
+ *   - arch/x86/kvm/svm/nested.c|2093| <<global>> struct kvm_x86_nested_ops svm_nested_ops.leave_nested = svm_leave_nested,
+ *   - arch/x86/kvm/svm/nested.c|1948| <<svm_set_nested_state>> svm_leave_nested(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|2009| <<svm_set_nested_state>> svm_leave_nested(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|225| <<svm_set_efer>> svm_leave_nested(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|1361| <<svm_vcpu_free>> svm_leave_nested(vcpu);
+ */
 void svm_leave_nested(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1340,13 +1575,49 @@ void svm_leave_nested(struct kvm_vcpu *vcpu)
 		svm->nested.nested_run_pending = 0;
 		svm->nested.vmcb12_gpa = INVALID_GPA;
 
+		/*
+		 * 在以下使用leave_guest_mode():
+		 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+		 */
 		leave_guest_mode(vcpu);
 
+		/*
+		 * 在以下使用svm_switch_vmcb():
+		 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+		 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+		 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 */
 		svm_switch_vmcb(svm, &svm->vmcb01);
 
 		nested_svm_uninit_mmu_context(vcpu);
 		vmcb_mark_all_dirty(svm->vmcb);
 
+		/*
+		 * 在以下使用kvm_apicv_activated():
+		 *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+		 *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+		 *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+		 *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+		 *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+		 *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+		 *
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+		 *
+		 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+		 */
 		if (kvm_apicv_activated(vcpu->kvm))
 			kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
@@ -1404,6 +1675,10 @@ static int nested_svm_intercept_ioio(struct vcpu_svm *svm)
 	return (val & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;
 }
 
+/*
+ * 在以下使用nested_svm_intercept():
+ *   - arch/x86/kvm/svm/nested.c|1598| <<nested_svm_exit_handled>> vmexit = nested_svm_intercept(svm);
+ */
 static int nested_svm_intercept(struct vcpu_svm *svm)
 {
 	u32 exit_code = svm->vmcb->control.exit_code;
@@ -1448,12 +1723,27 @@ static int nested_svm_intercept(struct vcpu_svm *svm)
 	return vmexit;
 }
 
+/*
+ * 在以下使用nested_svm_exit_handled():
+ *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+ *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+ *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+ */
 int nested_svm_exit_handled(struct vcpu_svm *svm)
 {
 	int vmexit;
 
 	vmexit = nested_svm_intercept(svm);
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	if (vmexit == NESTED_EXIT_DONE)
 		nested_svm_vmexit(svm);
 
@@ -1483,6 +1773,10 @@ static bool nested_svm_is_exception_vmexit(struct kvm_vcpu *vcpu, u8 vector,
 	return (svm->nested.ctl.intercepts[INTERCEPT_EXCEPTION] & BIT(vector));
 }
 
+/*
+ * 在以下使用nested_svm_inject_exception_vmexit():
+ *   - arch/x86/kvm/svm/nested.c|1708| <<svm_check_nested_events>> nested_svm_inject_exception_vmexit(vcpu);
+ */
 static void nested_svm_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;
@@ -1516,6 +1810,15 @@ static void nested_svm_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 		WARN_ON(ex->has_payload);
 	}
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
 
@@ -1524,6 +1827,12 @@ static inline bool nested_exit_on_init(struct vcpu_svm *svm)
 	return vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_INIT);
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->check_events:
+ *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+ *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+ *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+ */
 static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -1635,6 +1944,11 @@ int nested_svm_exit_special(struct vcpu_svm *svm)
 	return NESTED_EXIT_CONTINUE;
 }
 
+/*
+ * 在以下使用nested_svm_update_tsc_ratio_msr():
+ *   - arch/x86/kvm/svm/nested.c|852| <<nested_vmcb02_prepare_control>> nested_svm_update_tsc_ratio_msr(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2994| <<svm_set_msr(MSR_AMD64_TSC_RATIO)>> nested_svm_update_tsc_ratio_msr(vcpu);
+ */
 void nested_svm_update_tsc_ratio_msr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1645,6 +1959,10 @@ void nested_svm_update_tsc_ratio_msr(struct kvm_vcpu *vcpu)
 	svm_write_tsc_multiplier(vcpu);
 }
 
+/*
+ * 在以下使用nested_copy_vmcb_cache_to_control():
+ *   - arch/x86/kvm/svm/nested.c|2006| <<svm_get_nested_state>> nested_copy_vmcb_cache_to_control(ctl, &svm->nested.ctl);
+ */
 /* Inverse operation of nested_copy_vmcb_control_to_cache(). asid is copied too. */
 static void nested_copy_vmcb_cache_to_control(struct vmcb_control_area *dst,
 					      struct vmcb_ctrl_area_cached *from)
@@ -1748,6 +2066,14 @@ static int svm_get_nested_state(struct kvm_vcpu *vcpu,
 	return kvm_state.size;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->set_state:
+ *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+ *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+ *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+ *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+ *                                     user_kvm_nested_state, &kvm_state);
+ */
 static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state)
@@ -1811,6 +2137,12 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 		goto out_free;
 
 	ret = -EINVAL;
+	/*
+	 * 在以下使用__nested_copy_vmcb_control_to_cache():
+	 *   - arch/x86/kvm/svm/nested.c|456| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu,
+	 *                             &svm->nested.ctl, control);
+	 *   - arch/x86/kvm/svm/nested.c|1852| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+	 */
 	__nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
 	if (!__nested_vmcb_check_controls(vcpu, &ctl_cached))
 		goto out_free;
@@ -1857,7 +2189,21 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
 	nested_copy_vmcb_control_to_cache(svm, ctl);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	/*
+	 * 在以下使用nested_vmcb02_prepare_control():
+	 *   - arch/x86/kvm/svm/nested.c|919| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	 *   - arch/x86/kvm/svm/nested.c|1899| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+	 */
 	nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
 
 	/*
@@ -1867,6 +2213,19 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	 * Set it again to fix this.
 	 */
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+	 *              vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 *
+	 * 注释:
+	 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+	 * if we are emulating VM-Entry into a guest with NPT enabled.
+	 */
 	ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3,
 				  nested_npt_enabled(svm), false);
 	if (WARN_ON_ONCE(ret))
@@ -1883,6 +2242,15 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->get_nested_state_pages:
+ *   - arch/x86/kvm/svm/nested.c|2230| <<global>> .get_nested_state_pages = svm_get_nested_state_pages,
+ *   - arch/x86/kvm/vmx/nested.c|8994| <<global>> .get_nested_state_pages = vmx_get_nested_state_pages,
+ *   - arch/x86/kvm/x86.c|11305| <<vcpu_enter_guest(KVM_REQ_GET_NESTED_STATE_PAGES)>>
+ *                             if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
+ *
+ * struct kvm_x86_nested_ops svm_nested_ops.get_nested_state_pages = svm_get_nested_state_pages,
+ */
 static bool svm_get_nested_state_pages(struct kvm_vcpu *vcpu)
 {
 	if (WARN_ON(!is_guest_mode(vcpu)))
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 0635bd71c..7595d840e 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -465,6 +465,17 @@ static int __sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp,
 	INIT_LIST_HEAD(&sev->mirror_vms);
 	sev->need_init = false;
 
+	/*
+	 * 在以下使用kvm_set_apicv_inhibit():
+	 *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+	 *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+	 *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+	 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+	 *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+	 *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+	 */
 	kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
 
 	return 0;
@@ -4156,6 +4167,12 @@ static int sev_handle_vmgexit_msr_protocol(struct vcpu_svm *svm)
 		vcpu->arch.regs[VCPU_REGS_RAX] = cpuid_fn;
 		vcpu->arch.regs[VCPU_REGS_RCX] = 0;
 
+		/*
+		 * 在以下使用svm_invoke_exit_handler():
+		 *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+		 *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+		 *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+		 */
 		ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
 		if (!ret) {
 			/* Error, keep GHCB MSR value as-is */
@@ -4409,6 +4426,12 @@ int sev_handle_vmgexit(struct kvm_vcpu *vcpu)
 		ret = -EINVAL;
 		break;
 	default:
+		/*
+		 * 在以下使用svm_invoke_exit_handler():
+		 *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+		 *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+		 *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+		 */
 		ret = svm_invoke_exit_handler(vcpu, exit_code);
 	}
 
@@ -4689,6 +4712,10 @@ struct page *snp_safe_alloc_page_node(int node, gfp_t gfp)
 	return p;
 }
 
+/*
+ * 在以下使用sev_handle_rmp_fault():
+ *   - arch/x86/kvm/svm/svm.c|2006| <<npf_interception>> sev_handle_rmp_fault(vcpu, fault_address, error_code);
+ */
 void sev_handle_rmp_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code)
 {
 	struct kvm_memory_slot *slot;
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 1bfebe408..e50ca82af 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -237,6 +237,11 @@ int svm_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 				svm_free_nested(svm);
 
 		} else {
+			/*
+			 * 在以下使用svm_allocate_nested():
+			 *   - arch/x86/kvm/svm/svm.c|240| <<svm_set_efer>> int ret = svm_allocate_nested(svm);
+			 *   - arch/x86/kvm/svm/svm.c|4956| <<svm_leave_smm>> if (svm_allocate_nested(svm))
+			 */
 			int ret = svm_allocate_nested(svm);
 
 			if (ret) {
@@ -864,6 +869,12 @@ void svm_copy_lbrs(struct vmcb *to_vmcb, struct vmcb *from_vmcb)
 	vmcb_mark_dirty(to_vmcb, VMCB_LBR);
 }
 
+/*
+ * 在以下使用svm_enable_lbrv():
+ *   - arch/x86/kvm/svm/sev.c|972| <<__sev_launch_update_vmsa>> svm_enable_lbrv(vcpu);
+ *   - arch/x86/kvm/svm/sev.c|2439| <<snp_launch_update_vmsa>> svm_enable_lbrv(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|923| <<svm_update_lbrv>> svm_enable_lbrv(vcpu);
+ */
 void svm_enable_lbrv(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1249,6 +1260,12 @@ static void __svm_vcpu_reset(struct kvm_vcpu *vcpu)
 		sev_es_vcpu_reset(svm);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_reset:
+ *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+ *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+ *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+ */
 static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1265,6 +1282,15 @@ static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 		__svm_vcpu_reset(vcpu);
 }
 
+/*
+ * 在以下使用svm_switch_vmcb():
+ *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+ */
 void svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)
 {
 	svm->current_vmcb = target_vmcb;
@@ -1310,6 +1336,15 @@ static int svm_vcpu_create(struct kvm_vcpu *vcpu)
 
 	svm->vmcb01.ptr = page_address(vmcb01_page);
 	svm->vmcb01.pa = __sme_set(page_to_pfn(vmcb01_page) << PAGE_SHIFT);
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	if (vmsa_page)
@@ -1400,6 +1435,12 @@ static void svm_srso_vm_init(void) { }
 static void svm_srso_vm_destroy(void) { }
 #endif
 
+/*
+ * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+ *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+ *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+ */
 static void svm_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1519,10 +1560,20 @@ static void svm_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 	}
 }
 
+/*
+ * 在以下使用svm_set_vintr():
+ *   - arch/x86/kvm/svm/svm.c|4032| <<svm_enable_irq_window>> svm_set_vintr(svm);
+ */
 static void svm_set_vintr(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *control;
 
+	/*
+	 * 在以下使用kvm_vcpu_apicv_activated():
+	 *   - arch/x86/kvm/svm/svm.c|1574| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+	 *   - arch/x86/kvm/x86.c|11026| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|11773| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+	 */
 	/*
 	 * The following fields are ignored when AVIC is enabled
 	 */
@@ -1958,11 +2009,21 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 		error_code |= PFERR_PRIVATE_ACCESS;
 
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
+	/*
+	 * 在以下使用kvm_mmu_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
 				static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
 				svm->vmcb->control.insn_bytes : NULL,
 				svm->vmcb->control.insn_len);
 
+	/*
+	 * 只在这里调用sev_handle_rmp_fault()
+	 */
 	if (rc > 0 && error_code & PFERR_GUEST_RMP_MASK)
 		sev_handle_rmp_fault(vcpu, fault_address, error_code);
 
@@ -2105,6 +2166,13 @@ static int shutdown_interception(struct kvm_vcpu *vcpu)
 		if (is_smm(vcpu))
 			kvm_smm_changed(vcpu, false);
 #endif
+		/*
+		 * 在以下使用kvm_vcpu_reset():
+		 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+		 */
 		kvm_vcpu_reset(vcpu, true);
 	}
 
@@ -2197,11 +2265,19 @@ static int vmsave_interception(struct kvm_vcpu *vcpu)
 	return vmload_vmsave_interception(vcpu, false);
 }
 
+/*
+ * 在以下使用vmrun_interception():
+ *   - arch/x86/kvm/svm/svm.c|3325| <<global>> svm_exit_handlers[SVM_EXIT_VMRUN] = vmrun_interception,
+ *   - arch/x86/kvm/svm/svm.c|2282| <<emulate_svm_instr>> guest_mode_exit_codes[SVM_INSTR_VMRUN] = vmrun_interception,
+ */
 static int vmrun_interception(struct kvm_vcpu *vcpu)
 {
 	if (nested_svm_check_permissions(vcpu))
 		return 1;
 
+	/*
+	 * 只在这里调用
+	 */
 	return nested_svm_vmrun(vcpu);
 }
 
@@ -2234,6 +2310,10 @@ static int svm_instr_opcode(struct kvm_vcpu *vcpu)
 	return NONE_SVM_INSTR;
 }
 
+/*
+ * 在以下使用emulate_svm_instr():
+ *   - arch/x86/kvm/svm/svm.c|2356| <<gp_interception>> return emulate_svm_instr(vcpu, opcode);
+ */
 static int emulate_svm_instr(struct kvm_vcpu *vcpu, int opcode)
 {
 	const int guest_mode_exit_codes[] = {
@@ -2518,6 +2598,12 @@ static bool check_selective_cr0_intercepted(struct kvm_vcpu *vcpu,
 
 	if (cr0 ^ val) {
 		svm->vmcb->control.exit_code = SVM_EXIT_CR0_SEL_WRITE;
+		/*
+		 * 在以下使用nested_svm_exit_handled():
+		 *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+		 *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+		 *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+		 */
 		ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
 	}
 
@@ -2587,6 +2673,18 @@ static int cr_interception(struct kvm_vcpu *vcpu)
 			val = kvm_read_cr4(vcpu);
 			break;
 		case 8:
+			/*
+			 * 在以下使用kvm_get_cr8():
+			 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+			 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+			 */
 			val = kvm_get_cr8(vcpu);
 			break;
 		default:
@@ -2678,6 +2776,18 @@ static int cr8_write_interception(struct kvm_vcpu *vcpu)
 {
 	int r;
 
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	u8 cr8_prev = kvm_get_cr8(vcpu);
 	/* instruction emulation calls kvm_set_cr8() */
 	r = cr_interception(vcpu);
@@ -2922,6 +3032,11 @@ static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 
 		svm->tsc_ratio_msr = data;
 
+		/*
+		 * 在以下使用nested_svm_update_tsc_ratio_msr():
+		 *   - arch/x86/kvm/svm/nested.c|852| <<nested_vmcb02_prepare_control>> nested_svm_update_tsc_ratio_msr(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|2994| <<svm_set_msr(MSR_AMD64_TSC_RATIO)>> nested_svm_update_tsc_ratio_msr(vcpu);
+		 */
 		if (guest_cpu_cap_has(vcpu, X86_FEATURE_TSCRATEMSR) &&
 		    is_guest_mode(vcpu))
 			nested_svm_update_tsc_ratio_msr(vcpu);
@@ -3120,6 +3235,16 @@ static int interrupt_window_interception(struct kvm_vcpu *vcpu)
 	 * All vCPUs which run still run nested, will remain to have their
 	 * AVIC still inhibited due to per-cpu AVIC inhibition.
 	 */
+	/*
+	 * 在以下使用kvm_clear_apicv_inhibit():
+	 *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+	 *   - arch/x86/kvm/lapic.c|602| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|612| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+	 *   - arch/x86/kvm/lapic.c|623| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+	 *   - arch/x86/kvm/svm/svm.c|3238| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+	 *   - arch/x86/kvm/x86.c|6688| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+	 *   - arch/x86/kvm/x86.c|7245| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+	 */
 	kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
 
 	++vcpu->stat.irq_window_exits;
@@ -3500,6 +3625,12 @@ static int svm_handle_invalid_exit(struct kvm_vcpu *vcpu, u64 exit_code)
 	return 0;
 }
 
+/*
+ * 在以下使用svm_invoke_exit_handler():
+ *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+ *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+ *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+ */
 int svm_invoke_exit_handler(struct kvm_vcpu *vcpu, u64 exit_code)
 {
 	if (!svm_check_exit_valid(exit_code))
@@ -3556,6 +3687,12 @@ static void svm_get_entry_info(struct kvm_vcpu *vcpu, u32 *intr_info,
 
 }
 
+/*
+ * 在以下使用kvm_x86_ops->handle_exit:
+ *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+ *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+ *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+ */
 static int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3577,6 +3714,12 @@ static int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 
 		vmexit = nested_svm_exit_special(svm);
 
+		/*
+		 * 在以下使用nested_svm_exit_handled():
+		 *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+		 *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+		 *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+		 */
 		if (vmexit == NESTED_EXIT_CONTINUE)
 			vmexit = nested_svm_exit_handled(svm);
 
@@ -3596,6 +3739,12 @@ static int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	if (exit_fastpath != EXIT_FASTPATH_NONE)
 		return 1;
 
+	/*
+	 * 在以下使用svm_invoke_exit_handler():
+	 *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+	 *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+	 *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+	 */
 	return svm_invoke_exit_handler(vcpu, exit_code);
 }
 
@@ -3701,6 +3850,13 @@ static void svm_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 				       SVM_EVTINJ_VALID | type;
 }
 
+/*
+ * 在以下使用svm_complete_interrupt_delivery():
+ *   - arch/x86/kvm/svm/avic.c|351| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu,
+ *              icrl & APIC_MODE_MASK, icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+ *   - arch/x86/kvm/svm/svm.c|3854| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu,
+ *              delivery_mode, trig_mode, vector);
+ */
 void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 				     int trig_mode, int vector)
 {
@@ -3710,6 +3866,12 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 	 */
 	bool in_guest_mode = (smp_load_acquire(&vcpu->mode) == IN_GUEST_MODE);
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	/* Note, this is called iff the local APIC is in-kernel. */
 	if (!READ_ONCE(vcpu->arch.apic->apicv_active)) {
 		/* Process the interrupt via kvm_check_and_inject_events(). */
@@ -3738,6 +3900,11 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 				  int trig_mode, int vector)
 {
+	/*
+	 * 在以下使用kvm_lapic_set_irr():
+	 *   - arch/x86/kvm/svm/svm.c|3844| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+	 *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+	 */
 	kvm_lapic_set_irr(vector, apic);
 
 	/*
@@ -3748,6 +3915,13 @@ static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 	 * will signal the doorbell if the CPU has already entered the guest.
 	 */
 	smp_mb__after_atomic();
+	/*
+	 * 在以下使用svm_complete_interrupt_delivery():
+	 *   - arch/x86/kvm/svm/avic.c|351| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu,
+	 *              icrl & APIC_MODE_MASK, icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|3854| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu,
+	 *              delivery_mode, trig_mode, vector);
+	 */
 	svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);
 }
 
@@ -3881,6 +4055,15 @@ static int svm_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 	return 1;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->enable_irq_window:
+ *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+ *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+ *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+ *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+ *
+ * struct kvm_x86_ops svm_x86_ops.enable_irq_window = svm_enable_irq_window
+ */
 static void svm_enable_irq_window(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3894,6 +4077,17 @@ static void svm_enable_irq_window(struct kvm_vcpu *vcpu)
 	 * window under the assumption that the hardware will set the GIF.
 	 */
 	if (vgif || gif_set(svm)) {
+		/*
+		 * 在以下使用kvm_set_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|600| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|610| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|621| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/lapic.c|4017| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+		 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_backing_page>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+		 *   - arch/x86/kvm/svm/sev.c|468| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+		 *   - arch/x86/kvm/svm/svm.c|4081| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 */
 		/*
 		 * IRQ window is not needed when AVIC is enabled,
 		 * unless we have pending ExtINT since it cannot be injected
@@ -4041,19 +4235,51 @@ static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用sync_lapic_to_cr8():
+ *   - arch/x86/kvm/svm/svm.c|4277| <<svm_vcpu_run>> sync_lapic_to_cr8(vcpu);
+ */
 static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	u64 cr8;
 
+	/*
+	 * 这里之前是这样的.
+	 * -       if (nested_svm_virtualize_tpr(vcpu) ||
+	 * -           kvm_vcpu_apicv_active(vcpu))
+	 * +       if (nested_svm_virtualize_tpr(vcpu))
+	 *               return;
+	 *
+	 * OVMF不会在启动的时候写TPR.
+	 * 所以用pre-reset的value.
+	 * APICv被inhibit了.
+	 * 这样Windows初始化的时候会有问题.
+	 */
 	if (nested_svm_virtualize_tpr(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	cr8 = kvm_get_cr8(vcpu);
 	svm->vmcb->control.int_ctl &= ~V_TPR_MASK;
 	svm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;
 }
 
+/*
+ * 在以下使用svm_complete_soft_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4279| <<svm_complete_interrupts>> svm_complete_soft_interrupt(vcpu, vector, type);
+ */
 static void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 					int type)
 {
@@ -4086,6 +4312,11 @@ static void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 		kvm_rip_write(vcpu, svm->soft_int_old_rip);
 }
 
+/*
+ * 在以下使用svm_complete_interrupts():
+ *   - arch/x86/kvm/svm/svm.c|4341| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4571| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+ */
 static void svm_complete_interrupts(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4147,9 +4378,27 @@ static void svm_complete_interrupts(struct kvm_vcpu *vcpu)
 		break;
 	}
 	case SVM_EXITINTINFO_TYPE_INTR:
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, vector, false);
 		break;
 	case SVM_EXITINTINFO_TYPE_SOFT:
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, vector, true);
 		break;
 	default:
@@ -4166,6 +4415,11 @@ static void svm_cancel_injection(struct kvm_vcpu *vcpu)
 	control->exit_int_info = control->event_inj;
 	control->exit_int_info_err = control->event_inj_err;
 	control->event_inj = 0;
+	/*
+	 * 在以下使用svm_complete_interrupts():
+	 *   - arch/x86/kvm/svm/svm.c|4341| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|4571| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+	 */
 	svm_complete_interrupts(vcpu);
 }
 
@@ -4177,6 +4431,10 @@ static int svm_vcpu_pre_run(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用svm_exit_handlers_fastpath():
+ *   - arch/x86/kvm/svm/svm.c|4573| <<svm_vcpu_run>> return svm_exit_handlers_fastpath(vcpu);
+ */
 static fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4238,6 +4496,12 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 
 	trace_kvm_entry(vcpu, force_immediate_exit);
 
+	/*
+	 * struct vcpu_svm *svm:
+	 * -> struct vmcb *vmcb;
+	 *    -> struct vmcb_control_area control;
+	 *    -> struct vmcb_save_area save;
+	 */
 	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
@@ -4323,8 +4587,24 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 		vcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;
 		vcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;
 	}
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	vcpu->arch.regs_dirty = 0;
 
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 		kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
 
@@ -4337,6 +4617,13 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 
 	/* Any pending NMI will happen here */
 
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 		kvm_after_interrupt(vcpu);
 
@@ -4374,8 +4661,16 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 
 	trace_kvm_exit(vcpu, KVM_ISA_SVM);
 
+	/*
+	 * 在以下使用svm_complete_interrupts():
+	 *   - arch/x86/kvm/svm/svm.c|4341| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|4571| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+	 */
 	svm_complete_interrupts(vcpu);
 
+	/*
+	 * 只在这里调用
+	 */
 	return svm_exit_handlers_fastpath(vcpu);
 }
 
@@ -4439,6 +4734,12 @@ static bool svm_has_emulated_msr(struct kvm *kvm, u32 index)
 	return true;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+ *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+ *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+ *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+ */
 static void svm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4540,6 +4841,12 @@ static const struct __x86_intercept {
 #undef POST_EX
 #undef POST_MEM
 
+/*
+ * 在以下使用kvm_x86_ops->check_intercept:
+ *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+ *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+ *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+ */
 static int svm_check_intercept(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage,
@@ -4651,6 +4958,12 @@ static int svm_check_intercept(struct kvm_vcpu *vcpu,
 	if (static_cpu_has(X86_FEATURE_NRIPS))
 		vmcb->control.next_rip  = info->next_rip;
 	vmcb->control.exit_code = icpt_info.exit_code;
+	/*
+	 * 在以下使用nested_svm_exit_handled():
+	 *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+	 *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+	 *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+	 */
 	vmexit = nested_svm_exit_handled(svm);
 
 	ret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED
@@ -4662,6 +4975,13 @@ static int svm_check_intercept(struct kvm_vcpu *vcpu,
 
 static void svm_handle_exit_irqoff(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	if (to_svm(vcpu)->vmcb->control.exit_code == SVM_EXIT_INTR)
 		vcpu->arch.at_instruction_boundary = true;
 }
@@ -4781,6 +5101,11 @@ static int svm_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	if (kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.hsave_msr), &map_save))
 		goto unmap_map;
 
+	/*
+	 * 在以下使用svm_allocate_nested():
+	 *   - arch/x86/kvm/svm/svm.c|240| <<svm_set_efer>> int ret = svm_allocate_nested(svm);
+	 *   - arch/x86/kvm/svm/svm.c|4956| <<svm_leave_smm>> if (svm_allocate_nested(svm))
+	 */
 	if (svm_allocate_nested(svm))
 		goto unmap_save;
 
@@ -4800,6 +5125,11 @@ static int svm_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	vmcb12 = map.hva;
 	nested_copy_vmcb_control_to_cache(svm, &vmcb12->control);
 	nested_copy_vmcb_save_to_cache(svm, &vmcb12->save);
+	/*
+	 * 在以下使用enter_svm_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1009| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+	 *   - arch/x86/kvm/svm/svm.c|4939| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+	 */
 	ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
 
 	if (ret)
@@ -5031,6 +5361,14 @@ static int svm_vm_init(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->alloc_apic_backing_page:
+ *   - arch/x86/kvm/svm/svm.c|5481| <<global>> .alloc_apic_backing_page = svm_alloc_apic_backing_page,
+ *   - arch/x86/kvm/lapic.c|4386| <<kvm_create_lapic>> if (kvm_x86_ops.alloc_apic_backing_page)
+ *   - arch/x86/kvm/lapic.c|4387| <<kvm_create_lapic>> apic->regs = kvm_x86_call(alloc_apic_backing_page)(vcpu);
+ *
+ * struct kvm_x86_ops svm_x86_ops.alloc_apic_backing_page = svm_alloc_apic_backing_page,
+ */
 static void *svm_alloc_apic_backing_page(struct kvm_vcpu *vcpu)
 {
 	struct page *page = snp_safe_alloc_page();
@@ -5188,6 +5526,10 @@ static struct kvm_x86_ops svm_x86_ops __initdata = {
  * memory encryption support and override the default MMIO mask if
  * memory encryption is enabled.
  */
+/*
+ * 在以下使用svm_adjust_mmio_mask():
+ *   - arch/x86/kvm/svm/svm.c|5609| <<svm_hardware_setup>> svm_adjust_mmio_mask();
+ */
 static __init void svm_adjust_mmio_mask(void)
 {
 	unsigned int enc_bit, mask_bit;
@@ -5416,6 +5758,13 @@ static __init int svm_hardware_setup(void)
 		svm_x86_ops.vcpu_unblocking = NULL;
 		svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
 	} else if (!x2avic_enabled) {
+		/*
+		 * 在以下使用kvm_x86_ops->allow_apicv_in_x2apic_without_x2apic_virtualization:
+		 *   - arch/x86/kvm/svm/svm.c|5734| <<svm_hardware_setup>> svm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization = true;
+		 *   - arch/x86/kvm/x86.c|11103| <<kvm_vcpu_update_apicv>> if (... kvm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization)
+		 *
+		 * 只有amd svm使用
+		 */
 		svm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization = true;
 	}
 
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 58b9d168e..de432e4f1 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -763,6 +763,15 @@ static inline int nested_svm_simple_vmexit(struct vcpu_svm *svm, u32 exit_code)
 	svm->vmcb->control.exit_code   = exit_code;
 	svm->vmcb->control.exit_info_1 = 0;
 	svm->vmcb->control.exit_info_2 = 0;
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	return nested_svm_vmexit(svm);
 }
 
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index 5316c27f6..9a9e3b997 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -191,6 +191,32 @@ static inline bool cpu_has_vmx_apic_register_virt(void)
 
 static inline bool cpu_has_vmx_virtual_intr_delivery(void)
 {
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
 		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
 }
@@ -219,6 +245,10 @@ static inline bool cpu_has_vmx_vmfunc(void)
 		SECONDARY_EXEC_ENABLE_VMFUNC;
 }
 
+/*
+ * 在以下使用cpu_has_vmx_shadow_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|7893| <<cpu_has_vmx_shadow_vmcs>> if (!cpu_has_vmx_shadow_vmcs())
+ */
 static inline bool cpu_has_vmx_shadow_vmcs(void)
 {
 	/* check if the cpu supports writing r/o exit information fields */
@@ -270,6 +300,12 @@ static inline bool cpu_has_vmx_bus_lock_detection(void)
 	    SECONDARY_EXEC_BUS_LOCK_DETECTION;
 }
 
+/*
+ * 在以下使用cpu_has_vmx_apicv():
+ *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+ *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+ *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+ */
 static inline bool cpu_has_vmx_apicv(void)
 {
 	return cpu_has_vmx_apic_register_virt() &&
diff --git a/arch/x86/kvm/vmx/common.h b/arch/x86/kvm/vmx/common.h
index bc5ece765..754e7a9a7 100644
--- a/arch/x86/kvm/vmx/common.h
+++ b/arch/x86/kvm/vmx/common.h
@@ -40,6 +40,15 @@ struct vcpu_vt {
 	union vmx_exit_reason exit_reason;
 
 	unsigned long	exit_qualification;
+	/*
+	 * 在以下使用vcpu_vt->exit_intr_info:
+	 *   - arch/x86/kvm/vmx/tdx.c|957| <<tdx_vcpu_enter_exit>> vt->exit_intr_info = tdx->vp_enter_args.r9;
+	 *   - arch/x86/kvm/vmx/vmx.c|7343| <<vmx_vcpu_run>> vmx->vt.exit_intr_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.h|368| <<vmx_get_intr_info>> vt->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	 *   - arch/x86/kvm/vmx/vmx.h|370| <<vmx_get_intr_info>> return vt->exit_intr_info;
+	 *
+	 * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+	 */
 	u32		exit_intr_info;
 
 	/*
@@ -105,6 +114,13 @@ static inline int __vmx_handle_ept_violation(struct kvm_vcpu *vcpu, gpa_t gpa,
 	if (vt_is_tdx_private_gpa(vcpu->kvm, gpa))
 		error_code |= PFERR_PRIVATE_ACCESS;
 
+	/*
+	 * 在以下使用kvm_mmu_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
diff --git a/arch/x86/kvm/vmx/main.c b/arch/x86/kvm/vmx/main.c
index dbab1c15b..4a1a5c556 100644
--- a/arch/x86/kvm/vmx/main.c
+++ b/arch/x86/kvm/vmx/main.c
@@ -83,6 +83,12 @@ static void vt_vcpu_free(struct kvm_vcpu *vcpu)
 	vmx_vcpu_free(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_reset:
+ *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+ *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+ *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+ */
 static void vt_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	if (is_td_vcpu(vcpu)) {
@@ -115,6 +121,12 @@ static void vt_update_cpu_dirty_logging(struct kvm_vcpu *vcpu)
 	vmx_update_cpu_dirty_logging(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+ *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+ *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+ */
 static void vt_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu)) {
@@ -151,6 +163,12 @@ static fastpath_t vt_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 	return vmx_vcpu_run(vcpu, run_flags);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->handle_exit:
+ *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+ *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+ *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+ */
 static int vt_handle_exit(struct kvm_vcpu *vcpu,
 			  enum exit_fastpath_completion fastpath)
 {
@@ -273,15 +291,35 @@ static bool vt_apic_init_signal_blocked(struct kvm_vcpu *vcpu)
 	return vmx_apic_init_signal_blocked(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+ *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+ *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+ */
 static void vt_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	/* Only x2APIC mode is supported for TD. */
 	if (is_td_vcpu(vcpu))
 		return;
 
+	/*
+	 * 在以下使用vmx_set_virtual_apic_mode():
+	 *   - arch/x86/kvm/vmx/main.c|306| <<vt_set_virtual_apic_mode>> return vmx_set_virtual_apic_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6056| <<__nested_vmx_vmexit>> vmx_set_virtual_apic_mode(vcpu);
+	 */
 	return vmx_set_virtual_apic_mode(vcpu);
 }
 
+/*
+ * 在以下使用vt_hwapic_isr_update():
+ *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+ *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+ *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ */
 static void vt_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 {
 	if (is_td_vcpu(vcpu))
@@ -310,6 +348,12 @@ static void vt_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
 	vmx_deliver_interrupt(apic, delivery_mode, trig_mode, vector);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+ *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+ *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+ *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+ */
 static void vt_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu))
@@ -688,6 +732,11 @@ static void vt_get_entry_info(struct kvm_vcpu *vcpu, u32 *intr_info, u32 *error_
 	vmx_get_entry_info(vcpu, intr_info, error_code);
 }
 
+/*
+ * 在以下调用:
+ *   - arch/x86/kvm/x86.c|8806| <<prepare_emulation_failure_exit>> kvm_x86_call(get_exit_info)(vcpu, (u32 *)&info[0], &info[1], &info[2],
+ *   - arch/x86/kvm/x86.c|8871| <<kvm_prepare_event_vectoring_exit>> kvm_x86_call(get_exit_info)(vcpu, &reason, &info1, &info2,
+ */
 static void vt_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 			u64 *info1, u64 *info2, u32 *intr_info, u32 *error_code)
 {
@@ -708,14 +757,33 @@ static void vt_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 	vmx_update_cr8_intercept(vcpu, tpr, irr);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->set_apic_access_page_addr:
+ *   - arch/x86/kvm/vmx/main.c|992| <<global>> .set_apic_access_page_addr = vt_op(set_apic_access_page_addr),
+ *   - arch/x86/kvm/mmu/mmu.c|1675| <<kvm_unmap_gfn_range>> if (kvm_x86_ops.set_apic_access_page_addr &&
+ *   - arch/x86/kvm/vmx/vmx.c|9261| <<vmx_hardware_setup>> vt_x86_ops.set_apic_access_page_addr = NULL;
+ *   - arch/x86/kvm/x86.c|11306| <<kvm_vcpu_reload_apic_access_page>> kvm_x86_call(set_apic_access_page_addr)(vcpu);
+ *
+ * vt_set_apic_access_page_addr()
+ * vmx_set_apic_access_page_addr()
+ */
 static void vt_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu))
 		return;
 
+	/*
+	 * 只在这里调用
+	 */
 	vmx_set_apic_access_page_addr(vcpu);
 }
 
+/*
+ * 在以下使用refresh_apicv_exec_ctrl:
+ *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+ *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+ */
 static void vt_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu)) {
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index b8ea19691..fce203e22 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -20,9 +20,53 @@
 #include "vmx.h"
 #include "smm.h"
 
+/*
+ * kvm_vcpu_reset()
+ * -> if (is_guest_mode(vcpu))
+ *        kvm_leave_nested(vcpu);
+ * -> kvm_lapic_reset()
+ *    -> if (apic->apicv_active)
+ *           kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ * -> WARN_ON_ONCE(is_guest_mode(vcpu) || is_smm(vcpu));
+ *
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_LAPIC)
+ * -> kvm_vcpu_ioctl_set_lapic()
+ *    -> kvm_apic_set_state()
+ *       -> if (apic->apicv_active)
+ *              kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ */
+
+/*
+ * 在以下使用enable_shadow_vmcs:
+ *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+ *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+ *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+ *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+ *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+ *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+ *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+ */
 static bool __read_mostly enable_shadow_vmcs = 1;
 module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
 
+/*
+ * 在以下使用nested_early_check:
+ *   - arch/x86/kvm/vmx/nested.c|44| <<global>> static bool __read_mostly nested_early_check = 0;
+ *   - arch/x86/kvm/vmx/nested.c|45| <<global>> module_param(nested_early_check, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|2573| <<prepare_vmcs02_constant_state>> if (enable_ept && nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|3586| <<nested_vmx_check_vmentry_hw>> if (!nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|3955| <<nested_vmx_enter_non_root_mode>> if (!enable_ept && !nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|5748| <<__nested_vmx_vmexit>> WARN_ON_ONCE(nested_early_check);
+ */
 static bool __read_mostly nested_early_check = 0;
 module_param(nested_early_check, bool, S_IRUGO);
 
@@ -45,6 +89,14 @@ enum {
 	VMX_VMWRITE_BITMAP,
 	VMX_BITMAP_NR
 };
+/*
+ * 在以下使用vmx_bitmap[VMX_BITMAP_NR]:
+ *   - arch/x86/kvm/vmx/nested.c|8139| <<nested_vmx_hardware_setup>> vmx_bitmap[i] = (unsigned long *)
+ *   - arch/x86/kvm/vmx/nested.c|8141| <<nested_vmx_hardware_setup>> if (!vmx_bitmap[i]) {
+ *   - arch/x86/kvm/vmx/nested.c|68| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+ *   - arch/x86/kvm/vmx/nested.c|69| <<vmx_vmwrite_bitmap>> #define vmx_vmwrite_bitmap (vmx_bitmap[VMX_VMWRITE_BITMAP])
+ *   - arch/x86/kvm/vmx/nested.c|8101| <<nested_vmx_hardware_unsetup>> free_page((unsigned long )vmx_bitmap[i]);
+ */
 static unsigned long *vmx_bitmap[VMX_BITMAP_NR];
 
 #define vmx_vmread_bitmap                    (vmx_bitmap[VMX_VMREAD_BITMAP])
@@ -68,6 +120,10 @@ static struct shadow_vmcs_field shadow_read_write_fields[] = {
 static int max_shadow_read_write_fields =
 	ARRAY_SIZE(shadow_read_write_fields);
 
+/*
+ * 在以下使用init_vmcs_shadow_fields():
+ *   - arch/x86/kvm/vmx/nested.c|8147| <<nested_vmx_hardware_setup>> init_vmcs_shadow_fields();
+ */
 static void init_vmcs_shadow_fields(void)
 {
 	int i, j;
@@ -125,6 +181,12 @@ static void init_vmcs_shadow_fields(void)
 				continue;
 			break;
 		case GUEST_INTR_STATUS:
+			/*
+			 * 在以下使用cpu_has_vmx_apicv():
+			 *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+			 *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+			 *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+			 */
 			if (!cpu_has_vmx_apicv())
 				continue;
 			break;
@@ -176,6 +238,19 @@ static int nested_vmx_failValid(struct kvm_vcpu *vcpu,
 			    X86_EFLAGS_SF | X86_EFLAGS_OF))
 			| X86_EFLAGS_ZF);
 	get_vmcs12(vcpu)->vm_instruction_error = vm_instruction_error;
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	/*
 	 * We don't need to force sync to shadow VMCS because
 	 * VM_INSTRUCTION_ERROR is not shadowed. Enlightened VMCS 'shadows' all
@@ -202,6 +277,13 @@ static int nested_vmx_fail(struct kvm_vcpu *vcpu, u32 vm_instruction_error)
 	return nested_vmx_failValid(vcpu, vm_instruction_error);
 }
 
+/*
+ * 在以下使用nested_vmx_abort():
+ *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+ *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+ */
 static void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)
 {
 	/* TODO: not to reset guest simply here. */
@@ -214,15 +296,46 @@ static inline bool vmx_control_verify(u32 control, u32 low, u32 high)
 	return fixed_bits_valid(control, low, high);
 }
 
+/*
+ * 在以下使用vmx_control_msr():
+ *   - arch/x86/kvm/vmx/nested.c|1079| <<nested_vmx_max_atomic_switch_msrs>> u64 vmx_misc = vmx_control_msr(vmx->nested.msrs.misc_low,
+ *   - arch/x86/kvm/vmx/nested.c|1617| <<vmx_restore_control_msr>> supported = vmx_control_msr(*lowp, *highp);
+ *   - arch/x86/kvm/vmx/nested.c|1647| <<vmx_restore_vmx_misc>> u64 vmx_misc = vmx_control_msr(vmcs_config.nested.misc_low,
+ *   - arch/x86/kvm/vmx/nested.c|1683| <<vmx_restore_vmx_ept_vpid_cap>> u64 vmx_ept_vpid_cap = vmx_control_msr(vmcs_config.nested.ept_caps,
+ *   - arch/x86/kvm/vmx/nested.c|1800| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->pinbased_ctls_low, msrs->pinbased_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1808| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->procbased_ctls_low, msrs->procbased_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1816| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->exit_ctls_low, msrs->exit_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1824| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->entry_ctls_low, msrs->entry_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1831| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->misc_low, msrs->misc_high);
+ *   - arch/x86/kvm/vmx/nested.c|1851| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->secondary_ctls_low, msrs->secondary_ctls_high);
+ */
 static inline u64 vmx_control_msr(u32 low, u32 high)
 {
 	return low | ((u64)high << 32);
 }
 
+/*
+ * 在以下使用vmx_disable_shadow_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|498| <<free_nested>> vmx_disable_shadow_vmcs(vmx);
+ *   - arch/x86/kvm/vmx/nested.c|6317| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+ */
 static void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)
 {
 	secondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_SHADOW_VMCS);
 	vmcs_write64(VMCS_LINK_POINTER, INVALID_GPA);
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	vmx->nested.need_vmcs12_to_shadow_sync = false;
 }
 
@@ -271,6 +384,10 @@ static bool nested_evmcs_handle_vmclear(struct kvm_vcpu *vcpu, gpa_t vmptr)
 #endif
 }
 
+/*
+ * 在以下使用vmx_sync_vmcs_host_state():
+ *   - arch/x86/kvm/vmx/nested.c|315| <<vmx_switch_vmcs>> vmx_sync_vmcs_host_state(vmx, prev);
+ */
 static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 				     struct loaded_vmcs *prev)
 {
@@ -282,6 +399,13 @@ static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 	src = &prev->host_state;
 	dest = &vmx->loaded_vmcs->host_state;
 
+	/*
+	 * 在以下使用vmx_set_host_fs_gs():
+	 *   - arch/x86/kvm/vmx/nested.c|285| <<vmx_sync_vmcs_host_state>> vmx_set_host_fs_gs(dest,
+	 *                     src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
+	 *   - arch/x86/kvm/vmx/vmx.c|1303| <<vmx_prepare_switch_to_guest>> vmx_set_host_fs_gs(host_state,
+	 *                     fs_sel, gs_sel, fs_base, gs_base);
+	 */
 	vmx_set_host_fs_gs(dest, src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
 	dest->ldt_sel = src->ldt_sel;
 #ifdef CONFIG_X86_64
@@ -290,6 +414,15 @@ static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 #endif
 }
 
+/*
+ * 在以下使用vmx_switch_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|337| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3587| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/nested.c|3593| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3598| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3681| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|5083| <<__nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ */
 static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -302,12 +435,28 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 	cpu = get_cpu();
 	prev = vmx->loaded_vmcs;
 	vmx->loaded_vmcs = vmcs;
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 	vmx_sync_vmcs_host_state(vmx, prev);
 	put_cpu();
 
 	vcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	/*
 	 * All lazily updated registers will be reloaded from VMCS12 on both
 	 * vmentry and vmexit.
@@ -315,11 +464,23 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 	vcpu->arch.regs_dirty = 0;
 }
 
+/*
+ * 在以下使用nested_put_vmcs12_pages():
+ *   - arch/x86/kvm/vmx/nested.c|361| <<free_nested>> nested_put_vmcs12_pages(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5107| <<__nested_vmx_vmexit>> nested_put_vmcs12_pages(vcpu);
+ */
 static void nested_put_vmcs12_pages(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
 	kvm_vcpu_unmap(vcpu, &vmx->nested.apic_access_page_map);
+	/*
+	 * 在以下使用nested_vmx->virtual_apic_map:
+	 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+	 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+	 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+	 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+	 */
 	kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
 	kvm_vcpu_unmap(vcpu, &vmx->nested.pi_desc_map);
 	vmx->nested.pi_desc = NULL;
@@ -329,6 +490,11 @@ static void nested_put_vmcs12_pages(struct kvm_vcpu *vcpu)
  * Free whatever needs to be freed from vmx->nested when L1 goes down, or
  * just stops using VMX.
  */
+/*
+ * 在以下使用free_nested():
+ *   - arch/x86/kvm/vmx/nested.c|5538| <<handle_vmxoff>> free_nested(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6806| <<vmx_leave_nested>> free_nested(vcpu);
+ */
 static void free_nested(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -345,9 +511,43 @@ static void free_nested(struct kvm_vcpu *vcpu)
 	vmx->nested.smm.vmxon = false;
 	vmx->nested.vmxon_ptr = INVALID_GPA;
 	free_vpid(vmx->nested.vpid02);
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	vmx->nested.posted_intr_nv = -1;
 	vmx->nested.current_vmptr = INVALID_GPA;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用vmx_disable_shadow_vmcs():
+		 *   - arch/x86/kvm/vmx/nested.c|498| <<free_nested>> vmx_disable_shadow_vmcs(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+		 */
 		vmx_disable_shadow_vmcs(vmx);
 		vmcs_clear(vmx->vmcs01.shadow_vmcs);
 		free_vmcs(vmx->vmcs01.shadow_vmcs);
@@ -371,6 +571,10 @@ static void free_nested(struct kvm_vcpu *vcpu)
  * Ensure that the current vmcs of the logical processor is the
  * vmcs01 of the vcpu before calling free_nested().
  */
+/*
+ * 在以下使用nested_vmx_free_vcpu():
+ *   - arch/x86/kvm/vmx/vmx.c|8131| <<vmx_vcpu_free>> nested_vmx_free_vcpu(vcpu);
+ */
 void nested_vmx_free_vcpu(struct kvm_vcpu *vcpu)
 {
 	vcpu_load(vcpu);
@@ -461,6 +665,11 @@ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 	vmcs12->guest_physical_address = fault->address;
 }
 
+/*
+ * 在以下使用nested_ept_new_eptp():
+ *   - arch/x86/kvm/vmx/nested.c|627| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|7013| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+ */
 static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -472,16 +681,34 @@ static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 				nested_ept_get_eptp(vcpu));
 }
 
+/*
+ * 在以下使用nested_ept_init_mmu_context():
+ *   - arch/x86/kvm/vmx/nested.c|3098| <<prepare_vmcs02>> nested_ept_init_mmu_context(vcpu);
+ */
 static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(mmu_is_nested(vcpu));
 
 	vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	/*
+	 * 在以下使用nested_ept_new_eptp():
+	 *   - arch/x86/kvm/vmx/nested.c|627| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7013| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+	 */
 	nested_ept_new_eptp(vcpu);
 	vcpu->arch.mmu->get_guest_pgd     = nested_ept_get_eptp;
 	vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
 	vcpu->arch.mmu->get_pdptr         = kvm_pdptr_read;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_mmu *mmu;
+	 *    -> struct kvm_mmu root_mmu;
+	 *    -> struct kvm_mmu guest_mmu;
+	 *    -> struct kvm_mmu nested_mmu;
+	 *    -> struct kvm_mmu *walk_mmu;
+	 */
 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
 }
 
@@ -563,6 +790,21 @@ static int nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,
  * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
  * only the "disable intercept" case needs to be handled.
  */
+/*
+ * 在以下使用nested_vmx_disable_intercept_for_x2apic_msr():
+ *   - arch/x86/kvm/vmx/nested.c|828| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_disable_intercept_for_x2apic_msr(
+ *                   msr_bitmap_l1, msr_bitmap_l0, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_R | MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/nested.c|844| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_disable_intercept_for_x2apic_msr(
+ *                   msr_bitmap_l1, msr_bitmap_l0, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/nested.c|848| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_disable_intercept_for_x2apic_msr(
+ *                   msr_bitmap_l1, msr_bitmap_l0, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
+ *
+ * 注释:
+ * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1
+ * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
+ * only the "disable intercept" case needs to be handled.
+ * L1不trap的话, L0才不trap
+ */
 static void nested_vmx_disable_intercept_for_x2apic_msr(unsigned long *msr_bitmap_l1,
 							unsigned long *msr_bitmap_l0,
 							u32 msr, int type)
@@ -570,10 +812,17 @@ static void nested_vmx_disable_intercept_for_x2apic_msr(unsigned long *msr_bitma
 	if (type & MSR_TYPE_R && !vmx_test_msr_bitmap_read(msr_bitmap_l1, msr))
 		vmx_clear_msr_bitmap_read(msr_bitmap_l0, msr);
 
+	/*
+	 * L1不trap的话, L0才不trap
+	 */
 	if (type & MSR_TYPE_W && !vmx_test_msr_bitmap_write(msr_bitmap_l1, msr))
 		vmx_clear_msr_bitmap_write(msr_bitmap_l0, msr);
 }
 
+/*
+ * 在以下使用enable_x2apic_msr_intercepts():
+ *   - arch/x86/kvm/vmx/nested.c|718| <<nested_vmx_prepare_msr_bitmap>> enable_x2apic_msr_intercepts(msr_bitmap_l0);
+ */
 static inline void enable_x2apic_msr_intercepts(unsigned long *msr_bitmap)
 {
 	int msr;
@@ -601,6 +850,17 @@ void nested_vmx_set_msr_##rw##_intercept(struct vcpu_vmx *vmx,			\
 BUILD_NVMX_MSR_INTERCEPT_HELPER(read)
 BUILD_NVMX_MSR_INTERCEPT_HELPER(write)
 
+/*
+ * 在以下使用nested_vmx_set_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/nested.c|757| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|760| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|763| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|766| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|769| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|772| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|775| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|778| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ */
 static inline void nested_vmx_set_intercept_for_msr(struct vcpu_vmx *vmx,
 						    unsigned long *msr_bitmap_l1,
 						    unsigned long *msr_bitmap_l0,
@@ -618,6 +878,10 @@ static inline void nested_vmx_set_intercept_for_msr(struct vcpu_vmx *vmx,
  * Merge L0's and L1's MSR bitmap, return false to indicate that
  * we do not use the hardware.
  */
+/*
+ * 在以下使用nested_vmx_prepare_msr_bitmap():
+ *   - arch/x86/kvm/vmx/nested.c|3530| <<nested_get_vmcs12_pages>> if (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))
+ */
 static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 						 struct vmcs12 *vmcs12)
 {
@@ -658,8 +922,19 @@ static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 	 * 4-byte writes on 32-bit systems) up front to enable intercepts for
 	 * the x2APIC MSR range and selectively toggle those relevant to L2.
 	 */
+	/*
+	 * 首先把全部APIC的MSR都intercept了
+	 */
 	enable_x2apic_msr_intercepts(msr_bitmap_l0);
 
+	/*
+	 * 在以下使用nested_cpu_has_virt_x2apic_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|831| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|1034| <<nested_vmx_check_apicv_controls>> if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|1044| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *
+	 * 如果L1支持L2的APIC虚拟化
+	 */
 	if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
 		if (nested_cpu_has_apic_reg_virt(vmcs12)) {
 			/*
@@ -675,12 +950,36 @@ static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 			}
 		}
 
+		/*
+		 * 注释:
+		 * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1
+		 * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
+		 * only the "disable intercept" case needs to be handled.
+		 * L1不trap的话, L0才不trap
+		 */
 		nested_vmx_disable_intercept_for_x2apic_msr(
 			msr_bitmap_l1, msr_bitmap_l0,
 			X2APIC_MSR(APIC_TASKPRI),
 			MSR_TYPE_R | MSR_TYPE_W);
 
+		/*
+		 * 在以下使用nested_cpu_has_vid():
+		 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+		 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+		 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+		 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+		 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+		 */
 		if (nested_cpu_has_vid(vmcs12)) {
+			/*
+			 * 注释:
+			 * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1
+			 * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
+			 * only the "disable intercept" case needs to be handled.
+			 * L1不trap的话, L0才不trap
+			 */
 			nested_vmx_disable_intercept_for_x2apic_msr(
 				msr_bitmap_l1, msr_bitmap_l0,
 				X2APIC_MSR(APIC_EOI),
@@ -728,10 +1027,22 @@ static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * 在以下使用nested_cache_shadow_vmcs12():
+ * -> arch/x86/kvm/vmx/nested.c|4294| <<nested_vmx_run>> nested_cache_shadow_vmcs12(vcpu, vmcs12);
+ */
 static void nested_cache_shadow_vmcs12(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	struct gfn_to_hva_cache *ghc = &vmx->nested.shadow_vmcs12_cache;
 
 	if (!nested_cpu_has_shadow_vmcs(vmcs12) ||
@@ -743,10 +1054,28 @@ static void nested_cache_shadow_vmcs12(struct kvm_vcpu *vcpu,
 				      vmcs12->vmcs_link_pointer, VMCS12_SIZE))
 		return;
 
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	kvm_read_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),
 			      VMCS12_SIZE);
 }
 
+/*
+ * 在以下使用nested_flush_cached_shadow_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|5898| <<__nested_vmx_vmexit>> nested_flush_cached_shadow_vmcs12(vcpu, vmcs12);
+ */
 static void nested_flush_cached_shadow_vmcs12(struct kvm_vcpu *vcpu,
 					      struct vmcs12 *vmcs12)
 {
@@ -762,6 +1091,20 @@ static void nested_flush_cached_shadow_vmcs12(struct kvm_vcpu *vcpu,
 				      vmcs12->vmcs_link_pointer, VMCS12_SIZE))
 		return;
 
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	kvm_write_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),
 			       VMCS12_SIZE);
 }
@@ -786,9 +1129,28 @@ static int nested_vmx_check_apic_access_controls(struct kvm_vcpu *vcpu,
 		return 0;
 }
 
+/*
+ * 在以下使用nested_vmx_check_apicv_controls():
+ *   - arch/x86/kvm/vmx/nested.c|3026| <<nested_check_vm_execution_controls>> nested_vmx_check_apicv_controls(vcpu, vmcs12) ||
+ */
 static int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,
 					   struct vmcs12 *vmcs12)
 {
+	/*
+	 * 在以下使用nested_cpu_has_virt_x2apic_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|831| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|1034| <<nested_vmx_check_apicv_controls>> if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|1044| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *
+	 * 在以下使用nested_cpu_has_vid():
+	 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+	 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+	 */
 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
 	    !nested_cpu_has_apic_reg_virt(vmcs12) &&
 	    !nested_cpu_has_vid(vmcs12) &&
@@ -934,6 +1296,27 @@ static int nested_vmx_check_shadow_vmcs_controls(struct kvm_vcpu *vcpu,
 static int nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,
 				       struct vmx_msr_entry *e)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	/* x2APIC MSR accesses are not allowed */
 	if (CC(vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8))
 		return -EINVAL;
@@ -974,6 +1357,11 @@ static int nested_vmx_store_msr_check(struct kvm_vcpu *vcpu,
  * as possible, process all valid entries before failing rather than precheck
  * for a capacity violation.
  */
+/*
+ * 在以下使用nested_vmx_load_msr():
+ *   - arch/x86/kvm/vmx/nested.c|3719| <<nested_vmx_enter_non_root_mode>> failed_index = nested_vmx_load_msr(vcpu,
+ *   - arch/x86/kvm/vmx/nested.c|4993| <<load_vmcs12_host_state>> if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
+ */
 static u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)
 {
 	u32 i;
@@ -1010,6 +1398,10 @@ static u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)
 	return i + 1;
 }
 
+/*
+ * 在以下使用nested_vmx_get_vmexit_msr_value():
+ *   - arch/x86/kvm/vmx/nested.c|1378| <<nested_vmx_store_msr>> if (!nested_vmx_get_vmexit_msr_value(vcpu, e.index, &data))
+ */
 static bool nested_vmx_get_vmexit_msr_value(struct kvm_vcpu *vcpu,
 					    u32 msr_index,
 					    u64 *data)
@@ -1109,6 +1501,10 @@ static bool nested_msr_store_list_has_msr(struct kvm_vcpu *vcpu, u32 msr_index)
 	return false;
 }
 
+/*
+ * 在以下使用prepare_vmx_msr_autostore_list():
+ *   - arch/x86/kvm/vmx/nested.c|3012| <<prepare_vmcs02_rare>> prepare_vmx_msr_autostore_list(&vmx->vcpu, MSR_IA32_TSC);
+ */
 static void prepare_vmx_msr_autostore_list(struct kvm_vcpu *vcpu,
 					   u32 msr_index)
 {
@@ -1151,6 +1547,11 @@ static void prepare_vmx_msr_autostore_list(struct kvm_vcpu *vcpu,
  * Exit Qualification (for a VM-Entry consistency check VM-Exit) is assigned to
  * @entry_failure_code.
  */
+/*
+ * 在以下使用nested_vmx_load_cr3():
+ *   - arch/x86/kvm/vmx/nested.c|3131| <<prepare_vmcs02>> if (nested_vmx_load_cr3(vcpu, vmcs12->guest_cr3, nested_cpu_has_ept(vmcs12),
+ *   - arch/x86/kvm/vmx/nested.c|5440| <<load_vmcs12_host_state>> if (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, true, &ignored))
+ */
 static int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,
 			       bool nested_ept, bool reload_pdptrs,
 			       enum vm_entry_failure_code *entry_failure_code)
@@ -1268,6 +1669,10 @@ static bool is_bitwise_subset(u64 superset, u64 subset, u64 mask)
 	return (superset | subset) == superset;
 }
 
+/*
+ * 处理MSR_IA32_VMX_BASIC:
+ *   - arch/x86/kvm/vmx/nested.c|1772| <<vmx_set_vmx_msr(MSR_IA32_VMX_BASIC)>> return vmx_restore_vmx_basic(vmx, data);
+ */
 static int vmx_restore_vmx_basic(struct vcpu_vmx *vmx, u64 data)
 {
 	const u64 feature_bits = VMX_BASIC_DUAL_MONITOR_TREATMENT |
@@ -1456,6 +1861,10 @@ static int vmx_restore_fixed0_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)
  *
  * Returns 0 on success, non-0 otherwise.
  */
+/*
+ * 处理"KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR":
+ *   - arch/x86/kvm/vmx/vmx.c|2397| <<vmx_set_msr>> return vmx_set_vmx_msr(vcpu, msr_index, data);
+ */
 int vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1606,6 +2015,12 @@ int vmx_get_vmx_msr(struct nested_vmx_msrs *msrs, u32 msr_index, u64 *pdata)
  * VM-exit information fields (which are actually writable if the vCPU is
  * configured to support "VMWRITE to any supported field in the VMCS").
  */
+/*
+ * 在以下使用copy_shadow_to_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+ *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+ *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+ */
 static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 {
 	struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
@@ -1619,6 +2034,16 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 
 	preempt_disable();
 
+	/*
+	 * 在以下使用vmcs_load():
+	 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 */
 	vmcs_load(shadow_vmcs);
 
 	for (i = 0; i < max_shadow_read_write_fields; i++) {
@@ -1633,6 +2058,10 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 	preempt_enable();
 }
 
+/*
+ * 在以下使用copy_vmcs12_to_shadow():
+ *   - arch/x86/kvm/vmx/nested.c|2517| <<nested_sync_vmcs12_to_shadow>> copy_vmcs12_to_shadow(vmx);
+ */
 static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 {
 	const struct shadow_vmcs_field *fields[] = {
@@ -1652,6 +2081,16 @@ static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 	if (WARN_ON(!shadow_vmcs))
 		return;
 
+	/*
+	 * 在以下使用vmcs_load():
+	 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 */
 	vmcs_load(shadow_vmcs);
 
 	for (q = 0; q < ARRAY_SIZE(fields); q++) {
@@ -2187,6 +2626,10 @@ static enum nested_evmptrld_status nested_vmx_handle_enlightened_vmptrld(
 #endif
 }
 
+/*
+ * 在以下使用nested_sync_vmcs12_to_shadow():
+ *   - arch/x86/kvm/vmx/vmx.c|1271| <<vmx_prepare_switch_to_guest>> nested_sync_vmcs12_to_shadow(vcpu);
+ */
 void nested_sync_vmcs12_to_shadow(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -2196,6 +2639,19 @@ void nested_sync_vmcs12_to_shadow(struct kvm_vcpu *vcpu)
 	else
 		copy_vmcs12_to_shadow(vmx);
 
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	vmx->nested.need_vmcs12_to_shadow_sync = false;
 }
 
@@ -2327,6 +2783,10 @@ static void prepare_vmcs02_constant_state(struct vcpu_vmx *vmx)
 	vmx_set_constant_host_state(vmx);
 }
 
+/*
+ * 在以下使用prepare_vmcs02_early_rare():
+ *   - arch/x86/kvm/vmx/nested.c|2687| <<prepare_vmcs02_early>> prepare_vmcs02_early_rare(vmx, vmcs12);
+ */
 static void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,
 				      struct vmcs12 *vmcs12)
 {
@@ -2353,6 +2813,10 @@ static void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,
 	}
 }
 
+/*
+ * 在以下使用prepare_vmcs02_early():
+ *   - arch/x86/kvm/vmx/nested.c|3684| <<nested_vmx_enter_non_root_mode>> prepare_vmcs02_early(vmx, &vmx->vmcs01, vmcs12);
+ */
 static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs01,
 				 struct vmcs12 *vmcs12)
 {
@@ -2369,9 +2833,29 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 	exec_control |= (vmcs12->pin_based_vm_exec_control &
 			 ~PIN_BASED_VMX_PREEMPTION_TIMER);
 
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	/* Posted interrupts setting is only taken from vmcs12.  */
 	vmx->nested.pi_pending = false;
 	if (nested_cpu_has_posted_intr(vmcs12)) {
+		/*
+		 * 在以下使用nested_vmx->posted_intr_nv:
+		 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+		 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+		 *                                               vector == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+		 */
 		vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
 	} else {
 		vmx->nested.posted_intr_nv = -1;
@@ -2450,6 +2934,32 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 		if (vmx_umip_emulated() && (vmcs12->guest_cr4 & X86_CR4_UMIP))
 			exec_control |= SECONDARY_EXEC_DESC;
 
+		/*
+		 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+		 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+		 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+		 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+		 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+		 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+		 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+		 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+		 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+		 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+		 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+		 */
 		if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
 			vmcs_write16(GUEST_INTR_STATUS,
 				vmcs12->guest_intr_status);
@@ -2520,6 +3030,10 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 	}
 }
 
+/*
+ * 在以下使用prepare_vmcs02_rare():
+ *   - arch/x86/kvm/vmx/nested.c|2870| <<prepare_vmcs02>> prepare_vmcs02_rare(vmx, vmcs12);
+ */
 static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 {
 	struct hv_enlightened_vmcs *hv_evmcs = nested_vmx_evmcs(vmx);
@@ -2619,6 +3133,11 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 		vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, vmcs12->page_fault_error_code_match);
 	}
 
+	/*在以下使用cpu_has_vmx_apicv():
+	 *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+	 *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+	 */
 	if (cpu_has_vmx_apicv()) {
 		vmcs_write64(EOI_EXIT_BITMAP0, vmcs12->eoi_exit_bitmap0);
 		vmcs_write64(EOI_EXIT_BITMAP1, vmcs12->eoi_exit_bitmap1);
@@ -2650,6 +3169,10 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * 在以下使用prepare_vmcs02():
+ *   - arch/x86/kvm/vmx/nested.c|3712| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
@@ -2852,6 +3375,10 @@ static bool nested_vmx_check_eptp(struct kvm_vcpu *vcpu, u64 new_eptp)
 /*
  * Checks related to VM-Execution Control Fields
  */
+/*
+ * 在以下使用nested_check_vm_execution_controls():
+ *  - arch/x86/kvm/vmx/nested.c|3153| <<nested_vmx_check_controls>> if (nested_check_vm_execution_controls(vcpu, vmcs12) ||
+ */
 static int nested_check_vm_execution_controls(struct kvm_vcpu *vcpu,
                                               struct vmcs12 *vmcs12)
 {
@@ -3000,6 +3527,11 @@ static int nested_check_vm_entry_controls(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 在以下使用nested_vmx_check_controls():
+ *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+ *   - arch/x86/kvm/vmx/nested.c|7231| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+ */
 static int nested_vmx_check_controls(struct kvm_vcpu *vcpu,
 				     struct vmcs12 *vmcs12)
 {
@@ -3325,6 +3857,11 @@ static bool nested_get_evmcs_page(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * 在以下使用nested_get_vmcs12_pages():
+ *   - arch/x86/kvm/vmx/nested.c|3662| <<vmx_get_nested_state_pages>> if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|3815| <<nested_vmx_enter_non_root_mode>> if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
+ */
 static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -3347,6 +3884,13 @@ static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 		map = &vmx->nested.apic_access_page_map;
 
 		if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->apic_access_addr), map)) {
+			/*
+			 * 在以下修改APIC_ACCESS_ADDR:
+			 *   - arch/x86/kvm/vmx/nested.c|3887| <<nested_get_vmcs12_pages>>
+			 *                   vmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(map->pfn));
+			 *   - arch/x86/kvm/vmx/vmx.c|7430| <<vmx_set_apic_access_page_addr>>
+			 *                   vmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(pfn));
+			 */
 			vmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(map->pfn));
 		} else {
 			pr_debug_ratelimited("%s: no backing for APIC-access address in vmcs12\n",
@@ -3360,8 +3904,23 @@ static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 	}
 
 	if (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {
+		/*
+		 * 在以下使用nested_vmx->virtual_apic_map:
+		 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+		 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+		 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+		 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+		 */
 		map = &vmx->nested.virtual_apic_map;
 
+		/*
+		 * 在以下使用VIRTUAL_APIC_PAGE_ADDR:
+		 *   - arch/x86/kvm/vmx/nested.c|3488| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
+		 *   - arch/x86/kvm/vmx/nested.c|3506| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);
+		 *   - arch/x86/kvm/vmx/vmx.c|4800| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|4802| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, __pa(vmx->vcpu.arch.apic->regs));
+		 *   - arch/x86/kvm/vmx/vmx.c|6482| <<dump_vmcs>> pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
+		 */
 		if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->virtual_apic_page_addr), map)) {
 			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
 		} else if (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING) &&
@@ -3434,12 +3993,25 @@ static bool vmx_get_nested_state_pages(struct kvm_vcpu *vcpu)
 	}
 #endif
 
+	/*
+	 * 在以下使用nested_get_vmcs12_pages():
+	 *   - arch/x86/kvm/vmx/nested.c|3662| <<vmx_get_nested_state_pages>> if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|3815| <<nested_vmx_enter_non_root_mode>> if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
+	 */
 	if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
 		return false;
 
 	return true;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->write_log_dirty:
+ *   - arch/x86/kvm/vmx/nested.c|8462| <<global>> .write_log_dirty = nested_vmx_write_pml_buffer,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|225| <<FNAME(update_accessed_dirty_bits)>> if (kvm_x86_ops.nested_ops->write_log_dirty(vcpu, addr))
+ *
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.write_log_dirty = nested_vmx_write_pml_buffer,
+ */
 static int nested_vmx_write_pml_buffer(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	struct vmcs12 *vmcs12;
@@ -3512,6 +4084,12 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
  *	NVMX_VMENTRY_VMEXIT:  Consistency check VMExit
  *	NVMX_VMENTRY_KVM_INTERNAL_ERROR: KVM internal error
  */
+/*
+ * 在以下使用nested_vmx_enter_non_root_mode():
+ *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ */
 enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 							bool from_vmentry)
 {
@@ -3563,11 +4141,28 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	if (!enable_ept && !nested_early_check)
 		vmcs_writel(GUEST_CR3, vcpu->arch.cr3);
 
+	/*
+	 * 在以下使用vmx_switch_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|337| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3587| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|3593| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3598| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3681| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|5083| <<__nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
 
+	/*
+	 * 只在这里调用
+	 */
 	prepare_vmcs02_early(vmx, &vmx->vmcs01, vmcs12);
 
 	if (from_vmentry) {
+		/*
+		 * 在以下使用nested_get_vmcs12_pages():
+		 *   - arch/x86/kvm/vmx/nested.c|3662| <<vmx_get_nested_state_pages>> if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|3815| <<nested_vmx_enter_non_root_mode>> if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
+		 */
 		if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
 			vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 			return NVMX_VMENTRY_KVM_INTERNAL_ERROR;
@@ -3586,8 +4181,16 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 		}
 	}
 
+	/*
+	 * 在以下使用enter_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 */
 	enter_guest_mode(vcpu);
 
+	/*
+	 * 只在这里调用
+	 */
 	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
 		exit_reason.basic = EXIT_REASON_INVALID_STATE;
 		vmcs12->exit_qualification = entry_failure_code;
@@ -3595,6 +4198,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	}
 
 	if (from_vmentry) {
+		/*
+		 * 在以下使用nested_vmx_load_msr():
+		 *   - arch/x86/kvm/vmx/nested.c|3719| <<nested_vmx_enter_non_root_mode>> failed_index = nested_vmx_load_msr(vcpu,
+		 *   - arch/x86/kvm/vmx/nested.c|4993| <<load_vmcs12_host_state>> if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
+		 */
 		failed_index = nested_vmx_load_msr(vcpu,
 						   vmcs12->vm_entry_msr_load_addr,
 						   vmcs12->vm_entry_msr_load_count);
@@ -3654,6 +4262,13 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 vmentry_fail_vmexit_guest_mode:
 	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETTING)
 		vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	/*
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	leave_guest_mode(vcpu);
 
 vmentry_fail_vmexit:
@@ -3662,8 +4277,43 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	if (!from_vmentry)
 		return NVMX_VMENTRY_VMEXIT;
 
+	/*
+	 * 在以下使用load_vmcs12_host_state():
+	 *   - arch/x86/kvm/vmx/nested.c|4213| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|6031| <<__nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+	 */
 	load_vmcs12_host_state(vcpu, vmcs12);
 	vmcs12->vm_exit_reason = exit_reason.full;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 *
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
 		vmx->nested.need_vmcs12_to_shadow_sync = true;
 	return NVMX_VMENTRY_VMEXIT;
@@ -3673,6 +4323,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
  * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1
  * for running an L2 nested guest.
  */
+/*
+ * 在以下使用nested_vmx_run():
+ *   - arch/x86/kvm/vmx/nested.c|5591| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|5598| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+ */
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 {
 	struct vmcs12 *vmcs12;
@@ -3690,6 +4345,13 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用kvm_pmu_trigger_event():
+	 *   - arch/x86/kvm/vmx/nested.c|3714| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+	 *   - arch/x86/kvm/x86.c|9106| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+	 *   - arch/x86/kvm/x86.c|9445| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+	 *   - arch/x86/kvm/x86.c|9447| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+	 */
 	kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
 
 	if (CC(evmptrld_status == EVMPTRLD_VMFAIL))
@@ -3717,6 +4379,30 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 		/* Enlightened VMCS doesn't have launch state */
 		vmcs12->launch_state = !launch;
 	} else if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用enable_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+		 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+		 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+		 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+		 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+		 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+		 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+		 *
+		 *
+		 * 在以下使用copy_shadow_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+		 */
 		copy_shadow_to_vmcs12(vmx);
 	}
 
@@ -3738,6 +4424,11 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 			launch ? VMXERR_VMLAUNCH_NONCLEAR_VMCS
 			       : VMXERR_VMRESUME_NONLAUNCHED_VMCS);
 
+	/*
+	 * 在以下使用nested_vmx_check_controls():
+	 *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+	 *   - arch/x86/kvm/vmx/nested.c|7231| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+	 */
 	if (nested_vmx_check_controls(vcpu, vmcs12))
 		return nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
 
@@ -3753,6 +4444,13 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	 */
 	vmx->nested.nested_run_pending = 1;
 	vmx->nested.has_preemption_timer_deadline = false;
+	/*
+	 * 在以下使用nested_vmx_enter_non_root_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 *   - arch/x86/kvm/vmx/nested.h|26| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+	 *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 */
 	status = nested_vmx_enter_non_root_mode(vcpu, true);
 	if (unlikely(status != NVMX_VMENTRY_SUCCESS))
 		goto vmentry_failed;
@@ -3844,6 +4542,11 @@ vmcs12_guest_cr4(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 			vcpu->arch.cr4_guest_owned_bits));
 }
 
+/*
+ * 在以下使用vmcs12_save_pending_event():
+ *   - arch/x86/kvm/vmx/nested.c|5357| <<prepare_vmcs12>> vmcs12_save_pending_event(vcpu, vmcs12,
+ *           vm_exit_reason, exit_intr_info);
+ */
 static void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,
 				      struct vmcs12 *vmcs12,
 				      u32 vm_exit_reason, u32 exit_intr_info)
@@ -3911,6 +4614,11 @@ static void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,
 }
 
 
+/*
+ * 在以下使用nested_mark_vmcs12_pages_dirty():
+ *   - arch/x86/kvm/vmx/nested.c|4391| <<vmx_complete_nested_posted_interrupt>> nested_mark_vmcs12_pages_dirty(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6870| <<__vmx_handle_exit>> nested_mark_vmcs12_pages_dirty(vcpu);
+ */
 void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -3932,6 +4640,10 @@ void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用vmx_complete_nested_posted_interrupt():
+ *   - arch/x86/kvm/vmx/nested.c|4875| <<vmx_check_nested_events>> return vmx_complete_nested_posted_interrupt(vcpu);
+ */
 static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3939,6 +4651,15 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 	void *vapic_page;
 	u16 status;
 
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	if (!vmx->nested.pi_pending)
 		return 0;
 
@@ -3952,10 +4673,22 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 
 	max_irr = pi_find_highest_vector(vmx->nested.pi_desc);
 	if (max_irr > 0) {
+		/*
+		 * 在以下使用nested_vmx->virtual_apic_map:
+		 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+		 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+		 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+		 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+		 */
 		vapic_page = vmx->nested.virtual_apic_map.hva;
 		if (!vapic_page)
 			goto mmio_needed;
 
+		/*
+		 * 在以下使用__kvm_apic_update_irr():
+		 *   - arch/x86/kvm/lapic.c|791| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+		 *   - arch/x86/kvm/vmx/nested.c|4381| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+		 */
 		__kvm_apic_update_irr(vmx->nested.pi_desc->pir,
 			vapic_page, &max_irr);
 		status = vmcs_read16(GUEST_INTR_STATUS);
@@ -3966,6 +4699,11 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 在以下使用nested_mark_vmcs12_pages_dirty():
+	 *   - arch/x86/kvm/vmx/nested.c|4391| <<vmx_complete_nested_posted_interrupt>> nested_mark_vmcs12_pages_dirty(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6870| <<__vmx_handle_exit>> nested_mark_vmcs12_pages_dirty(vcpu);
+	 */
 	nested_mark_vmcs12_pages_dirty(vcpu);
 	return 0;
 
@@ -3974,6 +4712,11 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 	return -ENXIO;
 }
 
+/*
+ * 在以下使用nested_vmx_inject_exception_vmexit():
+ *   - arch/x86/kvm/vmx/nested.c|4797| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4820| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+ */
 static void nested_vmx_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;
@@ -4082,9 +4825,26 @@ static bool nested_vmx_preemption_timer_pending(struct kvm_vcpu *vcpu)
 	       to_vmx(vcpu)->nested.preemption_timer_expired;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->has_events:
+ *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+ *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+ *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+ *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+ *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.has_events = vmx_has_nested_events,
+ */
 static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * 在以下使用nested_vmx->virtual_apic_map:
+	 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+	 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+	 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+	 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+	 */
 	void *vapic = vmx->nested.virtual_apic_map.hva;
 	int max_irr, vppr;
 
@@ -4101,6 +4861,16 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
 	if (for_injection)
 		return false;
 
+	/*
+	 * 在以下使用nested_cpu_has_vid():
+	 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+	 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+	 */
 	if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
 	    __vmx_interrupt_blocked(vcpu))
 		return false;
@@ -4114,6 +4884,15 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
 	if ((max_irr & 0xf0) > (vppr & 0xf0))
 		return true;
 
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
 	    pi_test_on(vmx->nested.pi_desc)) {
 		max_irr = pi_find_highest_vector(vmx->nested.pi_desc);
@@ -4207,6 +4986,17 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
  *     delivery of a virtual interrupt; delivery of a virtual interrupt takes
  *     priority over external interrupts and lower priority events.
  */
+ /*
+  * 在以下使用kvm_check_nested_events():
+  *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+  *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+  *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+  *
+  * 在以下使用kvm_x86_nested_ops->check_events:
+  *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+  *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+  *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+  */
 static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -4281,6 +5071,11 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		if (block_nested_exceptions)
 			return -EBUSY;
 
+		/*
+		 * 在以下使用nested_vmx_inject_exception_vmexit():
+		 *   - arch/x86/kvm/vmx/nested.c|4797| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4820| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 */
 		nested_vmx_inject_exception_vmexit(vcpu);
 		return 0;
 	}
@@ -4304,6 +5099,11 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		if (block_nested_exceptions)
 			return -EBUSY;
 
+		/*
+		 * 在以下使用nested_vmx_inject_exception_vmexit():
+		 *   - arch/x86/kvm/vmx/nested.c|4797| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4820| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 */
 		nested_vmx_inject_exception_vmexit(vcpu);
 		return 0;
 	}
@@ -4317,6 +5117,12 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 	if (nested_vmx_preemption_timer_pending(vcpu)) {
 		if (block_nested_events)
 			return -EBUSY;
+		/*
+		 * 注释:
+		 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+		 * and modify vmcs12 to make it see what it would expect to see there if
+		 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+		 */
 		nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
 		return 0;
 	}
@@ -4373,6 +5179,20 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 			return 0;
 		}
 
+		/*
+		 * 在以下使用kvm_apic_has_interrupt():
+		 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+		 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+		 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+		 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+		 *
+		 * 核心思想:
+		 * 1. 判断kvm_apic_present()
+		 * 2. 根据TPR和ISR更新PPR
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+		 */
 		irq = kvm_apic_has_interrupt(vcpu);
 		if (WARN_ON_ONCE(irq < 0))
 			goto no_vmexit;
@@ -4385,6 +5205,17 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		 * through the local APIC trigger posted interrupt processing,
 		 * and enabling posted interrupts requires ACK-on-exit.
 		 */
+		/*
+		 * 在以下使用nested_vmx->posted_intr_nv:
+		 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+		 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+		 *                                               vector == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+		 */
 		if (irq == vmx->nested.posted_intr_nv) {
 			/*
 			 * Nested posted interrupts are delivered via RVI, i.e.
@@ -4394,6 +5225,15 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 			if (block_non_injected_events)
 				return -EBUSY;
 
+			/*
+			 * 在以下使用nested_vmx->pi_pending:
+			 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+			 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+			 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+			 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+			 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+			 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+			 */
 			vmx->nested.pi_pending = true;
 			kvm_apic_clear_irr(vcpu, irq);
 			goto no_vmexit;
@@ -4402,6 +5242,12 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		if (block_nested_events)
 			return -EBUSY;
 
+		/*
+		 * 注释:
+		 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+		 * and modify vmcs12 to make it see what it would expect to see there if
+		 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+		 */
 		nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT,
 				  INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR | irq, 0);
 
@@ -4415,6 +5261,9 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 	}
 
 no_vmexit:
+	/*
+	 * 只在这里调用
+	 */
 	return vmx_complete_nested_posted_interrupt(vcpu);
 }
 
@@ -4479,6 +5328,12 @@ static bool is_vmcs12_ext_field(unsigned long field)
 	return false;
 }
 
+/*
+ * 在以下使用sync_vmcs02_to_vmcs12_rare():
+ *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ */
 static void sync_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
@@ -4521,15 +5376,34 @@ static void sync_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 	vmcs12->guest_pending_dbg_exceptions =
 		vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
 
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
 }
 
+/*
+ * 在以下使用copy_vmcs02_to_vmcs12_rare():
+ *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+ *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+ */
 static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	int cpu;
 
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
 		return;
 
@@ -4538,8 +5412,21 @@ static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 
 	cpu = get_cpu();
 	vmx->loaded_vmcs = &vmx->nested.vmcs02;
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 
+	/*
+	 * 在以下使用sync_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 */
 	sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 
 	vmx->loaded_vmcs = &vmx->vmcs01;
@@ -4553,13 +5440,30 @@ static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
  * VM-entry controls is also updated, since this is really a guest
  * state bit.)
  */
+/*
+ * 在以下使用sync_vmcs02_to_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|5629| <<__nested_vmx_vmexit>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|7521| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+ */
 static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
+	/*
+	 * 在以下使用sync_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 */
 	if (nested_vmx_is_evmptr12_valid(vmx))
 		sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	vmx->nested.need_sync_vmcs02_to_vmcs12_rare =
 		!nested_vmx_is_evmptr12_valid(vmx);
 
@@ -4609,6 +5513,16 @@ static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 
 	vmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);
 
+	/*
+	 * 在以下使用nested_cpu_has_vid():
+	 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+	 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+	 */
 	if (nested_cpu_has_vid(vmcs12))
 		vmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);
 
@@ -4640,6 +5554,10 @@ static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
  * exit-information fields only. Other fields are modified by L1 with VMWRITE,
  * which already writes to vmcs12 directly.
  */
+/*
+ * 在以下使用prepare_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|5046| <<__nested_vmx_vmexit>> prepare_vmcs12(vcpu, vmcs12, vm_exit_reason,
+ */
 static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			   u32 vm_exit_reason, u32 exit_intr_info,
 			   unsigned long exit_qualification, u32 exit_insn_len)
@@ -4679,6 +5597,13 @@ static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 		 * during or after loading the guest state. Since this exit
 		 * does not fall in that category, we need to save the MSRs.
 		 */
+		/*
+		 * 在以下使用nested_vmx_abort():
+		 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+		 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+		 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+		 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+		 */
 		if (nested_vmx_store_msr(vcpu,
 					 vmcs12->vm_exit_msr_store_addr,
 					 vmcs12->vm_exit_msr_store_count))
@@ -4696,6 +5621,11 @@ static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
  * Failures During or After Loading Guest State").
  * This function should be called when the active VMCS is L1's (vmcs01).
  */
+/*
+ * 在以下使用load_vmcs12_host_state():
+ *   - arch/x86/kvm/vmx/nested.c|4213| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|6031| <<__nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+ */
 static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 				   struct vmcs12 *vmcs12)
 {
@@ -4735,6 +5665,13 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	 * Only PDPTE load can fail as the value of cr3 was checked on entry and
 	 * couldn't have changed.
 	 */
+	/*
+	 * 在以下使用nested_vmx_abort():
+	 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 */
 	if (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, true, &ignored))
 		nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
 
@@ -4814,6 +5751,17 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	kvm_set_dr(vcpu, 7, 0x400);
 	vmx_guest_debugctl_write(vcpu, 0);
 
+	/*
+	 * 在以下使用nested_vmx_load_msr():
+	 *   - arch/x86/kvm/vmx/nested.c|3719| <<nested_vmx_enter_non_root_mode>> failed_index = nested_vmx_load_msr(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|4993| <<load_vmcs12_host_state>> if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
+	 *
+	 * 在以下使用nested_vmx_abort():
+	 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 */
 	if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
 				vmcs12->vm_exit_msr_load_count))
 		nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
@@ -4844,6 +5792,10 @@ static inline u64 nested_vmx_get_vmcs01_guest_efer(struct vcpu_vmx *vmx)
 	return kvm_host.efer;
 }
 
+/*
+ * 在以下使用nested_vmx_restore_host_state():
+ *   - arch/x86/kvm/vmx/nested.c|5501| <<__nested_vmx_vmexit>> nested_vmx_restore_host_state(vcpu);
+ */
 static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -4949,6 +5901,13 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 	return;
 
 vmabort:
+	/*
+	 * 在以下使用nested_vmx_abort():
+	 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 */
 	nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
 }
 
@@ -4957,6 +5916,16 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
  * and modify vmcs12 to make it see what it would expect to see there if
  * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
  */
+/*
+ * 在以下使用__nested_vmx_vmexit():
+ *   - arch/x86/kvm/vmx/nested.h|45| <<nested_vmx_vmexit>> __nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
+ *   - arch/x86/kvm/vmx/vmx.c|8211| <<vmx_check_intercept>> __nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
+ *
+ * 注释:
+ * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+ * and modify vmcs12 to make it see what it would expect to see there if
+ * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+ */
 void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 			 u32 exit_intr_info, unsigned long exit_qualification,
 			 u32 exit_insn_len)
@@ -4993,6 +5962,13 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	if (enable_ept && is_pae_paging(vcpu))
 		vmx_ept_load_pdptrs(vcpu);
 
+	/*
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	leave_guest_mode(vcpu);
 
 	if (nested_cpu_has_preemption_timer(vmcs12))
@@ -5005,8 +5981,16 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	}
 
 	if (likely(!vmx->fail)) {
+		/*
+		 * 在以下使用sync_vmcs02_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|5629| <<__nested_vmx_vmexit>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7521| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 */
 		sync_vmcs02_to_vmcs12(vcpu, vmcs12);
 
+		/*
+		 * 只在这里调用prepare_vmcs12()
+		 */
 		if (vm_exit_reason != -1)
 			prepare_vmcs12(vcpu, vmcs12, vm_exit_reason,
 				       exit_intr_info, exit_qualification,
@@ -5045,6 +6029,15 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	kvm_clear_exception_queue(vcpu);
 	kvm_clear_interrupt_queue(vcpu);
 
+	/*
+	 * 在以下使用vmx_switch_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|337| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3587| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|3593| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3598| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3681| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|5083| <<__nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 
 	kvm_nested_vmexit_handle_ibrs(vcpu);
@@ -5059,8 +6052,24 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	if (vmx->nested.l1_tpr_threshold != -1)
 		vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
 
+	/*
+	 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page:
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	if (vmx->nested.change_vmcs01_virtual_apic_mode) {
 		vmx->nested.change_vmcs01_virtual_apic_mode = false;
+		/*
+		 * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+		 *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+		 *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+		 *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+		 *
+		 * 在以下使用vmx_set_virtual_apic_mode():
+		 *   - arch/x86/kvm/vmx/main.c|306| <<vt_set_virtual_apic_mode>> return vmx_set_virtual_apic_mode(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6056| <<__nested_vmx_vmexit>> vmx_set_virtual_apic_mode(vcpu);
+		 */
 		vmx_set_virtual_apic_mode(vcpu);
 	}
 
@@ -5069,23 +6078,137 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 		vmx_update_cpu_dirty_logging(vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_put_vmcs12_pages():
+	 *   - arch/x86/kvm/vmx/nested.c|361| <<free_nested>> nested_put_vmcs12_pages(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5107| <<__nested_vmx_vmexit>> nested_put_vmcs12_pages(vcpu);
+	 */
 	nested_put_vmcs12_pages(vcpu);
 
+	/*
+	 * 在以下使用reload_vmcs01_apic_access_page:
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	if (vmx->nested.reload_vmcs01_apic_access_page) {
 		vmx->nested.reload_vmcs01_apic_access_page = false;
+		/*
+		 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+		 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+		 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+		 *
+		 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+		 */
 		kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->pdate_vmcs01_apicv_status:
+	 *   - arch/x86/kvm/vmx/nested.c|5114| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+	 *   - arch/x86/kvm/vmx/nested.c|5115| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+	 *
+	 * 在__nested_vmx_vmexit()触发KVM_REQ_APICV_UPDATE
+	 *
+	 * vcpu_enter_guest(KVM_REQ_APICV_UPDATE)
+	 * -> kvm_vcpu_update_apicv()
+	 *    -> __kvm_vcpu_update_apicv()
+	 *       -> if (apic->apicv_active == activate) goto out ==> 返回!!!
+	 *       -> vt_refresh_apicv_exec_ctrl()
+	 */
 	if (vmx->nested.update_vmcs01_apicv_status) {
 		vmx->nested.update_vmcs01_apicv_status = false;
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+		 *               
+		 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+		 *
+		 *
+		 * vcpu_enter_guest(KVM_REQ_APICV_UPDATE)
+		 * -> kvm_vcpu_update_apicv()
+		 *    -> __kvm_vcpu_update_apicv()
+		 *       -> if (apic->apicv_active == activate) goto out ==> 返回!!!
+		 *       -> vt_refresh_apicv_exec_ctrl()
+		 */
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
 
+	/*
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *
+	 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+	 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+	 *
+	 * 在__nested_vmx_vmexit()触发kvm_apic_update_hwapic_isr()
+	 */
 	if (vmx->nested.update_vmcs01_hwapic_isr) {
+		/*
+		 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+		 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+		 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+		 *
+		 * 在__nested_vmx_vmexit()触发kvm_apic_update_hwapic_isr()
+		 */
 		vmx->nested.update_vmcs01_hwapic_isr = false;
+		/*
+		 * 只在这里调用
+		 *
+		 * 其实就是vmx_hwapic_isr_update()
+		 */
 		kvm_apic_update_hwapic_isr(vcpu);
 	}
 
+	/*
+	 * 在以下使用enable_shadow_vmcs: 
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 *
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	if ((vm_exit_reason != -1) &&
 	    (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
 		vmx->nested.need_vmcs12_to_shadow_sync = true;
@@ -5102,6 +6225,11 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 						       vmcs12->vm_exit_intr_error_code,
 						       KVM_ISA_VMX);
 
+		/*
+		 * 在以下使用load_vmcs12_host_state():
+		 *   - arch/x86/kvm/vmx/nested.c|4213| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|6031| <<__nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+		 */
 		load_vmcs12_host_state(vcpu, vmcs12);
 
 		/*
@@ -5150,6 +6278,29 @@ static void nested_vmx_triple_fault(struct kvm_vcpu *vcpu)
  * On success, returns 0. When the operand is invalid, returns 1 and throws
  * #UD, #GP, or #SS.
  */
+/*
+ * 在以下使用get_vmx_mem_address():
+ *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+ *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+ *           exit_qualification, instr_info, true, len, &gva))
+ *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+ *           exit_qualification, instr_info, false, len, &gva))
+ *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+ *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+ *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+ *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+ *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+ *
+ * 注释:
+ * Decode the memory-address operand of a vmx instruction, as recorded on an
+ * exit caused by such an instruction (run by a guest hypervisor).
+ * On success, returns 0. When the operand is invalid, returns 1 and throws
+ * #UD, #GP, or #SS.
+ */
 int get_vmx_mem_address(struct kvm_vcpu *vcpu, unsigned long exit_qualification,
 			u32 vmx_instruction_info, bool wr, int len, gva_t *ret)
 {
@@ -5282,6 +6433,29 @@ static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer,
 	struct x86_exception e;
 	int r;
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	if (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),
 				vmcs_read32(VMX_INSTRUCTION_INFO), false,
 				sizeof(*vmpointer), &gva)) {
@@ -5303,6 +6477,10 @@ static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer,
  * VMCS, unless such a shadow VMCS already exists. The newly allocated
  * VMCS is also VMCLEARed, so that it is ready for use.
  */
+/*
+ * 在以下使用alloc_shadow_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|6355| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+ */
 static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5326,6 +6504,11 @@ static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 	return loaded_vmcs->shadow_vmcs;
 }
 
+/*
+ * 在以下使用enter_vmx_operation():
+ *   - arch/x86/kvm/vmx/nested.c|6471| <<handle_vmxon>> ret = enter_vmx_operation(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|8116| <<vmx_set_nested_state>> ret = enter_vmx_operation(vcpu);
+ */
 static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5344,6 +6527,24 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 	if (!vmx->nested.cached_shadow_vmcs12)
 		goto out_cached_shadow_vmcs12;
 
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
 		goto out_shadow_vmcs;
 
@@ -5474,14 +6675,61 @@ static inline void nested_release_vmcs12(struct kvm_vcpu *vcpu)
 	if (vmx->nested.current_vmptr == INVALID_GPA)
 		return;
 
+	/*
+	 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 */
 	copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
 
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用copy_shadow_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+		 */
 		/* copy to memory all shadowed fields in case
 		   they were modified */
 		copy_shadow_to_vmcs12(vmx);
+		/*
+		 * 在以下使用vmx_disable_shadow_vmcs():
+		 *   - arch/x86/kvm/vmx/nested.c|498| <<free_nested>> vmx_disable_shadow_vmcs(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+		 */
 		vmx_disable_shadow_vmcs(vmx);
 	}
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	vmx->nested.posted_intr_nv = -1;
 
 	/* Flush VMCS12 to guest memory */
@@ -5565,6 +6813,20 @@ static int handle_vmresume(struct kvm_vcpu *vcpu)
 
 static int handle_vmread(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu)
 						    : get_vmcs12(vcpu);
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5597,6 +6859,13 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 		if (offset < 0)
 			return nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
 
+		/*
+		 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+		 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 */
 		if (!is_guest_mode(vcpu) && is_vmcs12_ext_field(field))
 			copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 
@@ -5633,6 +6902,29 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 		kvm_register_write(vcpu, (((instr_info) >> 3) & 0xf), value);
 	} else {
 		len = is_64_bit_mode(vcpu) ? 8 : 4;
+		/*
+		 * 在以下使用get_vmx_mem_address():
+		 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, true, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, false, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *
+		 * 注释:
+		 * Decode the memory-address operand of a vmx instruction, as recorded on an
+		 * exit caused by such an instruction (run by a guest hypervisor).
+		 * On success, returns 0. When the operand is invalid, returns 1 and throws
+		 * #UD, #GP, or #SS.
+		 */
 		if (get_vmx_mem_address(vcpu, exit_qualification,
 					instr_info, true, len, &gva))
 			return 1;
@@ -5645,6 +6937,11 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * 在以下使用is_shadow_field_rw():
+ *   - arch/x86/kvm/vmx/nested.c|6833| <<handle_vmwrite>> if (!is_guest_mode(vcpu) && !is_shadow_field_rw(field))
+ *   - arch/x86/kvm/vmx/nested.c|6855| <<handle_vmwrite>> if (!is_guest_mode(vcpu) && !is_shadow_field_rw(field)) {
+ */
 static bool is_shadow_field_rw(unsigned long field)
 {
 	switch (field) {
@@ -5671,6 +6968,20 @@ static bool is_shadow_field_ro(unsigned long field)
 
 static int handle_vmwrite(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu)
 						    : get_vmcs12(vcpu);
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5707,6 +7018,29 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 		value = kvm_register_read(vcpu, (((instr_info) >> 3) & 0xf));
 	else {
 		len = is_64_bit_mode(vcpu) ? 8 : 4;
+		/*
+		 * 在以下使用get_vmx_mem_address():
+		 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, true, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, false, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *
+		 * 注释:
+		 * Decode the memory-address operand of a vmx instruction, as recorded on an
+		 * exit caused by such an instruction (run by a guest hypervisor).
+		 * On success, returns 0. When the operand is invalid, returns 1 and throws
+		 * #UD, #GP, or #SS.
+		 */
 		if (get_vmx_mem_address(vcpu, exit_qualification,
 					instr_info, false, len, &gva))
 			return 1;
@@ -5729,6 +7063,13 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	    !nested_cpu_has_vmwrite_any_field(vcpu))
 		return nested_vmx_fail(vcpu, VMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);
 
+	/*
+	 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 */
 	/*
 	 * Ensure vmcs12 is up-to-date before any VMWRITE that dirties
 	 * vmcs12, else we may crush a field or consume a stale value.
@@ -5756,12 +7097,40 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	 * "dirty" vmcs12, all others go down the prepare_vmcs02() slow path.
 	 */
 	if (!is_guest_mode(vcpu) && !is_shadow_field_rw(field)) {
+		/*
+		 * 在以下使用enable_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+		 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+		 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+		 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+		 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+		 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+		 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+		 */
 		/*
 		 * L1 can read these fields without exiting, ensure the
 		 * shadow VMCS is up-to-date.
 		 */
 		if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
 			preempt_disable();
+			/*
+			 * 在以下使用vmcs_load():
+			 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 */
 			vmcs_load(vmx->vmcs01.shadow_vmcs);
 
 			__vmcs_writel(field, value);
@@ -5776,13 +7145,49 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * 在以下使用set_current_vmptr():
+ *   - arch/x86/kvm/vmx/nested.c|7171| <<handle_vmptrld>> set_current_vmptr(vmx, vmptr);
+ *   - arch/x86/kvm/vmx/nested.c|8390| <<vmx_set_nested_state>> set_current_vmptr(vmx, kvm_state->hdr.vmx.vmcs12_pa);
+ */
 static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 {
 	vmx->nested.current_vmptr = vmptr;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
 		secondary_exec_controls_setbit(vmx, SECONDARY_EXEC_SHADOW_VMCS);
 		vmcs_write64(VMCS_LINK_POINTER,
 			     __pa(vmx->vmcs01.shadow_vmcs));
+		/*
+		 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+		 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+		 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+		 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+		 */
 		vmx->nested.need_vmcs12_to_shadow_sync = true;
 	}
 	vmx->nested.dirty_vmcs12 = true;
@@ -5875,6 +7280,29 @@ static int handle_vmptrst(struct kvm_vcpu *vcpu)
 	if (unlikely(nested_vmx_is_evmptr12_valid(to_vmx(vcpu))))
 		return 1;
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	if (get_vmx_mem_address(vcpu, exit_qual, instr_info,
 				true, sizeof(gpa_t), &gva))
 		return 1;
@@ -5920,6 +7348,29 @@ static int handle_invept(struct kvm_vcpu *vcpu)
 	if (type >= 32 || !(types & (1 << type)))
 		return nested_vmx_fail(vcpu, VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	/* According to the Intel VMX instruction reference, the memory
 	 * operand is read even if it isn't needed (e.g., for type==global)
 	 */
@@ -6003,6 +7454,29 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 		return nested_vmx_fail(vcpu,
 			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	/* according to the intel vmx instruction reference, the memory
 	 * operand is read even if it isn't needed (e.g., for type==global)
 	 */
@@ -6067,6 +7541,10 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * 在以下使用nested_vmx_eptp_switching():
+ *   - arch/x86/kvm/vmx/nested.c|7523| <<handle_vmfunc>> if (nested_vmx_eptp_switching(vcpu, vmcs12))
+ */
 static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 				     struct vmcs12 *vmcs12)
 {
@@ -6091,6 +7569,11 @@ static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 			return 1;
 
 		vmcs12->ept_pointer = new_eptp;
+		/*
+		 * 在以下使用nested_ept_new_eptp():
+		 *   - arch/x86/kvm/vmx/nested.c|627| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|7013| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+		 */
 		nested_ept_new_eptp(vcpu);
 
 		if (!nested_cpu_has_vpid(vmcs12))
@@ -6188,6 +7671,10 @@ bool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,
 	return false;
 }
 
+/*
+ * 在以下使用nested_vmx_exit_handled_io():
+ *   - arch/x86/kvm/vmx/nested.c|7922| <<nested_vmx_l1_wants_exit>> return nested_vmx_exit_handled_io(vcpu, vmcs12);
+ */
 static bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
@@ -6335,6 +7822,11 @@ static bool nested_vmx_exit_handled_encls(struct kvm_vcpu *vcpu,
 	return vmcs12->encls_exiting_bitmap & BIT_ULL(encls_leaf);
 }
 
+/*
+ * 在以下使用nested_vmx_exit_handled_vmcs_access():
+ *   - arch/x86/kvm/vmx/nested.c|7902| <<nested_vmx_l1_wants_exit>> return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12, vmcs12->vmread_bitmap);
+ *   - arch/x86/kvm/vmx/nested.c|7905| <<nested_vmx_l1_wants_exit>> return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12, vmcs12->vmwrite_bitmap);
+ */
 static bool nested_vmx_exit_handled_vmcs_access(struct kvm_vcpu *vcpu,
 	struct vmcs12 *vmcs12, gpa_t bitmap)
 {
@@ -6380,6 +7872,10 @@ static bool nested_vmx_exit_handled_mtf(struct vmcs12 *vmcs12)
  * Return true if L0 wants to handle an exit from L2 regardless of whether or not
  * L1 wants the exit.  Only call this when in is_guest_mode (L2).
  */
+/*
+ * 在以下使用nested_vmx_l0_wants_exit():
+ *   - arch/x86/kvm/vmx/nested.c|7481| <<nested_vmx_reflect_vmexit>> if (nested_vmx_l0_wants_exit(vcpu, exit_reason))
+ */
 static bool nested_vmx_l0_wants_exit(struct kvm_vcpu *vcpu,
 				     union vmx_exit_reason exit_reason)
 {
@@ -6460,6 +7956,10 @@ static bool nested_vmx_l0_wants_exit(struct kvm_vcpu *vcpu,
  * Return 1 if L1 wants to intercept an exit from L2.  Only call this when in
  * is_guest_mode (L2).
  */
+/*
+ * 在以下使用nested_vmx_l1_wants_exit():
+ *   - arch/x86/kvm/vmx/nested.c|7081| <<nested_vmx_reflect_vmexit>> if (!nested_vmx_l1_wants_exit(vcpu, exit_reason))
+ */
 static bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu,
 				     union vmx_exit_reason exit_reason)
 {
@@ -6587,9 +8087,22 @@ static bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu,
  * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
  * reflected into L1.
  */
+/*
+ * 在以下使用nested_vmx_reflect_vmexit():
+ *   - arch/x86/kvm/vmx/vmx.c|6878| <<__vmx_handle_exit>> if (nested_vmx_reflect_vmexit(vcpu))
+ *
+ * 注释:
+ * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
+ * reflected into L1.
+ */
 bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct vcpu_vt vt;
+	 *    -> union vmx_exit_reason exit_reason;
+	 */
 	union vmx_exit_reason exit_reason = vmx->vt.exit_reason;
 	unsigned long exit_qual;
 	u32 exit_intr_info;
@@ -6619,6 +8132,24 @@ bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 	if (!nested_vmx_l1_wants_exit(vcpu, exit_reason))
 		return false;
 
+	/*
+	 * 在以下使用vmx_get_intr_info():
+	 *   - arch/x86/kvm/vmx/nested.c|6163| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->vt.exit_reason.full, vmx_get_intr_info(vcpu),
+	 *   - arch/x86/kvm/vmx/nested.c|6404| <<nested_vmx_l0_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6485| <<nested_vmx_l1_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6642| <<nested_vmx_reflect_vmexit>> exit_intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/tdx.c|1120| <<tdx_handle_exception_nmi>> u32 intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/tdx.c|2167| <<tdx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5140| <<handle_exception_nmi>> intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6104| <<vmx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|6974| <<vmx_handle_exit_irqoff>> handle_exception_irqoff(vcpu, vmx_get_intr_info(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|7016| <<vmx_recover_nmi_blocking>> exit_intr_info = vmx_get_intr_info(&vmx->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7255| <<vmx_handle_nmi>> if (... !is_nmi(vmx_get_intr_info(vcpu)))
+	 *
+	 * VM_EXIT_INTR_INFO:
+	 * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+	 */
 	/*
 	 * vmcs.VM_EXIT_INTR_INFO is only valid for EXCEPTION_NMI exits.  For
 	 * EXTERNAL_INTERRUPT, the value for vmcs12->vm_exit_intr_info would
@@ -6635,6 +8166,12 @@ bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 	exit_qual = vmx_get_exit_qual(vcpu);
 
 reflect_vmexit:
+	/*
+	 * 注释:
+	 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+	 * and modify vmcs12 to make it see what it would expect to see there if
+	 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+	 */
 	nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
 	return true;
 }
@@ -6723,10 +8260,41 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	 * vmcs12 state is in the vmcs12 already.
 	 */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用sync_vmcs02_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|5629| <<__nested_vmx_vmexit>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7521| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 */
 		sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		/*
+		 * 在以下使用sync_vmcs02_to_vmcs12_rare():
+		 *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 */
 		sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 	} else  {
+		/*
+		 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+		 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 */
 		copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		/*
+		 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+		 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+		 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+		 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+		 */
 		if (!vmx->nested.need_vmcs12_to_shadow_sync) {
 			if (nested_vmx_is_evmptr12_valid(vmx))
 				/*
@@ -6739,6 +8307,29 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 				copy_enlightened_to_vmcs12(vmx, 0);
 			else if (enable_shadow_vmcs)
 				copy_shadow_to_vmcs12(vmx);
+			/*
+			 * 在以下使用copy_shadow_to_vmcs12():
+			 *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+			 *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+			 *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+			 *
+			 * 在以下使用enable_shadow_vmcs:
+			 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+			 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+			 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+			 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+			 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+			 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+			 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+			 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+			 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+			 */
 		}
 	}
 
@@ -6754,6 +8345,20 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 
 	if (nested_cpu_has_shadow_vmcs(vmcs12) &&
 	    vmcs12->vmcs_link_pointer != INVALID_GPA) {
+		/*
+		 * 在以下使用get_shadow_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+		 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+		 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+		 */
 		if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
 				 get_shadow_vmcs12(vcpu), VMCS12_SIZE))
 			return -EFAULT;
@@ -6762,6 +8367,13 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	return kvm_state.size;
 }
 
+/*
+ * 在以下使用vmx_leave_nested():
+ *   - arch/x86/kvm/vmx/nested.c|8403| <<global>> struct kvm_x86_nested_ops vmx_nested_ops.leave_nested = vmx_leave_nested,
+ *   - arch/x86/kvm/vmx/nested.c|502| <<nested_vmx_free_vcpu>> vmx_leave_nested(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|7812| <<vmx_set_nested_state>> vmx_leave_nested(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2367| <<vmx_set_msr(MSR_IA32_FEAT_CTL)>> vmx_leave_nested(vcpu);
+ */
 void vmx_leave_nested(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu)) {
@@ -6771,6 +8383,16 @@ void vmx_leave_nested(struct kvm_vcpu *vcpu)
 	free_nested(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->set_state:
+ *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+ *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+ *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+ *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+ *                                     user_kvm_nested_state, &kvm_state);
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.set_state = vmx_set_nested_state,
+ */
 static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state)
@@ -6911,6 +8533,20 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 	ret = -EINVAL;
 	if (nested_cpu_has_shadow_vmcs(vmcs12) &&
 	    vmcs12->vmcs_link_pointer != INVALID_GPA) {
+		/*
+		 * 在以下使用get_shadow_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+		 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+		 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+		 */
 		struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
 
 		if (kvm_state->size <
@@ -6937,6 +8573,11 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 			kvm_state->hdr.vmx.preemption_timer_deadline;
 	}
 
+	/*
+	 * 在以下使用nested_vmx_check_controls():
+	 *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+	 *   - arch/x86/kvm/vmx/nested.c|7231| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+	 */
 	if (nested_vmx_check_controls(vcpu, vmcs12) ||
 	    nested_vmx_check_host_state(vcpu, vmcs12) ||
 	    nested_vmx_check_guest_state(vcpu, vmcs12, &ignored))
@@ -6944,6 +8585,13 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 
 	vmx->nested.dirty_vmcs12 = true;
 	vmx->nested.force_msr_bitmap_recalc = true;
+	/*
+	 * 在以下使用nested_vmx_enter_non_root_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 *   - arch/x86/kvm/vmx/nested.h|26| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+	 *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 */
 	ret = nested_vmx_enter_non_root_mode(vcpu, false);
 	if (ret)
 		goto error_guest_mode;
@@ -6958,8 +8606,30 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * 在以下使用nested_vmx_set_vmcs_shadowing_bitmap():
+ *   - arch/x86/kvm/vmx/vmx.c|4938| <<init_vmcs>> nested_vmx_set_vmcs_shadowing_bitmap();
+ */
 void nested_vmx_set_vmcs_shadowing_bitmap(void)
 {
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
 		vmcs_write64(VMREAD_BITMAP, __pa(vmx_vmread_bitmap));
 		vmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));
@@ -7000,6 +8670,10 @@ static u64 nested_vmx_calc_vmcs_enum_msr(void)
 	return (u64)max_idx << VMCS_FIELD_INDEX_SHIFT;
 }
 
+/*
+ * 在以下使用nested_vmx_setup_pinbased_ctls():
+ *   - arch/x86/kvm/vmx/nested.c|8830| <<nested_vmx_setup_ctls_msrs>> nested_vmx_setup_pinbased_ctls(vmcs_conf, msrs);
+ */
 static void nested_vmx_setup_pinbased_ctls(struct vmcs_config *vmcs_conf,
 					   struct nested_vmx_msrs *msrs)
 {
@@ -7096,6 +8770,10 @@ static void nested_vmx_setup_cpubased_ctls(struct vmcs_config *vmcs_conf,
 		~(CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);
 }
 
+/*
+ * 在以下使用nested_vmx_setup_secondary_ctls():
+ *   - arch/x86/kvm/vmx/nested.c|7634| <<nested_vmx_setup_ctls_msrs>> nested_vmx_setup_secondary_ctls(ept_caps, vmcs_conf, msrs);
+ */
 static void nested_vmx_setup_secondary_ctls(u32 ept_caps,
 					    struct vmcs_config *vmcs_conf,
 					    struct nested_vmx_msrs *msrs)
@@ -7237,6 +8915,11 @@ static void nested_vmx_setup_cr_fixed(struct nested_vmx_msrs *msrs)
  * bit in the high half is on if the corresponding bit in the control field
  * may be on. See also vmx_control_verify().
  */
+/*
+ * 在以下使用nested_vmx_setup_ctls_msrs():
+ *   - arch/x86/kvm/vmx/vmx.c|2852| <<vmx_check_processor_compat>> nested_vmx_setup_ctls_msrs(&vmcs_conf, vmx_cap.ept);
+ *   - arch/x86/kvm/vmx/vmx.c|8984| <<vmx_hardware_setup>> nested_vmx_setup_ctls_msrs(&vmcs_config, vmx_capability.ept);
+ */
 void nested_vmx_setup_ctls_msrs(struct vmcs_config *vmcs_conf, u32 ept_caps)
 {
 	struct nested_vmx_msrs *msrs = &vmcs_conf->nested;
@@ -7274,24 +8957,86 @@ void nested_vmx_setup_ctls_msrs(struct vmcs_config *vmcs_conf, u32 ept_caps)
 	msrs->vmcs_enum = nested_vmx_calc_vmcs_enum_msr();
 }
 
+/*
+ * 在以下使用nested_vmx_hardware_unsetup():
+ *   - arch/x86/kvm/vmx/nested.c|8956| <<nested_vmx_hardware_setup>> nested_vmx_hardware_unsetup();
+ *   - arch/x86/kvm/vmx/vmx.c|9017| <<vmx_hardware_unsetup>> nested_vmx_hardware_unsetup();
+ *   - arch/x86/kvm/vmx/vmx.c|9332| <<vmx_hardware_setup>> nested_vmx_hardware_unsetup();
+ */
 void nested_vmx_hardware_unsetup(void)
 {
 	int i;
 
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu)
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用vmx_bitmap[VMX_BITMAP_NR]:
+		 *   - arch/x86/kvm/vmx/nested.c|8139| <<nested_vmx_hardware_setup>> vmx_bitmap[i] = (unsigned long *)
+		 *   - arch/x86/kvm/vmx/nested.c|8141| <<nested_vmx_hardware_setup>> if (!vmx_bitmap[i]) {
+		 *   - arch/x86/kvm/vmx/nested.c|68| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+		 *   - arch/x86/kvm/vmx/nested.c|69| <<vmx_vmwrite_bitmap>> #define vmx_vmwrite_bitmap (vmx_bitmap[VMX_VMWRITE_BITMAP])
+		 *   - arch/x86/kvm/vmx/nested.c|8101| <<nested_vmx_hardware_unsetup>> free_page((unsigned long )vmx_bitmap[i]);
+		 */
 		for (i = 0; i < VMX_BITMAP_NR; i++)
 			free_page((unsigned long)vmx_bitmap[i]);
 	}
 }
 
+/*
+ * 在以下使用nested_vmx_hardware_setup():
+ *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 {
 	int i;
 
 	if (!cpu_has_vmx_shadow_vmcs())
 		enable_shadow_vmcs = 0;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
 		for (i = 0; i < VMX_BITMAP_NR; i++) {
+			/*
+			 * 在以下使用vmx_bitmap[VMX_BITMAP_NR]:
+			 *   - arch/x86/kvm/vmx/nested.c|8139| <<nested_vmx_hardware_setup>> vmx_bitmap[i] = (unsigned long *)
+			 *   - arch/x86/kvm/vmx/nested.c|8141| <<nested_vmx_hardware_setup>> if (!vmx_bitmap[i]) {
+			 *   - arch/x86/kvm/vmx/nested.c|68| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+			 *   - arch/x86/kvm/vmx/nested.c|69| <<vmx_vmwrite_bitmap>> #define vmx_vmwrite_bitmap (vmx_bitmap[VMX_VMWRITE_BITMAP])
+			 *   - arch/x86/kvm/vmx/nested.c|8101| <<nested_vmx_hardware_unsetup>> free_page((unsigned long )vmx_bitmap[i]);
+			 */
 			/*
 			 * The vmx_bitmap is not tied to a VM and so should
 			 * not be charged to a memcg.
diff --git a/arch/x86/kvm/vmx/nested.h b/arch/x86/kvm/vmx/nested.h
index 6eedcfc91..8c391b362 100644
--- a/arch/x86/kvm/vmx/nested.h
+++ b/arch/x86/kvm/vmx/nested.h
@@ -30,6 +30,31 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 			 u32 exit_intr_info, unsigned long exit_qualification,
 			 u32 exit_insn_len);
 
+/*
+ * 在以下使用nested_vmx_vmexit():
+ *   - arch/x86/kvm/vmx/hyperv.c|229| <<vmx_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_vmx_vmexit(vcpu, HV_VMX_SYNTHETIC_EXIT_REASON_TRAP_AFTER_FLUSH, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|460| <<nested_ept_inject_page_fault>> nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification);
+ *   - arch/x86/kvm/vmx/nested.c|4045| <<nested_vmx_inject_exception_vmexit>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|4269| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_INIT_SIGNAL, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4283| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_SIPI_SIGNAL, 0,
+ *   - arch/x86/kvm/vmx/nested.c|4320| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_MONITOR_TRAP_FLAG, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4341| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4357| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,
+ *   - arch/x86/kvm/vmx/nested.c|4383| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4392| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT,
+ *   - arch/x86/kvm/vmx/nested.c|4440| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT,
+ *   - arch/x86/kvm/vmx/nested.c|5179| <<nested_vmx_triple_fault>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|6183| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->vt.exit_reason.full,
+ *   - arch/x86/kvm/vmx/nested.c|6673| <<nested_vmx_reflect_vmexit>> nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|6804| <<vmx_leave_nested>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|6507| <<__vmx_handle_exit>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|8336| <<vmx_enter_smm>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *
+ * 注释:
+ * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+ * and modify vmcs12 to make it see what it would expect to see there if
+ * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+ */
 static inline void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 				     u32 exit_intr_info,
 				     unsigned long exit_qualification)
@@ -42,6 +67,16 @@ static inline void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	else
 		exit_insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 
+	/*
+	 * 在以下使用__nested_vmx_vmexit():
+	 *   - arch/x86/kvm/vmx/nested.h|45| <<nested_vmx_vmexit>> __nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|8211| <<vmx_check_intercept>> __nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
+	 *
+	 * 注释:
+	 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+	 * and modify vmcs12 to make it see what it would expect to see there if
+	 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+	 */
 	__nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
 			    exit_qualification, exit_insn_len);
 }
@@ -60,14 +95,44 @@ static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 	lockdep_assert_once(lockdep_is_held(&vcpu->mutex) ||
 			    !refcount_read(&vcpu->kvm->users_count));
 
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
+/*
+ * 在以下使用get_shadow_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+ *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+ *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+ *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+ *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+ *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+ *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+ *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+ *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+ */
 static inline struct vmcs12 *get_shadow_vmcs12(struct kvm_vcpu *vcpu)
 {
 	lockdep_assert_once(lockdep_is_held(&vcpu->mutex) ||
 			    !refcount_read(&vcpu->kvm->users_count));
 
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	return to_vmx(vcpu)->nested.cached_shadow_vmcs12;
 }
 
@@ -201,6 +266,12 @@ static inline bool nested_cpu_has_pml(struct vmcs12 *vmcs12)
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_PML);
 }
 
+/*
+ * 在以下使用nested_cpu_has_virt_x2apic_mode():
+ *   - arch/x86/kvm/vmx/nested.c|831| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+ *   - arch/x86/kvm/vmx/nested.c|1034| <<nested_vmx_check_apicv_controls>> if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ *   - arch/x86/kvm/vmx/nested.c|1044| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ */
 static inline bool nested_cpu_has_virt_x2apic_mode(struct vmcs12 *vmcs12)
 {
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
@@ -211,13 +282,54 @@ static inline bool nested_cpu_has_vpid(struct vmcs12 *vmcs12)
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VPID);
 }
 
+/*
+ * 在以下使用nested_cpu_has_apic_reg_virt():
+ *   - arch/x86/kvm/vmx/nested.c|746| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_apic_reg_virt(vmcs12)) {
+ *   - arch/x86/kvm/vmx/nested.c|879| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_apic_reg_virt(vmcs12) &&
+ */
 static inline bool nested_cpu_has_apic_reg_virt(struct vmcs12 *vmcs12)
 {
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_APIC_REGISTER_VIRT);
 }
 
+/*
+ * 在以下使用nested_cpu_has_vid():
+ *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+ *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+ *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+ *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+ *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+ *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+ *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+ */
 static inline bool nested_cpu_has_vid(struct vmcs12 *vmcs12)
 {
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 }
 
@@ -258,6 +370,18 @@ static inline bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)
  * In nested virtualization, check if L1 asked to exit on external interrupts.
  * For most existing hypervisors, this will always return true.
  */
+/*
+ * 在以下使用nested_exit_on_intr():
+ *   - arch/x86/kvm/vmx/nested.c|1052| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+ *   - arch/x86/kvm/vmx/nested.c|4938| <<vmx_check_nested_events>> if (!nested_exit_on_intr(vcpu)) {
+ *   - arch/x86/kvm/vmx/nested.c|7368| <<nested_vmx_l1_wants_exit>> return nested_exit_on_intr(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5322| <<vmx_interrupt_blocked>> if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|5337| <<vmx_interrupt_allowed>> if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+ *
+ * 注释:
+ * In nested virtualization, check if L1 asked to exit on external interrupts.
+ * For most existing hypervisors, this will always return true.
+ */
 static inline bool nested_exit_on_intr(struct kvm_vcpu *vcpu)
 {
 	return get_vmcs12(vcpu)->pin_based_vm_exec_control &
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 0b1736028..1908067ef 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -678,6 +678,32 @@ static void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)
 	int i;
 
 	for (i = 0; i < lbr->nr; i++) {
+		/*
+		 * 在以下使用vmx_set_intercept_for_msr():
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+		 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+		 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+		 *                      !to_vmx(vcpu)->spec_ctrl);
+		 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+		 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+		 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+		 *                      !guest_has_pred_cmd_msr(vcpu));
+		 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+		 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+		 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+		 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+		 */
 		vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
 		vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
 		if (lbr->info)
diff --git a/arch/x86/kvm/vmx/tdx.c b/arch/x86/kvm/vmx/tdx.c
index 66744f576..b30f74945 100644
--- a/arch/x86/kvm/vmx/tdx.c
+++ b/arch/x86/kvm/vmx/tdx.c
@@ -300,6 +300,12 @@ static void tdx_clear_page(struct page *page)
 	__mb();
 }
 
+/*
+ * 在以下使用tdx_no_vcpus_enter_start():
+ *   - arch/x86/kvm/vmx/tdx.c|1722| <<tdx_sept_drop_private_spte>> tdx_no_vcpus_enter_start(kvm);
+ *   - arch/x86/kvm/vmx/tdx.c|1813| <<tdx_sept_zap_private_spte>> tdx_no_vcpus_enter_start(kvm);
+ *   - arch/x86/kvm/vmx/tdx.c|1869| <<tdx_track>> tdx_no_vcpus_enter_start(kvm);
+ */
 static void tdx_no_vcpus_enter_start(struct kvm *kvm)
 {
 	struct kvm_tdx *kvm_tdx = to_kvm_tdx(kvm);
@@ -663,6 +669,11 @@ int tdx_vm_init(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * 在以下使用tdx_vcpu_create():
+ *   - arch/x86/kvm/vmx/main.c|71| <<vt_vcpu_create>> if (is_td_vcpu(vcpu))
+ *                                    return tdx_vcpu_create(vcpu);
+ */
 int tdx_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct kvm_tdx *kvm_tdx = to_kvm_tdx(vcpu->kvm);
@@ -671,6 +682,16 @@ int tdx_vcpu_create(struct kvm_vcpu *vcpu)
 	if (kvm_tdx->state != TD_STATE_INITIALIZED)
 		return -EIO;
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	/*
 	 * TDX module mandates APICv, which requires an in-kernel local APIC.
 	 * Disallow an in-kernel I/O APIC, because level-triggered interrupts
@@ -680,6 +701,18 @@ int tdx_vcpu_create(struct kvm_vcpu *vcpu)
 		return -EINVAL;
 
 	fpstate_set_confidential(&vcpu->arch.guest_fpu);
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	vcpu->arch.apic->guest_apic_protected = true;
 	INIT_LIST_HEAD(&tdx->vt.pi_wakeup_list);
 
@@ -710,6 +743,11 @@ int tdx_vcpu_create(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * 在以下使用tdx_vcpu_load():
+ *   - arch/x86/kvm/vmx/main.c|105| <<vt_vcpu_load>> if (is_td_vcpu(vcpu)) {
+ *                                   tdx_vcpu_load(vcpu, cpu);
+ */
 void tdx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_tdx *tdx = to_tdx(vcpu);
@@ -930,6 +968,10 @@ static __always_inline u32 tdx_to_vmx_exit_reason(struct kvm_vcpu *vcpu)
 	return exit_reason;
 }
 
+/*
+ * 在以下使用tdx_vcpu_enter_exit():
+ *   - arch/x86/kvm/vmx/tdx.c|1081| <<tdx_vcpu_run>> tdx_vcpu_enter_exit(vcpu);
+ */
 static noinstr void tdx_vcpu_enter_exit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_tdx *tdx = to_tdx(vcpu);
@@ -1023,6 +1065,11 @@ static void tdx_load_host_xsave_state(struct kvm_vcpu *vcpu)
 				DEBUGCTLMSR_FREEZE_PERFMON_ON_PMI | \
 				DEBUGCTLMSR_FREEZE_IN_SMM)
 
+/*
+ * 在以下使用tdx_vcpu_run():
+ *   - arch/x86/kvm/vmx/main.c|161| <<vt_vcpu_run>> if (is_td_vcpu(vcpu))
+ *                           return tdx_vcpu_run(vcpu, run_flags);
+ */
 fastpath_t tdx_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 {
 	struct vcpu_tdx *tdx = to_tdx(vcpu);
@@ -1697,6 +1744,12 @@ static int tdx_sept_drop_private_spte(struct kvm *kvm, gfn_t gfn,
 		 * The second retry is expected to succeed after kicking off all
 		 * other vCPUs and prevent them from invoking TDH.VP.ENTER.
 		 */
+		/*
+		 * 在以下使用tdx_no_vcpus_enter_start():
+		 *   - arch/x86/kvm/vmx/tdx.c|1722| <<tdx_sept_drop_private_spte>> tdx_no_vcpus_enter_start(kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|1813| <<tdx_sept_zap_private_spte>> tdx_no_vcpus_enter_start(kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|1869| <<tdx_track>> tdx_no_vcpus_enter_start(kvm);
+		 */
 		tdx_no_vcpus_enter_start(kvm);
 		err = tdh_mem_page_remove(&kvm_tdx->td, gpa, tdx_level, &entry,
 					  &level_state);
@@ -1787,6 +1840,12 @@ static int tdx_sept_zap_private_spte(struct kvm *kvm, gfn_t gfn,
 	err = tdh_mem_range_block(&kvm_tdx->td, gpa, tdx_level, &entry, &level_state);
 
 	if (unlikely(tdx_operand_busy(err))) {
+		/*
+		 * 在以下使用tdx_no_vcpus_enter_start():
+		 *   - arch/x86/kvm/vmx/tdx.c|1722| <<tdx_sept_drop_private_spte>> tdx_no_vcpus_enter_start(kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|1813| <<tdx_sept_zap_private_spte>> tdx_no_vcpus_enter_start(kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|1869| <<tdx_track>> tdx_no_vcpus_enter_start(kvm);
+		 */
 		/* After no vCPUs enter, the second retry is expected to succeed */
 		tdx_no_vcpus_enter_start(kvm);
 		err = tdh_mem_range_block(&kvm_tdx->td, gpa, tdx_level, &entry, &level_state);
@@ -1843,6 +1902,12 @@ static void tdx_track(struct kvm *kvm)
 
 	err = tdh_mem_track(&kvm_tdx->td);
 	if (unlikely(tdx_operand_busy(err))) {
+		/*
+		 * 在以下使用tdx_no_vcpus_enter_start():
+		 *   - arch/x86/kvm/vmx/tdx.c|1722| <<tdx_sept_drop_private_spte>> tdx_no_vcpus_enter_start(kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|1813| <<tdx_sept_zap_private_spte>> tdx_no_vcpus_enter_start(kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|1869| <<tdx_track>> tdx_no_vcpus_enter_start(kvm);
+		 */
 		/* After no vCPUs enter, the second retry is expected to succeed */
 		tdx_no_vcpus_enter_start(kvm);
 		err = tdh_mem_track(&kvm_tdx->td);
@@ -2034,6 +2099,11 @@ int tdx_complete_emulated_msr(struct kvm_vcpu *vcpu, int err)
 }
 
 
+/*
+ * 在以下使用tdx_handle_exit():
+ *   - arch/x86/kvm/vmx/main.c|176| <<vt_handle_exit>> if (is_td_vcpu(vcpu))
+ *                           return tdx_handle_exit(vcpu, fastpath);
+ */
 int tdx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t fastpath)
 {
 	struct vcpu_tdx *tdx = to_tdx(vcpu);
@@ -3114,6 +3184,13 @@ static int tdx_vcpu_init(struct kvm_vcpu *vcpu, struct kvm_tdx_cmd *cmd)
 	 */
 	apic_base = APIC_DEFAULT_PHYS_BASE | LAPIC_MODE_X2APIC |
 		(kvm_vcpu_is_reset_bsp(vcpu) ? MSR_IA32_APICBASE_BSP : 0);
+	/*
+	 * 在以下使用kvm_apic_set_base():
+	 *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+	 *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+	 *                         msr_info->host_initiated);
+	 *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+	 */
 	if (kvm_apic_set_base(vcpu, apic_base, true))
 		return -EINVAL;
 
@@ -3419,6 +3496,10 @@ static int __init __do_tdx_bringup(void)
 	return r;
 }
 
+/*
+ * 在以下使用__tdx_bringup():
+ *   - arch/x86/kvm/vmx/tdx.c|3632| <<tdx_bringup>> r = __tdx_bringup();
+ */
 static int __init __tdx_bringup(void)
 {
 	const struct tdx_sys_info_td_conf *td_conf;
@@ -3527,6 +3608,11 @@ static int __init __tdx_bringup(void)
 	return r;
 }
 
+/*
+ * 在以下使用tdx_cleanup():
+ *   - arch/x86/kvm/vmx/main.c|1090| <<vt_exit>> tdx_cleanup();
+ *   - arch/x86/kvm/vmx/main.c|1135| <<vt_init>> tdx_cleanup();
+ */
 void tdx_cleanup(void)
 {
 	if (enable_tdx) {
@@ -3536,6 +3622,10 @@ void tdx_cleanup(void)
 	}
 }
 
+/*
+ * 在以下使用tdx_bringup():
+ *   - arch/x86/kvm/vmx/main.c|1105| <<vt_init>> r = tdx_bringup();
+ */
 int __init tdx_bringup(void)
 {
 	int r, i;
@@ -3625,6 +3715,25 @@ int __init tdx_bringup(void)
 	return 0;
 }
 
+/*
+ * 24 static __init int vt_hardware_setup(void)
+ * 25 {
+ * 26         int ret;
+ * 27
+ * 28         ret = vmx_hardware_setup();
+ * 29         if (ret)
+ * 30                 return ret;
+ * 31
+ * 32         if (enable_tdx)
+ * 33                 tdx_hardware_setup();
+ * 34
+ * 35         return 0;
+ * 36 }
+ *
+ *
+ * 在以下使用tdx_hardware_setup():
+ *   - arch/x86/kvm/vmx/main.c|33| <<vt_hardware_setup>> tdx_hardware_setup();
+ */
 void __init tdx_hardware_setup(void)
 {
 	KVM_SANITY_CHECK_VM_STRUCT_SIZE(kvm_tdx);
diff --git a/arch/x86/kvm/vmx/vmcs.h b/arch/x86/kvm/vmx/vmcs.h
index b25625314..af690f291 100644
--- a/arch/x86/kvm/vmx/vmcs.h
+++ b/arch/x86/kvm/vmx/vmcs.h
@@ -72,6 +72,13 @@ struct loaded_vmcs {
 	unsigned long *msr_bitmap;
 	struct list_head loaded_vmcss_on_cpu_link;
 	struct vmcs_host_state host_state;
+	/*
+	 * 在以下使用loaded_vmcs->controls_shadow:
+	 *   - arch/x86/kvm/vmx/vmx.c|3028| <<alloc_loaded_vmcs>> memset(&loaded_vmcs->controls_shadow, 0,
+	 *   - arch/x86/kvm/vmx/vmx.h|764| <<BUILD_CONTROLS_SHADOW>> if (vmx->loaded_vmcs->controls_shadow.lname != val) { \
+	 *   - arch/x86/kvm/vmx/vmx.h|766| <<BUILD_CONTROLS_SHADOW>> vmx->loaded_vmcs->controls_shadow.lname = val; \
+	 *   - arch/x86/kvm/vmx/vmx.h|771| <<BUILD_CONTROLS_SHADOW>> return vmcs->controls_shadow.lname; \
+	 */
 	struct vmcs_controls_shadow controls_shadow;
 };
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index aa157fe5b..4b9aa8d42 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1204,6 +1204,13 @@ static void pt_guest_exit(struct vcpu_vmx *vmx)
 		wrmsrq(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);
 }
 
+/*
+ * 在以下使用vmx_set_host_fs_gs():
+ *   - arch/x86/kvm/vmx/nested.c|285| <<vmx_sync_vmcs_host_state>> vmx_set_host_fs_gs(dest,
+ *                     src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
+ *   - arch/x86/kvm/vmx/vmx.c|1303| <<vmx_prepare_switch_to_guest>> vmx_set_host_fs_gs(host_state,
+ *                     fs_sel, gs_sel, fs_base, gs_base);
+ */
 void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
 			unsigned long fs_base, unsigned long gs_base)
 {
@@ -1231,6 +1238,17 @@ void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
 	}
 }
 
+/*
+ * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+ *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+ *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+ *
+ *
+ * 在以下使用vmx_prepare_switch_to_guest():
+ *   - arch/x86/kvm/vmx/main.c|131| <<vt_prepare_switch_to_guest>> vmx_prepare_switch_to_guest(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3628| <<nested_vmx_check_vmentry_hw>> vmx_prepare_switch_to_guest(vcpu);
+ */
 void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1260,6 +1278,19 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	if (vmx->nested.need_vmcs12_to_shadow_sync)
 		nested_sync_vmcs12_to_shadow(vcpu);
 
@@ -1300,6 +1331,13 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 	gs_base = segment_base(gs_sel);
 #endif
 
+	/*
+	 * 在以下使用vmx_set_host_fs_gs():
+	 *   - arch/x86/kvm/vmx/nested.c|285| <<vmx_sync_vmcs_host_state>> vmx_set_host_fs_gs(dest,
+	 *                     src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
+	 *   - arch/x86/kvm/vmx/vmx.c|1303| <<vmx_prepare_switch_to_guest>> vmx_set_host_fs_gs(host_state,
+	 *                     fs_sel, gs_sel, fs_base, gs_base);
+	 */
 	vmx_set_host_fs_gs(host_state, fs_sel, gs_sel, fs_base, gs_base);
 	vt->guest_state_loaded = true;
 }
@@ -1395,6 +1433,13 @@ static void shrink_ple_window(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用vmx_vcpu_load_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ */
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1421,6 +1466,16 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu)
 	prev = per_cpu(current_vmcs, cpu);
 	if (prev != vmx->loaded_vmcs->vmcs) {
 		per_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;
+		/*
+		 * 在以下使用vmcs_load():
+		 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 */
 		vmcs_load(vmx->loaded_vmcs->vmcs);
 	}
 
@@ -1460,6 +1515,13 @@ void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (vcpu->scheduled_out && !kvm_pause_in_guest(vcpu->kvm))
 		shrink_ple_window(vcpu);
 
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 
 	vmx_vcpu_pi_load(vcpu, cpu);
@@ -1812,6 +1874,11 @@ void vmx_inject_exception(struct kvm_vcpu *vcpu)
 	} else
 		intr_info |= INTR_TYPE_HARD_EXCEPTION;
 
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);
 
 	vmx_clear_hlt(vcpu);
@@ -2558,6 +2625,11 @@ static u64 adjust_vmx_controls64(u64 ctl_opt, u32 msr)
 	r;									\
 })
 
+/*
+ * 在以下使用setup_vmcs_config():
+ *   - arch/x86/kvm/vmx/vmx.c|2847| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
+ *   - arch/x86/kvm/vmx/vmx.c|8821| <<vmx_hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+ */
 static int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 			     struct vmx_capability *vmx_cap)
 {
@@ -2609,6 +2681,33 @@ static int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 		_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;
 #endif
 
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))
 		_cpu_based_2nd_exec_control &= ~(
 				SECONDARY_EXEC_APIC_REGISTER_VIRT |
@@ -2774,6 +2873,11 @@ int vmx_check_processor_compat(void)
 	if (!__kvm_is_vmx_supported())
 		return -EIO;
 
+	/*
+	 * 在以下使用setup_vmcs_config():
+	 *   - arch/x86/kvm/vmx/vmx.c|2847| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8821| <<vmx_hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+	 */
 	if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
 		pr_err("Failed to setup VMCS config on CPU %d\n", cpu);
 		return -EIO;
@@ -3967,6 +4071,32 @@ static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 	vmx->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * 在以下使用vmx_set_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+ *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+ *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+ *                      !to_vmx(vcpu)->spec_ctrl);
+ *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+ *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+ *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *                      !guest_has_pred_cmd_msr(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+ *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+ *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+ *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+ */
 void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type, bool set)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3992,6 +4122,11 @@ void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type, bool se
 	}
 }
 
+/*
+ * 在以下使用vmx_update_msr_bitmap_x2apic():
+ *   - arch/x86/kvm/vmx/vmx.c|4442| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6859| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+ */
 static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -4036,6 +4171,32 @@ static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
 		msr_bitmap[read_idx] = ~0ull;
 	msr_bitmap[write_idx] = ~0ull;
 
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	/*
 	 * TPR reads and writes can be virtualized even if virtual interrupt
 	 * delivery is not in use.
@@ -4058,6 +4219,32 @@ void pt_update_intercept_for_msr(struct kvm_vcpu *vcpu)
 	bool flag = !(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN);
 	u32 i;
 
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
@@ -4100,6 +4287,32 @@ void vmx_recalc_msr_intercepts(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD, MSR_TYPE_RW);
 
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
 				  !to_vmx(vcpu)->spec_ctrl);
 
@@ -4121,6 +4334,10 @@ void vmx_recalc_msr_intercepts(struct kvm_vcpu *vcpu)
 	 */
 }
 
+/*
+ * 在以下使用vmx_deliver_nested_posted_interrupt():
+ *   - arch/x86/kvm/vmx/vmx.c|4180| <<vmx_deliver_posted_interrupt>> r = vmx_deliver_nested_posted_interrupt(vcpu, vector);
+ */
 static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
 						int vector)
 {
@@ -4133,8 +4350,28 @@ static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
 	 * enabled in its vmcs12, i.e. checking the vector also checks that
 	 * L1 has enabled posted interrupts for L2.
 	 */
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	if (is_guest_mode(vcpu) &&
 	    vector == vmx->nested.posted_intr_nv) {
+		/*
+		 * 在以下使用nested_vmx->pi_pending:
+		 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+		 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+		 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+		 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+		 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+		 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+		 */
 		/*
 		 * If a posted intr is not recognized by hardware,
 		 * we will accomplish it in the next vmentry.
@@ -4176,6 +4413,12 @@ static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 	if (!r)
 		return 0;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	/* Note, this is called iff the local APIC is in-kernel. */
 	if (!vcpu->arch.apic->apicv_active)
 		return -1;
@@ -4190,6 +4433,11 @@ void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
 	struct kvm_vcpu *vcpu = apic->vcpu;
 
 	if (vmx_deliver_posted_interrupt(vcpu, vector)) {
+		/*
+		 * 在以下使用kvm_lapic_set_irr():
+		 *   - arch/x86/kvm/svm/svm.c|3844| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+		 *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+		 */
 		kvm_lapic_set_irr(vector, apic);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 		kvm_vcpu_kick(vcpu);
@@ -4288,6 +4536,11 @@ void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
 	vmcs_writel(CR4_GUEST_HOST_MASK, ~vcpu->arch.cr4_guest_owned_bits);
 }
 
+/*
+ * 在以下使用vmx_pin_based_exec_ctrl():
+ *   - arch/x86/kvm/vmx/vmx.c|4620| <<vmx_refresh_apicv_exec_ctrl>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|4964| <<init_vmcs>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+ */
 static u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 {
 	u32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;
@@ -4340,17 +4593,115 @@ static u32 vmx_vmexit_ctrl(void)
 		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
 }
 
+/*
+ * 761 #define BUILD_CONTROLS_SHADOW(lname, uname, bits)                                               \
+ * 762 static inline void lname##_controls_set(struct vcpu_vmx *vmx, u##bits val)                      \
+ * 763 {                                                                                               \
+ * 764         if (vmx->loaded_vmcs->controls_shadow.lname != val) {                                   \
+ * 765                 vmcs_write##bits(uname, val);                                                   \
+ * 766                 vmx->loaded_vmcs->controls_shadow.lname = val;                                  \
+ * 767         }                                                                                       \
+ * 768 }                                                                                               \
+ * 769 static inline u##bits __##lname##_controls_get(struct loaded_vmcs *vmcs)                        \
+ * 770 {                                                                                               \
+ * 771         return vmcs->controls_shadow.lname;                                                     \
+ * 772 }                                                                                               \
+ * 773 static inline u##bits lname##_controls_get(struct vcpu_vmx *vmx)                                \
+ * 774 {                                                                                               \
+ * 775         return __##lname##_controls_get(vmx->loaded_vmcs);                                      \
+ * 776 }                                                                                               \
+ * 777 static __always_inline void lname##_controls_setbit(struct vcpu_vmx *vmx, u##bits val)          \
+ * 778 {                                                                                               \
+ * 779         BUILD_BUG_ON(!(val & (KVM_REQUIRED_VMX_##uname | KVM_OPTIONAL_VMX_##uname)));           \
+ * 780         lname##_controls_set(vmx, lname##_controls_get(vmx) | val);                             \
+ * 781 }                                                                                               \
+ * 782 static __always_inline void lname##_controls_clearbit(struct vcpu_vmx *vmx, u##bits val)        \
+ * 783 {                                                                                               \
+ * 784         BUILD_BUG_ON(!(val & (KVM_REQUIRED_VMX_##uname | KVM_OPTIONAL_VMX_##uname)));           \
+ * 785         lname##_controls_set(vmx, lname##_controls_get(vmx) & ~val);                            \
+ * 786 }
+ * 787 BUILD_CONTROLS_SHADOW(vm_entry, VM_ENTRY_CONTROLS, 32)
+ * 788 BUILD_CONTROLS_SHADOW(vm_exit, VM_EXIT_CONTROLS, 32)
+ * 789 BUILD_CONTROLS_SHADOW(pin, PIN_BASED_VM_EXEC_CONTROL, 32)
+ * 790 BUILD_CONTROLS_SHADOW(exec, CPU_BASED_VM_EXEC_CONTROL, 32)
+ * 791 BUILD_CONTROLS_SHADOW(secondary_exec, SECONDARY_VM_EXEC_CONTROL, 32)
+ * 792 BUILD_CONTROLS_SHADOW(tertiary_exec, TERTIARY_VM_EXEC_CONTROL, 64)
+ */
+
+/*
+ * 在以下使用refresh_apicv_exec_ctrl:
+ *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+ *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+ *
+ * 在以下使用vmx_refresh_apicv_exec_ctrl():
+ *   - arch/x86/kvm/vmx/main.c|740| <<vt_refresh_apicv_exec_ctrl>> vmx_refresh_apicv_exec_ctrl(vcpu);
+ *
+ * vcpu_enter_guest(KVM_REQ_APICV_UPDATE)
+ * -> kvm_vcpu_update_apicv()
+ *    -> __kvm_vcpu_update_apicv()
+ *       -> vt_refresh_apicv_exec_ctrl()
+ *
+ * vmx_refresh_apicv_exec_ctrl()在activate和deactivate APICv的时候都要被调用
+ */
 void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->pdate_vmcs01_apicv_status:
+		 *   - arch/x86/kvm/vmx/nested.c|5114| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+		 *   - arch/x86/kvm/vmx/nested.c|5115| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+		 *
+		 * 在__nested_vmx_vmexit()触发KVM_REQ_APICV_UPDATE
+		 *
+		 * vcpu_enter_guest(KVM_REQ_APICV_UPDATE)
+		 * -> kvm_vcpu_update_apicv()
+		 *    -> __kvm_vcpu_update_apicv()
+		 *       -> if (apic->apicv_active == activate) goto out ==> 返回!!!
+		 *       -> vt_refresh_apicv_exec_ctrl()
+		 */
 		vmx->nested.update_vmcs01_apicv_status = true;
 		return;
 	}
 
+	/*
+	 * 在以下使用vmx_pin_based_exec_ctrl():
+	 *   - arch/x86/kvm/vmx/vmx.c|4620| <<vmx_refresh_apicv_exec_ctrl>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+	 *   - arch/x86/kvm/vmx/vmx.c|4964| <<init_vmcs>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+	 *
+	 * 看上面注释
+	 */
 	pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
 
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (kvm_vcpu_apicv_active(vcpu)) {
 		secondary_exec_controls_setbit(vmx,
 					       SECONDARY_EXEC_APIC_REGISTER_VIRT |
@@ -4365,6 +4716,11 @@ void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 			tertiary_exec_controls_clearbit(vmx, TERTIARY_EXEC_IPI_VIRT);
 	}
 
+	/*
+	 * 在以下使用vmx_update_msr_bitmap_x2apic():
+	 *   - arch/x86/kvm/vmx/vmx.c|4442| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6859| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 */
 	vmx_update_msr_bitmap_x2apic(vcpu);
 }
 
@@ -4412,6 +4768,10 @@ static u32 vmx_exec_control(struct vcpu_vmx *vmx)
 	return exec_control;
 }
 
+/*
+ * 只在以下调用vmx_tertiary_exec_control():
+ *   - arch/x86/kvm/vmx/vmx.c|4996| <<init_vmcs>> tertiary_exec_controls_set(vmx, vmx_tertiary_exec_control(vmx));
+ */
 static u64 vmx_tertiary_exec_control(struct vcpu_vmx *vmx)
 {
 	u64 exec_control = vmcs_config.cpu_based_3rd_exec_ctrl;
@@ -4490,6 +4850,11 @@ vmx_adjust_secondary_exec_control(struct vcpu_vmx *vmx, u32 *exec_control,
 #define vmx_adjust_sec_exec_exiting(vmx, exec_control, lname, uname) \
 	vmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, uname##_EXITING, true)
 
+/*
+ * 在以下使用vmx_secondary_exec_control():
+ *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|8195| <<vmx_vcpu_after_set_cpuid>> vmx_secondary_exec_control(vmx));
+ */
 static u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)
 {
 	struct kvm_vcpu *vcpu = &vmx->vcpu;
@@ -4511,6 +4876,32 @@ static u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)
 		exec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;
 	if (kvm_pause_in_guest(vmx->vcpu.kvm))
 		exec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (!kvm_vcpu_apicv_active(vcpu))
 		exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |
 				  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
@@ -4610,11 +5001,18 @@ int vmx_vcpu_precreate(struct kvm *kvm)
 
 #define VMX_XSS_EXIT_BITMAP 0
 
+/*
+ * 在以下使用init_vmcs():
+ *   - arch/x86/kvm/vmx/vmx.c|4921| <<__vmx_vcpu_reset>> init_vmcs(vmx);
+ */
 static void init_vmcs(struct vcpu_vmx *vmx)
 {
 	struct kvm *kvm = vmx->vcpu.kvm;
 	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);
 
+	/*
+	 * 只在这里使用nested_vmx_set_vmcs_shadowing_bitmap()
+	 */
 	if (nested)
 		nested_vmx_set_vmcs_shadowing_bitmap();
 
@@ -4624,11 +5022,21 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	vmcs_write64(VMCS_LINK_POINTER, INVALID_GPA); /* 22.3.1.5 */
 
 	/* Control */
+	/*
+	 * 在以下使用vmx_pin_based_exec_ctrl():
+	 *   - arch/x86/kvm/vmx/vmx.c|4620| <<vmx_refresh_apicv_exec_ctrl>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+	 *   - arch/x86/kvm/vmx/vmx.c|4964| <<init_vmcs>> pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
+	 */
 	pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
 
 	exec_controls_set(vmx, vmx_exec_control(vmx));
 
 	if (cpu_has_secondary_exec_ctrls()) {
+		/*
+		 * 在以下使用vmx_secondary_exec_control():
+		 *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
+		 *   - arch/x86/kvm/vmx/vmx.c|8195| <<vmx_vcpu_after_set_cpuid>> vmx_secondary_exec_control(vmx));
+		 */
 		secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
 		if (vmx->ve_info)
 			vmcs_write64(VE_INFORMATION_ADDRESS,
@@ -4723,6 +5131,14 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	vmx_guest_debugctl_write(&vmx->vcpu, 0);
 
 	if (cpu_has_vmx_tpr_shadow()) {
+		/*
+		 * 在以下使用VIRTUAL_APIC_PAGE_ADDR:
+		 *   - arch/x86/kvm/vmx/nested.c|3488| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
+		 *   - arch/x86/kvm/vmx/nested.c|3506| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);
+		 *   - arch/x86/kvm/vmx/vmx.c|4800| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|4802| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, __pa(vmx->vcpu.arch.apic->regs));
+		 *   - arch/x86/kvm/vmx/vmx.c|6482| <<dump_vmcs>> pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
+		 */
 		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
 		if (cpu_need_tpr_shadow(&vmx->vcpu))
 			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,
@@ -4733,6 +5149,10 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	vmx_setup_uret_msrs(vmx);
 }
 
+/*
+ * 在以下使用__vmx_vcpu_reset():
+ *   - arch/x86/kvm/vmx/vmx.c|4965| <<vmx_vcpu_reset>> __vmx_vcpu_reset(vcpu);
+ */
 static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4745,6 +5165,17 @@ static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
 
 	vcpu_setup_sgx_lepubkeyhash(vcpu);
 
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	vmx->nested.posted_intr_nv = -1;
 	vmx->nested.vmxon_ptr = INVALID_GPA;
 	vmx->nested.current_vmptr = INVALID_GPA;
@@ -4765,6 +5196,10 @@ static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
 	__pi_set_sn(&vmx->vt.pi_desc);
 }
 
+/*
+ * 在以下使用vmx_vcpu_reset():
+ *   - arch/x86/kvm/vmx/main.c|93| <<vt_vcpu_reset>> vmx_vcpu_reset(vcpu, init_event);
+ */
 void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4815,8 +5250,24 @@ void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	if (kvm_mpx_supported())
 		vmcs_write64(GUEST_BNDCFGS, 0);
 
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */
 
+	/*
+	 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+	 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+	 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+	 *
+	 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+	 */
 	kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 
 	vpid_sync_context(vmx->vpid);
@@ -4826,6 +5277,12 @@ void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 void vmx_enable_irq_window(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 当guest中断控制状态变为"可以接收中断"的状态(例如EFLAGS.IF = 1, guest
+	 * 允许外部中断,而且guest的GUEST_INTERRUPTIBILITY_INFO表明没有阻止中断的状态)时,CPU
+	 * 将进行VM-exit给hypervisor,因为你请求了interrupt-window
+	 * exiting.见代码:
+	 */
 	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);
 }
 
@@ -4840,6 +5297,10 @@ void vmx_enable_nmi_window(struct kvm_vcpu *vcpu)
 	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);
 }
 
+/*
+ * 在以下使用vmx_inject_irq():
+ *   - arch/x86/kvm/vmx/main.c|645| <<vt_inject_irq>> vmx_inject_irq(vcpu, reinjected);
+ */
 void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4863,6 +5324,11 @@ void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 			     vmx->vcpu.arch.event_exit_inst_len);
 	} else
 		intr |= INTR_TYPE_EXT_INTR;
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
 
 	vmx_clear_hlt(vcpu);
@@ -4893,6 +5359,11 @@ void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 		return;
 	}
 
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 
@@ -4967,6 +5438,18 @@ bool __vmx_interrupt_blocked(struct kvm_vcpu *vcpu)
 
 bool vmx_interrupt_blocked(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用nested_exit_on_intr():
+	 *   - arch/x86/kvm/vmx/nested.c|1052| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|4938| <<vmx_check_nested_events>> if (!nested_exit_on_intr(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|7368| <<nested_vmx_l1_wants_exit>> return nested_exit_on_intr(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<vmx_interrupt_blocked>> if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *   - arch/x86/kvm/vmx/vmx.c|5337| <<vmx_interrupt_allowed>> if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *
+	 * 注释:
+	 * In nested virtualization, check if L1 asked to exit on external interrupts.
+	 * For most existing hypervisors, this will always return true.
+	 */
 	if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
 		return false;
 
@@ -4978,6 +5461,18 @@ int vmx_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 	if (to_vmx(vcpu)->nested.nested_run_pending)
 		return -EBUSY;
 
+	/*
+	 * 在以下使用nested_exit_on_intr():
+	 *   - arch/x86/kvm/vmx/nested.c|1052| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|4938| <<vmx_check_nested_events>> if (!nested_exit_on_intr(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|7368| <<nested_vmx_l1_wants_exit>> return nested_exit_on_intr(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<vmx_interrupt_blocked>> if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *   - arch/x86/kvm/vmx/vmx.c|5337| <<vmx_interrupt_allowed>> if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *
+	 * 注释:
+	 * In nested virtualization, check if L1 asked to exit on external interrupts.
+	 * For most existing hypervisors, this will always return true.
+	 */
 	/*
 	 * An IRQ must not be injected into L2 if it's supposed to VM-Exit,
 	 * e.g. if the IRQ arrived asynchronously after checking nested events.
@@ -5405,6 +5900,18 @@ static int handle_cr(struct kvm_vcpu *vcpu)
 			err = handle_set_cr4(vcpu, val);
 			return kvm_complete_insn_gp(vcpu, err);
 		case 8: {
+				/*
+				 * 在以下使用kvm_get_cr8():
+				 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+				 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+				 */
 				u8 cr8_prev = kvm_get_cr8(vcpu);
 				u8 cr8 = (u8)val;
 				err = kvm_set_cr8(vcpu, cr8);
@@ -5436,6 +5943,18 @@ static int handle_cr(struct kvm_vcpu *vcpu)
 			trace_kvm_cr_read(cr, val);
 			return kvm_skip_emulated_instruction(vcpu);
 		case 8:
+			/*
+			 * 在以下使用kvm_get_cr8():
+			 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+			 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+			 */
 			val = kvm_get_cr8(vcpu);
 			kvm_register_write(vcpu, reg, val);
 			trace_kvm_cr_read(cr, val);
@@ -5587,11 +6106,17 @@ static int handle_apic_access(struct kvm_vcpu *vcpu)
 	return kvm_emulate_instruction(vcpu, 0);
 }
 
+/*
+ * 处理EXIT_REASON_EOI_INDUCED
+ */
 static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
 	int vector = exit_qualification & 0xff;
 
+	/*
+	 * 只在这里使用kvm_apic_set_eoi_accelerated()
+	 */
 	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 	kvm_apic_set_eoi_accelerated(vcpu, vector);
 	return 1;
@@ -5722,6 +6247,13 @@ static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_mmu_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 }
 
@@ -5864,6 +6396,29 @@ static int handle_invpcid(struct kvm_vcpu *vcpu)
 	gpr_index = vmx_get_instr_info_reg2(vmx_instruction_info);
 	type = kvm_register_read(vcpu, gpr_index);
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	/* According to the Intel instruction reference, the memory operand
 	 * is read even if it isn't needed (e.g., for type==all)
 	 */
@@ -6008,6 +6563,12 @@ static int handle_notify(struct kvm_vcpu *vcpu)
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
  * to be done to userspace and return 0.
  */
+/*
+ * 在以下使用kvm_vmx_exit_handlers[]数组:
+ *   - arch/x86/kvm/vmx/vmx.c|6969| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+ *   - arch/x86/kvm/vmx/vmx.c|6972| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,
 	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
@@ -6066,6 +6627,10 @@ static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 static const int kvm_vmx_max_exit_handlers =
 	ARRAY_SIZE(kvm_vmx_exit_handlers);
 
+/*
+ * 在以下使用vmx_get_exit_info():
+ *   - arch/x86/kvm/vmx/main.c|700| <<vt_get_exit_info>> vmx_get_exit_info(vcpu, reason, info1, info2, intr_info, error_code);
+ */
 void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 		       u64 *info1, u64 *info2, u32 *intr_info, u32 *error_code)
 {
@@ -6089,6 +6654,11 @@ void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 
 void vmx_get_entry_info(struct kvm_vcpu *vcpu, u32 *intr_info, u32 *error_code)
 {
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	*intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
 	if (is_exception_with_error_code(*intr_info))
 		*error_code = vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE);
@@ -6257,6 +6827,32 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 	pr_err("Interruptibility = %08x  ActivityState = %08x\n",
 	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
 	       vmcs_read32(GUEST_ACTIVITY_STATE));
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
 		pr_err("InterruptStatus = %04x\n",
 		       vmcs_read16(GUEST_INTR_STATUS));
@@ -6323,6 +6919,32 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 		pr_err("TSC Multiplier = 0x%016llx\n",
 		       vmcs_read64(TSC_MULTIPLIER));
 	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW) {
+		/*
+		 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+		 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+		 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+		 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+		 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+		 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+		 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+		 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+		 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+		 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+		 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+		 */
 		if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
 			u16 status = vmcs_read16(GUEST_INTR_STATUS);
 			pr_err("SVI|RVI = %02x|%02x ", status >> 8, status & 0xff);
@@ -6330,6 +6952,14 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 		pr_cont("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
 		if (secondary_exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)
 			pr_err("APIC-access addr = 0x%016llx ", vmcs_read64(APIC_ACCESS_ADDR));
+		/*
+		 * 在以下使用VIRTUAL_APIC_PAGE_ADDR:
+		 *   - arch/x86/kvm/vmx/nested.c|3488| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
+		 *   - arch/x86/kvm/vmx/nested.c|3506| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);
+		 *   - arch/x86/kvm/vmx/vmx.c|4800| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|4802| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, __pa(vmx->vcpu.arch.apic->regs));
+		 *   - arch/x86/kvm/vmx/vmx.c|6482| <<dump_vmcs>> pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
+		 */
 		pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
 	}
 	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)
@@ -6366,10 +6996,17 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * 在以下使用__vmx_handle_exit():
+ *   - arch/x86/kvm/vmx/vmx.c|6982| <<vmx_handle_exit>> int ret = __vmx_handle_exit(vcpu, exit_fastpath);
+ */
 static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	union vmx_exit_reason exit_reason = vmx_get_exit_reason(vcpu);
+	/*
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	u32 vectoring_info = vmx->idt_vectoring_info;
 	u16 exit_handler_index;
 
@@ -6430,6 +7067,12 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 			return 1;
 		}
 
+		/*
+		 * 只在这里调用
+		 * 注释:
+		 * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
+		 * reflected into L1.
+		 */
 		if (nested_vmx_reflect_vmexit(vcpu))
 			return 1;
 	}
@@ -6464,6 +7107,11 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	     exit_reason.basic != EXIT_REASON_TASK_SWITCH &&
 	     exit_reason.basic != EXIT_REASON_NOTIFY &&
 	     exit_reason.basic != EXIT_REASON_EPT_MISCONFIG)) {
+		/*
+		 * 在以下使用kvm_prepare_event_vectoring_exit():
+		 *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
+		 *   - arch/x86/kvm/x86.c|9163| <<x86_emulate_instruction>> kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
+		 */
 		kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
 		return 0;
 	}
@@ -6509,6 +7157,12 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 
 	exit_handler_index = array_index_nospec((u16)exit_reason.basic,
 						kvm_vmx_max_exit_handlers);
+	/*
+	 * 在以下使用kvm_vmx_exit_handlers[]数组:
+	 *   - arch/x86/kvm/vmx/vmx.c|6969| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+	 */
 	if (!kvm_vmx_exit_handlers[exit_handler_index])
 		goto unexpected_vmexit;
 
@@ -6527,6 +7181,16 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->handle_exit:
+ *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+ *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+ *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+ *
+ *
+ * 在以下使用vmx_handle_exit():
+ *   - arch/x86/kvm/vmx/main.c|166| <<vt_handle_exit>> return vmx_handle_exit(vcpu, fastpath);
+ */
 int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	int ret = __vmx_handle_exit(vcpu, exit_fastpath);
@@ -6632,6 +7296,11 @@ void vmx_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 		vmcs_write32(TPR_THRESHOLD, tpr_threshold);
 }
 
+/*
+ * 在以下使用vmx_set_virtual_apic_mode():
+ *   - arch/x86/kvm/vmx/main.c|306| <<vt_set_virtual_apic_mode>> return vmx_set_virtual_apic_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6056| <<__nested_vmx_vmexit>> vmx_set_virtual_apic_mode(vcpu);
+ */
 void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -6646,6 +7315,12 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 
 	/* Postpone execution until vmcs01 is the current VMCS. */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page:
+		 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+		 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+		 */
 		vmx->nested.change_vmcs01_virtual_apic_mode = true;
 		return;
 	}
@@ -6664,6 +7339,17 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 		if (flexpriority_enabled) {
 			sec_exec_control |=
 				SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;
+			/*
+			 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+			 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+			 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+			 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+			 *
+			 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+			 */
 			kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 
 			/*
@@ -6683,9 +7369,18 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	}
 	secondary_exec_controls_set(vmx, sec_exec_control);
 
+	/*
+	 * 在以下使用vmx_update_msr_bitmap_x2apic():
+	 *   - arch/x86/kvm/vmx/vmx.c|4442| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6859| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 */
 	vmx_update_msr_bitmap_x2apic(vcpu);
 }
 
+/*
+ * 在以下使用vmx_set_apic_access_page_addr():
+ *   - arch/x86/kvm/vmx/main.c|730| <<vt_set_apic_access_page_addr>> vmx_set_apic_access_page_addr(vcpu);
+ */
 void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 {
 	const gfn_t gfn = APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT;
@@ -6699,6 +7394,22 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 
 	/* Defer reload until vmcs01 is the current VMCS. */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+		 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+		 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+		 *                       
+		 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+		 *
+		 * 在以下使用reload_vmcs01_apic_access_page:
+		 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+		 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+		 */
 		to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
 		return;
 	}
@@ -6707,6 +7418,15 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 	    SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
 		return;
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	/*
 	 * Explicitly grab the memslot using KVM's internal slot ID to ensure
 	 * KVM doesn't unintentionally grab a userspace memslot.  It _should_
@@ -6736,10 +7456,28 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 		return;
 
 	read_lock(&vcpu->kvm->mmu_lock);
+	/*
+	 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+	 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+	 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+	 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+	 *
+	 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+	 */
 	if (mmu_invalidate_retry_gfn(kvm, mmu_seq, gfn))
 		kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 	else
 		vmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(pfn));
+	/*
+	 * 在以下修改APIC_ACCESS_ADDR:
+	 *   - arch/x86/kvm/vmx/nested.c|3887| <<nested_get_vmcs12_pages>>
+	 *                   vmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(map->pfn));
+	 *   - arch/x86/kvm/vmx/vmx.c|7430| <<vmx_set_apic_access_page_addr>>
+	 *                   vmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(pfn));
+	 */
 
 	/*
 	 * Do not pin the APIC access page in memory so that it can be freely
@@ -6757,11 +7495,50 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 	read_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * kvm_vcpu_reset()
+ * -> if (is_guest_mode(vcpu))
+ *        kvm_leave_nested(vcpu);
+ * -> kvm_lapic_reset()
+ *    -> if (apic->apicv_active)
+ *           kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ * -> WARN_ON_ONCE(is_guest_mode(vcpu) || is_smm(vcpu));
+ *
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_LAPIC)
+ * -> kvm_vcpu_ioctl_set_lapic()
+ *    -> kvm_apic_set_state()
+ *       -> if (apic->apicv_active)
+ *              kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *
+ *
+ * 在以下使用vt_hwapic_isr_update():
+ *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+ *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+ *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *
+ * 在以下使用vmx_hwapic_isr_update():
+ *   - arch/x86/kvm/vmx/main.c|290| <<vt_hwapic_isr_update>> return vmx_hwapic_isr_update(vcpu, max_isr);
+ */
 void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 {
 	u16 status;
 	u8 old;
 
+	/*
+	 * 在以下使用enter_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 *
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	/*
 	 * If L2 is active, defer the SVI update until vmcs01 is loaded, as SVI
 	 * is only relevant for if and only if Virtual Interrupt Delivery is
@@ -6770,6 +7547,30 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 	 * VM-Exit, otherwise L1 with run with a stale SVI.
 	 */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用kvm_vcpu->wants_to_run:
+		 *   - arch/arm64/kvm/arm.c|1166| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/loongarch/kvm/vcpu.c|1795| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/mips/kvm/mips.c|436| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/powerpc/kvm/powerpc.c|1849| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/riscv/kvm/vcpu.c|900| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/s390/kvm/kvm-s390.c|5328| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/x86/kvm/vmx/vmx.c|6876| <<vmx_hwapic_isr_update>> WARN_ON_ONCE(vcpu->wants_to_run &&
+		 *   - arch/x86/kvm/x86.c|12138| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - virt/kvm/kvm_main.c|4465| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
+		 *   - virt/kvm/kvm_main.c|4467| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = false;
+		 *   - virt/kvm/kvm_main.c|6383| <<kvm_sched_out>> if (task_is_runnable(current) && vcpu->wants_to_run) {
+		 *
+		 * 在以下使用nested_cpu_has_vid():
+		 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+		 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+		 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+		 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+		 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+		 */
 		/*
 		 * KVM is supposed to forward intercepted L2 EOIs to L1 if VID
 		 * is enabled in vmcs12; as above, the EOIs affect L2's vAPIC.
@@ -6779,6 +7580,23 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 		 */
 		WARN_ON_ONCE(vcpu->wants_to_run &&
 			     nested_cpu_has_vid(get_vmcs12(vcpu)));
+		/*
+		 * 在以下使用vt_hwapic_isr_update():
+		 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+		 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+		 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+		 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *
+		 *
+		 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+		 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+		 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+		 *
+		 * 在__nested_vmx_vmexit()触发kvm_apic_update_hwapic_isr()
+		 */
 		to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
 		return;
 	}
@@ -6795,6 +7613,10 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 	}
 }
 
+/*
+ * 在以下使用vmx_set_rvi():
+ *   - arch/x86/kvm/vmx/vmx.c|7045| <<vmx_sync_pir_to_irr>> vmx_set_rvi(max_irr);
+ */
 static void vmx_set_rvi(int vector)
 {
 	u16 status;
@@ -6904,6 +7726,10 @@ static void handle_exception_irqoff(struct kvm_vcpu *vcpu, u32 intr_info)
 		kvm_machine_check();
 }
 
+/*
+ * 在以下使用handle_external_interrupt_irqoff():
+ *   - arch/x86/kvm/vmx/vmx.c|6981| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
+ */
 static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu,
 					     u32 intr_info)
 {
@@ -6913,21 +7739,49 @@ static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu,
 	    "unexpected VM-Exit interrupt info: 0x%x", intr_info))
 		return;
 
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
 	if (cpu_feature_enabled(X86_FEATURE_FRED))
 		fred_entry_from_kvm(EVENT_TYPE_EXTINT, vector);
 	else
 		vmx_do_interrupt_irqoff(gate_offset((gate_desc *)host_idt_base + vector));
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	kvm_after_interrupt(vcpu);
 
 	vcpu->arch.at_instruction_boundary = true;
 }
 
+/*
+ * 在以下调用:
+ *   - arch/x86/kvm/x86.c|11339| <<vcpu_enter_guest>> kvm_x86_call(handle_exit_irqoff)(vcpu);
+ *
+ * 在以下使用vmx_handle_exit_irqoff():
+ *   - arch/x86/kvm/vmx/main.c|976| <<global>> .handle_exit_irqoff = vmx_handle_exit_irqoff,
+ */
 void vmx_handle_exit_irqoff(struct kvm_vcpu *vcpu)
 {
 	if (to_vt(vcpu)->emulation_required)
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	if (vmx_get_exit_reason(vcpu).basic == EXIT_REASON_EXTERNAL_INTERRUPT)
 		handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
 	else if (vmx_get_exit_reason(vcpu).basic == EXIT_REASON_EXCEPTION_NMI)
@@ -7000,6 +7854,13 @@ static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 					      vmx->loaded_vmcs->entry_time));
 }
 
+/*
+ * 在以下使用__vmx_complete_interrupts():
+ *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu,
+ *        vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+ *   - arch/x86/kvm/vmx/vmx.c|7085| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu,
+ *        vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+ */
 static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 				      u32 idt_vectoring_info,
 				      int instr_len_field,
@@ -7013,6 +7874,10 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 
 	vcpu->arch.nmi_injected = false;
 	kvm_clear_exception_queue(vcpu);
+	/*
+	 * 设置:
+	 * vcpu->arch.interrupt.injected = false;
+	 */
 	kvm_clear_interrupt_queue(vcpu);
 
 	if (!idtv_info_valid)
@@ -7051,6 +7916,15 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 		fallthrough;
 	case INTR_TYPE_EXT_INTR:
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
 		break;
 	default:
@@ -7058,8 +7932,21 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用vmx_complete_interrupts():
+ *   - arch/x86/kvm/vmx/vmx.c|7418| <<vmx_vcpu_run>> vmx_complete_interrupts(vmx);
+ */
 static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 {
+	/*
+	 * 在以下使用__vmx_complete_interrupts():
+	 *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu,
+	 *        vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|7085| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu,
+	 *        vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+	 *
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 				  VM_EXIT_INSTRUCTION_LEN,
 				  IDT_VECTORING_ERROR_CODE);
@@ -7067,6 +7954,17 @@ static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 
 void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用__vmx_complete_interrupts():
+	 *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu,
+	 *        vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|7085| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu,
+	 *        vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+	 *
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	__vmx_complete_interrupts(vcpu,
 				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 				  VM_ENTRY_INSTRUCTION_LEN,
@@ -7180,17 +8078,36 @@ static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用vmx_handle_nmi():
+ *   - arch/x86/kvm/vmx/tdx.c|959| <<tdx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+ */
 noinstr void vmx_handle_nmi(struct kvm_vcpu *vcpu)
 {
 	if ((u16)vmx_get_exit_reason(vcpu).basic != EXIT_REASON_EXCEPTION_NMI ||
 	    !is_nmi(vmx_get_intr_info(vcpu)))
 		return;
 
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
 	if (cpu_feature_enabled(X86_FEATURE_FRED))
 		fred_entry_from_kvm(EVENT_TYPE_NMI, NMI_VECTOR);
 	else
 		vmx_do_nmi_irqoff();
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	kvm_after_interrupt(vcpu);
 }
 
@@ -7238,9 +8155,17 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 	}
 
 	vmx->vt.exit_reason.full = vmcs_read32(VM_EXIT_REASON);
+	/*
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	if (likely(!vmx_get_exit_reason(vcpu).failed_vmentry))
 		vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
 
+	/*
+	 * 在以下使用vmx_handle_nmi():
+	 *   - arch/x86/kvm/vmx/tdx.c|959| <<tdx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+	 */
 	vmx_handle_nmi(vcpu);
 
 out:
@@ -7282,6 +8207,19 @@ fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	/*
 	 * We did this in prepare_switch_to_guest, because it needs to
 	 * be within srcu_read_lock.
@@ -7292,6 +8230,15 @@ fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
 	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RIP))
 		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	vcpu->arch.regs_dirty = 0;
 
 	if (run_flags & KVM_RUN_LOAD_GUEST_DR6)
@@ -7717,6 +8664,10 @@ static void update_intel_pt_cfg(struct kvm_vcpu *vcpu)
 		vmx->pt_desc.ctl_bitmask &= ~(0xfULL << (32 + i * 4));
 }
 
+/*
+ * 在以下使用vmx_vcpu_after_set_cpuid():
+ *   - arch/x86/kvm/vmx/main.c|327| <<vt_vcpu_after_set_cpuid>> vmx_vcpu_after_set_cpuid(vcpu);
+ */
 void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7731,6 +8682,11 @@ void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 
 	vmx_setup_uret_msrs(vmx);
 
+	/*
+	 * 在以下使用vmx_secondary_exec_control():
+	 *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
+	 *   - arch/x86/kvm/vmx/vmx.c|8195| <<vmx_vcpu_after_set_cpuid>> vmx_secondary_exec_control(vmx));
+	 */
 	if (cpu_has_secondary_exec_ctrls())
 		vmcs_set_secondary_exec_control(vmx,
 						vmx_secondary_exec_control(vmx));
@@ -7932,6 +8888,12 @@ static bool vmx_is_io_intercepted(struct kvm_vcpu *vcpu,
 	return nested_vmx_check_io_bitmaps(vcpu, port, size);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->check_intercept:
+ *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+ *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+ *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+ */
 int vmx_check_intercept(struct kvm_vcpu *vcpu,
 			struct x86_instruction_info *info,
 			enum x86_intercept_stage stage,
@@ -8023,6 +8985,16 @@ int vmx_check_intercept(struct kvm_vcpu *vcpu,
 	if (!exit_insn_len || exit_insn_len > X86_MAX_INSTRUCTION_LENGTH)
 		return X86EMUL_UNHANDLEABLE;
 
+	/*
+	 * 在以下使用__nested_vmx_vmexit():
+	 *   - arch/x86/kvm/vmx/nested.h|45| <<nested_vmx_vmexit>> __nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|8211| <<vmx_check_intercept>> __nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
+	 *
+	 * 注释:
+	 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+	 * and modify vmcs12 to make it see what it would expect to see there if
+	 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+	 */
 	__nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
 			    exit_insn_len);
 	return X86EMUL_INTERCEPTED;
@@ -8058,6 +9030,25 @@ int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,
 	tscl = rdtsc();
 	guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
 	delta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;
+	/*
+	 * 在以下使用kvm_timer->timer_advance_ns:
+	 *   - arch/x86/kvm/debugfs.c|18| <<vcpu_get_timer_advance_ns>> *val = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2864| <<__wait_lapic_expire>> u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2874| <<__wait_lapic_expire>> __delay(min(guest_cycles, nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2878| <<__wait_lapic_expire>> ndelay(min_t(u32, delay_ns, timer_advance_ns));
+	 *   - arch/x86/kvm/lapic.c|2886| <<adjust_lapic_timer_advance>> u32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2898| <<adjust_lapic_timer_advance>> timer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2903| <<adjust_lapic_timer_advance>> timer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;
+	 *   - arch/x86/kvm/lapic.c|2906| <<adjust_lapic_timer_advance>> if (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))
+	 *   - arch/x86/kvm/lapic.c|2907| <<adjust_lapic_timer_advance>> timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/lapic.c|2908| <<adjust_lapic_timer_advance>> apic->lapic_timer.timer_advance_ns = timer_advance_ns;
+	 *   - arch/x86/kvm/lapic.c|2938| <<kvm_wait_lapic_expire>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns &&
+	 *   - arch/x86/kvm/lapic.c|2996| <<apic_timer_expired>> if (... vcpu->arch.apic->lapic_timer.timer_advance_ns)
+	 *   - arch/x86/kvm/lapic.c|3031| <<start_sw_tscdeadline>> if (... likely(ns > apic->lapic_timer.timer_advance_ns)) {
+	 *   - arch/x86/kvm/lapic.c|3033| <<start_sw_tscdeadline>> expire = ktime_sub_ns(expire, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/lapic.c|4402| <<kvm_create_lapic>> apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+	 *   - arch/x86/kvm/vmx/vmx.c|8902| <<vmx_set_hv_timer>> lapic_timer_advance_cycles = nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 */
 	lapic_timer_advance_cycles = nsec_to_cycles(vcpu,
 						    ktimer->timer_advance_ns);
 
@@ -8167,6 +9158,13 @@ int vmx_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	}
 
 	if (vmx->nested.smm.guest_mode) {
+		/*
+		 * 在以下使用nested_vmx_enter_non_root_mode():
+		 *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+		 *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+		 *   - arch/x86/kvm/vmx/nested.h|26| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+		 *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+		 */
 		ret = nested_vmx_enter_non_root_mode(vcpu, false);
 		if (ret)
 			return ret;
@@ -8333,6 +9331,11 @@ __init int vmx_hardware_setup(void)
 
 	vmx_setup_user_return_msrs();
 
+	/*
+	 * 在以下使用setup_vmcs_config():
+	 *   - arch/x86/kvm/vmx/vmx.c|2847| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8821| <<vmx_hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+	 */
 	if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
 		return -EIO;
 
@@ -8381,6 +9384,16 @@ __init int vmx_hardware_setup(void)
 		enable_sgx = false;
 #endif
 
+	/*
+	 * 在以下使用kvm_x86_ops->set_apic_access_page_addr:
+	 *   - arch/x86/kvm/vmx/main.c|992| <<global>> .set_apic_access_page_addr = vt_op(set_apic_access_page_addr),
+	 *   - arch/x86/kvm/mmu/mmu.c|1675| <<kvm_unmap_gfn_range>> if (kvm_x86_ops.set_apic_access_page_addr &&
+	 *   - arch/x86/kvm/vmx/vmx.c|9261| <<vmx_hardware_setup>> vt_x86_ops.set_apic_access_page_addr = NULL;
+	 *   - arch/x86/kvm/x86.c|11306| <<kvm_vcpu_reload_apic_access_page>> kvm_x86_call(set_apic_access_page_addr)(vcpu);
+	 *
+	 * vt_set_apic_access_page_addr()
+	 * vmx_set_apic_access_page_addr()
+	 */
 	/*
 	 * set_apic_access_page_addr() is used to reload apic access
 	 * page upon invalidation.  No need to do anything if not
@@ -8408,6 +9421,12 @@ __init int vmx_hardware_setup(void)
 		ple_window_shrink = 0;
 	}
 
+	/*
+	 * 在以下使用cpu_has_vmx_apicv():
+	 *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+	 *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+	 */
 	if (!cpu_has_vmx_apicv())
 		enable_apicv = 0;
 	if (!enable_apicv)
@@ -8492,6 +9511,12 @@ __init int vmx_hardware_setup(void)
 	if (nested) {
 		nested_vmx_setup_ctls_msrs(&vmcs_config, vmx_capability.ept);
 
+		/*
+		 * 在以下使用kvm_vmx_exit_handlers[]数组:
+		 *   - arch/x86/kvm/vmx/vmx.c|6969| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+		 *   - arch/x86/kvm/vmx/vmx.c|6972| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+		 */
 		r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
 		if (r)
 			return r;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index d3389baf3..e776c2ffa 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -74,6 +74,16 @@ struct pt_desc {
 struct nested_vmx {
 	/* Has the level1 guest done vmxon? */
 	bool vmxon;
+	/*
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|346| <<free_nested>> vmx->nested.vmxon_ptr = INVALID_GPA;
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<handle_vmxon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|5563| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5843| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|6703| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 *   - arch/x86/kvm/vmx/nested.c|6885| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->hdr.vmx.vmxon_pa;
+	 *   - arch/x86/kvm/vmx/vmx.c|4760| <<__vmx_vcpu_reset>> vmx->nested.vmxon_ptr = INVALID_GPA;
+	 */
 	gpa_t vmxon_ptr;
 	bool pml_full;
 
@@ -106,6 +116,19 @@ struct nested_vmx {
 	 * Indicates if the shadow vmcs or enlightened vmcs must be updated
 	 * with the data held by struct vmcs12.
 	 */
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	bool need_vmcs12_to_shadow_sync;
 	bool dirty_vmcs12;
 
@@ -122,6 +145,12 @@ struct nested_vmx {
 	 * Indicates lazily loaded guest state has not yet been decached from
 	 * vmcs02.
 	 */
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	bool need_sync_vmcs02_to_vmcs12_rare;
 
 	/*
@@ -131,10 +160,41 @@ struct nested_vmx {
 	 */
 	bool vmcs02_initialized;
 
+	/*
+	 * 在以下使用nested_vmx->change_vmcs01_virtual_apic_mode
+	 */
 	bool change_vmcs01_virtual_apic_mode;
+	/*
+	 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page:
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	bool reload_vmcs01_apic_access_page;
 	bool update_vmcs01_cpu_dirty_logging;
+	/*
+	 * 在以下使用nested_vmx->pdate_vmcs01_apicv_status:
+	 *   - arch/x86/kvm/vmx/nested.c|5114| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+	 *   - arch/x86/kvm/vmx/nested.c|5115| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+	 *
+	 * 在__nested_vmx_vmexit()触发KVM_REQ_APICV_UPDATE
+	 *
+	 * vcpu_enter_guest(KVM_REQ_APICV_UPDATE)
+	 * -> kvm_vcpu_update_apicv()
+	 *    -> __kvm_vcpu_update_apicv()
+	 *       -> if (apic->apicv_active == activate) goto out ==> 返回!!!
+	 *       -> vt_refresh_apicv_exec_ctrl()
+	 */
 	bool update_vmcs01_apicv_status;
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+	 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+	 *
+	 * 在__nested_vmx_vmexit()触发kvm_apic_update_hwapic_isr()
+	 */
 	bool update_vmcs01_hwapic_isr;
 
 	/*
@@ -157,11 +217,38 @@ struct nested_vmx {
 	 * pointers, so we must keep them pinned while L2 runs.
 	 */
 	struct kvm_host_map apic_access_page_map;
+	/*
+	 * 在以下使用nested_vmx->virtual_apic_map:
+	 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+	 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+	 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+	 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+	 */
 	struct kvm_host_map virtual_apic_map;
 	struct kvm_host_map pi_desc_map;
 
 	struct pi_desc *pi_desc;
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	bool pi_pending;
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	u16 posted_intr_nv;
 
 	struct hrtimer preemption_timer;
@@ -211,6 +298,25 @@ struct vcpu_vmx {
 	u8                    fail;
 	u8		      x2apic_msr_bitmap_mode;
 
+	/*
+	 * 在以下使用vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|1650| <<vmx_check_emulate_instruction>> if ((to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5119| <<handle_exception_nmi>> vect_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5632| <<handle_task_switch>> idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5633| <<handle_task_switch>> idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5634| <<handle_task_switch>> type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5650| <<handle_task_switch>> if (vmx->idt_vectoring_info &
+	 *   - arch/x86/kvm/vmx/vmx.c|5691| <<handle_ept_violation>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5896| <<handle_pml_full>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|6083| <<vmx_get_exit_info>> *info2 = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6379| <<__vmx_handle_exit>> u32 vectoring_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6985| <<vmx_recover_nmi_blocking>> idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|7246| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7257| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 *
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -334,10 +440,37 @@ static __always_inline unsigned long vmx_get_exit_qual(struct kvm_vcpu *vcpu)
 	return vt->exit_qualification;
 }
 
+/*
+ * 在以下使用vmx_get_intr_info():
+ *   - arch/x86/kvm/vmx/nested.c|6163| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->vt.exit_reason.full, vmx_get_intr_info(vcpu),
+ *   - arch/x86/kvm/vmx/nested.c|6404| <<nested_vmx_l0_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6485| <<nested_vmx_l1_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6642| <<nested_vmx_reflect_vmexit>> exit_intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/tdx.c|1120| <<tdx_handle_exception_nmi>> u32 intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/tdx.c|2167| <<tdx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5140| <<handle_exception_nmi>> intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6104| <<vmx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6972| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|6974| <<vmx_handle_exit_irqoff>> handle_exception_irqoff(vcpu, vmx_get_intr_info(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|7016| <<vmx_recover_nmi_blocking>> exit_intr_info = vmx_get_intr_info(&vmx->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7255| <<vmx_handle_nmi>> if (... !is_nmi(vmx_get_intr_info(vcpu)))
+ *
+ * VM_EXIT_INTR_INFO:
+ * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+ */
 static __always_inline u32 vmx_get_intr_info(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vt *vt = to_vt(vcpu);
 
+	/*
+	 * 在以下使用vcpu_vt->exit_intr_info:
+	 *   - arch/x86/kvm/vmx/tdx.c|957| <<tdx_vcpu_enter_exit>> vt->exit_intr_info = tdx->vp_enter_args.r9;
+	 *   - arch/x86/kvm/vmx/vmx.c|7343| <<vmx_vcpu_run>> vmx->vt.exit_intr_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.h|368| <<vmx_get_intr_info>> vt->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	 *   - arch/x86/kvm/vmx/vmx.h|370| <<vmx_get_intr_info>> return vt->exit_intr_info;
+	 *
+	 * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+	 */
 	if (!kvm_register_test_and_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_2) &&
 	    !WARN_ON_ONCE(is_td_vcpu(vcpu)))
 		vt->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
@@ -391,12 +524,64 @@ void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type, bool se
 static inline void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu,
 						 u32 msr, int type)
 {
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, msr, type, false);
 }
 
 static inline void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu,
 						u32 msr, int type)
 {
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, msr, type, true);
 }
 
diff --git a/arch/x86/kvm/vmx/vmx_ops.h b/arch/x86/kvm/vmx/vmx_ops.h
index 96677576c..6b0377d60 100644
--- a/arch/x86/kvm/vmx/vmx_ops.h
+++ b/arch/x86/kvm/vmx/vmx_ops.h
@@ -293,6 +293,16 @@ static inline void vmcs_clear(struct vmcs *vmcs)
 	vmx_asm1(vmclear, "m"(phys_addr), vmcs, phys_addr);
 }
 
+/*
+ * 在以下使用vmcs_load():
+ *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ */
 static inline void vmcs_load(struct vmcs *vmcs)
 {
 	u64 phys_addr = __pa(vmcs);
@@ -355,6 +365,13 @@ static inline void vpid_sync_vcpu_addr(int vpid, gva_t addr)
 		vpid_sync_context(vpid);
 }
 
+/*
+ * 在以下使用ept_sync_global():
+ *   - arch/x86/kvm/vmx/tdx.c|2862| <<tdx_flush_tlb_current>> ept_sync_global();
+ *   - arch/x86/kvm/vmx/tdx.c|2878| <<tdx_flush_tlb_all>> ept_sync_global();
+ *   - arch/x86/kvm/vmx/vmx.c|3277| <<vmx_flush_tlb_all>> ept_sync_global();
+ *   - arch/x86/kvm/vmx/vmx_ops.h|378| <<ept_sync_context>> ept_sync_global();
+ */
 static inline void ept_sync_global(void)
 {
 	__invept(VMX_EPT_EXTENT_GLOBAL, 0);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 706b6fd56..c52766a45 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1478,6 +1478,18 @@ int kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8)
 }
 EXPORT_SYMBOL_GPL(kvm_set_cr8);
 
+/*
+ * 在以下使用kvm_get_cr8():
+ *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+ */
 unsigned long kvm_get_cr8(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
@@ -2398,6 +2410,15 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2401| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3094| <<get_cpu_tsc_khz>> return __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3108| <<__get_kvmclock>> (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {
+ *   - arch/x86/kvm/x86.c|9335| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|9352| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|9371| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
 static unsigned long max_tsc_khz;
 
@@ -2496,6 +2517,10 @@ static inline bool gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * 在以下使用kvm_track_tsc_matching():
+ *   - arch/x86/kvm/x86.c|2716| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu, !matched);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 {
 #ifdef CONFIG_X86_64
@@ -2561,6 +2586,22 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * 在以下使用kvm_read_l1_tsc():
+ *   - arch/x86/kvm/hyperv.c|599| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2509| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2519| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2616| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2705| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|2729| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|1176| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2384| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|8696| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|3285| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|10089| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|11616| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/xen.c|253| <<kvm_xen_start_timer>> guest_tsc = kvm_read_l1_tsc(vcpu, host_tsc);
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
 	return vcpu->arch.l1_tsc_offset +
@@ -2654,6 +2695,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * 在以下使用__kvm_synchronize_tsc():
+ *   - arch/x86/kvm/x86.c|2783| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+ *   - arch/x86/kvm/x86.c|5807| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched, bool user_set_tsc)
 {
@@ -3003,6 +3049,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * 在以下使用pvclock_update_vm_gtod_copy():
+ *   - arch/x86/kvm/x86.c|3095| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6914| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9399| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12745| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3070,6 +3123,10 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10698| <<vcpu_enter_guest>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_request_tsc_page_update(kvm);
@@ -3094,6 +3151,10 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * 在以下使用__get_kvmclock():
+ *   - arch/x86/kvm/x86.c|3149| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3609,6 +3670,11 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * 在以下使用kvmclock_reset():
+ *   - arch/x86/kvm/x86.c|13215| <<kvm_arch_vcpu_destroy>> kvmclock_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|13297| <<kvm_vcpu_reset>> kvmclock_reset(vcpu);
+ */
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
 	kvm_gpc_deactivate(&vcpu->arch.pv_time);
@@ -3899,6 +3965,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_MTRRdefType:
 		return kvm_mtrr_set_msr(vcpu, msr, data);
 	case MSR_IA32_APICBASE:
+		/*
+		 * 在以下使用kvm_apic_set_base():
+		 *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+		 *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+		 *                         msr_info->host_initiated);
+		 *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+		 */
 		return kvm_apic_set_base(vcpu, data, msr_info->host_initiated);
 	case APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:
 		return kvm_x2apic_msr_write(vcpu, msr, data);
@@ -4048,6 +4121,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))
 			return 1;
 
+		/*
+		 * 在以下使用kvm_lapic_set_pv_eoi():
+		 *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+		 *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+		 *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+		 *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+		 */
 		if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
 			return 1;
 		break;
@@ -4319,6 +4399,27 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		msr_info->data = 1 << 24;
 		break;
 	case MSR_IA32_APICBASE:
+		/*
+		 * 在以下设置kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+		 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+		 * 在以下使用kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+		 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+		 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+		 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+		 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+		 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+		 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+		 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+		 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+		 */
 		msr_info->data = vcpu->arch.apic_base;
 		break;
 	case APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:
@@ -5091,6 +5192,13 @@ static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 	 * when this is true, for example allowing the vCPU to be marked
 	 * preempted if and only if the VM-Exit was due to a host interrupt.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	if (!vcpu->arch.at_instruction_boundary) {
 		vcpu->stat.preemption_other++;
 		return;
@@ -5155,6 +5263,18 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (vcpu->arch.apic->guest_apic_protected)
 		return -EINVAL;
 
@@ -5163,11 +5283,27 @@ static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 	return kvm_apic_get_state(vcpu, s);
 }
 
+/*
+ * 处理KVM_SET_LAPIC:
+ *   - arch/x86/kvm/x86.c|6140| <<kvm_arch_vcpu_ioctl>> r = kvm_vcpu_ioctl_set_lapic(vcpu, u.lapic);
+ */
 static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
 	int r;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (vcpu->arch.apic->guest_apic_protected)
 		return -EINVAL;
 
@@ -5179,6 +5315,11 @@ static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_cpu_accept_dm_intr():
+ *   - arch/x86/kvm/x86.c|5248| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu) &&
+ *   - arch/x86/kvm/x86.c|10864| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+ */
 static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -5187,6 +5328,17 @@ static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 	 * The actual injection will happen when the CPU is able to
 	 * deliver the interrupt.
 	 */
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 *
+	 * 注释:
+	 * check if there is pending interrupt from
+	 * non-APIC source without intack.
+	 */
 	if (kvm_cpu_has_extint(vcpu))
 		return false;
 
@@ -5195,8 +5347,18 @@ static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 		kvm_apic_accept_pic_intr(vcpu));
 }
 
+/*
+ * 在以下使用kvm_vcpu_ready_for_interrupt_injection():
+ *   - arch/x86/kvm/x86.c|10251| <<post_kvm_run_save>> kvm_vcpu_ready_for_interrupt_injection(vcpu);
+ *   - arch/x86/kvm/x86.c|11479| <<vcpu_run>> kvm_vcpu_ready_for_interrupt_injection(vcpu)) {
+ */
 static int kvm_vcpu_ready_for_interrupt_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_cpu_accept_dm_intr():
+	 *   - arch/x86/kvm/x86.c|5248| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|10864| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+	 */
 	/*
 	 * Do not cause an interrupt window exit if an exception
 	 * is pending or an event needs reinjection; userspace
@@ -5217,11 +5379,31 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 		return -EINVAL;
 
 	if (!irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, irq->irq, false);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 		return 0;
 	}
 
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 */
 	/*
 	 * With in-kernel LAPIC, we only use this to inject EXTINT, so
 	 * fail for in-kernel 8259.
@@ -5229,6 +5411,15 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 	if (pic_in_kernel(vcpu->kvm))
 		return -ENXIO;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	if (vcpu->arch.pending_external_vector != -1)
 		return -EEXIST;
 
@@ -5313,6 +5504,13 @@ static int kvm_vcpu_x86_set_ucna(struct kvm_vcpu *vcpu, struct kvm_x86_mce *mce,
 	    !(vcpu->arch.mci_ctl2_banks[mce->bank] & MCI_CTL2_CMCI_EN))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_apic_local_deliver():
+	 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+	 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+	 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+	 */
 	if (lapic_in_kernel(vcpu))
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
 
@@ -5530,6 +5728,14 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	}
 	kvm_x86_call(set_nmi_mask)(vcpu, events->nmi.masked);
 
+	/*
+	 * 在以下使用kvm_lapic->sipi_vector:
+	 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+	 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+	 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+	 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+	 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+	 */
 	if (events->flags & KVM_VCPUEVENT_VALID_SIPI_VECTOR &&
 	    lapic_in_kernel(vcpu))
 		vcpu->arch.apic->sipi_vector = events->sipi_vector;
@@ -5537,6 +5743,12 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	if (events->flags & KVM_VCPUEVENT_VALID_SMM) {
 #ifdef CONFIG_KVM_SMM
 		if (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {
+			/*
+			 * 在以下使用kvm_leave_nested():
+			 *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+			 *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+			 *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+			 */
 			kvm_leave_nested(vcpu);
 			kvm_smm_changed(vcpu, events->smi.smm);
 		}
@@ -6255,6 +6467,14 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		int idx;
 
 		r = -EINVAL;
+		/*
+		 * 在以下使用kvm_x86_nested_ops->set_state:
+		 *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+		 *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+		 *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+		 *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+		 *                                     user_kvm_nested_state, &kvm_state);
+		 */
 		if (!kvm_x86_ops.nested_ops->set_state)
 			break;
 
@@ -6278,6 +6498,14 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			break;
 
 		idx = srcu_read_lock(&vcpu->kvm->srcu);
+		/*
+		 * 在以下使用kvm_x86_nested_ops->set_state:
+		 *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+		 *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+		 *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+		 *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+		 *                                     user_kvm_nested_state, &kvm_state);
+		 */
 		r = kvm_x86_ops.nested_ops->set_state(vcpu, user_kvm_nested_state, &kvm_state);
 		srcu_read_unlock(&vcpu->kvm->srcu, idx);
 		break;
@@ -6447,8 +6675,26 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 			goto split_irqchip_unlock;
 		/* Pairs with irqchip_in_kernel. */
 		smp_wmb();
+		/*
+		 * 在以下使用kvm_arch->irqchip_mode:
+		 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+		 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		 */
 		kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
 		kvm->arch.nr_reserved_ioapic_pins = cap->args[0];
+		/*
+		 * 在以下使用kvm_clear_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|602| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|612| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|623| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/svm/svm.c|3238| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 *   - arch/x86/kvm/x86.c|6688| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+		 *   - arch/x86/kvm/x86.c|7245| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+		 */
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
 		r = 0;
 split_irqchip_unlock:
@@ -6460,6 +6706,15 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		if (cap->args[0] & ~KVM_X2APIC_API_VALID_FLAGS)
 			break;
 
+		/*
+		 * 在以下使用kvm_arch->x2apic_format:
+		 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+		 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+		 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+		 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+		 */
 		if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)
 			kvm->arch.x2apic_format = true;
 		if (cap->args[0] & KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)
@@ -6988,7 +7243,25 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		}
 		/* Write kvm->irq_routing before enabling irqchip_in_kernel. */
 		smp_wmb();
+		/*
+		 * 在以下使用kvm_arch->irqchip_mode:
+		 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+		 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		 */
 		kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		/*
+		 * 在以下使用kvm_clear_apicv_inhibit():
+		 *   - arch/x86/kvm/i8254.c|314| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+		 *   - arch/x86/kvm/lapic.c|602| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|612| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+		 *   - arch/x86/kvm/lapic.c|623| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+		 *   - arch/x86/kvm/svm/svm.c|3238| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+		 *   - arch/x86/kvm/x86.c|6688| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+		 *   - arch/x86/kvm/x86.c|7245| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+		 */
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
 	create_irqchip_unlock:
 		mutex_unlock(&kvm->lock);
@@ -7008,6 +7281,17 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		if (kvm->arch.vpit)
 			goto create_pit_unlock;
 		r = -ENOENT;
+		/*
+		 * 在以下使用pic_in_kernel():
+		 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+		 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+		 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+		 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+		 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+		 *                                        likely(!pic_in_kernel(vcpu->kvm));
+		 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+		 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+		 */
 		if (!pic_in_kernel(kvm))
 			goto create_pit_unlock;
 		r = -ENOMEM;
@@ -7028,6 +7312,13 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		}
 
 		r = -ENXIO;
+		/*
+		 * 在以下使用irqchip_full():
+		 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+		 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+		 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 */
 		if (!irqchip_full(kvm))
 			goto get_irqchip_out;
 		r = kvm_vm_ioctl_get_irqchip(kvm, chip);
@@ -7052,6 +7343,13 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		}
 
 		r = -ENXIO;
+		/*
+		 * 在以下使用irqchip_full():
+		 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+		 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+		 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 */
 		if (!irqchip_full(kvm))
 			goto set_irqchip_out;
 		r = kvm_vm_ioctl_set_irqchip(kvm, chip);
@@ -8211,6 +8509,18 @@ static unsigned long emulator_get_cr(struct x86_emulate_ctxt *ctxt, int cr)
 		value = kvm_read_cr4(vcpu);
 		break;
 	case 8:
+		/*
+		 * 在以下使用kvm_get_cr8():
+		 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+		 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+		 */
 		value = kvm_get_cr8(vcpu);
 		break;
 	default:
@@ -8419,6 +8729,12 @@ static int emulator_intercept(struct x86_emulate_ctxt *ctxt,
 			      struct x86_instruction_info *info,
 			      enum x86_intercept_stage stage)
 {
+	/*
+	 * 在以下使用kvm_x86_ops->check_intercept:
+	 *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+	 *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+	 *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+	 */
 	return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
 					     &ctxt->exception);
 }
@@ -8734,6 +9050,11 @@ void kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_prepare_emulation_failure_exit);
 
+/*
+ * 在以下使用kvm_prepare_event_vectoring_exit():
+ *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
+ *   - arch/x86/kvm/x86.c|9163| <<x86_emulate_instruction>> kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
+ */
 void kvm_prepare_event_vectoring_exit(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	u32 reason, intr_info, error_code;
@@ -9033,6 +9354,11 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			return 1;
 
 		if (r == X86EMUL_UNHANDLEABLE_VECTORING) {
+			/*
+			 * 在以下使用kvm_prepare_event_vectoring_exit():
+			 *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
+			 *   - arch/x86/kvm/x86.c|9163| <<x86_emulate_instruction>> kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
+			 */
 			kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
 			return 0;
 		}
@@ -9851,18 +10177,75 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)
 		.dest_id = apicid,
 	};
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * 在以下使用kvm_apicv_activated():
+ *   - arch/x86/kvm/ioapic.c|265| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4709| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|255| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|294| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|1462| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/nested.c|1605| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10175| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10181| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10215| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|11153| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|11183| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|11192| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
 }
 EXPORT_SYMBOL_GPL(kvm_apicv_activated);
 
+/*
+ * 在以下使用kvm_vcpu_apicv_activated():
+ *   - arch/x86/kvm/svm/svm.c|1574| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+ *   - arch/x86/kvm/x86.c|11026| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) &&
+ *   - arch/x86/kvm/x86.c|11773| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+ */
 bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10175| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10181| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10215| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|11153| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|11183| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|11192| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_get_apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/svm/svm.c|5354| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+	 *   - arch/x86/kvm/svm/svm.c|5594| <<svm_hardware_setup>> svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
+	 *   - arch/x86/kvm/x86.c|10158| <<kvm_vcpu_apicv_activated>> kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
+	 */
 	ulong vcpu_reasons =
 			kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
 
@@ -9885,16 +10268,55 @@ static void set_or_clear_apicv_inhibit(unsigned long *inhibits,
 	trace_kvm_apicv_inhibit_changed(reason, set, *inhibits);
 }
 
+/*
+ * 在以下使用kvm_apicv_init():
+ *   - arch/x86/kvm/x86.c|13845| <<kvm_arch_init_vm>> kvm_apicv_init(kvm);
+ */
 static void kvm_apicv_init(struct kvm *kvm)
 {
 	enum kvm_apicv_inhibit reason = enable_apicv ? APICV_INHIBIT_REASON_ABSENT :
 						       APICV_INHIBIT_REASON_DISABLED;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10175| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10181| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10215| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|11153| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|11183| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|11192| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	init_rwsem(&kvm->arch.apicv_update_lock);
 }
 
+/*
+ * 在以下使用kvm_sched_yield():
+ *   - arch/x86/kvm/x86.c|10309| <<kvm_sched_yield>> kvm_sched_yield(vcpu, a1);
+ *   - arch/x86/kvm/x86.c|10327| <<kvm_sched_yield>> kvm_sched_yield(vcpu, a0);
+ */
 static void kvm_sched_yield(struct kvm_vcpu *vcpu, unsigned long dest_id)
 {
 	struct kvm_vcpu *target = NULL;
@@ -9906,6 +10328,17 @@ static void kvm_sched_yield(struct kvm_vcpu *vcpu, unsigned long dest_id)
 		goto no_yield;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(vcpu->kvm->arch.apic_map);
 
 	if (likely(map) && dest_id <= map->max_apic_id) {
@@ -10082,8 +10515,34 @@ static int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt)
 		&ctxt->exception);
 }
 
+/*
+ * 在以下使用dm_request_for_irq_injection():
+ *   - arch/x86/kvm/x86.c|10863| <<vcpu_enter_guest>> dm_request_for_irq_injection(vcpu) &&
+ *   - arch/x86/kvm/x86.c|11478| <<vcpu_run>> if (dm_request_for_irq_injection(vcpu) &&
+ *
+ * 满足:
+ * 1. vcpu->run->request_interrupt_window设置了
+ * 2. 不在KVM模拟PIC (split??)
+ */
 static int dm_request_for_irq_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 *
+	 * 这个是在userspace的QEMU设置的.
+	 * If we have an interrupt but the guest is not ready to receive an
+	 * interrupt, request an interrupt window exit.  This will
+	 * cause a return to userspace as soon as the guest is ready to
+	 * receive interrupts.
+	 */
 	return vcpu->run->request_interrupt_window &&
 		likely(!pic_in_kernel(vcpu->kvm));
 }
@@ -10094,9 +10553,53 @@ static void post_kvm_run_save(struct kvm_vcpu *vcpu)
 	struct kvm_run *kvm_run = vcpu->run;
 
 	kvm_run->if_flag = kvm_x86_call(get_if_flag)(vcpu);
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	kvm_run->cr8 = kvm_get_cr8(vcpu);
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	kvm_run->apic_base = vcpu->arch.apic_base;
 
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 */
 	kvm_run->ready_for_interrupt_injection =
 		pic_in_kernel(vcpu->kvm) ||
 		kvm_vcpu_ready_for_interrupt_injection(vcpu);
@@ -10107,6 +10610,9 @@ static void post_kvm_run_save(struct kvm_vcpu *vcpu)
 		kvm_run->flags |= KVM_RUN_X86_GUEST_MODE;
 }
 
+/*
+ * CR8 在长模式下直接访问的是Local APIC中的TPR?
+ */
 static void update_cr8_intercept(struct kvm_vcpu *vcpu)
 {
 	int max_irr, tpr;
@@ -10117,9 +10623,22 @@ static void update_cr8_intercept(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 有APICv就不用
+	 */
 	if (vcpu->arch.apic->apicv_active)
 		return;
 
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3955| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|10328| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	if (!vcpu->arch.apic->vapic_addr)
 		max_irr = kvm_lapic_find_highest_irr(vcpu);
 	else
@@ -10130,10 +10649,20 @@ static void update_cr8_intercept(struct kvm_vcpu *vcpu)
 
 	tpr = kvm_lapic_get_cr8(vcpu);
 
+	/*
+	 * vmx_update_cr8_intercept
+	 * svm_update_cr8_intercept
+	 */
 	kvm_x86_call(update_cr8_intercept)(vcpu, tpr, max_irr);
 }
 
 
+/*
+ * 在以下使用kvm_check_nested_events():
+ *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+ *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+ */
 int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	if (kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {
@@ -10141,9 +10670,20 @@ int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用kvm_x86_nested_ops->check_events:
+	 *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+	 *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+	 *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+	 */
 	return kvm_x86_ops.nested_ops->check_events(vcpu);
 }
 
+/*
+ * 在以下使用kvm_inject_exception():
+ *   - arch/x86/kvm/x86.c|10394| <<kvm_check_and_inject_events>> kvm_inject_exception(vcpu);
+ *   - arch/x86/kvm/x86.c|10460| <<kvm_check_and_inject_events>> kvm_inject_exception(vcpu);
+ */
 static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -10160,6 +10700,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 				vcpu->arch.exception.error_code,
 				vcpu->arch.exception.injected);
 
+	/*
+	 * vmx_inject_exception()
+	 * svm_inject_exception()
+	 */
 	kvm_x86_call(inject_exception)(vcpu);
 }
 
@@ -10202,12 +10746,22 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * 只在以下使用kvm_check_and_inject_events():
+ *   - arch/x86/kvm/x86.c|11048| <<vcpu_enter_guest>> r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
 	bool can_inject;
 	int r;
 
+	/*
+	 * 在以下使用kvm_check_nested_events():
+	 *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+	 */
 	/*
 	 * Process nested events first, as nested VM-Exit supersedes event
 	 * re-injection.  If there's an event queued for re-injection, it will
@@ -10241,6 +10795,10 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 	 * *previous* instruction and must be serviced prior to recognizing any
 	 * new events in order to fully complete the previous instruction.
 	 */
+	/*
+	 * vmx_inject_irq()
+	 * svm_inject_irq()
+	 */
 	if (vcpu->arch.exception.injected)
 		kvm_inject_exception(vcpu);
 	else if (kvm_is_exception_pending(vcpu))
@@ -10249,6 +10807,9 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 		kvm_x86_call(inject_nmi)(vcpu);
 	else if (vcpu->arch.interrupt.injected)
 		kvm_x86_call(inject_irq)(vcpu, true);
+	/*
+	 * vt_inject_irq()
+	 */
 
 	/*
 	 * Exceptions that morph to VM-Exits are handled above, and pending
@@ -10363,24 +10924,63 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 			kvm_x86_call(enable_nmi_window)(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_cpu_has_injectable_intr():
+	 *   - arch/x86/kvm/svm/svm.c|2327| <<svm_set_gif>> if (... kvm_cpu_has_injectable_intr(&svm->vcpu) ||
+	 *   - arch/x86/kvm/vmx/nested.c|5116| <<__nested_vmx_vmexit>> if (kvm_cpu_has_injectable_intr(vcpu) || vcpu->arch.nmi_pending)
+	 *   - arch/x86/kvm/x86.c|10465| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10479| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu))
+	 */
 	if (kvm_cpu_has_injectable_intr(vcpu)) {
 		r = can_inject ? kvm_x86_call(interrupt_allowed)(vcpu, true) :
 				 -EBUSY;
 		if (r < 0)
 			goto out;
 		if (r) {
+			/*
+			 * 在以下使用kvm_cpu_get_interrupt():
+			 *   - arch/x86/kvm/x86.c|10521| <<kvm_check_and_inject_events>> int irq = kvm_cpu_get_interrupt(vcpu);
+			 *
+			 * Read pending interrupt vector and intack.
+			 */
 			int irq = kvm_cpu_get_interrupt(vcpu);
 
 			if (!WARN_ON_ONCE(irq == -1)) {
+				/*
+				 * 在以下使用kvm_queue_interrupt():
+				 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+				 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+				 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+				 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+				 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+				 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+				 */
 				kvm_queue_interrupt(vcpu, irq, false);
 				kvm_x86_call(inject_irq)(vcpu, false);
 				WARN_ON(kvm_x86_call(interrupt_allowed)(vcpu, true) < 0);
 			}
 		}
+		/*
+		 * 在以下使用kvm_x86_ops->enable_irq_window:
+		 *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+		 *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+		 *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+		 *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+		 */
 		if (kvm_cpu_has_injectable_intr(vcpu))
 			kvm_x86_call(enable_irq_window)(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_x86_nested_ops->has_events:
+	 *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+	 *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+	 *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+	 *
+	 * vmx_has_nested_events()
+	 */
 	if (is_guest_mode(vcpu) &&
 	    kvm_x86_ops.nested_ops->has_events &&
 	    kvm_x86_ops.nested_ops->has_events(vcpu, true))
@@ -10460,11 +11060,24 @@ void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
 	kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
 }
 
+/*
+ * 在以下使用kvm_make_scan_ioapic_request():
+ *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+ *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+ */
 void kvm_make_scan_ioapic_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * 在以下使用__kvm_vcpu_update_apicv():
+ *   - arch/x86/kvm/svm/nested.c|1268| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|10558| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+ */
 void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -10473,18 +11086,79 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 *
+	 * 在这里加锁!!!!
+	 */
 	down_read(&vcpu->kvm->arch.apicv_update_lock);
 	preempt_disable();
 
+	/*
+	 * 在以下使用kvm_vcpu_apicv_activated():
+	 *   - arch/x86/kvm/svm/svm.c|1574| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+	 *   - arch/x86/kvm/x86.c|11026| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|11773| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+	 */
 	/* Do not activate APICV when APIC is disabled */
 	activate = kvm_vcpu_apicv_activated(vcpu) &&
 		   (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active == activate)
 		goto out;
 
 	apic->apicv_active = activate;
+	/*
+	 * 在以下使用kvm_apic_update_apicv():
+	 *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+	 *
+	 * 就是修改三个field:
+	 * 1. apic->irr_pending
+	 * 2. apic->isr_count
+	 * 3. apic->highest_isr_cache
+	 */
 	kvm_apic_update_apicv(vcpu);
+	/*
+	 * 在以下使用refresh_apicv_exec_ctrl:
+	 *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+	 *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+	 *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+	 *
+	 * 在__nested_vmx_vmexit()触发KVM_REQ_APICV_UPDATE
+	 *
+	 * vcpu_enter_guest(KVM_REQ_APICV_UPDATE)
+	 * -> kvm_vcpu_update_apicv()
+	 *    -> __kvm_vcpu_update_apicv()
+	 *       -> if (apic->apicv_active == activate) goto out ==> 返回!!!
+	 *       -> vt_refresh_apicv_exec_ctrl()
+	 *
+	 * vmx_refresh_apicv_exec_ctrl()
+	 * avic_refresh_apicv_exec_ctrl()
+	 *
+	 * vmx_refresh_apicv_exec_ctrl()在activate和deactivate APICv的时候都要被调用
+	 */
 	kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
 
 	/*
@@ -10502,6 +11176,10 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(__kvm_vcpu_update_apicv);
 
+/*
+ * 处理KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/x86.c|10838| <<vcpu_enter_guest>> kvm_vcpu_update_apicv(vcpu);
+ */
 static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	if (!lapic_in_kernel(vcpu))
@@ -10518,23 +11196,76 @@ static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	 * despite being in x2APIC mode.  For simplicity, inhibiting the APIC
 	 * access page is sticky.
 	 */
+	/*
+	 * 只在以下使用kvm_inhibit_apic_access_page():
+	 *   - arch/x86/kvm/x86.c|10561| <<kvm_vcpu_update_apicv>> kvm_inhibit_apic_access_page(vcpu);
+	 *
+	 *
+	 * 在以下使用kvm_x86_ops->allow_apicv_in_x2apic_without_x2apic_virtualization:
+	 *   - arch/x86/kvm/svm/svm.c|5734| <<svm_hardware_setup>> svm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization = true;
+	 *   - arch/x86/kvm/x86.c|11103| <<kvm_vcpu_update_apicv>> if (... kvm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization)
+	 *
+	 * 只有amd svm使用
+	 */
 	if (apic_x2apic_mode(vcpu->arch.apic) &&
 	    kvm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization)
 		kvm_inhibit_apic_access_page(vcpu);
 
+	/*
+	 * 在以下使用__kvm_vcpu_update_apicv():
+	 *   - arch/x86/kvm/svm/nested.c|1268| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10558| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+	 */
 	__kvm_vcpu_update_apicv(vcpu);
 }
 
+/*
+ * 在以下使用__kvm_set_or_clear_apicv_inhibit():
+ *   - arch/x86/kvm/hyperv.c|161| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+ *   - arch/x86/kvm/x86.c|11293| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+ *   - arch/x86/kvm/x86.c|13156| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+ *                                                             APICV_INHIBIT_REASON_BLOCKIRQ, set);
+ */
 void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				      enum kvm_apicv_inhibit reason, bool set)
 {
 	unsigned long old, new;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
 
 	if (!(kvm_x86_ops.required_apicv_inhibits & BIT(reason)))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|10175| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|10181| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|10215| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|11153| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|11183| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|11192| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	old = new = kvm->arch.apicv_inhibit_reasons;
 
 	set_or_clear_apicv_inhibit(&new, reason, set);
@@ -10552,6 +11283,18 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 		 * side (handling the request) also prevents other vCPUs from
 		 * servicing the request with a stale apicv_inhibit_reasons.
 		 */
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+		 *
+		 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+		 */
 		kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
 		kvm->arch.apicv_inhibit_reasons = new;
 		if (new) {
@@ -10566,13 +11309,42 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 	}
 }
 
+/*
+ * 在以下使用kvm_set_or_clear_apicv_inhibit():
+ *   - arch/x86/include/asm/kvm_host.h|2560| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+ *   - arch/x86/include/asm/kvm_host.h|2566| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+ */
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set)
 {
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
+	/*
+	 * 在以下使用__kvm_set_or_clear_apicv_inhibit():
+	 *   - arch/x86/kvm/hyperv.c|161| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+	 *   - arch/x86/kvm/x86.c|11293| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+	 *   - arch/x86/kvm/x86.c|13156| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+	 *                                                             APICV_INHIBIT_REASON_BLOCKIRQ, set);
+	 */
 	__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
 	up_write(&kvm->arch.apicv_update_lock);
 }
@@ -10583,11 +11355,38 @@ static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
 	if (!kvm_apic_present(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 */
 	bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
 	vcpu->arch.highest_stale_pending_ioapic_eoi = -1;
 
 	kvm_x86_call(sync_pir_to_irr)(vcpu);
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	if (irqchip_split(vcpu->kvm))
 		kvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);
 #ifdef CONFIG_KVM_IOAPIC
@@ -10610,13 +11409,58 @@ static void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)
 	if (to_hv_vcpu(vcpu)) {
 		u64 eoi_exit_bitmap[4];
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+		 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+		 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+		 *              apic->vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+		 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+		 *              vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+		 *              vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+		 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+		 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+		 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+		 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+		 */
 		bitmap_or((ulong *)eoi_exit_bitmap,
 			  vcpu->arch.ioapic_handled_vectors,
 			  to_hv_synic(vcpu)->vec_bitmap, 256);
+		/*
+		 * 在以下使用kvm_x86_ops->load_eoi_exitmap:
+		 *   - arch/x86/kvm/vmx/main.c|976| <<global>> .load_eoi_exitmap = vt_op(load_eoi_exitmap),
+		 *   - arch/x86/kvm/x86.c|11144| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+		 *   - arch/x86/kvm/x86.c|11165| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(
+		 */
 		kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
 		return;
 	}
 #endif
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 *
+	 * 在以下使用kvm_x86_ops->load_eoi_exitmap:
+	 *   - arch/x86/kvm/vmx/main.c|976| <<global>> .load_eoi_exitmap = vt_op(load_eoi_exitmap),
+	 *   - arch/x86/kvm/x86.c|11144| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+	 *   - arch/x86/kvm/x86.c|11165| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(
+	 */
 	kvm_x86_call(load_eoi_exitmap)(
 		vcpu, (u64 *)vcpu->arch.ioapic_handled_vectors);
 }
@@ -10626,11 +11470,25 @@ void kvm_arch_guest_memory_reclaimed(struct kvm *kvm)
 	kvm_x86_call(guest_memory_reclaimed)(kvm);
 }
 
+/*
+ * 处理KVM_REQ_APIC_PAGE_RELOAD:
+ *   - arch/x86/kvm/x86.c|11485| <<vcpu_enter_guest(KVM_REQ_APIC_PAGE_RELOAD)>> kvm_vcpu_reload_apic_access_page(vcpu);
+ */
 static void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)
 {
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_x86_ops->set_apic_access_page_addr:
+	 *   - arch/x86/kvm/vmx/main.c|992| <<global>> .set_apic_access_page_addr = vt_op(set_apic_access_page_addr),
+	 *   - arch/x86/kvm/mmu/mmu.c|1675| <<kvm_unmap_gfn_range>> if (kvm_x86_ops.set_apic_access_page_addr &&
+	 *   - arch/x86/kvm/vmx/vmx.c|9261| <<vmx_hardware_setup>> vt_x86_ops.set_apic_access_page_addr = NULL;
+	 *   - arch/x86/kvm/x86.c|11306| <<kvm_vcpu_reload_apic_access_page>> kvm_x86_call(set_apic_access_page_addr)(vcpu);
+	 *
+	 * vt_set_apic_access_page_addr()
+	 * vmx_set_apic_access_page_addr()
+	 */
 	kvm_x86_call(set_apic_access_page_addr)(vcpu);
 }
 
@@ -10643,6 +11501,24 @@ static void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
+	/*
+	 * 注释:
+	 * "我们要求一个中断窗口/准备好注入中断"的条件成立.
+	 * 接下来KVM可能产生KVM_EXIT_IRQ_WINDOW_OPEN退出,
+	 * 让用户空间或管理层有机会注入中断
+	 *
+	 * 在以下使用kvm_cpu_accept_dm_intr():
+	 *   - arch/x86/kvm/x86.c|5248| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|10864| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+	 *
+	 * 在以下使用dm_request_for_irq_injection():
+	 *   - arch/x86/kvm/x86.c|10863| <<vcpu_enter_guest>> dm_request_for_irq_injection(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|11478| <<vcpu_run>> if (dm_request_for_irq_injection(vcpu) &&
+	 *
+	 * 满足:
+	 * 1. vcpu->run->request_interrupt_window设置了
+	 * 2. 不在KVM模拟PIC (split??)
+	 */
 	bool req_int_win =
 		dm_request_for_irq_injection(vcpu) &&
 		kvm_cpu_accept_dm_intr(vcpu);
@@ -10663,6 +11539,13 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 
 		if (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {
+			/*
+			 * 在以下使用kvm_x86_nested_ops->get_nested_state_pages:
+			 *   - arch/x86/kvm/svm/nested.c|2230| <<global>> .get_nested_state_pages = svm_get_nested_state_pages,
+			 *   - arch/x86/kvm/vmx/nested.c|8994| <<global>> .get_nested_state_pages = vmx_get_nested_state_pages,
+			 *   - arch/x86/kvm/x86.c|11305| <<vcpu_enter_guest(KVM_REQ_GET_NESTED_STATE_PAGES)>>
+			 *                             if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
+			 */
 			if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
 				r = 0;
 				goto out;
@@ -10744,6 +11627,23 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			process_nmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
 			BUG_ON(vcpu->arch.pending_ioapic_eoi > 255);
+			/*
+			 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+			 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+			 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+			 *              apic->vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+			 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+			 *              vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+			 *              vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+			 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+			 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+			 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+			 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+			 */
 			if (test_bit(vcpu->arch.pending_ioapic_eoi,
 				     vcpu->arch.ioapic_handled_vectors)) {
 				vcpu->run->exit_reason = KVM_EXIT_IOAPIC_EOI;
@@ -10757,6 +11657,17 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			vcpu_scan_ioapic(vcpu);
 		if (kvm_check_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu))
 			vcpu_load_eoi_exitmap(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_APIC_PAGE_RELOAD:
+		 *   - arch/x86/kvm/mmu/mmu.c|1686| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+		 *   - arch/x86/kvm/vmx/nested.c|6089| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5260| <<vmx_vcpu_reset>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|7331| <<vmx_set_virtual_apic_mode>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|7428| <<vmx_set_apic_access_page_addr>> kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
+		 *   - arch/x86/kvm/x86.c|11660| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
+		 *
+		 * 处理的函数: kvm_vcpu_reload_apic_access_page()
+		 */
 		if (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))
 			kvm_vcpu_reload_apic_access_page(vcpu);
 #ifdef CONFIG_KVM_HYPERV
@@ -10791,6 +11702,18 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		if (kvm_check_request(KVM_REQ_HV_STIMER, vcpu))
 			kvm_hv_process_stimers(vcpu);
 #endif
+		/*
+		 * 在以下使用KVM_REQ_APICV_UPDATE:
+		 *   - arch/x86/kvm/lapic.c|3966| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/lapic.c|4630| <<kvm_create_lapic>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1040| <<enter_svm_guest_mode>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1582| <<svm_leave_nested>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6115| <<__nested_vmx_vmexit>> kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11165| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+		 *   - arch/x86/kvm/x86.c|11545| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
+		 *
+		 * 处理KVM_REQ_APICV_UPDATE的函数: kvm_vcpu_update_apicv()
+		 */
 		if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
 			kvm_vcpu_update_apicv(vcpu);
 		if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
@@ -10808,6 +11731,13 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_x86_call(update_cpu_dirty_logging)(vcpu);
 
 		if (kvm_check_request(KVM_REQ_UPDATE_PROTECTED_GUEST_STATE, vcpu)) {
+			/*
+			 * 在以下使用kvm_vcpu_reset():
+			 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+			 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+			 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+			 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+			 */
 			kvm_vcpu_reset(vcpu, true);
 			if (vcpu->arch.mp_state != KVM_MP_STATE_RUNNABLE) {
 				r = 1;
@@ -10819,6 +11749,15 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||
 	    kvm_xen_has_interrupt(vcpu)) {
 		++vcpu->stat.req_event;
+		/*
+		 * 在以下使用kvm_apic_accept_events():
+		 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+		 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+		 *
+		 * 主要针对INIT和SIPI
+		 */
 		r = kvm_apic_accept_events(vcpu);
 		if (r < 0) {
 			r = 0;
@@ -10829,16 +11768,36 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			goto out;
 		}
 
+		/*
+		 * 只在这里使用
+		 */
 		r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
 		if (r < 0) {
 			r = 0;
 			goto out;
 		}
+		/*
+		 * 在以下使用kvm_x86_ops->enable_irq_window:
+		 *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+		 *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+		 *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+		 *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+		 *
+		 * vmx_enable_irq_window()
+		 * svm_enable_irq_window()
+		 */
 		if (req_int_win)
 			kvm_x86_call(enable_irq_window)(vcpu);
 
 		if (kvm_lapic_enabled(vcpu)) {
+			/*
+			 * 它存储的是TPR(Task Priority Register)的值
+			 * 值范围为0~15
+			 */
 			update_cr8_intercept(vcpu);
+			/*
+			 * 只在这里调用
+			 */
 			kvm_lapic_sync_to_vapic(vcpu);
 		}
 	}
@@ -10850,6 +11809,14 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 
 	preempt_disable();
 
+	/*
+	 * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+	 *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+	 *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+	 *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+	 *
+	 * vmx_prepare_switch_to_guest()
+	 */
 	kvm_x86_call(prepare_switch_to_guest)(vcpu);
 
 	/*
@@ -10946,6 +11913,12 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		 * per-VM state, and responding vCPUs must wait for the update
 		 * to complete before servicing KVM_REQ_APICV_UPDATE.
 		 */
+		/*
+		 * 在以下使用kvm_vcpu_apicv_activated():
+		 *   - arch/x86/kvm/svm/svm.c|1574| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+		 *   - arch/x86/kvm/x86.c|11026| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) &&
+		 *   - arch/x86/kvm/x86.c|11773| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+		 */
 		WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
 			     (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
 
@@ -11005,6 +11978,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * vmx_handle_exit_irqoff
+	 * svm_handle_exit_irqoff
+	 */
 	kvm_x86_call(handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -11026,10 +12003,24 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	 * interrupts on processors that implement an interrupt shadow, the
 	 * stat.exits increment will do nicely.
 	 */
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
 	local_irq_enable();
 	++vcpu->stat.exits;
 	local_irq_disable();
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	kvm_after_interrupt(vcpu);
 
 	/*
@@ -11064,12 +12055,31 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (unlikely(vcpu->arch.tsc_always_catchup))
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
 	if (unlikely(exit_fastpath == EXIT_FASTPATH_EXIT_USERSPACE))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_x86_ops->handle_exit:
+	 *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+	 *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+	 *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+	 */
 	r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -11119,12 +12129,38 @@ bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 	if (kvm_test_request(KVM_REQ_UPDATE_PROTECTED_GUEST_STATE, vcpu))
 		return true;
 
+	/*
+	 * 在以下使用kvm_cpu_has_interrupt():
+	 *   - arch/x86/kvm/svm/nested.c|1594| <<svm_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !svm_interrupt_blocked(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4348| <<vmx_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !vmx_interrupt_blocked(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11338| <<kvm_vcpu_has_events>> if (kvm_arch_interrupt_allowed(vcpu) && kvm_cpu_has_interrupt(vcpu))
+	 *
+	 * 先判断有没有PIC或者userspace的interrupt
+	 * 然后:
+	 *     1. 判断kvm_apic_present()
+	 *     2. 根据TPR和ISR更新PPR
+	 *        TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 *     3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 *
+	 * check if there is pending interrupt without
+	 * intack.
+	 */
 	if (kvm_arch_interrupt_allowed(vcpu) && kvm_cpu_has_interrupt(vcpu))
 		return true;
 
 	if (kvm_hv_has_stimer_pending(vcpu))
 		return true;
 
+	/*
+	 * 在以下使用kvm_x86_nested_ops->has_events:
+	 *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+	 *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+	 *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+	 *
+	 * vmx_has_nested_events()
+	 */
 	if (is_guest_mode(vcpu) &&
 	    kvm_x86_ops.nested_ops->has_events &&
 	    kvm_x86_ops.nested_ops->has_events(vcpu, false))
@@ -11186,6 +12222,12 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 	 * causes a spurious wakeup from HLT).
 	 */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用kvm_check_nested_events():
+		 *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+		 */
 		int r = kvm_check_nested_events(vcpu);
 
 		WARN_ON_ONCE(r == -EBUSY);
@@ -11193,6 +12235,15 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 			return 0;
 	}
 
+	/*
+	 * 在以下使用kvm_apic_accept_events():
+	 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+	 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+	 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+	 *
+	 * 主要针对INIT和SIPI
+	 */
 	if (kvm_apic_accept_events(vcpu) < 0)
 		return 0;
 	switch(vcpu->arch.mp_state) {
@@ -11226,6 +12277,13 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 		 * use a stale page translation. Assume that any code after
 		 * this point can start executing an instruction.
 		 */
+		/*
+		 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+		 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+		 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+		 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+		 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+		 */
 		vcpu->arch.at_instruction_boundary = false;
 		if (kvm_vcpu_running(vcpu)) {
 			r = vcpu_enter_guest(vcpu);
@@ -11243,6 +12301,20 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 		if (kvm_cpu_has_pending_timer(vcpu))
 			kvm_inject_pending_timer_irqs(vcpu);
 
+		/*
+		 * 在以下使用dm_request_for_irq_injection():
+		 *   - arch/x86/kvm/x86.c|10863| <<vcpu_enter_guest>> dm_request_for_irq_injection(vcpu) &&
+		 *   - arch/x86/kvm/x86.c|11478| <<vcpu_run>> if (dm_request_for_irq_injection(vcpu) &&
+		 *
+		 * 满足:
+		 * 1. vcpu->run->request_interrupt_window设置了
+		 * 2. 不在KVM模拟PIC (split??)
+		 *
+		 *
+		 * 在以下使用kvm_vcpu_ready_for_interrupt_injection():
+		 *   - arch/x86/kvm/x86.c|10251| <<post_kvm_run_save>> kvm_vcpu_ready_for_interrupt_injection(vcpu);
+		 *   - arch/x86/kvm/x86.c|11479| <<vcpu_run>> kvm_vcpu_ready_for_interrupt_injection(vcpu)) {
+		 */
 		if (dm_request_for_irq_injection(vcpu) &&
 			kvm_vcpu_ready_for_interrupt_injection(vcpu)) {
 			r = 0;
@@ -11253,6 +12325,13 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 
 		if (__xfer_to_guest_mode_work_pending()) {
 			kvm_vcpu_srcu_read_unlock(vcpu);
+			/*
+			 * 在以下使用xfer_to_guest_mode_handle_work():
+			 *   - arch/arm64/kvm/arm.c|1180| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/loongarch/kvm/vcpu.c|254| <<kvm_enter_guest_check>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/riscv/kvm/vcpu.c|913| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/x86/kvm/x86.c|11796| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+			 */
 			r = xfer_to_guest_mode_handle_work(vcpu);
 			kvm_vcpu_srcu_read_lock(vcpu);
 			if (r)
@@ -11504,6 +12583,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		kvm_vcpu_block(vcpu);
 		kvm_vcpu_srcu_read_lock(vcpu);
 
+		/*
+		 * 在以下使用kvm_apic_accept_events():
+		 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+		 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+		 *
+		 * 主要针对INIT和SIPI
+		 */
 		if (kvm_apic_accept_events(vcpu) < 0) {
 			r = 0;
 			goto out;
@@ -11710,6 +12798,18 @@ static void __get_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 skip_protected_regs:
 	sregs->cr0 = kvm_read_cr0(vcpu);
 	sregs->cr4 = kvm_read_cr4(vcpu);
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	sregs->cr8 = kvm_get_cr8(vcpu);
 	sregs->efer = vcpu->arch.efer;
 	sregs->apic_base = vcpu->arch.apic_base;
@@ -11767,6 +12867,15 @@ int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 
 	kvm_vcpu_srcu_read_lock(vcpu);
 
+	/*
+	 * 在以下使用kvm_apic_accept_events():
+	 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+	 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+	 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+	 *
+	 * 主要针对INIT和SIPI
+	 */
 	r = kvm_apic_accept_events(vcpu);
 	if (r < 0)
 		goto out;
@@ -11885,6 +12994,11 @@ static bool kvm_is_valid_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 	       kvm_is_valid_cr0(vcpu, sregs->cr0);
 }
 
+/*
+ * 在以下使用__set_sregs_common():
+ *   - arch/x86/kvm/x86.c|12732| <<__set_sregs>> int ret = __set_sregs_common(vcpu, sregs, &mmu_reset_needed, true);
+ *   - arch/x86/kvm/x86.c|12777| <<__set_sregs2>> ret = __set_sregs_common(vcpu, (struct kvm_sregs *)sregs2,
+ */
 static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 		int *mmu_reset_needed, bool update_pdptrs)
 {
@@ -11894,6 +13008,33 @@ static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 	if (!kvm_is_valid_sregs(vcpu, sregs))
 		return -EINVAL;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 *
+	 * 在以下使用kvm_apic_set_base():
+	 *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+	 *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+	 *                         msr_info->host_initiated);
+	 *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+	 */
 	if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
 		return -EINVAL;
 
@@ -11954,6 +13095,11 @@ static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 	return 0;
 }
 
+/*
+ * 在以下使用__set_sregs():
+ *   - arch/x86/kvm/x86.c|12807| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+ *   - arch/x86/kvm/x86.c|13003| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+ */
 static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 {
 	int pending_vec, max_bits;
@@ -11973,6 +13119,15 @@ static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 		(const unsigned long *)sregs->interrupt_bitmap, max_bits);
 
 	if (pending_vec < max_bits) {
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, pending_vec, false);
 		pr_debug("Set back pending irq %d\n", pending_vec);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
@@ -12024,6 +13179,11 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 		return -EINVAL;
 
 	vcpu_load(vcpu);
+	/*
+	 * 在以下使用__set_sregs():
+	 *   - arch/x86/kvm/x86.c|12807| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+	 *   - arch/x86/kvm/x86.c|13003| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+	 */
 	ret = __set_sregs(vcpu, sregs);
 	vcpu_put(vcpu);
 	return ret;
@@ -12038,6 +13198,23 @@ static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *
+	 * 根据注释, kvm_arch->apicv_update_lock(rw_semaphore)保护apicv_inhibit_reasons.
+	 * down_write(), 获取写锁, 阻塞, 独占访问, 不允许其他读/写
+	 * down_read(), 获取读锁, 阻塞(若有写者), 多个读者可共享
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
@@ -12046,6 +13223,13 @@ static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 			break;
 		}
 	}
+	/*
+	 * 在以下使用__kvm_set_or_clear_apicv_inhibit():
+	 *   - arch/x86/kvm/hyperv.c|161| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+	 *   - arch/x86/kvm/x86.c|11293| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+	 *   - arch/x86/kvm/x86.c|13156| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+	 *                                                             APICV_INHIBIT_REASON_BLOCKIRQ, set);
+	 */
 	__kvm_set_or_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_BLOCKIRQ, set);
 	up_write(&kvm->arch.apicv_update_lock);
 }
@@ -12197,6 +13381,10 @@ static void store_regs(struct kvm_vcpu *vcpu)
 				vcpu, &vcpu->run->s.regs.events);
 }
 
+/*
+ * 在以下使用sync_regs():
+ *   - arch/x86/kvm/x86.c|12251| <<kvm_arch_vcpu_ioctl_run>> r = if (kvm_run->kvm_dirty_regs) { sync_regs(vcpu);
+ */
 static int sync_regs(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {
@@ -12207,6 +13395,11 @@ static int sync_regs(struct kvm_vcpu *vcpu)
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_SREGS) {
 		struct kvm_sregs sregs = vcpu->run->s.regs.sregs;
 
+		/*
+		 * 在以下使用__set_sregs():
+		 *   - arch/x86/kvm/x86.c|12807| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+		 *   - arch/x86/kvm/x86.c|13003| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+		 */
 		if (__set_sregs(vcpu, &sregs))
 			return -EINVAL;
 
@@ -12247,6 +13440,15 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.last_vmentry_cpu = -1;
 	vcpu->arch.regs_avail = ~0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	vcpu->arch.regs_dirty = ~0;
 
 	kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
@@ -12300,6 +13502,15 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	}
 	kvm_pmu_init(vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	vcpu->arch.pending_external_vector = -1;
 	vcpu->arch.preempted_in_kernel = false;
 
@@ -12315,6 +13526,13 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	vcpu_load(vcpu);
 	kvm_vcpu_after_set_cpuid(vcpu);
 	kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	/*
+	 * 在以下使用kvm_vcpu_reset():
+	 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+	 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+	 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+	 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+	 */
 	kvm_vcpu_reset(vcpu, false);
 	kvm_init_mmu(vcpu);
 	vcpu_put(vcpu);
@@ -12388,6 +13606,13 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kvfree(vcpu->arch.cpuid_entries);
 }
 
+/*
+ * 在以下使用kvm_vcpu_reset():
+ *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -12404,6 +13629,12 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	WARN_ON_ONCE(!init_event &&
 		     (old_cr0 || kvm_read_cr3(vcpu) || kvm_read_cr4(vcpu)));
 
+	/*
+	 * 在以下使用kvm_leave_nested():
+	 *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+	 */
 	/*
 	 * SVM doesn't unconditionally VM-Exit on INIT and SHUTDOWN, thus it's
 	 * possible to INIT the vCPU while L2 is active.  Force the vCPU back
@@ -12489,6 +13720,12 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	cpuid_0x1 = kvm_find_cpuid_entry(vcpu, 1);
 	kvm_rdx_write(vcpu, cpuid_0x1 ? cpuid_0x1->eax : 0x600);
 
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_reset:
+	 *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+	 *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+	 *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+	 */
 	kvm_x86_call(vcpu_reset)(vcpu, init_event);
 
 	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
@@ -12672,6 +13909,27 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_is_reset_bsp);
 
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
 }
 
@@ -12715,6 +13973,14 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	atomic_set(&kvm->arch.noncoherent_dma_count, 0);
 
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->apic_map_lock:
+	 *   - arch/x86/kvm/lapic.c|412| <<kvm_recalculate_apic_map>> mutex_lock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|425| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|500| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/x86.c|13686| <<kvm_arch_init_vm>> mutex_init(&kvm->arch.apic_map_lock);
+	 */
 	mutex_init(&kvm->arch.apic_map_lock);
 	seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
@@ -12862,6 +14128,15 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		 * or fd copying.
 		 */
 		mutex_lock(&kvm->slots_lock);
+		/*
+		 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+		 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+		 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+		 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+		 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+		 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 */
 		__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
 					0, 0);
 		__x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
@@ -12875,6 +14150,17 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_pic_destroy(kvm);
 	kvm_ioapic_destroy(kvm);
 #endif
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
 	kfree(srcu_dereference_check(kvm->arch.pmu_event_filter, &kvm->srcu, 1));
 	kvm_mmu_uninit_vm(kvm);
@@ -13462,6 +14748,19 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 	    kvm_pv_async_pf_enabled(vcpu) &&
 	    !apf_put_user_ready(vcpu, work->arch.token)) {
 		vcpu->arch.apf.pageready_pending = true;
+		/*
+		 * 在以下使用kvm_apic_set_irq():
+		 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *
+		 * 调用__apic_accept_irq()
+		 */
 		kvm_apic_set_irq(vcpu, &irq, NULL);
 	}
 
@@ -13942,6 +15241,15 @@ module_init(kvm_x86_init);
 
 static void __exit kvm_x86_exit(void)
 {
+	/*
+	 * 在以下使用kvm_has_noapic_vcpu:
+	 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+	 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+	 */
 	WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
 }
 module_exit(kvm_x86_exit);
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index bcfd9b719..1ad501750 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -137,9 +137,21 @@ static inline unsigned int __shrink_ple_window(unsigned int val,
 void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu);
 int kvm_check_nested_events(struct kvm_vcpu *vcpu);
 
+/*
+ * 在以下使用kvm_leave_nested():
+ *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+ */
 /* Forcibly leave the nested mode in cases like a vCPU reset */
 static inline void kvm_leave_nested(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_x86_nested_ops->leave_nested:
+	 *   - arch/x86/kvm/svm/nested.c|2093| <<global>> .leave_nested = svm_leave_nested,
+	 *   - arch/x86/kvm/vmx/nested.c|8987| <<global>> .leave_nested = vmx_leave_nested,
+	 *   - arch/x86/kvm/x86.h|143| <<kvm_leave_nested>> kvm_x86_ops.nested_ops->leave_nested(vcpu);
+	 */
 	kvm_x86_ops.nested_ops->leave_nested(vcpu);
 }
 
@@ -166,6 +178,24 @@ static inline bool kvm_vcpu_has_run(struct kvm_vcpu *vcpu)
 	return vcpu->arch.last_vmentry_cpu != -1;
 }
 
+/*
+ * 在以下使用kvm_set_mp_state():
+ *   - arch/x86/kvm/lapic.c|4828| <<kvm_apic_accept_events>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/lapic.c|4830| <<kvm_apic_accept_events>> kvm_set_mp_state(vcpu, KVM_MP_STATE_INIT_RECEIVED);
+ *   - arch/x86/kvm/lapic.c|4847| <<kvm_apic_accept_events>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/svm/nested.c|1091| <<nested_svm_vmexit>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/svm/sev.c|3903| <<sev_snp_init_protected_guest_state>> kvm_set_mp_state(vcpu, KVM_MP_STATE_HALTED);
+ *   - arch/x86/kvm/svm/sev.c|3946| <<sev_snp_init_protected_guest_state>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/vmx/nested.c|3813| <<nested_vmx_run>> kvm_set_mp_state(vcpu, KVM_MP_STATE_INIT_RECEIVED);
+ *   - arch/x86/kvm/vmx/nested.c|5129| <<__nested_vmx_vmexit>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|11823| <<vcpu_block>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|11929| <<__kvm_emulate_halt>> kvm_set_mp_state(vcpu, state);
+ *   - arch/x86/kvm/x86.c|12505| <<kvm_arch_vcpu_ioctl_set_mpstate>> kvm_set_mp_state(vcpu, mp_state->mp_state);
+ *   - arch/x86/kvm/x86.c|12659| <<__set_sregs_common>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|12984| <<kvm_arch_vcpu_create>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|12986| <<kvm_arch_vcpu_create>> kvm_set_mp_state(vcpu, KVM_MP_STATE_UNINITIALIZED);
+ *   - arch/x86/kvm/x86.c|14237| <<kvm_arch_async_page_present>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ */
 static inline void kvm_set_mp_state(struct kvm_vcpu *vcpu, int mp_state)
 {
 	vcpu->arch.mp_state = mp_state;
@@ -187,6 +217,15 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.exception_vmexit.pending = false;
 }
 
+/*
+ * 在以下使用kvm_queue_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+ *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+ *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+ *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+ *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+ *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+ */
 static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 	bool soft)
 {
@@ -200,6 +239,16 @@ static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.interrupt.injected = false;
 }
 
+/*
+ * 在以下使用kvm_event_needs_reinjection():
+ *   - arch/x86/kvm/mmu/mmu.c|6292| <<kvm_mmu_write_protect_fault>> if (... (!direct && kvm_event_needs_reinjection(vcpu))) &&
+ *   - arch/x86/kvm/svm/nested.c|1549| <<svm_check_nested_events>> bool block_nested_events = block_nested_exceptions ||
+ *                                        kvm_event_needs_reinjection(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4232| <<vmx_check_nested_events>> bool block_non_injected_events = kvm_event_needs_reinjection(vcpu);
+ *   - arch/x86/kvm/x86.c|5249| <<kvm_vcpu_ready_for_interrupt_injection>> if (... !kvm_event_needs_reinjection(vcpu) &&
+ *   - arch/x86/kvm/x86.c|10435| <<kvm_check_and_inject_events>> can_inject = !kvm_event_needs_reinjection(vcpu);
+ *   - arch/x86/kvm/x86.c|13664| <<kvm_can_do_async_pf>> if (... kvm_event_needs_reinjection(vcpu) ||
+ */
 static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.exception.injected || vcpu->arch.interrupt.injected ||
@@ -556,12 +605,26 @@ static inline bool kvm_notify_vmexit_enabled(struct kvm *kvm)
 	return kvm->arch.notify_vmexit_flags & KVM_X86_NOTIFY_VMEXIT_ENABLED;
 }
 
+/*
+ * 在以下使用kvm_before_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ */
 static __always_inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,
 						 enum kvm_intr_type intr)
 {
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
 }
 
+/*
+ * 在以下使用kvm_after_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+ */
 static __always_inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 {
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index d6b2a665b..c9421f827 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -626,6 +626,16 @@ void kvm_xen_inject_vcpu_vector(struct kvm_vcpu *v)
 	irq.delivery_mode = APIC_DM_FIXED;
 	irq.level = 1;
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
 }
 
diff --git a/drivers/block/zram/zcomp.c b/drivers/block/zram/zcomp.c
index b1bd1daa0..cc93448be 100644
--- a/drivers/block/zram/zcomp.c
+++ b/drivers/block/zram/zcomp.c
@@ -225,6 +225,10 @@ void zcomp_destroy(struct zcomp *comp)
 	kfree(comp);
 }
 
+/*
+ * 在以下使用zcomp_create():
+ *   - drivers/block/zram/zram_drv.c|2522| <<disksize_store>> comp = zcomp_create(zram->comp_algs[prio],
+ */
 struct zcomp *zcomp_create(const char *alg, struct zcomp_params *params)
 {
 	struct zcomp *comp;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 975bdc5da..1a4a74624 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -843,6 +843,11 @@ static struct sk_buff *virtnet_build_skb(void *buf, unsigned int buflen,
 	return skb;
 }
 
+/*
+ * 在以下使用page_to_skb():
+ *   - drivers/net/virtio_net.c|2111| <<receive_big>> page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
+ *   - drivers/net/virtio_net.c|2496| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
+ */
 /* Called from bottom half context */
 static struct sk_buff *page_to_skb(struct virtnet_info *vi,
 				   struct receive_queue *rq,
@@ -2107,6 +2112,11 @@ static struct sk_buff *receive_big(struct net_device *dev,
 				   struct virtnet_rq_stats *stats)
 {
 	struct page *page = buf;
+	/*
+	 * 在以下使用page_to_skb():
+	 *   - drivers/net/virtio_net.c|2111| <<receive_big>> page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
+	 *   - drivers/net/virtio_net.c|2496| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
+	 */
 	struct sk_buff *skb =
 		page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
 
@@ -2493,6 +2503,11 @@ static struct sk_buff *receive_mergeable(struct net_device *dev,
 		rcu_read_unlock();
 	}
 
+	/*
+	 * 在以下使用page_to_skb():
+	 *   - drivers/net/virtio_net.c|2111| <<receive_big>> page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
+	 *   - drivers/net/virtio_net.c|2496| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
+	 */
 	head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
 	curr_skb = head_skb;
 
diff --git a/drivers/target/target_core_device.c b/drivers/target/target_core_device.c
index 7bb711b24..1cfab675a 100644
--- a/drivers/target/target_core_device.c
+++ b/drivers/target/target_core_device.c
@@ -1105,6 +1105,11 @@ void core_dev_release_virtual_lun0(void)
 /*
  * Common CDB parsing for kernel and user passthrough.
  */
+/*
+ * 在以下使用passthrough_parse_cdb():
+ *   - drivers/target/target_core_pscsi.c|928| <<pscsi_parse_cdb>> return passthrough_parse_cdb(cmd, pscsi_execute_cmd);
+ *   - drivers/target/target_core_user.c|2670| <<tcmu_parse_cdb>> return passthrough_parse_cdb(cmd, tcmu_queue_cmd);
+ */
 sense_reason_t
 passthrough_parse_cdb(struct se_cmd *cmd,
 	sense_reason_t (*exec_cmd)(struct se_cmd *cmd))
@@ -1144,6 +1149,14 @@ passthrough_parse_cdb(struct se_cmd *cmd,
 		if (cdb[0] == PERSISTENT_RESERVE_IN) {
 			cmd->execute_cmd = target_scsi3_emulate_pr_in;
 			size = get_unaligned_be16(&cdb[7]);
+			/*
+			 * 在以下使用target_cmd_size_check():
+			 *   - drivers/target/target_core_device.c|1147| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_device.c|1152| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_device.c|1161| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_device.c|1169| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_sbc.c|1067| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 */
 			return target_cmd_size_check(cmd, size);
 		}
 		if (cdb[0] == PERSISTENT_RESERVE_OUT) {
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index fe8beb7db..e612a229e 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -764,6 +764,12 @@ sbc_check_dpofua(struct se_device *dev, struct se_cmd *cmd, unsigned char *cdb)
 	return 0;
 }
 
+/*
+ * 在以下使用sbc_parse_cdb():
+ *   - drivers/target/target_core_file.c|909| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_exec_cmd_ops);
+ *   - drivers/target/target_core_iblock.c|1162| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_exec_cmd_ops);
+ *   - drivers/target/target_core_rd.c|653| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_exec_cmd_ops);
+ */
 sense_reason_t
 sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 {
@@ -1064,6 +1070,14 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 			size = sbc_get_size(cmd, sectors);
 	}
 
+	/*
+	 * 在以下使用target_cmd_size_check():
+	 *   - drivers/target/target_core_device.c|1147| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_device.c|1152| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_device.c|1161| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_device.c|1169| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_sbc.c|1067| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 */
 	return target_cmd_size_check(cmd, size);
 }
 EXPORT_SYMBOL(sbc_parse_cdb);
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 0a76bdfe5..6cb80901d 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -1382,6 +1382,14 @@ target_check_max_data_sg_nents(struct se_cmd *cmd, struct se_device *dev,
  *
  * Return: TCM_NO_SENSE
  */
+/*
+ * 在以下使用target_cmd_size_check():
+ *   - drivers/target/target_core_device.c|1147| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1152| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1161| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1169| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_sbc.c|1067| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+ */
 sense_reason_t
 target_cmd_size_check(struct se_cmd *cmd, unsigned int size)
 {
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 35ded4330..8453958c0 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -440,6 +440,14 @@ static void vhost_net_disable_vq(struct vhost_net *n,
 	struct vhost_poll *poll = n->poll + (nvq - n->vqs);
 	if (!vhost_vq_get_backend(vq))
 		return;
+	/*
+	 * 在以下使用vhost_poll_stop():
+	 *   - drivers/vhost/net.c|443| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+	 *   - drivers/vhost/vhost.c|319| <<vhost_poll_start>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/vhost.c|1450| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+	 *   - drivers/vhost/vhost.c|2530| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+	 */
 	vhost_poll_stop(poll);
 }
 
@@ -1360,11 +1368,34 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		n->vqs[i].rx_ring = NULL;
 		vhost_net_buf_init(&n->vqs[i].rxq);
 	}
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1371| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT,
+	 *       VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2413| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *       nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1431| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *       nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|700| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *       ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
 		       UIO_MAXIOV + VHOST_NET_BATCH,
 		       VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,
 		       NULL);
 
+	/*
+	 * 在以下使用vhost_poll_init():
+	 *   - drivers/vhost/vhost.c|682| <<vhost_dev_init>> vhost_poll_init(&vq->poll,
+	 *                 vq->handle_kick, EPOLLIN, dev, vq);
+	 *   - drivers/vhost/net.c|1368| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX,
+	 *                 handle_tx_net, EPOLLOUT, dev, vqs[VHOST_NET_VQ_TX]);
+	 *   - drivers/vhost/net.c|1370| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX,
+	 *                 handle_rx_net, EPOLLIN, dev, vqs[VHOST_NET_VQ_RX]);
+	 */
 	vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev,
 			vqs[VHOST_NET_VQ_TX]);
 	vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev,
@@ -1701,6 +1732,13 @@ static long vhost_net_set_owner(struct vhost_net *n)
 	int r;
 
 	mutex_lock(&n->dev.mutex);
+	/*
+	 * 在以下使用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1735| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|1670| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|1747| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 *   - drivers/vhost/vhost.c|2989| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> if (vhost_dev_has_owner(d)) {
+	 */
 	if (vhost_dev_has_owner(&n->dev)) {
 		r = -EBUSY;
 		goto out;
@@ -1806,7 +1844,23 @@ static long vhost_net_ioctl(struct file *f, unsigned int ioctl,
 		return vhost_net_set_owner(n);
 	default:
 		mutex_lock(&n->dev.mutex);
+		/*
+		 * 在以下使用vhost_dev_ioctl():
+		 *   - drivers/vhost/net.c|1847| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2537| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|367| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|886| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|910| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		/*
+		 * 在以下使用vhost_vring_ioctl():
+		 *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2540| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|369| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|719| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|912| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		if (r == -ENOIOCTLCMD)
 			r = vhost_vring_ioctl(&n->dev, ioctl, argp);
 		else
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 98e4f68f4..e1163ed48 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -39,6 +39,32 @@
 
 #include "vhost.h"
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ */
+
 #define VHOST_SCSI_VERSION  "v0.1"
 #define VHOST_SCSI_NAMELEN 256
 #define VHOST_SCSI_MAX_CDB_SIZE 32
@@ -252,7 +278,17 @@ struct vhost_scsi {
 };
 
 struct vhost_scsi_tmf {
+	/*
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1627| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1666| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	struct vhost_work vwork;
+	/*
+	 * 在以下使用vhost_scsi_tmf->flush_work:
+	 *   - drivers/vhost/scsi.c|516| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+	 *   - drivers/vhost/scsi.c|1668| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	 */
 	struct work_struct flush_work;
 	struct vhost_scsi *vhost;
 	struct vhost_scsi_virtqueue *svq;
@@ -482,6 +518,11 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 		struct vhost_scsi_tmf *tmf = container_of(se_cmd,
 					struct vhost_scsi_tmf, se_cmd);
 
+		/*
+		 * 在以下使用vhost_scsi_tmf->flush_work:
+		 *   - drivers/vhost/scsi.c|516| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+		 *   - drivers/vhost/scsi.c|1668| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+		 */
 		schedule_work(&tmf->flush_work);
 	} else {
 		struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -490,6 +531,15 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 					struct vhost_scsi_virtqueue, vq);
 
 		llist_add(&cmd->tvc_completion_list, &svq->completion_list);
+		/*
+		 * 在以下使用vhost_vq_work_queue():
+		 *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+		 *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+		 *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+		 *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+		 *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+		 *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+		 */
 		if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
 			vhost_scsi_drop_cmds(svq);
 	}
@@ -1545,6 +1595,14 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		pr_err("Faulted on virtio_scsi_ctrl_tmf_resp\n");
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf->vwork:
+ *   - drivers/vhost/scsi.c|1627| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+ *   - drivers/vhost/scsi.c|1666| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *
+ * 在以下使用vhost_scsi_tmf_resp_work():
+ *   - drivers/vhost/scsi.c|1662| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ */
 static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
@@ -1566,6 +1624,14 @@ static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 	vhost_scsi_release_tmf_res(tmf);
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf->flush_work:
+ *   - drivers/vhost/scsi.c|516| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+ *   - drivers/vhost/scsi.c|1668| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+ *
+ * 在以下使用vhost_scsi_tmf_flush_work():
+ *   - drivers/vhost/scsi.c|1668| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+ */
 static void vhost_scsi_tmf_flush_work(struct work_struct *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
@@ -1576,6 +1642,19 @@ static void vhost_scsi_tmf_flush_work(struct work_struct *work)
 	 * send our response.
 	 */
 	vhost_dev_flush(vq->dev);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1627| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1666| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	if (!vhost_vq_work_queue(vq, &tmf->vwork))
 		vhost_scsi_release_tmf_res(tmf);
 }
@@ -1604,7 +1683,26 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 	if (!tmf)
 		goto send_reject;
 
+	/*
+	 * 在以下使用vhost_scsi_tmf->flush_work:
+	 *   - drivers/vhost/scsi.c|516| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+	 *   - drivers/vhost/scsi.c|1668| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	 */
 	INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 *
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1627| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1666| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
 	tmf->vhost = vs;
 	tmf->svq = svq;
@@ -1821,6 +1919,15 @@ vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	}
 
 	llist_add(&evt->list, &vs->vs_event_list);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
 		vhost_scsi_complete_events(vs, true);
 }
@@ -1867,6 +1974,17 @@ static void vhost_scsi_flush(struct vhost_scsi *vs)
 	for (i = 0; i < vs->dev.nvqs; i++)
 		kref_put(&vs->old_inflight[i]->kref, vhost_scsi_done_inflight);
 
+	/*
+	 * 在以下使用vhost_dev_flush():
+	 *   - drivers/vhost/net.c|1436| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/net.c|1627| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/scsi.c|1613| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+	 *   - drivers/vhost/scsi.c|1934| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+	 *   - drivers/vhost/test.c|163| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/vhost.c|2282| <<vhost_dev_stop>> vhost_dev_flush(dev);
+	 *   - drivers/vhost/vhost.c|3385| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+	 *   - drivers/vhost/vsock.c|730| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+	 */
 	/* Flush both the vhost poll and vhost work */
 	vhost_dev_flush(&vs->dev);
 
@@ -2308,6 +2426,16 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 	if (!vqs)
 		goto err_local_vqs;
 
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
 
 	vs->vs_events_nr = 0;
@@ -2323,10 +2451,34 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 		vqs[i] = &svq->vq;
 		svq->vs = vs;
 		init_llist_head(&svq->completion_list);
+		/*
+		 * 在以下使用vhost_work_init():
+		 *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+		 *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+		 *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+		 *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+		 *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+		 *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+		 *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+		 */
 		vhost_work_init(&svq->completion_work,
 				vhost_scsi_complete_cmd_work);
 		svq->vq.handle_kick = vhost_scsi_handle_kick;
 	}
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1371| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT,
+	 *       VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2413| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *       nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1431| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *       nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|700| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *       ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vs->dev, vqs, nvqs, UIO_MAXIOV,
 		       VHOST_SCSI_WEIGHT, 0, true, NULL);
 
@@ -2376,6 +2528,11 @@ vhost_scsi_ioctl(struct file *f,
 	u32 events_missed;
 	u64 features;
 	int r, abi_version = VHOST_SCSI_ABI_VERSION;
+	/*
+	 * struct vhost_virtqueue:
+	 * -> struct vhost_worker __rcu *worker;
+	 * -> struct vhost_poll poll;
+	 */
 	struct vhost_virtqueue *vq = &vs->vqs[VHOST_SCSI_VQ_EVT].vq;
 
 	switch (ioctl) {
@@ -2425,12 +2582,31 @@ vhost_scsi_ioctl(struct file *f,
 	case VHOST_ATTACH_VRING_WORKER:
 	case VHOST_GET_VRING_WORKER:
 		mutex_lock(&vs->dev.mutex);
+		/*
+		 * 只在这里调用
+		 */
 		r = vhost_worker_ioctl(&vs->dev, ioctl, argp);
 		mutex_unlock(&vs->dev.mutex);
 		return r;
 	default:
 		mutex_lock(&vs->dev.mutex);
+		/*
+		 * 在以下使用vhost_dev_ioctl():
+		 *   - drivers/vhost/net.c|1847| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2537| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|367| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|886| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|910| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+		/*
+		 * 在以下使用vhost_vring_ioctl():
+		 *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2540| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|369| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|719| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|912| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		/* TODO: flush backend after dev ioctl. */
 		if (r == -ENOIOCTLCMD)
 			r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
diff --git a/drivers/vhost/test.c b/drivers/vhost/test.c
index 42c955a5b..96e68e22c 100644
--- a/drivers/vhost/test.c
+++ b/drivers/vhost/test.c
@@ -119,6 +119,20 @@ static int vhost_test_open(struct inode *inode, struct file *f)
 	dev = &n->dev;
 	vqs[VHOST_TEST_VQ] = &n->vqs[VHOST_TEST_VQ];
 	n->vqs[VHOST_TEST_VQ].handle_kick = handle_vq_kick;
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1371| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT,
+	 *       VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2413| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *       nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1431| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *       nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|700| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *       ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
 		       VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
 
@@ -282,6 +296,14 @@ static long vhost_test_set_backend(struct vhost_test *n, unsigned index, int fd)
 		goto err_vq;
 	}
 	if (!enable) {
+		/*
+		 * 在以下使用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|443| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|319| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1450| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 */
 		vhost_poll_stop(&vq->poll);
 		backend = vhost_vq_get_backend(vq);
 		vhost_vq_set_backend(vq, NULL);
diff --git a/drivers/vhost/vdpa.c b/drivers/vhost/vdpa.c
index af1e1fdfd..e74219e43 100644
--- a/drivers/vhost/vdpa.c
+++ b/drivers/vhost/vdpa.c
@@ -716,6 +716,14 @@ static long vhost_vdpa_vring_ioctl(struct vhost_vdpa *v, unsigned int cmd,
 		break;
 	}
 
+	/*
+	 * 在以下使用vhost_vring_ioctl():
+	 *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+	 *   - drivers/vhost/scsi.c|2540| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+	 *   - drivers/vhost/test.c|369| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+	 *   - drivers/vhost/vdpa.c|719| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+	 *   - drivers/vhost/vsock.c|912| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+	 */
 	r = vhost_vring_ioctl(&v->vdev, cmd, argp);
 	if (r)
 		return r;
@@ -883,6 +891,14 @@ static long vhost_vdpa_unlocked_ioctl(struct file *filep,
 		r = vhost_vdpa_resume(v);
 		break;
 	default:
+		/*
+		 * 在以下使用vhost_dev_ioctl():
+		 *   - drivers/vhost/net.c|1847| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2537| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|367| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|886| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|910| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		r = vhost_dev_ioctl(&v->vdev, cmd, argp);
 		if (r == -ENOIOCTLCMD)
 			r = vhost_vdpa_vring_ioctl(v, cmd, argp);
@@ -1428,6 +1444,20 @@ static int vhost_vdpa_open(struct inode *inode, struct file *filep)
 		vqs[i]->handle_kick = handle_vq_kick;
 		vqs[i]->call_ctx.ctx = NULL;
 	}
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1371| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT,
+	 *       VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2413| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *       nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1431| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *       nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|700| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *       ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,
 		       vhost_vdpa_process_iotlb_msg);
 
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 8570fdf2e..cc542edc5 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -34,6 +34,40 @@
 
 #include "vhost.h"
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ */
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ */
+
 static ushort max_mem_regions = 64;
 module_param(max_mem_regions, ushort, 0444);
 MODULE_PARM_DESC(max_mem_regions,
@@ -159,6 +193,82 @@ static void vhost_flush_work(struct vhost_work *work)
 	complete(&s->wait_event);
 }
 
+/*
+ * 这是4.14的
+ *
+ * [0] vhost_poll_func
+ * [0] eventfd_poll
+ * [0] vhost_poll_start.part.19
+ * [0] vhost_vring_ioctl
+ * [0] vhost_scsi_ioctl
+ * [0] do_vfs_ioctl
+ * [0] SyS_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] vhost_poll_func
+ * [0] vhost_poll_start.part.19
+ * [0] vhost_poll_start
+ * [0] vhost_net_enable_vq
+ * [0] handle_rx
+ * [0] handle_rx_net
+ * [0] vhost_worker
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ *
+ * 这是5.15的
+ *
+ * vhost_poll_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * ioeventfd_write
+ * __kvm_io_bus_write
+ * kvm_io_bus_write
+ * write_mmio
+ * emulator_read_write_onepage
+ * emulator_read_write
+ * x86_emulate_insn
+ * x86_emulate_instruction
+ * vcpu_enter_guest
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ * __GI___ioctl
+ * b'CPU 9/KVM' [10719]
+ *
+ *
+ * vhost_scsi_handle_vq
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ * b'vhost-10719' [10740]
+ *
+ *
+ * vhost_scsi_release_cmd
+ * target_release_cmd_kref
+ * target_put_sess_cmd
+ * process_one_work
+ * worker_thread
+ * kthread
+ * ret_from_fork
+ * b'kworker/53:0' [7266]
+ *
+ *
+ * vhost_scsi_complete_cmd_work
+ * vhost_worker
+ * kthread
+ * ret_from_fork
+ * b'vhost-10719' [10740]
+ */
+
+/*
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|255| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt)
 {
@@ -166,9 +276,41 @@ static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 
 	poll = container_of(pt, struct vhost_poll, table);
 	poll->wqh = wqh;
+	/*
+	 * wqh是eventfd_ctx->wqh
+	 */
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * 这是5.15的
+ *      
+ * vhost_poll_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * ioeventfd_write
+ * __kvm_io_bus_write
+ * kvm_io_bus_write
+ * write_mmio
+ * emulator_read_write_onepage
+ * emulator_read_write
+ * x86_emulate_insn          
+ * x86_emulate_instruction
+ * vcpu_enter_guest
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ * __GI___ioctl 
+ * b'CPU 9/KVM' [10719]
+ *
+ *
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|254| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|330| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask))
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -178,6 +320,22 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	if (!(key_to_poll(key) & poll->mask))
 		return 0;
 
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 *
+	 * vhost_poll_queue()就是把worker加到thread上
+	 */
 	if (!poll->dev->use_worker)
 		work->fn(work);
 	else
@@ -186,29 +344,117 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	return 0;
 }
 
+/*
+ * 在以下使用vhost_work_init():
+ *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
+	/*
+	 * 在以下使用VHOST_WORK_QUEUED:
+	 *   - drivers/vhost/vhost.c|343| <<vhost_work_init>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+	 *   - drivers/vhost/vhost.c|525| <<vhost_worker_queue>> if (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {
+	 *   - drivers/vhost/vhost.c|800| <<vhost_run_work_kthread_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+	 *   - drivers/vhost/vhost.c|832| <<vhost_run_work_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+	 */
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
 	work->fn = fn;
 }
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
+/*
+ * 42 static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
+ * 43 {
+ * 44         if (p && p->_qproc) {
+ * 45                 p->_qproc(filp, wait_address, p);
+ * ... ...
+ * 52                 smp_mb();
+ * 53         }
+ * 54 }
+ *
+ *
+ * 在以下使用vhost_poll_init():
+ *   - drivers/vhost/vhost.c|682| <<vhost_dev_init>> vhost_poll_init(&vq->poll,
+ *                 vq->handle_kick, EPOLLIN, dev, vq);
+ *   - drivers/vhost/net.c|1368| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX,
+ *                 handle_tx_net, EPOLLOUT, dev, vqs[VHOST_NET_VQ_TX]);
+ *   - drivers/vhost/net.c|1370| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX,
+ *                 handle_rx_net, EPOLLIN, dev, vqs[VHOST_NET_VQ_RX]);
+ */
 /* Init poll structure */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev,
 		     struct vhost_virtqueue *vq)
 {
+	/*
+	 * // Poll a file (eventfd or socket)
+	 * // Note: there's nothing vhost specific about this structure.
+	 * struct vhost_poll {
+	 *     poll_table              table;
+	 *     wait_queue_head_t       *wqh;
+	 *     wait_queue_entry_t      wait;
+	 *     struct vhost_work       work;
+	 *     __poll_t                mask;
+	 *     struct vhost_dev        *dev;
+	 *     struct vhost_virtqueue  *vq;
+	 * };
+	 */
 	init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	/*
+	 * typedef struct poll_table_struct {
+	 *     poll_queue_proc _qproc;
+	 *     __poll_t _key;
+	 * } poll_table;
+	 *
+	 *
+	 * 似乎加到wait head上
+	 *
+	 * 设置poll_table->_qproc = vhost_poll_func()
+	 */
 	init_poll_funcptr(&poll->table, vhost_poll_func);
 	poll->mask = mask;
 	poll->dev = dev;
 	poll->wqh = NULL;
 	poll->vq = vq;
 
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&poll->work, fn);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_init);
 
+/*
+ * // Poll a file (eventfd or socket)
+ * // Note: there's nothing vhost specific about this structure.
+ * struct vhost_poll {
+ *     poll_table              table;
+ *     wait_queue_head_t       *wqh;
+ *     wait_queue_entry_t      wait;
+ *     struct vhost_work       work;
+ *     __poll_t                mask;
+ *     struct vhost_dev        *dev;
+ *     struct vhost_virtqueue  *vq;
+ * };
+ *
+ * 在以下使用vhost_poll_start():
+ *   - drivers/vhost/net.c|458| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/test.c|292| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|2538| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ */
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
@@ -218,10 +464,51 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (poll->wqh)
 		return 0;
 
+	/*
+	 * 在以下使用vfs_poll():
+	 *   - drivers/hv/mshv_eventfd.c|501| <<mshv_irqfd_assign>> events = vfs_poll(fd_file(f), &irqfd->irqfd_polltbl);
+	 *   - drivers/vfio/virqfd.c|173| <<mshv_irqfd_assign>> events = vfs_poll(fd_file(irqfd), &virqfd->pt);
+	 *   - drivers/vhost/vhost.c|247| <<vhost_poll_start>> mask = vfs_poll(file, &poll->table);
+	 *   - drivers/virt/acrn/irqfd.c|157| <<acrn_irqfd_assign>> events = vfs_poll(fd_file(f), &irqfd->pt);
+	 *   - drivers/xen/privcmd.c|1024| <<privcmd_irqfd_assign>> events = vfs_poll(fd_file(f), &kirqfd->pt);
+	 *   - fs/aio.c|1733| <<aio_poll_complete_work>> mask = vfs_poll(req->file, &pt) & req->events;
+	 *   - fs/aio.c|1927| <<aio_poll>> mask = vfs_poll(req->file, &apt.pt) & req->events;
+	 *   - fs/eventpoll.c|1059| <<ep_item_poll>> res = vfs_poll(file, pt);
+	 *   - fs/select.c|480| <<select_poll_one>> return vfs_poll(fd_file(f), wait);
+	 *   - fs/select.c|870| <<do_pollfd>> mask = vfs_poll(fd_file(f), pwait);
+	 *   - io_uring/net.c|1797| <<io_connect>> if (vfs_poll(req->file, &pt) & EPOLLERR)
+	 *   - io_uring/poll.c|262| <<io_poll_check_events>> req->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;
+	 *   - io_uring/poll.c|581| <<__io_arm_poll_handler>> mask = vfs_poll(req->file, &ipt->pt) & poll->events;
+	 *   - io_uring/rw.c|46| <<io_file_supports_nowait>> return vfs_poll(req->file, &pt) & mask;
+	 *   - mm/memcontrol-v1.c|1177| <<memcg_write_event_control>> vfs_poll(fd_file(efile), &event->pt);
+	 *   - net/9p/trans_fd.c|238| <<p9_fd_poll>> ret = vfs_poll(ts->rd, pt);
+	 *   - net/9p/trans_fd.c|240| <<p9_fd_poll>> ret = (ret & ~EPOLLOUT) | (vfs_poll(ts->wr, pt) & ~EPOLLIN);
+	 *   - virt/kvm/eventfd.c|467| <<kvm_irqfd_assign>> events = vfs_poll(fd_file(f), &irqfd_pt.pt);
+	 *
+	 * typedef struct poll_table_struct {
+	 *     poll_queue_proc _qproc;
+	 *     __poll_t _key;
+	 * } poll_table;
+	 *
+	 * eventfd_poll()核心执行poll_table->_qproc = vhost_poll_func
+	 */
 	mask = vfs_poll(file, &poll->table);
+	/*
+	 * 在以下使用vhost_poll_wakeup():
+	 *   - drivers/vhost/vhost.c|254| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	 *   - drivers/vhost/vhost.c|330| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask))
+	 */
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
 	if (mask & EPOLLERR) {
+		/*
+		 * 在以下使用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|443| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|319| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1450| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 */
 		vhost_poll_stop(poll);
 		return -EINVAL;
 	}
@@ -230,6 +517,14 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_start);
 
+/*
+ * 在以下使用vhost_poll_stop():
+ *   - drivers/vhost/net.c|443| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+ *   - drivers/vhost/vhost.c|319| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|1450| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|2530| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ */
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
 void vhost_poll_stop(struct vhost_poll *poll)
@@ -241,19 +536,65 @@ void vhost_poll_stop(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_stop);
 
+/*
+ * 在以下使用vhost_worker_queue():
+ *   - drivers/vhost/vhost.c|360| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+ *   - drivers/vhost/vhost.c|416| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+ *   - drivers/vhost/vhost.c|824| <<vhost_attach_task_to_cgroups>> vhost_worker_queue(worker, &attach.work);
+ */
 static void vhost_worker_queue(struct vhost_worker *worker,
 			       struct vhost_work *work)
 {
+	/*
+	 * 在以下使用VHOST_WORK_QUEUED:
+	 *   - drivers/vhost/vhost.c|343| <<vhost_work_init>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+	 *   - drivers/vhost/vhost.c|525| <<vhost_worker_queue>> if (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {
+	 *   - drivers/vhost/vhost.c|800| <<vhost_run_work_kthread_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+	 *   - drivers/vhost/vhost.c|832| <<vhost_run_work_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+	 */
 	if (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {
+		/*
+		 * 在以下使用vhost_worker->work_list:
+		 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+		 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+		 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+		 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+		 *                                                         !llist_empty(&old_worker->work_list));
+		 */
 		/* We can only add the work to the list after we're
 		 * sure it was not in the list.
 		 * test_and_set_bit() implies a memory barrier.
 		 */
 		llist_add(&work->node, &worker->work_list);
+		/*
+		 * 1047 static const struct vhost_worker_ops kthread_ops = {
+		 * 1048         .create = vhost_kthread_worker_create,
+		 * 1049         .stop = vhost_kthread_do_stop,
+		 * 1050         .wakeup = vhost_kthread_wakeup,
+		 * 1051 };
+		 * 1052
+		 * 1053 static const struct vhost_worker_ops vhost_task_ops = {
+		 * 1054         .create = vhost_task_worker_create,
+		 * 1055         .stop = vhost_task_do_stop,
+		 * 1056         .wakeup = vhost_task_wakeup,
+		 * 1057 };
+		 */
 		worker->ops->wakeup(worker);
 	}
 }
 
+/*
+ * 在以下使用vhost_vq_work_queue():
+ *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+ *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+ *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+ *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+ *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+ *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+ */
 bool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)
 {
 	struct vhost_worker *worker;
@@ -263,6 +604,12 @@ bool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)
 	worker = rcu_dereference(vq->worker);
 	if (worker) {
 		queued = true;
+		/*
+		 * 在以下使用vhost_worker_queue():
+		 *   - drivers/vhost/vhost.c|360| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+		 *   - drivers/vhost/vhost.c|416| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+		 *   - drivers/vhost/vhost.c|824| <<vhost_attach_task_to_cgroups>> vhost_worker_queue(worker, &attach.work);
+		 */
 		vhost_worker_queue(worker, work);
 	}
 	rcu_read_unlock();
@@ -277,16 +624,54 @@ EXPORT_SYMBOL_GPL(vhost_vq_work_queue);
  *
  * The worker's flush_mutex must be held.
  */
+/*
+ * 在以下使用__vhost_worker_flush():
+ *   - drivers/vhost/vhost.c|329| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+ *   - drivers/vhost/vhost.c|695| <<vhost_attach_task_to_cgroups>> __vhost_worker_flush(worker);
+ *   - drivers/vhost/vhost.c|954| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+ *   - drivers/vhost/vhost.c|1017| <<vhost_free_worker>> __vhost_worker_flush(worker);
+ *
+ * 保证在这个flush之前下发的工作都完成了!
+ */
 static void __vhost_worker_flush(struct vhost_worker *worker)
 {
 	struct vhost_flush_struct flush;
 
+	/*
+	 * 在以下使用vhost_worker->attachment_cnt:
+	 *   - drivers/vhost/vhost.c|310| <<__vhost_worker_flush>> if (!worker->attachment_cnt || worker->killed)
+	 *   - drivers/vhost/vhost.c|512| <<vhost_worker_killed>> worker->attachment_cnt -= attach_cnt;
+	 *   - drivers/vhost/vhost.c|693| <<vhost_attach_task_to_cgroups>> saved_cnt = worker->attachment_cnt;
+	 *   - drivers/vhost/vhost.c|694| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = INT_MAX;
+	 *   - drivers/vhost/vhost.c|696| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = saved_cnt;
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_vq_attach_worker>> worker->attachment_cnt++;
+	 *   - drivers/vhost/vhost.c|938| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|945| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *   - drivers/vhost/vhost.c|955| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|1008| <<vhost_free_worker>> if (worker->attachment_cnt || worker->killed) {
+	 */
 	if (!worker->attachment_cnt || worker->killed)
 		return;
 
 	init_completion(&flush.wait_event);
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&flush.work, vhost_flush_work);
 
+	/*
+	 * 在以下使用vhost_worker_queue():
+	 *   - drivers/vhost/vhost.c|360| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+	 *   - drivers/vhost/vhost.c|416| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+	 *   - drivers/vhost/vhost.c|824| <<vhost_attach_task_to_cgroups>> vhost_worker_queue(worker, &attach.work);
+	 */
 	vhost_worker_queue(worker, &flush.work);
 	/*
 	 * Drop mutex in case our worker is killed and it needs to take the
@@ -297,23 +682,61 @@ static void __vhost_worker_flush(struct vhost_worker *worker)
 	mutex_lock(&worker->mutex);
 }
 
+/*
+ * 在以下使用vhost_worker_flush():
+ *   - drivers/vhost/vhost.c|704| <<vhost_dev_flush>> vhost_worker_flush(worker);
+ */
 static void vhost_worker_flush(struct vhost_worker *worker)
 {
 	mutex_lock(&worker->mutex);
+	/*
+	 * 在以下使用__vhost_worker_flush():
+	 *   - drivers/vhost/vhost.c|329| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|695| <<vhost_attach_task_to_cgroups>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|954| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|1017| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	mutex_unlock(&worker->mutex);
 }
 
+/*
+ * 在以下使用vhost_dev_flush():
+ *   - drivers/vhost/net.c|1436| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/net.c|1627| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/scsi.c|1613| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+ *   - drivers/vhost/scsi.c|1934| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+ *   - drivers/vhost/test.c|163| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/vhost.c|2282| <<vhost_dev_stop>> vhost_dev_flush(dev);
+ *   - drivers/vhost/vhost.c|3385| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+ *   - drivers/vhost/vsock.c|730| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+ */
 void vhost_dev_flush(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	unsigned long i;
 
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	xa_for_each(&dev->worker_xa, i, worker)
 		vhost_worker_flush(worker);
 }
 EXPORT_SYMBOL_GPL(vhost_dev_flush);
 
+/*
+ * 只在以下使用vhost_vq_has_work():
+ *   - drivers/vhost/net.c|577| <<vhost_net_busy_poll>> if (vhost_vq_has_work(vq)) {
+ */
 /* A lockless hint for busy polling code to exit the loop */
 bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 {
@@ -322,6 +745,17 @@ bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 
 	rcu_read_lock();
 	worker = rcu_dereference(vq->worker);
+	/*
+	 * 在以下使用vhost_worker->work_list:
+	 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+	 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+	 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+	 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *                                                         !llist_empty(&old_worker->work_list));
+	 */
 	if (worker && !llist_empty(&worker->work_list))
 		has_work = true;
 	rcu_read_unlock();
@@ -330,8 +764,44 @@ bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_has_work);
 
+/*
+ * 在以下使用vhost_poll_queue():
+ *   - drivers/vhost/net.c|415| <<vhost_zerocopy_complete>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|542| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|545| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|797| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|897| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1200| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1279| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|218| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|768| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|1984| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vsock.c|257| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|320| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *
+ * // Poll a file (eventfd or socket)
+ * // Note: there's nothing vhost specific about this structure.
+ * struct vhost_poll {
+ *     poll_table              table;
+ *     wait_queue_head_t       *wqh;
+ *     wait_queue_entry_t      wait;
+ *     struct vhost_work       work;
+ *     __poll_t                mask;
+ *     struct vhost_dev        *dev;
+ *     struct vhost_virtqueue  *vq;
+ * };
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(poll->vq, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_queue);
@@ -358,12 +828,21 @@ static void vhost_vring_call_reset(struct vhost_vring_call *call_ctx)
 	memset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer));
 }
 
+/*
+ * 在以下使用vhost_vq_is_setup():
+ *   - drivers/vhost/scsi.c|2127| <<vhost_scsi_set_endpoint>> if (!vhost_vq_is_setup(vq))
+ */
 bool vhost_vq_is_setup(struct vhost_virtqueue *vq)
 {
 	return vq->avail && vq->desc && vq->used && vhost_vq_access_ok(vq);
 }
 EXPORT_SYMBOL_GPL(vhost_vq_is_setup);
 
+/*
+ * 在以下使用vhost_vq_reset():
+ *   - drivers/vhost/vhost.c|1057| <<vhost_vq_reset>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|1818| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -397,6 +876,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	__vhost_vq_meta_reset(vq);
 }
 
+/*
+ * 在以下使用vhost_run_work_kthread_list():
+ *   - drivers/vhost/vhost.c|1294| <<vhost_kthread_worker_create>> task = kthread_create(vhost_run_work_kthread_list, worker, "%s", name);
+ */
 static int vhost_run_work_kthread_list(void *data)
 {
 	struct vhost_worker *worker = data;
@@ -404,6 +887,9 @@ static int vhost_run_work_kthread_list(void *data)
 	struct vhost_dev *dev = worker->dev;
 	struct llist_node *node;
 
+	/*
+	 * 和QEMU共享内存
+	 */
 	kthread_use_mm(dev->mm);
 
 	for (;;) {
@@ -414,6 +900,17 @@ static int vhost_run_work_kthread_list(void *data)
 			__set_current_state(TASK_RUNNING);
 			break;
 		}
+		/*
+		 * 在以下使用vhost_worker->work_list:
+		 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+		 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+		 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+		 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+		 *                                                         !llist_empty(&old_worker->work_list));
+		 */
 		node = llist_del_all(&worker->work_list);
 		if (!node)
 			schedule();
@@ -422,6 +919,13 @@ static int vhost_run_work_kthread_list(void *data)
 		/* make sure flag is seen after deletion */
 		smp_wmb();
 		llist_for_each_entry_safe(work, work_next, node, node) {
+			/*
+			 * 在以下使用VHOST_WORK_QUEUED:
+			 *   - drivers/vhost/vhost.c|343| <<vhost_work_init>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+			 *   - drivers/vhost/vhost.c|525| <<vhost_worker_queue>> if (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {
+			 *   - drivers/vhost/vhost.c|800| <<vhost_run_work_kthread_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+			 *   - drivers/vhost/vhost.c|832| <<vhost_run_work_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+			 */
 			clear_bit(VHOST_WORK_QUEUED, &work->flags);
 			__set_current_state(TASK_RUNNING);
 			kcov_remote_start_common(worker->kcov_handle);
@@ -435,12 +939,28 @@ static int vhost_run_work_kthread_list(void *data)
 	return 0;
 }
 
+/*
+ * 在以下使用vhost_run_work_list():
+ *   - drivers/vhost/vhost.c|639| <<vhost_worker_killed>> vhost_run_work_list(worker);
+ *   - drivers/vhost/vhost.c|957| <<vhost_task_worker_create>> vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
+ */
 static bool vhost_run_work_list(void *data)
 {
 	struct vhost_worker *worker = data;
 	struct vhost_work *work, *work_next;
 	struct llist_node *node;
 
+	/*
+	 * 在以下使用vhost_worker->work_list:
+	 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+	 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+	 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+	 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *                                                         !llist_empty(&old_worker->work_list));
+	 */
 	node = llist_del_all(&worker->work_list);
 	if (node) {
 		__set_current_state(TASK_RUNNING);
@@ -449,6 +969,13 @@ static bool vhost_run_work_list(void *data)
 		/* make sure flag is seen after deletion */
 		smp_wmb();
 		llist_for_each_entry_safe(work, work_next, node, node) {
+			/*
+			 * 在以下使用VHOST_WORK_QUEUED:
+			 *   - drivers/vhost/vhost.c|343| <<vhost_work_init>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+			 *   - drivers/vhost/vhost.c|525| <<vhost_worker_queue>> if (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {
+			 *   - drivers/vhost/vhost.c|800| <<vhost_run_work_kthread_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+			 *   - drivers/vhost/vhost.c|832| <<vhost_run_work_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+			 */
 			clear_bit(VHOST_WORK_QUEUED, &work->flags);
 			kcov_remote_start_common(worker->kcov_handle);
 			work->fn(work);
@@ -460,6 +987,10 @@ static bool vhost_run_work_list(void *data)
 	return !!node;
 }
 
+/*
+ * 在以下使用vhost_worker_killed():
+ *   - drivers/vhost/vhost.c|1266| <<vhost_task_worker_create>> vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
+ */
 static void vhost_worker_killed(void *data)
 {
 	struct vhost_worker *worker = data;
@@ -483,9 +1014,27 @@ static void vhost_worker_killed(void *data)
 		mutex_unlock(&vq->mutex);
 	}
 
+	/*
+	 * 在以下使用vhost_worker->attachment_cnt:
+	 *   - drivers/vhost/vhost.c|310| <<__vhost_worker_flush>> if (!worker->attachment_cnt || worker->killed)
+	 *   - drivers/vhost/vhost.c|512| <<vhost_worker_killed>> worker->attachment_cnt -= attach_cnt;
+	 *   - drivers/vhost/vhost.c|693| <<vhost_attach_task_to_cgroups>> saved_cnt = worker->attachment_cnt;
+	 *   - drivers/vhost/vhost.c|694| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = INT_MAX;
+	 *   - drivers/vhost/vhost.c|696| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = saved_cnt;
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_vq_attach_worker>> worker->attachment_cnt++;
+	 *   - drivers/vhost/vhost.c|938| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|945| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *   - drivers/vhost/vhost.c|955| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|1008| <<vhost_free_worker>> if (worker->attachment_cnt || worker->killed) {
+	 */
 	worker->attachment_cnt -= attach_cnt;
 	if (attach_cnt)
 		synchronize_rcu();
+	/*
+	 * 在以下使用vhost_run_work_list():
+	 *   - drivers/vhost/vhost.c|639| <<vhost_worker_killed>> vhost_run_work_list(worker);
+	 *   - drivers/vhost/vhost.c|957| <<vhost_task_worker_create>> vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
+	 */
 	/*
 	 * Finish vhost_worker_flush calls and any other works that snuck in
 	 * before the synchronize_rcu.
@@ -581,6 +1130,20 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * 在以下使用vhost_dev_init():
+ *   - drivers/vhost/net.c|1371| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+ *       VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT,
+ *       VHOST_NET_WEIGHT, true, NULL);
+ *   - drivers/vhost/scsi.c|2413| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+ *       nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+ *       VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+ *   - drivers/vhost/vdpa.c|1431| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+ *       nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+ *   - drivers/vhost/vsock.c|700| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+ *       ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -601,8 +1164,40 @@ void vhost_dev_init(struct vhost_dev *dev,
 	dev->iov_limit = iov_limit;
 	dev->weight = weight;
 	dev->byte_weight = byte_weight;
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	dev->use_worker = use_worker;
 	dev->msg_handler = msg_handler;
+	/*
+	 * 在以下使用vhost_dev->fork_owner:
+	 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+	 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+	 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+	 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+	 *
+	 * 注释:
+	 * If fork_owner is true we use vhost_tasks to create
+	 * the worker so all settings/limits like cgroups, NPROC,
+	 * scheduler, etc are inherited from the owner. If false,
+	 * we use kthreads and only attach to the same cgroups
+	 * as the owner for compat with older kernels.
+	 * here we use true as default value.
+	 * The default value is set by fork_from_owner_default
+	 */
 	dev->fork_owner = fork_from_owner_default;
 	init_waitqueue_head(&dev->wait);
 	INIT_LIST_HEAD(&dev->read_list);
@@ -618,7 +1213,21 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->nheads = NULL;
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
+		/*
+		 * 在以下使用vhost_vq_reset():
+		 *   - drivers/vhost/vhost.c|1057| <<vhost_vq_reset>> vhost_vq_reset(dev, vq);
+		 *   - drivers/vhost/vhost.c|1818| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+		 */
 		vhost_vq_reset(dev, vq);
+		/*
+		 * 在以下使用vhost_poll_init():
+		 *   - drivers/vhost/vhost.c|682| <<vhost_dev_init>> vhost_poll_init(&vq->poll,
+		 *                 vq->handle_kick, EPOLLIN, dev, vq);
+		 *   - drivers/vhost/net.c|1368| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX,
+		 *                 handle_tx_net, EPOLLOUT, dev, vqs[VHOST_NET_VQ_TX]);
+		 *   - drivers/vhost/net.c|1370| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX,
+		 *                 handle_rx_net, EPOLLIN, dev, vqs[VHOST_NET_VQ_RX]);
+		 */
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
 					EPOLLIN, dev, vq);
@@ -626,6 +1235,20 @@ void vhost_dev_init(struct vhost_dev *dev,
 }
 EXPORT_SYMBOL_GPL(vhost_dev_init);
 
+/*
+ * 在以下使用vhost_dev_check_owner():
+ *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/net.c|1657| <<vhost_net_reset_owner>> err = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|190| <<vhost_test_run>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|238| <<vhost_test_reset_owner>> err = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/test.c|282| <<vhost_test_set_backend>> r = vhost_dev_check_owner(&n->dev);
+ *   - drivers/vhost/vdpa.c|1253| <<vhost_vdpa_process_iotlb_msg>> r = vhost_dev_check_owner(dev);
+ *   - drivers/vhost/vhost.c|1619| <<vhost_worker_ioctl>> ret = vhost_dev_check_owner(dev);
+ *   - drivers/vhost/vhost.c|2967| <<vhost_dev_ioctl>> r = vhost_dev_check_owner(d);
+ *   - drivers/vhost/vhost.h|236| <<vhost_dev_ioctl>> long vhost_dev_check_owner(struct vhost_dev *);
+ *   - drivers/vhost/vsock.c|583| <<vhost_vsock_start>> ret = vhost_dev_check_owner(&vsock->dev);
+ *   - drivers/vhost/vsock.c|648| <<vhost_vsock_stop>> ret = vhost_dev_check_owner(&vsock->dev);
+ */
 /* Caller should have device mutex */
 long vhost_dev_check_owner(struct vhost_dev *dev)
 {
@@ -640,6 +1263,10 @@ struct vhost_attach_cgroups_struct {
 	int ret;
 };
 
+/*
+ * 在以下使用vhost_attach_cgroups_work():
+ *   - drivers/vhost/vhost.c|1113| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ */
 static void vhost_attach_cgroups_work(struct vhost_work *work)
 {
 	struct vhost_attach_cgroups_struct *s;
@@ -648,24 +1275,71 @@ static void vhost_attach_cgroups_work(struct vhost_work *work)
 	s->ret = cgroup_attach_task_all(s->owner, current);
 }
 
+/*
+ * 在以下使用vhost_attach_task_to_cgroups():
+ *   - drivers/vhost/vhost.c|1358| <<vhost_kthread_worker_create>> ret = vhost_attach_task_to_cgroups(worker);
+ */
 static int vhost_attach_task_to_cgroups(struct vhost_worker *worker)
 {
+	/*
+	 * struct vhost_attach_cgroups_struct {
+	 *     struct vhost_work work;
+	 *     struct task_struct *owner;
+	 *     int ret;
+	 * };
+	 */
 	struct vhost_attach_cgroups_struct attach;
 	int saved_cnt;
 
 	attach.owner = current;
 
+	/*
+	 * 在以下使用vhost_work_init():
+	 *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 *   - drivers/vhost/scsi.c|2337| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 *   - drivers/vhost/scsi.c|2352| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+	 *   - drivers/vhost/vhost.c|234| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+	 *   - drivers/vhost/vhost.c|334| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+	 *   - drivers/vhost/vhost.c|717| <<vhost_attach_task_to_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	 *   - drivers/vhost/vsock.c|688| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+	 */
 	vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+	/*
+	 * 在以下使用vhost_worker_queue():
+	 *   - drivers/vhost/vhost.c|360| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+	 *   - drivers/vhost/vhost.c|416| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+	 *   - drivers/vhost/vhost.c|824| <<vhost_attach_task_to_cgroups>> vhost_worker_queue(worker, &attach.work);
+	 */
 	vhost_worker_queue(worker, &attach.work);
 
 	mutex_lock(&worker->mutex);
 
+	/*
+	 * 在以下使用vhost_worker->attachment_cnt:
+	 *   - drivers/vhost/vhost.c|310| <<__vhost_worker_flush>> if (!worker->attachment_cnt || worker->killed)
+	 *   - drivers/vhost/vhost.c|512| <<vhost_worker_killed>> worker->attachment_cnt -= attach_cnt;
+	 *   - drivers/vhost/vhost.c|693| <<vhost_attach_task_to_cgroups>> saved_cnt = worker->attachment_cnt;
+	 *   - drivers/vhost/vhost.c|694| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = INT_MAX;
+	 *   - drivers/vhost/vhost.c|696| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = saved_cnt;
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_vq_attach_worker>> worker->attachment_cnt++;
+	 *   - drivers/vhost/vhost.c|938| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|945| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *   - drivers/vhost/vhost.c|955| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|1008| <<vhost_free_worker>> if (worker->attachment_cnt || worker->killed) {
+	 */
 	/*
 	 * Bypass attachment_cnt check in __vhost_worker_flush:
 	 * Temporarily change it to INT_MAX to bypass the check
 	 */
 	saved_cnt = worker->attachment_cnt;
 	worker->attachment_cnt = INT_MAX;
+	/*
+	 * 在以下使用__vhost_worker_flush():
+	 *   - drivers/vhost/vhost.c|329| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|695| <<vhost_attach_task_to_cgroups>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|954| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|1017| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	worker->attachment_cnt = saved_cnt;
 
@@ -674,6 +1348,13 @@ static int vhost_attach_task_to_cgroups(struct vhost_worker *worker)
 	return attach.ret;
 }
 
+/*
+ * 在以下使用vhost_dev_has_owner():
+ *   - drivers/vhost/net.c|1735| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+ *   - drivers/vhost/vhost.c|1670| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+ *   - drivers/vhost/vhost.c|1747| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+ *   - drivers/vhost/vhost.c|2989| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> if (vhost_dev_has_owner(d)) {
+ */
 /* Caller should have device mutex */
 bool vhost_dev_has_owner(struct vhost_dev *dev)
 {
@@ -681,8 +1362,26 @@ bool vhost_dev_has_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_has_owner);
 
+/*
+ * 在以下使用vhost_attach_mm():
+ *   - drivers/vhost/vhost.c|1752| <<vhost_dev_set_owner>> vhost_attach_mm(dev);
+ */
 static void vhost_attach_mm(struct vhost_dev *dev)
 {
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	/* No owner, become one */
 	if (dev->use_worker) {
 		dev->mm = get_task_mm(current);
@@ -698,11 +1397,30 @@ static void vhost_attach_mm(struct vhost_dev *dev)
 	}
 }
 
+/*
+ * 在以下使用vhost_detach_mm():
+ *   - drivers/vhost/vhost.c|1785| <<vhost_dev_set_owner>> vhost_detach_mm(dev);
+ *   - drivers/vhost/vhost.c|1891| <<vhost_dev_cleanup>> vhost_detach_mm(dev);
+ */
 static void vhost_detach_mm(struct vhost_dev *dev)
 {
 	if (!dev->mm)
 		return;
 
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	if (dev->use_worker)
 		mmput(dev->mm);
 	else
@@ -711,23 +1429,69 @@ static void vhost_detach_mm(struct vhost_dev *dev)
 	dev->mm = NULL;
 }
 
+/*
+ * 在以下使用vhost_worker_destroy():
+ *   - drivers/vhost/vhost.c|1269| <<vhost_workers_free>> vhost_worker_destroy(dev, worker);
+ *   - drivers/vhost/vhost.c|1631| <<vhost_free_worker>> vhost_worker_destroy(dev, worker);
+ */
 static void vhost_worker_destroy(struct vhost_dev *dev,
 				 struct vhost_worker *worker)
 {
 	if (!worker)
 		return;
 
+	/*
+	 * 在以下使用vhost_worker->work_list:
+	 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+	 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+	 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+	 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *                                                         !llist_empty(&old_worker->work_list));
+	 */
 	WARN_ON(!llist_empty(&worker->work_list));
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	xa_erase(&dev->worker_xa, worker->id);
 	worker->ops->stop(worker);
 	kfree(worker);
 }
 
+/*
+ * 在以下使用vhost_workers_free():
+ *   - drivers/vhost/vhost.c|1890| <<vhost_dev_cleanup>> vhost_workers_free(dev);
+ */
 static void vhost_workers_free(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	unsigned long i;
 
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	if (!dev->use_worker)
 		return;
 
@@ -737,31 +1501,168 @@ static void vhost_workers_free(struct vhost_dev *dev)
 	 * Free the default worker we created and cleanup workers userspace
 	 * created but couldn't clean up (it forgot or crashed).
 	 */
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	xa_for_each(&dev->worker_xa, i, worker)
 		vhost_worker_destroy(dev, worker);
 	xa_destroy(&dev->worker_xa);
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ *
+ * 1485 static const struct vhost_worker_ops kthread_ops = {
+ * 1486         .create = vhost_kthread_worker_create,
+ * 1487         .stop = vhost_kthread_do_stop,
+ * 1488         .wakeup = vhost_kthread_wakeup,
+ * 1489 };
+ * 1490
+ * 1491 static const struct vhost_worker_ops vhost_task_ops = {
+ * 1492         .create = vhost_task_worker_create,
+ * 1493         .stop = vhost_task_do_stop,
+ * 1494         .wakeup = vhost_task_wakeup,
+ * 1495 };
+ */
 static void vhost_task_wakeup(struct vhost_worker *worker)
 {
+	/*
+	 * 85 void vhost_task_wake(struct vhost_task *vtsk)
+	 * 86 {
+	 * 87         wake_up_process(vtsk->task);
+	 * 88 }
+	 * 89 EXPORT_SYMBOL_GPL(vhost_task_wake);
+	 *
+	 * 在以下使用vhost_task_wake():
+	 *   - arch/x86/kvm/mmu/mmu.c|7446| <<kvm_wake_nx_recovery_thread>> vhost_task_wake(nx_thread);
+	 *   - drivers/vhost/vhost.c|1538| <<vhost_task_wakeup>> return vhost_task_wake(worker->vtsk);
+	 *   - kernel/vhost_task.c|108| <<vhost_task_stop>> vhost_task_wake(vtsk);
+	 */
 	return vhost_task_wake(worker->vtsk);
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ *
+ * 1485 static const struct vhost_worker_ops kthread_ops = {
+ * 1486         .create = vhost_kthread_worker_create,
+ * 1487         .stop = vhost_kthread_do_stop,
+ * 1488         .wakeup = vhost_kthread_wakeup,
+ * 1489 };
+ * 1490
+ * 1491 static const struct vhost_worker_ops vhost_task_ops = {
+ * 1492         .create = vhost_task_worker_create,
+ * 1493         .stop = vhost_task_do_stop,
+ * 1494         .wakeup = vhost_task_wakeup,
+ * 1495 };
+ */
 static void vhost_kthread_wakeup(struct vhost_worker *worker)
 {
 	wake_up_process(worker->kthread_task);
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ *
+ * 1485 static const struct vhost_worker_ops kthread_ops = {
+ * 1486         .create = vhost_kthread_worker_create,
+ * 1487         .stop = vhost_kthread_do_stop,
+ * 1488         .wakeup = vhost_kthread_wakeup,
+ * 1489 };
+ * 1490
+ * 1491 static const struct vhost_worker_ops vhost_task_ops = {
+ * 1492         .create = vhost_task_worker_create,
+ * 1493         .stop = vhost_task_do_stop,
+ * 1494         .wakeup = vhost_task_wakeup,
+ * 1495 };
+ *
+ * 在以下使用vhost_task_do_stop:
+ *   - drivers/vhost/vhost.c|1463| <<global>> struct vhost_worker_ops vhost_task_ops.stop = vhost_task_do_stop,
+ *   - drivers/vhost/vhost.c|1399| <<vhost_task_worker_create>> vhost_task_do_stop(worker);
+ */
 static void vhost_task_do_stop(struct vhost_worker *worker)
 {
+	/*
+	 * 在以下使用vhost_task_stop():
+	 *   - arch/x86/kvm/mmu/mmu.c|7821| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - drivers/vhost/vhost.c|1592| <<vhost_task_do_stop>> return vhost_task_stop(worker->vtsk);
+	 */
 	return vhost_task_stop(worker->vtsk);
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ *
+ * 1485 static const struct vhost_worker_ops kthread_ops = {
+ * 1486         .create = vhost_kthread_worker_create,
+ * 1487         .stop = vhost_kthread_do_stop,
+ * 1488         .wakeup = vhost_kthread_wakeup,
+ * 1489 };
+ * 1490
+ * 1491 static const struct vhost_worker_ops vhost_task_ops = {
+ * 1492         .create = vhost_task_worker_create,
+ * 1493         .stop = vhost_task_do_stop,
+ * 1494         .wakeup = vhost_task_wakeup,
+ * 1495 };
+ *
+ * 在以下使用vhost_kthread_do_stop():
+ *   - drivers/vhost/vhost.c|1457| <<global>> struct vhost_worker_ops kthread_ops.stop = vhost_kthread_do_stop,
+ *   - drivers/vhost/vhost.c|1443| <<vhost_kthread_worker_create>> vhost_kthread_do_stop(worker);
+ */
 static void vhost_kthread_do_stop(struct vhost_worker *worker)
 {
 	kthread_stop(worker->kthread_task);
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ *
+ * 1485 static const struct vhost_worker_ops kthread_ops = {
+ * 1486         .create = vhost_kthread_worker_create,
+ * 1487         .stop = vhost_kthread_do_stop,
+ * 1488         .wakeup = vhost_kthread_wakeup,
+ * 1489 };
+ * 1490
+ * 1491 static const struct vhost_worker_ops vhost_task_ops = {
+ * 1492         .create = vhost_task_worker_create,
+ * 1493         .stop = vhost_task_do_stop,
+ * 1494         .wakeup = vhost_task_wakeup,
+ * 1495 };
+ */
 static int vhost_task_worker_create(struct vhost_worker *worker,
 				    struct vhost_dev *dev, const char *name)
 {
@@ -769,15 +1670,50 @@ static int vhost_task_worker_create(struct vhost_worker *worker,
 	u32 id;
 	int ret;
 
+	/*
+	 * 在以下使用vhost_run_work_list():
+	 *   - drivers/vhost/vhost.c|639| <<vhost_worker_killed>> vhost_run_work_list(worker);
+	 *   - drivers/vhost/vhost.c|957| <<vhost_task_worker_create>> vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
+	 *
+	 *
+	 * 在以下使用vhost_task_create():
+	 *   - arch/x86/kvm/mmu/mmu.c|7783| <<kvm_mmu_start_lpage_recovery>> nx_thread = vhost_task_create(
+	 *       kvm_nx_huge_page_recovery_worker, kvm_nx_huge_page_recovery_worker_kill,
+	 *       kvm, "kvm-nx-lpage-recovery");
+	 *   - drivers/vhost/vhost.c|957| <<vhost_task_worker_create>> vtsk = vhost_task_create(
+	 *       vhost_run_work_list, vhost_worker_killed, worker, name);
+	 */
 	vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
 				 worker, name);
 	if (IS_ERR(vtsk))
 		return PTR_ERR(vtsk);
 
 	worker->vtsk = vtsk;
+	/*
+	 * 在以下使用vhost_task_start():
+	 *   - arch/x86/kvm/mmu/mmu.c|7790| <<kvm_mmu_start_lpage_recovery>> vhost_task_start(nx_thread);
+	 *   - drivers/vhost/vhost.c|963| <<vhost_task_worker_create>> vhost_task_start(vtsk);
+	 */
 	vhost_task_start(vtsk);
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
 	if (ret < 0) {
+		/*
+		 * 在以下使用vhost_task_do_stop:
+		 *   - drivers/vhost/vhost.c|1463| <<global>> struct vhost_worker_ops vhost_task_ops.stop = vhost_task_do_stop,
+		 *   - drivers/vhost/vhost.c|1399| <<vhost_task_worker_create>> vhost_task_do_stop(worker);
+		 */
 		vhost_task_do_stop(worker);
 		return ret;
 	}
@@ -785,6 +1721,26 @@ static int vhost_task_worker_create(struct vhost_worker *worker,
 	return 0;
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ *
+ * 1485 static const struct vhost_worker_ops kthread_ops = {
+ * 1486         .create = vhost_kthread_worker_create,
+ * 1487         .stop = vhost_kthread_do_stop,
+ * 1488         .wakeup = vhost_kthread_wakeup,
+ * 1489 };
+ * 1490
+ * 1491 static const struct vhost_worker_ops vhost_task_ops = {
+ * 1492         .create = vhost_task_worker_create,
+ * 1493         .stop = vhost_task_do_stop,
+ * 1494         .wakeup = vhost_task_wakeup,
+ * 1495 };
+ */
 static int vhost_kthread_worker_create(struct vhost_worker *worker,
 				       struct vhost_dev *dev, const char *name)
 {
@@ -792,16 +1748,34 @@ static int vhost_kthread_worker_create(struct vhost_worker *worker,
 	u32 id;
 	int ret;
 
+	/*
+	 * 只在这里使用vhost_run_work_kthread_list()
+	 */
 	task = kthread_create(vhost_run_work_kthread_list, worker, "%s", name);
 	if (IS_ERR(task))
 		return PTR_ERR(task);
 
 	worker->kthread_task = task;
 	wake_up_process(task);
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
 	if (ret < 0)
 		goto stop_worker;
 
+	/*
+	 * 只在这里调用
+	 */
 	ret = vhost_attach_task_to_cgroups(worker);
 	if (ret)
 		goto stop_worker;
@@ -810,10 +1784,23 @@ static int vhost_kthread_worker_create(struct vhost_worker *worker,
 	return 0;
 
 stop_worker:
+	/*
+	 * 在以下使用vhost_kthread_do_stop():
+	 *   - drivers/vhost/vhost.c|1457| <<global>> struct vhost_worker_ops kthread_ops.stop = vhost_kthread_do_stop,
+	 *   - drivers/vhost/vhost.c|1443| <<vhost_kthread_worker_create>> vhost_kthread_do_stop(worker);
+	 */
 	vhost_kthread_do_stop(worker);
 	return ret;
 }
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ */
 static const struct vhost_worker_ops kthread_ops = {
 	.create = vhost_kthread_worker_create,
 	.stop = vhost_kthread_do_stop,
@@ -826,11 +1813,59 @@ static const struct vhost_worker_ops vhost_task_ops = {
 	.wakeup = vhost_task_wakeup,
 };
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker()
+ */
+/*
+ * 在以下使用vhost_worker_create():
+ *   - drivers/vhost/vhost.c|953| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+ *   - drivers/vhost/vhost.c|1119| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+ */
 static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	char name[TASK_COMM_LEN];
 	int ret;
+	/*
+	 * 在以下使用vhost_dev->fork_owner:
+	 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+	 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+	 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+	 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+	 *
+	 * 注释:
+	 * If fork_owner is true we use vhost_tasks to create
+	 * the worker so all settings/limits like cgroups, NPROC,
+	 * scheduler, etc are inherited from the owner. If false,
+	 * we use kthreads and only attach to the same cgroups
+	 * as the owner for compat with older kernels.
+	 * here we use true as default value.
+	 * The default value is set by fork_from_owner_default
+	 */
 	const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
 							       &kthread_ops;
 
@@ -843,6 +1878,17 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	snprintf(name, sizeof(name), "vhost-%d", current->pid);
 
 	mutex_init(&worker->mutex);
+	/*
+	 * 在以下使用vhost_worker->work_list:
+	 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+	 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+	 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+	 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *                                                         !llist_empty(&old_worker->work_list));
+	 */
 	init_llist_head(&worker->work_list);
 	worker->kcov_handle = kcov_common_handle();
 	ret = ops->create(worker, dev, name);
@@ -856,10 +1902,41 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	return NULL;
 }
 
+/*
+ * 在以下使用__vhost_vq_attach_worker():
+ *   - drivers/vhost/vhost.c|974| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+ *   - drivers/vhost/vhost.c|1161| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+ */
 /* Caller must have device mutex */
 static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				     struct vhost_worker *worker)
 {
+	/*
+	 * struct vhost_task {
+	 *     bool (*fn)(void *data);
+	 *     void (*handle_sigkill)(void *data);
+	 *     void *data;
+	 *     struct completion exited;
+	 *     unsigned long flags;
+	 *     struct task_struct *task;
+	 *     // serialize SIGKILL and vhost_task_stop calls
+	 *     struct mutex exit_mutex;
+	 * };
+	 *
+	 * struct vhost_worker {
+	 *     struct task_struct *kthread_task;
+	 *     struct vhost_task       *vtsk;
+	 *     struct vhost_dev        *dev;
+	 *     // Used to serialize device wide flushing with worker swapping.
+	 *     struct mutex            mutex;
+	 *     struct llist_head       work_list;
+	 *     u64                     kcov_handle;
+	 *     u32                     id;
+	 *     int                     attachment_cnt;
+	 *     bool                    killed;
+	 *     const struct vhost_worker_ops *ops;
+	 * };
+	 */
 	struct vhost_worker *old_worker;
 
 	mutex_lock(&worker->mutex);
@@ -873,6 +1950,19 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 	old_worker = rcu_dereference_check(vq->worker,
 					   lockdep_is_held(&vq->mutex));
 	rcu_assign_pointer(vq->worker, worker);
+	/*
+	 * 在以下使用vhost_worker->attachment_cnt:
+	 *   - drivers/vhost/vhost.c|310| <<__vhost_worker_flush>> if (!worker->attachment_cnt || worker->killed)
+	 *   - drivers/vhost/vhost.c|512| <<vhost_worker_killed>> worker->attachment_cnt -= attach_cnt;
+	 *   - drivers/vhost/vhost.c|693| <<vhost_attach_task_to_cgroups>> saved_cnt = worker->attachment_cnt;
+	 *   - drivers/vhost/vhost.c|694| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = INT_MAX;
+	 *   - drivers/vhost/vhost.c|696| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = saved_cnt;
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_vq_attach_worker>> worker->attachment_cnt++;
+	 *   - drivers/vhost/vhost.c|938| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|945| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *   - drivers/vhost/vhost.c|955| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|1008| <<vhost_free_worker>> if (worker->attachment_cnt || worker->killed) {
+	 */
 	worker->attachment_cnt++;
 
 	if (!old_worker) {
@@ -906,6 +1996,17 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 
 		old_worker->attachment_cnt--;
 		mutex_unlock(&old_worker->mutex);
+		/*
+		 * 在以下使用vhost_worker->work_list:
+		 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+		 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+		 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+		 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+		 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+		 *                                                         !llist_empty(&old_worker->work_list));
+		 */
 		/*
 		 * vsock can queue anytime after VHOST_VSOCK_SET_GUEST_CID.
 		 * Warn if it adds support for multiple workers but forgets to
@@ -919,12 +2020,23 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 
 	/* Make sure new vq queue/flush/poll calls see the new worker */
 	synchronize_rcu();
+	/*
+	 * 在以下使用__vhost_worker_flush():
+	 *   - drivers/vhost/vhost.c|329| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|695| <<vhost_attach_task_to_cgroups>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|954| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|1017| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	/* Make sure whatever was queued gets run */
 	__vhost_worker_flush(old_worker);
 	old_worker->attachment_cnt--;
 	mutex_unlock(&old_worker->mutex);
 }
 
+/*
+ * 处理VHOST_ATTACH_VRING_WORKER:
+ *   - drivers/vhost/vhost.c|2164| <<vhost_worker_ioctl(VHOST_ATTACH_VRING_WORKER)>> ret = vhost_vq_attach_worker(vq, &ring_worker);
+ */
  /* Caller must have device mutex */
 static int vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				  struct vhost_vring_worker *info)
@@ -933,23 +2045,63 @@ static int vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 	struct vhost_dev *dev = vq->dev;
 	struct vhost_worker *worker;
 
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	if (!dev->use_worker)
 		return -EINVAL;
 
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
 	if (!worker || worker->id != info->worker_id)
 		return -ENODEV;
 
+	/*
+	 * 在以下使用__vhost_vq_attach_worker():
+	 *   - drivers/vhost/vhost.c|974| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+	 *   - drivers/vhost/vhost.c|1161| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+	 */
 	__vhost_vq_attach_worker(vq, worker);
 	return 0;
 }
 
+/*
+ * 在以下使用vhost_new_worker():
+ *   - drivers/vhost/vhost.c|1042| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> ret = vhost_new_worker(dev, &state);
+ */
 /* Caller must have device mutex */
 static int vhost_new_worker(struct vhost_dev *dev,
 			    struct vhost_worker_state *info)
 {
 	struct vhost_worker *worker;
 
+	/*
+	 * 在以下使用vhost_worker_create():
+	 *   - drivers/vhost/vhost.c|953| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+	 *   - drivers/vhost/vhost.c|1119| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+	 */
 	worker = vhost_worker_create(dev);
 	if (!worker)
 		return -ENOMEM;
@@ -958,18 +2110,57 @@ static int vhost_new_worker(struct vhost_dev *dev,
 	return 0;
 }
 
+/*
+ * 处理VHOST_FREE_WORKER:
+ *   - drivers/vhost/vhost.c|2144| <<vhost_worker_ioctl(VHOST_FREE_WORKER)>> return vhost_free_worker(dev, &state);
+ */
 /* Caller must have device mutex */
 static int vhost_free_worker(struct vhost_dev *dev,
 			     struct vhost_worker_state *info)
 {
+	/*
+	 * struct vhost_worker_state {
+	 *     //
+	 *     // For VHOST_NEW_WORKER the kernel will return the new vhost_worker id.
+	 *     // For VHOST_FREE_WORKER this must be set to the id of the vhost_worker
+	 *     // to free.
+	 *     //
+	 *     unsigned int worker_id;
+	 * };
+	 */
 	unsigned long index = info->worker_id;
 	struct vhost_worker *worker;
 
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
 	if (!worker || worker->id != info->worker_id)
 		return -ENODEV;
 
 	mutex_lock(&worker->mutex);
+	/*
+	 * 在以下使用vhost_worker->attachment_cnt:
+	 *   - drivers/vhost/vhost.c|310| <<__vhost_worker_flush>> if (!worker->attachment_cnt || worker->killed)
+	 *   - drivers/vhost/vhost.c|512| <<vhost_worker_killed>> worker->attachment_cnt -= attach_cnt;
+	 *   - drivers/vhost/vhost.c|693| <<vhost_attach_task_to_cgroups>> saved_cnt = worker->attachment_cnt;
+	 *   - drivers/vhost/vhost.c|694| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = INT_MAX;
+	 *   - drivers/vhost/vhost.c|696| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = saved_cnt;
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_vq_attach_worker>> worker->attachment_cnt++;
+	 *   - drivers/vhost/vhost.c|938| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|945| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *   - drivers/vhost/vhost.c|955| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|1008| <<vhost_free_worker>> if (worker->attachment_cnt || worker->killed) {
+	 */
 	if (worker->attachment_cnt || worker->killed) {
 		mutex_unlock(&worker->mutex);
 		return -EBUSY;
@@ -979,13 +2170,30 @@ static int vhost_free_worker(struct vhost_dev *dev,
 	 * to zero. Make sure flushes are flushed from the queue before
 	 * freeing.
 	 */
+	/*
+	 * 在以下使用__vhost_worker_flush():
+	 *   - drivers/vhost/vhost.c|329| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|695| <<vhost_attach_task_to_cgroups>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|954| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|1017| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	mutex_unlock(&worker->mutex);
 
+	/*
+	 * 在以下使用vhost_worker_destroy():
+	 *   - drivers/vhost/vhost.c|1269| <<vhost_workers_free>> vhost_worker_destroy(dev, worker);
+	 *   - drivers/vhost/vhost.c|1631| <<vhost_free_worker>> vhost_worker_destroy(dev, worker);
+	 */
 	vhost_worker_destroy(dev, worker);
 	return 0;
 }
 
+/*
+ * 在以下使用vhost_get_vq_from_user():
+ *   - drivers/vhost/vhost.c|2153| <<vhost_worker_ioctl>> ret = vhost_get_vq_from_user(dev, argp, &vq, &idx);
+ *   - drivers/vhost/vhost.c|3292| <<vhost_vring_ioctl>> r = vhost_get_vq_from_user(d, argp, &vq, &idx);
+ */
 static int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,
 				  struct vhost_virtqueue **vq, u32 *id)
 {
@@ -1007,6 +2215,39 @@ static int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,
 	return 0;
 }
 
+/*
+ * Legacy的方式.
+ *
+ * VHOST_SET_OWNER
+ * -> vhost_dev_set_owner()
+ *    -> vhost_worker_create()
+ *    -> for (i = 0; i < dev->nvqs; i++)
+ *         __vhost_vq_attach_worker(dev->vqs[i], worker)
+ *
+ * 新的multiqueue/worker的方式.
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_NEW_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_NEW_WORKER
+ *          -> vhost_new_worker()
+ *             -> vhost_worker_create()
+ *
+ * vhost_scsi_ioctl()
+ * -> VHOST_ATTACH_VRING_WORKER
+ *    -> vhost_worker_ioctl()
+ *       -> VHOST_ATTACH_VRING_WORKER
+ *          -> vhost_vq_attach_worker()
+ *             -> __vhost_vq_attach_worker(
+ */
+/*
+ * 处理这面这些:
+ * case VHOST_NEW_WORKER:
+ * case VHOST_FREE_WORKER:
+ * case VHOST_ATTACH_VRING_WORKER:
+ * case VHOST_GET_VRING_WORKER:
+ *   - drivers/vhost/scsi.c|2525| <<vhost_scsi_ioctl>> r = vhost_worker_ioctl(&vs->dev, ioctl, argp);
+ */
 /* Caller must have device mutex */
 long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 			void __user *argp)
@@ -1018,9 +2259,30 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 	long ret;
 	u32 idx;
 
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	if (!dev->use_worker)
 		return -EINVAL;
 
+	/*
+	 * 在以下使用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1735| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|1670| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|1747| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 *   - drivers/vhost/vhost.c|2989| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> if (vhost_dev_has_owner(d)) {
+	 */
 	if (!vhost_dev_has_owner(dev))
 		return -EINVAL;
 
@@ -1036,6 +2298,24 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 		 * NPROC value but kthreads do not. To avoid userspace overflowing
 		 * the system with worker threads fork_owner must be true.
 		 */
+		/*
+		 * 在以下使用vhost_dev->fork_owner:
+		 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+		 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+		 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+		 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+		 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+		 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+		 *
+		 * 注释:
+		 * If fork_owner is true we use vhost_tasks to create
+		 * the worker so all settings/limits like cgroups, NPROC,
+		 * scheduler, etc are inherited from the owner. If false,
+		 * we use kthreads and only attach to the same cgroups
+		 * as the owner for compat with older kernels.
+		 * here we use true as default value.
+		 * The default value is set by fork_from_owner_default
+		 */
 		if (!dev->fork_owner)
 			return -EFAULT;
 
@@ -1055,6 +2335,11 @@ long vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,
 		return -ENOIOCTLCMD;
 	}
 
+	/*
+	 * 在以下使用vhost_get_vq_from_user():
+2167  *   - drivers/vhost/vhost.c|2153| <<vhost_worker_ioctl>> ret = vhost_get_vq_from_user(dev, argp, &vq, &idx);
+2168  *   - drivers/vhost/vhost.c|3292| <<vhost_vring_ioctl>> r = vhost_get_vq_from_user(d, argp, &vq, &idx);
+	 */
 	ret = vhost_get_vq_from_user(dev, argp, &vq, &idx);
 	if (ret)
 		return ret;
@@ -1097,6 +2382,13 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 	struct vhost_worker *worker;
 	int err, i;
 
+	/*
+	 * 在以下使用vhost_dev_has_owner():
+	 *   - drivers/vhost/net.c|1735| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+	 *   - drivers/vhost/vhost.c|1670| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+	 *   - drivers/vhost/vhost.c|1747| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+	 *   - drivers/vhost/vhost.c|2989| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> if (vhost_dev_has_owner(d)) {
+	 */
 	/* Is there an owner already? */
 	if (vhost_dev_has_owner(dev)) {
 		err = -EBUSY;
@@ -1109,6 +2401,20 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 	if (err)
 		goto err_iovecs;
 
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	if (dev->use_worker) {
 		/*
 		 * This should be done last, because vsock can queue work
@@ -1122,6 +2428,11 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 			goto err_worker;
 		}
 
+		/*
+		 * 在以下使用__vhost_vq_attach_worker():
+		 *   - drivers/vhost/vhost.c|974| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+		 *   - drivers/vhost/vhost.c|1161| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+		 */
 		for (i = 0; i < dev->nvqs; i++)
 			__vhost_vq_attach_worker(dev->vqs[i], worker);
 	}
@@ -1156,6 +2467,24 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 
 	vhost_dev_cleanup(dev);
 
+	/*
+	 * 在以下使用vhost_dev->fork_owner:
+	 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+	 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+	 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+	 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+	 *
+	 * 注释:
+	 * If fork_owner is true we use vhost_tasks to create
+	 * the worker so all settings/limits like cgroups, NPROC,
+	 * scheduler, etc are inherited from the owner. If false,
+	 * we use kthreads and only attach to the same cgroups
+	 * as the owner for compat with older kernels.
+	 * here we use true as default value.
+	 * The default value is set by fork_from_owner_default
+	 */
 	dev->fork_owner = fork_from_owner_default;
 	dev->umem = umem;
 	/* We don't need VQ locks below since vhost_dev_cleanup makes sure
@@ -1166,11 +2495,29 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner);
 
+/*
+ * 在以下使用vhost_dev_stop():
+ *   - drivers/vhost/net.c|1458| <<vhost_net_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/net.c|1667| <<vhost_net_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/scsi.c|2454| <<vhost_scsi_release>> vhost_dev_stop(&vs->dev);
+ *   - drivers/vhost/test.c|173| <<vhost_test_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/test.c|248| <<vhost_test_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/vdpa.c|1482| <<vhost_vdpa_release>> vhost_dev_stop(&v->vdev);
+ *   - drivers/vhost/vsock.c|783| <<vhost_vsock_dev_release>> vhost_dev_stop(&vsock->dev);
+ */
 void vhost_dev_stop(struct vhost_dev *dev)
 {
 	int i;
 
 	for (i = 0; i < dev->nvqs; ++i) {
+		/*
+		 * 在以下使用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|443| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|319| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1450| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 */
 		if (dev->vqs[i]->kick && dev->vqs[i]->handle_kick)
 			vhost_poll_stop(&dev->vqs[i]->poll);
 	}
@@ -1199,6 +2546,15 @@ void vhost_clear_msg(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_clear_msg);
 
+/*
+ * 在以下使用vhost_dev_cleanup():
+ *   - drivers/vhost/net.c|1459| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/scsi.c|2455| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+ *   - drivers/vhost/test.c|174| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/vdpa.c|1395| <<vhost_vdpa_cleanup>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vhost.c|2436| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+ *   - drivers/vhost/vsock.c|787| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev)
 {
 	int i;
@@ -1210,6 +2566,11 @@ void vhost_dev_cleanup(struct vhost_dev *dev)
 			fput(dev->vqs[i]->kick);
 		if (dev->vqs[i]->call_ctx.ctx)
 			eventfd_ctx_put(dev->vqs[i]->call_ctx.ctx);
+		/*
+		 * 在以下使用vhost_vq_reset():
+		 *   - drivers/vhost/vhost.c|1057| <<vhost_vq_reset>> vhost_vq_reset(dev, vq);
+		 *   - drivers/vhost/vhost.c|1818| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+		 */
 		vhost_vq_reset(dev, dev->vqs[i]);
 	}
 	vhost_dev_free_iovecs(dev);
@@ -1670,6 +3031,12 @@ static int vhost_process_iotlb_msg(struct vhost_dev *dev, u32 asid,
 
 	return ret;
 }
+/*
+ * 在以下使用vhost_chr_write_iter():
+ *   - drivers/vhost/net.c|1874| <<vhost_net_chr_write_iter>> return vhost_chr_write_iter(dev, from);
+ *   - drivers/vhost/vdpa.c|1314| <<vhost_vdpa_chr_write_iter>> return vhost_chr_write_iter(dev, from);
+ *   - drivers/vhost/vsock.c|937| <<vhost_vsock_chr_write_iter>> return vhost_chr_write_iter(dev, from);
+ */
 ssize_t vhost_chr_write_iter(struct vhost_dev *dev,
 			     struct iov_iter *from)
 {
@@ -2130,6 +3497,15 @@ static long vhost_vring_set_num_addr(struct vhost_dev *d,
 
 	return r;
 }
+
+/*
+ * 在以下使用vhost_vring_ioctl():
+ *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/scsi.c|2540| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+ *   - drivers/vhost/test.c|369| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/vdpa.c|719| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+ *   - drivers/vhost/vsock.c|912| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+ */
 long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
 	struct file *eventfp, *filep = NULL;
@@ -2141,6 +3517,11 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 	u32 idx;
 	long r;
 
+	/*
+	 * 在以下使用vhost_get_vq_from_user():
+	 *   - drivers/vhost/vhost.c|2153| <<vhost_worker_ioctl>> ret = vhost_get_vq_from_user(dev, argp, &vq, &idx);
+	 *   - drivers/vhost/vhost.c|3292| <<vhost_vring_ioctl>> r = vhost_get_vq_from_user(d, argp, &vq, &idx);
+	 */
 	r = vhost_get_vq_from_user(d, argp, &vq, &idx);
 	if (r < 0)
 		return r;
@@ -2251,6 +3632,14 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 		r = -ENOIOCTLCMD;
 	}
 
+	/*
+	 * 在以下使用vhost_poll_stop():
+	 *   - drivers/vhost/net.c|443| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/test.c|285| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+	 *   - drivers/vhost/vhost.c|319| <<vhost_poll_start>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/vhost.c|1450| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+	 *   - drivers/vhost/vhost.c|2530| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+	 */
 	if (pollstop && vq->handle_kick)
 		vhost_poll_stop(&vq->poll);
 
@@ -2297,6 +3686,14 @@ int vhost_init_device_iotlb(struct vhost_dev *d)
 }
 EXPORT_SYMBOL_GPL(vhost_init_device_iotlb);
 
+/*
+ * 在以下使用vhost_dev_ioctl():
+ *   - drivers/vhost/net.c|1847| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/scsi.c|2537| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+ *   - drivers/vhost/test.c|367| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/vdpa.c|886| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+ *   - drivers/vhost/vsock.c|910| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp);
+ */
 /* Caller must have device mutex */
 long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
@@ -2313,6 +3710,13 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 
 #ifdef CONFIG_VHOST_ENABLE_FORK_OWNER_CONTROL
 	if (ioctl == VHOST_SET_FORK_FROM_OWNER) {
+		/*
+		 * 在以下使用vhost_dev_has_owner():
+		 *   - drivers/vhost/net.c|1735| <<vhost_net_set_owner>> if (vhost_dev_has_owner(&n->dev)) {
+		 *   - drivers/vhost/vhost.c|1670| <<vhost_worker_ioctl>> if (!vhost_dev_has_owner(dev))
+		 *   - drivers/vhost/vhost.c|1747| <<vhost_dev_set_owner>> if (vhost_dev_has_owner(dev)) {
+		 *   - drivers/vhost/vhost.c|2989| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> if (vhost_dev_has_owner(d)) {
+		 */
 		/* Only allow modification before owner is set */
 		if (vhost_dev_has_owner(d)) {
 			r = -EBUSY;
@@ -2329,11 +3733,47 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 			r = -EINVAL;
 			goto done;
 		}
+		/*
+		 * 在以下使用vhost_dev->fork_owner:
+		 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+		 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+		 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+		 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+		 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+		 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+		 *
+		 * 注释:
+		 * If fork_owner is true we use vhost_tasks to create
+		 * the worker so all settings/limits like cgroups, NPROC,
+		 * scheduler, etc are inherited from the owner. If false,
+		 * we use kthreads and only attach to the same cgroups
+		 * as the owner for compat with older kernels.
+		 * here we use true as default value.
+		 * The default value is set by fork_from_owner_default
+		 */
 		d->fork_owner = !!fork_owner_val;
 		r = 0;
 		goto done;
 	}
 	if (ioctl == VHOST_GET_FORK_FROM_OWNER) {
+		/*
+		 * 在以下使用vhost_dev->fork_owner:
+		 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+		 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+		 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+		 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+		 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+		 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+		 *
+		 * 注释:
+		 * If fork_owner is true we use vhost_tasks to create
+		 * the worker so all settings/limits like cgroups, NPROC,
+		 * scheduler, etc are inherited from the owner. If false,
+		 * we use kthreads and only attach to the same cgroups
+		 * as the owner for compat with older kernels.
+		 * here we use true as default value.
+		 * The default value is set by fork_from_owner_default
+		 */
 		u8 fork_owner_val = d->fork_owner;
 
 		if (fork_owner_val != VHOST_FORK_OWNER_TASK &&
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 621a6d9a8..6dedee4b8 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -19,6 +19,13 @@ struct vhost_work;
 struct vhost_task;
 typedef void (*vhost_work_fn_t)(struct vhost_work *work);
 
+/*
+ * 在以下使用VHOST_WORK_QUEUED:
+ *   - drivers/vhost/vhost.c|343| <<vhost_work_init>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+ *   - drivers/vhost/vhost.c|525| <<vhost_worker_queue>> if (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {
+ *   - drivers/vhost/vhost.c|800| <<vhost_run_work_kthread_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+ *   - drivers/vhost/vhost.c|832| <<vhost_run_work_list>> clear_bit(VHOST_WORK_QUEUED, &work->flags);
+ */
 #define VHOST_WORK_QUEUED 1
 struct vhost_work {
 	struct llist_node	node;
@@ -42,9 +49,33 @@ struct vhost_worker {
 	struct vhost_dev	*dev;
 	/* Used to serialize device wide flushing with worker swapping. */
 	struct mutex		mutex;
+	/*
+	 * 在以下使用vhost_worker->work_list:
+	 *   - 116 drivers/vhost/vhost.c|547| <<vhost_worker_queue>> llist_add(&work->node, &worker->work_list);
+	 *   - drivers/vhost/vhost.c|705| <<vhost_vq_has_work>> if (worker && !llist_empty(&worker->work_list))
+	 *   - drivers/vhost/vhost.c|834| <<vhost_run_work_kthread_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|873| <<vhost_run_work_list>> node = llist_del_all(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1303| <<vhost_worker_destroy>> WARN_ON(!llist_empty(&worker->work_list));
+	 *   - drivers/vhost/vhost.c|1652| <<vhost_worker_create>> init_llist_head(&worker->work_list);
+	 *   - drivers/vhost/vhost.c|1765| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *                                                         !llist_empty(&old_worker->work_list));
+	 */
 	struct llist_head	work_list;
 	u64			kcov_handle;
 	u32			id;
+	/*
+	 * 在以下使用vhost_worker->attachment_cnt:
+	 *   - drivers/vhost/vhost.c|310| <<__vhost_worker_flush>> if (!worker->attachment_cnt || worker->killed)
+	 *   - drivers/vhost/vhost.c|512| <<vhost_worker_killed>> worker->attachment_cnt -= attach_cnt;
+	 *   - drivers/vhost/vhost.c|693| <<vhost_attach_task_to_cgroups>> saved_cnt = worker->attachment_cnt;
+	 *   - drivers/vhost/vhost.c|694| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = INT_MAX;
+	 *   - drivers/vhost/vhost.c|696| <<vhost_attach_task_to_cgroups>> worker->attachment_cnt = saved_cnt;
+	 *   - drivers/vhost/vhost.c|907| <<__vhost_vq_attach_worker>> worker->attachment_cnt++;
+	 *   - drivers/vhost/vhost.c|938| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|945| <<__vhost_vq_attach_worker>> WARN_ON(!old_worker->attachment_cnt &&
+	 *   - drivers/vhost/vhost.c|955| <<__vhost_vq_attach_worker>> old_worker->attachment_cnt--;
+	 *   - drivers/vhost/vhost.c|1008| <<vhost_free_worker>> if (worker->attachment_cnt || worker->killed) {
+	 */
 	int			attachment_cnt;
 	bool			killed;
 	const struct vhost_worker_ops *ops;
@@ -189,7 +220,33 @@ struct vhost_dev {
 	int iov_limit;
 	int weight;
 	int byte_weight;
+	/*
+	 * 在以下使用vhost_dev->worker_xa:
+	 *   - drivers/vhost/vhost.c|1071| <<global>> xa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);
+	 *   - drivers/vhost/vhost.c|680| <<vhost_dev_flush>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1249| <<vhost_worker_destroy>> xa_erase(&dev->worker_xa, worker->id);
+	 *   - drivers/vhost/vhost.c|1268| <<vhost_workers_free>> xa_for_each(&dev->worker_xa, i, worker)
+	 *   - drivers/vhost/vhost.c|1270| <<vhost_workers_free>> xa_destroy(&dev->worker_xa);
+	 *   - drivers/vhost/vhost.c|1332| <<vhost_task_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1354| <<vhost_kthread_worker_create>> ret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);
+	 *   - drivers/vhost/vhost.c|1556| <<vhost_vq_attach_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 *   - drivers/vhost/vhost.c|1594| <<vhost_free_worker>> worker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);
+	 */
 	struct xarray worker_xa;
+	/*
+	 * 在以下设置vhost_dev->use_worker:
+	 * scsi => true
+	 * net  => true
+	 *   - drivers/vhost/vhost.c|1142| <<vhost_dev_init>> dev->use_worker = use_worker;
+	 * 在以下使用vhost_dev->use_worker:
+	 *   - drivers/vhost/vhost.c|326| <<vhost_poll_wakeup>> if (!poll->dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1333| <<vhost_attach_mm>> if (dev->use_worker) {
+	 *   - drivers/vhost/vhost.c|1357| <<vhost_detach_mm>> if (dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1414| <<vhost_workers_free>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|1957| <<vhost_vq_attach_worker>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_worker_ioctl>> if (!dev->use_worker)
+	 *   - drivers/vhost/vhost.c|2260| <<vhost_dev_set_owner>> if (dev->use_worker) {
+	 */
 	bool use_worker;
 	/*
 	 * If fork_owner is true we use vhost_tasks to create
@@ -200,6 +257,24 @@ struct vhost_dev {
 	 * here we use true as default value.
 	 * The default value is set by fork_from_owner_default
 	 */
+	/*
+	 * 在以下使用vhost_dev->fork_owner:
+	 *   - drivers/vhost/vhost.c|1066| <<global>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|1477| <<vhost_worker_create>> const struct vhost_worker_ops *ops = dev->fork_owner ? &vhost_task_ops :
+	 *   - drivers/vhost/vhost.c|1793| <<vhost_worker_ioctl(VHOST_NEW_WORKER)>> if (!dev->fork_owner)
+	 *   - drivers/vhost/vhost.c|1925| <<vhost_dev_reset_owner>> dev->fork_owner = fork_from_owner_default;
+	 *   - drivers/vhost/vhost.c|3126| <<vhost_dev_ioctl(VHOST_SET_FORK_FROM_OWNER)>> d->fork_owner = !!fork_owner_val;
+	 *   - drivers/vhost/vhost.c|3131| <<vhost_dev_ioctl(VHOST_GET_FORK_FROM_OWNER)>> u8 fork_owner_val = d->fork_owner;
+	 *
+	 * 注释:
+	 * If fork_owner is true we use vhost_tasks to create
+	 * the worker so all settings/limits like cgroups, NPROC,
+	 * scheduler, etc are inherited from the owner. If false,
+	 * we use kthreads and only attach to the same cgroups
+	 * as the owner for compat with older kernels.
+	 * here we use true as default value.
+	 * The default value is set by fork_from_owner_default
+	 */
 	bool fork_owner;
 	int (*msg_handler)(struct vhost_dev *dev, u32 asid,
 			   struct vhost_iotlb_msg *msg);
diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
index ae01457ea..2d951c125 100644
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@ -289,6 +289,15 @@ vhost_transport_send_pkt(struct sk_buff *skb)
 		atomic_inc(&vsock->queued_replies);
 
 	virtio_vsock_skb_queue_tail(&vsock->send_pkt_queue, skb);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
 
 	rcu_read_unlock();
@@ -595,6 +604,15 @@ static int vhost_vsock_start(struct vhost_vsock *vsock)
 		mutex_unlock(&vq->mutex);
 	}
 
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|519| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1605| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1860| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|528| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|292| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|601| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	/* Some packets may have been queued before the device was started,
 	 * let's kick the send worker to send them.
 	 */
@@ -679,6 +697,20 @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)
 	vsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;
 	vsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;
 
+	/*
+	 * 在以下使用vhost_dev_init():
+	 *   - drivers/vhost/net.c|1371| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH, VHOST_NET_PKT_WEIGHT,
+	 *       VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2413| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *       nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *       VHOST_TEST_VQ_MAX, UIO_MAXIOV, VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1431| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *       nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|700| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *       ARRAY_SIZE(vsock->vqs), UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
 		       UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT,
 		       VHOST_VSOCK_WEIGHT, true, NULL);
@@ -875,7 +907,23 @@ static long vhost_vsock_dev_ioctl(struct file *f, unsigned int ioctl,
 		return 0;
 	default:
 		mutex_lock(&vsock->dev.mutex);
+		/*
+		 * 在以下使用vhost_dev_ioctl():
+		 *   - drivers/vhost/net.c|1847| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2537| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|367| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|886| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|910| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		r = vhost_dev_ioctl(&vsock->dev, ioctl, argp);
+		/*
+		 * 在以下使用vhost_vring_ioctl():
+		 *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/scsi.c|2540| <<vhost_scsi_ioctl>> r = vhost_vring_ioctl(&vs->dev, ioctl, argp);
+		 *   - drivers/vhost/test.c|369| <<vhost_test_ioctl>> r = vhost_vring_ioctl(&n->dev, ioctl, argp);
+		 *   - drivers/vhost/vdpa.c|719| <<vhost_vdpa_vring_ioctl>> r = vhost_vring_ioctl(&v->vdev, cmd, argp);
+		 *   - drivers/vhost/vsock.c|912| <<vhost_vsock_dev_ioctl>> r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
+		 */
 		if (r == -ENOIOCTLCMD)
 			r = vhost_vring_ioctl(&vsock->dev, ioctl, argp);
 		else
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index e299e1834..673c2685d 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -19,6 +19,11 @@
 #include <linux/mm.h>
 #include <linux/page_reporting.h>
 
+/*
+ * stress-ng --vm 2 --vm-bytes 80% --vm-hang 0
+ * echo 3 > /proc/sys/vm/drop_caches
+ */
+
 /*
  * Balloon device works in 4K page units.  So each page is pointed to by
  * multiple balloon pages.  All memory counters in this driver are in balloon
@@ -58,15 +63,39 @@ struct virtio_balloon {
 
 	/* Balloon's own wq for cpu-intensive work items */
 	struct workqueue_struct *balloon_wq;
+	/*
+	 * 在以下使用virtio_balloon->report_free_page_work:
+	 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+	 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+	 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+	 */
 	/* The free page reporting work item submitted to the balloon wq */
 	struct work_struct report_free_page_work;
 
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_stats_work:
+	 *   - drivers/virtio/virtio_balloon.c|549| <<stats_request>> queue_work(system_freezable_wq, &vb->update_balloon_stats_work);
+	 *   - drivers/virtio/virtio_balloon.c|1220| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_stats_work, update_balloon_stats_func);
+	 *   - drivers/virtio/virtio_balloon.c|1433| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_stats_work);
+	 */
 	/* The balloon servicing is delegated to a freezable workqueue. */
 	struct work_struct update_balloon_stats_work;
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_size_work:
+	 *   - drivers/virtio/virtio_balloon.c|615| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+	 *   - drivers/virtio/virtio_balloon.c|1156| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+	 *   - drivers/virtio/virtio_balloon.c|1356| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+	 */
 	struct work_struct update_balloon_size_work;
 
 	/* Prevent updating balloon when it is being canceled. */
 	spinlock_t stop_update_lock;
+	/*
+	 * 在以下使用virtio_balloon->stop_update:
+	 *   - drivers/virtio/virtio_balloon.c|445| <<stats_request>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|553| <<virtballoon_changed>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|1205| <<virtballoon_remove>> vb->stop_update = true;
+	 */
 	bool stop_update;
 	/* Bitmap to indicate if reading the related config fields are needed */
 	unsigned long config_read_bitmap;
@@ -180,6 +209,13 @@ static void balloon_ack(struct virtqueue *vq)
 	wake_up(&vb->acked);
 }
 
+/*
+ * 在以下使用tell_host():
+ *   - drivers/virtio/virtio_balloon.c|325| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+ *   - drivers/virtio/virtio_balloon.c|392| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+ *   - drivers/virtio/virtio_balloon.c|1001| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+ *   - drivers/virtio/virtio_balloon.c|1013| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+ */
 static void tell_host(struct virtio_balloon *vb, struct virtqueue *vq)
 {
 	struct scatterlist sg;
@@ -196,6 +232,10 @@ static void tell_host(struct virtio_balloon *vb, struct virtqueue *vq)
 
 }
 
+/*
+ * 在以下使用virtballoon_free_page_report():
+ *   - drivers/virtio/virtio_balloon.c|1105| <<virtballoon_probe>> vb->pr_dev_info.report = virtballoon_free_page_report;
+ */
 static int virtballoon_free_page_report(struct page_reporting_dev_info *pr_dev_info,
 				   struct scatterlist *sg, unsigned int nents)
 {
@@ -223,6 +263,13 @@ static int virtballoon_free_page_report(struct page_reporting_dev_info *pr_dev_i
 	return 0;
 }
 
+/*
+ * 在以下使用set_page_pfns():
+ *   - drivers/virtio/virtio_balloon.c|281| <<fill_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+ *   - drivers/virtio/virtio_balloon.c|330| <<leak_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+ *   - drivers/virtio/virtio_balloon.c|920| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, newpage);
+ *   - drivers/virtio/virtio_balloon.c|925| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, page);
+ */
 static void set_page_pfns(struct virtio_balloon *vb,
 			  __virtio32 pfns[], struct page *page)
 {
@@ -239,6 +286,10 @@ static void set_page_pfns(struct virtio_balloon *vb,
 					  page_to_balloon_pfn(page) + i);
 }
 
+/*
+ * 在以下使用fill_balloon():
+ *   - drivers/virtio/virtio_balloon.c|654| <<update_balloon_size_func>> diff -= fill_balloon(vb, diff);
+ */
 static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 {
 	unsigned int num_allocated_pages;
@@ -262,6 +313,11 @@ static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 			break;
 		}
 
+		/*
+		 * 在以下使用balloon_page_push():
+		 *   - drivers/virtio/virtio_balloon.c|288| <<fill_balloon>> balloon_page_push(&pages, page);
+		 *   - drivers/virtio/virtio_balloon.c|819| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+		 */
 		balloon_page_push(&pages, page);
 	}
 
@@ -272,6 +328,13 @@ static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 	while ((page = balloon_page_pop(&pages))) {
 		balloon_page_enqueue(&vb->vb_dev_info, page);
 
+		/*
+		 * 在以下使用set_page_pfns():
+		 *   - drivers/virtio/virtio_balloon.c|281| <<fill_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+		 *   - drivers/virtio/virtio_balloon.c|330| <<leak_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+		 *   - drivers/virtio/virtio_balloon.c|920| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, newpage);
+		 *   - drivers/virtio/virtio_balloon.c|925| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, page);
+		 */
 		set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
 		vb->num_pages += VIRTIO_BALLOON_PAGES_PER_PAGE;
 		if (!virtio_has_feature(vb->vdev,
@@ -281,6 +344,13 @@ static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 	}
 
 	num_allocated_pages = vb->num_pfns;
+	/*
+	 * 在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|325| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|392| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1001| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1013| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	/* Did we get any? */
 	if (vb->num_pfns != 0)
 		tell_host(vb, vb->inflate_vq);
@@ -289,6 +359,10 @@ static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 	return num_allocated_pages;
 }
 
+/*
+ * 在以下使用release_pages_balloon():
+ *   - drivers/virtio/virtio_balloon.c|343| <<leak_balloon>> release_pages_balloon(vb, &pages);
+ */
 static void release_pages_balloon(struct virtio_balloon *vb,
 				 struct list_head *pages)
 {
@@ -303,6 +377,26 @@ static void release_pages_balloon(struct virtio_balloon *vb,
 	}
 }
 
+/*
+ *
+ * __page_reporting_notify
+ * free_pcppages_bulk
+ * free_frozen_page_commit
+ * __free_frozen_pages
+ * leak_balloon
+ * update_balloon_size_func
+ * process_one_work
+ * worker_thread
+ * kthread
+ * ret_from_fork
+ * ret_from_fork_asm
+ *
+ *
+ * 在以下使用leak_balloon():
+ *   - drivers/virtio/virtio_balloon.c|597| <<update_balloon_size_func>> diff += leak_balloon(vb, -diff);
+ *   - drivers/virtio/virtio_balloon.c|979| <<virtio_balloon_oom_notify>> *freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
+ *   - drivers/virtio/virtio_balloon.c|1175| <<remove_common>> leak_balloon(vb, vb->num_pages);
+ */
 static unsigned int leak_balloon(struct virtio_balloon *vb, size_t num)
 {
 	unsigned int num_freed_pages;
@@ -321,6 +415,13 @@ static unsigned int leak_balloon(struct virtio_balloon *vb, size_t num)
 		page = balloon_page_dequeue(vb_dev_info);
 		if (!page)
 			break;
+		/*
+		 * 在以下使用set_page_pfns():
+		 *   - drivers/virtio/virtio_balloon.c|281| <<fill_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+		 *   - drivers/virtio/virtio_balloon.c|330| <<leak_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+		 *   - drivers/virtio/virtio_balloon.c|920| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, newpage);
+		 *   - drivers/virtio/virtio_balloon.c|925| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, page);
+		 */
 		set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
 		list_add(&page->lru, &pages);
 		vb->num_pages -= VIRTIO_BALLOON_PAGES_PER_PAGE;
@@ -332,6 +433,13 @@ static unsigned int leak_balloon(struct virtio_balloon *vb, size_t num)
 	 * virtio_has_feature(vdev, VIRTIO_BALLOON_F_MUST_TELL_HOST);
 	 * is true, we *have* to do it in this order
 	 */
+	/*
+	 * 在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|325| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|392| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1001| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1013| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	if (vb->num_pfns != 0)
 		tell_host(vb, vb->deflate_vq);
 	release_pages_balloon(vb, &pages);
@@ -398,6 +506,11 @@ static inline unsigned int update_balloon_vm_stats(struct virtio_balloon *vb)
 }
 #endif /* CONFIG_VM_EVENT_COUNTERS */
 
+/*
+ * 在以下使用update_balloon_stats():
+ *   - drivers/virtio/virtio_balloon.c|564| <<stats_handle_request>> num_stats = update_balloon_stats(vb);
+ *   - drivers/virtio/virtio_balloon.c|801| <<init_vqs>> num_stats = update_balloon_stats(vb);
+ */
 static unsigned int update_balloon_stats(struct virtio_balloon *vb)
 {
 	struct sysinfo i;
@@ -431,11 +544,21 @@ static unsigned int update_balloon_stats(struct virtio_balloon *vb)
  * we delegate the job to a freezable workqueue that will do the actual work via
  * stats_handle_request().
  */
+/*
+ * 在以下使用stats_request():
+ *   - drivers/virtio/virtio_balloon.c|774| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_STATS].callback = stats_request;
+ */
 static void stats_request(struct virtqueue *vq)
 {
 	struct virtio_balloon *vb = vq->vdev->priv;
 
 	spin_lock(&vb->stop_update_lock);
+	/*
+	 * 在以下使用virtio_balloon->stop_update:
+	 *   - drivers/virtio/virtio_balloon.c|445| <<stats_request>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|553| <<virtballoon_changed>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|1205| <<virtballoon_remove>> vb->stop_update = true;
+	 */
 	if (!vb->stop_update) {
 		start_wakeup_event(vb, VIRTIO_BALLOON_WAKEUP_SIGNAL_STATS);
 		queue_work(system_freezable_wq, &vb->update_balloon_stats_work);
@@ -443,12 +566,21 @@ static void stats_request(struct virtqueue *vq)
 	spin_unlock(&vb->stop_update_lock);
 }
 
+/*
+ * 在以下使用stats_handle_request():
+ *   - drivers/virtio/virtio_balloon.c|695| <<update_balloon_stats_func>> stats_handle_request(vb);
+ */
 static void stats_handle_request(struct virtio_balloon *vb)
 {
 	struct virtqueue *vq;
 	struct scatterlist sg;
 	unsigned int len, num_stats;
 
+	/*
+	 * 在以下使用update_balloon_stats():
+	 *   - drivers/virtio/virtio_balloon.c|564| <<stats_handle_request>> num_stats = update_balloon_stats(vb);
+	 *   - drivers/virtio/virtio_balloon.c|801| <<init_vqs>> num_stats = update_balloon_stats(vb);
+	 */
 	num_stats = update_balloon_stats(vb);
 
 	vq = vb->stats_vq;
@@ -459,6 +591,12 @@ static void stats_handle_request(struct virtio_balloon *vb)
 	virtqueue_kick(vq);
 }
 
+/*
+ * 在以下使用towards_target():
+ *   - drivers/virtio/virtio_balloon.c|722| <<update_balloon_size_func>> diff = towards_target(vb);
+ *   - drivers/virtio/virtio_balloon.c|1357| <<virtballoon_probe>> if (towards_target(vb))
+ *   - drivers/virtio/virtio_balloon.c|1462| <<virtballoon_restore>> if (towards_target(vb))
+ */
 static inline s64 towards_target(struct virtio_balloon *vb)
 {
 	s64 target;
@@ -476,6 +614,12 @@ static inline s64 towards_target(struct virtio_balloon *vb)
 	return target - vb->num_pages;
 }
 
+/*
+ * 在以下使用return_free_pages_to_mm():
+ *   - drivers/virtio/virtio_balloon.c|839| <<report_free_page_func>> return_free_pages_to_mm(vb, ULONG_MAX);
+ *   - drivers/virtio/virtio_balloon.c|929| <<shrink_free_pages>> blocks_freed = return_free_pages_to_mm(vb, blocks_to_free);
+ *   - drivers/virtio/virtio_balloon.c|1158| <<remove_common>> return_free_pages_to_mm(vb, ULONG_MAX);
+ */
 /* Gives back @num_to_return blocks of free pages to mm. */
 static unsigned long return_free_pages_to_mm(struct virtio_balloon *vb,
 					     unsigned long num_to_return)
@@ -497,6 +641,10 @@ static unsigned long return_free_pages_to_mm(struct virtio_balloon *vb,
 	return num_returned;
 }
 
+/*
+ * 在以下使用virtio_balloon_queue_free_page_work():
+ *   - drivers/virtio/virtio_balloon.c|527| <<virtballoon_changed>> virtio_balloon_queue_free_page_work(vb);
+ */
 static void virtio_balloon_queue_free_page_work(struct virtio_balloon *vb)
 {
 	if (!virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT))
@@ -507,21 +655,49 @@ static void virtio_balloon_queue_free_page_work(struct virtio_balloon *vb)
 			     &vb->config_read_bitmap))
 		return;
 
+	/*
+	 * 在以下使用virtio_balloon->report_free_page_work:
+	 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+	 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+	 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+	 */
 	queue_work(vb->balloon_wq, &vb->report_free_page_work);
 }
 
+/*
+ * 在以下使用start_update_balloon_size():
+ *   - drivers/virtio/virtio_balloon.c|637| <<virtballoon_changed>> start_update_balloon_size(vb);
+ */
 static void start_update_balloon_size(struct virtio_balloon *vb)
 {
 	start_wakeup_event(vb, VIRTIO_BALLOON_WAKEUP_SIGNAL_ADJUST);
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_size_work:
+	 *   - drivers/virtio/virtio_balloon.c|615| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+	 *   - drivers/virtio/virtio_balloon.c|1156| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+	 *   - drivers/virtio/virtio_balloon.c|1356| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+	 */
 	queue_work(system_freezable_wq, &vb->update_balloon_size_work);
 }
 
+/*
+ * 在以下使用virtballoon_changed():
+ *   - struct virtio_driver virtio_balloon_driver.config_changed = virtballoon_changed,
+ *   - drivers/virtio/virtio_balloon.c|1084| <<virtballoon_probe>> virtballoon_changed(vdev);
+ *   - drivers/virtio/virtio_balloon.c|1171| <<virtballoon_restore>> virtballoon_changed(vdev);
+ */
 static void virtballoon_changed(struct virtio_device *vdev)
 {
 	struct virtio_balloon *vb = vdev->priv;
 	unsigned long flags;
 
 	spin_lock_irqsave(&vb->stop_update_lock, flags);
+	/*
+	 * 在以下使用virtio_balloon->stop_update:
+	 *   - drivers/virtio/virtio_balloon.c|445| <<stats_request>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|553| <<virtballoon_changed>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|1205| <<virtballoon_remove>> vb->stop_update = true;
+	 */
 	if (!vb->stop_update) {
 		start_update_balloon_size(vb);
 		virtio_balloon_queue_free_page_work(vb);
@@ -538,6 +714,15 @@ static void update_balloon_size(struct virtio_balloon *vb)
 			 &actual);
 }
 
+/*
+ * 在以下使用virtio_balloon->update_balloon_stats_work:
+ *   - drivers/virtio/virtio_balloon.c|549| <<stats_request>> queue_work(system_freezable_wq, &vb->update_balloon_stats_work);
+ *   - drivers/virtio/virtio_balloon.c|1220| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_stats_work, update_balloon_stats_func);
+ *   - drivers/virtio/virtio_balloon.c|1433| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_stats_work);
+ *
+ * 在以下使用update_balloon_stats_func():
+ *   - drivers/virtio/virtio_balloon.c|1220| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_stats_work, update_balloon_stats_func);
+ */
 static void update_balloon_stats_func(struct work_struct *work)
 {
 	struct virtio_balloon *vb;
@@ -550,6 +735,15 @@ static void update_balloon_stats_func(struct work_struct *work)
 	finish_wakeup_event(vb);
 }
 
+/*
+ * 在以下使用virtio_balloon->update_balloon_size_work:
+ *   - drivers/virtio/virtio_balloon.c|615| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+ *   - drivers/virtio/virtio_balloon.c|1156| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+ *   - drivers/virtio/virtio_balloon.c|1356| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+ *
+ * 在以下使用update_balloon_size_func():
+ *   - drivers/virtio/virtio_balloon.c|1117| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+ */
 static void update_balloon_size_func(struct work_struct *work)
 {
 	struct virtio_balloon *vb;
@@ -560,9 +754,21 @@ static void update_balloon_size_func(struct work_struct *work)
 
 	process_wakeup_event(vb, VIRTIO_BALLOON_WAKEUP_SIGNAL_ADJUST);
 
+	/*
+	 * 在以下使用towards_target():
+	 *   - drivers/virtio/virtio_balloon.c|722| <<update_balloon_size_func>> diff = towards_target(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1357| <<virtballoon_probe>> if (towards_target(vb))
+	 *   - drivers/virtio/virtio_balloon.c|1462| <<virtballoon_restore>> if (towards_target(vb))
+	 */
 	diff = towards_target(vb);
 
 	if (diff) {
+		/*
+		 * 在以下使用leak_balloon():
+		 *   - drivers/virtio/virtio_balloon.c|597| <<update_balloon_size_func>> diff += leak_balloon(vb, -diff);
+		 *   - drivers/virtio/virtio_balloon.c|979| <<virtio_balloon_oom_notify>> *freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
+		 *   - drivers/virtio/virtio_balloon.c|1175| <<remove_common>> leak_balloon(vb, vb->num_pages);
+		 */
 		if (diff > 0)
 			diff -= fill_balloon(vb, diff);
 		else
@@ -621,6 +827,11 @@ static int init_vqs(struct virtio_balloon *vb)
 		 * Prime this virtqueue with one buffer so the hypervisor can
 		 * use it to signal us later (it can't be broken yet!).
 		 */
+		/*
+		 * 在以下使用update_balloon_stats():
+		 *   - drivers/virtio/virtio_balloon.c|564| <<stats_handle_request>> num_stats = update_balloon_stats(vb);
+		 *   - drivers/virtio/virtio_balloon.c|801| <<init_vqs>> num_stats = update_balloon_stats(vb);
+		 */
 		num_stats = update_balloon_stats(vb);
 
 		sg_init_one(&sg, vb->stats, sizeof(vb->stats[0]) * num_stats);
@@ -692,6 +903,10 @@ static int send_cmd_id_stop(struct virtio_balloon *vb)
 	return err;
 }
 
+/*
+ * 在以下使用get_free_page_and_send():
+ *   - drivers/virtio/virtio_balloon.c|787| <<send_free_pages>> err = get_free_page_and_send(vb);
+ */
 static int get_free_page_and_send(struct virtio_balloon *vb)
 {
 	struct virtqueue *vq = vb->free_page_vq;
@@ -725,6 +940,11 @@ static int get_free_page_and_send(struct virtio_balloon *vb)
 		}
 		virtqueue_kick(vq);
 		spin_lock_irq(&vb->free_page_list_lock);
+		/*
+		 * 在以下使用balloon_page_push():
+		 *   - drivers/virtio/virtio_balloon.c|288| <<fill_balloon>> balloon_page_push(&pages, page);
+		 *   - drivers/virtio/virtio_balloon.c|819| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+		 */
 		balloon_page_push(&vb->free_page_list, page);
 		vb->num_free_page_blocks++;
 		spin_unlock_irq(&vb->free_page_list_lock);
@@ -739,6 +959,10 @@ static int get_free_page_and_send(struct virtio_balloon *vb)
 	return 0;
 }
 
+/*
+ * 在以下使用send_free_pages():
+ *   - drivers/virtio/virtio_balloon.c|781| <<virtio_balloon_report_free_page>> err = send_free_pages(vb);
+ */
 static int send_free_pages(struct virtio_balloon *vb)
 {
 	int err;
@@ -768,6 +992,15 @@ static int send_free_pages(struct virtio_balloon *vb)
 	return 0;
 }
 
+/*
+ * 在以下使用virtio_balloon->report_free_page_work:
+ *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+ *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+ *
+ * 在以下使用virtio_balloon_report_free_page():
+ *   - drivers/virtio/virtio_balloon.c|804| <<report_free_page_func>> virtio_balloon_report_free_page(vb);
+ */
 static void virtio_balloon_report_free_page(struct virtio_balloon *vb)
 {
 	int err;
@@ -788,6 +1021,15 @@ static void virtio_balloon_report_free_page(struct virtio_balloon *vb)
 		dev_err(dev, "Failed to send a stop id, err = %d\n", err);
 }
 
+/*
+ * 在以下使用virtio_balloon->report_free_page_work:
+ *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+ *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+ *
+ * 在以下使用report_free_page_func():
+ *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ */
 static void report_free_page_func(struct work_struct *work)
 {
 	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
@@ -796,11 +1038,26 @@ static void report_free_page_func(struct work_struct *work)
 
 	cmd_id_received = virtio_balloon_cmd_id_received(vb);
 	if (cmd_id_received == VIRTIO_BALLOON_CMD_ID_DONE) {
+		/*
+		 * 在以下使用return_free_pages_to_mm():
+		 *   - drivers/virtio/virtio_balloon.c|839| <<report_free_page_func>> return_free_pages_to_mm(vb, ULONG_MAX);
+		 *   - drivers/virtio/virtio_balloon.c|929| <<shrink_free_pages>> blocks_freed = return_free_pages_to_mm(vb, blocks_to_free);
+		 *   - drivers/virtio/virtio_balloon.c|1158| <<remove_common>> return_free_pages_to_mm(vb, ULONG_MAX);
+		 */
 		/* Pass ULONG_MAX to give back all the free pages */
 		return_free_pages_to_mm(vb, ULONG_MAX);
 	} else if (cmd_id_received != VIRTIO_BALLOON_CMD_ID_STOP &&
 		   cmd_id_received !=
 		   virtio32_to_cpu(vb->vdev, vb->cmd_id_active)) {
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work:
+		 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 *
+		 * 在以下使用virtio_balloon_report_free_page():
+		 *   - drivers/virtio/virtio_balloon.c|804| <<report_free_page_func>> virtio_balloon_report_free_page(vb);
+		 */
 		virtio_balloon_report_free_page(vb);
 	}
 }
@@ -824,6 +1081,10 @@ static void report_free_page_func(struct work_struct *work)
  * This function preforms the balloon page migration task.
  * Called through movable_operations->migrate_page
  */
+/*
+ * 在以下使用virtballoon_migratepage():
+ *   - drivers/virtio/virtio_balloon.c|1204| <<virtballoon_probe>> vb->vb_dev_info.migratepage = virtballoon_migratepage;
+ */
 static int virtballoon_migratepage(struct balloon_dev_info *vb_dev_info,
 		struct page *newpage, struct page *page, enum migrate_mode mode)
 {
@@ -862,12 +1123,40 @@ static int virtballoon_migratepage(struct balloon_dev_info *vb_dev_info,
 	__count_vm_event(BALLOON_MIGRATE);
 	spin_unlock_irqrestore(&vb_dev_info->pages_lock, flags);
 	vb->num_pfns = VIRTIO_BALLOON_PAGES_PER_PAGE;
+	/*
+	 * 在以下使用set_page_pfns():
+	 *   - drivers/virtio/virtio_balloon.c|281| <<fill_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+	 *   - drivers/virtio/virtio_balloon.c|330| <<leak_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+	 *   - drivers/virtio/virtio_balloon.c|920| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, newpage);
+	 *   - drivers/virtio/virtio_balloon.c|925| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, page);
+	 */
 	set_page_pfns(vb, vb->pfns, newpage);
+	/*
+	 * 在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|325| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|392| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1001| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1013| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	tell_host(vb, vb->inflate_vq);
 
 	/* balloon's page migration 2nd step -- deflate "page" */
 	vb->num_pfns = VIRTIO_BALLOON_PAGES_PER_PAGE;
+	/*
+	 * 在以下使用set_page_pfns():
+	 *   - drivers/virtio/virtio_balloon.c|281| <<fill_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+	 *   - drivers/virtio/virtio_balloon.c|330| <<leak_balloon>> set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
+	 *   - drivers/virtio/virtio_balloon.c|920| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, newpage);
+	 *   - drivers/virtio/virtio_balloon.c|925| <<virtballoon_migratepage>> set_page_pfns(vb, vb->pfns, page);
+	 */
 	set_page_pfns(vb, vb->pfns, page);
+	/*
+	 * 在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|325| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|392| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1001| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1013| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	tell_host(vb, vb->deflate_vq);
 
 	mutex_unlock(&vb->balloon_lock);
@@ -887,6 +1176,12 @@ static unsigned long shrink_free_pages(struct virtio_balloon *vb,
 	pages_to_free = round_up(pages_to_free,
 				 VIRTIO_BALLOON_HINT_BLOCK_PAGES);
 	blocks_to_free = pages_to_free / VIRTIO_BALLOON_HINT_BLOCK_PAGES;
+	/*
+	 * 在以下使用return_free_pages_to_mm():
+	 *   - drivers/virtio/virtio_balloon.c|839| <<report_free_page_func>> return_free_pages_to_mm(vb, ULONG_MAX);
+	 *   - drivers/virtio/virtio_balloon.c|929| <<shrink_free_pages>> blocks_freed = return_free_pages_to_mm(vb, blocks_to_free);
+	 *   - drivers/virtio/virtio_balloon.c|1158| <<remove_common>> return_free_pages_to_mm(vb, ULONG_MAX);
+	 */
 	blocks_freed = return_free_pages_to_mm(vb, blocks_to_free);
 
 	return blocks_freed * VIRTIO_BALLOON_HINT_BLOCK_PAGES;
@@ -915,6 +1210,12 @@ static int virtio_balloon_oom_notify(struct notifier_block *nb,
 						 struct virtio_balloon, oom_nb);
 	unsigned long *freed = parm;
 
+	/*
+	 * 在以下使用leak_balloon():
+	 *   - drivers/virtio/virtio_balloon.c|597| <<update_balloon_size_func>> diff += leak_balloon(vb, -diff);
+	 *   - drivers/virtio/virtio_balloon.c|979| <<virtio_balloon_oom_notify>> *freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
+	 *   - drivers/virtio/virtio_balloon.c|1175| <<remove_common>> leak_balloon(vb, vb->num_pages);
+	 */
 	*freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
 		  VIRTIO_BALLOON_PAGES_PER_PAGE;
 	update_balloon_size(vb);
@@ -927,6 +1228,10 @@ static void virtio_balloon_unregister_shrinker(struct virtio_balloon *vb)
 	shrinker_free(vb->shrinker);
 }
 
+/*
+ * 配置VIRTIO_BALLOON_F_FREE_PAGE_HINT:
+ *   - drivers/virtio/virtio_balloon.c|1309| <<virtballoon_probe>> err = virtio_balloon_register_shrinker(vb);
+ */
 static int virtio_balloon_register_shrinker(struct virtio_balloon *vb)
 {
 	vb->shrinker = shrinker_alloc(0, "virtio-balloon");
@@ -966,6 +1271,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 	init_waitqueue_head(&vb->acked);
 	vb->vdev = vdev;
 
+	/*
+	 * 在以下使用balloon_devinfo_init()"
+	 *   - arch/powerpc/platforms/pseries/cmm.c|553| <<cmm_balloon_compaction_init>> balloon_devinfo_init(&b_dev_info);
+	 *   - drivers/misc/vmw_balloon.c|1880| <<vmballoon_init>> balloon_devinfo_init(&balloon.b_dev_info);
+	 *   - drivers/virtio/virtio_balloon.c|1197| <<virtballoon_probe>> balloon_devinfo_init(&vb->vb_dev_info);
+	 */
 	balloon_devinfo_init(&vb->vb_dev_info);
 
 	err = init_vqs(vb);
@@ -990,6 +1301,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 			err = -ENOMEM;
 			goto out_del_vqs;
 		}
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work:
+		 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 */
 		INIT_WORK(&vb->report_free_page_work, report_free_page_func);
 		vb->cmd_id_received_cache = VIRTIO_BALLOON_CMD_ID_STOP;
 		vb->cmd_id_active = cpu_to_virtio32(vb->vdev,
@@ -1035,6 +1352,11 @@ static int virtballoon_probe(struct virtio_device *vdev)
 				 poison_val, &poison_val);
 	}
 
+	/*
+	 * struct virtio_balloon *vb;
+	 * -> struct virtqueue *reporting_vq;
+	 * -> struct page_reporting_dev_info pr_dev_info;
+	 */
 	vb->pr_dev_info.report = virtballoon_free_page_report;
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_REPORTING)) {
 		unsigned int capacity;
@@ -1062,6 +1384,11 @@ static int virtballoon_probe(struct virtio_device *vdev)
 		vb->pr_dev_info.order = 5;
 #endif
 
+		/*
+		 * 在以下使用page_reporting_register():
+		 *   - drivers/hv/hv_balloon.c|1667| <<enable_page_reporting>> ret = page_reporting_register(&dm_device.pr_dev_info);
+		 *   - drivers/virtio/virtio_balloon.c|1225| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+		 */
 		err = page_reporting_register(&vb->pr_dev_info);
 		if (err)
 			goto out_unregister_oom;
@@ -1080,6 +1407,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 
 	virtio_device_ready(vdev);
 
+	/*
+	 * 在以下使用virtio_balloon->stop_update:
+	 *   - drivers/virtio/virtio_balloon.c|445| <<stats_request>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|553| <<virtballoon_changed>> if (!vb->stop_update) {
+	 *   - drivers/virtio/virtio_balloon.c|1205| <<virtballoon_remove>> vb->stop_update = true;
+	 */
 	if (towards_target(vb))
 		virtballoon_changed(vdev);
 	return 0;
@@ -1103,11 +1436,23 @@ static int virtballoon_probe(struct virtio_device *vdev)
 
 static void remove_common(struct virtio_balloon *vb)
 {
+	/*
+	 * 在以下使用leak_balloon():
+	 *   - drivers/virtio/virtio_balloon.c|597| <<update_balloon_size_func>> diff += leak_balloon(vb, -diff);
+	 *   - drivers/virtio/virtio_balloon.c|979| <<virtio_balloon_oom_notify>> *freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
+	 *   - drivers/virtio/virtio_balloon.c|1175| <<remove_common>> leak_balloon(vb, vb->num_pages);
+	 */
 	/* There might be pages left in the balloon: free them. */
 	while (vb->num_pages)
 		leak_balloon(vb, vb->num_pages);
 	update_balloon_size(vb);
 
+	/*
+	 * 在以下使用return_free_pages_to_mm():
+	 *   - drivers/virtio/virtio_balloon.c|839| <<report_free_page_func>> return_free_pages_to_mm(vb, ULONG_MAX);
+	 *   - drivers/virtio/virtio_balloon.c|929| <<shrink_free_pages>> blocks_freed = return_free_pages_to_mm(vb, blocks_to_free);
+	 *   - drivers/virtio/virtio_balloon.c|1158| <<remove_common>> return_free_pages_to_mm(vb, ULONG_MAX);
+	 */
 	/* There might be free pages that are being reported: release them. */
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT))
 		return_free_pages_to_mm(vb, ULONG_MAX);
@@ -1135,6 +1480,12 @@ static void virtballoon_remove(struct virtio_device *vdev)
 	cancel_work_sync(&vb->update_balloon_stats_work);
 
 	if (virtio_has_feature(vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT)) {
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work:
+		 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 */
 		cancel_work_sync(&vb->report_free_page_work);
 		destroy_workqueue(vb->balloon_wq);
 	}
diff --git a/fs/eventfd.c b/fs/eventfd.c
index af42b2c7d..bae935108 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -121,6 +121,9 @@ static __poll_t eventfd_poll(struct file *file, poll_table *wait)
 	__poll_t events = 0;
 	u64 count;
 
+	/*
+	 * 执行poll_table->_qproc()
+	 */
 	poll_wait(file, &ctx->wqh, wait);
 
 	/*
diff --git a/include/kvm/iodev.h b/include/kvm/iodev.h
index 56619e332..ee5608cd1 100644
--- a/include/kvm/iodev.h
+++ b/include/kvm/iodev.h
@@ -33,6 +33,30 @@ struct kvm_io_device {
 	const struct kvm_io_device_ops *ops;
 };
 
+/*
+ * 在以下使用kvm_iodevice_init():
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1819| <<vgic_register_its_iodev>> kvm_iodevice_init(&iodev->dev, &kvm_io_gic_ops);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|492| <<vgic_v2_init_dist_iodev>> kvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|764| <<vgic_v3_init_dist_iodev>> kvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|814| <<vgic_register_redist_iodev>> kvm_iodevice_init(&rd_dev->dev, &kvm_io_gic_ops);
+ *   - arch/loongarch/kvm/intc/eiointc.c|645| <<kvm_eiointc_create>> kvm_iodevice_init(device, &kvm_eiointc_ops);
+ *   - arch/loongarch/kvm/intc/eiointc.c|656| <<kvm_eiointc_create>> kvm_iodevice_init(device, &kvm_eiointc_virt_ops);
+ *   - arch/loongarch/kvm/intc/ipi.c|417| <<kvm_ipi_create>> kvm_iodevice_init(device, &kvm_ipi_ops);
+ *   - arch/loongarch/kvm/intc/pch_pic.c|337| <<kvm_pch_pic_init>> kvm_iodevice_init(device, &kvm_pch_pic_ops);
+ *   - arch/mips/kvm/loongson_ipi.c|208| <<kvm_init_loongson_ipi>> kvm_iodevice_init(device, &kvm_ipi_ops);
+ *   - arch/powerpc/kvm/mpic.c|1447| <<map_mmio>> kvm_iodevice_init(&opp->mmio, &mpic_mmio_ops);
+ *   - arch/riscv/kvm/aia_aplic.c|601| <<kvm_riscv_aia_aplic_init>> kvm_iodevice_init(&aplic->iodev, &aplic_iodoev_ops);
+ *   - arch/riscv/kvm/aia_imsic.c|1101| <<kvm_riscv_vcpu_aia_imsic_init>> kvm_iodevice_init(&imsic->iodev, &imsic_iodoev_ops);
+ *   - arch/x86/kvm/i8254.c|773| <<kvm_create_pit>> kvm_iodevice_init(&pit->dev, &pit_dev_ops);
+ *   - arch/x86/kvm/i8254.c|780| <<kvm_create_pit>> kvm_iodevice_init(&pit->speaker_dev, &speaker_dev_ops);
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_master, &picdev_master_ops);
+ *   - arch/x86/kvm/i8259.c|613| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_slave, &picdev_slave_ops);
+ *   - arch/x86/kvm/i8259.c|614| <<kvm_pic_init>> kvm_iodevice_init(&s->dev_elcr, &picdev_elcr_ops);
+ *   - arch/x86/kvm/ioapic.c|814| <<kvm_ioapic_init>> kvm_iodevice_init(&ioapic->dev, &ioapic_mmio_ops);
+ *   - arch/x86/kvm/lapic.c|4431| <<kvm_create_lapic>> kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
+ *   - virt/kvm/coalesced_mmio.c|136| <<kvm_vm_ioctl_register_coalesced_mmio>> kvm_iodevice_init(&dev->dev, &coalesced_mmio_ops);
+ *   - virt/kvm/eventfd.c|899| <<kvm_assign_ioeventfd_idx>> kvm_iodevice_init(&p->dev, &ioeventfd_ops);
+ */
 static inline void kvm_iodevice_init(struct kvm_io_device *dev,
 				     const struct kvm_io_device_ops *ops)
 {
diff --git a/include/linux/balloon_compaction.h b/include/linux/balloon_compaction.h
index 7cfe48769..4493b49ea 100644
--- a/include/linux/balloon_compaction.h
+++ b/include/linux/balloon_compaction.h
@@ -67,6 +67,12 @@ extern size_t balloon_page_list_enqueue(struct balloon_dev_info *b_dev_info,
 extern size_t balloon_page_list_dequeue(struct balloon_dev_info *b_dev_info,
 				     struct list_head *pages, size_t n_req_pages);
 
+/*
+ * 在以下使用balloon_devinfo_init()"
+ *   - arch/powerpc/platforms/pseries/cmm.c|553| <<cmm_balloon_compaction_init>> balloon_devinfo_init(&b_dev_info);
+ *   - drivers/misc/vmw_balloon.c|1880| <<vmballoon_init>> balloon_devinfo_init(&balloon.b_dev_info);
+ *   - drivers/virtio/virtio_balloon.c|1197| <<virtballoon_probe>> balloon_devinfo_init(&vb->vb_dev_info);
+ */
 static inline void balloon_devinfo_init(struct balloon_dev_info *balloon)
 {
 	balloon->isolated_pages = 0;
@@ -135,6 +141,11 @@ static inline void balloon_page_finalize(struct page *page)
  *
  * Caller must ensure the page is private and protect the list.
  */
+/*
+ * 在以下使用balloon_page_push():
+ *   - drivers/virtio/virtio_balloon.c|288| <<fill_balloon>> balloon_page_push(&pages, page);
+ *   - drivers/virtio/virtio_balloon.c|819| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+ */
 static inline void balloon_page_push(struct list_head *pages, struct page *page)
 {
 	list_add(&page->lru, pages);
@@ -147,6 +158,11 @@ static inline void balloon_page_push(struct list_head *pages, struct page *page)
  *
  * Caller must ensure the page is private and protect the list.
  */
+/*
+ * 在以下使用balloon_page_pop():
+ *   - drivers/virtio/virtio_balloon.c|278| <<fill_balloon>> while ((page = balloon_page_pop(&pages))) {
+ *   - drivers/virtio/virtio_balloon.c|494| <<return_free_pages_to_mm>> page = balloon_page_pop(&vb->free_page_list);
+ */
 static inline struct page *balloon_page_pop(struct list_head *pages)
 {
 	struct page *page = list_first_entry_or_null(pages, struct page, lru);
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 15656b7fb..41fcf4626 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -380,6 +380,21 @@ struct kvm_vcpu {
 		bool dy_eligible;
 	} spin_loop;
 #endif
+	/*
+	 * 在以下使用kvm_vcpu->wants_to_run:
+	 *   - arch/arm64/kvm/arm.c|1166| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - arch/loongarch/kvm/vcpu.c|1795| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/mips/kvm/mips.c|436| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/powerpc/kvm/powerpc.c|1849| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/riscv/kvm/vcpu.c|900| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - arch/s390/kvm/kvm-s390.c|5328| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/x86/kvm/vmx/vmx.c|6876| <<vmx_hwapic_isr_update>> WARN_ON_ONCE(vcpu->wants_to_run &&
+	 *   - arch/x86/kvm/x86.c|12138| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - virt/kvm/kvm_main.c|4465| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
+	 *   - virt/kvm/kvm_main.c|4467| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = false;
+	 *   - virt/kvm/kvm_main.c|6383| <<kvm_sched_out>> if (task_is_runnable(current) && vcpu->wants_to_run) {
+	 */
 	bool wants_to_run;
 	bool preempted;
 	bool ready;
@@ -777,6 +792,17 @@ struct kvm {
 	struct kvm_memslots __memslots[KVM_MAX_NR_ADDRESS_SPACES][2];
 	/* The current active memslot set for each address space */
 	struct kvm_memslots __rcu *memslots[KVM_MAX_NR_ADDRESS_SPACES];
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	struct xarray vcpu_array;
 	/*
 	 * Protected by slots_lock, but can be read outside if an
@@ -987,11 +1013,33 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 	i = array_index_nospec(i, num_vcpus);
 
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu.  */
 	smp_rmb();
 	return xa_load(&kvm->vcpu_array, i);
 }
 
+/*
+ * 在以下使用kvm->vcpu_array:
+ *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+ *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+ *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+ *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+ *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+ *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+ *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+ *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+ */
 #define kvm_for_each_vcpu(idx, vcpup, kvm)				\
 	if (atomic_read(&kvm->online_vcpus))				\
 		xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0,	\
diff --git a/include/linux/poll.h b/include/linux/poll.h
index 12bb18e8b..3b9a39bc5 100644
--- a/include/linux/poll.h
+++ b/include/linux/poll.h
@@ -75,6 +75,32 @@ static inline bool file_can_poll(struct file *file)
 	return file->f_op->poll;
 }
 
+/*
+ * 在以下使用vfs_poll():
+ *   - drivers/hv/mshv_eventfd.c|501| <<mshv_irqfd_assign>> events = vfs_poll(fd_file(f), &irqfd->irqfd_polltbl);
+ *   - drivers/vfio/virqfd.c|173| <<mshv_irqfd_assign>> events = vfs_poll(fd_file(irqfd), &virqfd->pt);
+ *   - drivers/vhost/vhost.c|247| <<vhost_poll_start>> mask = vfs_poll(file, &poll->table);
+ *   - drivers/virt/acrn/irqfd.c|157| <<acrn_irqfd_assign>> events = vfs_poll(fd_file(f), &irqfd->pt);
+ *   - drivers/xen/privcmd.c|1024| <<privcmd_irqfd_assign>> events = vfs_poll(fd_file(f), &kirqfd->pt);
+ *   - fs/aio.c|1733| <<aio_poll_complete_work>> mask = vfs_poll(req->file, &pt) & req->events;
+ *   - fs/aio.c|1927| <<aio_poll>> mask = vfs_poll(req->file, &apt.pt) & req->events;
+ *   - fs/eventpoll.c|1059| <<ep_item_poll>> res = vfs_poll(file, pt);
+ *   - fs/select.c|480| <<select_poll_one>> return vfs_poll(fd_file(f), wait);
+ *   - fs/select.c|870| <<do_pollfd>> mask = vfs_poll(fd_file(f), pwait);
+ *   - io_uring/net.c|1797| <<io_connect>> if (vfs_poll(req->file, &pt) & EPOLLERR)
+ *   - io_uring/poll.c|262| <<io_poll_check_events>> req->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;
+ *   - io_uring/poll.c|581| <<__io_arm_poll_handler>> mask = vfs_poll(req->file, &ipt->pt) & poll->events;
+ *   - io_uring/rw.c|46| <<io_file_supports_nowait>> return vfs_poll(req->file, &pt) & mask;
+ *   - mm/memcontrol-v1.c|1177| <<memcg_write_event_control>> vfs_poll(fd_file(efile), &event->pt);
+ *   - net/9p/trans_fd.c|238| <<p9_fd_poll>> ret = vfs_poll(ts->rd, pt);
+ *   - net/9p/trans_fd.c|240| <<p9_fd_poll>> ret = (ret & ~EPOLLOUT) | (vfs_poll(ts->wr, pt) & ~EPOLLIN);
+ *   - virt/kvm/eventfd.c|467| <<kvm_irqfd_assign>> events = vfs_poll(fd_file(f), &irqfd_pt.pt);
+ *
+ * typedef struct poll_table_struct {
+ *     poll_queue_proc _qproc;
+ *     __poll_t _key;
+ * } poll_table;
+ */
 static inline __poll_t vfs_poll(struct file *file, struct poll_table_struct *pt)
 {
 	if (unlikely(!file->f_op->poll))
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index f0f0d49d2..3d116e278 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -209,6 +209,13 @@ struct kvm_xen_exit {
 /* for KVM_RUN, returned by mmap(vcpu_fd, offset=0) */
 struct kvm_run {
 	/* in */
+	/*
+	 * 这个是在userspace的QEMU设置的.
+	 * If we have an interrupt but the guest is not ready to receive an
+	 * interrupt, request an interrupt window exit.  This will
+	 * cause a return to userspace as soon as the guest is ready to
+	 * receive interrupts.
+	 */
 	__u8 request_interrupt_window;
 	__u8 HINT_UNSAFE_IN_KVM(immediate_exit);
 	__u8 padding1[6];
diff --git a/kernel/entry/kvm.c b/kernel/entry/kvm.c
index 8485f6386..025c3422e 100644
--- a/kernel/entry/kvm.c
+++ b/kernel/entry/kvm.c
@@ -3,6 +3,10 @@
 #include <linux/entry-kvm.h>
 #include <linux/kvm_host.h>
 
+/*
+ * 在以下使用xfer_to_guest_mode_work():
+ *   - kernel/entry/kvm.c|47| <<xfer_to_guest_mode_handle_work>> return xfer_to_guest_mode_work(vcpu, ti_work);
+ */
 static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 {
 	do {
@@ -28,6 +32,13 @@ static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 	return 0;
 }
 
+/*
+ * 在以下使用xfer_to_guest_mode_handle_work():
+ *   - arch/arm64/kvm/arm.c|1180| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/loongarch/kvm/vcpu.c|254| <<kvm_enter_guest_check>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|913| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/x86/kvm/x86.c|11796| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+ */
 int xfer_to_guest_mode_handle_work(struct kvm_vcpu *vcpu)
 {
 	unsigned long ti_work;
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 31b072e8d..c337190fd 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1593,6 +1593,27 @@ void kthread_destroy_worker(struct kthread_worker *worker)
 }
 EXPORT_SYMBOL(kthread_destroy_worker);
 
+/*
+ * 在以下使用kthread_use_mm():
+ *   - arch/powerpc/platforms/book3s/vas-api.c|201| <<vas_update_csb>> kthread_use_mm(task_ref->mm);
+ *   - arch/x86/kvm/xen.c|1926| <<kvm_xen_set_evtchn>> kthread_use_mm(kvm->mm);
+ *   - drivers/dma/idxd/cdev.c|753| <<idxd_copy_cr>> kthread_use_mm(mm);
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h|285| <<read_user_wptr>> kthread_use_mm(mmptr); \
+ *   - drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c|3301| <<copy_context_work_handler>> kthread_use_mm(mm);
+ *   - drivers/gpu/drm/amd/amdkfd/kfd_process.c|189| <<kfd_sdma_activity_worker>> kthread_use_mm(mm);
+ *   - drivers/gpu/drm/amd/amdkfd/kfd_process.c|2236| <<send_exception_work_handler>> kthread_use_mm(mm);
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_mman.c|1854| <<i915_gem_mman_live_selftests>> kthread_use_mm(current->active_mm);
+ *   - drivers/gpu/drm/xe/xe_sync.c|82| <<user_fence_worker>> kthread_use_mm(ufence->mm);
+ *   - drivers/gpu/drm/xe/xe_vm.c|4134| <<xe_vm_snapshot_capture_delayed>> kthread_use_mm(snap->snap[i].mm);
+ *   - drivers/iommu/iommufd/pages.c|2063| <<iopt_pages_rw_access>> kthread_use_mm(pages->source_mm);
+ *   - drivers/usb/gadget/function/f_fs.c|859| <<ffs_user_copy_worker>> kthread_use_mm(io_data->mm);
+ *   - drivers/usb/gadget/legacy/inode.c|476| <<ep_user_copy_worker>> kthread_use_mm(mm);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|182| <<vdpasim_work_fn>> kthread_use_mm(mm);
+ *   - drivers/vfio/vfio_iommu_type1.c|3048| <<vfio_iommu_type1_dma_rw_chunk>> kthread_use_mm(mm);
+ *   - drivers/vhost/vhost.c|893| <<vhost_run_work_kthread_list>> kthread_use_mm(dev->mm);
+ *   - fs/bcachefs/fs-io-direct.c|576| <<bch2_dio_write_continue>> kthread_use_mm(mm);
+ *   - lib/kunit/user_alloc.c|47| <<kunit_attach_mm>> kthread_use_mm(mm);
+ */
 /**
  * kthread_use_mm - make the calling kthread operate on an address space
  * @mm: address space to operate on
diff --git a/kernel/vhost_task.c b/kernel/vhost_task.c
index 27107dcc1..02a95a072 100644
--- a/kernel/vhost_task.c
+++ b/kernel/vhost_task.c
@@ -8,6 +8,25 @@
 #include <linux/sched/vhost_task.h>
 #include <linux/sched/signal.h>
 
+/*
+ * vhost_task是vhost为worker实现的一种特殊任务(task)机制. 它不是典型的kthread,
+ * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"用户空间进程(如QEMU进程)
+ * 的mm(内存地址空间)和cgroup.
+ *
+ * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+ * 也不完全继承用户进程的 cgroup.
+ */
+
+/*
+ * 在以下使用VHOST_TASK_FLAGS_STOP:
+ *   - kernel/vhost_task.c|53| <<vhost_task_fn>> if (test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+ *   - kernel/vhost_task.c|69| <<vhost_task_fn>> if (!test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+ *   - kernel/vhost_task.c|113| <<vhost_task_stop>> set_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags);
+ *
+ * 在以下使用VHOST_TASK_FLAGS_KILLED:
+ *   - kernel/vhost_task.c|70| <<vhost_task_fn>> set_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags);
+ *   - kernel/vhost_task.c|112| <<vhost_task_stop>> if (!test_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags)) {
+ */
 enum vhost_task_flags {
 	VHOST_TASK_FLAGS_STOP,
 	VHOST_TASK_FLAGS_KILLED,
@@ -41,6 +60,12 @@ static int vhost_task_fn(void *data)
 		/* mb paired w/ vhost_task_stop */
 		set_current_state(TASK_INTERRUPTIBLE);
 
+		/*
+		 * 在以下使用VHOST_TASK_FLAGS_STOP:
+		 *   - kernel/vhost_task.c|53| <<vhost_task_fn>> if (test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+		 *   - kernel/vhost_task.c|69| <<vhost_task_fn>> if (!test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+		 *   - kernel/vhost_task.c|113| <<vhost_task_stop>> set_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags);
+		 */
 		if (test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
 			__set_current_state(TASK_RUNNING);
 			break;
@@ -57,7 +82,18 @@ static int vhost_task_fn(void *data)
 	 * When the vhost layer has called vhost_task_stop it's already stopped
 	 * new work and flushed.
 	 */
+	/*
+	 * 在以下使用VHOST_TASK_FLAGS_STOP:
+	 *   - kernel/vhost_task.c|53| <<vhost_task_fn>> if (test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+	 *   - kernel/vhost_task.c|69| <<vhost_task_fn>> if (!test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+	 *   - kernel/vhost_task.c|113| <<vhost_task_stop>> set_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags);
+	 */
 	if (!test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+		/*
+		 * 在以下使用VHOST_TASK_FLAGS_KILLED:
+		 *   - kernel/vhost_task.c|70| <<vhost_task_fn>> set_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags);
+		 *   - kernel/vhost_task.c|112| <<vhost_task_stop>> if (!test_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags)) {
+		 */
 		set_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags);
 		vtsk->handle_sigkill(vtsk->data);
 	}
@@ -73,6 +109,12 @@ static int vhost_task_fn(void *data)
  *
  * wake up the vhost_task worker thread
  */
+/*
+ * 在以下使用vhost_task_wake():
+ *   - arch/x86/kvm/mmu/mmu.c|7446| <<kvm_wake_nx_recovery_thread>> vhost_task_wake(nx_thread);
+ *   - drivers/vhost/vhost.c|1538| <<vhost_task_wakeup>> return vhost_task_wake(worker->vtsk);
+ *   - kernel/vhost_task.c|108| <<vhost_task_stop>> vhost_task_wake(vtsk);
+ */
 void vhost_task_wake(struct vhost_task *vtsk)
 {
 	wake_up_process(vtsk->task);
@@ -86,11 +128,33 @@ EXPORT_SYMBOL_GPL(vhost_task_wake);
  * vhost_task_fn ensures the worker thread exits after
  * VHOST_TASK_FLAGS_STOP becomes true.
  */
+/*
+ * 在以下使用vhost_task_stop():
+ *   - arch/x86/kvm/mmu/mmu.c|7821| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+ *   - drivers/vhost/vhost.c|1592| <<vhost_task_do_stop>> return vhost_task_stop(worker->vtsk);
+ */
 void vhost_task_stop(struct vhost_task *vtsk)
 {
 	mutex_lock(&vtsk->exit_mutex);
+	/*
+	 * 在以下使用VHOST_TASK_FLAGS_KILLED:
+	 *   - kernel/vhost_task.c|70| <<vhost_task_fn>> set_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags);
+	 *   - kernel/vhost_task.c|112| <<vhost_task_stop>> if (!test_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags)) {
+	 */
 	if (!test_bit(VHOST_TASK_FLAGS_KILLED, &vtsk->flags)) {
+		/*
+		 * 在以下使用VHOST_TASK_FLAGS_STOP:
+		 *   - kernel/vhost_task.c|53| <<vhost_task_fn>> if (test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+		 *   - kernel/vhost_task.c|69| <<vhost_task_fn>> if (!test_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags)) {
+		 *   - kernel/vhost_task.c|113| <<vhost_task_stop>> set_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags);
+		 */
 		set_bit(VHOST_TASK_FLAGS_STOP, &vtsk->flags);
+		/*
+		 * 在以下使用vhost_task_wake():
+		 *   - arch/x86/kvm/mmu/mmu.c|7446| <<kvm_wake_nx_recovery_thread>> vhost_task_wake(nx_thread);
+		 *   - drivers/vhost/vhost.c|1538| <<vhost_task_wakeup>> return vhost_task_wake(worker->vtsk);
+		 *   - kernel/vhost_task.c|108| <<vhost_task_stop>> vhost_task_wake(vtsk);
+		 */
 		vhost_task_wake(vtsk);
 	}
 	mutex_unlock(&vtsk->exit_mutex);
@@ -116,6 +180,14 @@ EXPORT_SYMBOL_GPL(vhost_task_stop);
  * failure. The returned task is inactive, and the caller must fire it up
  * through vhost_task_start().
  */
+/*
+ * 在以下使用vhost_task_create():
+ *   - arch/x86/kvm/mmu/mmu.c|7783| <<kvm_mmu_start_lpage_recovery>> nx_thread = vhost_task_create(
+ *       kvm_nx_huge_page_recovery_worker, kvm_nx_huge_page_recovery_worker_kill,
+ *       kvm, "kvm-nx-lpage-recovery");
+ *   - drivers/vhost/vhost.c|957| <<vhost_task_worker_create>> vtsk = vhost_task_create(
+ *       vhost_run_work_list, vhost_worker_killed, worker, name);
+ */
 struct vhost_task *vhost_task_create(bool (*fn)(void *),
 				     void (*handle_sigkill)(void *), void *arg,
 				     const char *name)
@@ -138,11 +210,24 @@ struct vhost_task *vhost_task_create(bool (*fn)(void *),
 	init_completion(&vtsk->exited);
 	mutex_init(&vtsk->exit_mutex);
 	vtsk->data = arg;
+	/*
+	 * 比如vhost_run_work_list()
+	 */
 	vtsk->fn = fn;
 	vtsk->handle_sigkill = handle_sigkill;
 
 	args.fn_arg = vtsk;
 
+	/*
+	 * vhost_task是vhost为worker实现的一种特殊任务(task)机制.
+	 * 它不是典型的kthread,
+	 * 而是通过copy_process()创建一个新的进程/任务,这样可以"继承"
+	 * 用户空间进程(如QEMU进程)
+	 * 的mm(内存地址空间)和cgroup.
+	 *
+	 * kthread 则是标准内核线程(kernel thread),不继承用户进程的mm,
+	 * 也不完全继承用户进程的 cgroup. 
+	 */
 	tsk = copy_process(NULL, 0, NUMA_NO_NODE, &args);
 	if (IS_ERR(tsk)) {
 		kfree(vtsk);
@@ -158,6 +243,11 @@ EXPORT_SYMBOL_GPL(vhost_task_create);
  * vhost_task_start - start a vhost_task created with vhost_task_create
  * @vtsk: vhost_task to wake up
  */
+/*
+ * 在以下使用vhost_task_start():
+ *   - arch/x86/kvm/mmu/mmu.c|7790| <<kvm_mmu_start_lpage_recovery>> vhost_task_start(nx_thread);
+ *   - drivers/vhost/vhost.c|963| <<vhost_task_worker_create>> vhost_task_start(vtsk);
+ */
 void vhost_task_start(struct vhost_task *vtsk)
 {
 	wake_up_new_task(vtsk->task);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 9c38a95e9..9110471f3 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -3055,6 +3055,12 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	pmd_populate(mm, pmd, pgtable);
 }
 
+/*
+ * 在以下使用split_huge_pmd_locked():
+ *   - mm/huge_memory.c|3077| <<__split_huge_pmd>> split_huge_pmd_locked(vma, range.start, pmd, freeze);
+ *   - mm/rmap.c|1953| <<try_to_unmap_one>> split_huge_pmd_locked(vma, pvmw.address,
+ *   - mm/rmap.c|2330| <<try_to_migrate_one>> split_huge_pmd_locked(vma, pvmw.address,
+ */
 void split_huge_pmd_locked(struct vm_area_struct *vma, unsigned long address,
 			   pmd_t *pmd, bool freeze)
 {
@@ -3074,6 +3080,12 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 				(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);
 	mmu_notifier_invalidate_range_start(&range);
 	ptl = pmd_lock(vma->vm_mm, pmd);
+	/*
+	 * 在以下使用split_huge_pmd_locked():
+	 *   - mm/huge_memory.c|3077| <<__split_huge_pmd>> split_huge_pmd_locked(vma, range.start, pmd, freeze);
+	 *   - mm/rmap.c|1953| <<try_to_unmap_one>> split_huge_pmd_locked(vma, pvmw.address,
+	 *   - mm/rmap.c|2330| <<try_to_migrate_one>> split_huge_pmd_locked(vma, pvmw.address,
+	 */
 	split_huge_pmd_locked(vma, range.start, pmd, freeze);
 	spin_unlock(ptl);
 	mmu_notifier_invalidate_range_end(&range);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d1d037f97..de8db8f62 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -853,6 +853,12 @@ static inline void move_to_free_list(struct page *page, struct zone *zone,
 	}
 }
 
+/*
+ * 在以下使用__del_page_from_free_list():
+ *   - mm/page_alloc.c|881| <<del_page_from_free_list>> __del_page_from_free_list(page, zone, order, migratetype);
+ *   - mm/page_alloc.c|995| <<__free_one_page>> __del_page_from_free_list(buddy, zone, order, buddy_mt);
+ *   - mm/page_alloc.c|1723| <<page_del_and_expand>> __del_page_from_free_list(page, zone, high, migratetype);
+ */
 static inline void __del_page_from_free_list(struct page *page, struct zone *zone,
 					     unsigned int order, int migratetype)
 {
diff --git a/mm/page_reporting.c b/mm/page_reporting.c
index e4c428e61..f12a2cdce 100644
--- a/mm/page_reporting.c
+++ b/mm/page_reporting.c
@@ -11,6 +11,22 @@
 #include "page_reporting.h"
 #include "internal.h"
 
+/*
+ * 在以下使用page_reporting_order:
+ *   - drivers/hv/hv_balloon.c|1640| <<hv_free_page_report>> page_reporting_order = pageblock_order;
+ *   - mm/page_reporting.c|379| <<page_reporting_register>> page_reporting_order = prdev->order;
+ *   - mm/page_reporting.c|381| <<page_reporting_register>> page_reporting_order = pageblock_order;
+ * 在以下使用page_reporting_order:
+ *   - mm/page_reporting.c|36| <<global>> module_param_cb(page_reporting_order, &page_reporting_param_ops,
+ *   - mm/page_reporting.c|37| <<global>> &page_reporting_order, 0644);
+ *   - mm/page_reporting.c|38| <<global>> MODULE_PARM_DESC(page_reporting_order, "Set page reporting order");
+ *   - drivers/hv/hv_balloon.c|1585| <<hv_free_page_report>> WARN_ON_ONCE(sgl->length < (HV_HYP_PAGE_SIZE << page_reporting_order));
+ *   - drivers/hv/hv_balloon.c|1673| <<enable_page_reporting>> pr_info(...page_reporting_order);
+ *   - mm/page_reporting.c|269| <<page_reporting_process_zone>> watermark = low_wmark_pages(zone) + (PAGE_REPORTING_CAPACITY << page_reporting_order);
+ *   - mm/page_reporting.c|279| <<page_reporting_process_zone>> for (order = page_reporting_order; order < NR_PAGE_ORDERS; order++) {
+ *   - mm/page_reporting.c|377| <<page_reporting_register>> if (page_reporting_order == -1) {
+ *   - mm/page_reporting.h|40| <<page_reporting_notify_free>> if (order < page_reporting_order)
+ */
 /* Initialize to an unsupported value */
 unsigned int page_reporting_order = -1;
 
@@ -57,6 +73,11 @@ enum {
 };
 
 /* request page reporting */
+/*
+ * 在以下使用__page_reporting_request():
+ *   - mm/page_reporting.c|99| <<__page_reporting_notify>> __page_reporting_request(prdev);
+ *   - mm/page_reporting.c|389| <<page_reporting_register>> __page_reporting_request(prdev);
+ */
 static void
 __page_reporting_request(struct page_reporting_dev_info *prdev)
 {
@@ -349,6 +370,11 @@ static void page_reporting_process(struct work_struct *work)
 static DEFINE_MUTEX(page_reporting_mutex);
 DEFINE_STATIC_KEY_FALSE(page_reporting_enabled);
 
+/*
+ * 在以下使用page_reporting_register():
+ *   - drivers/hv/hv_balloon.c|1667| <<enable_page_reporting>> ret = page_reporting_register(&dm_device.pr_dev_info);
+ *   - drivers/virtio/virtio_balloon.c|1225| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+ */
 int page_reporting_register(struct page_reporting_dev_info *prdev)
 {
 	int err = 0;
@@ -369,6 +395,14 @@ int page_reporting_register(struct page_reporting_dev_info *prdev)
 	 * pageblock_order.
 	 */
 
+	/*
+	 * 在以下使用page_reporting_order:
+	 *   - drivers/hv/hv_balloon.c|1640| <<hv_free_page_report>> page_reporting_order = pageblock_order;
+	 *   - mm/page_reporting.c|379| <<page_reporting_register>> page_reporting_order = prdev->order;
+	 *   - mm/page_reporting.c|381| <<page_reporting_register>> page_reporting_order = pageblock_order;
+	 *
+	 * pageblock_order是9!!!
+	 */
 	if (page_reporting_order == -1) {
 		if (prdev->order > 0 && prdev->order <= MAX_PAGE_ORDER)
 			page_reporting_order = prdev->order;
diff --git a/mm/rmap.c b/mm/rmap.c
index 568198e9e..dc514c788 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1950,6 +1950,12 @@ static bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,
 				 * We temporarily have to drop the PTL and
 				 * restart so we can process the PTE-mapped THP.
 				 */
+				/*
+				 * 在以下使用split_huge_pmd_locked():
+				 *   - mm/huge_memory.c|3077| <<__split_huge_pmd>> split_huge_pmd_locked(vma, range.start, pmd, freeze);
+				 *   - mm/rmap.c|1953| <<try_to_unmap_one>> split_huge_pmd_locked(vma, pvmw.address,
+				 *   - mm/rmap.c|2330| <<try_to_migrate_one>> split_huge_pmd_locked(vma, pvmw.address,
+				 */
 				split_huge_pmd_locked(vma, pvmw.address,
 						      pvmw.pmd, false);
 				flags &= ~TTU_SPLIT_HUGE_PMD;
@@ -2327,6 +2333,12 @@ static bool try_to_migrate_one(struct folio *folio, struct vm_area_struct *vma,
 		/* PMD-mapped THP migration entry */
 		if (!pvmw.pte) {
 			if (flags & TTU_SPLIT_HUGE_PMD) {
+				/*
+				 * 在以下使用split_huge_pmd_locked():
+				 *   - mm/huge_memory.c|3077| <<__split_huge_pmd>> split_huge_pmd_locked(vma, range.start, pmd, freeze);
+				 *   - mm/rmap.c|1953| <<try_to_unmap_one>> split_huge_pmd_locked(vma, pvmw.address,
+				 *   - mm/rmap.c|2330| <<try_to_migrate_one>> split_huge_pmd_locked(vma, pvmw.address,
+				 */
 				split_huge_pmd_locked(vma, pvmw.address,
 						      pvmw.pmd, true);
 				ret = false;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 6c07dd423..6852844bc 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -486,8 +486,30 @@ void kvm_destroy_vcpus(struct kvm *kvm)
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		kvm_vcpu_destroy(vcpu);
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		xa_erase(&kvm->vcpu_array, i);
 
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		/*
 		 * Assert that the vCPU isn't visible in any way, to ensure KVM
 		 * doesn't trigger a use-after-free if destroying vCPUs results
@@ -1122,6 +1144,17 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	mutex_init(&kvm->slots_arch_lock);
 	spin_lock_init(&kvm->mn_invalidate_lock);
 	rcuwait_init(&kvm->mn_memslots_update_rcuwait);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_init(&kvm->vcpu_array);
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 	xa_init(&kvm->mem_attr_array);
@@ -3994,6 +4027,17 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 		if (idx == me->vcpu_idx)
 			continue;
 
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		vcpu = xa_load(&kvm->vcpu_array, idx);
 		if (!READ_ONCE(vcpu->ready))
 			continue;
@@ -4213,6 +4257,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 	}
 
 	vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
 	WARN_ON_ONCE(r == -EBUSY);
 	if (r)
@@ -4248,6 +4303,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 kvm_put_xa_erase:
 	mutex_unlock(&vcpu->mutex);
 	kvm_put_kvm_no_destroy(kvm);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
 unlock_vcpu_destroy:
 	mutex_unlock(&kvm->lock);
@@ -4462,6 +4528,21 @@ static long kvm_vcpu_ioctl(struct file *filp,
 
 			put_pid(oldpid);
 		}
+		/*
+		 * 在以下使用kvm_vcpu->wants_to_run:
+		 *   - arch/arm64/kvm/arm.c|1166| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/loongarch/kvm/vcpu.c|1795| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/mips/kvm/mips.c|436| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/powerpc/kvm/powerpc.c|1849| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/riscv/kvm/vcpu.c|900| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/s390/kvm/kvm-s390.c|5328| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/x86/kvm/vmx/vmx.c|6876| <<vmx_hwapic_isr_update>> WARN_ON_ONCE(vcpu->wants_to_run &&
+		 *   - arch/x86/kvm/x86.c|12138| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - virt/kvm/kvm_main.c|4465| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
+		 *   - virt/kvm/kvm_main.c|4467| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = false;
+		 *   - virt/kvm/kvm_main.c|6383| <<kvm_sched_out>> if (task_is_runnable(current) && vcpu->wants_to_run) {
+		 */
 		vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
 		r = kvm_arch_vcpu_ioctl_run(vcpu);
 		vcpu->wants_to_run = false;
@@ -5932,6 +6013,28 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 }
 EXPORT_SYMBOL_GPL(kvm_io_bus_read);
 
+/*
+ * 在以下使用kvm_io_bus_register_dev():
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1824| <<vgic_register_its_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|823| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|1101| <<vgic_register_dist_iodev>> return kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,
+ *   - arch/loongarch/kvm/intc/eiointc.c|647| <<kvm_eiointc_create>> ret = kvm_io_bus_register_dev(kvm, KVM_IOCSR_BUS,
+ *   - arch/loongarch/kvm/intc/eiointc.c|657| <<kvm_eiointc_create>> ret = kvm_io_bus_register_dev(kvm, KVM_IOCSR_BUS,
+ *   - arch/loongarch/kvm/intc/ipi.c|419| <<kvm_ipi_create>> ret = kvm_io_bus_register_dev(kvm, KVM_IOCSR_BUS, IOCSR_IPI_BASE, IOCSR_IPI_SIZE, device);
+ *   - arch/loongarch/kvm/intc/pch_pic.c|340| <<kvm_pch_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, addr, PCH_PIC_SIZE, device);
+ *   - arch/mips/kvm/loongson_ipi.c|211| <<kvm_init_loongson_ipi>> kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, addr, 0x400, device);
+ *   - arch/powerpc/kvm/mpic.c|1449| <<map_mmio>> kvm_io_bus_register_dev(opp->kvm, KVM_MMIO_BUS,
+ *   - arch/riscv/kvm/aia_aplic.c|603| <<kvm_riscv_aia_aplic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS,
+ *   - arch/riscv/kvm/aia_imsic.c|1103| <<kvm_riscv_vcpu_aia_imsic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/i8254.c|774| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
+ *   - arch/x86/kvm/i8254.c|781| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
+ *   - arch/x86/kvm/i8259.c|607| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
+ *   - arch/x86/kvm/i8259.c|616| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_elcr);
+ *   - arch/x86/kvm/ioapic.c|755| <<kvm_ioapic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, ioapic->base_address,
+ *   - virt/kvm/coalesced_mmio.c|141| <<kvm_vm_ioctl_register_coalesced_mmio>> ret = kvm_io_bus_register_dev(kvm,
+ *   - virt/kvm/eventfd.c|901| <<kvm_assign_ioeventfd_idx>> ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
+ */
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev)
 {
-- 
2.50.1 (Apple Git-155)

