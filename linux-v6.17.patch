From 0e8cd1b6db268d850ef5bfe89fcffcc09bbc3be2 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 10 Nov 2025 09:22:48 -0800
Subject: [PATCH 1/1] linux-v6.17

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/include/asm/apic.h            |    5 +
 arch/x86/include/asm/kvm_host.h        |  254 ++++
 arch/x86/include/asm/spec-ctrl.h       |    4 +
 arch/x86/include/asm/vmx.h             |    9 +
 arch/x86/kernel/kvm.c                  |    3 +
 arch/x86/kvm/cpuid.c                   |   37 +
 arch/x86/kvm/hyperv.c                  |   63 +
 arch/x86/kvm/i8259.c                   |    9 +
 arch/x86/kvm/ioapic.c                  |   70 +
 arch/x86/kvm/irq.c                     |  450 ++++++
 arch/x86/kvm/irq.h                     |   78 +
 arch/x86/kvm/kvm_cache_regs.h          |   37 +
 arch/x86/kvm/lapic.c                   | 1907 ++++++++++++++++++++++++
 arch/x86/kvm/lapic.h                   |  236 +++
 arch/x86/kvm/mmu/mmu.c                 |   39 +
 arch/x86/kvm/pmu.c                     |   14 +
 arch/x86/kvm/smm.c                     |    6 +
 arch/x86/kvm/svm/avic.c                |   91 ++
 arch/x86/kvm/svm/hyperv.c              |    9 +
 arch/x86/kvm/svm/nested.c              |  328 ++++
 arch/x86/kvm/svm/sev.c                 |   16 +
 arch/x86/kvm/svm/svm.c                 |  307 ++++
 arch/x86/kvm/svm/svm.h                 |    9 +
 arch/x86/kvm/vmx/capabilities.h        |   36 +
 arch/x86/kvm/vmx/common.h              |   16 +
 arch/x86/kvm/vmx/main.c                |   50 +
 arch/x86/kvm/vmx/nested.c              | 1677 +++++++++++++++++++++
 arch/x86/kvm/vmx/nested.h              |  124 ++
 arch/x86/kvm/vmx/pmu_intel.c           |   26 +
 arch/x86/kvm/vmx/tdx.c                 |   29 +
 arch/x86/kvm/vmx/vmx.c                 |  864 +++++++++++
 arch/x86/kvm/vmx/vmx.h                 |  175 +++
 arch/x86/kvm/vmx/vmx_ops.h             |   17 +
 arch/x86/kvm/x86.c                     | 1082 ++++++++++++++
 arch/x86/kvm/x86.h                     |   63 +
 arch/x86/kvm/xen.c                     |   10 +
 drivers/net/virtio_net.c               |   15 +
 drivers/target/target_core_device.c    |   13 +
 drivers/target/target_core_sbc.c       |   14 +
 drivers/target/target_core_transport.c |    8 +
 drivers/virtio/virtio_balloon.c        |   51 +
 include/linux/kvm_host.h               |   48 +
 include/uapi/linux/kvm.h               |    7 +
 kernel/entry/kvm.c                     |   11 +
 virt/kvm/kvm_main.c                    |  103 ++
 45 files changed, 8420 insertions(+)

diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 07ba4935e..23fbbee85 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -511,6 +511,11 @@ static inline bool is_vector_pending(unsigned int vector)
  * 16 bytes aligned. The status of each vector is kept in a single
  * bit.
  */
+/*
+ * 在以下使用apic_find_highest_vector():
+ *   - arch/x86/kvm/lapic.c|814| <<apic_search_irr>> return apic_find_highest_vector(apic->regs + APIC_IRR);
+ *   - arch/x86/kvm/lapic.c|1010| <<apic_find_highest_isr>> result = apic_find_highest_vector(apic->regs + APIC_ISR);
+ */
 static inline int apic_find_highest_vector(void *bitmap)
 {
 	int vec;
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index f19a76d3c..e44419591 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -783,6 +783,15 @@ struct kvm_vcpu_arch {
 	 */
 	unsigned long regs[NR_VCPU_REGS];
 	u32 regs_avail;
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	u32 regs_dirty;
 
 	unsigned long cr0;
@@ -798,16 +807,74 @@ struct kvm_vcpu_arch {
 	u32 hflags;
 	u64 efer;
 	u64 host_debugctl;
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	u64 apic_base;
 	struct kvm_lapic *apic;    /* kernel irqchip context */
 	bool load_eoi_exitmap_pending;
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 */
 	DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	unsigned long apic_attention;
 	int32_t apic_arb_prio;
 	int mp_state;
 	u64 ia32_misc_enable_msr;
 	u64 smbase;
 	u64 smi_count;
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	bool at_instruction_boundary;
 	bool tpr_access_reporting;
 	bool xfd_no_write_intercept;
@@ -1042,6 +1109,15 @@ struct kvm_vcpu_arch {
 	} pv;
 
 	int pending_ioapic_eoi;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	int pending_external_vector;
 	int highest_stale_pending_ioapic_eoi;
 
@@ -1395,14 +1471,73 @@ struct kvm_arch {
 	struct kvm_pit *vpit;
 #endif
 	atomic_t vapics_in_nmi_mode;
+	/*
+	 * 在以下使用kvm_arch->apic_map_lock:
+	 *   - arch/x86/kvm/lapic.c|412| <<kvm_recalculate_apic_map>> mutex_lock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|425| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|500| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/x86.c|13686| <<kvm_arch_init_vm>> mutex_init(&kvm->arch.apic_map_lock);
+	 */
 	struct mutex apic_map_lock;
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	struct kvm_apic_map __rcu *apic_map;
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_t apic_map_dirty;
 
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 */
 	bool apic_access_memslot_enabled;
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_inhibited:
+	 *   - arch/x86/kvm/lapic.c|2668| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *                    kvm->arch.apic_access_memslot_inhibited)
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_inhibited = true;
+	 */
 	bool apic_access_memslot_inhibited;
 
 	/* Protects apicv_inhibit_reasons */
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	struct rw_semaphore apicv_update_lock;
 	unsigned long apicv_inhibit_reasons;
 
@@ -1420,6 +1555,13 @@ struct kvm_arch {
 	u64 last_tsc_nsec;
 	u64 last_tsc_write;
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2686| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|5802| <<kvm_arch_tsc_set_attr>> matched = (vcpu->arch.virtual_tsc_khz &&
+	 *                                   kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *                                   kvm->arch.last_tsc_offset == offset)
+	 */
 	u64 last_tsc_offset;
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
@@ -1452,11 +1594,28 @@ struct kvm_arch {
 
 	u64 disabled_quirks;
 
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	enum kvm_irqchip_mode irqchip_mode;
 	u8 nr_reserved_ioapic_pins;
 
 	bool disabled_lapic_found;
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	bool x2apic_format;
 	bool x2apic_broadcast_quirk_disabled;
 
@@ -1701,6 +1860,12 @@ struct kvm_x86_ops {
 
 	void (*hardware_unsetup)(void);
 	bool (*has_emulated_msr)(struct kvm *kvm, u32 index);
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+	 *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+	 *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+	 *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+	 */
 	void (*vcpu_after_set_cpuid)(struct kvm_vcpu *vcpu);
 
 	unsigned int vm_size;
@@ -1712,8 +1877,20 @@ struct kvm_x86_ops {
 	int (*vcpu_precreate)(struct kvm *kvm);
 	int (*vcpu_create)(struct kvm_vcpu *vcpu);
 	void (*vcpu_free)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_reset:
+	 *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+	 *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+	 *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+	 */
 	void (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);
 
+	/*
+	 * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+	 *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+	 *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+	 *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+	 */
 	void (*prepare_switch_to_guest)(struct kvm_vcpu *vcpu);
 	void (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
@@ -1777,6 +1954,12 @@ struct kvm_x86_ops {
 	int (*vcpu_pre_run)(struct kvm_vcpu *vcpu);
 	enum exit_fastpath_completion (*vcpu_run)(struct kvm_vcpu *vcpu,
 						  u64 run_flags);
+	/*
+	 * 在以下使用kvm_x86_ops->handle_exit:
+	 *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+	 *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+	 *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+	 */
 	int (*handle_exit)(struct kvm_vcpu *vcpu,
 		enum exit_fastpath_completion exit_fastpath);
 	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);
@@ -1801,6 +1984,13 @@ struct kvm_x86_ops {
 	 */
 	bool (*set_vnmi_pending)(struct kvm_vcpu *vcpu);
 	void (*enable_nmi_window)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_ops->enable_irq_window:
+	 *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+	 *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+	 *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+	 *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+	 */
 	void (*enable_irq_window)(struct kvm_vcpu *vcpu);
 	void (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);
 
@@ -1809,7 +1999,19 @@ struct kvm_x86_ops {
 	bool allow_apicv_in_x2apic_without_x2apic_virtualization;
 	void (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);
 	void (*hwapic_isr_update)(struct kvm_vcpu *vcpu, int isr);
+	/*
+	 * 在以下使用kvm_x86_ops->load_eoi_exitmap:
+	 *   - arch/x86/kvm/vmx/main.c|976| <<global>> .load_eoi_exitmap = vt_op(load_eoi_exitmap),
+	 *   - arch/x86/kvm/x86.c|11144| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+	 *   - arch/x86/kvm/x86.c|11165| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(
+	 */
 	void (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
+	/*
+	 * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+	 *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+	 *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+	 *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+	 */
 	void (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);
 	void (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu);
 	void (*deliver_interrupt)(struct kvm_lapic *apic, int delivery_mode,
@@ -1855,6 +2057,12 @@ struct kvm_x86_ops {
 	void (*get_entry_info)(struct kvm_vcpu *vcpu,
 			       u32 *intr_info, u32 *error_code);
 
+	/*
+	 * 在以下使用kvm_x86_ops->check_intercept:
+	 *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+	 *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+	 *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+	 */
 	int (*check_intercept)(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage,
@@ -1916,6 +2124,12 @@ struct kvm_x86_ops {
 	/*
 	 * Returns vCPU specific APICv inhibit reasons
 	 */
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_get_apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/svm/svm.c|5354| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+	 *   - arch/x86/kvm/svm/svm.c|5594| <<svm_hardware_setup>> svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
+	 *   - arch/x86/kvm/x86.c|10158| <<kvm_vcpu_apicv_activated>> kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
+	 */
 	unsigned long (*vcpu_get_apicv_inhibit_reasons)(struct kvm_vcpu *vcpu);
 
 	gva_t (*get_untagged_addr)(struct kvm_vcpu *vcpu, gva_t gva, unsigned int flags);
@@ -1926,19 +2140,59 @@ struct kvm_x86_ops {
 };
 
 struct kvm_x86_nested_ops {
+	/*
+	 * 在以下使用kvm_x86_nested_ops->leave_nested:
+	 *   - arch/x86/kvm/svm/nested.c|2093| <<global>> .leave_nested = svm_leave_nested,
+	 *   - arch/x86/kvm/vmx/nested.c|8987| <<global>> .leave_nested = vmx_leave_nested,
+	 *   - arch/x86/kvm/x86.h|143| <<kvm_leave_nested>> kvm_x86_ops.nested_ops->leave_nested(vcpu);
+	 */
 	void (*leave_nested)(struct kvm_vcpu *vcpu);
 	bool (*is_exception_vmexit)(struct kvm_vcpu *vcpu, u8 vector,
 				    u32 error_code);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->check_events:
+	 *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+	 *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+	 *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+	 */
 	int (*check_events)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->has_events:
+	 *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+	 *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+	 *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+	 */
 	bool (*has_events)(struct kvm_vcpu *vcpu, bool for_injection);
 	void (*triple_fault)(struct kvm_vcpu *vcpu);
 	int (*get_state)(struct kvm_vcpu *vcpu,
 			 struct kvm_nested_state __user *user_kvm_nested_state,
 			 unsigned user_data_size);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->set_state:
+	 *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+	 *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+	 *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+	 *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+	 *					user_kvm_nested_state, &kvm_state);
+	 */
 	int (*set_state)(struct kvm_vcpu *vcpu,
 			 struct kvm_nested_state __user *user_kvm_nested_state,
 			 struct kvm_nested_state *kvm_state);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->get_nested_state_pages:
+	 *   - arch/x86/kvm/svm/nested.c|2230| <<global>> .get_nested_state_pages = svm_get_nested_state_pages,
+	 *   - arch/x86/kvm/vmx/nested.c|8994| <<global>> .get_nested_state_pages = vmx_get_nested_state_pages,
+	 *   - arch/x86/kvm/x86.c|11305| <<vcpu_enter_guest(KVM_REQ_GET_NESTED_STATE_PAGES)>>
+	 *                             if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
+	 */
 	bool (*get_nested_state_pages)(struct kvm_vcpu *vcpu);
+	/*
+	 * 在以下使用kvm_x86_nested_ops->write_log_dirty:
+	 *   - arch/x86/kvm/vmx/nested.c|8462| <<global>> .write_log_dirty = nested_vmx_write_pml_buffer,
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|225| <<FNAME(update_accessed_dirty_bits)>> if (kvm_x86_ops.nested_ops->write_log_dirty(vcpu, addr))
+	 */
 	int (*write_log_dirty)(struct kvm_vcpu *vcpu, gpa_t l2_gpa);
 
 	int (*enable_evmcs)(struct kvm_vcpu *vcpu,
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 00b7e0398..ee2154712 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -24,6 +24,10 @@ extern void x86_virt_spec_ctrl(u64 guest_virt_spec_ctrl, bool guest);
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * 在以下使用x86_spec_ctrl_set_guest():
+ *   - arch/x86/kvm/svm/svm.c|4491| <<svm_vcpu_run>> x86_spec_ctrl_set_guest(svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_virt_spec_ctrl)
 {
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index cca7d6641..443b998ea 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -492,6 +492,15 @@ enum vmcs_field {
 #define VMX_AR_RESERVD_MASK 0xfffe0f00
 
 #define TSS_PRIVATE_MEMSLOT			(KVM_USER_MEM_SLOTS + 0)
+/*
+ * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+ *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+ *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+ *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+ *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ */
 #define APIC_ACCESS_PAGE_PRIVATE_MEMSLOT	(KVM_USER_MEM_SLOTS + 1)
 #define IDENTITY_PAGETABLE_PRIVATE_MEMSLOT	(KVM_USER_MEM_SLOTS + 2)
 
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 8ae750cde..5c7270a88 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -344,6 +344,9 @@ static notrace __maybe_unused void kvm_guest_apic_eoi_write(void)
 	 * there's no need for lock or memory barriers.
 	 * An optimization barrier is implied in apic write.
 	 */
+	/*
+	 * 如果还有
+	 */
 	if (__test_and_clear_bit(KVM_PV_EOI_BIT, this_cpu_ptr(&kvm_apic_eoi)))
 		return;
 	apic_native_eoi();
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index e2836a255..20195b7aa 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -283,6 +283,27 @@ static void kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)
 		kvm_update_feature_runtime(vcpu, best, X86_FEATURE_OSXSAVE,
 					   kvm_is_cr4_bit_set(vcpu, X86_CR4_OSXSAVE));
 
+		/*
+		 * 在以下设置kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+		 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+		 * 在以下使用kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+		 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+		 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+		 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+		 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+		 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+		 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+		 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+		 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+		 */
 		kvm_update_feature_runtime(vcpu, best, X86_FEATURE_APIC,
 					   vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
 
@@ -357,6 +378,11 @@ static u32 cpuid_get_reg_unsafe(struct kvm_cpuid_entry2 *entry, u32 reg)
 static int cpuid_func_emulated(struct kvm_cpuid_entry2 *entry, u32 func,
 			       bool include_partially_emulated);
 
+/*
+ * 在以下使用kvm_vcpu_after_set_cpuid():
+ *   - arch/x86/kvm/cpuid.c|568| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ *   - arch/x86/kvm/x86.c|13087| <<kvm_arch_vcpu_create>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -440,6 +466,12 @@ void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 
 	kvm_hv_set_cpuid(vcpu, kvm_cpuid_has_hyperv(vcpu));
 
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+	 *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+	 *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+	 *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+	 */
 	/* Invoke the vendor callback only after the above state is updated. */
 	kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
 
@@ -488,6 +520,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * 在以下使用kvm_set_cpuid():
+ *   - arch/x86/kvm/cpuid.c|617| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|643| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 72b19a88a..712bbc616 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -134,6 +134,19 @@ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&vcpu->kvm->arch.apicv_update_lock);
 
 	if (auto_eoi_new)
@@ -492,6 +505,16 @@ static int synic_set_irq(struct kvm_vcpu_hv_synic *synic, u32 sint)
 	irq.vector = vector;
 	irq.level = 1;
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
 	trace_kvm_hv_synic_set_irq(vcpu->vcpu_id, sint, irq.vector, ret);
 	return ret;
@@ -841,6 +864,19 @@ static int stimer_notify_direct(struct kvm_vcpu_hv_stimer *stimer)
 		.vector = stimer->config.apic_vector
 	};
 
+	/*
+	 * 在以下使用kvm_apic_set_irq():
+	 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *
+	 * 调用__apic_accept_irq()
+	 */
 	if (lapic_in_kernel(vcpu))
 		return !kvm_apic_set_irq(vcpu, &irq, NULL);
 	return 0;
@@ -1556,6 +1592,13 @@ static int kvm_hv_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data, bool host)
 
 		if (!(data & HV_X64_MSR_VP_ASSIST_PAGE_ENABLE)) {
 			hv_vcpu->hv_vapic = data;
+			/*
+			 * 在以下使用kvm_lapic_set_pv_eoi():
+			 *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+			 *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+			 *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+			 *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+			 */
 			if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
 				return 1;
 			break;
@@ -1574,6 +1617,13 @@ static int kvm_hv_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data, bool host)
 			return 1;
 		hv_vcpu->hv_vapic = data;
 		kvm_vcpu_mark_page_dirty(vcpu, gfn);
+		/*
+		 * 在以下使用kvm_lapic_set_pv_eoi():
+		 *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+		 *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+		 *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+		 *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+		 */
 		if (kvm_lapic_set_pv_eoi(vcpu,
 					    gfn_to_gpa(gfn) | KVM_MSR_ENABLED,
 					    sizeof(struct hv_vp_assist_page)))
@@ -2216,6 +2266,19 @@ static void kvm_hv_send_ipi_to_many(struct kvm *kvm, u32 vector,
 					    valid_bank_mask, sparse_banks))
 			continue;
 
+		/*
+		 * 在以下使用kvm_apic_set_irq():
+		 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *
+		 * 调用__apic_accept_irq()
+		 */
 		/* We fail only when APIC is disabled */
 		kvm_apic_set_irq(vcpu, &irq, NULL);
 	}
diff --git a/arch/x86/kvm/i8259.c b/arch/x86/kvm/i8259.c
index 2ac7f1678..59db46ea0 100644
--- a/arch/x86/kvm/i8259.c
+++ b/arch/x86/kvm/i8259.c
@@ -211,6 +211,11 @@ int kvm_pic_set_irq(struct kvm_kernel_irq_routing_entry *e, struct kvm *kvm,
 /*
  * acknowledge interrupt 'irq'
  */
+/*
+ * 在以下使用pic_intack():
+ *   - arch/x86/kvm/i8259.c|241| <<kvm_pic_read_irq>> pic_intack(&s->pics[0], irq);
+ *   - arch/x86/kvm/i8259.c|245| <<kvm_pic_read_irq>> pic_intack(&s->pics[1], irq2);
+ */
 static inline void pic_intack(struct kvm_kpic_state *s, int irq)
 {
 	s->isr |= 1 << irq;
@@ -582,6 +587,10 @@ static const struct kvm_io_device_ops picdev_elcr_ops = {
 	.write    = picdev_elcr_write,
 };
 
+/*
+ * 在以下使用kvm_pic_init():
+ *   - arch/x86/kvm/x86.c|7022| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> r = kvm_pic_init(kvm);
+ */
 int kvm_pic_init(struct kvm *kvm)
 {
 	struct kvm_pic *s;
diff --git a/arch/x86/kvm/ioapic.c b/arch/x86/kvm/ioapic.c
index 2b5d389bc..e34e3abc2 100644
--- a/arch/x86/kvm/ioapic.c
+++ b/arch/x86/kvm/ioapic.c
@@ -114,6 +114,24 @@ static void __rtc_irq_eoi_tracking_restore_one(struct kvm_vcpu *vcpu)
 	union kvm_ioapic_redirect_entry *e;
 
 	e = &ioapic->redirtbl[RTC_GSI];
+	/*
+	 * 在以下使用kvm_apic_match_dest():
+	 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+	 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+	 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+	 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+	 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+	 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+	 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+	 */
 	if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 				 e->fields.dest_id,
 				 kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
@@ -188,6 +206,24 @@ static void ioapic_lazy_update_eoi(struct kvm_ioapic *ioapic, int irq)
 	union kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];
 
 	kvm_for_each_vcpu(i, vcpu, ioapic->kvm) {
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 					 entry->fields.dest_id,
 					 entry->fields.dest_mode) ||
@@ -307,6 +343,14 @@ void kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm)
 {
 	if (!ioapic_in_kernel(kvm))
 		return;
+	/*
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 }
 
@@ -466,6 +510,14 @@ static void ioapic_write_indirect(struct kvm_ioapic *ioapic, u32 val)
 			kvm_make_scan_ioapic_request_mask(ioapic->kvm,
 							  vcpu_bitmap);
 		} else {
+			/*
+			 * 在以下使用kvm_make_scan_ioapic_request():
+			 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+			 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+			 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+			 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+			 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+			 */
 			kvm_make_scan_ioapic_request(ioapic->kvm);
 		}
 		break;
@@ -503,6 +555,16 @@ static int ioapic_service(struct kvm_ioapic *ioapic, int irq, bool line_status)
 		 * if rtc_irq_check_coalesced returns false).
 		 */
 		BUG_ON(ioapic->rtc_status.pending_eoi != 0);
+		/*
+		 * 在以下使用kvm_irq_delivery_to_apic():
+		 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+		 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+		 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+		 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+		 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+		 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+		 */
 		ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
 					       &ioapic->rtc_status.dest_map);
 		ioapic->rtc_status.pending_eoi = (ret < 0 ? 0 : ret);
@@ -796,6 +858,14 @@ void kvm_set_ioapic(struct kvm *kvm, struct kvm_ioapic_state *state)
 	memcpy(ioapic, state, sizeof(struct kvm_ioapic_state));
 	ioapic->irr = 0;
 	ioapic->irr_delivered = 0;
+	/*
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 	kvm_ioapic_inject_all(ioapic, state->irr);
 	spin_unlock(&ioapic->lock);
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 16da89259..1a169c1ae 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -24,6 +24,11 @@
  * check if there are pending timer events
  * to be processed.
  */
+/*
+ * 在以下使用kvm_cpu_has_pending_timer():
+ *   - arch/x86/kvm/x86.c|11346| <<vcpu_run>> if (kvm_cpu_has_pending_timer(vcpu))
+ *   - virt/kvm/kvm_main.c|3621| <<kvm_vcpu_check_block>> if (kvm_cpu_has_pending_timer(vcpu))
+ */
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	int r = 0;
@@ -39,13 +44,39 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 /*
  * check if there is a pending userspace external interrupt
  */
+/*
+ * 在以下使用pending_userspace_extint():
+ *   - arch/x86/kvm/irq.c|110| <<kvm_cpu_has_extint>> return pending_userspace_extint(v);
+ */
 static int pending_userspace_extint(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	return v->arch.pending_external_vector != -1;
 }
 
+/*
+ * 在以下使用get_userspace_extint():
+ *   - arch/x86/kvm/irq.c|172| <<kvm_cpu_get_extint>> return get_userspace_extint(v);
+ */
 static int get_userspace_extint(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	int vector = vcpu->arch.pending_external_vector;
 
 	vcpu->arch.pending_external_vector = -1;
@@ -56,6 +87,13 @@ static int get_userspace_extint(struct kvm_vcpu *vcpu)
  * check if there is pending interrupt from
  * non-APIC source without intack.
  */
+/*
+ * 在以下使用kvm_cpu_has_extint():
+ *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+ *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+ *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+ *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+ */
 int kvm_cpu_has_extint(struct kvm_vcpu *v)
 {
 	/*
@@ -79,10 +117,35 @@ int kvm_cpu_has_extint(struct kvm_vcpu *v)
 		return 0;
 
 #ifdef CONFIG_KVM_IOAPIC
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 *
+	 * struct kvm_arch:
+	 * -> struct kvm_pic *vpic;
+	 *    -> int output;
+	 */
 	if (pic_in_kernel(v->kvm))
 		return v->kvm->arch.vpic->output;
 #endif
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	WARN_ON_ONCE(!irqchip_split(v->kvm));
 	return pending_userspace_extint(v);
 }
@@ -93,14 +156,42 @@ int kvm_cpu_has_extint(struct kvm_vcpu *v)
  * interrupt from apic will handled by hardware,
  * we don't need to check it here.
  */
+/*
+ * 在以下使用kvm_cpu_has_injectable_intr():
+ *   - arch/x86/kvm/svm/svm.c|2327| <<svm_set_gif>> if (... kvm_cpu_has_injectable_intr(&svm->vcpu) ||
+ *   - arch/x86/kvm/vmx/nested.c|5116| <<__nested_vmx_vmexit>> if (kvm_cpu_has_injectable_intr(vcpu) || vcpu->arch.nmi_pending)
+ *   - arch/x86/kvm/x86.c|10465| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10479| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu))
+ */
 int kvm_cpu_has_injectable_intr(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 */
 	if (kvm_cpu_has_extint(v))
 		return 1;
 
 	if (!is_guest_mode(v) && kvm_vcpu_apicv_active(v))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_apic_has_interrupt():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+	 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+	 *
+	 * 核心思想:
+	 * 1. 判断kvm_apic_present()
+	 * 2. 根据TPR和ISR更新PPR
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 */
 	return kvm_apic_has_interrupt(v) != -1; /* LAPIC */
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_has_injectable_intr);
@@ -109,14 +200,63 @@ EXPORT_SYMBOL_GPL(kvm_cpu_has_injectable_intr);
  * check if there is pending interrupt without
  * intack.
  */
+/*
+ * 在以下使用kvm_cpu_has_interrupt():
+ *   - arch/x86/kvm/svm/nested.c|1594| <<svm_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !svm_interrupt_blocked(vcpu)) {
+ *   - arch/x86/kvm/vmx/nested.c|4348| <<vmx_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !vmx_interrupt_blocked(vcpu)) {
+ *   - arch/x86/kvm/x86.c|11338| <<kvm_vcpu_has_events>> if (kvm_arch_interrupt_allowed(vcpu) && kvm_cpu_has_interrupt(vcpu))
+ *
+ * 先判断有没有PIC或者userspace的interrupt
+ * 然后:
+ *     1. 判断kvm_apic_present()
+ *     2. 根据TPR和ISR更新PPR
+ *        TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ *     3. 选出符合当前PPR的最高的irr, 没有就返回-1
+ *
+ * check if there is pending interrupt without
+ * intack.
+ */
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 */
 	if (kvm_cpu_has_extint(v))
 		return 1;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
 		return kvm_x86_call(protected_apic_has_interrupt)(v);
 
+	/*
+	 * 在以下使用kvm_apic_has_interrupt():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+	 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+	 *
+	 * 核心思想:
+	 * 1. 判断kvm_apic_present()
+	 * 2. 根据TPR和ISR更新PPR
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 */
 	return kvm_apic_has_interrupt(v) != -1;	/* LAPIC */
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_has_interrupt);
@@ -125,8 +265,24 @@ EXPORT_SYMBOL_GPL(kvm_cpu_has_interrupt);
  * Read pending interrupt(from non-APIC source)
  * vector and intack.
  */
+/*
+ * 在以下使用kvm_cpu_get_extint():
+ *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_get_interrupt>> int vector = kvm_cpu_get_extint(v);
+ *   - arch/x86/kvm/vmx/nested.c|4366| <<vmx_check_nested_events>> irq = kvm_cpu_get_extint(vcpu);
+ *
+ * 注释:
+ * Read pending interrupt(from non-APIC source)
+ * vector and intack.
+ */
 int kvm_cpu_get_extint(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 */
 	if (!kvm_cpu_has_extint(v)) {
 		WARN_ON(!lapic_in_kernel(v));
 		return -1;
@@ -141,10 +297,31 @@ int kvm_cpu_get_extint(struct kvm_vcpu *v)
 #endif
 
 #ifdef CONFIG_KVM_IOAPIC
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 */
 	if (pic_in_kernel(v->kvm))
 		return kvm_pic_read_irq(v->kvm); /* PIC */
 #endif
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	WARN_ON_ONCE(!irqchip_split(v->kvm));
 	return get_userspace_extint(v);
 }
@@ -153,13 +330,47 @@ EXPORT_SYMBOL_GPL(kvm_cpu_get_extint);
 /*
  * Read pending interrupt vector and intack.
  */
+/*
+ * 在以下使用kvm_cpu_get_interrupt():
+ *   - arch/x86/kvm/x86.c|10521| <<kvm_check_and_inject_events>> int irq = kvm_cpu_get_interrupt(vcpu);
+ *
+ * Read pending interrupt vector and intack.
+ */
 int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 {
+	/*
+	 * 在以下使用kvm_cpu_get_extint():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_get_interrupt>> int vector = kvm_cpu_get_extint(v);
+	 *   - arch/x86/kvm/vmx/nested.c|4366| <<vmx_check_nested_events>> irq = kvm_cpu_get_extint(vcpu);
+	 *
+	 * 注释:
+	 * Read pending interrupt(from non-APIC source)
+	 * vector and intack.
+	 */
 	int vector = kvm_cpu_get_extint(v);
 	if (vector != -1)
 		return vector;			/* PIC */
 
+	/*
+	 * 在以下使用kvm_apic_has_interrupt():
+	 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+	 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+	 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+	 *
+	 * 核心思想:
+	 * 1. 判断kvm_apic_present()
+	 * 2. 根据TPR和ISR更新PPR
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 */
 	vector = kvm_apic_has_interrupt(v);	/* APIC */
+	/*
+	 * 在以下使用kvm_apic_ack_interrupt():
+	 *   - arch/x86/kvm/irq.c|187| <<kvm_cpu_get_interrupt>> kvm_apic_ack_interrupt(v, vector);
+	 *   - arch/x86/kvm/vmx/nested.c|4413| <<vmx_check_nested_events>> kvm_apic_ack_interrupt(vcpu, irq);
+	 */
 	if (vector != -1)
 		kvm_apic_ack_interrupt(v, vector);
 
@@ -187,6 +398,13 @@ bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	bool resample = args->flags & KVM_IRQFD_FLAG_RESAMPLE;
 
+	/*
+	 * 在以下使用irqchip_full():
+	 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+	 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+	 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 */
 	return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
 }
 
@@ -195,6 +413,16 @@ bool kvm_arch_irqchip_in_kernel(struct kvm *kvm)
 	return irqchip_in_kernel(kvm);
 }
 
+/*
+ * 在以下使用kvm_irq_delivery_to_apic():
+ *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+ *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 			     struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -203,9 +431,31 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	unsigned long i, dest_vcpu_bitmap[BITS_TO_LONGS(KVM_MAX_VCPUS)];
 	unsigned int dest_vcpus = 0;
 
+	/*
+	 * struct kvm_lapic_irq {
+	 *     u32 vector;
+	 *     u16 delivery_mode;
+	 *     u16 dest_mode;
+	 *     bool level;
+	 *     u16 trig_mode;
+	 *     u32 shorthand;
+	 *     u32 dest_id;          
+	 *     bool msi_redir_hint;
+	 * };
+	 *
+	 * 在以下使用kvm_irq_delivery_to_apic_fast():
+	 *   - arch/x86/kvm/irq.c|424| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+	 */
 	if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
 		return r;
 
+	/*
+	 * 在以下使用kvm_lowest_prio_delivery():
+	 *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+	 */
 	if (irq->dest_mode == APIC_DEST_PHYSICAL &&
 	    irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
 		pr_info("apic: phys broadcast and lowest prio\n");
@@ -218,13 +468,50 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		if (!kvm_apic_present(vcpu))
 			continue;
 
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (!kvm_apic_match_dest(vcpu, src, irq->shorthand,
 					irq->dest_id, irq->dest_mode))
 			continue;
 
+		/*
+		 * 在以下使用kvm_lowest_prio_delivery():
+		 *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+		 *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+		 *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+		 */
 		if (!kvm_lowest_prio_delivery(irq)) {
 			if (r < 0)
 				r = 0;
+			/*
+			 * 在以下使用kvm_apic_set_irq():
+			 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+			 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *
+			 * 调用__apic_accept_irq()
+			 */
 			r += kvm_apic_set_irq(vcpu, irq, dest_map);
 		} else if (kvm_apic_sw_enabled(vcpu->arch.apic)) {
 			if (!kvm_vector_hashing_enabled()) {
@@ -246,12 +533,32 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		lowest = kvm_get_vcpu(kvm, idx);
 	}
 
+	/*
+	 * 在以下使用kvm_apic_set_irq():
+	 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+	 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+	 *
+	 * 调用__apic_accept_irq()
+	 */
 	if (lowest)
 		r = kvm_apic_set_irq(lowest, irq, dest_map);
 
 	return r;
 }
 
+/*
+ * 在以下使用kvm_msi_to_lapic_irq():
+ *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+ *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+ *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+ */
 static void kvm_msi_to_lapic_irq(struct kvm *kvm,
 				 struct kvm_kernel_irq_routing_entry *e,
 				 struct kvm_lapic_irq *irq)
@@ -263,6 +570,15 @@ static void kvm_msi_to_lapic_irq(struct kvm *kvm,
 	trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
 			      (u64)msg.address_hi << 32 : 0), msg.data);
 
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
 	irq->vector = msg.arch_data.vector;
 	irq->dest_mode = kvm_lapic_irq_dest_mode(msg.arch_addr_lo.dest_mode_logical);
@@ -276,12 +592,33 @@ static void kvm_msi_to_lapic_irq(struct kvm *kvm,
 static inline bool kvm_msi_route_invalid(struct kvm *kvm,
 		struct kvm_kernel_irq_routing_entry *e)
 {
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
 }
 
 int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 		struct kvm *kvm, int irq_source_id, int level, bool line_status)
 {
+	/*
+	 * struct kvm_lapic_irq {
+	 *     u32 vector;
+	 *     u16 delivery_mode;
+	 *     u16 dest_mode;
+	 *     bool level;
+	 *     u16 trig_mode;
+	 *     u32 shorthand;
+	 *     u32 dest_id;
+	 *     bool msi_redir_hint;
+	 * };
+	 */
 	struct kvm_lapic_irq irq;
 
 	if (kvm_msi_route_invalid(kvm, e))
@@ -290,8 +627,25 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 	if (!level)
 		return -1;
 
+	/*
+	 * 在以下使用kvm_msi_to_lapic_irq():
+	 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+	 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+	 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+	 */
 	kvm_msi_to_lapic_irq(kvm, e, &irq);
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
 }
 
@@ -313,8 +667,20 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (kvm_msi_route_invalid(kvm, e))
 			return -EINVAL;
 
+		/*
+		 * 在以下使用kvm_msi_to_lapic_irq():
+		 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+		 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+		 */
 		kvm_msi_to_lapic_irq(kvm, e, &irq);
 
+		/*
+		 * 在以下使用kvm_irq_delivery_to_apic_fast():
+		 *   - arch/x86/kvm/irq.c|424| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+		 *   - arch/x86/kvm/irq.c|554| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+		 */
 		if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
 			return r;
 		break;
@@ -361,6 +727,16 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	switch (ue->type) {
 #ifdef CONFIG_KVM_IOAPIC
 	case KVM_IRQ_ROUTING_IRQCHIP:
+		/*
+		 * 在以下使用irqchip_split():
+		 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+		 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+		 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+		 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+		 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+		 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+		 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+		 */
 		if (irqchip_split(kvm))
 			return -EINVAL;
 		e->irqchip.pin = ue->u.irqchip.pin;
@@ -411,6 +787,10 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_intr_is_single_vcpu():
+ *   - arch/x86/kvm/irq.c|801| <<kvm_pi_update_irte>> if (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||
+ */
 bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			     struct kvm_vcpu **dest_vcpu)
 {
@@ -418,6 +798,9 @@ bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 	unsigned long i;
 	struct kvm_vcpu *vcpu;
 
+	/*
+	 * 只在这里调用
+	 */
 	if (kvm_intr_is_single_vcpu_fast(kvm, irq, dest_vcpu))
 		return true;
 
@@ -425,6 +808,24 @@ bool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,
 		if (!kvm_apic_present(vcpu))
 			continue;
 
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (!kvm_apic_match_dest(vcpu, NULL, irq->shorthand,
 					irq->dest_id, irq->dest_mode))
 			continue;
@@ -442,6 +843,24 @@ EXPORT_SYMBOL_GPL(kvm_intr_is_single_vcpu);
 void kvm_scan_ioapic_irq(struct kvm_vcpu *vcpu, u32 dest_id, u16 dest_mode,
 			 u8 vector, unsigned long *ioapic_handled_vectors)
 {
+	/*
+	 * 在以下使用kvm_apic_match_dest():
+	 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+	 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+	 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+	 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+	 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+	 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+	 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+	 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+	 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+	 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+	 */
 	/*
 	 * Intercept EOI if the vCPU is the target of the new IRQ routing, or
 	 * the vCPU has a pending IRQ from the old routing, i.e. if the vCPU
@@ -486,6 +905,13 @@ void kvm_scan_ioapic_routes(struct kvm_vcpu *vcpu,
 			if (entry->type != KVM_IRQ_ROUTING_MSI)
 				continue;
 
+			/*
+			 * 在以下使用kvm_msi_to_lapic_irq():
+			 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+			 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+			 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+			 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+			 */
 			kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
 
 			if (!irq.trig_mode)
@@ -504,6 +930,23 @@ void kvm_arch_irq_routing_update(struct kvm *kvm)
 	kvm_hv_irq_routing_update(kvm);
 #endif
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 *
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	if (irqchip_split(kvm))
 		kvm_make_scan_ioapic_request(kvm);
 }
@@ -521,6 +964,13 @@ static int kvm_pi_update_irte(struct kvm_kernel_irqfd *irqfd,
 		return -EINVAL;
 
 	if (entry && entry->type == KVM_IRQ_ROUTING_MSI) {
+		/*
+		 * 在以下使用kvm_msi_to_lapic_irq():
+		 *   - arch/x86/kvm/irq.c|552| <<kvm_set_msi>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|575| <<kvm_arch_set_irq_inatomic>> kvm_msi_to_lapic_irq(kvm, e, &irq);
+		 *   - arch/x86/kvm/irq.c|806| <<kvm_scan_ioapic_routes>> kvm_msi_to_lapic_irq(vcpu->kvm, entry, &irq);
+		 *   - arch/x86/kvm/irq.c|858| <<kvm_pi_update_irte>> kvm_msi_to_lapic_irq(kvm, entry, &irq);
+		 */
 		kvm_msi_to_lapic_irq(kvm, entry, &irq);
 
 		/*
diff --git a/arch/x86/kvm/irq.h b/arch/x86/kvm/irq.h
index 5e62c1f79..c0813335a 100644
--- a/arch/x86/kvm/irq.h
+++ b/arch/x86/kvm/irq.h
@@ -73,8 +73,23 @@ int kvm_setup_default_ioapic_and_pic_routing(struct kvm *kvm);
 int kvm_vm_ioctl_get_irqchip(struct kvm *kvm, struct kvm_irqchip *chip);
 int kvm_vm_ioctl_set_irqchip(struct kvm *kvm, struct kvm_irqchip *chip);
 
+/*
+ * 在以下使用irqchip_full():
+ *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+ *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+ *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+ */
 static inline int irqchip_full(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
@@ -88,14 +103,50 @@ static __always_inline int irqchip_full(struct kvm *kvm)
 }
 #endif
 
+/*
+ * 在以下使用pic_in_kernel():
+ *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+ *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+ *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+ *                                        likely(!pic_in_kernel(vcpu->kvm));
+ *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+ *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+ */
 static inline int pic_in_kernel(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用irqchip_full():
+	 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+	 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+	 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+	 */
 	return irqchip_full(kvm);
 }
 
 
+/*
+ * 在以下使用irqchip_split():
+ *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+ *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+ *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+ *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+ *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+ *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+ */
 static inline int irqchip_split(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
@@ -103,8 +154,35 @@ static inline int irqchip_split(struct kvm *kvm)
 	return mode == KVM_IRQCHIP_SPLIT;
 }
 
+/*
+ * 在以下使用irqchip_in_kernel():
+ *   - arch/x86/kvm/irq.c|220| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.c|225| <<kvm_arch_irqchip_in_kernel>> return irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.c|369| <<kvm_vm_ioctl_irq_line>> if (!irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/irq.c|380| <<kvm_arch_can_set_irq_routing>> return irqchip_in_kernel(kvm);
+ *   - arch/x86/kvm/irq.c|550| <<kvm_pi_update_irte>> if (WARN_ON_ONCE(!irqchip_in_kernel(kvm) || !kvm_arch_has_irq_bypass()))
+ *   - arch/x86/kvm/lapic.c|357| <<kvm_recalculate_apic_map>> WARN_ONCE(!irqchip_in_kernel(kvm),
+ *   - arch/x86/kvm/lapic.c|2956| <<kvm_create_lapic>> if (!irqchip_in_kernel(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/avic.c|714| <<avic_init_vcpu>> if (!enable_apicv || !irqchip_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/vmx/posted_intr.c|154| <<vmx_can_use_vtd_pi>> return irqchip_in_kernel(kvm) && kvm_arch_has_irq_bypass() &&
+ *   - arch/x86/kvm/vmx/vmx.c|4591| <<vmx_alloc_ipiv_pid_table>> if (!irqchip_in_kernel(kvm) || !enable_ipiv)
+ *   - arch/x86/kvm/x86.c|5252| <<kvm_vcpu_ioctl_interrupt>> if (!irqchip_in_kernel(vcpu->kvm)) {
+ *   - arch/x86/kvm/x86.c|5902| <<kvm_vcpu_ioctl_enable_cap>> if (!irqchip_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/x86.c|6493| <<kvm_vm_ioctl_enable_cap>> if (irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|6717| <<kvm_vm_ioctl_enable_cap>> if (!irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|7023| <<kvm_arch_vm_ioctl>> if (irqchip_in_kernel(kvm))
+ *   - arch/x86/kvm/x86.c|12423| <<kvm_arch_vcpu_create>> if (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu)
+ */
 static inline int irqchip_in_kernel(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index 36a8786db..5db4ae307 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -69,10 +69,26 @@ static inline bool kvm_register_is_available(struct kvm_vcpu *vcpu,
 	return test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
 }
 
+/*
+ * 在以下使用kvm_register_is_dirty():
+ *   - arch/x86/kvm/vmx/vmx.c|3237| <<vmx_ept_load_pdptrs>> if (!kvm_register_is_dirty(vcpu, VCPU_EXREG_PDPTR))
+ *   - arch/x86/kvm/vmx/vmx.c|3402| <<vmx_load_mmu_pgd>> else if (kvm_register_is_dirty(vcpu, VCPU_EXREG_CR3))
+ *   - arch/x86/kvm/vmx/vmx.c|7533| <<vmx_vcpu_run>> if (kvm_register_is_dirty(vcpu, VCPU_REGS_RSP))
+ *   - arch/x86/kvm/vmx/vmx.c|7535| <<vmx_vcpu_run>> if (kvm_register_is_dirty(vcpu, VCPU_REGS_RIP))
+ */
 static inline bool kvm_register_is_dirty(struct kvm_vcpu *vcpu,
 					 enum kvm_reg reg)
 {
 	kvm_assert_register_caching_allowed(vcpu);
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
 }
 
@@ -88,6 +104,15 @@ static inline void kvm_register_mark_dirty(struct kvm_vcpu *vcpu,
 {
 	kvm_assert_register_caching_allowed(vcpu);
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	__set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
 }
 
@@ -222,12 +247,24 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);
 }
 
+/*
+ * 在以下使用enter_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+ */
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
 	vcpu->stat.guest_mode = 1;
 }
 
+/*
+ * 在以下使用leave_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+ */
 static inline void leave_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags &= ~HF_GUEST_MASK;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 5fc437341..fc7f8e651 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -53,6 +53,28 @@
 #define mod_64(x, y) ((x) % (y))
 #endif
 
+/*
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ *
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ */
+
 /* 14 is the version for Xeon and Pentium 8.4.8*/
 #define APIC_VERSION			0x14UL
 #define LAPIC_MMIO_LENGTH		(1 << 12)
@@ -101,6 +123,15 @@ bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 		apic_test_vector(vector, apic->regs + APIC_IRR);
 }
 
+/*
+ * 在以下使用kvm_has_noapic_vcpu:
+ *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+ *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+ */
 __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 EXPORT_SYMBOL_GPL(kvm_has_noapic_vcpu);
 
@@ -142,11 +173,28 @@ static bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)
 	return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
 }
 
+/*
+ * 在以下使用kvm_apic_calc_x2apic_ldr():
+ *   - arch/x86/kvm/lapic.c|698| <<kvm_apic_set_x2apic_id>> u32 ldr = kvm_apic_calc_x2apic_ldr(id);
+ *   - arch/x86/kvm/lapic.c|4331| <<kvm_apic_state_fixup>> *ldr = kvm_apic_calc_x2apic_ldr(x2apic_id);
+ *
+ * 在x2apic下.
+ * LDR[31:16] = Cluster ID
+ * LDR[15:0]  = Logical ID Mask
+ *
+ * kvm_apic_calc_x2apic_ldr()根据给定的x2APIC ID,
+ * 计算出该处理器应有的 LDR 值.
+ */
 static inline u32 kvm_apic_calc_x2apic_ldr(u32 id)
 {
 	return ((id >> 4) << 16) | (1 << (id & 0xf));
 }
 
+/*
+ * 在以下使用kvm_apic_map_get_logical_dest():
+ *   - arch/x86/kvm/lapic.c|347| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
+ *   - arch/x86/kvm/lapic.c|1785| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+ */
 static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 		u32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {
 	switch (map->logical_mode) {
@@ -187,12 +235,19 @@ static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 	}
 }
 
+/*
+ * 在以下使用kvm_recalculate_phys_map():
+ *   - arch/x86/kvm/lapic.c|496| <<kvm_recalculate_apic_map>> r = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);
+ */
 static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 				    struct kvm_vcpu *vcpu,
 				    bool *xapic_id_mismatch)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u32 x2apic_id = kvm_x2apic_id(apic);
+	/*
+	 * 返回: kvm_lapic_get_reg(apic, APIC_ID) >> 24;
+	 */
 	u32 xapic_id = kvm_xapic_id(apic);
 	u32 physical_id;
 
@@ -236,6 +291,15 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 	 * manually modified its xAPIC IDs, events targeting that ID are
 	 * supposed to be recognized by all vCPUs with said ID.
 	 */
+	/*
+	 * 在以下使用kvm_arch->x2apic_format:
+	 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+	 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+	 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+	 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+	 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+	 */
 	if (vcpu->kvm->arch.x2apic_format) {
 		/* See also kvm_apic_match_physical_addr(). */
 		if (apic_x2apic_mode(apic) || x2apic_id > 0xff)
@@ -263,6 +327,10 @@ static int kvm_recalculate_phys_map(struct kvm_apic_map *new,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_recalculate_logical_map():
+ *   - arch/x86/kvm/lapic.c|508| <<kvm_recalculate_apic_map>> kvm_recalculate_logical_map(new, vcpu);
+ */
 static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 					struct kvm_vcpu *vcpu)
 {
@@ -285,6 +353,11 @@ static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 	if (apic_x2apic_mode(apic)) {
 		logical_mode = KVM_APIC_MODE_X2APIC;
 	} else {
+		/*
+		 * 代码:
+		 * #define         GET_APIC_LOGICAL_ID(x)  (((x) >> 24) & 0xFFu)
+		 * #define         SET_APIC_LOGICAL_ID(x)  (((x) << 24))
+		 */
 		ldr = GET_APIC_LOGICAL_ID(ldr);
 		if (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)
 			logical_mode = KVM_APIC_MODE_XAPIC_FLAT;
@@ -310,9 +383,18 @@ static void kvm_recalculate_logical_map(struct kvm_apic_map *new,
 	 * reversing the LDR calculation to get cluster of APICs, i.e. no
 	 * additional work is required.
 	 */
+	/*
+	 * 在kvm_apic_map_get_logical_dest()中
+	 * 如果是KVM_APIC_MODE_X2APIC直接使用phys_map[]
+	 */
 	if (apic_x2apic_mode(apic))
 		return;
 
+	/*
+	 * 在以下使用kvm_apic_map_get_logical_dest():
+	 *   - arch/x86/kvm/lapic.c|347| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
+	 *   - arch/x86/kvm/lapic.c|1785| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+	 */
 	if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
 							&cluster, &mask))) {
 		new->logical_mode = KVM_APIC_MODE_MAP_DISABLED;
@@ -341,6 +423,14 @@ enum {
 	DIRTY
 };
 
+/*
+ * 在以下使用kvm_recalculate_apic_map():
+ *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+ */
 static void kvm_recalculate_apic_map(struct kvm *kvm)
 {
 	struct kvm_apic_map *new, *old = NULL;
@@ -350,6 +440,19 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	bool xapic_id_mismatch;
 	int r;
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	/* Read kvm->arch.apic_map_dirty before kvm->arch.apic_map.  */
 	if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
 		return;
@@ -357,9 +460,49 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	WARN_ONCE(!irqchip_in_kernel(kvm),
 		  "Dirty APIC map without an in-kernel local APIC");
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_lock:
+	 *   - arch/x86/kvm/lapic.c|412| <<kvm_recalculate_apic_map>> mutex_lock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|425| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|500| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/x86.c|13686| <<kvm_arch_init_vm>> mutex_init(&kvm->arch.apic_map_lock);
+	 */
 	mutex_lock(&kvm->arch.apic_map_lock);
 
 retry:
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *
+	 *
+	 * 1200  * atomic_cmpxchg_acquire() - atomic compare and exchange with acquire ordering
+	 * 1201  * @v: pointer to atomic_t
+	 * 1202  * @old: int value to compare with
+	 * 1203  * @new: int value to assign
+	 * 1204  *
+	 * 1205  * If (@v == @old), atomically updates @v to @new with acquire ordering.
+	 * 1206  * Otherwise, @v is not modified and relaxed ordering is provided.
+	 * 1207  *
+	 * 1208  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_acquire() there.
+	 * 1209  *
+	 * 1210  * Return: The original value of @v.
+	 * ... ...
+	 * 1212 static __always_inline int
+	 * 1213 atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
+	 *
+	 * 只有kvm->arch.apic_map_dirty是DIRTY的时候才改成UPDATE_IN_PROGRESS
+	 * 返回旧的value
+	 */
 	/*
 	 * Read kvm->arch.apic_map_dirty before kvm->arch.apic_map (if clean)
 	 * or the APIC registers (if dirty).  Note, on retry the map may have
@@ -383,6 +526,9 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	 */
 	xapic_id_mismatch = false;
 
+	/*
+	 * kvm_x2apic_id()返回vcpu_id
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		if (kvm_apic_present(vcpu))
 			max_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));
@@ -394,6 +540,18 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	if (!new)
 		goto out;
 
+	/*
+	 * 1198 struct kvm_apic_map {
+	 * 1199         struct rcu_head rcu;
+	 * 1200         enum kvm_apic_logical_mode logical_mode;
+	 * 1201         u32 max_apic_id;
+	 * 1202         union {
+	 * 1203                 struct kvm_lapic *xapic_flat_map[8];
+	 * 1204                 struct kvm_lapic *xapic_cluster_map[16][4];
+	 * 1205         };
+	 * 1206         struct kvm_lapic *phys_map[];
+	 * 1207 };
+	 */
 	new->max_apic_id = max_id;
 	new->logical_mode = KVM_APIC_MODE_SW_DISABLED;
 
@@ -421,24 +579,82 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	 * and all unwanted aliasing that results in disabling the optimized
 	 * map also applies to APICv.
 	 */
+	/*
+	 * 注释:
+	 * APICv is disabled because not all vCPUs have a 1:1 mapping between
+	 * APIC ID and vCPU, _and_ KVM is not applying its x2APIC hotplug hack.
+	 */
 	if (!new)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
 
+	/*
+	 * 注释:
+	 * AVIC is disabled because not all vCPUs with a valid LDR have a 1:1
+	 * mapping between logical ID and vCPU.
+	 */
 	if (!new || new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
 
+	/*
+	 * 注释:
+	 * For simplicity, the APIC acceleration is inhibited
+	 * first time either APIC ID or APIC base are changed by the guest
+	 * from their reset values.
+	 */
 	if (xapic_id_mismatch)
 		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
 	else
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
 
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	old = rcu_dereference_protected(kvm->arch.apic_map,
 			lockdep_is_held(&kvm->arch.apic_map_lock));
 	rcu_assign_pointer(kvm->arch.apic_map, new);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *
+	 *
+	 * 1220  * atomic_cmpxchg_release() - atomic compare and exchange with release ordering
+	 * 1221  * @v: pointer to atomic_t
+	 * 1222  * @old: int value to compare with
+	 * 1223  * @new: int value to assign
+	 * 1224  *
+	 * 1225  * If (@v == @old), atomically updates @v to @new with release ordering.
+	 * 1226  * Otherwise, @v is not modified and relaxed ordering is provided.
+	 * 1227  *
+	 * 1228  * Unsafe to use in noinstr code; use raw_atomic_cmpxchg_release() there.
+	 * 1229  *
+	 * 1230  * Return: The original value of @v.
+	 * ... ...
+	 * 1232 static __always_inline int
+	 * 1233 atomic_cmpxchg_release(atomic_t *v, int old, int new)
+	 *
+	 * 只有kvm->arch.apic_map_dirty是UPDATE_IN_PROGRESS的时候才改成CLEAN
+	 */
 	/*
 	 * Write kvm->arch.apic_map before clearing apic->apic_map_dirty.
 	 * If another update has come in, leave it DIRTY.
@@ -450,15 +666,36 @@ static void kvm_recalculate_apic_map(struct kvm *kvm)
 	if (old)
 		kvfree_rcu(old, rcu);
 
+	/*
+	 * 在以下使用kvm_make_scan_ioapic_request():
+	 *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+	 *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+	 *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+	 */
 	kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * 在以下使用apic_set_spiv():
+ *   - arch/x86/kvm/lapic.c|2713| <<kvm_lapic_reg_write(APIC_SPIV)>> apic_set_spiv(apic, val & mask);
+ *   - arch/x86/kvm/lapic.c|3299| <<kvm_lapic_reset>> apic_set_spiv(apic, 0xff);
+ *   - arch/x86/kvm/lapic.c|3734| <<kvm_apic_set_state>> apic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));
+ */
 static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 {
 	bool enabled = val & APIC_SPIV_APIC_ENABLED;
 
 	kvm_lapic_set_reg(apic, APIC_SPIV, val);
 
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (enabled != apic->sw_enabled) {
 		apic->sw_enabled = enabled;
 		if (enabled)
@@ -466,6 +703,19 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 		else
 			static_branch_inc(&apic_sw_disabled.key);
 
+		/*
+		 * 在以下使用kvm_arch->apic_map_dirty:
+		 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+		 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+		 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+		 */
 		atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 	}
 
@@ -476,24 +726,74 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_set_xapic_id():
+ *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+ *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_xapic_id(struct kvm_lapic *apic, u8 id)
 {
 	kvm_lapic_set_reg(apic, APIC_ID, id << 24);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
 static inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)
 {
 	kvm_lapic_set_reg(apic, APIC_LDR, id);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
 static inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)
 {
 	kvm_lapic_set_reg(apic, APIC_DFR, val);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
+/*
+ * 在以下使用kvm_apic_set_x2apic_id():
+ *   - arch/x86/kvm/lapic.c|3266| <<__kvm_apic_set_base>> kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+ */
 static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 {
 	u32 ldr = kvm_apic_calc_x2apic_ldr(id);
@@ -502,6 +802,19 @@ static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 
 	kvm_lapic_set_reg(apic, APIC_ID, id);
 	kvm_lapic_set_reg(apic, APIC_LDR, ldr);
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 }
 
@@ -606,6 +919,11 @@ static u8 count_vectors(void *bitmap)
 	return count;
 }
 
+/*
+ * 在以下使用__kvm_apic_update_irr():
+ *   - arch/x86/kvm/lapic.c|791| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+ *   - arch/x86/kvm/vmx/nested.c|4381| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+ */
 bool __kvm_apic_update_irr(unsigned long *pir, void *regs, int *max_irr)
 {
 	unsigned long pir_vals[NR_PIR_WORDS];
@@ -647,23 +965,73 @@ EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, unsigned long *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
+	/*
+	 * 在以下使用__kvm_apic_update_irr():
+	 *   - arch/x86/kvm/lapic.c|791| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+	 *   - arch/x86/kvm/vmx/nested.c|4381| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+	 */
 	bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	if (unlikely(!apic->apicv_active && irr_updated))
 		apic->irr_pending = true;
 	return irr_updated;
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_irr);
 
+/*
+ * 在以下使用apic_search_irr():
+ *   - arch/x86/kvm/lapic.c|702| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+ *   - arch/x86/kvm/lapic.c|720| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+ */
 static inline int apic_search_irr(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用apic_find_highest_vector():
+	 *   - arch/x86/kvm/lapic.c|814| <<apic_search_irr>> return apic_find_highest_vector(apic->regs + APIC_IRR);
+	 *   - arch/x86/kvm/lapic.c|1010| <<apic_find_highest_isr>> result = apic_find_highest_vector(apic->regs + APIC_ISR);
+	 */
 	return apic_find_highest_vector(apic->regs + APIC_IRR);
 }
 
+/*
+ * 在以下使用apic_find_highest_irr():
+ *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+ *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+ */
 static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 {
 	int result;
 
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	/*
 	 * Note that irr_pending is just a hint. It will be always
 	 * true with virtual interrupt delivery enabled.
@@ -671,41 +1039,106 @@ static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 	if (!apic->irr_pending)
 		return -1;
 
+	/*
+	 * 在以下使用apic_search_irr():
+	 *   - arch/x86/kvm/lapic.c|702| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|720| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+	 */
 	result = apic_search_irr(apic);
 	ASSERT(result == -1 || result >= 16);
 
 	return result;
 }
 
+/*
+ * 在以下使用apic_clear_irr():
+ *   - arch/x86/kvm/lapic.c|722| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|3273| <<kvm_apic_ack_interrupt>> apic_clear_irr(vector, apic);
+ */
 static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (unlikely(apic->apicv_active)) {
 		apic_clear_vector(vec, apic->regs + APIC_IRR);
 	} else {
+		/*
+		 * 在以下使用kvm_lapic->irr_pending:
+		 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+		 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+		 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+		 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+		 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+		 *                                               vcpu->arch.apic->irr_pending = true;
+		 */
 		apic->irr_pending = false;
 		apic_clear_vector(vec, apic->regs + APIC_IRR);
+		/*
+		 * 在以下使用apic_search_irr():
+		 *   - arch/x86/kvm/lapic.c|702| <<apic_find_highest_irr>> result = apic_search_irr(apic);
+		 *   - arch/x86/kvm/lapic.c|720| <<apic_clear_irr>> if (apic_search_irr(apic) != -1)
+		 */
 		if (apic_search_irr(apic) != -1)
 			apic->irr_pending = true;
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_clear_irr():
+ *   - arch/x86/kvm/vmx/nested.c|4412| <<vmx_check_nested_events>> kvm_apic_clear_irr(vcpu, irq);
+ */
 void kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)
 {
+	/*
+	 * 在以下使用apic_clear_irr():
+	 *   - arch/x86/kvm/lapic.c|722| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|3273| <<kvm_apic_ack_interrupt>> apic_clear_irr(vector, apic);
+	 */
 	apic_clear_irr(vec, vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_clear_irr);
 
+/*
+ * 在以下使用apic_vector_to_isr():
+ *   - arch/x86/kvm/lapic.c|803| <<apic_set_isr>> apic_vector_to_isr(vec, apic)))
+ *   - arch/x86/kvm/lapic.c|867| <<apic_clear_isr>> apic_vector_to_isr(vec, apic)))
+ */
 static void *apic_vector_to_isr(int vec, struct kvm_lapic *apic)
 {
 	return apic->regs + APIC_ISR + APIC_VECTOR_TO_REG_OFFSET(vec);
 }
 
+/*
+ * 在以下使用apic_set_isr():
+ *   - arch/x86/kvm/lapic.c|3288| <<kvm_apic_ack_interrupt>> apic_set_isr(vector, apic);
+ */
 static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 {
 	if (__test_and_set_bit(APIC_VECTOR_TO_BIT_NUMBER(vec),
 			       apic_vector_to_isr(vec, apic)))
 		return;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 */
 	/*
 	 * With APIC virtualization enabled, all caching is disabled
 	 * because the processor can modify ISR under the hood.  Instead
@@ -714,8 +1147,28 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	if (unlikely(apic->apicv_active))
 		kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
 	else {
+		/*
+		 * 在以下使用kvm_lapic->isr_count:
+		 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+		 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+		 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+		 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+		 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+		 */
 		++apic->isr_count;
 		BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+		/*
+		 * 在以下使用kvm_lapic->highest_isr_cache:
+		 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+		 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+		 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+		 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+		 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+		 */
 		/*
 		 * ISR (in service register) bit is set when injecting an interrupt.
 		 * The highest vector is injected. Thus the latest bit set matches
@@ -729,21 +1182,50 @@ static inline int apic_find_highest_isr(struct kvm_lapic *apic)
 {
 	int result;
 
+	/*
+	 * 在以下使用kvm_lapic->isr_count:
+	 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+	 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+	 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+	 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+	 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+	 */
 	/*
 	 * Note that isr_count is always 1, and highest_isr_cache
 	 * is always -1, with APIC virtualization enabled.
 	 */
 	if (!apic->isr_count)
 		return -1;
+	/*
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	if (likely(apic->highest_isr_cache != -1))
 		return apic->highest_isr_cache;
 
+	/*
+	 * 在以下使用apic_find_highest_vector():
+	 *   - arch/x86/kvm/lapic.c|814| <<apic_search_irr>> return apic_find_highest_vector(apic->regs + APIC_IRR);
+	 *   - arch/x86/kvm/lapic.c|1010| <<apic_find_highest_isr>> result = apic_find_highest_vector(apic->regs + APIC_ISR);
+	 */
 	result = apic_find_highest_vector(apic->regs + APIC_ISR);
 	ASSERT(result == -1 || result >= 16);
 
 	return result;
 }
 
+/*
+ * 在以下使用apic_set_eoi():
+ *   - arch/x86/kvm/lapic.c|2065| <<apic_set_eoi>> apic_clear_isr(vector, apic);
+ */
 static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 {
 	if (!__test_and_clear_bit(APIC_VECTOR_TO_BIT_NUMBER(vec),
@@ -757,28 +1239,90 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	 * on the other hand isr_count and highest_isr_cache are unused
 	 * and must be left alone.
 	 */
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 */
 	if (unlikely(apic->apicv_active))
 		kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
 	else {
+		/*
+		 * 在以下使用kvm_lapic->isr_count:
+		 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+		 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+		 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+		 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+		 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+		 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+		 */
 		--apic->isr_count;
 		BUG_ON(apic->isr_count < 0);
+		/*
+		 * 在以下使用kvm_lapic->highest_isr_cache:
+		 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+		 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+		 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+		 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+		 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+		 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+		 */
 		apic->highest_isr_cache = -1;
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_update_hwapic_isr():
+ *   - arch/x86/kvm/vmx/nested.c|5121| <<__nested_vmx_vmexit>> kvm_apic_update_hwapic_isr(vcpu);
+ */
 void kvm_apic_update_hwapic_isr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (WARN_ON_ONCE(!lapic_in_kernel(vcpu)) || !apic->apicv_active)
 		return;
 
+	/*
+	 * 在以下使用vt_hwapic_isr_update():
+	 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+	 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+	 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+	 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+	 *
+	 * vt_hwapic_isr_update()
+	 * vmx_hwapic_isr_update()
+	 */
 	kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_hwapic_isr);
 
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用apic_find_highest_irr():
+	 *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	/* This may race with setting of irr in __apic_accept_irq() and
 	 * value returned may be wrong, but kvm_vcpu_kick() in __apic_accept_irq
 	 * will cause vmexit immediately and the value will be recalculated
@@ -792,6 +1336,19 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * 在以下使用kvm_apic_set_irq():
+ *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *
+ * 调用__apic_accept_irq()
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -816,6 +1373,19 @@ static int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,
 		min((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {
 		if (map->phys_map[min + i]) {
 			vcpu = map->phys_map[min + i]->vcpu;
+			/*
+			 * 在以下使用kvm_apic_set_irq():
+			 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+			 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *
+			 * 调用__apic_accept_irq()
+			 */
 			count += kvm_apic_set_irq(vcpu, irq, NULL);
 		}
 	}
@@ -823,6 +1393,10 @@ static int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,
 	return count;
 }
 
+/*
+ * 在以下使用kvm_pv_send_ipi():
+ *   - arch/x86/kvm/x86.c|10321| <<____kvm_emulate_hypercall(KVM_HC_SEND_IPI)>> ret = kvm_pv_send_ipi(vcpu->kvm, a0, a1, a2, a3, op_64_bit);
+ */
 int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 		    unsigned long ipi_bitmap_high, u32 min,
 		    unsigned long icr, int op_64_bit)
@@ -841,6 +1415,17 @@ int kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,
 	irq.trig_mode = icr & APIC_INT_LEVELTRIG;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
 	count = -EOPNOTSUPP;
@@ -873,14 +1458,35 @@ static inline bool pv_eoi_enabled(struct kvm_vcpu *vcpu)
 	return vcpu->arch.pv_eoi.msr_val & KVM_MSR_ENABLED;
 }
 
+/*
+ * 在以下使用pv_eoi_set_pending():
+ *   - arch/x86/kvm/lapic.c|3979| <<apic_sync_pv_eoi_to_guest>> pv_eoi_set_pending(apic->vcpu);
+ */
 static void pv_eoi_set_pending(struct kvm_vcpu *vcpu)
 {
 	if (pv_eoi_put_user(vcpu, KVM_PV_EOI_ENABLED) < 0)
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
 }
 
+/*
+ * 在以下使用pv_eoi_test_and_clr_pending():
+ *   - arch/x86/kvm/lapic.c|4012| <<apic_sync_pv_eoi_from_guest>> if (pv_eoi_test_and_clr_pending(vcpu))
+ */
 static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 {
 	u8 val;
@@ -893,6 +1499,19 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 	if (val && pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0)
 		return false;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	/*
 	 * Clear pending bit in any case: it will be set again on vmentry.
 	 * While this might not be ideal from performance point of view,
@@ -903,18 +1522,64 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * 在以下使用apic_has_interrupt_for_ppr():
+ *   - arch/x86/kvm/lapic.c|945| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+ *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+ *
+ * 选出最高的irr, 不修改任何数据
+ *
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ * 
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ */
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
+	/*
+	 * 在以下使用apic_find_highest_irr():
+	 *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	if (kvm_x86_ops.sync_pir_to_irr)
 		highest_irr = kvm_x86_call(sync_pir_to_irr)(apic->vcpu);
 	else
 		highest_irr = apic_find_highest_irr(apic);
+	/*
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 0xFF = 255
+	 */
 	if (highest_irr == -1 || (highest_irr & 0xF0) <= ppr)
 		return -1;
 	return highest_irr;
 }
 
+/*
+ * 在以下使用__apic_update_ppr():
+ *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+ *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+ *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+ *
+ * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 根据TPR和ISR更新PPR
+ */
 static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 {
 	u32 tpr, isrv, ppr, old_ppr;
@@ -937,24 +1602,141 @@ static bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)
 	return ppr < old_ppr;
 }
 
+/*
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ * 
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ *
+ *
+ * 在以下使用apic_update_ppr():
+ *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+ *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+ *
+ * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 根据TPR和ISR更新PPR
+ * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+ * 如果有, 则设置KVM_REQ_EVENT
+ */
 static void apic_update_ppr(struct kvm_lapic *apic)
 {
 	u32 ppr;
 
+	/*
+	 * 在以下使用apic_has_interrupt_for_ppr():
+	 *   - arch/x86/kvm/lapic.c|945| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+	 *
+	 * 选出最高的irr, 不修改任何数据
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 *
+	 *
+	 * 在以下使用__apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+	 *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 */
 	if (__apic_update_ppr(apic, &ppr) &&
 	    apic_has_interrupt_for_ppr(apic, ppr) != -1)
 		kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 }
 
+/*
+ * 关于PPR:
+ * 它记录了当前CPU接受中断的优先级水平.
+ * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+ *
+ * 关于TPR:
+ * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+ * CPU.低于或等于门槛的中断会被屏蔽或延后.
+ * 举个简单数字例子:
+ * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+ *
+ * 1. 当一个中断(优先级为N)从外部到达 LAPIC.
+ * 2. LAPIC首先看TPR: 如果N<=TPR的门槛,那么中断不会被交付给CPU.TPR 起"扼门"作用.
+ * 3. 若N>TPR,则中断可以通过门,被交付给CPU.
+ * 4. 在交付后,CPU(通过LAPIC)会把PPR更新为N(或与N对应的优先级等级),表明:"我正在处理优先级N的中断".
+ * 5. 当CPU完成处理该中断并发出EOI(End Of Interrupt)后,PPR值可能下降(变为处理完后新的最高优先级水平).
+ * 6. 这样,下次中断到来时,还得再经过TPR检查,然后可能被交付,并更新PPR.
+ * 因此: TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 二者合起来保证: 即便中断优先级很高,
+ * 如果当前CPU正在处理一个更高/已被接受的优先级任务,系统也能正确决定是否接受新的中断.
+ */
+
+/*
+ * 在以下使用kvm_apic_update_ppr():
+ *   - arch/x86/kvm/vmx/vmx.c|5546| <<handle_tpr_below_threshold>> kvm_apic_update_ppr(vcpu);
+ */
 void kvm_apic_update_ppr(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_ppr);
 
+/*
+ * 在以下使用apic_set_tpr():
+ *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+ *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+ *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+ */
 static void apic_set_tpr(struct kvm_lapic *apic, u32 tpr)
 {
 	kvm_lapic_set_reg(apic, APIC_TASKPRI, tpr);
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
 }
 
@@ -964,6 +1746,10 @@ static bool kvm_apic_broadcast(struct kvm_lapic *apic, u32 mda)
 			X2APIC_BROADCAST : APIC_BROADCAST);
 }
 
+/*
+ * 在以下使用kvm_apic_match_physical_addr():
+ *   - arch/x86/kvm/lapic.c|1813| <<kvm_apic_match_dest>> return kvm_apic_match_physical_addr(target, mda);
+ */
 static bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)
 {
 	if (kvm_apic_broadcast(apic, mda))
@@ -984,6 +1770,10 @@ static bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)
 	return mda == kvm_xapic_id(apic);
 }
 
+/*
+ * 在以下使用kvm_apic_match_logical_addr():
+ *   - arch/x86/kvm/lapic.c|1815| <<kvm_apic_match_dest>> return kvm_apic_match_logical_addr(target, mda);
+ */
 static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
 {
 	u32 logical_id;
@@ -993,6 +1783,9 @@ static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
 
 	logical_id = kvm_lapic_get_reg(apic, APIC_LDR);
 
+	/*
+	 * 如果是x2apic这里就退出了
+	 */
 	if (apic_x2apic_mode(apic))
 		return ((logical_id >> 16) == (mda >> 16))
 		       && (logical_id & mda & 0xffff) != 0;
@@ -1026,6 +1819,10 @@ static bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)
  * important when userspace wants to use x2APIC-format MSIs, because
  * APIC_BROADCAST (0xff) is a legal route for "cluster 0, CPUs 0-7".
  */
+/*
+ * 在以下使用kvm_apic_mda():
+ *   - arch/x86/kvm/lapic.c|1807| <<kvm_apic_match_dest>> u32 mda = kvm_apic_mda(vcpu, dest, source, target);
+ */
 static u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,
 		struct kvm_lapic *source, struct kvm_lapic *target)
 {
@@ -1038,6 +1835,24 @@ static u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,
 	return dest_id;
 }
 
+/*
+ * 在以下使用kvm_apic_match_dest():
+ *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+ *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id, 
+ *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+ *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+ *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+ *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+ *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+ *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+ *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+ */
 bool kvm_apic_match_dest(struct kvm_vcpu *vcpu, struct kvm_lapic *source,
 			   int shorthand, unsigned int dest, int dest_mode)
 {
@@ -1113,6 +1928,12 @@ static bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,
  * means that the interrupt should be dropped.  In this case, *bitmap would be
  * zero and *dst undefined.
  */
+/*
+ * 在以下使用kvm_apic_map_get_dest_lapic():
+ *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+ *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+ *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+ */
 static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 		struct kvm_lapic **src, struct kvm_lapic_irq *irq,
 		struct kvm_apic_map *map, struct kvm_lapic ***dst,
@@ -1138,14 +1959,28 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 			*dst = &map->phys_map[dest_id];
 			*bitmap = 1;
 		}
+		/*
+		 * 如果mode是APIC_DEST_PHYSICAL一定在这里退出
+		 */
 		return true;
 	}
 
 	*bitmap = 0;
+	/*
+	 * 在以下使用kvm_apic_map_get_logical_dest():
+	 *   - arch/x86/kvm/lapic.c|347| <<kvm_recalculate_logical_map>> if (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,
+	 *   - arch/x86/kvm/lapic.c|1785| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+	 */
 	if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
 				(u16 *)bitmap))
 		return false;
 
+	/*
+	 * 在以下使用kvm_lowest_prio_delivery():
+	 *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+	 *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+	 */
 	if (!kvm_lowest_prio_delivery(irq))
 		return true;
 
@@ -1179,6 +2014,11 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * 在以下使用kvm_irq_delivery_to_apic_fast():
+ *   - arch/x86/kvm/irq.c|424| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq.c|554| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -1195,19 +2035,62 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 			*r = 0;
 			return true;
 		}
+		/*
+		 * 在以下使用kvm_apic_set_irq():
+		 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *
+		 * 调用__apic_accept_irq()
+		 */
 		*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
 		return true;
 	}
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 在以下使用kvm_apic_map_get_dest_lapic():
+	 *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+	 *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+	 *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+	 */
 	ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
 	if (ret) {
 		*r = 0;
 		for_each_set_bit(i, &bitmap, 16) {
 			if (!dst[i])
 				continue;
+			/*
+			 * 在以下使用kvm_apic_set_irq():
+			 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+			 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+			 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+			 *
+			 * 调用__apic_accept_irq()
+			 */
 			*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
 		}
 	}
@@ -1230,6 +2113,10 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
  *	   interrupt.
  * - Otherwise, use remapped mode to inject the interrupt.
  */
+/*
+ * 在以下使用kvm_intr_is_single_vcpu_fast():
+ *   - arch/x86/kvm/irq.c|667| <<kvm_intr_is_single_vcpu>> if (kvm_intr_is_single_vcpu_fast(kvm, irq, dest_vcpu))
+ */
 bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			struct kvm_vcpu **dest_vcpu)
 {
@@ -1242,8 +2129,25 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
 		return false;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 在以下使用kvm_apic_map_get_dest_lapic():
+	 *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+	 *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+	 *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+	 */
 	if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
 			hweight16(bitmap) == 1) {
 		unsigned long i = find_first_bit(&bitmap, 16);
@@ -1297,6 +2201,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 				apic_clear_vector(vector, apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vt_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		kvm_x86_call(deliver_interrupt)(apic, delivery_mode,
 						trig_mode, vector);
 		break;
@@ -1333,6 +2241,14 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 
 	case APIC_DM_STARTUP:
 		result = 1;
+		/*
+		 * 在以下使用kvm_lapic->sipi_vector:
+		 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+		 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+		 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+		 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+		 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+		 */
 		apic->sipi_vector = vector;
 		/* make sure sipi_vector is visible for the receiver */
 		smp_wmb();
@@ -1363,6 +2279,11 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
  * out the destination vcpus array and set the bitmap or it traverses to
  * each available vcpu to identify the same.
  */
+/*
+ * 在以下使用kvm_bitmap_or_dest_vcpus:
+ *   - arch/x86/kvm/ioapic.c|458| <<ioapic_write_indirect>> kvm_bitmap_or_dest_vcpus(ioapic->kvm, &irq,
+ *   - arch/x86/kvm/ioapic.c|471| <<ioapic_write_indirect>> kvm_bitmap_or_dest_vcpus(ioapic->kvm, &irq,
+ */
 void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 			      unsigned long *vcpu_bitmap)
 {
@@ -1375,8 +2296,25 @@ void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 	bool ret;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(kvm->arch.apic_map);
 
+	/*
+	 * 在以下使用kvm_apic_map_get_dest_lapic():
+	 *   - arch/x86/kvm/lapic.c|1958| <<kvm_irq_delivery_to_apic_fast>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);
+	 *   - arch/x86/kvm/lapic.c|2011| <<kvm_intr_is_single_vcpu_fast>> if (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&
+	 *   - arch/x86/kvm/lapic.c|2167| <<kvm_bitmap_or_dest_vcpus>> ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
+	 */
 	ret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,
 					  &bitmap);
 	if (ret) {
@@ -1390,6 +2328,24 @@ void kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			if (!kvm_apic_present(vcpu))
 				continue;
+			/*
+			 * 在以下使用kvm_apic_match_dest():
+			 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+			 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+			 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+			 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+			 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+			 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+			 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+			 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+			 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+			 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+			 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+			 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+			 */
 			if (!kvm_apic_match_dest(vcpu, NULL,
 						 irq->shorthand,
 						 irq->dest_id,
@@ -1406,15 +2362,47 @@ int kvm_apic_compare_prio(struct kvm_vcpu *vcpu1, struct kvm_vcpu *vcpu2)
 	return vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;
 }
 
+/*
+ * 在以下使用kvm_ioapic_handles_vector():
+ *   - arch/x86/kvm/lapic.c|1804| <<kvm_ioapic_send_eoi>> if (!kvm_ioapic_handles_vector(apic, vector))
+ *   - arch/x86/kvm/lapic.c|3988| <<apic_sync_pv_eoi_to_guest>> if (... kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+ */
 static bool kvm_ioapic_handles_vector(struct kvm_lapic *apic, int vector)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 */
 	return test_bit(vector, apic->vcpu->arch.ioapic_handled_vectors);
 }
 
+/*
+ * 在以下使用kvm_ioapic_send_eoi():
+ *   - arch/x86/kvm/lapic.c|1876| <<apic_set_eoi>> kvm_ioapic_send_eoi(apic, vector);
+ *   - arch/x86/kvm/lapic.c|1891| <<kvm_apic_set_eoi_accelerated>> kvm_ioapic_send_eoi(apic, vector);
+ */
 static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 {
 	int __maybe_unused trigger_mode;
 
+	/*
+	 * 在以下使用kvm_ioapic_handles_vector():
+	 *   - arch/x86/kvm/lapic.c|1804| <<kvm_ioapic_send_eoi>> if (!kvm_ioapic_handles_vector(apic, vector))
+	 *   - arch/x86/kvm/lapic.c|3988| <<apic_sync_pv_eoi_to_guest>> if (... kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	/* Eoi the ioapic only if the ioapic doesn't own the vector. */
 	if (!kvm_ioapic_handles_vector(apic, vector))
 		return;
@@ -1427,6 +2415,16 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 	if (apic->vcpu->arch.highest_stale_pending_ioapic_eoi == vector)
 		kvm_make_request(KVM_REQ_SCAN_IOAPIC, apic->vcpu);
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	/* Request a KVM exit to inform the userspace IOAPIC. */
 	if (irqchip_split(apic->vcpu->kvm)) {
 		apic->vcpu->arch.pending_ioapic_eoi = vector;
@@ -1444,6 +2442,11 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 #endif
 }
 
+/*
+ * 在以下使用apic_set_eoi():
+ *   - arch/x86/kvm/lapic.c|2960| <<kvm_lapic_reg_write(APIC_EOI)>> apic_set_eoi(apic);
+ *   - arch/x86/kvm/lapic.c|4447| <<apic_sync_pv_eoi_from_guest>> vector = apic_set_eoi(apic);
+ */
 static int apic_set_eoi(struct kvm_lapic *apic)
 {
 	int vector = apic_find_highest_isr(apic);
@@ -1458,11 +2461,31 @@ static int apic_set_eoi(struct kvm_lapic *apic)
 		return vector;
 
 	apic_clear_isr(vector, apic);
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
 
 	if (kvm_hv_synic_has_vector(apic->vcpu, vector))
 		kvm_hv_synic_send_eoi(apic->vcpu, vector);
 
+	/*
+	 * 在以下使用kvm_ioapic_send_eoi():
+	 *   - arch/x86/kvm/lapic.c|1876| <<apic_set_eoi>> kvm_ioapic_send_eoi(apic, vector);
+	 *   - arch/x86/kvm/lapic.c|1891| <<kvm_apic_set_eoi_accelerated>> kvm_ioapic_send_eoi(apic, vector);
+	 */
 	kvm_ioapic_send_eoi(apic, vector);
 	kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 	return vector;
@@ -1472,12 +2495,21 @@ static int apic_set_eoi(struct kvm_lapic *apic)
  * this interface assumes a trap-like exit, which has already finished
  * desired side effect including vISR and vPPR update.
  */
+/*
+ * 在以下使用kvm_apic_set_eoi_accelerated():
+ *   - arch/x86/kvm/vmx/vmx.c|5632| <<handle_apic_eoi_induced>> kvm_apic_set_eoi_accelerated(vcpu, vector);
+ */
 void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	trace_kvm_eoi(apic, vector);
 
+	/*
+	 * 在以下使用kvm_ioapic_send_eoi():
+	 *   - arch/x86/kvm/lapic.c|1876| <<apic_set_eoi>> kvm_ioapic_send_eoi(apic, vector);
+	 *   - arch/x86/kvm/lapic.c|1891| <<kvm_apic_set_eoi_accelerated>> kvm_ioapic_send_eoi(apic, vector);
+	 */
 	kvm_ioapic_send_eoi(apic, vector);
 	kvm_make_request(KVM_REQ_EVENT, apic->vcpu);
 }
@@ -1504,6 +2536,16 @@ void kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)
 
 	trace_kvm_apic_ipi(icr_low, irq.dest_id);
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_send_ipi);
@@ -1564,6 +2606,21 @@ static u32 __apic_read(struct kvm_lapic *apic, unsigned int offset)
 		val = apic_get_tmcct(apic);
 		break;
 	case APIC_PROCPRI:
+		/*
+		 * 在以下使用apic_update_ppr():
+		 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+		 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+		 *
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 根据TPR和ISR更新PPR
+		 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+		 * 如果有, 则设置KVM_REQ_EVENT
+		 */
 		apic_update_ppr(apic);
 		val = kvm_lapic_get_reg(apic, offset);
 		break;
@@ -1762,6 +2819,18 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u32 reg;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	/*
 	 * Assume a timer IRQ was "injected" if the APIC is protected.  KVM's
 	 * copy of the vIRR is bogus, it's the responsibility of the caller to
@@ -1775,6 +2844,12 @@ static bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)
 		int vec = reg & APIC_VECTOR_MASK;
 		void *bitmap = apic->regs + APIC_ISR;
 
+		/*
+		 * 在以下修改kvm_lapic->apicv_active:
+		 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+		 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+		 */
 		if (apic->apicv_active)
 			bitmap = apic->regs + APIC_IRR;
 
@@ -1870,6 +2945,13 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
 
+	/*
+	 * 在以下使用kvm_apic_local_deliver():
+	 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+	 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+	 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+	 */
 	kvm_apic_local_deliver(apic, APIC_LVTT);
 	if (apic_lvtt_tscdeadline(apic)) {
 		ktimer->tscdeadline = 0;
@@ -1890,6 +2972,12 @@ static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 	if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
 		ktimer->expired_tscdeadline = ktimer->tscdeadline;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (!from_timer_fn && apic->apicv_active) {
 		WARN_ON(kvm_get_running_vcpu() != vcpu);
 		kvm_apic_inject_pending_timer_irqs(apic);
@@ -2252,6 +3340,13 @@ static int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 	switch (reg) {
 	case APIC_ID:		/* Local APIC ID */
 		if (!apic_x2apic_mode(apic)) {
+			/*
+			 * 在以下使用kvm_apic_set_xapic_id():
+			 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+			 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 */
 			kvm_apic_set_xapic_id(apic, val >> 24);
 		} else {
 			ret = 1;
@@ -2260,6 +3355,12 @@ static int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 
 	case APIC_TASKPRI:
 		report_tpr_access(apic, true);
+		/*
+		 * 在以下使用apic_set_tpr():
+		 *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+		 *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+		 *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+		 */
 		apic_set_tpr(apic, val & 0xff);
 		break;
 
@@ -2384,6 +3485,14 @@ static int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)
 		break;
 	}
 
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	/*
 	 * Recalculate APIC maps if necessary, e.g. if the software enable bit
 	 * was toggled, the APIC ID changed, etc...   The maps are marked dirty
@@ -2498,15 +3607,52 @@ void kvm_free_lapic(struct kvm_vcpu *vcpu)
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	if (!vcpu->arch.apic) {
+		/*
+		 * 在以下使用kvm_has_noapic_vcpu:
+		 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+		 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+		 */
 		static_branch_dec(&kvm_has_noapic_vcpu);
 		return;
 	}
 
 	hrtimer_cancel(&apic->lapic_timer.timer);
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
 		static_branch_slow_dec_deferred(&apic_hw_disabled);
 
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (!apic->sw_enabled)
 		static_branch_slow_dec_deferred(&apic_sw_disabled);
 
@@ -2545,6 +3691,12 @@ void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)
 
 void kvm_lapic_set_tpr(struct kvm_vcpu *vcpu, unsigned long cr8)
 {
+	/*
+	 * 在以下使用apic_set_tpr():
+	 *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+	 *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+	 *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+	 */
 	apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
 }
 
@@ -2557,8 +3709,34 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * 在以下使用__kvm_apic_set_base():
+ *   - arch/x86/kvm/lapic.c|3286| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+ *   - arch/x86/kvm/lapic.c|3516| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+ */
 static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	u64 old_value = vcpu->arch.apic_base;
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
@@ -2573,17 +3751,61 @@ static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	/* update jump label if enable bit changes */
 	if ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE) {
 		if (value & MSR_IA32_APICBASE_ENABLE) {
+			/*
+			 * 在以下使用kvm_apic_set_xapic_id():
+			 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+			 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+			 */
 			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
 			static_branch_slow_dec_deferred(&apic_hw_disabled);
 			/* Check if there are APF page ready requests pending */
 			kvm_make_request(KVM_REQ_APF_READY, vcpu);
 		} else {
 			static_branch_inc(&apic_hw_disabled.key);
+			/*
+			 * 在以下使用kvm_arch->apic_map_dirty:
+			 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+			 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+			 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+			 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+			 */
 			atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
 		}
 	}
 
+	/*
+	 * 这个patch引入的代码.
+	 *
+	 * KVM: x86: Reinitialize xAPIC ID when userspace forces x2APIC => xAPIC
+	 * https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=052c3b99cbc8d227f8cb8edf1519197808d1d653
+	 *
+	 * -	if (((old_value ^ value) & X2APIC_ENABLE) && (value & X2APIC_ENABLE))
+	 * -		kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+	 * +	if ((old_value ^ value) & X2APIC_ENABLE) {
+	 * +		if (value & X2APIC_ENABLE)
+	 * +			kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
+	 * +		else if (value & MSR_IA32_APICBASE_ENABLE)
+	 * +			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 * +	}
+	 *
+	 * 在以下使用kvm_apic_set_xapic_id():
+	 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+	 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 */
 	if ((old_value ^ value) & X2APIC_ENABLE) {
+		/*
+		 * 只在这一个地方调用kvm_apic_set_x2apic_id()
+		 */
 		if (value & X2APIC_ENABLE)
 			kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
 		else if (value & MSR_IA32_APICBASE_ENABLE)
@@ -2592,9 +3814,39 @@ static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE)) {
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
+		/*
+		 * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+		 *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+		 *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+		 *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+		 *
+		 * vt_set_virtual_apic_mode()
+		 * vmx_set_virtual_apic_mode()
+		 */
 		kvm_x86_call(set_virtual_apic_mode)(vcpu);
 	}
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	apic->base_address = apic->vcpu->arch.apic_base &
 			     MSR_IA32_APICBASE_BASE;
 
@@ -2605,11 +3857,39 @@ static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_set_base():
+ *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+ *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+ *                         msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+ */
 int kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value, bool host_initiated)
 {
 	enum lapic_mode old_mode = kvm_get_apic_mode(vcpu);
 	enum lapic_mode new_mode = kvm_apic_mode(value);
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (vcpu->arch.apic_base == value)
 		return 0;
 
@@ -2625,12 +3905,31 @@ int kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value, bool host_initiated)
 			return 1;
 	}
 
+	/*
+	 * 在以下使用__kvm_apic_set_base():
+	 *   - arch/x86/kvm/lapic.c|3286| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+	 *   - arch/x86/kvm/lapic.c|3516| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+	 */
 	__kvm_apic_set_base(vcpu, value);
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	kvm_recalculate_apic_map(vcpu->kvm);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(kvm_apic_set_base);
 
+/*
+ * 在以下使用kvm_apic_update_apicv():
+ *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+ *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+ */
 void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2648,26 +3947,90 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 	 *        state prior to updating KVM's metadata caches, so that KVM
 	 *        can safely search the IRR and set irr_pending accordingly.
 	 */
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	apic->irr_pending = true;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 在以下使用kvm_lapic->isr_count:
+	 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+	 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+	 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+	 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+	 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+	 */
 	if (apic->apicv_active)
 		apic->isr_count = 1;
 	else
 		apic->isr_count = count_vectors(apic->regs + APIC_ISR);
 
+	/*
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	apic->highest_isr_cache = -1;
 }
 
+/*
+ * 在以下使用kvm_alloc_apic_access_page():
+ *   - arch/x86/kvm/svm/avic.c|287| <<avic_init_backing_page>> ret = kvm_alloc_apic_access_page(vcpu->kvm);
+ *   - arch/x86/kvm/vmx/vmx.c|7480| <<vmx_vcpu_create>> err = kvm_alloc_apic_access_page(vcpu->kvm);
+ */
 int kvm_alloc_apic_access_page(struct kvm *kvm)
 {
 	void __user *hva;
 	int ret = 0;
 
 	mutex_lock(&kvm->slots_lock);
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 *
+	 * 在以下使用kvm_arch->apic_access_memslot_inhibited:
+	 *   - arch/x86/kvm/lapic.c|2668| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *                    kvm->arch.apic_access_memslot_inhibited)
+	 *   - arch/x86/kvm/lapic.c|2712| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_inhibited = true;
+	 */
 	if (kvm->arch.apic_access_memslot_enabled ||
 	    kvm->arch.apic_access_memslot_inhibited)
 		goto out;
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
 				      APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);
 	if (IS_ERR(hva)) {
@@ -2682,10 +4045,22 @@ int kvm_alloc_apic_access_page(struct kvm *kvm)
 }
 EXPORT_SYMBOL_GPL(kvm_alloc_apic_access_page);
 
+/*
+ * 在以下使用kvm_inhibit_apic_access_page():
+ *   - arch/x86/kvm/x86.c|10561| <<kvm_vcpu_update_apicv>> kvm_inhibit_apic_access_page(vcpu);
+ */
 void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
 
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 */
 	if (!kvm->arch.apic_access_memslot_enabled)
 		return;
 
@@ -2693,7 +4068,24 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 
 	mutex_lock(&kvm->slots_lock);
 
+	/*
+	 * 在以下使用kvm_arch->apic_access_memslot_enabled:
+	 *   - arch/x86/kvm/lapic.c|2667| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+	 *   - arch/x86/kvm/lapic.c|2678| <<kvm_alloc_apic_access_page>> kvm->arch.apic_access_memslot_enabled = true;
+	 *   - arch/x86/kvm/lapic.c|2689| <<kvm_inhibit_apic_access_page>> if (!kvm->arch.apic_access_memslot_enabled)
+	 *   - arch/x86/kvm/lapic.c|2696| <<kvm_inhibit_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled) {
+	 *   - arch/x86/kvm/lapic.c|2706| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_enabled = false;
+	 */
 	if (kvm->arch.apic_access_memslot_enabled) {
+		/*
+		 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+		 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+		 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+		 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+		 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+		 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 */
 		__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
 		/*
 		 * Clear "enabled" after the memslot is deleted so that a
@@ -2705,6 +4097,12 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 		 */
 		kvm->arch.apic_access_memslot_enabled = false;
 
+		/*
+		 * 在以下使用kvm_arch->apic_access_memslot_inhibited:
+		 *   - arch/x86/kvm/lapic.c|2668| <<kvm_alloc_apic_access_page>> if (kvm->arch.apic_access_memslot_enabled ||
+		 *                    kvm->arch.apic_access_memslot_inhibited)
+		 *   - arch/x86/kvm/lapic.c|2712| <<kvm_inhibit_apic_access_page>> kvm->arch.apic_access_memslot_inhibited = true;
+		 */
 		/*
 		 * Mark the memslot as inhibited to prevent reallocating the
 		 * memslot during vCPU creation, e.g. if a vCPU is hotplugged.
@@ -2717,12 +4115,22 @@ void kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)
 	kvm_vcpu_srcu_read_lock(vcpu);
 }
 
+/*
+ * 在以下使用kvm_lapic_reset():
+ *   - arch/x86/kvm/x86.c|13265| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	u64 msr_val;
 	int i;
 
+	/*
+	 * 只有pi_apicv_pre_state_restore
+	 * 在以下调用:
+	 *   - arch/x86/kvm/lapic.c|3262| <<kvm_lapic_reset>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3731| <<kvm_apic_set_state>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 */
 	kvm_x86_call(apicv_pre_state_restore)(vcpu);
 
 	if (!init_event) {
@@ -2736,6 +4144,11 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 		 * The recalculation needed for this vCPU will be done after
 		 * all APIC state has been initialized (see below).
 		 */
+		/*
+		 * 在以下使用__kvm_apic_set_base():
+		 *   - arch/x86/kvm/lapic.c|3286| <<kvm_apic_set_base>> __kvm_apic_set_base(vcpu, value);
+		 *   - arch/x86/kvm/lapic.c|3516| <<kvm_lapic_reset>> __kvm_apic_set_base(vcpu, msr_val);
+		 */
 		__kvm_apic_set_base(vcpu, msr_val);
 	}
 
@@ -2745,6 +4158,13 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 	/* Stop the timer in case it's a reset to an active apic */
 	hrtimer_cancel(&apic->lapic_timer.timer);
 
+	/*
+	 * 在以下使用kvm_apic_set_xapic_id():
+	 *   - arch/x86/kvm/lapic.c|2873| <<kvm_lapic_reg_write(APIC_ID)>> kvm_apic_set_xapic_id(apic, val >> 24);
+	 *   - arch/x86/kvm/lapic.c|3226| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3268| <<__kvm_apic_set_base>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 *   - arch/x86/kvm/lapic.c|3564| <<kvm_lapic_reset>> kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
+	 */
 	/* The xAPIC ID is set at RESET even if the APIC was already enabled. */
 	if (!init_event)
 		kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
@@ -2778,20 +4198,79 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 		kvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);
 		kvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);
 	}
+	/*
+	 * 在以下使用kvm_apic_update_apicv():
+	 *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+	 */
 	kvm_apic_update_apicv(vcpu);
 	update_divide_count(apic);
 	atomic_set(&apic->lapic_timer.pending, 0);
 
 	vcpu->arch.pv_eoi.msr_val = 0;
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active) {
 		kvm_x86_call(apicv_post_state_restore)(vcpu);
+		/*
+		 * 在以下使用vt_hwapic_isr_update():
+		 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+		 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+		 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+		 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *
+		 * vmx_hwapic_isr_update()
+		 */
 		kvm_x86_call(hwapic_isr_update)(vcpu, -1);
 	}
 
 	vcpu->arch.apic_arb_prio = 0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	vcpu->arch.apic_attention = 0;
 
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	kvm_recalculate_apic_map(vcpu->kvm);
 }
 
@@ -2816,6 +4295,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_apic_local_deliver():
+ *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2840,6 +4326,13 @@ void kvm_apic_nmi_wd_deliver(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 在以下使用kvm_apic_local_deliver():
+	 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+	 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+	 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+	 */
 	if (apic)
 		kvm_apic_local_deliver(apic, APIC_LVT0);
 }
@@ -2871,6 +4364,15 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	ASSERT(vcpu != NULL);
 
 	if (!irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用kvm_has_noapic_vcpu:
+		 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+		 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+		 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+		 */
 		static_branch_inc(&kvm_has_noapic_vcpu);
 		return 0;
 	}
@@ -2899,6 +4401,27 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	if (lapic_timer_advance)
 		apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	/*
 	 * Stuff the APIC ENABLE bit in lieu of temporarily incrementing
 	 * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().
@@ -2918,6 +4441,12 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	 * the request will ensure the vCPU gets the correct state before VM-Entry.
 	 */
 	if (enable_apicv) {
+		/*
+		 * 在以下修改kvm_lapic->apicv_active:
+		 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+		 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+		 */
 		apic->apicv_active = true;
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
@@ -2930,6 +4459,20 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	return -ENOMEM;
 }
 
+/*
+ * 在以下使用kvm_apic_has_interrupt():
+ *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+ *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+ *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+ *
+ * 核心思想:
+ * 1. 判断kvm_apic_present()
+ * 2. 根据TPR和ISR更新PPR
+ * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+ * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+ */
 int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2938,10 +4481,41 @@ int kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)
 	if (!kvm_apic_present(vcpu))
 		return -1;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (apic->guest_apic_protected)
 		return -1;
 
+	/*
+	 * 在以下使用__apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+	 *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 */
 	__apic_update_ppr(apic, &ppr);
+	/*
+	 * 在以下使用apic_has_interrupt_for_ppr():
+	 *   - arch/x86/kvm/lapic.c|945| <<apic_update_ppr>> apic_has_interrupt_for_ppr(apic, ppr) != -1)
+	 *   - arch/x86/kvm/lapic.c|3038| <<kvm_apic_has_interrupt>> return apic_has_interrupt_for_ppr(apic, ppr);
+	 *
+	 * 选出最高的irr, 不修改任何数据
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 */
 	return apic_has_interrupt_for_ppr(apic, ppr);
 }
 EXPORT_SYMBOL_GPL(kvm_apic_has_interrupt);
@@ -2968,6 +4542,11 @@ void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_ack_interrupt():
+ *   - arch/x86/kvm/irq.c|187| <<kvm_cpu_get_interrupt>> kvm_apic_ack_interrupt(v, vector);
+ *   - arch/x86/kvm/vmx/nested.c|4413| <<vmx_check_nested_events>> kvm_apic_ack_interrupt(vcpu, irq);
+ */
 void kvm_apic_ack_interrupt(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2983,8 +4562,28 @@ void kvm_apic_ack_interrupt(struct kvm_vcpu *vcpu, int vector)
 	 * because the process would deliver it through the IDT.
 	 */
 
+	/*
+	 * 在以下使用apic_clear_irr():
+	 *   - arch/x86/kvm/lapic.c|722| <<kvm_apic_clear_irr>> apic_clear_irr(vec, vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|3273| <<kvm_apic_ack_interrupt>> apic_clear_irr(vector, apic);
+	 */
 	apic_clear_irr(vector, apic);
 	if (kvm_hv_synic_auto_eoi_set(vcpu, vector)) {
+		/*
+		 * 在以下使用apic_update_ppr():
+		 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+		 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+		 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+		 *
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 根据TPR和ISR更新PPR
+		 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+		 * 如果有, 则设置KVM_REQ_EVENT
+		 */
 		/*
 		 * For auto-EOI interrupts, there might be another pending
 		 * interrupt above PPR, so check whether to raise another
@@ -2998,7 +4597,19 @@ void kvm_apic_ack_interrupt(struct kvm_vcpu *vcpu, int vector)
 		 * a concurrent interrupt injection, but that would have
 		 * triggered KVM_REQ_EVENT already.
 		 */
+		/*
+		 * 只在这里调用
+		 */
 		apic_set_isr(vector, apic);
+		/*
+		 * 在以下使用__apic_update_ppr():
+		 *   - arch/x86/kvm/lapic.c|1034| <<apic_update_ppr>> if (__apic_update_ppr(apic, &ppr) &&
+		 *   - arch/x86/kvm/lapic.c|3167| <<kvm_apic_has_interrupt>> __apic_update_ppr(apic, &ppr);
+		 *   - arch/x86/kvm/lapic.c|3240| <<kvm_apic_ack_interrupt>> __apic_update_ppr(apic, &ppr);
+		 *
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 根据TPR和ISR更新PPR
+		 */
 		__apic_update_ppr(apic, &ppr);
 	}
 
@@ -3014,6 +4625,15 @@ static int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,
 		u32 *ldr = (u32 *)(s->regs + APIC_LDR);
 		u64 icr;
 
+		/*
+		 * 在以下使用kvm_arch->x2apic_format:
+		 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+		 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+		 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+		 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+		 */
 		if (vcpu->kvm->arch.x2apic_format) {
 			if (*id != x2apic_id)
 				return -EINVAL;
@@ -3057,6 +4677,15 @@ static int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_apic_get_state():
+ *   - arch/x86/kvm/x86.c|5203| <<kvm_vcpu_ioctl_get_lapic>> return kvm_apic_get_state(vcpu, s);
+ *
+ * 122 #define KVM_APIC_REG_SIZE 0x400
+ * 123 struct kvm_lapic_state {
+ * 124         char regs[KVM_APIC_REG_SIZE];
+ * 125 };
+ */
 int kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 {
 	memcpy(s->regs, vcpu->arch.apic->regs, sizeof(*s));
@@ -3070,11 +4699,28 @@ int kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	return kvm_apic_state_fixup(vcpu, s, false);
 }
 
+/*
+ * 在以下使用kvm_apic_set_state():
+ *   - arch/x86/kvm/x86.c|5214| <<kvm_vcpu_ioctl_set_lapic>> r = kvm_apic_set_state(vcpu, s);
+ *
+ * 122 #define KVM_APIC_REG_SIZE 0x400
+ * 123 struct kvm_lapic_state {
+ * 124         char regs[KVM_APIC_REG_SIZE];
+ * 125 };
+ */
 int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 	int r;
 
+	/*
+	 * 只有pi_apicv_pre_state_restore
+	 * 在以下调用:
+	 *   - arch/x86/kvm/lapic.c|3262| <<kvm_lapic_reset>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3731| <<kvm_apic_set_state>> kvm_x86_call(apicv_pre_state_restore)(vcpu);
+	 *
+	 * 似乎是清零一些intel posted interrupt的数据
+	 */
 	kvm_x86_call(apicv_pre_state_restore)(vcpu);
 
 	/* set SPIV separately to get count of SW disabled APICs right */
@@ -3082,15 +4728,59 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 
 	r = kvm_apic_state_fixup(vcpu, s, true);
 	if (r) {
+		/*
+		 * 在以下使用kvm_recalculate_apic_map():
+		 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+		 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+		 */
 		kvm_recalculate_apic_map(vcpu->kvm);
 		return r;
 	}
 	memcpy(vcpu->arch.apic->regs, s->regs, sizeof(*s));
 
+	/*
+	 * 在以下使用kvm_arch->apic_map_dirty:
+	 *   - arch/x86/kvm/lapic.c|376| <<kvm_recalculate_apic_map>> if (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)
+	 *   - arch/x86/kvm/lapic.c|392| <<kvm_recalculate_apic_map>> if (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> atomic_cmpxchg_release(&kvm->arch.apic_map_dirty,
+	 *   - arch/x86/kvm/lapic.c|504| <<apic_set_spiv>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|517| <<kvm_apic_set_xapic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|523| <<kvm_apic_set_ldr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|529| <<kvm_apic_set_dfr>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|540| <<kvm_apic_set_x2apic_id>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3133| <<__kvm_apic_set_base>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 *   - arch/x86/kvm/lapic.c|3920| <<kvm_apic_set_state>> atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	 */
 	atomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);
+	/*
+	 * 在以下使用kvm_recalculate_apic_map():
+	 *   - arch/x86/kvm/lapic.c|2930| <<kvm_lapic_reg_write>> kvm_recalculate_apic_map(apic->vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3180| <<kvm_apic_set_base>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3497| <<kvm_lapic_reset>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3915| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 *   - arch/x86/kvm/lapic.c|3921| <<kvm_apic_set_state>> kvm_recalculate_apic_map(vcpu->kvm);
+	 */
 	kvm_recalculate_apic_map(vcpu->kvm);
 	kvm_apic_set_version(vcpu);
 
+	/*
+	 * 在以下使用apic_update_ppr():
+	 *   - arch/x86/kvm/lapic.c|951| <<kvm_apic_update_ppr>> apic_update_ppr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|958| <<apic_set_tpr>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1471| <<apic_set_eoi>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|1577| <<__apic_read>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|2865| <<kvm_lapic_reset>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3091| <<kvm_apic_ack_interrupt>> apic_update_ppr(apic);
+	 *   - arch/x86/kvm/lapic.c|3192| <<kvm_apic_set_state>> apic_update_ppr(apic);
+	 *
+	 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 * 根据TPR和ISR更新PPR
+	 * 选出最高的irr, 不修改任何数据(必须是符合PPR的)
+	 * 如果有, 则设置KVM_REQ_EVENT
+	 */
 	apic_update_ppr(apic);
 	cancel_apic_timer(apic);
 	apic->lapic_timer.expired_tscdeadline = 0;
@@ -3099,11 +4789,37 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	update_divide_count(apic);
 	__start_apic_timer(apic, APIC_TMCCT);
 	kvm_lapic_set_reg(apic, APIC_TMCCT, 0);
+	/*
+	 * 在以下使用kvm_apic_update_apicv():
+	 *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+	 */
 	kvm_apic_update_apicv(vcpu);
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active) {
 		kvm_x86_call(apicv_post_state_restore)(vcpu);
+		/*
+		 * 在以下使用vt_hwapic_isr_update():
+		 *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+		 *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+		 *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+		 *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+		 *
+		 * vmx_hwapic_isr_update()
+		 */
 		kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
 	}
+	/*
+	 * 这里是unconditionally!!
+	 */
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 
 #ifdef CONFIG_KVM_IOAPIC
@@ -3136,6 +4852,10 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
  * last entry. If yes, set EOI on guests's behalf.
  * Clear PV EOI in guest memory in any case.
  */
+/*
+ * 在以下使用apic_sync_pv_eoi_from_guest():
+ *   - arch/x86/kvm/lapic.c|4045| <<kvm_lapic_sync_from_vapic>> apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
+ */
 static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
@@ -3153,26 +4873,73 @@ static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 	 */
 	BUG_ON(!pv_eoi_enabled(vcpu));
 
+	/*
+	 * 只在这里调用
+	 */
 	if (pv_eoi_test_and_clr_pending(vcpu))
 		return;
 	vector = apic_set_eoi(apic);
 	trace_kvm_pv_eoi(apic, vector);
 }
 
+/*
+ * 在以下使用kvm_lapic_sync_from_vapic():
+ *   - arch/x86/kvm/x86.c|11450| <<vcpu_enter_guest>> kvm_lapic_sync_from_vapic(vcpu);
+ *   - arch/x86/kvm/x86.c|11463| <<vcpu_enter_guest>> kvm_lapic_sync_from_vapic(vcpu);
+ *
+ * KVM 暴露一个"PV EOI page"给 Guest
+ * Guest写入EOI时更新对应位
+ * Host 基于该位确认中断优先级恢复状态
+ */
 void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
 {
 	u32 data;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
 		apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
 
 	if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 		return;
 
+	/*
+	 *  在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+	 */
 	if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				  sizeof(u32)))
 		return;
 
+	/*
+	 * 在以下使用apic_set_tpr():
+	 *   - arch/x86/kvm/lapic.c|2310| <<kvm_lapic_reg_write(APIC_TASKPRI)>> apic_set_tpr(apic, val & 0xff);
+	 *   - arch/x86/kvm/lapic.c|2595| <<kvm_lapic_set_tpr>> apic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);
+	 *   - arch/x86/kvm/lapic.c|3314| <<kvm_lapic_sync_from_vapic>> apic_set_tpr(vcpu->arch.apic, data & 0xff);
+	 *
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 *
+	 * 关于TPR:
+	 * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+	 * CPU.低于或等于门槛的中断会被屏蔽或延后.
+	 * 举个简单数字例子:
+	 * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+	 */
 	apic_set_tpr(vcpu->arch.apic, data & 0xff);
 }
 
@@ -3182,9 +4949,40 @@ void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
  * Detect whether it's safe to enable PV EOI and
  * if yes do so.
  */
+/*
+ * 在以下使用apic_sync_pv_eoi_to_guest():
+ *   - arch/x86/kvm/lapic.c|3988| <<kvm_lapic_sync_to_vapic>> apic_sync_pv_eoi_to_guest(vcpu, apic);
+ */
 static void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 *
+	 *
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 *
+	 *
+	 * 在以下使用kvm_ioapic_handles_vector():
+	 *   - arch/x86/kvm/lapic.c|1804| <<kvm_ioapic_send_eoi>> if (!kvm_ioapic_handles_vector(apic, vector))
+	 *   - arch/x86/kvm/lapic.c|3988| <<apic_sync_pv_eoi_to_guest>> if (... kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	if (!pv_eoi_enabled(vcpu) ||
 	    /* IRR set or many bits in ISR: could be nested. */
 	    apic->irr_pending ||
@@ -3202,18 +5000,57 @@ static void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,
 	pv_eoi_set_pending(apic->vcpu);
 }
 
+/*
+ * 在以下使用kvm_lapic_sync_to_vapic():
+ *   - arch/x86/kvm/x86.c|11208| <<vcpu_enter_guest>> kvm_lapic_sync_to_vapic(vcpu);
+ */
 void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 {
 	u32 data, tpr;
 	int max_irr, max_isr;
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * 只在这里调用:
+	 * Detect whether it's safe to enable PV EOI and
+	 * if yes do so.
+	 */
 	apic_sync_pv_eoi_to_guest(vcpu, apic);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 		return;
 
+	/*
+	 * 关于PPR:
+	 * 它记录了当前CPU接受中断的优先级水平.
+	 * 只有优先级高于(或等于)PPR的中断才会被允许进入CPU.
+	 *
+	 * 关于TPR:
+	 * 它用来告诉LAPIC: 只有那些中断优先级高于这个门槛的中断,才可以被送去
+	 * CPU.低于或等于门槛的中断会被屏蔽或延后.
+	 * 举个简单数字例子:
+	 * 如果TPR设置为8,那么所有优先级为8或以下的中断都会被"暂时"盖住,不让进入处理;而优先级9或以上的中断则可以进入.
+	 */
 	tpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;
+	/*
+	 * 在以下使用apic_find_highest_irr():
+	 *   - arch/x86/kvm/lapic.c|809| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|961| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3461| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	max_irr = apic_find_highest_irr(apic);
 	if (max_irr < 0)
 		max_irr = 0;
@@ -3222,22 +5059,55 @@ void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 		max_isr = 0;
 	data = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);
 
+	/* 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+	 */
 	kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				sizeof(u32));
 }
 
+/*
+ * 处理KVM_SET_VAPIC_ADDR:
+ *   - arch/x86/kvm/x86.c|6127| <<kvm_arch_vcpu_ioctl>> r = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);
+ */
 int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
 {
 	if (vapic_addr) {
+		/*
+		 *  在以下使用kvm_lapic->vapic_cache:
+		 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+		 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+		 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+		 */
 		if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
 					&vcpu->arch.apic->vapic_cache,
 					vapic_addr, sizeof(u32)))
 			return -EINVAL;
+		/*
+		 * 在以下使用kvm_vcpu_arch->apic_attention:
+		 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+		 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+		 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+		 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+		 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+		 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+		 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+		 */
 		__set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
 	} else {
 		__clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
 	}
 
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3955| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|10328| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	vcpu->arch.apic->vapic_addr = vapic_addr;
 	return 0;
 }
@@ -3314,6 +5184,13 @@ int kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)
 	return kvm_lapic_msr_read(vcpu->arch.apic, reg, data);
 }
 
+/*
+ * 在以下使用kvm_lapic_set_pv_eoi():
+ *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+ *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+ *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+ *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+ */
 int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 {
 	u64 addr = data & ~KVM_MSR_ENABLED;
@@ -3340,6 +5217,15 @@ int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_apic_accept_events():
+ *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+ *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+ *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+ *
+ * 主要针对INIT和SIPI
+ */
 int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -3350,6 +5236,12 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		return 0;
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用kvm_check_nested_events():
+		 *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+		 */
 		r = kvm_check_nested_events(vcpu);
 		if (r < 0)
 			return r == -EBUSY ? 0 : r;
@@ -3372,6 +5264,13 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 	}
 
 	if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+		/*
+		 * 在以下使用kvm_vcpu_reset():
+		 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+		 */
 		kvm_vcpu_reset(vcpu, true);
 		if (kvm_vcpu_is_bsp(apic->vcpu))
 			kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
@@ -3382,6 +5281,14 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		if (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {
 			/* evaluate pending_events before reading the vector */
 			smp_rmb();
+			/*
+			 * 在以下使用kvm_lapic->sipi_vector:
+			 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+			 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+			 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+			 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+			 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+			 */
 			sipi_vector = apic->sipi_vector;
 			kvm_x86_call(vcpu_deliver_sipi_vector)(vcpu,
 							       sipi_vector);
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 72de14527..a6cb3658b 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -65,14 +65,71 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	bool apicv_active;
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	bool sw_enabled;
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	/* Select registers in the vAPIC cannot be read/written. */
 	bool guest_apic_protected;
+	/*
+	 * 在以下使用kvm_lapic->isr_count:
+	 *   - arch/x86/kvm/lapic.c|778| <<apic_set_isr>> ++apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|779| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+	 *   - arch/x86/kvm/lapic.c|797| <<apic_find_highest_isr>> if (!apic->isr_count)
+	 *   - arch/x86/kvm/lapic.c|824| <<apic_clear_isr>> --apic->isr_count;
+	 *   - arch/x86/kvm/lapic.c|825| <<apic_clear_isr>> BUG_ON(apic->isr_count < 0);
+	 *   - arch/x86/kvm/lapic.c|2943| <<kvm_apic_update_apicv>> apic->isr_count = 1;
+	 *   - arch/x86/kvm/lapic.c|2945| <<kvm_apic_update_apicv>> apic->isr_count = count_vectors(apic->regs + APIC_ISR);
+	 */
 	/* Number of bits set in ISR. */
 	s16 isr_count;
+	/*
+	 * 在以下使用kvm_lapic->highest_isr_cache:
+	 *   - arch/x86/kvm/lapic.c|785| <<apic_set_isr>> apic->highest_isr_cache = vec;
+	 *   - arch/x86/kvm/lapic.c|799| <<apic_find_highest_isr>> if (likely(apic->highest_isr_cache != -1))
+	 *   - arch/x86/kvm/lapic.c|800| <<apic_find_highest_isr>> return apic->highest_isr_cache;
+	 *   - arch/x86/kvm/lapic.c|826| <<apic_clear_isr>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|2947| <<kvm_apic_update_apicv>> apic->highest_isr_cache = -1;
+	 *   - arch/x86/kvm/lapic.c|3663| <<apic_sync_pv_eoi_to_guest>> if (... apic->highest_isr_cache == -1 ||
+	 *   - arch/x86/kvm/lapic.c|3665| <<apic_sync_pv_eoi_to_guest>> kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {
+	 */
 	/* The highest vector set in ISR; if -1 - invalid, must scan ISR. */
 	int highest_isr_cache;
 	/**
@@ -81,9 +138,46 @@ struct kvm_lapic {
 	 * Note: Only one register, the TPR, is used by the microcode.
 	 */
 	void *regs;
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3955| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|10328| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	gpa_t vapic_addr;
+	/*
+	 * 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3852| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3939| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+	 *   - arch/x86/kvm/lapic.c|3947| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apic->vapic_cache,
+	 */
 	struct gfn_to_hva_cache vapic_cache;
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1688| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1699| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|4093| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|4097| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.c|4104| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.h|272| <<kvm_apic_has_pending_init_or_sipi>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|289| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1552| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4242| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4246| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|4256| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4260| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|5654| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|5656| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|12255| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 */
 	unsigned long pending_events;
+	/*
+	 * 在以下使用kvm_lapic->sipi_vector:
+	 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+	 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+	 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+	 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+	 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+	 */
 	unsigned int sipi_vector;
 	int nr_lvt_entries;
 };
@@ -149,9 +243,26 @@ void kvm_lapic_exit(void);
 
 u64 kvm_lapic_readable_reg_mask(struct kvm_lapic *apic);
 
+/*
+ * 在以下使用kvm_lapic_set_irr():
+ *   - arch/x86/kvm/svm/svm.c|3844| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	apic_set_vector(vec, apic->regs + APIC_IRR);
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	/*
 	 * irr_pending must be true if any interrupt is pending; set it after
 	 * APIC_IRR to avoid race with apic_clear_irr
@@ -164,10 +275,28 @@ static inline u32 kvm_lapic_get_reg(struct kvm_lapic *apic, int reg_off)
 	return apic_get_reg(apic->regs, reg_off);
 }
 
+/*
+ * 在以下使用kvm_has_noapic_vcpu:
+ *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+ *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+ *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+ */
 DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
 
 static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_has_noapic_vcpu:
+	 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+	 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+	 */
 	if (static_branch_unlikely(&kvm_has_noapic_vcpu))
 		return vcpu->arch.apic;
 	return true;
@@ -177,6 +306,27 @@ extern struct static_key_false_deferred apic_hw_disabled;
 
 static inline bool kvm_apic_hw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	if (static_branch_unlikely(&apic_hw_disabled.key))
 		return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
 	return true;
@@ -186,6 +336,13 @@ extern struct static_key_false_deferred apic_sw_disabled;
 
 static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|484| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|485| <<apic_set_spiv>> apic->sw_enabled = enabled;
+	 *   - arch/x86/kvm/lapic.c|2935| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|240| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (static_branch_unlikely(&apic_sw_disabled.key))
 		return apic->sw_enabled;
 	return true;
@@ -203,11 +360,63 @@ static inline int kvm_lapic_enabled(struct kvm_vcpu *vcpu)
 
 static inline int apic_x2apic_mode(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
 }
 
+/*
+ * 在以下使用kvm_vcpu_apicv_active():
+ *   - arch/x86/kvm/irq.c|178| <<kvm_cpu_has_injectable_intr>> if (!is_guest_mode(v) && kvm_vcpu_apicv_active(v))
+ *   - arch/x86/kvm/lapic.c|160| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu) &&
+ *   - arch/x86/kvm/svm/avic.c|788| <<avic_pi_update_irte>> .is_guest_mode = kvm_vcpu_apicv_active(vcpu),
+ *   - arch/x86/kvm/svm/avic.c|1050| <<avic_refresh_virtual_apic_mode>> if (kvm_vcpu_apicv_active(vcpu)) {
+ *   - arch/x86/kvm/svm/avic.c|1082| <<avic_refresh_apicv_exec_ctrl>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/avic.c|1090| <<avic_vcpu_blocking>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/avic.c|1116| <<avic_vcpu_unblocking>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/nested.c|932| <<enter_svm_guest_mode>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1099| <<init_vmcb>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1208| <<init_vmcb>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1453| <<svm_vcpu_load>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|1459| <<svm_vcpu_put>> if (kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/main.c|742| <<vt_refresh_apicv_exec_ctrl>> KVM_BUG_ON(!kvm_vcpu_apicv_active(vcpu), vcpu->kvm);
+ *   - arch/x86/kvm/vmx/vmx.c|4058| <<vmx_update_msr_bitmap_x2apic>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_pin_based_exec_ctrl>> if (!kvm_vcpu_apicv_active(&vmx->vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4428| <<vmx_refresh_apicv_exec_ctrl>> if (kvm_vcpu_apicv_active(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|4497| <<vmx_tertiary_exec_control>> if (!enable_ipiv || !kvm_vcpu_apicv_active(&vmx->vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4588| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7064| <<vmx_sync_pir_to_irr>> if (!is_guest_mode(vcpu) && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7074| <<vmx_load_eoi_exitmap>> if (!kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/x86.c|11530| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) != kvm_vcpu_apicv_active(vcpu)) &&
+ *   - arch/x86/kvm/x86.c|11991| <<kvm_arch_dy_has_pending_interrupt>> return kvm_vcpu_apicv_active(vcpu) &&
+ */
 static inline bool kvm_vcpu_apicv_active(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	return lapic_in_kernel(vcpu) && vcpu->arch.apic->apicv_active;
 }
 
@@ -222,6 +431,12 @@ static inline bool kvm_apic_init_sipi_allowed(struct kvm_vcpu *vcpu)
 	       !kvm_x86_call(apic_init_signal_blocked)(vcpu);
 }
 
+/*
+ * 在以下使用kvm_lowest_prio_delivery():
+ *   - arch/x86/kvm/irq.c|433| <<kvm_irq_delivery_to_apic>> irq->dest_id == 0xff && kvm_lowest_prio_delivery(irq)) {
+ *   - arch/x86/kvm/irq.c|466| <<kvm_irq_delivery_to_apic>> if (!kvm_lowest_prio_delivery(irq)) {
+ *   - arch/x86/kvm/lapic.c|1940| <<kvm_apic_map_get_dest_lapic>> if (!kvm_lowest_prio_delivery(irq))
+ */
 static inline bool kvm_lowest_prio_delivery(struct kvm_lapic_irq *irq)
 {
 	return (irq->delivery_mode == APIC_DM_LOWEST ||
@@ -258,6 +473,27 @@ static inline enum lapic_mode kvm_apic_mode(u64 apic_base)
 
 static inline enum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	return kvm_apic_mode(vcpu->arch.apic_base);
 }
 
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 6e838cb6c..7f5bf20f1 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -1663,6 +1663,15 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	if (tdp_mmu_enabled)
 		flush = kvm_tdp_mmu_unmap_gfn_range(kvm, range, flush);
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	if (kvm_x86_ops.set_apic_access_page_addr &&
 	    range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
@@ -4382,6 +4391,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * 在以下使用handle_mmio_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|6339| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4654,6 +4667,15 @@ static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 	if (slot->flags & KVM_MEMSLOT_INVALID)
 		return RET_PF_RETRY;
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
 		/*
 		 * Don't map L1's APIC access page into L2, KVM doesn't support
@@ -4839,6 +4861,13 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 	if (!flags) {
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
 
+		/*
+		 * 在以下使用kvm_mmu_page_fault():
+		 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+		 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+		 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+		 */
 		r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
 				insn_len);
 	} else if (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {
@@ -6290,6 +6319,13 @@ static int kvm_mmu_write_protect_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return RET_PF_EMULATE;
 }
 
+/*
+ * 在以下使用kvm_mmu_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -6318,6 +6354,9 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
 			return -EFAULT;
 
+		/*
+		 * 只在这里调用handle_mmio_page_fault()
+		 */
 		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
 		if (r == RET_PF_EMULATE)
 			goto emulate;
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 75e9cfc68..6e01c9fc3 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -608,6 +608,13 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
 		kvm_pmu_call(deliver_pmi)(vcpu);
+		/*
+		 * 在以下使用kvm_apic_local_deliver():
+		 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+		 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+		 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+		 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+		 */
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
@@ -860,6 +867,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 							 select_user;
 }
 
+/*
+ * 在以下使用kvm_pmu_trigger_event():
+ *   - arch/x86/kvm/vmx/nested.c|3714| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9106| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9445| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9447| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 eventsel)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
diff --git a/arch/x86/kvm/smm.c b/arch/x86/kvm/smm.c
index 9864c0571..c0cd2bf76 100644
--- a/arch/x86/kvm/smm.c
+++ b/arch/x86/kvm/smm.c
@@ -641,6 +641,12 @@ int emulator_leave_smm(struct x86_emulate_ctxt *ctxt)
 #endif
 		ret = rsm_load_state_32(ctxt, &smram.smram32);
 
+	/*
+	 * 在以下使用kvm_leave_nested():
+	 *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+	 */
 	/*
 	 * If RSM fails and triggers shutdown, architecturally the shutdown
 	 * occurs *before* the transition to guest mode.  But due to KVM's
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index a34c5c3b1..074b2f413 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -79,6 +79,11 @@ static bool next_vm_id_wrapped = 0;
 static DEFINE_SPINLOCK(svm_vm_data_hash_lock);
 bool x2avic_enabled;
 
+/*
+ * 在以下使用avic_activate_vmcb():
+ *   - arch/x86/kvm/svm/avic.c|246| <<avic_init_vmcb>> avic_activate_vmcb(svm);
+ *   - arch/x86/kvm/svm/avic.c|1096| <<avic_refresh_virtual_apic_mode>> avic_activate_vmcb(svm);
+ */
 static void avic_activate_vmcb(struct vcpu_svm *svm)
 {
 	struct vmcb *vmcb = svm->vmcb01.ptr;
@@ -242,6 +247,11 @@ void avic_init_vmcb(struct vcpu_svm *svm, struct vmcb *vmcb)
 	vmcb->control.avic_physical_id = __sme_set(__pa(kvm_svm->avic_physical_id_table));
 	vmcb->control.avic_vapic_bar = APIC_DEFAULT_PHYS_BASE;
 
+	/*
+	 * 在以下使用avic_activate_vmcb():
+	 *   - arch/x86/kvm/svm/avic.c|246| <<avic_init_vmcb>> avic_activate_vmcb(svm);
+	 *   - arch/x86/kvm/svm/avic.c|1096| <<avic_refresh_virtual_apic_mode>> avic_activate_vmcb(svm);
+	 */
 	if (kvm_apicv_activated(svm->vcpu.kvm))
 		avic_activate_vmcb(svm);
 	else
@@ -265,6 +275,12 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
 	    (id > X2AVIC_MAX_PHYSICAL_ID)) {
 		kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_TOO_BIG);
+		/*
+		 * 在以下修改kvm_lapic->apicv_active:
+		 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+		 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+		 */
 		vcpu->arch.apic->apicv_active = false;
 		return 0;
 	}
@@ -308,6 +324,10 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * 在以下使用avic_ring_doorbell():
+ *   - arch/x86/kvm/svm/svm.c|3831| <<svm_complete_interrupt_delivery>> avic_ring_doorbell(vcpu);
+ */
 void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -329,7 +349,26 @@ void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 
 static void avic_kick_vcpu(struct kvm_vcpu *vcpu, u32 icrl)
 {
+	/*
+	 * 在以下使用kvm_lapic->irr_pending:
+	 *   - arch/x86/kvm/lapic.c|675| <<kvm_apic_update_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|704| <<apic_find_highest_irr>> if (!apic->irr_pending)
+	 *   - arch/x86/kvm/lapic.c|728| <<apic_clear_irr>> apic->irr_pending = false;
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|2940| <<kvm_apic_update_apicv>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/lapic.c|3661| <<apic_sync_pv_eoi_to_guest>> apic->irr_pending ||
+	 *   - arch/x86/kvm/lapic.h|159| <<kvm_lapic_set_irr>> apic->irr_pending = true;
+	 *   - arch/x86/kvm/svm/avic.c|332| <<avic_kick_vcpu>> if (!pv_eoi_enabled(vcpu) ||
+	 *                                               vcpu->arch.apic->irr_pending = true;
+	 */
 	vcpu->arch.apic->irr_pending = true;
+	/*
+	 * 在以下使用svm_complete_interrupt_delivery():
+	 *   - arch/x86/kvm/svm/avic.c|351| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu,
+	 *              icrl & APIC_MODE_MASK, icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|3854| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu,
+	 *              delivery_mode, trig_mode, vector);
+	 */
 	svm_complete_interrupt_delivery(vcpu,
 					icrl & APIC_MODE_MASK,
 					icrl & APIC_INT_LEVELTRIG,
@@ -471,6 +510,24 @@ static void avic_kick_target_vcpus(struct kvm *kvm, struct kvm_lapic *source,
 	 * since entered the guest will have processed pending IRQs at VMRUN.
 	 */
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用kvm_apic_match_dest():
+		 *   - arch/x86/kvm/ioapic.c|117| <<__rtc_irq_eoi_tracking_restore_one>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, e->fields.dest_id, kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
+		 *   - arch/x86/kvm/ioapic.c|191| <<ioapic_lazy_update_eoi>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, entry->fields.dest_id,
+		 *       entry->fields.dest_mode) || kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/irq.c|439| <<kvm_irq_delivery_to_apic>> if (!kvm_apic_match_dest(vcpu,
+		 *       src, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|674| <<kvm_intr_is_single_vcpu>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/irq.c|698| <<kvm_scan_ioapic_irq>> if (kvm_apic_match_dest(vcpu,
+		 *       NULL, APIC_DEST_NOSHORT, dest_id, dest_mode)) {
+		 *   - arch/x86/kvm/lapic.c|2180| <<kvm_bitmap_or_dest_vcpus>> if (!kvm_apic_match_dest(vcpu,
+		 *       NULL, irq->shorthand, irq->dest_id, irq->dest_mode))
+		 *   - arch/x86/kvm/svm/avic.c|503| <<avic_kick_target_vcpus>> if (kvm_apic_match_dest(vcpu,
+		 *       source, icrl & APIC_SHORT_MASK, dest, icrl & APIC_DEST_MASK))
+		 */
 		if (kvm_apic_match_dest(vcpu, source, icrl & APIC_SHORT_MASK,
 					dest, icrl & APIC_DEST_MASK))
 			avic_kick_vcpu(vcpu, icrl);
@@ -529,6 +586,14 @@ int avic_incomplete_ipi_interception(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_get_apicv_inhibit_reasons:
+ *   - arch/x86/kvm/svm/svm.c|5354| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+ *   - arch/x86/kvm/svm/svm.c|5594| <<svm_hardware_setup>> svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
+ *   - arch/x86/kvm/x86.c|10158| <<kvm_vcpu_apicv_activated>> kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
+ *
+ * struct kvm_x86_ops svm_x86_ops.vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+ */
 unsigned long avic_vcpu_get_apicv_inhibit_reasons(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu))
@@ -723,6 +788,11 @@ int avic_init_vcpu(struct vcpu_svm *svm)
 	return ret;
 }
 
+/*
+ * 在以下使用avic_apicv_post_state_restore():
+ *   - arch/x86/kvm/svm/svm.c|5146| <<global>> .apicv_post_state_restore = avic_apicv_post_state_restore,
+ *   - arch/x86/kvm/svm/avic.c|1053| <<avic_refresh_virtual_apic_mode>> avic_apicv_post_state_restore(vcpu);
+ */
 void avic_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 {
 	avic_handle_dfr_update(vcpu);
@@ -1016,6 +1086,16 @@ void avic_vcpu_put(struct kvm_vcpu *vcpu)
 							   AVIC_STOP_RUNNING);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+ *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+ *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+ *
+ * 在以下使用avic_refresh_virtual_apic_mode():
+ *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/svm/avic.c|1117| <<avic_refresh_apicv_exec_ctrl>> avic_refresh_virtual_apic_mode(vcpu);
+ */
 void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1033,6 +1113,11 @@ void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 		 * accordingly before re-activating.
 		 */
 		avic_apicv_post_state_restore(vcpu);
+		/*
+		 * 在以下使用avic_activate_vmcb():
+		 *   - arch/x86/kvm/svm/avic.c|246| <<avic_init_vmcb>> avic_activate_vmcb(svm);
+		 *   - arch/x86/kvm/svm/avic.c|1096| <<avic_refresh_virtual_apic_mode>> avic_activate_vmcb(svm);
+		 */
 		avic_activate_vmcb(svm);
 	} else {
 		avic_deactivate_vmcb(svm);
@@ -1040,6 +1125,12 @@ void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	vmcb_mark_dirty(vmcb, VMCB_AVIC);
 }
 
+/*
+ * 在以下使用refresh_apicv_exec_ctrl:
+ *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+ *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+ */
 void avic_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	if (!enable_apicv)
diff --git a/arch/x86/kvm/svm/hyperv.c b/arch/x86/kvm/svm/hyperv.c
index 088f6429b..8d1fdac78 100644
--- a/arch/x86/kvm/svm/hyperv.c
+++ b/arch/x86/kvm/svm/hyperv.c
@@ -14,5 +14,14 @@ void svm_hv_inject_synthetic_vmexit_post_tlb_flush(struct kvm_vcpu *vcpu)
 	svm->vmcb->control.exit_code_hi = 0;
 	svm->vmcb->control.exit_info_1 = HV_SVM_ENL_EXITCODE_TRAP_AFTER_FLUSH;
 	svm->vmcb->control.exit_info_2 = 0;
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index b7fd2e869..ec854641c 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -53,6 +53,15 @@ static void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,
 	vmcb->control.exit_info_1 &= ~0xffffffffULL;
 	vmcb->control.exit_info_1 |= fault->error_code;
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
 
@@ -123,6 +132,17 @@ static bool nested_vmcb_needs_vls_intercept(struct vcpu_svm *svm)
 	return false;
 }
 
+/*
+ * 在以下使用recalc_intercepts():
+ *   - arch/x86/kvm/svm/nested.c|873| <<nested_vmcb02_prepare_control>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/sev.c|4524| <<sev_es_init_vmcb>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.c|654| <<set_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.c|663| <<clr_dr_intercepts>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|466| <<set_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|476| <<clr_exception_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|485| <<svm_set_intercept>> recalc_intercepts(svm);
+ *   - arch/x86/kvm/svm/svm.h|494| <<svm_clr_intercept>> recalc_intercepts(svm);
+ */
 void recalc_intercepts(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *c, *h;
@@ -395,6 +415,11 @@ static bool nested_vmcb_check_save(struct kvm_vcpu *vcpu)
 	return __nested_vmcb_check_save(vcpu, save);
 }
 
+/*
+ * 在以下使用nested_vmcb_check_controls():
+ *   - arch/x86/kvm/svm/nested.c|986| <<nested_svm_vmrun>>
+ *        if (!nested_vmcb_check_save(vcpu) || !nested_vmcb_check_controls(vcpu)) {
+ */
 static bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -403,6 +428,11 @@ static bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu)
 	return __nested_vmcb_check_controls(vcpu, ctl);
 }
 
+/*
+ * 在以下使用__nested_copy_vmcb_control_to_cache():
+ *   - arch/x86/kvm/svm/nested.c|456| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu, &svm->nested.ctl, control);
+ *   - arch/x86/kvm/svm/nested.c|1852| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+ */
 static
 void __nested_copy_vmcb_control_to_cache(struct kvm_vcpu *vcpu,
 					 struct vmcb_ctrl_area_cached *to,
@@ -453,6 +483,12 @@ void __nested_copy_vmcb_control_to_cache(struct kvm_vcpu *vcpu,
 void nested_copy_vmcb_control_to_cache(struct vcpu_svm *svm,
 				       struct vmcb_control_area *control)
 {
+	/*
+	 * 在以下使用__nested_copy_vmcb_control_to_cache():
+	 *   - arch/x86/kvm/svm/nested.c|456| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu,
+	 *                                  &svm->nested.ctl, control);
+	 *   - arch/x86/kvm/svm/nested.c|1852| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+	 */
 	__nested_copy_vmcb_control_to_cache(&svm->vcpu, &svm->nested.ctl, control);
 }
 
@@ -482,6 +518,10 @@ void nested_copy_vmcb_save_to_cache(struct vcpu_svm *svm,
  * Synchronize fields that are written by the processor, so that
  * they can be copied back into the vmcb12.
  */
+/*
+ * 在以下使用nested_sync_control_from_vmcb02():
+ *   - arch/x86/kvm/svm/svm.c|4464| <<svm_vcpu_run>> nested_sync_control_from_vmcb02(svm);
+ */
 void nested_sync_control_from_vmcb02(struct vcpu_svm *svm)
 {
 	u32 mask;
@@ -516,6 +556,14 @@ void nested_sync_control_from_vmcb02(struct vcpu_svm *svm)
  * Transfer any event that L0 or L1 wanted to inject into L2 to
  * EXIT_INT_INFO.
  */
+/*
+ * 在以下使用nested_save_pending_event_to_vmcb12():
+ *   - arch/x86/kvm/svm/nested.c|1133| <<nested_svm_vmexit>> nested_save_pending_event_to_vmcb12(svm, vmcb12);
+ *
+ * 注释:
+ * Transfer any event that L0 or L1 wanted to inject into L2 to
+ * EXIT_INT_INFO.
+ */
 static void nested_save_pending_event_to_vmcb12(struct vcpu_svm *svm,
 						struct vmcb *vmcb12)
 {
@@ -575,6 +623,19 @@ static void nested_svm_transition_tlb_flush(struct kvm_vcpu *vcpu)
  * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
  * if we are emulating VM-Entry into a guest with NPT enabled.
  */
+/*
+ * 在以下使用nested_svm_load_cr3():
+ *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+ *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+ *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+ *              vmcb01->save.cr3, false, true);
+ *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+ *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+ *
+ * 注释:
+ * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+ * if we are emulating VM-Entry into a guest with NPT enabled.
+ */
 static int nested_svm_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,
 			       bool nested_npt, bool reload_pdptrs)
 {
@@ -605,6 +666,10 @@ void nested_vmcb02_compute_g_pat(struct vcpu_svm *svm)
 	svm->nested.vmcb02.ptr->save.g_pat = svm->vmcb01.ptr->save.g_pat;
 }
 
+/*
+ * 在以下使用nested_vmcb02_prepare_save():
+ *   - arch/x86/kvm/svm/nested.c|920| <<enter_svm_guest_mode>> nested_vmcb02_prepare_save(svm, vmcb12);
+ */
 static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12)
 {
 	bool new_vmcb12 = false;
@@ -700,6 +765,11 @@ static bool is_evtinj_nmi(u32 evtinj)
 	return type == SVM_EVTINJ_TYPE_NMI;
 }
 
+/*
+ * 在以下使用nested_vmcb02_prepare_control():
+ *   - arch/x86/kvm/svm/nested.c|919| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+ *   - arch/x86/kvm/svm/nested.c|1899| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+ */
 static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 					  unsigned long vmcb12_rip,
 					  unsigned long vmcb12_csbase)
@@ -715,6 +785,11 @@ static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 
 	nested_svm_transition_tlb_flush(vcpu);
 
+	/*
+	 * 在以下使用enter_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 */
 	/* Enter Guest-Mode */
 	enter_guest_mode(vcpu);
 
@@ -788,6 +863,11 @@ static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 
 	vmcb02->control.tsc_offset = vcpu->arch.tsc_offset;
 
+	/*
+	 * 在以下使用nested_svm_update_tsc_ratio_msr():
+	 *   - arch/x86/kvm/svm/nested.c|852| <<nested_vmcb02_prepare_control>> nested_svm_update_tsc_ratio_msr(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2994| <<svm_set_msr(MSR_AMD64_TSC_RATIO)>> nested_svm_update_tsc_ratio_msr(vcpu);
+	 */
 	if (guest_cpu_cap_has(vcpu, X86_FEATURE_TSCRATEMSR) &&
 	    svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
 		nested_svm_update_tsc_ratio_msr(vcpu);
@@ -880,6 +960,11 @@ static void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to
 	to_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;
 }
 
+/*
+ * 在以下使用enter_svm_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|1009| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+ *   - arch/x86/kvm/svm/svm.c|4939| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+ */
 int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 			 struct vmcb *vmcb12, bool from_vmrun)
 {
@@ -910,10 +995,37 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 
 	nested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	/*
+	 * 在以下使用nested_vmcb02_prepare_control():
+	 *   - arch/x86/kvm/svm/nested.c|919| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	 *   - arch/x86/kvm/svm/nested.c|1899| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+	 */
 	nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
 	nested_vmcb02_prepare_save(svm, vmcb12);
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+	 *              vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 *
+	 * 注释:
+	 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+	 * if we are emulating VM-Entry into a guest with NPT enabled.
+	 */
 	ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3,
 				  nested_npt_enabled(svm), from_vmrun);
 	if (ret)
@@ -932,6 +1044,10 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 	return 0;
 }
 
+/*
+ * 在以下使用nested_svm_vmrun():
+ *   - arch/x86/kvm/svm/svm.c|2242| <<vmrun_interception>> return nested_svm_vmrun(vcpu);
+ */
 int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1001,6 +1117,13 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 
 	svm->nested.nested_run_pending = 1;
 
+	/*
+	 * 在以下使用enter_svm_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1009| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+	 *   - arch/x86/kvm/svm/svm.c|4939| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+	 *
+	 * 这里进入guest mode!
+	 */
 	if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
 		goto out_exit_err;
 
@@ -1017,6 +1140,15 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 	svm->vmcb->control.exit_info_1  = 0;
 	svm->vmcb->control.exit_info_2  = 0;
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 
 out:
@@ -1062,6 +1194,15 @@ void svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb)
 	to_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;
 }
 
+/*
+ * 在以下使用nested_svm_vmexit():
+ *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+ */
 int nested_svm_vmexit(struct vcpu_svm *svm)
 {
 	struct kvm_vcpu *vcpu = &svm->vcpu;
@@ -1081,6 +1222,13 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	vmcb12 = map.hva;
 
 	/* Exit Guest-Mode */
+	/*
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	leave_guest_mode(vcpu);
 	svm->nested.vmcb12_gpa = 0;
 	WARN_ON_ONCE(svm->nested.nested_run_pending);
@@ -1117,6 +1265,14 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	vmcb12->control.exit_info_1       = vmcb02->control.exit_info_1;
 	vmcb12->control.exit_info_2       = vmcb02->control.exit_info_2;
 
+	/*
+	 * 在以下使用nested_save_pending_event_to_vmcb12():
+	 *   - arch/x86/kvm/svm/nested.c|1133| <<nested_svm_vmexit>> nested_save_pending_event_to_vmcb12(svm, vmcb12);
+	 *
+	 * 注释:
+	 * Transfer any event that L0 or L1 wanted to inject into L2 to
+	 * EXIT_INT_INFO.
+	 */
 	if (vmcb12->control.exit_code != SVM_EXIT_ERR)
 		nested_save_pending_event_to_vmcb12(svm, vmcb12);
 
@@ -1144,6 +1300,15 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 
 	kvm_nested_vmexit_handle_ibrs(vcpu);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	/*
@@ -1239,6 +1404,19 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 
 	nested_svm_uninit_mmu_context(vcpu);
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+	 *              vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 *
+	 * 注释:
+	 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+	 * if we are emulating VM-Entry into a guest with NPT enabled.
+	 */
 	rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
 	if (rc)
 		return 1;
@@ -1264,6 +1442,11 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	 * Un-inhibit the AVIC right away, so that other vCPUs can start
 	 * to benefit from it right away.
 	 */
+	/*
+	 * 在以下使用__kvm_vcpu_update_apicv():
+	 *   - arch/x86/kvm/svm/nested.c|1268| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10558| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+	 */
 	if (kvm_apicv_activated(vcpu->kvm))
 		__kvm_vcpu_update_apicv(vcpu);
 
@@ -1281,6 +1464,11 @@ static void nested_svm_triple_fault(struct kvm_vcpu *vcpu)
 	nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
 }
 
+/*
+ * 在以下使用svm_allocate_nested():
+ *   - arch/x86/kvm/svm/svm.c|240| <<svm_set_efer>> int ret = svm_allocate_nested(svm);
+ *   - arch/x86/kvm/svm/svm.c|4956| <<svm_leave_smm>> if (svm_allocate_nested(svm))
+ */
 int svm_allocate_nested(struct vcpu_svm *svm)
 {
 	struct page *vmcb02_page;
@@ -1311,6 +1499,15 @@ void svm_free_nested(struct vcpu_svm *svm)
 	if (!svm->nested.initialized)
 		return;
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	if (WARN_ON_ONCE(svm->vmcb != svm->vmcb01.ptr))
 		svm_switch_vmcb(svm, &svm->vmcb01);
 
@@ -1332,6 +1529,24 @@ void svm_free_nested(struct vcpu_svm *svm)
 	svm->nested.initialized = false;
 }
 
+/*
+ * 在以下使用kvm_leave_nested():
+ *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+ *
+ * 在以下使用kvm_x86_nested_ops->leave_nested:
+ *   - arch/x86/kvm/svm/nested.c|2093| <<global>> .leave_nested = svm_leave_nested,
+ *   - arch/x86/kvm/vmx/nested.c|8987| <<global>> .leave_nested = vmx_leave_nested,
+ *   - arch/x86/kvm/x86.h|143| <<kvm_leave_nested>> kvm_x86_ops.nested_ops->leave_nested(vcpu);
+ *
+ * 在以下使用svm_leave_nested():
+ *   - arch/x86/kvm/svm/nested.c|2093| <<global>> struct kvm_x86_nested_ops svm_nested_ops.leave_nested = svm_leave_nested,
+ *   - arch/x86/kvm/svm/nested.c|1948| <<svm_set_nested_state>> svm_leave_nested(vcpu);
+ *   - arch/x86/kvm/svm/nested.c|2009| <<svm_set_nested_state>> svm_leave_nested(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|225| <<svm_set_efer>> svm_leave_nested(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|1361| <<svm_vcpu_free>> svm_leave_nested(vcpu);
+ */
 void svm_leave_nested(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1340,8 +1555,24 @@ void svm_leave_nested(struct kvm_vcpu *vcpu)
 		svm->nested.nested_run_pending = 0;
 		svm->nested.vmcb12_gpa = INVALID_GPA;
 
+		/*
+		 * 在以下使用leave_guest_mode():
+		 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+		 */
 		leave_guest_mode(vcpu);
 
+		/*
+		 * 在以下使用svm_switch_vmcb():
+		 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+		 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+		 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+		 */
 		svm_switch_vmcb(svm, &svm->vmcb01);
 
 		nested_svm_uninit_mmu_context(vcpu);
@@ -1404,6 +1635,10 @@ static int nested_svm_intercept_ioio(struct vcpu_svm *svm)
 	return (val & mask) ? NESTED_EXIT_DONE : NESTED_EXIT_HOST;
 }
 
+/*
+ * 在以下使用nested_svm_intercept():
+ *   - arch/x86/kvm/svm/nested.c|1598| <<nested_svm_exit_handled>> vmexit = nested_svm_intercept(svm);
+ */
 static int nested_svm_intercept(struct vcpu_svm *svm)
 {
 	u32 exit_code = svm->vmcb->control.exit_code;
@@ -1448,12 +1683,27 @@ static int nested_svm_intercept(struct vcpu_svm *svm)
 	return vmexit;
 }
 
+/*
+ * 在以下使用nested_svm_exit_handled():
+ *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+ *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+ *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+ */
 int nested_svm_exit_handled(struct vcpu_svm *svm)
 {
 	int vmexit;
 
 	vmexit = nested_svm_intercept(svm);
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	if (vmexit == NESTED_EXIT_DONE)
 		nested_svm_vmexit(svm);
 
@@ -1483,6 +1733,10 @@ static bool nested_svm_is_exception_vmexit(struct kvm_vcpu *vcpu, u8 vector,
 	return (svm->nested.ctl.intercepts[INTERCEPT_EXCEPTION] & BIT(vector));
 }
 
+/*
+ * 在以下使用nested_svm_inject_exception_vmexit():
+ *   - arch/x86/kvm/svm/nested.c|1708| <<svm_check_nested_events>> nested_svm_inject_exception_vmexit(vcpu);
+ */
 static void nested_svm_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;
@@ -1516,6 +1770,15 @@ static void nested_svm_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 		WARN_ON(ex->has_payload);
 	}
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
 
@@ -1524,6 +1787,12 @@ static inline bool nested_exit_on_init(struct vcpu_svm *svm)
 	return vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_INIT);
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->check_events:
+ *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+ *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+ *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+ */
 static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -1635,6 +1904,11 @@ int nested_svm_exit_special(struct vcpu_svm *svm)
 	return NESTED_EXIT_CONTINUE;
 }
 
+/*
+ * 在以下使用nested_svm_update_tsc_ratio_msr():
+ *   - arch/x86/kvm/svm/nested.c|852| <<nested_vmcb02_prepare_control>> nested_svm_update_tsc_ratio_msr(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2994| <<svm_set_msr(MSR_AMD64_TSC_RATIO)>> nested_svm_update_tsc_ratio_msr(vcpu);
+ */
 void nested_svm_update_tsc_ratio_msr(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1645,6 +1919,10 @@ void nested_svm_update_tsc_ratio_msr(struct kvm_vcpu *vcpu)
 	svm_write_tsc_multiplier(vcpu);
 }
 
+/*
+ * 在以下使用nested_copy_vmcb_cache_to_control():
+ *   - arch/x86/kvm/svm/nested.c|2006| <<svm_get_nested_state>> nested_copy_vmcb_cache_to_control(ctl, &svm->nested.ctl);
+ */
 /* Inverse operation of nested_copy_vmcb_control_to_cache(). asid is copied too. */
 static void nested_copy_vmcb_cache_to_control(struct vmcb_control_area *dst,
 					      struct vmcb_ctrl_area_cached *from)
@@ -1748,6 +2026,14 @@ static int svm_get_nested_state(struct kvm_vcpu *vcpu,
 	return kvm_state.size;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->set_state:
+ *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+ *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+ *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+ *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+ *                                     user_kvm_nested_state, &kvm_state);
+ */
 static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state)
@@ -1811,6 +2097,12 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 		goto out_free;
 
 	ret = -EINVAL;
+	/*
+	 * 在以下使用__nested_copy_vmcb_control_to_cache():
+	 *   - arch/x86/kvm/svm/nested.c|456| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu,
+	 *                             &svm->nested.ctl, control);
+	 *   - arch/x86/kvm/svm/nested.c|1852| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+	 */
 	__nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
 	if (!__nested_vmcb_check_controls(vcpu, &ctl_cached))
 		goto out_free;
@@ -1857,7 +2149,21 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
 	nested_copy_vmcb_control_to_cache(svm, ctl);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	/*
+	 * 在以下使用nested_vmcb02_prepare_control():
+	 *   - arch/x86/kvm/svm/nested.c|919| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	 *   - arch/x86/kvm/svm/nested.c|1899| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+	 */
 	nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
 
 	/*
@@ -1867,6 +2173,19 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	 * Set it again to fix this.
 	 */
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|980| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1325| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu,
+	 *              vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2005| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu,
+	 *              vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 *
+	 * 注释:
+	 * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
+	 * if we are emulating VM-Entry into a guest with NPT enabled.
+	 */
 	ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3,
 				  nested_npt_enabled(svm), false);
 	if (WARN_ON_ONCE(ret))
@@ -1883,6 +2202,15 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->get_nested_state_pages:
+ *   - arch/x86/kvm/svm/nested.c|2230| <<global>> .get_nested_state_pages = svm_get_nested_state_pages,
+ *   - arch/x86/kvm/vmx/nested.c|8994| <<global>> .get_nested_state_pages = vmx_get_nested_state_pages,
+ *   - arch/x86/kvm/x86.c|11305| <<vcpu_enter_guest(KVM_REQ_GET_NESTED_STATE_PAGES)>>
+ *                             if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
+ *
+ * struct kvm_x86_nested_ops svm_nested_ops.get_nested_state_pages = svm_get_nested_state_pages,
+ */
 static bool svm_get_nested_state_pages(struct kvm_vcpu *vcpu)
 {
 	if (WARN_ON(!is_guest_mode(vcpu)))
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 0635bd71c..6894afa6a 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -4156,6 +4156,12 @@ static int sev_handle_vmgexit_msr_protocol(struct vcpu_svm *svm)
 		vcpu->arch.regs[VCPU_REGS_RAX] = cpuid_fn;
 		vcpu->arch.regs[VCPU_REGS_RCX] = 0;
 
+		/*
+		 * 在以下使用svm_invoke_exit_handler():
+		 *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+		 *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+		 *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+		 */
 		ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
 		if (!ret) {
 			/* Error, keep GHCB MSR value as-is */
@@ -4409,6 +4415,12 @@ int sev_handle_vmgexit(struct kvm_vcpu *vcpu)
 		ret = -EINVAL;
 		break;
 	default:
+		/*
+		 * 在以下使用svm_invoke_exit_handler():
+		 *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+		 *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+		 *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+		 */
 		ret = svm_invoke_exit_handler(vcpu, exit_code);
 	}
 
@@ -4689,6 +4701,10 @@ struct page *snp_safe_alloc_page_node(int node, gfp_t gfp)
 	return p;
 }
 
+/*
+ * 在以下使用sev_handle_rmp_fault():
+ *   - arch/x86/kvm/svm/svm.c|2006| <<npf_interception>> sev_handle_rmp_fault(vcpu, fault_address, error_code);
+ */
 void sev_handle_rmp_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code)
 {
 	struct kvm_memory_slot *slot;
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 1bfebe408..9c483338f 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -237,6 +237,11 @@ int svm_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 				svm_free_nested(svm);
 
 		} else {
+			/*
+			 * 在以下使用svm_allocate_nested():
+			 *   - arch/x86/kvm/svm/svm.c|240| <<svm_set_efer>> int ret = svm_allocate_nested(svm);
+			 *   - arch/x86/kvm/svm/svm.c|4956| <<svm_leave_smm>> if (svm_allocate_nested(svm))
+			 */
 			int ret = svm_allocate_nested(svm);
 
 			if (ret) {
@@ -864,6 +869,12 @@ void svm_copy_lbrs(struct vmcb *to_vmcb, struct vmcb *from_vmcb)
 	vmcb_mark_dirty(to_vmcb, VMCB_LBR);
 }
 
+/*
+ * 在以下使用svm_enable_lbrv():
+ *   - arch/x86/kvm/svm/sev.c|972| <<__sev_launch_update_vmsa>> svm_enable_lbrv(vcpu);
+ *   - arch/x86/kvm/svm/sev.c|2439| <<snp_launch_update_vmsa>> svm_enable_lbrv(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|923| <<svm_update_lbrv>> svm_enable_lbrv(vcpu);
+ */
 void svm_enable_lbrv(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1249,6 +1260,12 @@ static void __svm_vcpu_reset(struct kvm_vcpu *vcpu)
 		sev_es_vcpu_reset(svm);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_reset:
+ *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+ *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+ *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+ */
 static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1265,6 +1282,15 @@ static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 		__svm_vcpu_reset(vcpu);
 }
 
+/*
+ * 在以下使用svm_switch_vmcb():
+ *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+ */
 void svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)
 {
 	svm->current_vmcb = target_vmcb;
@@ -1310,6 +1336,15 @@ static int svm_vcpu_create(struct kvm_vcpu *vcpu)
 
 	svm->vmcb01.ptr = page_address(vmcb01_page);
 	svm->vmcb01.pa = __sme_set(page_to_pfn(vmcb01_page) << PAGE_SHIFT);
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|918| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1159| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1332| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1369| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1898| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1319| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	if (vmsa_page)
@@ -1400,6 +1435,12 @@ static void svm_srso_vm_init(void) { }
 static void svm_srso_vm_destroy(void) { }
 #endif
 
+/*
+ * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+ *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+ *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+ */
 static void svm_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1519,6 +1560,10 @@ static void svm_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 	}
 }
 
+/*
+ * 在以下使用svm_set_vintr():
+ *   - arch/x86/kvm/svm/svm.c|4032| <<svm_enable_irq_window>> svm_set_vintr(svm);
+ */
 static void svm_set_vintr(struct vcpu_svm *svm)
 {
 	struct vmcb_control_area *control;
@@ -1958,11 +2003,21 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 		error_code |= PFERR_PRIVATE_ACCESS;
 
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
+	/*
+	 * 在以下使用kvm_mmu_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
 				static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
 				svm->vmcb->control.insn_bytes : NULL,
 				svm->vmcb->control.insn_len);
 
+	/*
+	 * 只在这里调用sev_handle_rmp_fault()
+	 */
 	if (rc > 0 && error_code & PFERR_GUEST_RMP_MASK)
 		sev_handle_rmp_fault(vcpu, fault_address, error_code);
 
@@ -2105,6 +2160,13 @@ static int shutdown_interception(struct kvm_vcpu *vcpu)
 		if (is_smm(vcpu))
 			kvm_smm_changed(vcpu, false);
 #endif
+		/*
+		 * 在以下使用kvm_vcpu_reset():
+		 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+		 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+		 */
 		kvm_vcpu_reset(vcpu, true);
 	}
 
@@ -2197,11 +2259,19 @@ static int vmsave_interception(struct kvm_vcpu *vcpu)
 	return vmload_vmsave_interception(vcpu, false);
 }
 
+/*
+ * 在以下使用vmrun_interception():
+ *   - arch/x86/kvm/svm/svm.c|3325| <<global>> svm_exit_handlers[SVM_EXIT_VMRUN] = vmrun_interception,
+ *   - arch/x86/kvm/svm/svm.c|2282| <<emulate_svm_instr>> guest_mode_exit_codes[SVM_INSTR_VMRUN] = vmrun_interception,
+ */
 static int vmrun_interception(struct kvm_vcpu *vcpu)
 {
 	if (nested_svm_check_permissions(vcpu))
 		return 1;
 
+	/*
+	 * 只在这里调用
+	 */
 	return nested_svm_vmrun(vcpu);
 }
 
@@ -2234,6 +2304,10 @@ static int svm_instr_opcode(struct kvm_vcpu *vcpu)
 	return NONE_SVM_INSTR;
 }
 
+/*
+ * 在以下使用emulate_svm_instr():
+ *   - arch/x86/kvm/svm/svm.c|2356| <<gp_interception>> return emulate_svm_instr(vcpu, opcode);
+ */
 static int emulate_svm_instr(struct kvm_vcpu *vcpu, int opcode)
 {
 	const int guest_mode_exit_codes[] = {
@@ -2518,6 +2592,12 @@ static bool check_selective_cr0_intercepted(struct kvm_vcpu *vcpu,
 
 	if (cr0 ^ val) {
 		svm->vmcb->control.exit_code = SVM_EXIT_CR0_SEL_WRITE;
+		/*
+		 * 在以下使用nested_svm_exit_handled():
+		 *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+		 *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+		 *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+		 */
 		ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
 	}
 
@@ -2587,6 +2667,18 @@ static int cr_interception(struct kvm_vcpu *vcpu)
 			val = kvm_read_cr4(vcpu);
 			break;
 		case 8:
+			/*
+			 * 在以下使用kvm_get_cr8():
+			 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+			 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+			 */
 			val = kvm_get_cr8(vcpu);
 			break;
 		default:
@@ -2678,6 +2770,18 @@ static int cr8_write_interception(struct kvm_vcpu *vcpu)
 {
 	int r;
 
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	u8 cr8_prev = kvm_get_cr8(vcpu);
 	/* instruction emulation calls kvm_set_cr8() */
 	r = cr_interception(vcpu);
@@ -2922,6 +3026,11 @@ static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 
 		svm->tsc_ratio_msr = data;
 
+		/*
+		 * 在以下使用nested_svm_update_tsc_ratio_msr():
+		 *   - arch/x86/kvm/svm/nested.c|852| <<nested_vmcb02_prepare_control>> nested_svm_update_tsc_ratio_msr(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|2994| <<svm_set_msr(MSR_AMD64_TSC_RATIO)>> nested_svm_update_tsc_ratio_msr(vcpu);
+		 */
 		if (guest_cpu_cap_has(vcpu, X86_FEATURE_TSCRATEMSR) &&
 		    is_guest_mode(vcpu))
 			nested_svm_update_tsc_ratio_msr(vcpu);
@@ -3500,6 +3609,12 @@ static int svm_handle_invalid_exit(struct kvm_vcpu *vcpu, u64 exit_code)
 	return 0;
 }
 
+/*
+ * 在以下使用svm_invoke_exit_handler():
+ *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+ *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+ *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+ */
 int svm_invoke_exit_handler(struct kvm_vcpu *vcpu, u64 exit_code)
 {
 	if (!svm_check_exit_valid(exit_code))
@@ -3556,6 +3671,12 @@ static void svm_get_entry_info(struct kvm_vcpu *vcpu, u32 *intr_info,
 
 }
 
+/*
+ * 在以下使用kvm_x86_ops->handle_exit:
+ *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+ *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+ *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+ */
 static int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3577,6 +3698,12 @@ static int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 
 		vmexit = nested_svm_exit_special(svm);
 
+		/*
+		 * 在以下使用nested_svm_exit_handled():
+		 *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+		 *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+		 *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+		 */
 		if (vmexit == NESTED_EXIT_CONTINUE)
 			vmexit = nested_svm_exit_handled(svm);
 
@@ -3596,6 +3723,12 @@ static int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	if (exit_fastpath != EXIT_FASTPATH_NONE)
 		return 1;
 
+	/*
+	 * 在以下使用svm_invoke_exit_handler():
+	 *   - arch/x86/kvm/svm/sev.c|4159| <<sev_handle_vmgexit_msr_protocol>> ret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);
+	 *   - arch/x86/kvm/svm/sev.c|4412| <<sev_handle_vmgexit>> ret = svm_invoke_exit_handler(vcpu, exit_code);
+	 *   - arch/x86/kvm/svm/svm.c|3700| <<svm_handle_exit>> return svm_invoke_exit_handler(vcpu, exit_code);
+	 */
 	return svm_invoke_exit_handler(vcpu, exit_code);
 }
 
@@ -3701,6 +3834,13 @@ static void svm_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 				       SVM_EVTINJ_VALID | type;
 }
 
+/*
+ * 在以下使用svm_complete_interrupt_delivery():
+ *   - arch/x86/kvm/svm/avic.c|351| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu,
+ *              icrl & APIC_MODE_MASK, icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+ *   - arch/x86/kvm/svm/svm.c|3854| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu,
+ *              delivery_mode, trig_mode, vector);
+ */
 void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 				     int trig_mode, int vector)
 {
@@ -3710,6 +3850,12 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 	 */
 	bool in_guest_mode = (smp_load_acquire(&vcpu->mode) == IN_GUEST_MODE);
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	/* Note, this is called iff the local APIC is in-kernel. */
 	if (!READ_ONCE(vcpu->arch.apic->apicv_active)) {
 		/* Process the interrupt via kvm_check_and_inject_events(). */
@@ -3738,6 +3884,11 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 				  int trig_mode, int vector)
 {
+	/*
+	 * 在以下使用kvm_lapic_set_irr():
+	 *   - arch/x86/kvm/svm/svm.c|3844| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+	 *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+	 */
 	kvm_lapic_set_irr(vector, apic);
 
 	/*
@@ -3748,6 +3899,13 @@ static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 	 * will signal the doorbell if the CPU has already entered the guest.
 	 */
 	smp_mb__after_atomic();
+	/*
+	 * 在以下使用svm_complete_interrupt_delivery():
+	 *   - arch/x86/kvm/svm/avic.c|351| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu,
+	 *              icrl & APIC_MODE_MASK, icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|3854| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu,
+	 *              delivery_mode, trig_mode, vector);
+	 */
 	svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);
 }
 
@@ -3881,6 +4039,15 @@ static int svm_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 	return 1;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->enable_irq_window:
+ *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+ *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+ *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+ *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+ *
+ * struct kvm_x86_ops svm_x86_ops.enable_irq_window = svm_enable_irq_window
+ */
 static void svm_enable_irq_window(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4041,19 +4208,51 @@ static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用sync_lapic_to_cr8():
+ *   - arch/x86/kvm/svm/svm.c|4277| <<svm_vcpu_run>> sync_lapic_to_cr8(vcpu);
+ */
 static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	u64 cr8;
 
+	/*
+	 * 这里之前是这样的.
+	 * -       if (nested_svm_virtualize_tpr(vcpu) ||
+	 * -           kvm_vcpu_apicv_active(vcpu))
+	 * +       if (nested_svm_virtualize_tpr(vcpu))
+	 *               return;
+	 *
+	 * OVMF不会在启动的时候写TPR.
+	 * 所以用pre-reset的value.
+	 * APICv被inhibit了.
+	 * 这样Windows初始化的时候会有问题.
+	 */
 	if (nested_svm_virtualize_tpr(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	cr8 = kvm_get_cr8(vcpu);
 	svm->vmcb->control.int_ctl &= ~V_TPR_MASK;
 	svm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;
 }
 
+/*
+ * 在以下使用svm_complete_soft_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4279| <<svm_complete_interrupts>> svm_complete_soft_interrupt(vcpu, vector, type);
+ */
 static void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 					int type)
 {
@@ -4086,6 +4285,11 @@ static void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 		kvm_rip_write(vcpu, svm->soft_int_old_rip);
 }
 
+/*
+ * 在以下使用svm_complete_interrupts():
+ *   - arch/x86/kvm/svm/svm.c|4341| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4571| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+ */
 static void svm_complete_interrupts(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4147,9 +4351,27 @@ static void svm_complete_interrupts(struct kvm_vcpu *vcpu)
 		break;
 	}
 	case SVM_EXITINTINFO_TYPE_INTR:
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, vector, false);
 		break;
 	case SVM_EXITINTINFO_TYPE_SOFT:
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, vector, true);
 		break;
 	default:
@@ -4166,6 +4388,11 @@ static void svm_cancel_injection(struct kvm_vcpu *vcpu)
 	control->exit_int_info = control->event_inj;
 	control->exit_int_info_err = control->event_inj_err;
 	control->event_inj = 0;
+	/*
+	 * 在以下使用svm_complete_interrupts():
+	 *   - arch/x86/kvm/svm/svm.c|4341| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|4571| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+	 */
 	svm_complete_interrupts(vcpu);
 }
 
@@ -4177,6 +4404,10 @@ static int svm_vcpu_pre_run(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用svm_exit_handlers_fastpath():
+ *   - arch/x86/kvm/svm/svm.c|4573| <<svm_vcpu_run>> return svm_exit_handlers_fastpath(vcpu);
+ */
 static fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4238,6 +4469,12 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 
 	trace_kvm_entry(vcpu, force_immediate_exit);
 
+	/*
+	 * struct vcpu_svm *svm:
+	 * -> struct vmcb *vmcb;
+	 *    -> struct vmcb_control_area control;
+	 *    -> struct vmcb_save_area save;
+	 */
 	svm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];
 	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
@@ -4323,8 +4560,24 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 		vcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;
 		vcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;
 	}
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	vcpu->arch.regs_dirty = 0;
 
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 		kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
 
@@ -4337,6 +4590,13 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 
 	/* Any pending NMI will happen here */
 
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	if (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))
 		kvm_after_interrupt(vcpu);
 
@@ -4374,8 +4634,16 @@ static __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 
 	trace_kvm_exit(vcpu, KVM_ISA_SVM);
 
+	/*
+	 * 在以下使用svm_complete_interrupts():
+	 *   - arch/x86/kvm/svm/svm.c|4341| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|4571| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+	 */
 	svm_complete_interrupts(vcpu);
 
+	/*
+	 * 只在这里调用
+	 */
 	return svm_exit_handlers_fastpath(vcpu);
 }
 
@@ -4439,6 +4707,12 @@ static bool svm_has_emulated_msr(struct kvm *kvm, u32 index)
 	return true;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+ *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+ *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+ *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+ */
 static void svm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4540,6 +4814,12 @@ static const struct __x86_intercept {
 #undef POST_EX
 #undef POST_MEM
 
+/*
+ * 在以下使用kvm_x86_ops->check_intercept:
+ *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+ *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+ *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+ */
 static int svm_check_intercept(struct kvm_vcpu *vcpu,
 			       struct x86_instruction_info *info,
 			       enum x86_intercept_stage stage,
@@ -4651,6 +4931,12 @@ static int svm_check_intercept(struct kvm_vcpu *vcpu,
 	if (static_cpu_has(X86_FEATURE_NRIPS))
 		vmcb->control.next_rip  = info->next_rip;
 	vmcb->control.exit_code = icpt_info.exit_code;
+	/*
+	 * 在以下使用nested_svm_exit_handled():
+	 *   - arch/x86/kvm/svm/svm.c|2540| <<check_selective_cr0_intercepted>> ret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);
+	 *   - arch/x86/kvm/svm/svm.c|3630| <<svm_handle_exit>> vmexit = nested_svm_exit_handled(svm);
+	 *   - arch/x86/kvm/svm/svm.c|4783| <<svm_check_intercept>> vmexit = nested_svm_exit_handled(svm);
+	 */
 	vmexit = nested_svm_exit_handled(svm);
 
 	ret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED
@@ -4662,6 +4948,13 @@ static int svm_check_intercept(struct kvm_vcpu *vcpu,
 
 static void svm_handle_exit_irqoff(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	if (to_svm(vcpu)->vmcb->control.exit_code == SVM_EXIT_INTR)
 		vcpu->arch.at_instruction_boundary = true;
 }
@@ -4781,6 +5074,11 @@ static int svm_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	if (kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.hsave_msr), &map_save))
 		goto unmap_map;
 
+	/*
+	 * 在以下使用svm_allocate_nested():
+	 *   - arch/x86/kvm/svm/svm.c|240| <<svm_set_efer>> int ret = svm_allocate_nested(svm);
+	 *   - arch/x86/kvm/svm/svm.c|4956| <<svm_leave_smm>> if (svm_allocate_nested(svm))
+	 */
 	if (svm_allocate_nested(svm))
 		goto unmap_save;
 
@@ -4800,6 +5098,11 @@ static int svm_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	vmcb12 = map.hva;
 	nested_copy_vmcb_control_to_cache(svm, &vmcb12->control);
 	nested_copy_vmcb_save_to_cache(svm, &vmcb12->save);
+	/*
+	 * 在以下使用enter_svm_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1009| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+	 *   - arch/x86/kvm/svm/svm.c|4939| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+	 */
 	ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
 
 	if (ret)
@@ -5188,6 +5491,10 @@ static struct kvm_x86_ops svm_x86_ops __initdata = {
  * memory encryption support and override the default MMIO mask if
  * memory encryption is enabled.
  */
+/*
+ * 在以下使用svm_adjust_mmio_mask():
+ *   - arch/x86/kvm/svm/svm.c|5609| <<svm_hardware_setup>> svm_adjust_mmio_mask();
+ */
 static __init void svm_adjust_mmio_mask(void)
 {
 	unsigned int enc_bit, mask_bit;
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 58b9d168e..de432e4f1 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -763,6 +763,15 @@ static inline int nested_svm_simple_vmexit(struct vcpu_svm *svm, u32 exit_code)
 	svm->vmcb->control.exit_code   = exit_code;
 	svm->vmcb->control.exit_info_1 = 0;
 	svm->vmcb->control.exit_info_2 = 0;
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1103| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1601| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1662| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|766| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	return nested_svm_vmexit(svm);
 }
 
diff --git a/arch/x86/kvm/vmx/capabilities.h b/arch/x86/kvm/vmx/capabilities.h
index 5316c27f6..9a9e3b997 100644
--- a/arch/x86/kvm/vmx/capabilities.h
+++ b/arch/x86/kvm/vmx/capabilities.h
@@ -191,6 +191,32 @@ static inline bool cpu_has_vmx_apic_register_virt(void)
 
 static inline bool cpu_has_vmx_virtual_intr_delivery(void)
 {
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	return vmcs_config.cpu_based_2nd_exec_ctrl &
 		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
 }
@@ -219,6 +245,10 @@ static inline bool cpu_has_vmx_vmfunc(void)
 		SECONDARY_EXEC_ENABLE_VMFUNC;
 }
 
+/*
+ * 在以下使用cpu_has_vmx_shadow_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|7893| <<cpu_has_vmx_shadow_vmcs>> if (!cpu_has_vmx_shadow_vmcs())
+ */
 static inline bool cpu_has_vmx_shadow_vmcs(void)
 {
 	/* check if the cpu supports writing r/o exit information fields */
@@ -270,6 +300,12 @@ static inline bool cpu_has_vmx_bus_lock_detection(void)
 	    SECONDARY_EXEC_BUS_LOCK_DETECTION;
 }
 
+/*
+ * 在以下使用cpu_has_vmx_apicv():
+ *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+ *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+ *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+ */
 static inline bool cpu_has_vmx_apicv(void)
 {
 	return cpu_has_vmx_apic_register_virt() &&
diff --git a/arch/x86/kvm/vmx/common.h b/arch/x86/kvm/vmx/common.h
index bc5ece765..754e7a9a7 100644
--- a/arch/x86/kvm/vmx/common.h
+++ b/arch/x86/kvm/vmx/common.h
@@ -40,6 +40,15 @@ struct vcpu_vt {
 	union vmx_exit_reason exit_reason;
 
 	unsigned long	exit_qualification;
+	/*
+	 * 在以下使用vcpu_vt->exit_intr_info:
+	 *   - arch/x86/kvm/vmx/tdx.c|957| <<tdx_vcpu_enter_exit>> vt->exit_intr_info = tdx->vp_enter_args.r9;
+	 *   - arch/x86/kvm/vmx/vmx.c|7343| <<vmx_vcpu_run>> vmx->vt.exit_intr_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.h|368| <<vmx_get_intr_info>> vt->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	 *   - arch/x86/kvm/vmx/vmx.h|370| <<vmx_get_intr_info>> return vt->exit_intr_info;
+	 *
+	 * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+	 */
 	u32		exit_intr_info;
 
 	/*
@@ -105,6 +114,13 @@ static inline int __vmx_handle_ept_violation(struct kvm_vcpu *vcpu, gpa_t gpa,
 	if (vt_is_tdx_private_gpa(vcpu->kvm, gpa))
 		error_code |= PFERR_PRIVATE_ACCESS;
 
+	/*
+	 * 在以下使用kvm_mmu_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
diff --git a/arch/x86/kvm/vmx/main.c b/arch/x86/kvm/vmx/main.c
index dbab1c15b..2bb865ddf 100644
--- a/arch/x86/kvm/vmx/main.c
+++ b/arch/x86/kvm/vmx/main.c
@@ -83,6 +83,12 @@ static void vt_vcpu_free(struct kvm_vcpu *vcpu)
 	vmx_vcpu_free(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_reset:
+ *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+ *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+ *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+ */
 static void vt_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	if (is_td_vcpu(vcpu)) {
@@ -115,6 +121,12 @@ static void vt_update_cpu_dirty_logging(struct kvm_vcpu *vcpu)
 	vmx_update_cpu_dirty_logging(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+ *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+ *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+ */
 static void vt_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu)) {
@@ -151,6 +163,12 @@ static fastpath_t vt_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 	return vmx_vcpu_run(vcpu, run_flags);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->handle_exit:
+ *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+ *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+ *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+ */
 static int vt_handle_exit(struct kvm_vcpu *vcpu,
 			  enum exit_fastpath_completion fastpath)
 {
@@ -273,6 +291,12 @@ static bool vt_apic_init_signal_blocked(struct kvm_vcpu *vcpu)
 	return vmx_apic_init_signal_blocked(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->set_virtual_apic_mode:
+ *   - arch/x86/kvm/svm/svm.c|5353| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/vmx/main.c|985| <<global>> .set_virtual_apic_mode = vt_op(set_virtual_apic_mode),
+ *   - arch/x86/kvm/lapic.c|3817| <<__kvm_apic_set_base>> kvm_x86_call(set_virtual_apic_mode)(vcpu);
+ */
 static void vt_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	/* Only x2APIC mode is supported for TD. */
@@ -282,6 +306,15 @@ static void vt_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	return vmx_set_virtual_apic_mode(vcpu);
 }
 
+/*
+ * 在以下使用vt_hwapic_isr_update():
+ *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+ *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+ *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ */
 static void vt_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 {
 	if (is_td_vcpu(vcpu))
@@ -310,6 +343,12 @@ static void vt_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
 	vmx_deliver_interrupt(apic, delivery_mode, trig_mode, vector);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->vcpu_after_set_cpuid:
+ *   - arch/x86/kvm/svm/svm.c|5231| <<global>> .vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,
+ *   - arch/x86/kvm/vmx/main.c|979| <<global>> .vcpu_after_set_cpuid = vt_op(vcpu_after_set_cpuid),
+ *   - arch/x86/kvm/cpuid.c|465| <<kvm_vcpu_after_set_cpuid>> kvm_x86_call(vcpu_after_set_cpuid)(vcpu);
+ */
 static void vt_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu))
@@ -688,6 +727,11 @@ static void vt_get_entry_info(struct kvm_vcpu *vcpu, u32 *intr_info, u32 *error_
 	vmx_get_entry_info(vcpu, intr_info, error_code);
 }
 
+/*
+ * 在以下调用:
+ *   - arch/x86/kvm/x86.c|8806| <<prepare_emulation_failure_exit>> kvm_x86_call(get_exit_info)(vcpu, (u32 *)&info[0], &info[1], &info[2],
+ *   - arch/x86/kvm/x86.c|8871| <<kvm_prepare_event_vectoring_exit>> kvm_x86_call(get_exit_info)(vcpu, &reason, &info1, &info2,
+ */
 static void vt_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 			u64 *info1, u64 *info2, u32 *intr_info, u32 *error_code)
 {
@@ -716,6 +760,12 @@ static void vt_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 	vmx_set_apic_access_page_addr(vcpu);
 }
 
+/*
+ * 在以下使用refresh_apicv_exec_ctrl:
+ *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+ *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+ */
 static void vt_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	if (is_td_vcpu(vcpu)) {
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index b8ea19691..89f94390b 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -20,9 +20,53 @@
 #include "vmx.h"
 #include "smm.h"
 
+/*
+ * kvm_vcpu_reset()
+ * -> if (is_guest_mode(vcpu))
+ *        kvm_leave_nested(vcpu);
+ * -> kvm_lapic_reset()
+ *    -> if (apic->apicv_active)
+ *           kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ * -> WARN_ON_ONCE(is_guest_mode(vcpu) || is_smm(vcpu));
+ *
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_LAPIC)
+ * -> kvm_vcpu_ioctl_set_lapic()
+ *    -> kvm_apic_set_state()
+ *       -> if (apic->apicv_active)
+ *              kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ */
+
+/*
+ * 在以下使用enable_shadow_vmcs:
+ *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+ *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+ *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+ *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+ *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+ *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+ *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+ *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+ */
 static bool __read_mostly enable_shadow_vmcs = 1;
 module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
 
+/*
+ * 在以下使用nested_early_check:
+ *   - arch/x86/kvm/vmx/nested.c|44| <<global>> static bool __read_mostly nested_early_check = 0;
+ *   - arch/x86/kvm/vmx/nested.c|45| <<global>> module_param(nested_early_check, bool, S_IRUGO);
+ *   - arch/x86/kvm/vmx/nested.c|2573| <<prepare_vmcs02_constant_state>> if (enable_ept && nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|3586| <<nested_vmx_check_vmentry_hw>> if (!nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|3955| <<nested_vmx_enter_non_root_mode>> if (!enable_ept && !nested_early_check)
+ *   - arch/x86/kvm/vmx/nested.c|5748| <<__nested_vmx_vmexit>> WARN_ON_ONCE(nested_early_check);
+ */
 static bool __read_mostly nested_early_check = 0;
 module_param(nested_early_check, bool, S_IRUGO);
 
@@ -45,6 +89,14 @@ enum {
 	VMX_VMWRITE_BITMAP,
 	VMX_BITMAP_NR
 };
+/*
+ * 在以下使用vmx_bitmap[VMX_BITMAP_NR]:
+ *   - arch/x86/kvm/vmx/nested.c|8139| <<nested_vmx_hardware_setup>> vmx_bitmap[i] = (unsigned long *)
+ *   - arch/x86/kvm/vmx/nested.c|8141| <<nested_vmx_hardware_setup>> if (!vmx_bitmap[i]) {
+ *   - arch/x86/kvm/vmx/nested.c|68| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+ *   - arch/x86/kvm/vmx/nested.c|69| <<vmx_vmwrite_bitmap>> #define vmx_vmwrite_bitmap (vmx_bitmap[VMX_VMWRITE_BITMAP])
+ *   - arch/x86/kvm/vmx/nested.c|8101| <<nested_vmx_hardware_unsetup>> free_page((unsigned long )vmx_bitmap[i]);
+ */
 static unsigned long *vmx_bitmap[VMX_BITMAP_NR];
 
 #define vmx_vmread_bitmap                    (vmx_bitmap[VMX_VMREAD_BITMAP])
@@ -68,6 +120,10 @@ static struct shadow_vmcs_field shadow_read_write_fields[] = {
 static int max_shadow_read_write_fields =
 	ARRAY_SIZE(shadow_read_write_fields);
 
+/*
+ * 在以下使用init_vmcs_shadow_fields():
+ *   - arch/x86/kvm/vmx/nested.c|8147| <<nested_vmx_hardware_setup>> init_vmcs_shadow_fields();
+ */
 static void init_vmcs_shadow_fields(void)
 {
 	int i, j;
@@ -125,6 +181,12 @@ static void init_vmcs_shadow_fields(void)
 				continue;
 			break;
 		case GUEST_INTR_STATUS:
+			/*
+			 * 在以下使用cpu_has_vmx_apicv():
+			 *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+			 *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+			 *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+			 */
 			if (!cpu_has_vmx_apicv())
 				continue;
 			break;
@@ -176,6 +238,19 @@ static int nested_vmx_failValid(struct kvm_vcpu *vcpu,
 			    X86_EFLAGS_SF | X86_EFLAGS_OF))
 			| X86_EFLAGS_ZF);
 	get_vmcs12(vcpu)->vm_instruction_error = vm_instruction_error;
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	/*
 	 * We don't need to force sync to shadow VMCS because
 	 * VM_INSTRUCTION_ERROR is not shadowed. Enlightened VMCS 'shadows' all
@@ -202,6 +277,13 @@ static int nested_vmx_fail(struct kvm_vcpu *vcpu, u32 vm_instruction_error)
 	return nested_vmx_failValid(vcpu, vm_instruction_error);
 }
 
+/*
+ * 在以下使用nested_vmx_abort():
+ *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+ *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+ *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+ */
 static void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)
 {
 	/* TODO: not to reset guest simply here. */
@@ -214,15 +296,46 @@ static inline bool vmx_control_verify(u32 control, u32 low, u32 high)
 	return fixed_bits_valid(control, low, high);
 }
 
+/*
+ * 在以下使用vmx_control_msr():
+ *   - arch/x86/kvm/vmx/nested.c|1079| <<nested_vmx_max_atomic_switch_msrs>> u64 vmx_misc = vmx_control_msr(vmx->nested.msrs.misc_low,
+ *   - arch/x86/kvm/vmx/nested.c|1617| <<vmx_restore_control_msr>> supported = vmx_control_msr(*lowp, *highp);
+ *   - arch/x86/kvm/vmx/nested.c|1647| <<vmx_restore_vmx_misc>> u64 vmx_misc = vmx_control_msr(vmcs_config.nested.misc_low,
+ *   - arch/x86/kvm/vmx/nested.c|1683| <<vmx_restore_vmx_ept_vpid_cap>> u64 vmx_ept_vpid_cap = vmx_control_msr(vmcs_config.nested.ept_caps,
+ *   - arch/x86/kvm/vmx/nested.c|1800| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->pinbased_ctls_low, msrs->pinbased_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1808| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->procbased_ctls_low, msrs->procbased_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1816| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->exit_ctls_low, msrs->exit_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1824| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->entry_ctls_low, msrs->entry_ctls_high);
+ *   - arch/x86/kvm/vmx/nested.c|1831| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->misc_low, msrs->misc_high);
+ *   - arch/x86/kvm/vmx/nested.c|1851| <<vmx_get_vmx_msr>> *pdata = vmx_control_msr(msrs->secondary_ctls_low, msrs->secondary_ctls_high);
+ */
 static inline u64 vmx_control_msr(u32 low, u32 high)
 {
 	return low | ((u64)high << 32);
 }
 
+/*
+ * 在以下使用vmx_disable_shadow_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|498| <<free_nested>> vmx_disable_shadow_vmcs(vmx);
+ *   - arch/x86/kvm/vmx/nested.c|6317| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+ */
 static void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)
 {
 	secondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_SHADOW_VMCS);
 	vmcs_write64(VMCS_LINK_POINTER, INVALID_GPA);
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	vmx->nested.need_vmcs12_to_shadow_sync = false;
 }
 
@@ -271,6 +384,10 @@ static bool nested_evmcs_handle_vmclear(struct kvm_vcpu *vcpu, gpa_t vmptr)
 #endif
 }
 
+/*
+ * 在以下使用vmx_sync_vmcs_host_state():
+ *   - arch/x86/kvm/vmx/nested.c|315| <<vmx_switch_vmcs>> vmx_sync_vmcs_host_state(vmx, prev);
+ */
 static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 				     struct loaded_vmcs *prev)
 {
@@ -282,6 +399,13 @@ static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 	src = &prev->host_state;
 	dest = &vmx->loaded_vmcs->host_state;
 
+	/*
+	 * 在以下使用vmx_set_host_fs_gs():
+	 *   - arch/x86/kvm/vmx/nested.c|285| <<vmx_sync_vmcs_host_state>> vmx_set_host_fs_gs(dest,
+	 *                     src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
+	 *   - arch/x86/kvm/vmx/vmx.c|1303| <<vmx_prepare_switch_to_guest>> vmx_set_host_fs_gs(host_state,
+	 *                     fs_sel, gs_sel, fs_base, gs_base);
+	 */
 	vmx_set_host_fs_gs(dest, src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
 	dest->ldt_sel = src->ldt_sel;
 #ifdef CONFIG_X86_64
@@ -290,6 +414,15 @@ static void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,
 #endif
 }
 
+/*
+ * 在以下使用vmx_switch_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|337| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3587| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+ *   - arch/x86/kvm/vmx/nested.c|3593| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3598| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|3681| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ *   - arch/x86/kvm/vmx/nested.c|5083| <<__nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+ */
 static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -302,12 +435,28 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 	cpu = get_cpu();
 	prev = vmx->loaded_vmcs;
 	vmx->loaded_vmcs = vmcs;
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 	vmx_sync_vmcs_host_state(vmx, prev);
 	put_cpu();
 
 	vcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	/*
 	 * All lazily updated registers will be reloaded from VMCS12 on both
 	 * vmentry and vmexit.
@@ -315,11 +464,23 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 	vcpu->arch.regs_dirty = 0;
 }
 
+/*
+ * 在以下使用nested_put_vmcs12_pages():
+ *   - arch/x86/kvm/vmx/nested.c|361| <<free_nested>> nested_put_vmcs12_pages(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|5107| <<__nested_vmx_vmexit>> nested_put_vmcs12_pages(vcpu);
+ */
 static void nested_put_vmcs12_pages(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
 	kvm_vcpu_unmap(vcpu, &vmx->nested.apic_access_page_map);
+	/*
+	 * 在以下使用nested_vmx->virtual_apic_map:
+	 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+	 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+	 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+	 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+	 */
 	kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
 	kvm_vcpu_unmap(vcpu, &vmx->nested.pi_desc_map);
 	vmx->nested.pi_desc = NULL;
@@ -329,6 +490,11 @@ static void nested_put_vmcs12_pages(struct kvm_vcpu *vcpu)
  * Free whatever needs to be freed from vmx->nested when L1 goes down, or
  * just stops using VMX.
  */
+/*
+ * 在以下使用free_nested():
+ *   - arch/x86/kvm/vmx/nested.c|5538| <<handle_vmxoff>> free_nested(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6806| <<vmx_leave_nested>> free_nested(vcpu);
+ */
 static void free_nested(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -345,9 +511,43 @@ static void free_nested(struct kvm_vcpu *vcpu)
 	vmx->nested.smm.vmxon = false;
 	vmx->nested.vmxon_ptr = INVALID_GPA;
 	free_vpid(vmx->nested.vpid02);
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	vmx->nested.posted_intr_nv = -1;
 	vmx->nested.current_vmptr = INVALID_GPA;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用vmx_disable_shadow_vmcs():
+		 *   - arch/x86/kvm/vmx/nested.c|498| <<free_nested>> vmx_disable_shadow_vmcs(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+		 */
 		vmx_disable_shadow_vmcs(vmx);
 		vmcs_clear(vmx->vmcs01.shadow_vmcs);
 		free_vmcs(vmx->vmcs01.shadow_vmcs);
@@ -371,6 +571,10 @@ static void free_nested(struct kvm_vcpu *vcpu)
  * Ensure that the current vmcs of the logical processor is the
  * vmcs01 of the vcpu before calling free_nested().
  */
+/*
+ * 在以下使用nested_vmx_free_vcpu():
+ *   - arch/x86/kvm/vmx/vmx.c|8131| <<vmx_vcpu_free>> nested_vmx_free_vcpu(vcpu);
+ */
 void nested_vmx_free_vcpu(struct kvm_vcpu *vcpu)
 {
 	vcpu_load(vcpu);
@@ -461,6 +665,11 @@ static void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,
 	vmcs12->guest_physical_address = fault->address;
 }
 
+/*
+ * 在以下使用nested_ept_new_eptp():
+ *   - arch/x86/kvm/vmx/nested.c|627| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|7013| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+ */
 static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -472,16 +681,34 @@ static void nested_ept_new_eptp(struct kvm_vcpu *vcpu)
 				nested_ept_get_eptp(vcpu));
 }
 
+/*
+ * 在以下使用nested_ept_init_mmu_context():
+ *   - arch/x86/kvm/vmx/nested.c|3098| <<prepare_vmcs02>> nested_ept_init_mmu_context(vcpu);
+ */
 static void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)
 {
 	WARN_ON(mmu_is_nested(vcpu));
 
 	vcpu->arch.mmu = &vcpu->arch.guest_mmu;
+	/*
+	 * 在以下使用nested_ept_new_eptp():
+	 *   - arch/x86/kvm/vmx/nested.c|627| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7013| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+	 */
 	nested_ept_new_eptp(vcpu);
 	vcpu->arch.mmu->get_guest_pgd     = nested_ept_get_eptp;
 	vcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;
 	vcpu->arch.mmu->get_pdptr         = kvm_pdptr_read;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_mmu *mmu;
+	 *    -> struct kvm_mmu root_mmu;
+	 *    -> struct kvm_mmu guest_mmu;
+	 *    -> struct kvm_mmu nested_mmu;
+	 *    -> struct kvm_mmu *walk_mmu;
+	 */
 	vcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;
 }
 
@@ -563,6 +790,21 @@ static int nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,
  * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
  * only the "disable intercept" case needs to be handled.
  */
+/*
+ * 在以下使用nested_vmx_disable_intercept_for_x2apic_msr():
+ *   - arch/x86/kvm/vmx/nested.c|828| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_disable_intercept_for_x2apic_msr(
+ *                   msr_bitmap_l1, msr_bitmap_l0, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_R | MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/nested.c|844| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_disable_intercept_for_x2apic_msr(
+ *                   msr_bitmap_l1, msr_bitmap_l0, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/nested.c|848| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_disable_intercept_for_x2apic_msr(
+ *                   msr_bitmap_l1, msr_bitmap_l0, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);
+ *
+ * 注释:
+ * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1
+ * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
+ * only the "disable intercept" case needs to be handled.
+ * L1不trap的话, L0才不trap
+ */
 static void nested_vmx_disable_intercept_for_x2apic_msr(unsigned long *msr_bitmap_l1,
 							unsigned long *msr_bitmap_l0,
 							u32 msr, int type)
@@ -570,10 +812,17 @@ static void nested_vmx_disable_intercept_for_x2apic_msr(unsigned long *msr_bitma
 	if (type & MSR_TYPE_R && !vmx_test_msr_bitmap_read(msr_bitmap_l1, msr))
 		vmx_clear_msr_bitmap_read(msr_bitmap_l0, msr);
 
+	/*
+	 * L1不trap的话, L0才不trap
+	 */
 	if (type & MSR_TYPE_W && !vmx_test_msr_bitmap_write(msr_bitmap_l1, msr))
 		vmx_clear_msr_bitmap_write(msr_bitmap_l0, msr);
 }
 
+/*
+ * 在以下使用enable_x2apic_msr_intercepts():
+ *   - arch/x86/kvm/vmx/nested.c|718| <<nested_vmx_prepare_msr_bitmap>> enable_x2apic_msr_intercepts(msr_bitmap_l0);
+ */
 static inline void enable_x2apic_msr_intercepts(unsigned long *msr_bitmap)
 {
 	int msr;
@@ -601,6 +850,17 @@ void nested_vmx_set_msr_##rw##_intercept(struct vcpu_vmx *vmx,			\
 BUILD_NVMX_MSR_INTERCEPT_HELPER(read)
 BUILD_NVMX_MSR_INTERCEPT_HELPER(write)
 
+/*
+ * 在以下使用nested_vmx_set_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/nested.c|757| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|760| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|763| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|766| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|769| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|772| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|775| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ *   - arch/x86/kvm/vmx/nested.c|778| <<nested_vmx_prepare_msr_bitmap>> nested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,
+ */
 static inline void nested_vmx_set_intercept_for_msr(struct vcpu_vmx *vmx,
 						    unsigned long *msr_bitmap_l1,
 						    unsigned long *msr_bitmap_l0,
@@ -618,6 +878,10 @@ static inline void nested_vmx_set_intercept_for_msr(struct vcpu_vmx *vmx,
  * Merge L0's and L1's MSR bitmap, return false to indicate that
  * we do not use the hardware.
  */
+/*
+ * 在以下使用nested_vmx_prepare_msr_bitmap():
+ *   - arch/x86/kvm/vmx/nested.c|3530| <<nested_get_vmcs12_pages>> if (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))
+ */
 static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 						 struct vmcs12 *vmcs12)
 {
@@ -658,8 +922,19 @@ static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 	 * 4-byte writes on 32-bit systems) up front to enable intercepts for
 	 * the x2APIC MSR range and selectively toggle those relevant to L2.
 	 */
+	/*
+	 * 首先把全部APIC的MSR都intercept了
+	 */
 	enable_x2apic_msr_intercepts(msr_bitmap_l0);
 
+	/*
+	 * 在以下使用nested_cpu_has_virt_x2apic_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|831| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|1034| <<nested_vmx_check_apicv_controls>> if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|1044| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *
+	 * 如果L1支持L2的APIC虚拟化
+	 */
 	if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
 		if (nested_cpu_has_apic_reg_virt(vmcs12)) {
 			/*
@@ -675,12 +950,36 @@ static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 			}
 		}
 
+		/*
+		 * 注释:
+		 * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1
+		 * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
+		 * only the "disable intercept" case needs to be handled.
+		 * L1不trap的话, L0才不trap
+		 */
 		nested_vmx_disable_intercept_for_x2apic_msr(
 			msr_bitmap_l1, msr_bitmap_l0,
 			X2APIC_MSR(APIC_TASKPRI),
 			MSR_TYPE_R | MSR_TYPE_W);
 
+		/*
+		 * 在以下使用nested_cpu_has_vid():
+		 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+		 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+		 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+		 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+		 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+		 */
 		if (nested_cpu_has_vid(vmcs12)) {
+			/*
+			 * 注释:
+			 * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1
+			 * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,
+			 * only the "disable intercept" case needs to be handled.
+			 * L1不trap的话, L0才不trap
+			 */
 			nested_vmx_disable_intercept_for_x2apic_msr(
 				msr_bitmap_l1, msr_bitmap_l0,
 				X2APIC_MSR(APIC_EOI),
@@ -728,10 +1027,22 @@ static inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,
 	return true;
 }
 
+/*
+ * 在以下使用nested_cache_shadow_vmcs12():
+ * -> arch/x86/kvm/vmx/nested.c|4294| <<nested_vmx_run>> nested_cache_shadow_vmcs12(vcpu, vmcs12);
+ */
 static void nested_cache_shadow_vmcs12(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	struct gfn_to_hva_cache *ghc = &vmx->nested.shadow_vmcs12_cache;
 
 	if (!nested_cpu_has_shadow_vmcs(vmcs12) ||
@@ -743,10 +1054,28 @@ static void nested_cache_shadow_vmcs12(struct kvm_vcpu *vcpu,
 				      vmcs12->vmcs_link_pointer, VMCS12_SIZE))
 		return;
 
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	kvm_read_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),
 			      VMCS12_SIZE);
 }
 
+/*
+ * 在以下使用nested_flush_cached_shadow_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|5898| <<__nested_vmx_vmexit>> nested_flush_cached_shadow_vmcs12(vcpu, vmcs12);
+ */
 static void nested_flush_cached_shadow_vmcs12(struct kvm_vcpu *vcpu,
 					      struct vmcs12 *vmcs12)
 {
@@ -762,6 +1091,20 @@ static void nested_flush_cached_shadow_vmcs12(struct kvm_vcpu *vcpu,
 				      vmcs12->vmcs_link_pointer, VMCS12_SIZE))
 		return;
 
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	kvm_write_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),
 			       VMCS12_SIZE);
 }
@@ -786,9 +1129,28 @@ static int nested_vmx_check_apic_access_controls(struct kvm_vcpu *vcpu,
 		return 0;
 }
 
+/*
+ * 在以下使用nested_vmx_check_apicv_controls():
+ *   - arch/x86/kvm/vmx/nested.c|3026| <<nested_check_vm_execution_controls>> nested_vmx_check_apicv_controls(vcpu, vmcs12) ||
+ */
 static int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,
 					   struct vmcs12 *vmcs12)
 {
+	/*
+	 * 在以下使用nested_cpu_has_virt_x2apic_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|831| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|1034| <<nested_vmx_check_apicv_controls>> if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|1044| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+	 *
+	 * 在以下使用nested_cpu_has_vid():
+	 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+	 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+	 */
 	if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
 	    !nested_cpu_has_apic_reg_virt(vmcs12) &&
 	    !nested_cpu_has_vid(vmcs12) &&
@@ -934,6 +1296,27 @@ static int nested_vmx_check_shadow_vmcs_controls(struct kvm_vcpu *vcpu,
 static int nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,
 				       struct vmx_msr_entry *e)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	/* x2APIC MSR accesses are not allowed */
 	if (CC(vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8))
 		return -EINVAL;
@@ -974,6 +1357,11 @@ static int nested_vmx_store_msr_check(struct kvm_vcpu *vcpu,
  * as possible, process all valid entries before failing rather than precheck
  * for a capacity violation.
  */
+/*
+ * 在以下使用nested_vmx_load_msr():
+ *   - arch/x86/kvm/vmx/nested.c|3719| <<nested_vmx_enter_non_root_mode>> failed_index = nested_vmx_load_msr(vcpu,
+ *   - arch/x86/kvm/vmx/nested.c|4993| <<load_vmcs12_host_state>> if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
+ */
 static u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)
 {
 	u32 i;
@@ -1010,6 +1398,10 @@ static u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)
 	return i + 1;
 }
 
+/*
+ * 在以下使用nested_vmx_get_vmexit_msr_value():
+ *   - arch/x86/kvm/vmx/nested.c|1378| <<nested_vmx_store_msr>> if (!nested_vmx_get_vmexit_msr_value(vcpu, e.index, &data))
+ */
 static bool nested_vmx_get_vmexit_msr_value(struct kvm_vcpu *vcpu,
 					    u32 msr_index,
 					    u64 *data)
@@ -1109,6 +1501,10 @@ static bool nested_msr_store_list_has_msr(struct kvm_vcpu *vcpu, u32 msr_index)
 	return false;
 }
 
+/*
+ * 在以下使用prepare_vmx_msr_autostore_list():
+ *   - arch/x86/kvm/vmx/nested.c|3012| <<prepare_vmcs02_rare>> prepare_vmx_msr_autostore_list(&vmx->vcpu, MSR_IA32_TSC);
+ */
 static void prepare_vmx_msr_autostore_list(struct kvm_vcpu *vcpu,
 					   u32 msr_index)
 {
@@ -1151,6 +1547,11 @@ static void prepare_vmx_msr_autostore_list(struct kvm_vcpu *vcpu,
  * Exit Qualification (for a VM-Entry consistency check VM-Exit) is assigned to
  * @entry_failure_code.
  */
+/*
+ * 在以下使用nested_vmx_load_cr3():
+ *   - arch/x86/kvm/vmx/nested.c|3131| <<prepare_vmcs02>> if (nested_vmx_load_cr3(vcpu, vmcs12->guest_cr3, nested_cpu_has_ept(vmcs12),
+ *   - arch/x86/kvm/vmx/nested.c|5440| <<load_vmcs12_host_state>> if (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, true, &ignored))
+ */
 static int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,
 			       bool nested_ept, bool reload_pdptrs,
 			       enum vm_entry_failure_code *entry_failure_code)
@@ -1268,6 +1669,10 @@ static bool is_bitwise_subset(u64 superset, u64 subset, u64 mask)
 	return (superset | subset) == superset;
 }
 
+/*
+ * 处理MSR_IA32_VMX_BASIC:
+ *   - arch/x86/kvm/vmx/nested.c|1772| <<vmx_set_vmx_msr(MSR_IA32_VMX_BASIC)>> return vmx_restore_vmx_basic(vmx, data);
+ */
 static int vmx_restore_vmx_basic(struct vcpu_vmx *vmx, u64 data)
 {
 	const u64 feature_bits = VMX_BASIC_DUAL_MONITOR_TREATMENT |
@@ -1456,6 +1861,10 @@ static int vmx_restore_fixed0_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)
  *
  * Returns 0 on success, non-0 otherwise.
  */
+/*
+ * 处理"KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR":
+ *   - arch/x86/kvm/vmx/vmx.c|2397| <<vmx_set_msr>> return vmx_set_vmx_msr(vcpu, msr_index, data);
+ */
 int vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1606,6 +2015,12 @@ int vmx_get_vmx_msr(struct nested_vmx_msrs *msrs, u32 msr_index, u64 *pdata)
  * VM-exit information fields (which are actually writable if the vCPU is
  * configured to support "VMWRITE to any supported field in the VMCS").
  */
+/*
+ * 在以下使用copy_shadow_to_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+ *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+ *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+ */
 static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 {
 	struct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;
@@ -1619,6 +2034,16 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 
 	preempt_disable();
 
+	/*
+	 * 在以下使用vmcs_load():
+	 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 */
 	vmcs_load(shadow_vmcs);
 
 	for (i = 0; i < max_shadow_read_write_fields; i++) {
@@ -1633,6 +2058,10 @@ static void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)
 	preempt_enable();
 }
 
+/*
+ * 在以下使用copy_vmcs12_to_shadow():
+ *   - arch/x86/kvm/vmx/nested.c|2517| <<nested_sync_vmcs12_to_shadow>> copy_vmcs12_to_shadow(vmx);
+ */
 static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 {
 	const struct shadow_vmcs_field *fields[] = {
@@ -1652,6 +2081,16 @@ static void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)
 	if (WARN_ON(!shadow_vmcs))
 		return;
 
+	/*
+	 * 在以下使用vmcs_load():
+	 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+	 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+	 */
 	vmcs_load(shadow_vmcs);
 
 	for (q = 0; q < ARRAY_SIZE(fields); q++) {
@@ -2187,6 +2626,10 @@ static enum nested_evmptrld_status nested_vmx_handle_enlightened_vmptrld(
 #endif
 }
 
+/*
+ * 在以下使用nested_sync_vmcs12_to_shadow():
+ *   - arch/x86/kvm/vmx/vmx.c|1271| <<vmx_prepare_switch_to_guest>> nested_sync_vmcs12_to_shadow(vcpu);
+ */
 void nested_sync_vmcs12_to_shadow(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -2196,6 +2639,19 @@ void nested_sync_vmcs12_to_shadow(struct kvm_vcpu *vcpu)
 	else
 		copy_vmcs12_to_shadow(vmx);
 
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	vmx->nested.need_vmcs12_to_shadow_sync = false;
 }
 
@@ -2327,6 +2783,10 @@ static void prepare_vmcs02_constant_state(struct vcpu_vmx *vmx)
 	vmx_set_constant_host_state(vmx);
 }
 
+/*
+ * 在以下使用prepare_vmcs02_early_rare():
+ *   - arch/x86/kvm/vmx/nested.c|2687| <<prepare_vmcs02_early>> prepare_vmcs02_early_rare(vmx, vmcs12);
+ */
 static void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,
 				      struct vmcs12 *vmcs12)
 {
@@ -2353,6 +2813,10 @@ static void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,
 	}
 }
 
+/*
+ * 在以下使用prepare_vmcs02_early():
+ *   - arch/x86/kvm/vmx/nested.c|3684| <<nested_vmx_enter_non_root_mode>> prepare_vmcs02_early(vmx, &vmx->vmcs01, vmcs12);
+ */
 static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs01,
 				 struct vmcs12 *vmcs12)
 {
@@ -2369,9 +2833,29 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 	exec_control |= (vmcs12->pin_based_vm_exec_control &
 			 ~PIN_BASED_VMX_PREEMPTION_TIMER);
 
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	/* Posted interrupts setting is only taken from vmcs12.  */
 	vmx->nested.pi_pending = false;
 	if (nested_cpu_has_posted_intr(vmcs12)) {
+		/*
+		 * 在以下使用nested_vmx->posted_intr_nv:
+		 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+		 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+		 *                                               vector == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+		 */
 		vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
 	} else {
 		vmx->nested.posted_intr_nv = -1;
@@ -2450,6 +2934,32 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 		if (vmx_umip_emulated() && (vmcs12->guest_cr4 & X86_CR4_UMIP))
 			exec_control |= SECONDARY_EXEC_DESC;
 
+		/*
+		 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+		 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+		 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+		 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+		 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+		 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+		 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+		 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+		 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+		 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+		 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+		 */
 		if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
 			vmcs_write16(GUEST_INTR_STATUS,
 				vmcs12->guest_intr_status);
@@ -2520,6 +3030,10 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 	}
 }
 
+/*
+ * 在以下使用prepare_vmcs02_rare():
+ *   - arch/x86/kvm/vmx/nested.c|2870| <<prepare_vmcs02>> prepare_vmcs02_rare(vmx, vmcs12);
+ */
 static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 {
 	struct hv_enlightened_vmcs *hv_evmcs = nested_vmx_evmcs(vmx);
@@ -2619,6 +3133,11 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 		vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, vmcs12->page_fault_error_code_match);
 	}
 
+	/*在以下使用cpu_has_vmx_apicv():
+	 *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+	 *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+	 */
 	if (cpu_has_vmx_apicv()) {
 		vmcs_write64(EOI_EXIT_BITMAP0, vmcs12->eoi_exit_bitmap0);
 		vmcs_write64(EOI_EXIT_BITMAP1, vmcs12->eoi_exit_bitmap1);
@@ -2650,6 +3169,10 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * 在以下使用prepare_vmcs02():
+ *   - arch/x86/kvm/vmx/nested.c|3712| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
@@ -2852,6 +3375,10 @@ static bool nested_vmx_check_eptp(struct kvm_vcpu *vcpu, u64 new_eptp)
 /*
  * Checks related to VM-Execution Control Fields
  */
+/*
+ * 在以下使用nested_check_vm_execution_controls():
+ *  - arch/x86/kvm/vmx/nested.c|3153| <<nested_vmx_check_controls>> if (nested_check_vm_execution_controls(vcpu, vmcs12) ||
+ */
 static int nested_check_vm_execution_controls(struct kvm_vcpu *vcpu,
                                               struct vmcs12 *vmcs12)
 {
@@ -3000,6 +3527,11 @@ static int nested_check_vm_entry_controls(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 在以下使用nested_vmx_check_controls():
+ *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+ *   - arch/x86/kvm/vmx/nested.c|7231| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+ */
 static int nested_vmx_check_controls(struct kvm_vcpu *vcpu,
 				     struct vmcs12 *vmcs12)
 {
@@ -3325,6 +3857,11 @@ static bool nested_get_evmcs_page(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * 在以下使用nested_get_vmcs12_pages():
+ *   - arch/x86/kvm/vmx/nested.c|3662| <<vmx_get_nested_state_pages>> if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
+ *   - arch/x86/kvm/vmx/nested.c|3815| <<nested_vmx_enter_non_root_mode>> if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
+ */
 static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -3360,8 +3897,23 @@ static bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)
 	}
 
 	if (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {
+		/*
+		 * 在以下使用nested_vmx->virtual_apic_map:
+		 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+		 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+		 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+		 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+		 */
 		map = &vmx->nested.virtual_apic_map;
 
+		/*
+		 * 在以下使用VIRTUAL_APIC_PAGE_ADDR:
+		 *   - arch/x86/kvm/vmx/nested.c|3488| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
+		 *   - arch/x86/kvm/vmx/nested.c|3506| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);
+		 *   - arch/x86/kvm/vmx/vmx.c|4800| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|4802| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, __pa(vmx->vcpu.arch.apic->regs));
+		 *   - arch/x86/kvm/vmx/vmx.c|6482| <<dump_vmcs>> pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
+		 */
 		if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->virtual_apic_page_addr), map)) {
 			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
 		} else if (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING) &&
@@ -3434,12 +3986,25 @@ static bool vmx_get_nested_state_pages(struct kvm_vcpu *vcpu)
 	}
 #endif
 
+	/*
+	 * 在以下使用nested_get_vmcs12_pages():
+	 *   - arch/x86/kvm/vmx/nested.c|3662| <<vmx_get_nested_state_pages>> if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|3815| <<nested_vmx_enter_non_root_mode>> if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
+	 */
 	if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
 		return false;
 
 	return true;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->write_log_dirty:
+ *   - arch/x86/kvm/vmx/nested.c|8462| <<global>> .write_log_dirty = nested_vmx_write_pml_buffer,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|225| <<FNAME(update_accessed_dirty_bits)>> if (kvm_x86_ops.nested_ops->write_log_dirty(vcpu, addr))
+ *
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.write_log_dirty = nested_vmx_write_pml_buffer,
+ */
 static int nested_vmx_write_pml_buffer(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	struct vmcs12 *vmcs12;
@@ -3512,6 +4077,12 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
  *	NVMX_VMENTRY_VMEXIT:  Consistency check VMExit
  *	NVMX_VMENTRY_KVM_INTERNAL_ERROR: KVM internal error
  */
+/*
+ * 在以下使用nested_vmx_enter_non_root_mode():
+ *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+ */
 enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 							bool from_vmentry)
 {
@@ -3563,11 +4134,28 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	if (!enable_ept && !nested_early_check)
 		vmcs_writel(GUEST_CR3, vcpu->arch.cr3);
 
+	/*
+	 * 在以下使用vmx_switch_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|337| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3587| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|3593| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3598| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3681| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|5083| <<__nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
 
+	/*
+	 * 只在这里调用
+	 */
 	prepare_vmcs02_early(vmx, &vmx->vmcs01, vmcs12);
 
 	if (from_vmentry) {
+		/*
+		 * 在以下使用nested_get_vmcs12_pages():
+		 *   - arch/x86/kvm/vmx/nested.c|3662| <<vmx_get_nested_state_pages>> if (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|3815| <<nested_vmx_enter_non_root_mode>> if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
+		 */
 		if (unlikely(!nested_get_vmcs12_pages(vcpu))) {
 			vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 			return NVMX_VMENTRY_KVM_INTERNAL_ERROR;
@@ -3586,8 +4174,16 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 		}
 	}
 
+	/*
+	 * 在以下使用enter_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 */
 	enter_guest_mode(vcpu);
 
+	/*
+	 * 只在这里调用
+	 */
 	if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
 		exit_reason.basic = EXIT_REASON_INVALID_STATE;
 		vmcs12->exit_qualification = entry_failure_code;
@@ -3595,6 +4191,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	}
 
 	if (from_vmentry) {
+		/*
+		 * 在以下使用nested_vmx_load_msr():
+		 *   - arch/x86/kvm/vmx/nested.c|3719| <<nested_vmx_enter_non_root_mode>> failed_index = nested_vmx_load_msr(vcpu,
+		 *   - arch/x86/kvm/vmx/nested.c|4993| <<load_vmcs12_host_state>> if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
+		 */
 		failed_index = nested_vmx_load_msr(vcpu,
 						   vmcs12->vm_entry_msr_load_addr,
 						   vmcs12->vm_entry_msr_load_count);
@@ -3654,6 +4255,13 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 vmentry_fail_vmexit_guest_mode:
 	if (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETTING)
 		vcpu->arch.tsc_offset -= vmcs12->tsc_offset;
+	/*
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	leave_guest_mode(vcpu);
 
 vmentry_fail_vmexit:
@@ -3662,8 +4270,43 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
 	if (!from_vmentry)
 		return NVMX_VMENTRY_VMEXIT;
 
+	/*
+	 * 在以下使用load_vmcs12_host_state():
+	 *   - arch/x86/kvm/vmx/nested.c|4213| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|6031| <<__nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+	 */
 	load_vmcs12_host_state(vcpu, vmcs12);
 	vmcs12->vm_exit_reason = exit_reason.full;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 *
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
 		vmx->nested.need_vmcs12_to_shadow_sync = true;
 	return NVMX_VMENTRY_VMEXIT;
@@ -3673,6 +4316,11 @@ enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
  * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1
  * for running an L2 nested guest.
  */
+/*
+ * 在以下使用nested_vmx_run():
+ *   - arch/x86/kvm/vmx/nested.c|5591| <<handle_vmlaunch>> return nested_vmx_run(vcpu, true);
+ *   - arch/x86/kvm/vmx/nested.c|5598| <<handle_vmresume>> return nested_vmx_run(vcpu, false);
+ */
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 {
 	struct vmcs12 *vmcs12;
@@ -3690,6 +4338,13 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用kvm_pmu_trigger_event():
+	 *   - arch/x86/kvm/vmx/nested.c|3714| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+	 *   - arch/x86/kvm/x86.c|9106| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+	 *   - arch/x86/kvm/x86.c|9445| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+	 *   - arch/x86/kvm/x86.c|9447| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+	 */
 	kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
 
 	if (CC(evmptrld_status == EVMPTRLD_VMFAIL))
@@ -3717,6 +4372,30 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 		/* Enlightened VMCS doesn't have launch state */
 		vmcs12->launch_state = !launch;
 	} else if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用enable_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+		 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+		 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+		 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+		 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+		 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+		 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+		 *
+		 *
+		 * 在以下使用copy_shadow_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+		 */
 		copy_shadow_to_vmcs12(vmx);
 	}
 
@@ -3738,6 +4417,11 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 			launch ? VMXERR_VMLAUNCH_NONCLEAR_VMCS
 			       : VMXERR_VMRESUME_NONLAUNCHED_VMCS);
 
+	/*
+	 * 在以下使用nested_vmx_check_controls():
+	 *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+	 *   - arch/x86/kvm/vmx/nested.c|7231| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+	 */
 	if (nested_vmx_check_controls(vcpu, vmcs12))
 		return nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);
 
@@ -3753,6 +4437,13 @@ static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)
 	 */
 	vmx->nested.nested_run_pending = 1;
 	vmx->nested.has_preemption_timer_deadline = false;
+	/*
+	 * 在以下使用nested_vmx_enter_non_root_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 *   - arch/x86/kvm/vmx/nested.h|26| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+	 *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 */
 	status = nested_vmx_enter_non_root_mode(vcpu, true);
 	if (unlikely(status != NVMX_VMENTRY_SUCCESS))
 		goto vmentry_failed;
@@ -3844,6 +4535,11 @@ vmcs12_guest_cr4(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 			vcpu->arch.cr4_guest_owned_bits));
 }
 
+/*
+ * 在以下使用vmcs12_save_pending_event():
+ *   - arch/x86/kvm/vmx/nested.c|5357| <<prepare_vmcs12>> vmcs12_save_pending_event(vcpu, vmcs12,
+ *           vm_exit_reason, exit_intr_info);
+ */
 static void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,
 				      struct vmcs12 *vmcs12,
 				      u32 vm_exit_reason, u32 exit_intr_info)
@@ -3911,6 +4607,11 @@ static void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,
 }
 
 
+/*
+ * 在以下使用nested_mark_vmcs12_pages_dirty():
+ *   - arch/x86/kvm/vmx/nested.c|4391| <<vmx_complete_nested_posted_interrupt>> nested_mark_vmcs12_pages_dirty(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6870| <<__vmx_handle_exit>> nested_mark_vmcs12_pages_dirty(vcpu);
+ */
 void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -3932,6 +4633,10 @@ void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用vmx_complete_nested_posted_interrupt():
+ *   - arch/x86/kvm/vmx/nested.c|4875| <<vmx_check_nested_events>> return vmx_complete_nested_posted_interrupt(vcpu);
+ */
 static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3939,6 +4644,15 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 	void *vapic_page;
 	u16 status;
 
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	if (!vmx->nested.pi_pending)
 		return 0;
 
@@ -3952,10 +4666,22 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 
 	max_irr = pi_find_highest_vector(vmx->nested.pi_desc);
 	if (max_irr > 0) {
+		/*
+		 * 在以下使用nested_vmx->virtual_apic_map:
+		 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+		 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+		 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+		 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+		 */
 		vapic_page = vmx->nested.virtual_apic_map.hva;
 		if (!vapic_page)
 			goto mmio_needed;
 
+		/*
+		 * 在以下使用__kvm_apic_update_irr():
+		 *   - arch/x86/kvm/lapic.c|791| <<kvm_apic_update_irr>> bool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);
+		 *   - arch/x86/kvm/vmx/nested.c|4381| <<vmx_complete_nested_posted_interrupt>> __kvm_apic_update_irr(vmx->nested.pi_desc->pir,
+		 */
 		__kvm_apic_update_irr(vmx->nested.pi_desc->pir,
 			vapic_page, &max_irr);
 		status = vmcs_read16(GUEST_INTR_STATUS);
@@ -3966,6 +4692,11 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 在以下使用nested_mark_vmcs12_pages_dirty():
+	 *   - arch/x86/kvm/vmx/nested.c|4391| <<vmx_complete_nested_posted_interrupt>> nested_mark_vmcs12_pages_dirty(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6870| <<__vmx_handle_exit>> nested_mark_vmcs12_pages_dirty(vcpu);
+	 */
 	nested_mark_vmcs12_pages_dirty(vcpu);
 	return 0;
 
@@ -3974,6 +4705,11 @@ static int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)
 	return -ENXIO;
 }
 
+/*
+ * 在以下使用nested_vmx_inject_exception_vmexit():
+ *   - arch/x86/kvm/vmx/nested.c|4797| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4820| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+ */
 static void nested_vmx_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;
@@ -4082,9 +4818,26 @@ static bool nested_vmx_preemption_timer_pending(struct kvm_vcpu *vcpu)
 	       to_vmx(vcpu)->nested.preemption_timer_expired;
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->has_events:
+ *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+ *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+ *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+ *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+ *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.has_events = vmx_has_nested_events,
+ */
 static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * 在以下使用nested_vmx->virtual_apic_map:
+	 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+	 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+	 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+	 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+	 */
 	void *vapic = vmx->nested.virtual_apic_map.hva;
 	int max_irr, vppr;
 
@@ -4101,6 +4854,16 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
 	if (for_injection)
 		return false;
 
+	/*
+	 * 在以下使用nested_cpu_has_vid():
+	 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+	 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+	 */
 	if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
 	    __vmx_interrupt_blocked(vcpu))
 		return false;
@@ -4114,6 +4877,15 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
 	if ((max_irr & 0xf0) > (vppr & 0xf0))
 		return true;
 
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
 	    pi_test_on(vmx->nested.pi_desc)) {
 		max_irr = pi_find_highest_vector(vmx->nested.pi_desc);
@@ -4207,6 +4979,17 @@ static bool vmx_has_nested_events(struct kvm_vcpu *vcpu, bool for_injection)
  *     delivery of a virtual interrupt; delivery of a virtual interrupt takes
  *     priority over external interrupts and lower priority events.
  */
+ /*
+  * 在以下使用kvm_check_nested_events():
+  *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+  *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+  *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+  *
+  * 在以下使用kvm_x86_nested_ops->check_events:
+  *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+  *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+  *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+  */
 static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -4281,6 +5064,11 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		if (block_nested_exceptions)
 			return -EBUSY;
 
+		/*
+		 * 在以下使用nested_vmx_inject_exception_vmexit():
+		 *   - arch/x86/kvm/vmx/nested.c|4797| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4820| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 */
 		nested_vmx_inject_exception_vmexit(vcpu);
 		return 0;
 	}
@@ -4304,6 +5092,11 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		if (block_nested_exceptions)
 			return -EBUSY;
 
+		/*
+		 * 在以下使用nested_vmx_inject_exception_vmexit():
+		 *   - arch/x86/kvm/vmx/nested.c|4797| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|4820| <<vmx_check_nested_events>> nested_vmx_inject_exception_vmexit(vcpu);
+		 */
 		nested_vmx_inject_exception_vmexit(vcpu);
 		return 0;
 	}
@@ -4317,6 +5110,12 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 	if (nested_vmx_preemption_timer_pending(vcpu)) {
 		if (block_nested_events)
 			return -EBUSY;
+		/*
+		 * 注释:
+		 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+		 * and modify vmcs12 to make it see what it would expect to see there if
+		 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+		 */
 		nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
 		return 0;
 	}
@@ -4373,6 +5172,20 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 			return 0;
 		}
 
+		/*
+		 * 在以下使用kvm_apic_has_interrupt():
+		 *   - arch/x86/kvm/irq.c|181| <<kvm_cpu_has_injectable_intr>> return kvm_apic_has_interrupt(v) != -1;
+		 *   - arch/x86/kvm/irq.c|204| <<kvm_cpu_has_interrupt>> return kvm_apic_has_interrupt(v) != -1;
+		 *   - arch/x86/kvm/irq.c|292| <<kvm_cpu_get_interrupt>> vector = kvm_apic_has_interrupt(v);
+		 *   - arch/x86/kvm/vmx/nested.c|3627| <<nested_vmx_enter_non_root_mode>> kvm_apic_has_interrupt(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|4376| <<vmx_check_nested_events>> irq = kvm_apic_has_interrupt(vcpu);
+		 *
+		 * 核心思想:
+		 * 1. 判断kvm_apic_present()
+		 * 2. 根据TPR和ISR更新PPR
+		 * TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+		 * 3. 选出符合当前PPR的最高的irr, 没有就返回-1
+		 */
 		irq = kvm_apic_has_interrupt(vcpu);
 		if (WARN_ON_ONCE(irq < 0))
 			goto no_vmexit;
@@ -4385,6 +5198,17 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		 * through the local APIC trigger posted interrupt processing,
 		 * and enabling posted interrupts requires ACK-on-exit.
 		 */
+		/*
+		 * 在以下使用nested_vmx->posted_intr_nv:
+		 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+		 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+		 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+		 *                                               vector == vmx->nested.posted_intr_nv) {
+		 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+		 */
 		if (irq == vmx->nested.posted_intr_nv) {
 			/*
 			 * Nested posted interrupts are delivered via RVI, i.e.
@@ -4394,6 +5218,15 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 			if (block_non_injected_events)
 				return -EBUSY;
 
+			/*
+			 * 在以下使用nested_vmx->pi_pending:
+			 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+			 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+			 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+			 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+			 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+			 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+			 */
 			vmx->nested.pi_pending = true;
 			kvm_apic_clear_irr(vcpu, irq);
 			goto no_vmexit;
@@ -4402,6 +5235,12 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 		if (block_nested_events)
 			return -EBUSY;
 
+		/*
+		 * 注释:
+		 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+		 * and modify vmcs12 to make it see what it would expect to see there if
+		 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+		 */
 		nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT,
 				  INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR | irq, 0);
 
@@ -4415,6 +5254,9 @@ static int vmx_check_nested_events(struct kvm_vcpu *vcpu)
 	}
 
 no_vmexit:
+	/*
+	 * 只在这里调用
+	 */
 	return vmx_complete_nested_posted_interrupt(vcpu);
 }
 
@@ -4479,6 +5321,12 @@ static bool is_vmcs12_ext_field(unsigned long field)
 	return false;
 }
 
+/*
+ * 在以下使用sync_vmcs02_to_vmcs12_rare():
+ *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ */
 static void sync_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
@@ -4521,15 +5369,34 @@ static void sync_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 	vmcs12->guest_pending_dbg_exceptions =
 		vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
 
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
 }
 
+/*
+ * 在以下使用copy_vmcs02_to_vmcs12_rare():
+ *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+ *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+ */
 static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	int cpu;
 
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
 		return;
 
@@ -4538,8 +5405,21 @@ static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 
 	cpu = get_cpu();
 	vmx->loaded_vmcs = &vmx->nested.vmcs02;
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 
+	/*
+	 * 在以下使用sync_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 */
 	sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 
 	vmx->loaded_vmcs = &vmx->vmcs01;
@@ -4553,13 +5433,30 @@ static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
  * VM-entry controls is also updated, since this is really a guest
  * state bit.)
  */
+/*
+ * 在以下使用sync_vmcs02_to_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|5629| <<__nested_vmx_vmexit>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|7521| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+ */
 static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
+	/*
+	 * 在以下使用sync_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 */
 	if (nested_vmx_is_evmptr12_valid(vmx))
 		sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	vmx->nested.need_sync_vmcs02_to_vmcs12_rare =
 		!nested_vmx_is_evmptr12_valid(vmx);
 
@@ -4609,6 +5506,16 @@ static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 
 	vmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);
 
+	/*
+	 * 在以下使用nested_cpu_has_vid():
+	 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+	 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+	 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+	 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+	 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+	 */
 	if (nested_cpu_has_vid(vmcs12))
 		vmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);
 
@@ -4640,6 +5547,10 @@ static void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
  * exit-information fields only. Other fields are modified by L1 with VMWRITE,
  * which already writes to vmcs12 directly.
  */
+/*
+ * 在以下使用prepare_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|5046| <<__nested_vmx_vmexit>> prepare_vmcs12(vcpu, vmcs12, vm_exit_reason,
+ */
 static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			   u32 vm_exit_reason, u32 exit_intr_info,
 			   unsigned long exit_qualification, u32 exit_insn_len)
@@ -4679,6 +5590,13 @@ static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 		 * during or after loading the guest state. Since this exit
 		 * does not fall in that category, we need to save the MSRs.
 		 */
+		/*
+		 * 在以下使用nested_vmx_abort():
+		 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+		 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+		 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+		 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+		 */
 		if (nested_vmx_store_msr(vcpu,
 					 vmcs12->vm_exit_msr_store_addr,
 					 vmcs12->vm_exit_msr_store_count))
@@ -4696,6 +5614,11 @@ static void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
  * Failures During or After Loading Guest State").
  * This function should be called when the active VMCS is L1's (vmcs01).
  */
+/*
+ * 在以下使用load_vmcs12_host_state():
+ *   - arch/x86/kvm/vmx/nested.c|4213| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+ *   - arch/x86/kvm/vmx/nested.c|6031| <<__nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+ */
 static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 				   struct vmcs12 *vmcs12)
 {
@@ -4735,6 +5658,13 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	 * Only PDPTE load can fail as the value of cr3 was checked on entry and
 	 * couldn't have changed.
 	 */
+	/*
+	 * 在以下使用nested_vmx_abort():
+	 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 */
 	if (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, true, &ignored))
 		nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
 
@@ -4814,6 +5744,17 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	kvm_set_dr(vcpu, 7, 0x400);
 	vmx_guest_debugctl_write(vcpu, 0);
 
+	/*
+	 * 在以下使用nested_vmx_load_msr():
+	 *   - arch/x86/kvm/vmx/nested.c|3719| <<nested_vmx_enter_non_root_mode>> failed_index = nested_vmx_load_msr(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|4993| <<load_vmcs12_host_state>> if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
+	 *
+	 * 在以下使用nested_vmx_abort():
+	 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 */
 	if (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,
 				vmcs12->vm_exit_msr_load_count))
 		nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
@@ -4844,6 +5785,10 @@ static inline u64 nested_vmx_get_vmcs01_guest_efer(struct vcpu_vmx *vmx)
 	return kvm_host.efer;
 }
 
+/*
+ * 在以下使用nested_vmx_restore_host_state():
+ *   - arch/x86/kvm/vmx/nested.c|5501| <<__nested_vmx_vmexit>> nested_vmx_restore_host_state(vcpu);
+ */
 static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 {
 	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@ -4949,6 +5894,13 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
 	return;
 
 vmabort:
+	/*
+	 * 在以下使用nested_vmx_abort():
+	 *   - arch/x86/kvm/vmx/nested.c|5022| <<prepare_vmcs12>> nested_vmx_abort(vcpu,
+	 *   - arch/x86/kvm/vmx/nested.c|5076| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5161| <<load_vmcs12_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 *   - arch/x86/kvm/vmx/nested.c|5294| <<nested_vmx_restore_host_state>> nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
+	 */
 	nested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);
 }
 
@@ -4957,6 +5909,16 @@ static void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)
  * and modify vmcs12 to make it see what it would expect to see there if
  * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
  */
+/*
+ * 在以下使用__nested_vmx_vmexit():
+ *   - arch/x86/kvm/vmx/nested.h|45| <<nested_vmx_vmexit>> __nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
+ *   - arch/x86/kvm/vmx/vmx.c|8211| <<vmx_check_intercept>> __nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
+ *
+ * 注释:
+ * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+ * and modify vmcs12 to make it see what it would expect to see there if
+ * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+ */
 void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 			 u32 exit_intr_info, unsigned long exit_qualification,
 			 u32 exit_insn_len)
@@ -4993,6 +5955,13 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	if (enable_ept && is_pae_paging(vcpu))
 		vmx_ept_load_pdptrs(vcpu);
 
+	/*
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	leave_guest_mode(vcpu);
 
 	if (nested_cpu_has_preemption_timer(vmcs12))
@@ -5005,8 +5974,16 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	}
 
 	if (likely(!vmx->fail)) {
+		/*
+		 * 在以下使用sync_vmcs02_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|5629| <<__nested_vmx_vmexit>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7521| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 */
 		sync_vmcs02_to_vmcs12(vcpu, vmcs12);
 
+		/*
+		 * 只在这里调用prepare_vmcs12()
+		 */
 		if (vm_exit_reason != -1)
 			prepare_vmcs12(vcpu, vmcs12, vm_exit_reason,
 				       exit_intr_info, exit_qualification,
@@ -5045,6 +6022,15 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	kvm_clear_exception_queue(vcpu);
 	kvm_clear_interrupt_queue(vcpu);
 
+	/*
+	 * 在以下使用vmx_switch_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|337| <<free_nested>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3587| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);
+	 *   - arch/x86/kvm/vmx/nested.c|3593| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3598| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|3681| <<nested_vmx_enter_non_root_mode>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 *   - arch/x86/kvm/vmx/nested.c|5083| <<__nested_vmx_vmexit>> vmx_switch_vmcs(vcpu, &vmx->vmcs01);
+	 */
 	vmx_switch_vmcs(vcpu, &vmx->vmcs01);
 
 	kvm_nested_vmexit_handle_ibrs(vcpu);
@@ -5059,6 +6045,12 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	if (vmx->nested.l1_tpr_threshold != -1)
 		vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
 
+	/*
+	 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page:
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	if (vmx->nested.change_vmcs01_virtual_apic_mode) {
 		vmx->nested.change_vmcs01_virtual_apic_mode = false;
 		vmx_set_virtual_apic_mode(vcpu);
@@ -5069,23 +6061,86 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 		vmx_update_cpu_dirty_logging(vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_put_vmcs12_pages():
+	 *   - arch/x86/kvm/vmx/nested.c|361| <<free_nested>> nested_put_vmcs12_pages(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5107| <<__nested_vmx_vmexit>> nested_put_vmcs12_pages(vcpu);
+	 */
 	nested_put_vmcs12_pages(vcpu);
 
+	/*
+	 * 在以下使用reload_vmcs01_apic_access_page:
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	if (vmx->nested.reload_vmcs01_apic_access_page) {
 		vmx->nested.reload_vmcs01_apic_access_page = false;
 		kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->pdate_vmcs01_apicv_status:
+	 *   - arch/x86/kvm/vmx/nested.c|5114| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+	 *   - arch/x86/kvm/vmx/nested.c|5115| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+	 */
 	if (vmx->nested.update_vmcs01_apicv_status) {
 		vmx->nested.update_vmcs01_apicv_status = false;
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+	 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+	 */
 	if (vmx->nested.update_vmcs01_hwapic_isr) {
+		/*
+		 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+		 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+		 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+		 */
 		vmx->nested.update_vmcs01_hwapic_isr = false;
+		/*
+		 * 只在这里调用
+		 * 其实就是vmx_hwapic_isr_update()
+		 */
 		kvm_apic_update_hwapic_isr(vcpu);
 	}
 
+	/*
+	 * 在以下使用enable_shadow_vmcs: 
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 *
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	if ((vm_exit_reason != -1) &&
 	    (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
 		vmx->nested.need_vmcs12_to_shadow_sync = true;
@@ -5102,6 +6157,11 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 						       vmcs12->vm_exit_intr_error_code,
 						       KVM_ISA_VMX);
 
+		/*
+		 * 在以下使用load_vmcs12_host_state():
+		 *   - arch/x86/kvm/vmx/nested.c|4213| <<nested_vmx_enter_non_root_mode>> load_vmcs12_host_state(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|6031| <<__nested_vmx_vmexit>> load_vmcs12_host_state(vcpu, vmcs12);
+		 */
 		load_vmcs12_host_state(vcpu, vmcs12);
 
 		/*
@@ -5150,6 +6210,29 @@ static void nested_vmx_triple_fault(struct kvm_vcpu *vcpu)
  * On success, returns 0. When the operand is invalid, returns 1 and throws
  * #UD, #GP, or #SS.
  */
+/*
+ * 在以下使用get_vmx_mem_address():
+ *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+ *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+ *           exit_qualification, instr_info, true, len, &gva))
+ *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+ *           exit_qualification, instr_info, false, len, &gva))
+ *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+ *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+ *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+ *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+ *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+ *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+ *
+ * 注释:
+ * Decode the memory-address operand of a vmx instruction, as recorded on an
+ * exit caused by such an instruction (run by a guest hypervisor).
+ * On success, returns 0. When the operand is invalid, returns 1 and throws
+ * #UD, #GP, or #SS.
+ */
 int get_vmx_mem_address(struct kvm_vcpu *vcpu, unsigned long exit_qualification,
 			u32 vmx_instruction_info, bool wr, int len, gva_t *ret)
 {
@@ -5282,6 +6365,29 @@ static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer,
 	struct x86_exception e;
 	int r;
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	if (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),
 				vmcs_read32(VMX_INSTRUCTION_INFO), false,
 				sizeof(*vmpointer), &gva)) {
@@ -5303,6 +6409,10 @@ static int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer,
  * VMCS, unless such a shadow VMCS already exists. The newly allocated
  * VMCS is also VMCLEARed, so that it is ready for use.
  */
+/*
+ * 在以下使用alloc_shadow_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|6355| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+ */
 static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5326,6 +6436,11 @@ static struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)
 	return loaded_vmcs->shadow_vmcs;
 }
 
+/*
+ * 在以下使用enter_vmx_operation():
+ *   - arch/x86/kvm/vmx/nested.c|6471| <<handle_vmxon>> ret = enter_vmx_operation(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|8116| <<vmx_set_nested_state>> ret = enter_vmx_operation(vcpu);
+ */
 static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -5344,6 +6459,24 @@ static int enter_vmx_operation(struct kvm_vcpu *vcpu)
 	if (!vmx->nested.cached_shadow_vmcs12)
 		goto out_cached_shadow_vmcs12;
 
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
 		goto out_shadow_vmcs;
 
@@ -5474,14 +6607,61 @@ static inline void nested_release_vmcs12(struct kvm_vcpu *vcpu)
 	if (vmx->nested.current_vmptr == INVALID_GPA)
 		return;
 
+	/*
+	 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 */
 	copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
 
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用copy_shadow_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+		 */
 		/* copy to memory all shadowed fields in case
 		   they were modified */
 		copy_shadow_to_vmcs12(vmx);
+		/*
+		 * 在以下使用vmx_disable_shadow_vmcs():
+		 *   - arch/x86/kvm/vmx/nested.c|498| <<free_nested>> vmx_disable_shadow_vmcs(vmx);
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<nested_release_vmcs12>> vmx_disable_shadow_vmcs(vmx);
+		 */
 		vmx_disable_shadow_vmcs(vmx);
 	}
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	vmx->nested.posted_intr_nv = -1;
 
 	/* Flush VMCS12 to guest memory */
@@ -5565,6 +6745,20 @@ static int handle_vmresume(struct kvm_vcpu *vcpu)
 
 static int handle_vmread(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu)
 						    : get_vmcs12(vcpu);
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5597,6 +6791,13 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 		if (offset < 0)
 			return nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
 
+		/*
+		 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+		 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 */
 		if (!is_guest_mode(vcpu) && is_vmcs12_ext_field(field))
 			copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 
@@ -5633,6 +6834,29 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 		kvm_register_write(vcpu, (((instr_info) >> 3) & 0xf), value);
 	} else {
 		len = is_64_bit_mode(vcpu) ? 8 : 4;
+		/*
+		 * 在以下使用get_vmx_mem_address():
+		 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, true, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, false, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *
+		 * 注释:
+		 * Decode the memory-address operand of a vmx instruction, as recorded on an
+		 * exit caused by such an instruction (run by a guest hypervisor).
+		 * On success, returns 0. When the operand is invalid, returns 1 and throws
+		 * #UD, #GP, or #SS.
+		 */
 		if (get_vmx_mem_address(vcpu, exit_qualification,
 					instr_info, true, len, &gva))
 			return 1;
@@ -5645,6 +6869,11 @@ static int handle_vmread(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * 在以下使用is_shadow_field_rw():
+ *   - arch/x86/kvm/vmx/nested.c|6833| <<handle_vmwrite>> if (!is_guest_mode(vcpu) && !is_shadow_field_rw(field))
+ *   - arch/x86/kvm/vmx/nested.c|6855| <<handle_vmwrite>> if (!is_guest_mode(vcpu) && !is_shadow_field_rw(field)) {
+ */
 static bool is_shadow_field_rw(unsigned long field)
 {
 	switch (field) {
@@ -5671,6 +6900,20 @@ static bool is_shadow_field_ro(unsigned long field)
 
 static int handle_vmwrite(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用get_shadow_vmcs12():
+	 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+	 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+	 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+	 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+	 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+	 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+	 */
 	struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu)
 						    : get_vmcs12(vcpu);
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
@@ -5707,6 +6950,29 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 		value = kvm_register_read(vcpu, (((instr_info) >> 3) & 0xf));
 	else {
 		len = is_64_bit_mode(vcpu) ? 8 : 4;
+		/*
+		 * 在以下使用get_vmx_mem_address():
+		 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, true, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qualification, instr_info, false, len, &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+		 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+		 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+		 *
+		 * 注释:
+		 * Decode the memory-address operand of a vmx instruction, as recorded on an
+		 * exit caused by such an instruction (run by a guest hypervisor).
+		 * On success, returns 0. When the operand is invalid, returns 1 and throws
+		 * #UD, #GP, or #SS.
+		 */
 		if (get_vmx_mem_address(vcpu, exit_qualification,
 					instr_info, false, len, &gva))
 			return 1;
@@ -5729,6 +6995,13 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	    !nested_cpu_has_vmwrite_any_field(vcpu))
 		return nested_vmx_fail(vcpu, VMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);
 
+	/*
+	 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+	 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+	 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+	 */
 	/*
 	 * Ensure vmcs12 is up-to-date before any VMWRITE that dirties
 	 * vmcs12, else we may crush a field or consume a stale value.
@@ -5756,12 +7029,40 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	 * "dirty" vmcs12, all others go down the prepare_vmcs02() slow path.
 	 */
 	if (!is_guest_mode(vcpu) && !is_shadow_field_rw(field)) {
+		/*
+		 * 在以下使用enable_shadow_vmcs:
+		 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+		 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+		 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+		 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+		 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+		 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+		 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+		 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+		 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+		 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+		 */
 		/*
 		 * L1 can read these fields without exiting, ensure the
 		 * shadow VMCS is up-to-date.
 		 */
 		if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
 			preempt_disable();
+			/*
+			 * 在以下使用vmcs_load():
+			 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+			 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+			 */
 			vmcs_load(vmx->vmcs01.shadow_vmcs);
 
 			__vmcs_writel(field, value);
@@ -5776,13 +7077,49 @@ static int handle_vmwrite(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * 在以下使用set_current_vmptr():
+ *   - arch/x86/kvm/vmx/nested.c|7171| <<handle_vmptrld>> set_current_vmptr(vmx, vmptr);
+ *   - arch/x86/kvm/vmx/nested.c|8390| <<vmx_set_nested_state>> set_current_vmptr(vmx, kvm_state->hdr.vmx.vmcs12_pa);
+ */
 static void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)
 {
 	vmx->nested.current_vmptr = vmptr;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
 		secondary_exec_controls_setbit(vmx, SECONDARY_EXEC_SHADOW_VMCS);
 		vmcs_write64(VMCS_LINK_POINTER,
 			     __pa(vmx->vmcs01.shadow_vmcs));
+		/*
+		 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+		 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+		 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+		 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+		 */
 		vmx->nested.need_vmcs12_to_shadow_sync = true;
 	}
 	vmx->nested.dirty_vmcs12 = true;
@@ -5875,6 +7212,29 @@ static int handle_vmptrst(struct kvm_vcpu *vcpu)
 	if (unlikely(nested_vmx_is_evmptr12_valid(to_vmx(vcpu))))
 		return 1;
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	if (get_vmx_mem_address(vcpu, exit_qual, instr_info,
 				true, sizeof(gpa_t), &gva))
 		return 1;
@@ -5920,6 +7280,29 @@ static int handle_invept(struct kvm_vcpu *vcpu)
 	if (type >= 32 || !(types & (1 << type)))
 		return nested_vmx_fail(vcpu, VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	/* According to the Intel VMX instruction reference, the memory
 	 * operand is read even if it isn't needed (e.g., for type==global)
 	 */
@@ -6003,6 +7386,29 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 		return nested_vmx_fail(vcpu,
 			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	/* according to the intel vmx instruction reference, the memory
 	 * operand is read even if it isn't needed (e.g., for type==global)
 	 */
@@ -6067,6 +7473,10 @@ static int handle_invvpid(struct kvm_vcpu *vcpu)
 	return nested_vmx_succeed(vcpu);
 }
 
+/*
+ * 在以下使用nested_vmx_eptp_switching():
+ *   - arch/x86/kvm/vmx/nested.c|7523| <<handle_vmfunc>> if (nested_vmx_eptp_switching(vcpu, vmcs12))
+ */
 static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 				     struct vmcs12 *vmcs12)
 {
@@ -6091,6 +7501,11 @@ static int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,
 			return 1;
 
 		vmcs12->ept_pointer = new_eptp;
+		/*
+		 * 在以下使用nested_ept_new_eptp():
+		 *   - arch/x86/kvm/vmx/nested.c|627| <<nested_ept_init_mmu_context>> nested_ept_new_eptp(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|7013| <<nested_vmx_eptp_switching>> nested_ept_new_eptp(vcpu);
+		 */
 		nested_ept_new_eptp(vcpu);
 
 		if (!nested_cpu_has_vpid(vmcs12))
@@ -6188,6 +7603,10 @@ bool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,
 	return false;
 }
 
+/*
+ * 在以下使用nested_vmx_exit_handled_io():
+ *   - arch/x86/kvm/vmx/nested.c|7922| <<nested_vmx_l1_wants_exit>> return nested_vmx_exit_handled_io(vcpu, vmcs12);
+ */
 static bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,
 				       struct vmcs12 *vmcs12)
 {
@@ -6335,6 +7754,11 @@ static bool nested_vmx_exit_handled_encls(struct kvm_vcpu *vcpu,
 	return vmcs12->encls_exiting_bitmap & BIT_ULL(encls_leaf);
 }
 
+/*
+ * 在以下使用nested_vmx_exit_handled_vmcs_access():
+ *   - arch/x86/kvm/vmx/nested.c|7902| <<nested_vmx_l1_wants_exit>> return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12, vmcs12->vmread_bitmap);
+ *   - arch/x86/kvm/vmx/nested.c|7905| <<nested_vmx_l1_wants_exit>> return nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12, vmcs12->vmwrite_bitmap);
+ */
 static bool nested_vmx_exit_handled_vmcs_access(struct kvm_vcpu *vcpu,
 	struct vmcs12 *vmcs12, gpa_t bitmap)
 {
@@ -6380,6 +7804,10 @@ static bool nested_vmx_exit_handled_mtf(struct vmcs12 *vmcs12)
  * Return true if L0 wants to handle an exit from L2 regardless of whether or not
  * L1 wants the exit.  Only call this when in is_guest_mode (L2).
  */
+/*
+ * 在以下使用nested_vmx_l0_wants_exit():
+ *   - arch/x86/kvm/vmx/nested.c|7481| <<nested_vmx_reflect_vmexit>> if (nested_vmx_l0_wants_exit(vcpu, exit_reason))
+ */
 static bool nested_vmx_l0_wants_exit(struct kvm_vcpu *vcpu,
 				     union vmx_exit_reason exit_reason)
 {
@@ -6460,6 +7888,10 @@ static bool nested_vmx_l0_wants_exit(struct kvm_vcpu *vcpu,
  * Return 1 if L1 wants to intercept an exit from L2.  Only call this when in
  * is_guest_mode (L2).
  */
+/*
+ * 在以下使用nested_vmx_l1_wants_exit():
+ *   - arch/x86/kvm/vmx/nested.c|7081| <<nested_vmx_reflect_vmexit>> if (!nested_vmx_l1_wants_exit(vcpu, exit_reason))
+ */
 static bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu,
 				     union vmx_exit_reason exit_reason)
 {
@@ -6587,9 +8019,22 @@ static bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu,
  * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
  * reflected into L1.
  */
+/*
+ * 在以下使用nested_vmx_reflect_vmexit():
+ *   - arch/x86/kvm/vmx/vmx.c|6878| <<__vmx_handle_exit>> if (nested_vmx_reflect_vmexit(vcpu))
+ *
+ * 注释:
+ * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
+ * reflected into L1.
+ */
 bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct vcpu_vt vt;
+	 *    -> union vmx_exit_reason exit_reason;
+	 */
 	union vmx_exit_reason exit_reason = vmx->vt.exit_reason;
 	unsigned long exit_qual;
 	u32 exit_intr_info;
@@ -6619,6 +8064,24 @@ bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 	if (!nested_vmx_l1_wants_exit(vcpu, exit_reason))
 		return false;
 
+	/*
+	 * 在以下使用vmx_get_intr_info():
+	 *   - arch/x86/kvm/vmx/nested.c|6163| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->vt.exit_reason.full, vmx_get_intr_info(vcpu),
+	 *   - arch/x86/kvm/vmx/nested.c|6404| <<nested_vmx_l0_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6485| <<nested_vmx_l1_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|6642| <<nested_vmx_reflect_vmexit>> exit_intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/tdx.c|1120| <<tdx_handle_exception_nmi>> u32 intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/tdx.c|2167| <<tdx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5140| <<handle_exception_nmi>> intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6104| <<vmx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|6974| <<vmx_handle_exit_irqoff>> handle_exception_irqoff(vcpu, vmx_get_intr_info(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|7016| <<vmx_recover_nmi_blocking>> exit_intr_info = vmx_get_intr_info(&vmx->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7255| <<vmx_handle_nmi>> if (... !is_nmi(vmx_get_intr_info(vcpu)))
+	 *
+	 * VM_EXIT_INTR_INFO:
+	 * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+	 */
 	/*
 	 * vmcs.VM_EXIT_INTR_INFO is only valid for EXCEPTION_NMI exits.  For
 	 * EXTERNAL_INTERRUPT, the value for vmcs12->vm_exit_intr_info would
@@ -6635,6 +8098,12 @@ bool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)
 	exit_qual = vmx_get_exit_qual(vcpu);
 
 reflect_vmexit:
+	/*
+	 * 注释:
+	 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+	 * and modify vmcs12 to make it see what it would expect to see there if
+	 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+	 */
 	nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
 	return true;
 }
@@ -6723,10 +8192,41 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	 * vmcs12 state is in the vmcs12 already.
 	 */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用sync_vmcs02_to_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|5629| <<__nested_vmx_vmexit>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7521| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		 */
 		sync_vmcs02_to_vmcs12(vcpu, vmcs12);
+		/*
+		 * 在以下使用sync_vmcs02_to_vmcs12_rare():
+		 *   - arch/x86/kvm/vmx/nested.c|5212| <<copy_vmcs02_to_vmcs12_rare>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|5235| <<sync_vmcs02_to_vmcs12>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7696| <<vmx_get_nested_state>> sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 */
 		sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
 	} else  {
+		/*
+		 * 在以下使用copy_vmcs02_to_vmcs12_rare():
+		 *   - arch/x86/kvm/vmx/nested.c|6293| <<nested_release_vmcs12>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 *   - arch/x86/kvm/vmx/nested.c|6460| <<handle_vmread>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|6610| <<handle_vmwrite>> copy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
+		 *   - arch/x86/kvm/vmx/nested.c|7698| <<vmx_get_nested_state>> copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		 */
 		copy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));
+		/*
+		 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+		 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+		 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+		 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+		 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+		 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+		 */
 		if (!vmx->nested.need_vmcs12_to_shadow_sync) {
 			if (nested_vmx_is_evmptr12_valid(vmx))
 				/*
@@ -6739,6 +8239,29 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 				copy_enlightened_to_vmcs12(vmx, 0);
 			else if (enable_shadow_vmcs)
 				copy_shadow_to_vmcs12(vmx);
+			/*
+			 * 在以下使用copy_shadow_to_vmcs12():
+			 *   - arch/x86/kvm/vmx/nested.c|4229| <<nested_vmx_run>> copy_shadow_to_vmcs12(vmx);
+			 *   - arch/x86/kvm/vmx/nested.c|6316| <<nested_release_vmcs12>> copy_shadow_to_vmcs12(vmx);
+			 *   - arch/x86/kvm/vmx/nested.c|7710| <<vmx_get_nested_state>> copy_shadow_to_vmcs12(vmx);
+			 *
+			 * 在以下使用enable_shadow_vmcs:
+			 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+			 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+			 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+			 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+			 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+			 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+			 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+			 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+			 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+			 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+			 */
 		}
 	}
 
@@ -6754,6 +8277,20 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 
 	if (nested_cpu_has_shadow_vmcs(vmcs12) &&
 	    vmcs12->vmcs_link_pointer != INVALID_GPA) {
+		/*
+		 * 在以下使用get_shadow_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+		 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+		 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+		 */
 		if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
 				 get_shadow_vmcs12(vcpu), VMCS12_SIZE))
 			return -EFAULT;
@@ -6762,6 +8299,13 @@ static int vmx_get_nested_state(struct kvm_vcpu *vcpu,
 	return kvm_state.size;
 }
 
+/*
+ * 在以下使用vmx_leave_nested():
+ *   - arch/x86/kvm/vmx/nested.c|8403| <<global>> struct kvm_x86_nested_ops vmx_nested_ops.leave_nested = vmx_leave_nested,
+ *   - arch/x86/kvm/vmx/nested.c|502| <<nested_vmx_free_vcpu>> vmx_leave_nested(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|7812| <<vmx_set_nested_state>> vmx_leave_nested(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|2367| <<vmx_set_msr(MSR_IA32_FEAT_CTL)>> vmx_leave_nested(vcpu);
+ */
 void vmx_leave_nested(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu)) {
@@ -6771,6 +8315,16 @@ void vmx_leave_nested(struct kvm_vcpu *vcpu)
 	free_nested(vcpu);
 }
 
+/*
+ * 在以下使用kvm_x86_nested_ops->set_state:
+ *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+ *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+ *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+ *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+ *                                     user_kvm_nested_state, &kvm_state);
+ *
+ * struct kvm_x86_nested_ops vmx_nested_ops.set_state = vmx_set_nested_state,
+ */
 static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 				struct kvm_nested_state __user *user_kvm_nested_state,
 				struct kvm_nested_state *kvm_state)
@@ -6911,6 +8465,20 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 	ret = -EINVAL;
 	if (nested_cpu_has_shadow_vmcs(vmcs12) &&
 	    vmcs12->vmcs_link_pointer != INVALID_GPA) {
+		/*
+		 * 在以下使用get_shadow_vmcs12():
+		 *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+		 *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+		 *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+		 *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+		 *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+		 *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+		 *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+		 */
 		struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
 
 		if (kvm_state->size <
@@ -6937,6 +8505,11 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 			kvm_state->hdr.vmx.preemption_timer_deadline;
 	}
 
+	/*
+	 * 在以下使用nested_vmx_check_controls():
+	 *   - arch/x86/kvm/vmx/nested.c|3940| <<nested_vmx_run>> if (nested_vmx_check_controls(vcpu, vmcs12))
+	 *   - arch/x86/kvm/vmx/nested.c|7231| <<vmx_set_nested_state>> if (nested_vmx_check_controls(vcpu, vmcs12) ||
+	 */
 	if (nested_vmx_check_controls(vcpu, vmcs12) ||
 	    nested_vmx_check_host_state(vcpu, vmcs12) ||
 	    nested_vmx_check_guest_state(vcpu, vmcs12, &ignored))
@@ -6944,6 +8517,13 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 
 	vmx->nested.dirty_vmcs12 = true;
 	vmx->nested.force_msr_bitmap_recalc = true;
+	/*
+	 * 在以下使用nested_vmx_enter_non_root_mode():
+	 *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+	 *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 *   - arch/x86/kvm/vmx/nested.h|26| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+	 *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+	 */
 	ret = nested_vmx_enter_non_root_mode(vcpu, false);
 	if (ret)
 		goto error_guest_mode;
@@ -6958,8 +8538,30 @@ static int vmx_set_nested_state(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * 在以下使用nested_vmx_set_vmcs_shadowing_bitmap():
+ *   - arch/x86/kvm/vmx/vmx.c|4938| <<init_vmcs>> nested_vmx_set_vmcs_shadowing_bitmap();
+ */
 void nested_vmx_set_vmcs_shadowing_bitmap(void)
 {
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
 		vmcs_write64(VMREAD_BITMAP, __pa(vmx_vmread_bitmap));
 		vmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));
@@ -7000,6 +8602,10 @@ static u64 nested_vmx_calc_vmcs_enum_msr(void)
 	return (u64)max_idx << VMCS_FIELD_INDEX_SHIFT;
 }
 
+/*
+ * 在以下使用nested_vmx_setup_pinbased_ctls():
+ *   - arch/x86/kvm/vmx/nested.c|8830| <<nested_vmx_setup_ctls_msrs>> nested_vmx_setup_pinbased_ctls(vmcs_conf, msrs);
+ */
 static void nested_vmx_setup_pinbased_ctls(struct vmcs_config *vmcs_conf,
 					   struct nested_vmx_msrs *msrs)
 {
@@ -7096,6 +8702,10 @@ static void nested_vmx_setup_cpubased_ctls(struct vmcs_config *vmcs_conf,
 		~(CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);
 }
 
+/*
+ * 在以下使用nested_vmx_setup_secondary_ctls():
+ *   - arch/x86/kvm/vmx/nested.c|7634| <<nested_vmx_setup_ctls_msrs>> nested_vmx_setup_secondary_ctls(ept_caps, vmcs_conf, msrs);
+ */
 static void nested_vmx_setup_secondary_ctls(u32 ept_caps,
 					    struct vmcs_config *vmcs_conf,
 					    struct nested_vmx_msrs *msrs)
@@ -7237,6 +8847,11 @@ static void nested_vmx_setup_cr_fixed(struct nested_vmx_msrs *msrs)
  * bit in the high half is on if the corresponding bit in the control field
  * may be on. See also vmx_control_verify().
  */
+/*
+ * 在以下使用nested_vmx_setup_ctls_msrs():
+ *   - arch/x86/kvm/vmx/vmx.c|2852| <<vmx_check_processor_compat>> nested_vmx_setup_ctls_msrs(&vmcs_conf, vmx_cap.ept);
+ *   - arch/x86/kvm/vmx/vmx.c|8984| <<vmx_hardware_setup>> nested_vmx_setup_ctls_msrs(&vmcs_config, vmx_capability.ept);
+ */
 void nested_vmx_setup_ctls_msrs(struct vmcs_config *vmcs_conf, u32 ept_caps)
 {
 	struct nested_vmx_msrs *msrs = &vmcs_conf->nested;
@@ -7274,24 +8889,86 @@ void nested_vmx_setup_ctls_msrs(struct vmcs_config *vmcs_conf, u32 ept_caps)
 	msrs->vmcs_enum = nested_vmx_calc_vmcs_enum_msr();
 }
 
+/*
+ * 在以下使用nested_vmx_hardware_unsetup():
+ *   - arch/x86/kvm/vmx/nested.c|8956| <<nested_vmx_hardware_setup>> nested_vmx_hardware_unsetup();
+ *   - arch/x86/kvm/vmx/vmx.c|9017| <<vmx_hardware_unsetup>> nested_vmx_hardware_unsetup();
+ *   - arch/x86/kvm/vmx/vmx.c|9332| <<vmx_hardware_setup>> nested_vmx_hardware_unsetup();
+ */
 void nested_vmx_hardware_unsetup(void)
 {
 	int i;
 
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu)
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
+		/*
+		 * 在以下使用vmx_bitmap[VMX_BITMAP_NR]:
+		 *   - arch/x86/kvm/vmx/nested.c|8139| <<nested_vmx_hardware_setup>> vmx_bitmap[i] = (unsigned long *)
+		 *   - arch/x86/kvm/vmx/nested.c|8141| <<nested_vmx_hardware_setup>> if (!vmx_bitmap[i]) {
+		 *   - arch/x86/kvm/vmx/nested.c|68| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+		 *   - arch/x86/kvm/vmx/nested.c|69| <<vmx_vmwrite_bitmap>> #define vmx_vmwrite_bitmap (vmx_bitmap[VMX_VMWRITE_BITMAP])
+		 *   - arch/x86/kvm/vmx/nested.c|8101| <<nested_vmx_hardware_unsetup>> free_page((unsigned long )vmx_bitmap[i]);
+		 */
 		for (i = 0; i < VMX_BITMAP_NR; i++)
 			free_page((unsigned long)vmx_bitmap[i]);
 	}
 }
 
+/*
+ * 在以下使用nested_vmx_hardware_setup():
+ *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 __init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))
 {
 	int i;
 
 	if (!cpu_has_vmx_shadow_vmcs())
 		enable_shadow_vmcs = 0;
+	/*
+	 * 在以下使用enable_shadow_vmcs:
+	 *   - arch/x86/kvm/vmx/nested.c|23| <<global>> static bool __read_mostly enable_shadow_vmcs = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|24| <<global>> module_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);
+	 *   - arch/x86/kvm/vmx/nested.c|7894| <<nested_vmx_hardware_setup>> enable_shadow_vmcs = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|7895| <<nested_vmx_hardware_setup>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|420| <<free_nested>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|3993| <<nested_vmx_enter_non_root_mode>> if (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx))
+	 *   - arch/x86/kvm/vmx/nested.c|4057| <<nested_vmx_run>> } else if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|5576| <<__nested_vmx_vmexit>> (enable_shadow_vmcs || nested_vmx_is_evmptr12_valid(vmx)))
+	 *   - arch/x86/kvm/vmx/nested.c|5833| <<enter_vmx_operation>> if (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))
+	 *   - arch/x86/kvm/vmx/nested.c|5965| <<nested_release_vmcs12>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|6288| <<handle_vmwrite>> if (enable_shadow_vmcs && is_shadow_field_ro(field)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6317| <<set_current_vmptr>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7283| <<vmx_get_nested_state>> else if (enable_shadow_vmcs)
+	 *   - arch/x86/kvm/vmx/nested.c|7556| <<nested_vmx_set_vmcs_shadowing_bitmap>> if (enable_shadow_vmcs) {
+	 *   - arch/x86/kvm/vmx/nested.c|7883| <<nested_vmx_hardware_unsetup>> if (enable_shadow_vmcs) {
+	 */
 	if (enable_shadow_vmcs) {
 		for (i = 0; i < VMX_BITMAP_NR; i++) {
+			/*
+			 * 在以下使用vmx_bitmap[VMX_BITMAP_NR]:
+			 *   - arch/x86/kvm/vmx/nested.c|8139| <<nested_vmx_hardware_setup>> vmx_bitmap[i] = (unsigned long *)
+			 *   - arch/x86/kvm/vmx/nested.c|8141| <<nested_vmx_hardware_setup>> if (!vmx_bitmap[i]) {
+			 *   - arch/x86/kvm/vmx/nested.c|68| <<vmx_vmread_bitmap>> #define vmx_vmread_bitmap (vmx_bitmap[VMX_VMREAD_BITMAP])
+			 *   - arch/x86/kvm/vmx/nested.c|69| <<vmx_vmwrite_bitmap>> #define vmx_vmwrite_bitmap (vmx_bitmap[VMX_VMWRITE_BITMAP])
+			 *   - arch/x86/kvm/vmx/nested.c|8101| <<nested_vmx_hardware_unsetup>> free_page((unsigned long )vmx_bitmap[i]);
+			 */
 			/*
 			 * The vmx_bitmap is not tied to a VM and so should
 			 * not be charged to a memcg.
diff --git a/arch/x86/kvm/vmx/nested.h b/arch/x86/kvm/vmx/nested.h
index 6eedcfc91..8c391b362 100644
--- a/arch/x86/kvm/vmx/nested.h
+++ b/arch/x86/kvm/vmx/nested.h
@@ -30,6 +30,31 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 			 u32 exit_intr_info, unsigned long exit_qualification,
 			 u32 exit_insn_len);
 
+/*
+ * 在以下使用nested_vmx_vmexit():
+ *   - arch/x86/kvm/vmx/hyperv.c|229| <<vmx_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_vmx_vmexit(vcpu, HV_VMX_SYNTHETIC_EXIT_REASON_TRAP_AFTER_FLUSH, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|460| <<nested_ept_inject_page_fault>> nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification);
+ *   - arch/x86/kvm/vmx/nested.c|4045| <<nested_vmx_inject_exception_vmexit>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|4269| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_INIT_SIGNAL, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4283| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_SIPI_SIGNAL, 0,
+ *   - arch/x86/kvm/vmx/nested.c|4320| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_MONITOR_TRAP_FLAG, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4341| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4357| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,
+ *   - arch/x86/kvm/vmx/nested.c|4383| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|4392| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT,
+ *   - arch/x86/kvm/vmx/nested.c|4440| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT,
+ *   - arch/x86/kvm/vmx/nested.c|5179| <<nested_vmx_triple_fault>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/nested.c|6183| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->vt.exit_reason.full,
+ *   - arch/x86/kvm/vmx/nested.c|6673| <<nested_vmx_reflect_vmexit>> nested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);
+ *   - arch/x86/kvm/vmx/nested.c|6804| <<vmx_leave_nested>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|6507| <<__vmx_handle_exit>> nested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|8336| <<vmx_enter_smm>> nested_vmx_vmexit(vcpu, -1, 0, 0);
+ *
+ * 注释:
+ * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+ * and modify vmcs12 to make it see what it would expect to see there if
+ * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+ */
 static inline void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 				     u32 exit_intr_info,
 				     unsigned long exit_qualification)
@@ -42,6 +67,16 @@ static inline void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	else
 		exit_insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 
+	/*
+	 * 在以下使用__nested_vmx_vmexit():
+	 *   - arch/x86/kvm/vmx/nested.h|45| <<nested_vmx_vmexit>> __nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|8211| <<vmx_check_intercept>> __nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
+	 *
+	 * 注释:
+	 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+	 * and modify vmcs12 to make it see what it would expect to see there if
+	 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+	 */
 	__nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
 			    exit_qualification, exit_insn_len);
 }
@@ -60,14 +95,44 @@ static inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)
 	lockdep_assert_once(lockdep_is_held(&vcpu->mutex) ||
 			    !refcount_read(&vcpu->kvm->users_count));
 
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	return to_vmx(vcpu)->nested.cached_vmcs12;
 }
 
+/*
+ * 在以下使用get_shadow_vmcs12():
+ *   - arch/x86/kvm/vmx/nested.c|851| <<nested_cache_shadow_vmcs12>> kvm_read_guest_cached(vmx->vcpu.kvm,
+ *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+ *   - arch/x86/kvm/vmx/nested.c|870| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm,
+ *            ghc, get_shadow_vmcs12(vcpu), VMCS12_SIZE);
+ *   - arch/x86/kvm/vmx/nested.c|6029| <<handle_vmread>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+ *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6135| <<handle_vmwrite>> struct vmcs12 *vmcs12 = is_guest_mode(vcpu) ?
+ *            get_shadow_vmcs12(vcpu) : get_vmcs12(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|7233| <<vmx_get_nested_state>> if (copy_to_user(user_vmx_nested_state->shadow_vmcs12,
+ *            get_shadow_vmcs12(vcpu), VMCS12_SIZE))
+ *   - arch/x86/kvm/vmx/nested.c|7399| <<vmx_set_nested_state>> struct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);
+ */
 static inline struct vmcs12 *get_shadow_vmcs12(struct kvm_vcpu *vcpu)
 {
 	lockdep_assert_once(lockdep_is_held(&vcpu->mutex) ||
 			    !refcount_read(&vcpu->kvm->users_count));
 
+	/*
+	 * struct vcpu_vmx *vmx:
+	 * -> struct nested_vmx nested;
+	 *    -> struct vmcs12 *cached_vmcs12;
+	 *    -> struct vmcs12 *cached_shadow_vmcs12;
+	 *    -> struct gfn_to_hva_cache shadow_vmcs12_cache;
+	 *    -> struct gfn_to_hva_cache vmcs12_cache;
+	 */
 	return to_vmx(vcpu)->nested.cached_shadow_vmcs12;
 }
 
@@ -201,6 +266,12 @@ static inline bool nested_cpu_has_pml(struct vmcs12 *vmcs12)
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_PML);
 }
 
+/*
+ * 在以下使用nested_cpu_has_virt_x2apic_mode():
+ *   - arch/x86/kvm/vmx/nested.c|831| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_virt_x2apic_mode(vmcs12)) {
+ *   - arch/x86/kvm/vmx/nested.c|1034| <<nested_vmx_check_apicv_controls>> if (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ *   - arch/x86/kvm/vmx/nested.c|1044| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&
+ */
 static inline bool nested_cpu_has_virt_x2apic_mode(struct vmcs12 *vmcs12)
 {
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);
@@ -211,13 +282,54 @@ static inline bool nested_cpu_has_vpid(struct vmcs12 *vmcs12)
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VPID);
 }
 
+/*
+ * 在以下使用nested_cpu_has_apic_reg_virt():
+ *   - arch/x86/kvm/vmx/nested.c|746| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_apic_reg_virt(vmcs12)) {
+ *   - arch/x86/kvm/vmx/nested.c|879| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_apic_reg_virt(vmcs12) &&
+ */
 static inline bool nested_cpu_has_apic_reg_virt(struct vmcs12 *vmcs12)
 {
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_APIC_REGISTER_VIRT);
 }
 
+/*
+ * 在以下使用nested_cpu_has_vid():
+ *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+ *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+ *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+ *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+ *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+ *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+ *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+ */
 static inline bool nested_cpu_has_vid(struct vmcs12 *vmcs12)
 {
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 }
 
@@ -258,6 +370,18 @@ static inline bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)
  * In nested virtualization, check if L1 asked to exit on external interrupts.
  * For most existing hypervisors, this will always return true.
  */
+/*
+ * 在以下使用nested_exit_on_intr():
+ *   - arch/x86/kvm/vmx/nested.c|1052| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+ *   - arch/x86/kvm/vmx/nested.c|4938| <<vmx_check_nested_events>> if (!nested_exit_on_intr(vcpu)) {
+ *   - arch/x86/kvm/vmx/nested.c|7368| <<nested_vmx_l1_wants_exit>> return nested_exit_on_intr(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5322| <<vmx_interrupt_blocked>> if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|5337| <<vmx_interrupt_allowed>> if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+ *
+ * 注释:
+ * In nested virtualization, check if L1 asked to exit on external interrupts.
+ * For most existing hypervisors, this will always return true.
+ */
 static inline bool nested_exit_on_intr(struct kvm_vcpu *vcpu)
 {
 	return get_vmcs12(vcpu)->pin_based_vm_exec_control &
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 0b1736028..1908067ef 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -678,6 +678,32 @@ static void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)
 	int i;
 
 	for (i = 0; i < lbr->nr; i++) {
+		/*
+		 * 在以下使用vmx_set_intercept_for_msr():
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+		 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+		 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+		 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+		 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+		 *                      !to_vmx(vcpu)->spec_ctrl);
+		 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+		 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+		 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+		 *                      !guest_has_pred_cmd_msr(vcpu));
+		 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+		 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+		 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+		 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+		 */
 		vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
 		vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
 		if (lbr->info)
diff --git a/arch/x86/kvm/vmx/tdx.c b/arch/x86/kvm/vmx/tdx.c
index 66744f576..eac657ad3 100644
--- a/arch/x86/kvm/vmx/tdx.c
+++ b/arch/x86/kvm/vmx/tdx.c
@@ -671,6 +671,16 @@ int tdx_vcpu_create(struct kvm_vcpu *vcpu)
 	if (kvm_tdx->state != TD_STATE_INITIALIZED)
 		return -EIO;
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	/*
 	 * TDX module mandates APICv, which requires an in-kernel local APIC.
 	 * Disallow an in-kernel I/O APIC, because level-triggered interrupts
@@ -680,6 +690,18 @@ int tdx_vcpu_create(struct kvm_vcpu *vcpu)
 		return -EINVAL;
 
 	fpstate_set_confidential(&vcpu->arch.guest_fpu);
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	vcpu->arch.apic->guest_apic_protected = true;
 	INIT_LIST_HEAD(&tdx->vt.pi_wakeup_list);
 
@@ -3114,6 +3136,13 @@ static int tdx_vcpu_init(struct kvm_vcpu *vcpu, struct kvm_tdx_cmd *cmd)
 	 */
 	apic_base = APIC_DEFAULT_PHYS_BASE | LAPIC_MODE_X2APIC |
 		(kvm_vcpu_is_reset_bsp(vcpu) ? MSR_IA32_APICBASE_BSP : 0);
+	/*
+	 * 在以下使用kvm_apic_set_base():
+	 *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+	 *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+	 *                         msr_info->host_initiated);
+	 *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+	 */
 	if (kvm_apic_set_base(vcpu, apic_base, true))
 		return -EINVAL;
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index aa157fe5b..2ee3b5ec7 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1204,6 +1204,13 @@ static void pt_guest_exit(struct vcpu_vmx *vmx)
 		wrmsrq(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);
 }
 
+/*
+ * 在以下使用vmx_set_host_fs_gs():
+ *   - arch/x86/kvm/vmx/nested.c|285| <<vmx_sync_vmcs_host_state>> vmx_set_host_fs_gs(dest,
+ *                     src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
+ *   - arch/x86/kvm/vmx/vmx.c|1303| <<vmx_prepare_switch_to_guest>> vmx_set_host_fs_gs(host_state,
+ *                     fs_sel, gs_sel, fs_base, gs_base);
+ */
 void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
 			unsigned long fs_base, unsigned long gs_base)
 {
@@ -1231,6 +1238,17 @@ void vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,
 	}
 }
 
+/*
+ * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+ *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+ *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+ *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+ *
+ *
+ * 在以下使用vmx_prepare_switch_to_guest():
+ *   - arch/x86/kvm/vmx/main.c|131| <<vt_prepare_switch_to_guest>> vmx_prepare_switch_to_guest(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|3628| <<nested_vmx_check_vmentry_hw>> vmx_prepare_switch_to_guest(vcpu);
+ */
 void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1260,6 +1278,19 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	if (vmx->nested.need_vmcs12_to_shadow_sync)
 		nested_sync_vmcs12_to_shadow(vcpu);
 
@@ -1300,6 +1331,13 @@ void vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)
 	gs_base = segment_base(gs_sel);
 #endif
 
+	/*
+	 * 在以下使用vmx_set_host_fs_gs():
+	 *   - arch/x86/kvm/vmx/nested.c|285| <<vmx_sync_vmcs_host_state>> vmx_set_host_fs_gs(dest,
+	 *                     src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);
+	 *   - arch/x86/kvm/vmx/vmx.c|1303| <<vmx_prepare_switch_to_guest>> vmx_set_host_fs_gs(host_state,
+	 *                     fs_sel, gs_sel, fs_base, gs_base);
+	 */
 	vmx_set_host_fs_gs(host_state, fs_sel, gs_sel, fs_base, gs_base);
 	vt->guest_state_loaded = true;
 }
@@ -1395,6 +1433,13 @@ static void shrink_ple_window(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用vmx_vcpu_load_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ */
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1421,6 +1466,16 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu)
 	prev = per_cpu(current_vmcs, cpu);
 	if (prev != vmx->loaded_vmcs->vmcs) {
 		per_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;
+		/*
+		 * 在以下使用vmcs_load():
+		 *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+		 *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+		 */
 		vmcs_load(vmx->loaded_vmcs->vmcs);
 	}
 
@@ -1460,6 +1515,13 @@ void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (vcpu->scheduled_out && !kvm_pause_in_guest(vcpu->kvm))
 		shrink_ple_window(vcpu);
 
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|314| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4659| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4664| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1463| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 
 	vmx_vcpu_pi_load(vcpu, cpu);
@@ -1812,6 +1874,11 @@ void vmx_inject_exception(struct kvm_vcpu *vcpu)
 	} else
 		intr_info |= INTR_TYPE_HARD_EXCEPTION;
 
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);
 
 	vmx_clear_hlt(vcpu);
@@ -2558,6 +2625,11 @@ static u64 adjust_vmx_controls64(u64 ctl_opt, u32 msr)
 	r;									\
 })
 
+/*
+ * 在以下使用setup_vmcs_config():
+ *   - arch/x86/kvm/vmx/vmx.c|2847| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
+ *   - arch/x86/kvm/vmx/vmx.c|8821| <<vmx_hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+ */
 static int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 			     struct vmx_capability *vmx_cap)
 {
@@ -2609,6 +2681,33 @@ static int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 		_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;
 #endif
 
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))
 		_cpu_based_2nd_exec_control &= ~(
 				SECONDARY_EXEC_APIC_REGISTER_VIRT |
@@ -2774,6 +2873,11 @@ int vmx_check_processor_compat(void)
 	if (!__kvm_is_vmx_supported())
 		return -EIO;
 
+	/*
+	 * 在以下使用setup_vmcs_config():
+	 *   - arch/x86/kvm/vmx/vmx.c|2847| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8821| <<vmx_hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+	 */
 	if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
 		pr_err("Failed to setup VMCS config on CPU %d\n", cpu);
 		return -EIO;
@@ -3967,6 +4071,32 @@ static void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)
 	vmx->nested.force_msr_bitmap_recalc = true;
 }
 
+/*
+ * 在以下使用vmx_set_intercept_for_msr():
+ *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+ *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+ *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+ *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+ *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+ *                      !to_vmx(vcpu)->spec_ctrl);
+ *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+ *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+ *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *                      !guest_has_pred_cmd_msr(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+ *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+ *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+ *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+ */
 void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type, bool set)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3992,6 +4122,11 @@ void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type, bool se
 	}
 }
 
+/*
+ * 在以下使用vmx_update_msr_bitmap_x2apic():
+ *   - arch/x86/kvm/vmx/vmx.c|4442| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6859| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+ */
 static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -4036,6 +4171,32 @@ static void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)
 		msr_bitmap[read_idx] = ~0ull;
 	msr_bitmap[write_idx] = ~0ull;
 
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	/*
 	 * TPR reads and writes can be virtualized even if virtual interrupt
 	 * delivery is not in use.
@@ -4058,6 +4219,32 @@ void pt_update_intercept_for_msr(struct kvm_vcpu *vcpu)
 	bool flag = !(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN);
 	u32 i;
 
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
@@ -4100,6 +4287,32 @@ void vmx_recalc_msr_intercepts(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		vmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD, MSR_TYPE_RW);
 
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
 				  !to_vmx(vcpu)->spec_ctrl);
 
@@ -4121,6 +4334,10 @@ void vmx_recalc_msr_intercepts(struct kvm_vcpu *vcpu)
 	 */
 }
 
+/*
+ * 在以下使用vmx_deliver_nested_posted_interrupt():
+ *   - arch/x86/kvm/vmx/vmx.c|4180| <<vmx_deliver_posted_interrupt>> r = vmx_deliver_nested_posted_interrupt(vcpu, vector);
+ */
 static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
 						int vector)
 {
@@ -4133,8 +4350,28 @@ static int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,
 	 * enabled in its vmcs12, i.e. checking the vector also checks that
 	 * L1 has enabled posted interrupts for L2.
 	 */
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	if (is_guest_mode(vcpu) &&
 	    vector == vmx->nested.posted_intr_nv) {
+		/*
+		 * 在以下使用nested_vmx->pi_pending:
+		 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+		 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+		 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+		 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+		 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+		 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+		 */
 		/*
 		 * If a posted intr is not recognized by hardware,
 		 * we will accomplish it in the next vmentry.
@@ -4176,6 +4413,12 @@ static int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)
 	if (!r)
 		return 0;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	/* Note, this is called iff the local APIC is in-kernel. */
 	if (!vcpu->arch.apic->apicv_active)
 		return -1;
@@ -4190,6 +4433,11 @@ void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
 	struct kvm_vcpu *vcpu = apic->vcpu;
 
 	if (vmx_deliver_posted_interrupt(vcpu, vector)) {
+		/*
+		 * 在以下使用kvm_lapic_set_irr():
+		 *   - arch/x86/kvm/svm/svm.c|3844| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+		 *   - arch/x86/kvm/vmx/vmx.c|4436| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+		 */
 		kvm_lapic_set_irr(vector, apic);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 		kvm_vcpu_kick(vcpu);
@@ -4340,17 +4588,63 @@ static u32 vmx_vmexit_ctrl(void)
 		~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);
 }
 
+/*
+ * 在以下使用refresh_apicv_exec_ctrl:
+ *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+ *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+ *
+ * 在以下使用vmx_refresh_apicv_exec_ctrl():
+ *   - arch/x86/kvm/vmx/main.c|740| <<vt_refresh_apicv_exec_ctrl>> vmx_refresh_apicv_exec_ctrl(vcpu);
+ *
+ * vcpu_enter_guest()
+ * -> kvm_vcpu_update_apicv()
+ *    -> __kvm_vcpu_update_apicv()
+ *       -> vt_refresh_apicv_exec_ctrl()
+ */
 void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->pdate_vmcs01_apicv_status:
+		 *   - arch/x86/kvm/vmx/nested.c|5114| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+		 *   - arch/x86/kvm/vmx/nested.c|5115| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+		 */
 		vmx->nested.update_vmcs01_apicv_status = true;
 		return;
 	}
 
 	pin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));
 
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (kvm_vcpu_apicv_active(vcpu)) {
 		secondary_exec_controls_setbit(vmx,
 					       SECONDARY_EXEC_APIC_REGISTER_VIRT |
@@ -4365,6 +4659,11 @@ void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 			tertiary_exec_controls_clearbit(vmx, TERTIARY_EXEC_IPI_VIRT);
 	}
 
+	/*
+	 * 在以下使用vmx_update_msr_bitmap_x2apic():
+	 *   - arch/x86/kvm/vmx/vmx.c|4442| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6859| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 */
 	vmx_update_msr_bitmap_x2apic(vcpu);
 }
 
@@ -4490,6 +4789,11 @@ vmx_adjust_secondary_exec_control(struct vcpu_vmx *vmx, u32 *exec_control,
 #define vmx_adjust_sec_exec_exiting(vmx, exec_control, lname, uname) \
 	vmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, uname##_EXITING, true)
 
+/*
+ * 在以下使用vmx_secondary_exec_control():
+ *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
+ *   - arch/x86/kvm/vmx/vmx.c|8195| <<vmx_vcpu_after_set_cpuid>> vmx_secondary_exec_control(vmx));
+ */
 static u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)
 {
 	struct kvm_vcpu *vcpu = &vmx->vcpu;
@@ -4511,6 +4815,32 @@ static u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)
 		exec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;
 	if (kvm_pause_in_guest(vmx->vcpu.kvm))
 		exec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (!kvm_vcpu_apicv_active(vcpu))
 		exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |
 				  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
@@ -4610,11 +4940,18 @@ int vmx_vcpu_precreate(struct kvm *kvm)
 
 #define VMX_XSS_EXIT_BITMAP 0
 
+/*
+ * 在以下使用init_vmcs():
+ *   - arch/x86/kvm/vmx/vmx.c|4921| <<__vmx_vcpu_reset>> init_vmcs(vmx);
+ */
 static void init_vmcs(struct vcpu_vmx *vmx)
 {
 	struct kvm *kvm = vmx->vcpu.kvm;
 	struct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);
 
+	/*
+	 * 只在这里使用nested_vmx_set_vmcs_shadowing_bitmap()
+	 */
 	if (nested)
 		nested_vmx_set_vmcs_shadowing_bitmap();
 
@@ -4629,6 +4966,11 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	exec_controls_set(vmx, vmx_exec_control(vmx));
 
 	if (cpu_has_secondary_exec_ctrls()) {
+		/*
+		 * 在以下使用vmx_secondary_exec_control():
+		 *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
+		 *   - arch/x86/kvm/vmx/vmx.c|8195| <<vmx_vcpu_after_set_cpuid>> vmx_secondary_exec_control(vmx));
+		 */
 		secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
 		if (vmx->ve_info)
 			vmcs_write64(VE_INFORMATION_ADDRESS,
@@ -4723,6 +5065,14 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	vmx_guest_debugctl_write(&vmx->vcpu, 0);
 
 	if (cpu_has_vmx_tpr_shadow()) {
+		/*
+		 * 在以下使用VIRTUAL_APIC_PAGE_ADDR:
+		 *   - arch/x86/kvm/vmx/nested.c|3488| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
+		 *   - arch/x86/kvm/vmx/nested.c|3506| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);
+		 *   - arch/x86/kvm/vmx/vmx.c|4800| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|4802| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, __pa(vmx->vcpu.arch.apic->regs));
+		 *   - arch/x86/kvm/vmx/vmx.c|6482| <<dump_vmcs>> pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
+		 */
 		vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
 		if (cpu_need_tpr_shadow(&vmx->vcpu))
 			vmcs_write64(VIRTUAL_APIC_PAGE_ADDR,
@@ -4733,6 +5083,10 @@ static void init_vmcs(struct vcpu_vmx *vmx)
 	vmx_setup_uret_msrs(vmx);
 }
 
+/*
+ * 在以下使用__vmx_vcpu_reset():
+ *   - arch/x86/kvm/vmx/vmx.c|4965| <<vmx_vcpu_reset>> __vmx_vcpu_reset(vcpu);
+ */
 static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4745,6 +5099,17 @@ static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
 
 	vcpu_setup_sgx_lepubkeyhash(vcpu);
 
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	vmx->nested.posted_intr_nv = -1;
 	vmx->nested.vmxon_ptr = INVALID_GPA;
 	vmx->nested.current_vmptr = INVALID_GPA;
@@ -4765,6 +5130,10 @@ static void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)
 	__pi_set_sn(&vmx->vt.pi_desc);
 }
 
+/*
+ * 在以下使用vmx_vcpu_reset():
+ *   - arch/x86/kvm/vmx/main.c|93| <<vt_vcpu_reset>> vmx_vcpu_reset(vcpu, init_event);
+ */
 void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4815,6 +5184,11 @@ void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	if (kvm_mpx_supported())
 		vmcs_write64(GUEST_BNDCFGS, 0);
 
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */
 
 	kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
@@ -4826,6 +5200,12 @@ void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 void vmx_enable_irq_window(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 当guest中断控制状态变为"可以接收中断"的状态(例如EFLAGS.IF = 1, guest
+	 * 允许外部中断,而且guest的GUEST_INTERRUPTIBILITY_INFO表明没有阻止中断的状态)时,CPU
+	 * 将进行VM-exit给hypervisor,因为你请求了interrupt-window
+	 * exiting.见代码:
+	 */
 	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);
 }
 
@@ -4840,6 +5220,10 @@ void vmx_enable_nmi_window(struct kvm_vcpu *vcpu)
 	exec_controls_setbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);
 }
 
+/*
+ * 在以下使用vmx_inject_irq():
+ *   - arch/x86/kvm/vmx/main.c|645| <<vt_inject_irq>> vmx_inject_irq(vcpu, reinjected);
+ */
 void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -4863,6 +5247,11 @@ void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 			     vmx->vcpu.arch.event_exit_inst_len);
 	} else
 		intr |= INTR_TYPE_EXT_INTR;
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
 
 	vmx_clear_hlt(vcpu);
@@ -4893,6 +5282,11 @@ void vmx_inject_nmi(struct kvm_vcpu *vcpu)
 		return;
 	}
 
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD,
 			INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);
 
@@ -4967,6 +5361,18 @@ bool __vmx_interrupt_blocked(struct kvm_vcpu *vcpu)
 
 bool vmx_interrupt_blocked(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用nested_exit_on_intr():
+	 *   - arch/x86/kvm/vmx/nested.c|1052| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|4938| <<vmx_check_nested_events>> if (!nested_exit_on_intr(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|7368| <<nested_vmx_l1_wants_exit>> return nested_exit_on_intr(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<vmx_interrupt_blocked>> if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *   - arch/x86/kvm/vmx/vmx.c|5337| <<vmx_interrupt_allowed>> if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *
+	 * 注释:
+	 * In nested virtualization, check if L1 asked to exit on external interrupts.
+	 * For most existing hypervisors, this will always return true.
+	 */
 	if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
 		return false;
 
@@ -4978,6 +5384,18 @@ int vmx_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 	if (to_vmx(vcpu)->nested.nested_run_pending)
 		return -EBUSY;
 
+	/*
+	 * 在以下使用nested_exit_on_intr():
+	 *   - arch/x86/kvm/vmx/nested.c|1052| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+	 *   - arch/x86/kvm/vmx/nested.c|4938| <<vmx_check_nested_events>> if (!nested_exit_on_intr(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|7368| <<nested_vmx_l1_wants_exit>> return nested_exit_on_intr(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5322| <<vmx_interrupt_blocked>> if (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *   - arch/x86/kvm/vmx/vmx.c|5337| <<vmx_interrupt_allowed>> if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
+	 *
+	 * 注释:
+	 * In nested virtualization, check if L1 asked to exit on external interrupts.
+	 * For most existing hypervisors, this will always return true.
+	 */
 	/*
 	 * An IRQ must not be injected into L2 if it's supposed to VM-Exit,
 	 * e.g. if the IRQ arrived asynchronously after checking nested events.
@@ -5405,6 +5823,18 @@ static int handle_cr(struct kvm_vcpu *vcpu)
 			err = handle_set_cr4(vcpu, val);
 			return kvm_complete_insn_gp(vcpu, err);
 		case 8: {
+				/*
+				 * 在以下使用kvm_get_cr8():
+				 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+				 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+				 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+				 */
 				u8 cr8_prev = kvm_get_cr8(vcpu);
 				u8 cr8 = (u8)val;
 				err = kvm_set_cr8(vcpu, cr8);
@@ -5436,6 +5866,18 @@ static int handle_cr(struct kvm_vcpu *vcpu)
 			trace_kvm_cr_read(cr, val);
 			return kvm_skip_emulated_instruction(vcpu);
 		case 8:
+			/*
+			 * 在以下使用kvm_get_cr8():
+			 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+			 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+			 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+			 */
 			val = kvm_get_cr8(vcpu);
 			kvm_register_write(vcpu, reg, val);
 			trace_kvm_cr_read(cr, val);
@@ -5587,11 +6029,17 @@ static int handle_apic_access(struct kvm_vcpu *vcpu)
 	return kvm_emulate_instruction(vcpu, 0);
 }
 
+/*
+ * 处理EXIT_REASON_EOI_INDUCED
+ */
 static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmx_get_exit_qual(vcpu);
 	int vector = exit_qualification & 0xff;
 
+	/*
+	 * 只在这里使用kvm_apic_set_eoi_accelerated()
+	 */
 	/* EOI-induced VM exit is trap-like and thus no need to adjust IP */
 	kvm_apic_set_eoi_accelerated(vcpu, vector);
 	return 1;
@@ -5722,6 +6170,13 @@ static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_mmu_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4860| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *   - arch/x86/kvm/svm/svm.c|2000| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *   - arch/x86/kvm/vmx/common.h|117| <<__vmx_handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|6163| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 }
 
@@ -5864,6 +6319,29 @@ static int handle_invpcid(struct kvm_vcpu *vcpu)
 	gpr_index = vmx_get_instr_info_reg2(vmx_instruction_info);
 	type = kvm_register_read(vcpu, gpr_index);
 
+	/*
+	 * 在以下使用get_vmx_mem_address():
+	 *   - arch/x86/kvm/vmx/nested.c|6211| <<nested_vmx_get_vmptr>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmcs_read32(VMX_INSTRUCTION_INFO), false, sizeof(*vmpointer), &gva)) {
+	 *   - arch/x86/kvm/vmx/nested.c|6634| <<handle_vmread>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, true, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<handle_vmwrite>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qualification, instr_info, false, len, &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6936| <<handle_vmptrst>> if (get_vmx_mem_address(vcpu,
+	 *           exit_qual, instr_info, true, sizeof(gpa_t), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|6984| <<handle_invept>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/nested.c|7067| <<handle_invvpid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *   - arch/x86/kvm/vmx/vmx.c|6287| <<handle_invpcid>> if (get_vmx_mem_address(vcpu,
+	 *           vmx_get_exit_qual(vcpu), vmx_instruction_info, false, sizeof(operand), &gva))
+	 *
+	 * 注释:
+	 * Decode the memory-address operand of a vmx instruction, as recorded on an
+	 * exit caused by such an instruction (run by a guest hypervisor).
+	 * On success, returns 0. When the operand is invalid, returns 1 and throws
+	 * #UD, #GP, or #SS.
+	 */
 	/* According to the Intel instruction reference, the memory operand
 	 * is read even if it isn't needed (e.g., for type==all)
 	 */
@@ -6008,6 +6486,12 @@ static int handle_notify(struct kvm_vcpu *vcpu)
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
  * to be done to userspace and return 0.
  */
+/*
+ * 在以下使用kvm_vmx_exit_handlers[]数组:
+ *   - arch/x86/kvm/vmx/vmx.c|6969| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+ *   - arch/x86/kvm/vmx/vmx.c|6972| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+ */
 static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,
 	[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,
@@ -6066,6 +6550,10 @@ static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 static const int kvm_vmx_max_exit_handlers =
 	ARRAY_SIZE(kvm_vmx_exit_handlers);
 
+/*
+ * 在以下使用vmx_get_exit_info():
+ *   - arch/x86/kvm/vmx/main.c|700| <<vt_get_exit_info>> vmx_get_exit_info(vcpu, reason, info1, info2, intr_info, error_code);
+ */
 void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 		       u64 *info1, u64 *info2, u32 *intr_info, u32 *error_code)
 {
@@ -6089,6 +6577,11 @@ void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,
 
 void vmx_get_entry_info(struct kvm_vcpu *vcpu, u32 *intr_info, u32 *error_code)
 {
+	/*
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	*intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
 	if (is_exception_with_error_code(*intr_info))
 		*error_code = vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE);
@@ -6257,6 +6750,32 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 	pr_err("Interruptibility = %08x  ActivityState = %08x\n",
 	       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
 	       vmcs_read32(GUEST_ACTIVITY_STATE));
+	/*
+	 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+	 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+	 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+	 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+	 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+	 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+	 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+	 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+	 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+	 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+	 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+	 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+	 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+	 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+	 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+	 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+	 */
 	if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
 		pr_err("InterruptStatus = %04x\n",
 		       vmcs_read16(GUEST_INTR_STATUS));
@@ -6323,6 +6842,32 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 		pr_err("TSC Multiplier = 0x%016llx\n",
 		       vmcs_read64(TSC_MULTIPLIER));
 	if (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW) {
+		/*
+		 * 使用SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY的地方:
+		 *   - arch/x86/kvm/vmx/capabilities.h|195| <<cpu_has_vmx_virtual_intr_delivery>>
+		 *      return vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;
+		 *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02_early>>
+		 *      exec_control &= ~( ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.c|2600| <<prepare_vmcs02_early>>
+		 *      if (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/nested.c|7443| <<nested_vmx_setup_secondary_ctls>>
+		 *      msrs->secondary_ctls_high &= ... | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |
+		 *   - arch/x86/kvm/vmx/nested.h|241| <<nested_cpu_has_vid>>
+		 *      return nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2659| <<setup_vmcs_config>> _cpu_based_2nd_exec_control &= ~(
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT |
+		 *      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|2709| <<setup_vmcs_config>>
+		 *      if (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		 *   - arch/x86/kvm/vmx/vmx.c|4431| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_setbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4437| <<vmx_refresh_apicv_exec_ctrl>> secondary_exec_controls_clearbit(vmx,
+		 *      SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|4590| <<vmx_secondary_exec_control>> if (!kvm_vcpu_apicv_active(vcpu))
+		 *      exec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
+		 *   - arch/x86/kvm/vmx/vmx.c|6417| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)
+		 *   - arch/x86/kvm/vmx/vmx.c|6483| <<dump_vmcs>> if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
+		 */
 		if (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {
 			u16 status = vmcs_read16(GUEST_INTR_STATUS);
 			pr_err("SVI|RVI = %02x|%02x ", status >> 8, status & 0xff);
@@ -6330,6 +6875,14 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 		pr_cont("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
 		if (secondary_exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)
 			pr_err("APIC-access addr = 0x%016llx ", vmcs_read64(APIC_ACCESS_ADDR));
+		/*
+		 * 在以下使用VIRTUAL_APIC_PAGE_ADDR:
+		 *   - arch/x86/kvm/vmx/nested.c|3488| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));
+		 *   - arch/x86/kvm/vmx/nested.c|3506| <<nested_get_vmcs12_pages>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);
+		 *   - arch/x86/kvm/vmx/vmx.c|4800| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|4802| <<init_vmcs>> vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, __pa(vmx->vcpu.arch.apic->regs));
+		 *   - arch/x86/kvm/vmx/vmx.c|6482| <<dump_vmcs>> pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
+		 */
 		pr_cont("virt-APIC addr = 0x%016llx\n", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));
 	}
 	if (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)
@@ -6366,10 +6919,17 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*
+ * 在以下使用__vmx_handle_exit():
+ *   - arch/x86/kvm/vmx/vmx.c|6982| <<vmx_handle_exit>> int ret = __vmx_handle_exit(vcpu, exit_fastpath);
+ */
 static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	union vmx_exit_reason exit_reason = vmx_get_exit_reason(vcpu);
+	/*
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	u32 vectoring_info = vmx->idt_vectoring_info;
 	u16 exit_handler_index;
 
@@ -6430,6 +6990,12 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 			return 1;
 		}
 
+		/*
+		 * 只在这里调用
+		 * 注释:
+		 * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was
+		 * reflected into L1.
+		 */
 		if (nested_vmx_reflect_vmexit(vcpu))
 			return 1;
 	}
@@ -6464,6 +7030,11 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	     exit_reason.basic != EXIT_REASON_TASK_SWITCH &&
 	     exit_reason.basic != EXIT_REASON_NOTIFY &&
 	     exit_reason.basic != EXIT_REASON_EPT_MISCONFIG)) {
+		/*
+		 * 在以下使用kvm_prepare_event_vectoring_exit():
+		 *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
+		 *   - arch/x86/kvm/x86.c|9163| <<x86_emulate_instruction>> kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
+		 */
 		kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
 		return 0;
 	}
@@ -6509,6 +7080,12 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 
 	exit_handler_index = array_index_nospec((u16)exit_reason.basic,
 						kvm_vmx_max_exit_handlers);
+	/*
+	 * 在以下使用kvm_vmx_exit_handlers[]数组:
+	 *   - arch/x86/kvm/vmx/vmx.c|6969| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+	 */
 	if (!kvm_vmx_exit_handlers[exit_handler_index])
 		goto unexpected_vmexit;
 
@@ -6527,6 +7104,16 @@ static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_x86_ops->handle_exit:
+ *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+ *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+ *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+ *
+ *
+ * 在以下使用vmx_handle_exit():
+ *   - arch/x86/kvm/vmx/main.c|166| <<vt_handle_exit>> return vmx_handle_exit(vcpu, fastpath);
+ */
 int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)
 {
 	int ret = __vmx_handle_exit(vcpu, exit_fastpath);
@@ -6646,6 +7233,12 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 
 	/* Postpone execution until vmcs01 is the current VMCS. */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page:
+		 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+		 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+		 */
 		vmx->nested.change_vmcs01_virtual_apic_mode = true;
 		return;
 	}
@@ -6683,9 +7276,18 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	}
 	secondary_exec_controls_set(vmx, sec_exec_control);
 
+	/*
+	 * 在以下使用vmx_update_msr_bitmap_x2apic():
+	 *   - arch/x86/kvm/vmx/vmx.c|4442| <<vmx_refresh_apicv_exec_ctrl>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|6859| <<vmx_set_virtual_apic_mode>> vmx_update_msr_bitmap_x2apic(vcpu);
+	 */
 	vmx_update_msr_bitmap_x2apic(vcpu);
 }
 
+/*
+ * 在以下使用vmx_set_apic_access_page_addr():
+ *   - arch/x86/kvm/vmx/main.c|730| <<vt_set_apic_access_page_addr>> vmx_set_apic_access_page_addr(vcpu);
+ */
 void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 {
 	const gfn_t gfn = APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT;
@@ -6699,6 +7301,12 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 
 	/* Defer reload until vmcs01 is the current VMCS. */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用reload_vmcs01_apic_access_page:
+		 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+		 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+		 */
 		to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
 		return;
 	}
@@ -6707,6 +7315,15 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 	    SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))
 		return;
 
+	/*
+	 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+	 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+	 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+	 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+	 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+	 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+	 */
 	/*
 	 * Explicitly grab the memslot using KVM's internal slot ID to ensure
 	 * KVM doesn't unintentionally grab a userspace memslot.  It _should_
@@ -6757,11 +7374,50 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 	read_unlock(&vcpu->kvm->mmu_lock);
 }
 
+/*
+ * kvm_vcpu_reset()
+ * -> if (is_guest_mode(vcpu))
+ *        kvm_leave_nested(vcpu);
+ * -> kvm_lapic_reset()
+ *    -> if (apic->apicv_active)
+ *           kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ * -> WARN_ON_ONCE(is_guest_mode(vcpu) || is_smm(vcpu));
+ *
+ *
+ * kvm_arch_vcpu_ioctl(KVM_SET_LAPIC)
+ * -> kvm_vcpu_ioctl_set_lapic()
+ *    -> kvm_apic_set_state()
+ *       -> if (apic->apicv_active)
+ *              kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *
+ *
+ * 在以下使用vt_hwapic_isr_update():
+ *   - arch/x86/kvm/vmx/main.c|952| <<global>> .hwapic_isr_update = vt_op(hwapic_isr_update),
+ *   - arch/x86/kvm/lapic.c|944| <<apic_set_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, vec);
+ *   - arch/x86/kvm/lapic.c|1036| <<apic_clear_isr>> kvm_x86_call(hwapic_isr_update)(apic->vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|1077| <<kvm_apic_update_hwapic_isr>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *   - arch/x86/kvm/lapic.c|3755| <<kvm_lapic_reset>> kvm_x86_call(hwapic_isr_update)(vcpu, -1);
+ *   - arch/x86/kvm/lapic.c|4297| <<kvm_apic_set_state>> kvm_x86_call(hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
+ *
+ * 在以下使用vmx_hwapic_isr_update():
+ *   - arch/x86/kvm/vmx/main.c|290| <<vt_hwapic_isr_update>> return vmx_hwapic_isr_update(vcpu, max_isr);
+ */
 void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 {
 	u16 status;
 	u8 old;
 
+	/*
+	 * 在以下使用enter_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|719| <<nested_vmcb02_prepare_control>> enter_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3610| <<nested_vmx_enter_non_root_mode>> enter_guest_mode(vcpu);
+	 *
+	 * 在以下使用leave_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1089| <<nested_svm_vmexit>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1353| <<svm_leave_nested>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|3914| <<nested_vmx_enter_non_root_mode>> leave_guest_mode(vcpu);
+	 *   - arch/x86/kvm/vmx/nested.c|5343| <<__nested_vmx_vmexit>> leave_guest_mode(vcpu);
+	 */
 	/*
 	 * If L2 is active, defer the SVI update until vmcs01 is loaded, as SVI
 	 * is only relevant for if and only if Virtual Interrupt Delivery is
@@ -6770,6 +7426,30 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 	 * VM-Exit, otherwise L1 with run with a stale SVI.
 	 */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用kvm_vcpu->wants_to_run:
+		 *   - arch/arm64/kvm/arm.c|1166| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/loongarch/kvm/vcpu.c|1795| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/mips/kvm/mips.c|436| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/powerpc/kvm/powerpc.c|1849| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/riscv/kvm/vcpu.c|900| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/s390/kvm/kvm-s390.c|5328| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/x86/kvm/vmx/vmx.c|6876| <<vmx_hwapic_isr_update>> WARN_ON_ONCE(vcpu->wants_to_run &&
+		 *   - arch/x86/kvm/x86.c|12138| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - virt/kvm/kvm_main.c|4465| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
+		 *   - virt/kvm/kvm_main.c|4467| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = false;
+		 *   - virt/kvm/kvm_main.c|6383| <<kvm_sched_out>> if (task_is_runnable(current) && vcpu->wants_to_run) {
+		 *
+		 * 在以下使用nested_cpu_has_vid():
+		 *   - arch/x86/kvm/vmx/nested.c|771| <<nested_vmx_prepare_msr_bitmap>> if (nested_cpu_has_vid(vmcs12)) {
+		 *   - arch/x86/kvm/vmx/nested.c|886| <<nested_vmx_check_apicv_controls>> !nested_cpu_has_vid(vmcs12) &&
+		 *   - arch/x86/kvm/vmx/nested.c|902| <<nested_vmx_check_apicv_controls>> if (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))
+		 *   - arch/x86/kvm/vmx/nested.c|913| <<nested_vmx_check_apicv_controls>> (CC(!nested_cpu_has_vid(vmcs12)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4365| <<vmx_has_nested_events>> if (!nested_cpu_has_vid(get_vmcs12(vcpu)) ||
+		 *   - arch/x86/kvm/vmx/nested.c|4905| <<sync_vmcs02_to_vmcs12>> if (nested_cpu_has_vid(vmcs12))
+		 *   - arch/x86/kvm/vmx/vmx.c|7142| <<vmx_hwapic_isr_update>> nested_cpu_has_vid(get_vmcs12(vcpu)));
+		 */
 		/*
 		 * KVM is supposed to forward intercepted L2 EOIs to L1 if VID
 		 * is enabled in vmcs12; as above, the EOIs affect L2's vAPIC.
@@ -6779,6 +7459,12 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 		 */
 		WARN_ON_ONCE(vcpu->wants_to_run &&
 			     nested_cpu_has_vid(get_vmcs12(vcpu)));
+		/*
+		 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+		 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+		 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+		 */
 		to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
 		return;
 	}
@@ -6795,6 +7481,10 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 	}
 }
 
+/*
+ * 在以下使用vmx_set_rvi():
+ *   - arch/x86/kvm/vmx/vmx.c|7045| <<vmx_sync_pir_to_irr>> vmx_set_rvi(max_irr);
+ */
 static void vmx_set_rvi(int vector)
 {
 	u16 status;
@@ -6904,6 +7594,10 @@ static void handle_exception_irqoff(struct kvm_vcpu *vcpu, u32 intr_info)
 		kvm_machine_check();
 }
 
+/*
+ * 在以下使用handle_external_interrupt_irqoff():
+ *   - arch/x86/kvm/vmx/vmx.c|6981| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
+ */
 static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu,
 					     u32 intr_info)
 {
@@ -6913,21 +7607,49 @@ static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu,
 	    "unexpected VM-Exit interrupt info: 0x%x", intr_info))
 		return;
 
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
 	if (cpu_feature_enabled(X86_FEATURE_FRED))
 		fred_entry_from_kvm(EVENT_TYPE_EXTINT, vector);
 	else
 		vmx_do_interrupt_irqoff(gate_offset((gate_desc *)host_idt_base + vector));
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	kvm_after_interrupt(vcpu);
 
 	vcpu->arch.at_instruction_boundary = true;
 }
 
+/*
+ * 在以下调用:
+ *   - arch/x86/kvm/x86.c|11339| <<vcpu_enter_guest>> kvm_x86_call(handle_exit_irqoff)(vcpu);
+ *
+ * 在以下使用vmx_handle_exit_irqoff():
+ *   - arch/x86/kvm/vmx/main.c|976| <<global>> .handle_exit_irqoff = vmx_handle_exit_irqoff,
+ */
 void vmx_handle_exit_irqoff(struct kvm_vcpu *vcpu)
 {
 	if (to_vt(vcpu)->emulation_required)
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	if (vmx_get_exit_reason(vcpu).basic == EXIT_REASON_EXTERNAL_INTERRUPT)
 		handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
 	else if (vmx_get_exit_reason(vcpu).basic == EXIT_REASON_EXCEPTION_NMI)
@@ -7000,6 +7722,13 @@ static void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)
 					      vmx->loaded_vmcs->entry_time));
 }
 
+/*
+ * 在以下使用__vmx_complete_interrupts():
+ *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu,
+ *        vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+ *   - arch/x86/kvm/vmx/vmx.c|7085| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu,
+ *        vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+ */
 static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 				      u32 idt_vectoring_info,
 				      int instr_len_field,
@@ -7013,6 +7742,10 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 
 	vcpu->arch.nmi_injected = false;
 	kvm_clear_exception_queue(vcpu);
+	/*
+	 * 设置:
+	 * vcpu->arch.interrupt.injected = false;
+	 */
 	kvm_clear_interrupt_queue(vcpu);
 
 	if (!idtv_info_valid)
@@ -7051,6 +7784,15 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 		vcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);
 		fallthrough;
 	case INTR_TYPE_EXT_INTR:
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
 		break;
 	default:
@@ -7058,8 +7800,21 @@ static void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用vmx_complete_interrupts():
+ *   - arch/x86/kvm/vmx/vmx.c|7418| <<vmx_vcpu_run>> vmx_complete_interrupts(vmx);
+ */
 static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 {
+	/*
+	 * 在以下使用__vmx_complete_interrupts():
+	 *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu,
+	 *        vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|7085| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu,
+	 *        vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+	 *
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
 				  VM_EXIT_INSTRUCTION_LEN,
 				  IDT_VECTORING_ERROR_CODE);
@@ -7067,6 +7822,17 @@ static void vmx_complete_interrupts(struct vcpu_vmx *vmx)
 
 void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用__vmx_complete_interrupts():
+	 *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu,
+	 *        vmx->idt_vectoring_info, VM_EXIT_INSTRUCTION_LEN, IDT_VECTORING_ERROR_CODE);
+	 *   - arch/x86/kvm/vmx/vmx.c|7085| <<vmx_cancel_injection>> __vmx_complete_interrupts(vcpu,
+	 *        vmcs_read32(VM_ENTRY_INTR_INFO_FIELD), VM_ENTRY_INSTRUCTION_LEN, VM_ENTRY_EXCEPTION_ERROR_CODE);
+	 *
+	 * bit 0-7: Vector of interrupt or exception
+	 * bit 8-10: interrupt type: 0-7不同的数, 0是external interrupt
+	 * bit 31: valid (cleared on every vm exit)
+	 */
 	__vmx_complete_interrupts(vcpu,
 				  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
 				  VM_ENTRY_INSTRUCTION_LEN,
@@ -7180,17 +7946,36 @@ static fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用vmx_handle_nmi():
+ *   - arch/x86/kvm/vmx/tdx.c|959| <<tdx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+ */
 noinstr void vmx_handle_nmi(struct kvm_vcpu *vcpu)
 {
 	if ((u16)vmx_get_exit_reason(vcpu).basic != EXIT_REASON_EXCEPTION_NMI ||
 	    !is_nmi(vmx_get_intr_info(vcpu)))
 		return;
 
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
 	if (cpu_feature_enabled(X86_FEATURE_FRED))
 		fred_entry_from_kvm(EVENT_TYPE_NMI, NMI_VECTOR);
 	else
 		vmx_do_nmi_irqoff();
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	kvm_after_interrupt(vcpu);
 }
 
@@ -7238,9 +8023,17 @@ static noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,
 	}
 
 	vmx->vt.exit_reason.full = vmcs_read32(VM_EXIT_REASON);
+	/*
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	if (likely(!vmx_get_exit_reason(vcpu).failed_vmentry))
 		vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
 
+	/*
+	 * 在以下使用vmx_handle_nmi():
+	 *   - arch/x86/kvm/vmx/tdx.c|959| <<tdx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_vcpu_enter_exit>> vmx_handle_nmi(vcpu);
+	 */
 	vmx_handle_nmi(vcpu);
 
 out:
@@ -7282,6 +8075,19 @@ fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 		vmcs_write32(PLE_WINDOW, vmx->ple_window);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	/*
 	 * We did this in prepare_switch_to_guest, because it needs to
 	 * be within srcu_read_lock.
@@ -7292,6 +8098,15 @@ fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu, u64 run_flags)
 		vmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);
 	if (kvm_register_is_dirty(vcpu, VCPU_REGS_RIP))
 		vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	vcpu->arch.regs_dirty = 0;
 
 	if (run_flags & KVM_RUN_LOAD_GUEST_DR6)
@@ -7717,6 +8532,10 @@ static void update_intel_pt_cfg(struct kvm_vcpu *vcpu)
 		vmx->pt_desc.ctl_bitmask &= ~(0xfULL << (32 + i * 4));
 }
 
+/*
+ * 在以下使用vmx_vcpu_after_set_cpuid():
+ *   - arch/x86/kvm/vmx/main.c|327| <<vt_vcpu_after_set_cpuid>> vmx_vcpu_after_set_cpuid(vcpu);
+ */
 void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7731,6 +8550,11 @@ void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 
 	vmx_setup_uret_msrs(vmx);
 
+	/*
+	 * 在以下使用vmx_secondary_exec_control():
+	 *   - arch/x86/kvm/vmx/vmx.c|4795| <<init_vmcs>> secondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));
+	 *   - arch/x86/kvm/vmx/vmx.c|8195| <<vmx_vcpu_after_set_cpuid>> vmx_secondary_exec_control(vmx));
+	 */
 	if (cpu_has_secondary_exec_ctrls())
 		vmcs_set_secondary_exec_control(vmx,
 						vmx_secondary_exec_control(vmx));
@@ -7932,6 +8756,12 @@ static bool vmx_is_io_intercepted(struct kvm_vcpu *vcpu,
 	return nested_vmx_check_io_bitmaps(vcpu, port, size);
 }
 
+/*
+ * 在以下使用kvm_x86_ops->check_intercept:
+ *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+ *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+ *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+ */
 int vmx_check_intercept(struct kvm_vcpu *vcpu,
 			struct x86_instruction_info *info,
 			enum x86_intercept_stage stage,
@@ -8023,6 +8853,16 @@ int vmx_check_intercept(struct kvm_vcpu *vcpu,
 	if (!exit_insn_len || exit_insn_len > X86_MAX_INSTRUCTION_LENGTH)
 		return X86EMUL_UNHANDLEABLE;
 
+	/*
+	 * 在以下使用__nested_vmx_vmexit():
+	 *   - arch/x86/kvm/vmx/nested.h|45| <<nested_vmx_vmexit>> __nested_vmx_vmexit(vcpu, vm_exit_reason, exit_intr_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|8211| <<vmx_check_intercept>> __nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
+	 *
+	 * 注释:
+	 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1
+	 * and modify vmcs12 to make it see what it would expect to see there if
+	 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())
+	 */
 	__nested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification,
 			    exit_insn_len);
 	return X86EMUL_INTERCEPTED;
@@ -8167,6 +9007,13 @@ int vmx_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	}
 
 	if (vmx->nested.smm.guest_mode) {
+		/*
+		 * 在以下使用nested_vmx_enter_non_root_mode():
+		 *   - arch/x86/kvm/vmx/nested.c|3777| <<nested_vmx_run>> status = nested_vmx_enter_non_root_mode(vcpu, true);
+		 *   - arch/x86/kvm/vmx/nested.c|6982| <<vmx_set_nested_state>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+		 *   - arch/x86/kvm/vmx/nested.h|26| <<vmx_set_nested_state>> enum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,
+		 *   - arch/x86/kvm/vmx/vmx.c|8355| <<vmx_leave_smm>> ret = nested_vmx_enter_non_root_mode(vcpu, false);
+		 */
 		ret = nested_vmx_enter_non_root_mode(vcpu, false);
 		if (ret)
 			return ret;
@@ -8333,6 +9180,11 @@ __init int vmx_hardware_setup(void)
 
 	vmx_setup_user_return_msrs();
 
+	/*
+	 * 在以下使用setup_vmcs_config():
+	 *   - arch/x86/kvm/vmx/vmx.c|2847| <<vmx_check_processor_compat>> if (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8821| <<vmx_hardware_setup>> if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
+	 */
 	if (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)
 		return -EIO;
 
@@ -8408,6 +9260,12 @@ __init int vmx_hardware_setup(void)
 		ple_window_shrink = 0;
 	}
 
+	/*
+	 * 在以下使用cpu_has_vmx_apicv():
+	 *   - arch/x86/kvm/vmx/nested.c|128| <<init_vmcs_shadow_fields>> if (!cpu_has_vmx_apicv())
+	 *   - arch/x86/kvm/vmx/nested.c|2769| <<prepare_vmcs02_rare>> if (cpu_has_vmx_apicv()) {
+	 *   - arch/x86/kvm/vmx/vmx.c|8761| <<vmx_hardware_setup>> if (!cpu_has_vmx_apicv())
+	 */
 	if (!cpu_has_vmx_apicv())
 		enable_apicv = 0;
 	if (!enable_apicv)
@@ -8492,6 +9350,12 @@ __init int vmx_hardware_setup(void)
 	if (nested) {
 		nested_vmx_setup_ctls_msrs(&vmcs_config, vmx_capability.ept);
 
+		/*
+		 * 在以下使用kvm_vmx_exit_handlers[]数组:
+		 *   - arch/x86/kvm/vmx/vmx.c|6969| <<__vmx_handle_exit>> if (!kvm_vmx_exit_handlers[exit_handler_index])
+		 *   - arch/x86/kvm/vmx/vmx.c|6972| <<__vmx_handle_exit>> return kvm_vmx_exit_handlers[exit_handler_index](vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|9184| <<vmx_hardware_setup>> r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
+		 */
 		r = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);
 		if (r)
 			return r;
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index d3389baf3..ef1a42108 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -74,6 +74,16 @@ struct pt_desc {
 struct nested_vmx {
 	/* Has the level1 guest done vmxon? */
 	bool vmxon;
+	/*
+	 * 在以下使用nested_vmx->vmxon_ptr:
+	 *   - arch/x86/kvm/vmx/nested.c|346| <<free_nested>> vmx->nested.vmxon_ptr = INVALID_GPA;
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<handle_vmxon>> vmx->nested.vmxon_ptr = vmptr;
+	 *   - arch/x86/kvm/vmx/nested.c|5563| <<handle_vmclear>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|5843| <<handle_vmptrld>> if (vmptr == vmx->nested.vmxon_ptr)
+	 *   - arch/x86/kvm/vmx/nested.c|6703| <<vmx_get_nested_state>> kvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;
+	 *   - arch/x86/kvm/vmx/nested.c|6885| <<vmx_set_nested_state>> vmx->nested.vmxon_ptr = kvm_state->hdr.vmx.vmxon_pa;
+	 *   - arch/x86/kvm/vmx/vmx.c|4760| <<__vmx_vcpu_reset>> vmx->nested.vmxon_ptr = INVALID_GPA;
+	 */
 	gpa_t vmxon_ptr;
 	bool pml_full;
 
@@ -106,6 +116,19 @@ struct nested_vmx {
 	 * Indicates if the shadow vmcs or enlightened vmcs must be updated
 	 * with the data held by struct vmcs12.
 	 */
+	/*
+	 * 在以下使用nested_vmx->need_vmcs12_to_shadow_sync:
+	 *   - arch/x86/kvm/vmx/nested.c|230| <<nested_vmx_failValid>> to_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|296| <<vmx_disable_shadow_vmcs>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|2599| <<nested_sync_vmcs12_to_shadow>> vmx->nested.need_vmcs12_to_shadow_sync = false;
+	 *   - arch/x86/kvm/vmx/nested.c|3797| <<nested_get_evmcs_page>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|4243| <<nested_vmx_enter_non_root_mode>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6066| <<__nested_vmx_vmexit>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|6929| <<set_current_vmptr>> vmx->nested.need_vmcs12_to_shadow_sync = true;
+	 *   - arch/x86/kvm/vmx/nested.c|7941| <<vmx_get_nested_state>> if (!vmx->nested.need_vmcs12_to_shadow_sync) {
+	 *   - arch/x86/kvm/vmx/vmx.c|1281| <<vmx_prepare_switch_to_guest>> if (vmx->nested.need_vmcs12_to_shadow_sync)
+	 *   - arch/x86/kvm/vmx/vmx.c|8007| <<vmx_vcpu_run>> WARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);
+	 */
 	bool need_vmcs12_to_shadow_sync;
 	bool dirty_vmcs12;
 
@@ -122,6 +145,12 @@ struct nested_vmx {
 	 * Indicates lazily loaded guest state has not yet been decached from
 	 * vmcs02.
 	 */
+	/*
+	 * 在以下使用nested_vmx->need_sync_vmcs02_to_vmcs12_rare:
+	 *   - arch/x86/kvm/vmx/nested.c|5295| <<sync_vmcs02_to_vmcs12_rare>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;
+	 *   - arch/x86/kvm/vmx/nested.c|5311| <<copy_vmcs02_to_vmcs12_rare>> if (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)
+	 *   - arch/x86/kvm/vmx/nested.c|5365| <<sync_vmcs02_to_vmcs12>> vmx->nested.need_sync_vmcs02_to_vmcs12_rare = !nested_vmx_is_evmptr12_valid(vmx);
+	 */
 	bool need_sync_vmcs02_to_vmcs12_rare;
 
 	/*
@@ -131,10 +160,31 @@ struct nested_vmx {
 	 */
 	bool vmcs02_initialized;
 
+	/*
+	 * 在以下使用nested_vmx->change_vmcs01_virtual_apic_mode
+	 */
 	bool change_vmcs01_virtual_apic_mode;
+	/*
+	 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page:
+	 *   - arch/x86/kvm/vmx/nested.c|5497| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5498| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	bool reload_vmcs01_apic_access_page;
 	bool update_vmcs01_cpu_dirty_logging;
+	/*
+	 * 在以下使用nested_vmx->pdate_vmcs01_apicv_status:
+	 *   - arch/x86/kvm/vmx/nested.c|5114| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+	 *   - arch/x86/kvm/vmx/nested.c|5115| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4359| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+	 */
 	bool update_vmcs01_apicv_status;
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+	 *   - arch/x86/kvm/vmx/nested.c|5119| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5120| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6878| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+	 */
 	bool update_vmcs01_hwapic_isr;
 
 	/*
@@ -157,11 +207,38 @@ struct nested_vmx {
 	 * pointers, so we must keep them pinned while L2 runs.
 	 */
 	struct kvm_host_map apic_access_page_map;
+	/*
+	 * 在以下使用nested_vmx->virtual_apic_map:
+	 *   - arch/x86/kvm/vmx/nested.c|395| <<nested_put_vmcs12_pages>> kvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map);
+	 *   - arch/x86/kvm/vmx/nested.c|3668| <<nested_get_vmcs12_pages>> map = &vmx->nested.virtual_apic_map;
+	 *   - arch/x86/kvm/vmx/nested.c|4377| <<vmx_complete_nested_posted_interrupt>> vapic_page = vmx->nested.virtual_apic_map.hva;
+	 *   - arch/x86/kvm/vmx/nested.c|4510| <<vmx_has_nested_events>> void *vapic = vmx->nested.virtual_apic_map.hva;
+	 */
 	struct kvm_host_map virtual_apic_map;
 	struct kvm_host_map pi_desc_map;
 
 	struct pi_desc *pi_desc;
+	/*
+	 * 在以下使用nested_vmx->pi_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|2614| <<prepare_vmcs02_early>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4364| <<vmx_complete_nested_posted_interrupt>> if (!vmx->nested.pi_pending)
+	 *   - arch/x86/kvm/vmx/nested.c|4370| <<vmx_complete_nested_posted_interrupt>> vmx->nested.pi_pending = false;
+	 *   - arch/x86/kvm/vmx/nested.c|4549| <<vmx_has_nested_events>> if (vmx->nested.pi_pending && vmx->nested.pi_desc &&
+	 *   - arch/x86/kvm/vmx/nested.c|4854| <<vmx_check_nested_events>> vmx->nested.pi_pending = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|4346| <<vmx_deliver_nested_posted_interrupt>> vmx->nested.pi_pending = true;
+	 */
 	bool pi_pending;
+	/*
+	 * 在以下使用nested_vmx->posted_intr_nv:
+	 *   - arch/x86/kvm/vmx/nested.c|348| <<free_nested>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|2396| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;
+	 *   - arch/x86/kvm/vmx/nested.c|2398| <<prepare_vmcs02_early>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|4423| <<vmx_check_nested_events>> if (irq == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/nested.c|5520| <<nested_release_vmcs12>> vmx->nested.posted_intr_nv = -1;
+	 *   - arch/x86/kvm/vmx/vmx.c|4142| <<vmx_deliver_nested_posted_interrupt>> if (is_guest_mode(vcpu) &&
+	 *                                               vector == vmx->nested.posted_intr_nv) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4759| <<__vmx_vcpu_reset>> vmx->nested.posted_intr_nv = -1;
+	 */
 	u16 posted_intr_nv;
 
 	struct hrtimer preemption_timer;
@@ -211,6 +288,25 @@ struct vcpu_vmx {
 	u8                    fail;
 	u8		      x2apic_msr_bitmap_mode;
 
+	/*
+	 * 在以下使用vcpu_vmx->idt_vectoring_info:
+	 *   - arch/x86/kvm/vmx/vmx.c|1650| <<vmx_check_emulate_instruction>> if ((to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5119| <<handle_exception_nmi>> vect_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|5632| <<handle_task_switch>> idt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5633| <<handle_task_switch>> idt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5634| <<handle_task_switch>> type = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);
+	 *   - arch/x86/kvm/vmx/vmx.c|5650| <<handle_task_switch>> if (vmx->idt_vectoring_info &
+	 *   - arch/x86/kvm/vmx/vmx.c|5691| <<handle_ept_violation>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|5896| <<handle_pml_full>> if (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&
+	 *   - arch/x86/kvm/vmx/vmx.c|6083| <<vmx_get_exit_info>> *info2 = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6379| <<__vmx_handle_exit>> u32 vectoring_info = vmx->idt_vectoring_info;
+	 *   - arch/x86/kvm/vmx/vmx.c|6985| <<vmx_recover_nmi_blocking>> idtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;
+	 *   - arch/x86/kvm/vmx/vmx.c|7078| <<vmx_complete_interrupts>> __vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,
+	 *   - arch/x86/kvm/vmx/vmx.c|7246| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7257| <<vmx_vcpu_enter_exit>> vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+	 *
+	 * 描述原本准备注入给 guest 的中断,但由于 VM-exit 等原因被中断了.
+	 */
 	u32                   idt_vectoring_info;
 	ulong                 rflags;
 
@@ -334,10 +430,37 @@ static __always_inline unsigned long vmx_get_exit_qual(struct kvm_vcpu *vcpu)
 	return vt->exit_qualification;
 }
 
+/*
+ * 在以下使用vmx_get_intr_info():
+ *   - arch/x86/kvm/vmx/nested.c|6163| <<handle_vmfunc>> nested_vmx_vmexit(vcpu, vmx->vt.exit_reason.full, vmx_get_intr_info(vcpu),
+ *   - arch/x86/kvm/vmx/nested.c|6404| <<nested_vmx_l0_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6485| <<nested_vmx_l1_wants_exit>> intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|6642| <<nested_vmx_reflect_vmexit>> exit_intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/tdx.c|1120| <<tdx_handle_exception_nmi>> u32 intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/tdx.c|2167| <<tdx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5140| <<handle_exception_nmi>> intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6104| <<vmx_get_exit_info>> *intr_info = vmx_get_intr_info(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|6972| <<vmx_handle_exit_irqoff>> handle_external_interrupt_irqoff(vcpu, vmx_get_intr_info(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|6974| <<vmx_handle_exit_irqoff>> handle_exception_irqoff(vcpu, vmx_get_intr_info(vcpu));
+ *   - arch/x86/kvm/vmx/vmx.c|7016| <<vmx_recover_nmi_blocking>> exit_intr_info = vmx_get_intr_info(&vmx->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7255| <<vmx_handle_nmi>> if (... !is_nmi(vmx_get_intr_info(vcpu)))
+ *
+ * VM_EXIT_INTR_INFO:
+ * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+ */
 static __always_inline u32 vmx_get_intr_info(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vt *vt = to_vt(vcpu);
 
+	/*
+	 * 在以下使用vcpu_vt->exit_intr_info:
+	 *   - arch/x86/kvm/vmx/tdx.c|957| <<tdx_vcpu_enter_exit>> vt->exit_intr_info = tdx->vp_enter_args.r9;
+	 *   - arch/x86/kvm/vmx/vmx.c|7343| <<vmx_vcpu_run>> vmx->vt.exit_intr_info = 0;
+	 *   - arch/x86/kvm/vmx/vmx.h|368| <<vmx_get_intr_info>> vt->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	 *   - arch/x86/kvm/vmx/vmx.h|370| <<vmx_get_intr_info>> return vt->exit_intr_info;
+	 *
+	 * 告诉VMM(Hypervisor)此次VM-exit是由于某个中断,异常或NMI等原因引起的
+	 */
 	if (!kvm_register_test_and_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_2) &&
 	    !WARN_ON_ONCE(is_td_vcpu(vcpu)))
 		vt->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
@@ -391,12 +514,64 @@ void vmx_set_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type, bool se
 static inline void vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu,
 						 u32 msr, int type)
 {
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, msr, type, false);
 }
 
 static inline void vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu,
 						u32 msr, int type)
 {
+	/*
+	 * 在以下使用vmx_set_intercept_for_msr():
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|681| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|682| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|684| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|687| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|688| <<vmx_update_intercept_for_lbr_msrs>> vmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);
+	 *   - arch/x86/kvm/vmx/vmx.c|4128| <<vmx_update_msr_bitmap_x2apic>> vmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,
+	 *                      !(mode & MSR_BITMAP_MODE_X2APIC));
+	 *   - arch/x86/kvm/vmx/vmx.c|4146| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4147| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4148| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4149| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4151| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4152| <<pt_update_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);
+	 *   - arch/x86/kvm/vmx/vmx.c|4188| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_SPEC_CTRL, MSR_TYPE_RW,
+	 *                      !to_vmx(vcpu)->spec_ctrl);
+	 *   - arch/x86/kvm/vmx/vmx.c|4192| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_XFD));
+	 *   - arch/x86/kvm/vmx/vmx.c|4196| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+	 *                      !guest_has_pred_cmd_msr(vcpu));
+	 *   - arch/x86/kvm/vmx/vmx.c|4200| <<vmx_recalc_msr_intercepts>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,
+	 *                      !guest_cpu_cap_has(vcpu, X86_FEATURE_FLUSH_L1D));
+	 *   - arch/x86/kvm/vmx/vmx.h|473| <<vmx_disable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, false);
+	 *   - arch/x86/kvm/vmx/vmx.h|479| <<vmx_enable_intercept_for_msr>> vmx_set_intercept_for_msr(vcpu, msr, type, true);
+	 */
 	vmx_set_intercept_for_msr(vcpu, msr, type, true);
 }
 
diff --git a/arch/x86/kvm/vmx/vmx_ops.h b/arch/x86/kvm/vmx/vmx_ops.h
index 96677576c..6b0377d60 100644
--- a/arch/x86/kvm/vmx/vmx_ops.h
+++ b/arch/x86/kvm/vmx/vmx_ops.h
@@ -293,6 +293,16 @@ static inline void vmcs_clear(struct vmcs *vmcs)
 	vmx_asm1(vmclear, "m"(phys_addr), vmcs, phys_addr);
 }
 
+/*
+ * 在以下使用vmcs_load():
+ *   - arch/x86/kvm/vmx/nested.c|1673| <<copy_shadow_to_vmcs12>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1682| <<copy_shadow_to_vmcs12>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1706| <<copy_vmcs12_to_shadow>> vmcs_load(shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|1718| <<copy_vmcs12_to_shadow>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5921| <<handle_vmwrite>> vmcs_load(vmx->vmcs01.shadow_vmcs);
+ *   - arch/x86/kvm/vmx/nested.c|5926| <<handle_vmwrite>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ *   - arch/x86/kvm/vmx/vmx.c|1424| <<vmx_vcpu_load_vmcs>> vmcs_load(vmx->loaded_vmcs->vmcs);
+ */
 static inline void vmcs_load(struct vmcs *vmcs)
 {
 	u64 phys_addr = __pa(vmcs);
@@ -355,6 +365,13 @@ static inline void vpid_sync_vcpu_addr(int vpid, gva_t addr)
 		vpid_sync_context(vpid);
 }
 
+/*
+ * 在以下使用ept_sync_global():
+ *   - arch/x86/kvm/vmx/tdx.c|2862| <<tdx_flush_tlb_current>> ept_sync_global();
+ *   - arch/x86/kvm/vmx/tdx.c|2878| <<tdx_flush_tlb_all>> ept_sync_global();
+ *   - arch/x86/kvm/vmx/vmx.c|3277| <<vmx_flush_tlb_all>> ept_sync_global();
+ *   - arch/x86/kvm/vmx/vmx_ops.h|378| <<ept_sync_context>> ept_sync_global();
+ */
 static inline void ept_sync_global(void)
 {
 	__invept(VMX_EPT_EXTENT_GLOBAL, 0);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 706b6fd56..62e7177a5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1478,6 +1478,18 @@ int kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8)
 }
 EXPORT_SYMBOL_GPL(kvm_set_cr8);
 
+/*
+ * 在以下使用kvm_get_cr8():
+ *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+ *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+ *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+ */
 unsigned long kvm_get_cr8(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
@@ -2398,6 +2410,15 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2401| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3094| <<get_cpu_tsc_khz>> return __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3108| <<__get_kvmclock>> (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {
+ *   - arch/x86/kvm/x86.c|9335| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|9352| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|9371| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
 static unsigned long max_tsc_khz;
 
@@ -2496,6 +2517,10 @@ static inline bool gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * 在以下使用kvm_track_tsc_matching():
+ *   - arch/x86/kvm/x86.c|2716| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu, !matched);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 {
 #ifdef CONFIG_X86_64
@@ -2561,6 +2586,22 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * 在以下使用kvm_read_l1_tsc():
+ *   - arch/x86/kvm/hyperv.c|599| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2509| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2519| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2616| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2705| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|2729| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|1176| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2384| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|8696| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|3285| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|10089| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|11616| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/xen.c|253| <<kvm_xen_start_timer>> guest_tsc = kvm_read_l1_tsc(vcpu, host_tsc);
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
 	return vcpu->arch.l1_tsc_offset +
@@ -2654,6 +2695,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * 在以下使用__kvm_synchronize_tsc():
+ *   - arch/x86/kvm/x86.c|2783| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+ *   - arch/x86/kvm/x86.c|5807| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched, bool user_set_tsc)
 {
@@ -3003,6 +3049,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * 在以下使用pvclock_update_vm_gtod_copy():
+ *   - arch/x86/kvm/x86.c|3095| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6914| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9399| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12745| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3070,6 +3123,10 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10698| <<vcpu_enter_guest>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_request_tsc_page_update(kvm);
@@ -3094,6 +3151,10 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * 在以下使用__get_kvmclock():
+ *   - arch/x86/kvm/x86.c|3149| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3609,6 +3670,11 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * 在以下使用kvmclock_reset():
+ *   - arch/x86/kvm/x86.c|13215| <<kvm_arch_vcpu_destroy>> kvmclock_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|13297| <<kvm_vcpu_reset>> kvmclock_reset(vcpu);
+ */
 static void kvmclock_reset(struct kvm_vcpu *vcpu)
 {
 	kvm_gpc_deactivate(&vcpu->arch.pv_time);
@@ -3899,6 +3965,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_MTRRdefType:
 		return kvm_mtrr_set_msr(vcpu, msr, data);
 	case MSR_IA32_APICBASE:
+		/*
+		 * 在以下使用kvm_apic_set_base():
+		 *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+		 *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+		 *                         msr_info->host_initiated);
+		 *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+		 */
 		return kvm_apic_set_base(vcpu, data, msr_info->host_initiated);
 	case APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:
 		return kvm_x2apic_msr_write(vcpu, msr, data);
@@ -4048,6 +4121,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))
 			return 1;
 
+		/*
+		 * 在以下使用kvm_lapic_set_pv_eoi():
+		 *   - arch/x86/kvm/hyperv.c|1572| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu, 0, 0))
+		 *   - arch/x86/kvm/hyperv.c|1590| <<kvm_hv_set_msr(HV_X64_MSR_VP_ASSIST_PAGE)>> if (kvm_lapic_set_pv_eoi(vcpu,
+		 *        gfn_to_gpa(gfn) | KVM_MSR_ENABLED, sizeof(struct hv_vp_assist_page)))
+		 *   - arch/x86/kvm/x86.c|4084| <<kvm_set_msr_common(MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
+		 */
 		if (kvm_lapic_set_pv_eoi(vcpu, data, sizeof(u8)))
 			return 1;
 		break;
@@ -4319,6 +4399,27 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		msr_info->data = 1 << 24;
 		break;
 	case MSR_IA32_APICBASE:
+		/*
+		 * 在以下设置kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+		 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+		 * 在以下使用kvm_vcpu_arch->apic_base:
+		 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+		 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+		 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+		 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+		 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+		 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+		 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+		 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+		 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+		 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+		 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+		 */
 		msr_info->data = vcpu->arch.apic_base;
 		break;
 	case APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:
@@ -5091,6 +5192,13 @@ static void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)
 	 * when this is true, for example allowing the vCPU to be marked
 	 * preempted if and only if the VM-Exit was due to a host interrupt.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+	 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+	 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+	 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+	 */
 	if (!vcpu->arch.at_instruction_boundary) {
 		vcpu->stat.preemption_other++;
 		return;
@@ -5155,6 +5263,18 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (vcpu->arch.apic->guest_apic_protected)
 		return -EINVAL;
 
@@ -5163,11 +5283,27 @@ static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 	return kvm_apic_get_state(vcpu, s);
 }
 
+/*
+ * 处理KVM_SET_LAPIC:
+ *   - arch/x86/kvm/x86.c|6140| <<kvm_arch_vcpu_ioctl>> r = kvm_vcpu_ioctl_set_lapic(vcpu, u.lapic);
+ */
 static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
 	int r;
 
+	/*
+	 * 在以下使用kvm_lapic->guest_apic_protected:
+	 *   - arch/x86/kvm/irq.c|231| <<kvm_cpu_has_interrupt>> if (lapic_in_kernel(v) && v->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|2195| <<lapic_timer_int_injected>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/lapic.c|3576| <<kvm_apic_has_interrupt>> if (apic->guest_apic_protected)
+	 *   - arch/x86/kvm/vmx/tdx.c|693| <<tdx_vcpu_create>> vcpu->arch.apic->guest_apic_protected = true;
+	 *   - arch/x86/kvm/x86.c|5205| <<kvm_vcpu_ioctl_get_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *   - arch/x86/kvm/x86.c|5218| <<kvm_vcpu_ioctl_set_lapic>> if (vcpu->arch.apic->guest_apic_protected)
+	 *
+	 * 注释:
+	 * Select registers in the vAPIC cannot be read/written.
+	 */
 	if (vcpu->arch.apic->guest_apic_protected)
 		return -EINVAL;
 
@@ -5179,6 +5315,11 @@ static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_cpu_accept_dm_intr():
+ *   - arch/x86/kvm/x86.c|5248| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu) &&
+ *   - arch/x86/kvm/x86.c|10864| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+ */
 static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -5187,6 +5328,17 @@ static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 	 * The actual injection will happen when the CPU is able to
 	 * deliver the interrupt.
 	 */
+	/*
+	 * 在以下使用kvm_cpu_has_extint():
+	 *   - arch/x86/kvm/irq.c|121| <<kvm_cpu_has_injectable_intr>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|137| <<kvm_cpu_has_interrupt>> if (kvm_cpu_has_extint(v))
+	 *   - arch/x86/kvm/irq.c|153| <<kvm_cpu_get_extint>> if (!kvm_cpu_has_extint(v)) {
+	 *   - arch/x86/kvm/x86.c|5223| <<kvm_cpu_accept_dm_intr>> if (kvm_cpu_has_extint(vcpu))
+	 *
+	 * 注释:
+	 * check if there is pending interrupt from
+	 * non-APIC source without intack.
+	 */
 	if (kvm_cpu_has_extint(vcpu))
 		return false;
 
@@ -5195,8 +5347,18 @@ static int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)
 		kvm_apic_accept_pic_intr(vcpu));
 }
 
+/*
+ * 在以下使用kvm_vcpu_ready_for_interrupt_injection():
+ *   - arch/x86/kvm/x86.c|10251| <<post_kvm_run_save>> kvm_vcpu_ready_for_interrupt_injection(vcpu);
+ *   - arch/x86/kvm/x86.c|11479| <<vcpu_run>> kvm_vcpu_ready_for_interrupt_injection(vcpu)) {
+ */
 static int kvm_vcpu_ready_for_interrupt_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_cpu_accept_dm_intr():
+	 *   - arch/x86/kvm/x86.c|5248| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|10864| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+	 */
 	/*
 	 * Do not cause an interrupt window exit if an exception
 	 * is pending or an event needs reinjection; userspace
@@ -5217,11 +5379,31 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 		return -EINVAL;
 
 	if (!irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, irq->irq, false);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 		return 0;
 	}
 
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 */
 	/*
 	 * With in-kernel LAPIC, we only use this to inject EXTINT, so
 	 * fail for in-kernel 8259.
@@ -5229,6 +5411,15 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 	if (pic_in_kernel(vcpu->kvm))
 		return -ENXIO;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	if (vcpu->arch.pending_external_vector != -1)
 		return -EEXIST;
 
@@ -5313,6 +5504,13 @@ static int kvm_vcpu_x86_set_ucna(struct kvm_vcpu *vcpu, struct kvm_x86_mce *mce,
 	    !(vcpu->arch.mci_ctl2_banks[mce->bank] & MCI_CTL2_CMCI_EN))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_apic_local_deliver():
+	 *   - arch/x86/kvm/lapic.c|1873| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+	 *   - arch/x86/kvm/lapic.c|2906| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+	 *   - arch/x86/kvm/pmu.c|611| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+	 *   - arch/x86/kvm/x86.c|5350| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+	 */
 	if (lapic_in_kernel(vcpu))
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
 
@@ -5530,6 +5728,14 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	}
 	kvm_x86_call(set_nmi_mask)(vcpu, events->nmi.masked);
 
+	/*
+	 * 在以下使用kvm_lapic->sipi_vector:
+	 *   - arch/x86/kvm/lapic.c|1696| <<__apic_accept_irq>> apic->sipi_vector = vector;
+	 *   - arch/x86/kvm/lapic.c|4108| <<kvm_apic_accept_events>> sipi_vector = apic->sipi_vector;
+	 *   - arch/x86/kvm/vmx/nested.c|4263| <<vmx_check_nested_events>> nested_vmx_vmexit(vcpu,
+	 *                                        EXIT_REASON_SIPI_SIGNAL, 0, apic->sipi_vector & 0xFFUL);
+	 *   - arch/x86/kvm/x86.c|5628| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> vcpu->arch.apic->sipi_vector = events->sipi_vector;
+	 */
 	if (events->flags & KVM_VCPUEVENT_VALID_SIPI_VECTOR &&
 	    lapic_in_kernel(vcpu))
 		vcpu->arch.apic->sipi_vector = events->sipi_vector;
@@ -5537,6 +5743,12 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	if (events->flags & KVM_VCPUEVENT_VALID_SMM) {
 #ifdef CONFIG_KVM_SMM
 		if (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {
+			/*
+			 * 在以下使用kvm_leave_nested():
+			 *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+			 *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+			 *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+			 */
 			kvm_leave_nested(vcpu);
 			kvm_smm_changed(vcpu, events->smi.smm);
 		}
@@ -6255,6 +6467,14 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		int idx;
 
 		r = -EINVAL;
+		/*
+		 * 在以下使用kvm_x86_nested_ops->set_state:
+		 *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+		 *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+		 *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+		 *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+		 *                                     user_kvm_nested_state, &kvm_state);
+		 */
 		if (!kvm_x86_ops.nested_ops->set_state)
 			break;
 
@@ -6278,6 +6498,14 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			break;
 
 		idx = srcu_read_lock(&vcpu->kvm->srcu);
+		/*
+		 * 在以下使用kvm_x86_nested_ops->set_state:
+		 *   - arch/x86/kvm/svm/nested.c|1932| <<global>> .set_state = svm_set_nested_state,
+		 *   - arch/x86/kvm/vmx/nested.c|7654| <<global>> .set_state = vmx_set_nested_state,
+		 *   - arch/x86/kvm/x86.c|6439| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> if (!kvm_x86_ops.nested_ops->set_state)
+		 *   - arch/x86/kvm/x86.c|6462| <<kvm_arch_vcpu_ioctl(KVM_SET_NESTED_STATE)>> r = kvm_x86_ops.nested_ops->set_state(vcpu,
+		 *                                     user_kvm_nested_state, &kvm_state);
+		 */
 		r = kvm_x86_ops.nested_ops->set_state(vcpu, user_kvm_nested_state, &kvm_state);
 		srcu_read_unlock(&vcpu->kvm->srcu, idx);
 		break;
@@ -6447,6 +6675,14 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 			goto split_irqchip_unlock;
 		/* Pairs with irqchip_in_kernel. */
 		smp_wmb();
+		/*
+		 * 在以下使用kvm_arch->irqchip_mode:
+		 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+		 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		 */
 		kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
 		kvm->arch.nr_reserved_ioapic_pins = cap->args[0];
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
@@ -6460,6 +6696,15 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		if (cap->args[0] & ~KVM_X2APIC_API_VALID_FLAGS)
 			break;
 
+		/*
+		 * 在以下使用kvm_arch->x2apic_format:
+		 *   - arch/x86/kvm/irq.c|481| <<kvm_msi_to_lapic_irq>> trace_kvm_msi_set_irq(msg.address_lo | (kvm->arch.x2apic_format ?
+		 *   - arch/x86/kvm/irq.c|484| <<kvm_msi_to_lapic_irq>> irq->dest_id = x86_msi_msg_get_destid(&msg, kvm->arch.x2apic_format);
+		 *   - arch/x86/kvm/irq.c|497| <<kvm_msi_route_invalid>> return kvm->arch.x2apic_format && (e->msi.address_hi & 0xff);
+		 *   - arch/x86/kvm/lapic.c|270| <<kvm_recalculate_phys_map>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/lapic.c|4305| <<kvm_apic_state_fixup>> if (vcpu->kvm->arch.x2apic_format) {
+		 *   - arch/x86/kvm/x86.c|6700| <<kvm_vm_ioctl_enable_cap(KVM_CAP_X2APIC_API/KVM_X2APIC_API_USE_32BIT_IDS)>> kvm->arch.x2apic_format = true;
+		 */
 		if (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)
 			kvm->arch.x2apic_format = true;
 		if (cap->args[0] & KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)
@@ -6988,6 +7233,14 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		}
 		/* Write kvm->irq_routing before enabling irqchip_in_kernel. */
 		smp_wmb();
+		/*
+		 * 在以下使用kvm_arch->irqchip_mode:
+		 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/x86.c|6499| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+		 *   - arch/x86/kvm/x86.c|7040| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		 */
 		kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
 		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
 	create_irqchip_unlock:
@@ -7008,6 +7261,17 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		if (kvm->arch.vpit)
 			goto create_pit_unlock;
 		r = -ENOENT;
+		/*
+		 * 在以下使用pic_in_kernel():
+		 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+		 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+		 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+		 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+		 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+		 *                                        likely(!pic_in_kernel(vcpu->kvm));
+		 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+		 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+		 */
 		if (!pic_in_kernel(kvm))
 			goto create_pit_unlock;
 		r = -ENOMEM;
@@ -7028,6 +7292,13 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		}
 
 		r = -ENXIO;
+		/*
+		 * 在以下使用irqchip_full():
+		 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+		 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+		 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 */
 		if (!irqchip_full(kvm))
 			goto get_irqchip_out;
 		r = kvm_vm_ioctl_get_irqchip(kvm, chip);
@@ -7052,6 +7323,13 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		}
 
 		r = -ENXIO;
+		/*
+		 * 在以下使用irqchip_full():
+		 *   - arch/x86/kvm/irq.c|213| <<kvm_arch_irqfd_allowed>> return resample ? irqchip_full(kvm) : irqchip_in_kernel(kvm);
+		 *   - arch/x86/kvm/irq.h|93| <<pic_in_kernel>> return irqchip_full(kvm);
+		 *   - arch/x86/kvm/x86.c|7080| <<kvm_arch_vm_ioctl(KVM_GET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 *   - arch/x86/kvm/x86.c|7104| <<kvm_arch_vm_ioctl(KVM_SET_IRQCHIP)>> if (!irqchip_full(kvm))
+		 */
 		if (!irqchip_full(kvm))
 			goto set_irqchip_out;
 		r = kvm_vm_ioctl_set_irqchip(kvm, chip);
@@ -8211,6 +8489,18 @@ static unsigned long emulator_get_cr(struct x86_emulate_ctxt *ctxt, int cr)
 		value = kvm_read_cr4(vcpu);
 		break;
 	case 8:
+		/*
+		 * 在以下使用kvm_get_cr8():
+		 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+		 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+		 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+		 */
 		value = kvm_get_cr8(vcpu);
 		break;
 	default:
@@ -8419,6 +8709,12 @@ static int emulator_intercept(struct x86_emulate_ctxt *ctxt,
 			      struct x86_instruction_info *info,
 			      enum x86_intercept_stage stage)
 {
+	/*
+	 * 在以下使用kvm_x86_ops->check_intercept:
+	 *   - arch/x86/kvm/svm/svm.c|5261| <<global>> .check_intercept = svm_check_intercept,
+	 *   - arch/x86/kvm/vmx/main.c|1002| <<global>> .check_intercept = vmx_check_intercept,
+	 *   - arch/x86/kvm/x86.c|8672| <<emulator_intercept>> return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
+	 */
 	return kvm_x86_call(check_intercept)(emul_to_vcpu(ctxt), info, stage,
 					     &ctxt->exception);
 }
@@ -8734,6 +9030,11 @@ void kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_prepare_emulation_failure_exit);
 
+/*
+ * 在以下使用kvm_prepare_event_vectoring_exit():
+ *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
+ *   - arch/x86/kvm/x86.c|9163| <<x86_emulate_instruction>> kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
+ */
 void kvm_prepare_event_vectoring_exit(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	u32 reason, intr_info, error_code;
@@ -9033,6 +9334,11 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			return 1;
 
 		if (r == X86EMUL_UNHANDLEABLE_VECTORING) {
+			/*
+			 * 在以下使用kvm_prepare_event_vectoring_exit():
+			 *   - arch/x86/kvm/vmx/vmx.c|6498| <<__vmx_handle_exit>> kvm_prepare_event_vectoring_exit(vcpu, INVALID_GPA);
+			 *   - arch/x86/kvm/x86.c|9163| <<x86_emulate_instruction>> kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
+			 */
 			kvm_prepare_event_vectoring_exit(vcpu, cr2_or_gpa);
 			return 0;
 		}
@@ -9851,6 +10157,16 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)
 		.dest_id = apicid,
 	};
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
@@ -9863,6 +10179,12 @@ EXPORT_SYMBOL_GPL(kvm_apicv_activated);
 bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 {
 	ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_get_apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/svm/svm.c|5354| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+	 *   - arch/x86/kvm/svm/svm.c|5594| <<svm_hardware_setup>> svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
+	 *   - arch/x86/kvm/x86.c|10158| <<kvm_vcpu_apicv_activated>> kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
+	 */
 	ulong vcpu_reasons =
 			kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
 
@@ -9892,9 +10214,27 @@ static void kvm_apicv_init(struct kvm *kvm)
 
 	set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	init_rwsem(&kvm->arch.apicv_update_lock);
 }
 
+/*
+ * 在以下使用kvm_sched_yield():
+ *   - arch/x86/kvm/x86.c|10309| <<kvm_sched_yield>> kvm_sched_yield(vcpu, a1);
+ *   - arch/x86/kvm/x86.c|10327| <<kvm_sched_yield>> kvm_sched_yield(vcpu, a0);
+ */
 static void kvm_sched_yield(struct kvm_vcpu *vcpu, unsigned long dest_id)
 {
 	struct kvm_vcpu *target = NULL;
@@ -9906,6 +10246,17 @@ static void kvm_sched_yield(struct kvm_vcpu *vcpu, unsigned long dest_id)
 		goto no_yield;
 
 	rcu_read_lock();
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	map = rcu_dereference(vcpu->kvm->arch.apic_map);
 
 	if (likely(map) && dest_id <= map->max_apic_id) {
@@ -10082,8 +10433,34 @@ static int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt)
 		&ctxt->exception);
 }
 
+/*
+ * 在以下使用dm_request_for_irq_injection():
+ *   - arch/x86/kvm/x86.c|10863| <<vcpu_enter_guest>> dm_request_for_irq_injection(vcpu) &&
+ *   - arch/x86/kvm/x86.c|11478| <<vcpu_run>> if (dm_request_for_irq_injection(vcpu) &&
+ *
+ * 满足:
+ * 1. vcpu->run->request_interrupt_window设置了
+ * 2. 不在KVM模拟PIC (split??)
+ */
 static int dm_request_for_irq_injection(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 *
+	 * 这个是在userspace的QEMU设置的.
+	 * If we have an interrupt but the guest is not ready to receive an
+	 * interrupt, request an interrupt window exit.  This will
+	 * cause a return to userspace as soon as the guest is ready to
+	 * receive interrupts.
+	 */
 	return vcpu->run->request_interrupt_window &&
 		likely(!pic_in_kernel(vcpu->kvm));
 }
@@ -10094,9 +10471,53 @@ static void post_kvm_run_save(struct kvm_vcpu *vcpu)
 	struct kvm_run *kvm_run = vcpu->run;
 
 	kvm_run->if_flag = kvm_x86_call(get_if_flag)(vcpu);
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	kvm_run->cr8 = kvm_get_cr8(vcpu);
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	kvm_run->apic_base = vcpu->arch.apic_base;
 
+	/*
+	 * 在以下使用pic_in_kernel():
+	 *   - arch/x86/kvm/irq.c|105| <<kvm_cpu_has_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/irq.c|167| <<kvm_cpu_get_extint>> if (pic_in_kernel(v->kvm))
+	 *   - arch/x86/kvm/x86.c|5262| <<kvm_vcpu_ioctl_interrupt>> if (pic_in_kernel(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|7060| <<kvm_arch_vm_ioctl>> if (!pic_in_kernel(kvm))
+	 *   - arch/x86/kvm/x86.c|10157| <<dm_request_for_irq_injection>> return vcpu->run->request_interrupt_window &&
+	 *                                        likely(!pic_in_kernel(vcpu->kvm));
+	 *   - arch/x86/kvm/x86.c|10170| <<post_kvm_run_save>> kvm_run->ready_for_interrupt_injection =
+	 *            pic_in_kernel(vcpu->kvm) || kvm_vcpu_ready_for_interrupt_injection(vcpu);
+	 */
 	kvm_run->ready_for_interrupt_injection =
 		pic_in_kernel(vcpu->kvm) ||
 		kvm_vcpu_ready_for_interrupt_injection(vcpu);
@@ -10107,6 +10528,9 @@ static void post_kvm_run_save(struct kvm_vcpu *vcpu)
 		kvm_run->flags |= KVM_RUN_X86_GUEST_MODE;
 }
 
+/*
+ * CR8 在长模式下直接访问的是Local APIC中的TPR?
+ */
 static void update_cr8_intercept(struct kvm_vcpu *vcpu)
 {
 	int max_irr, tpr;
@@ -10117,9 +10541,22 @@ static void update_cr8_intercept(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *
+	 * 有APICv就不用
+	 */
 	if (vcpu->arch.apic->apicv_active)
 		return;
 
+	/*
+	 * 在以下使用kvm_lapic->vapic_addr:
+	 *   - arch/x86/kvm/lapic.c|3955| <<kvm_lapic_set_vapic_addr>> vcpu->arch.apic->vapic_addr = vapic_addr;
+	 *   - arch/x86/kvm/x86.c|10328| <<update_cr8_intercept>> if (!vcpu->arch.apic->vapic_addr)
+	 */
 	if (!vcpu->arch.apic->vapic_addr)
 		max_irr = kvm_lapic_find_highest_irr(vcpu);
 	else
@@ -10130,10 +10567,20 @@ static void update_cr8_intercept(struct kvm_vcpu *vcpu)
 
 	tpr = kvm_lapic_get_cr8(vcpu);
 
+	/*
+	 * vmx_update_cr8_intercept
+	 * svm_update_cr8_intercept
+	 */
 	kvm_x86_call(update_cr8_intercept)(vcpu, tpr, max_irr);
 }
 
 
+/*
+ * 在以下使用kvm_check_nested_events():
+ *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+ *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+ */
 int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 {
 	if (kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {
@@ -10141,9 +10588,20 @@ int kvm_check_nested_events(struct kvm_vcpu *vcpu)
 		return 1;
 	}
 
+	/*
+	 * 在以下使用kvm_x86_nested_ops->check_events:
+	 *   - arch/x86/kvm/svm/nested.c|1950| <<global>> .check_events = svm_check_nested_events,
+	 *   - arch/x86/kvm/vmx/nested.c|8275| <<global>> .check_events = vmx_check_nested_events,
+	 *   - arch/x86/kvm/x86.c|10529| <<kvm_check_nested_events>> return kvm_x86_ops.nested_ops->check_events(vcpu);
+	 */
 	return kvm_x86_ops.nested_ops->check_events(vcpu);
 }
 
+/*
+ * 在以下使用kvm_inject_exception():
+ *   - arch/x86/kvm/x86.c|10394| <<kvm_check_and_inject_events>> kvm_inject_exception(vcpu);
+ *   - arch/x86/kvm/x86.c|10460| <<kvm_check_and_inject_events>> kvm_inject_exception(vcpu);
+ */
 static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -10160,6 +10618,10 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
 				vcpu->arch.exception.error_code,
 				vcpu->arch.exception.injected);
 
+	/*
+	 * vmx_inject_exception()
+	 * svm_inject_exception()
+	 */
 	kvm_x86_call(inject_exception)(vcpu);
 }
 
@@ -10202,12 +10664,22 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * 只在以下使用kvm_check_and_inject_events():
+ *   - arch/x86/kvm/x86.c|11048| <<vcpu_enter_guest>> r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
 	bool can_inject;
 	int r;
 
+	/*
+	 * 在以下使用kvm_check_nested_events():
+	 *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+	 */
 	/*
 	 * Process nested events first, as nested VM-Exit supersedes event
 	 * re-injection.  If there's an event queued for re-injection, it will
@@ -10241,6 +10713,10 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 	 * *previous* instruction and must be serviced prior to recognizing any
 	 * new events in order to fully complete the previous instruction.
 	 */
+	/*
+	 * vmx_inject_irq()
+	 * svm_inject_irq()
+	 */
 	if (vcpu->arch.exception.injected)
 		kvm_inject_exception(vcpu);
 	else if (kvm_is_exception_pending(vcpu))
@@ -10249,6 +10725,9 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 		kvm_x86_call(inject_nmi)(vcpu);
 	else if (vcpu->arch.interrupt.injected)
 		kvm_x86_call(inject_irq)(vcpu, true);
+	/*
+	 * vt_inject_irq()
+	 */
 
 	/*
 	 * Exceptions that morph to VM-Exits are handled above, and pending
@@ -10363,24 +10842,63 @@ static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 			kvm_x86_call(enable_nmi_window)(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_cpu_has_injectable_intr():
+	 *   - arch/x86/kvm/svm/svm.c|2327| <<svm_set_gif>> if (... kvm_cpu_has_injectable_intr(&svm->vcpu) ||
+	 *   - arch/x86/kvm/vmx/nested.c|5116| <<__nested_vmx_vmexit>> if (kvm_cpu_has_injectable_intr(vcpu) || vcpu->arch.nmi_pending)
+	 *   - arch/x86/kvm/x86.c|10465| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10479| <<kvm_check_and_inject_events>> if (kvm_cpu_has_injectable_intr(vcpu))
+	 */
 	if (kvm_cpu_has_injectable_intr(vcpu)) {
 		r = can_inject ? kvm_x86_call(interrupt_allowed)(vcpu, true) :
 				 -EBUSY;
 		if (r < 0)
 			goto out;
 		if (r) {
+			/*
+			 * 在以下使用kvm_cpu_get_interrupt():
+			 *   - arch/x86/kvm/x86.c|10521| <<kvm_check_and_inject_events>> int irq = kvm_cpu_get_interrupt(vcpu);
+			 *
+			 * Read pending interrupt vector and intack.
+			 */
 			int irq = kvm_cpu_get_interrupt(vcpu);
 
 			if (!WARN_ON_ONCE(irq == -1)) {
+				/*
+				 * 在以下使用kvm_queue_interrupt():
+				 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+				 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+				 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+				 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+				 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+				 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+				 */
 				kvm_queue_interrupt(vcpu, irq, false);
 				kvm_x86_call(inject_irq)(vcpu, false);
 				WARN_ON(kvm_x86_call(interrupt_allowed)(vcpu, true) < 0);
 			}
 		}
+		/*
+		 * 在以下使用kvm_x86_ops->enable_irq_window:
+		 *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+		 *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+		 *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+		 *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+		 */
 		if (kvm_cpu_has_injectable_intr(vcpu))
 			kvm_x86_call(enable_irq_window)(vcpu);
 	}
 
+	/*
+	 * 在以下使用kvm_x86_nested_ops->has_events:
+	 *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+	 *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+	 *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+	 *
+	 * vmx_has_nested_events()
+	 */
 	if (is_guest_mode(vcpu) &&
 	    kvm_x86_ops.nested_ops->has_events &&
 	    kvm_x86_ops.nested_ops->has_events(vcpu, true))
@@ -10460,11 +10978,24 @@ void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
 	kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
 }
 
+/*
+ * 在以下使用kvm_make_scan_ioapic_request():
+ *   - arch/x86/kvm/ioapic.c|310| <<kvm_arch_post_irq_ack_notifier_list_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/ioapic.c|469| <<ioapic_write_indirect>> kvm_make_scan_ioapic_request(ioapic->kvm);
+ *   - arch/x86/kvm/ioapic.c|799| <<kvm_set_ioapic>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/irq.c|746| <<kvm_arch_irq_routing_update>> kvm_make_scan_ioapic_request(kvm);
+ *   - arch/x86/kvm/lapic.c|576| <<kvm_recalculate_apic_map>> kvm_make_scan_ioapic_request(kvm);
+ */
 void kvm_make_scan_ioapic_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
 }
 
+/*
+ * 在以下使用__kvm_vcpu_update_apicv():
+ *   - arch/x86/kvm/svm/nested.c|1268| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|10558| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+ */
 void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -10473,6 +11004,19 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_read(&vcpu->kvm->arch.apicv_update_lock);
 	preempt_disable();
 
@@ -10480,11 +11024,32 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	activate = kvm_vcpu_apicv_activated(vcpu) &&
 		   (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);
 
+	/*
+	 * 在以下修改kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|3401| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_init_backing_page>> vcpu->arch.apic->apicv_active = false;
+	 *   - arch/x86/kvm/x86.c|10737| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 */
 	if (apic->apicv_active == activate)
 		goto out;
 
 	apic->apicv_active = activate;
+	/*
+	 * 在以下使用kvm_apic_update_apicv():
+	 *   - arch/x86/kvm/lapic.c|3132| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/lapic.c|3567| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10735| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+	 */
 	kvm_apic_update_apicv(vcpu);
+	/*
+	 * 在以下使用refresh_apicv_exec_ctrl:
+	 *   - arch/x86/kvm/svm/svm.c|5215| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+	 *   - arch/x86/kvm/vmx/main.c|957| <<global>> .refresh_apicv_exec_ctrl = vt_op(refresh_apicv_exec_ctrl),
+	 *   - arch/x86/kvm/x86.c|10907| <<__kvm_vcpu_update_apicv>> kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
+	 *
+	 * vmx_refresh_apicv_exec_ctrl()
+	 * avic_refresh_apicv_exec_ctrl()
+	 */
 	kvm_x86_call(refresh_apicv_exec_ctrl)(vcpu);
 
 	/*
@@ -10502,6 +11067,10 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(__kvm_vcpu_update_apicv);
 
+/*
+ * 处理KVM_REQ_APICV_UPDATE:
+ *   - arch/x86/kvm/x86.c|10838| <<vcpu_enter_guest>> kvm_vcpu_update_apicv(vcpu);
+ */
 static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 {
 	if (!lapic_in_kernel(vcpu))
@@ -10522,6 +11091,11 @@ static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	    kvm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization)
 		kvm_inhibit_apic_access_page(vcpu);
 
+	/*
+	 * 在以下使用__kvm_vcpu_update_apicv():
+	 *   - arch/x86/kvm/svm/nested.c|1268| <<nested_svm_vmexit>> __kvm_vcpu_update_apicv(vcpu);
+	 *   - arch/x86/kvm/x86.c|10558| <<kvm_vcpu_update_apicv>> __kvm_vcpu_update_apicv(vcpu);
+	 */
 	__kvm_vcpu_update_apicv(vcpu);
 }
 
@@ -10530,6 +11104,19 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 {
 	unsigned long old, new;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
 
 	if (!(kvm_x86_ops.required_apicv_inhibits & BIT(reason)))
@@ -10572,6 +11159,19 @@ void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 	__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
 	up_write(&kvm->arch.apicv_update_lock);
@@ -10583,11 +11183,38 @@ static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
 	if (!kvm_apic_present(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 */
 	bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
 	vcpu->arch.highest_stale_pending_ioapic_eoi = -1;
 
 	kvm_x86_call(sync_pir_to_irr)(vcpu);
 
+	/*
+	 * 在以下使用irqchip_split():
+	 *   - arch/x86/kvm/irq.c|109| <<kvm_cpu_has_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|171| <<kvm_cpu_get_extint>> WARN_ON_ONCE(!irqchip_split(v->kvm));
+	 *   - arch/x86/kvm/irq.c|394| <<kvm_set_routing_entry>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/irq.c|537| <<kvm_arch_irq_routing_update>> if (irqchip_split(kvm))
+	 *   - arch/x86/kvm/lapic.c|1431| <<kvm_ioapic_send_eoi>> if (irqchip_split(apic->vcpu->kvm)) {
+	 *   - arch/x86/kvm/vmx/tdx.c|679| <<tdx_vcpu_create>> if (!irqchip_split(vcpu->kvm))
+	 *   - arch/x86/kvm/x86.c|10747| <<vcpu_scan_ioapic>> if (irqchip_split(vcpu->kvm))
+	 */
 	if (irqchip_split(vcpu->kvm))
 		kvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);
 #ifdef CONFIG_KVM_IOAPIC
@@ -10610,13 +11237,58 @@ static void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)
 	if (to_hv_vcpu(vcpu)) {
 		u64 eoi_exit_bitmap[4];
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+		 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+		 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+		 *              apic->vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+		 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+		 *              vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+		 *              vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+		 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+		 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+		 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+		 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+		 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+		 */
 		bitmap_or((ulong *)eoi_exit_bitmap,
 			  vcpu->arch.ioapic_handled_vectors,
 			  to_hv_synic(vcpu)->vec_bitmap, 256);
+		/*
+		 * 在以下使用kvm_x86_ops->load_eoi_exitmap:
+		 *   - arch/x86/kvm/vmx/main.c|976| <<global>> .load_eoi_exitmap = vt_op(load_eoi_exitmap),
+		 *   - arch/x86/kvm/x86.c|11144| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+		 *   - arch/x86/kvm/x86.c|11165| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(
+		 */
 		kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
 		return;
 	}
 #endif
+	/*
+	 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+	 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+	 *              apic->vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+	 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+	 *              vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+	 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+	 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+	 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+	 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+	 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+	 *
+	 * 在以下使用kvm_x86_ops->load_eoi_exitmap:
+	 *   - arch/x86/kvm/vmx/main.c|976| <<global>> .load_eoi_exitmap = vt_op(load_eoi_exitmap),
+	 *   - arch/x86/kvm/x86.c|11144| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+	 *   - arch/x86/kvm/x86.c|11165| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(
+	 */
 	kvm_x86_call(load_eoi_exitmap)(
 		vcpu, (u64 *)vcpu->arch.ioapic_handled_vectors);
 }
@@ -10643,6 +11315,24 @@ static void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)
 static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
 	int r;
+	/*
+	 * 注释:
+	 * "我们要求一个中断窗口/准备好注入中断"的条件成立.
+	 * 接下来KVM可能产生KVM_EXIT_IRQ_WINDOW_OPEN退出,
+	 * 让用户空间或管理层有机会注入中断
+	 *
+	 * 在以下使用kvm_cpu_accept_dm_intr():
+	 *   - arch/x86/kvm/x86.c|5248| <<kvm_vcpu_ready_for_interrupt_injection>> kvm_cpu_accept_dm_intr(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|10864| <<vcpu_enter_guest>> kvm_cpu_accept_dm_intr(vcpu);
+	 *
+	 * 在以下使用dm_request_for_irq_injection():
+	 *   - arch/x86/kvm/x86.c|10863| <<vcpu_enter_guest>> dm_request_for_irq_injection(vcpu) &&
+	 *   - arch/x86/kvm/x86.c|11478| <<vcpu_run>> if (dm_request_for_irq_injection(vcpu) &&
+	 *
+	 * 满足:
+	 * 1. vcpu->run->request_interrupt_window设置了
+	 * 2. 不在KVM模拟PIC (split??)
+	 */
 	bool req_int_win =
 		dm_request_for_irq_injection(vcpu) &&
 		kvm_cpu_accept_dm_intr(vcpu);
@@ -10663,6 +11353,13 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 
 		if (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {
+			/*
+			 * 在以下使用kvm_x86_nested_ops->get_nested_state_pages:
+			 *   - arch/x86/kvm/svm/nested.c|2230| <<global>> .get_nested_state_pages = svm_get_nested_state_pages,
+			 *   - arch/x86/kvm/vmx/nested.c|8994| <<global>> .get_nested_state_pages = vmx_get_nested_state_pages,
+			 *   - arch/x86/kvm/x86.c|11305| <<vcpu_enter_guest(KVM_REQ_GET_NESTED_STATE_PAGES)>>
+			 *                             if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
+			 */
 			if (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {
 				r = 0;
 				goto out;
@@ -10744,6 +11441,23 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			process_nmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
 			BUG_ON(vcpu->arch.pending_ioapic_eoi > 255);
+			/*
+			 * 在以下使用kvm_vcpu_arch->ioapic_handled_vectors:
+			 *   - arch/x86/include/asm/kvm_host.h|804| <<global>> DECLARE_BITMAP(ioapic_handled_vectors, 256);
+			 *   - arch/x86/kvm/lapic.c|1796| <<kvm_ioapic_handles_vector>> return test_bit(vector,
+			 *              apic->vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|10913| <<vcpu_scan_ioapic>> bitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);
+			 *   - arch/x86/kvm/x86.c|10929| <<vcpu_scan_ioapic>> kvm_scan_ioapic_routes(vcpu,
+			 *              vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|10932| <<vcpu_scan_ioapic>> kvm_ioapic_scan_entry(vcpu,
+			 *              vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|10951| <<vcpu_load_eoi_exitmap>> bitmap_or((ulong *)eoi_exit_bitmap,
+			 *              vcpu->arch.ioapic_handled_vectors, to_hv_synic(vcpu)->vec_bitmap, 256);
+			 *   - arch/x86/kvm/x86.c|10958| <<vcpu_load_eoi_exitmap>> kvm_x86_call(load_eoi_exitmap)(vcpu,
+			 *              (u64 *)vcpu->arch.ioapic_handled_vectors);
+			 *   - arch/x86/kvm/x86.c|11103| <<vcpu_enter_guest(KVM_REQ_IOAPIC_EOI_EXIT)>>
+			 *              if (test_bit(vcpu->arch.pending_ioapic_eoi, vcpu->arch.ioapic_handled_vectors)) {
+			 */
 			if (test_bit(vcpu->arch.pending_ioapic_eoi,
 				     vcpu->arch.ioapic_handled_vectors)) {
 				vcpu->run->exit_reason = KVM_EXIT_IOAPIC_EOI;
@@ -10808,6 +11522,13 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_x86_call(update_cpu_dirty_logging)(vcpu);
 
 		if (kvm_check_request(KVM_REQ_UPDATE_PROTECTED_GUEST_STATE, vcpu)) {
+			/*
+			 * 在以下使用kvm_vcpu_reset():
+			 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+			 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+			 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+			 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+			 */
 			kvm_vcpu_reset(vcpu, true);
 			if (vcpu->arch.mp_state != KVM_MP_STATE_RUNNABLE) {
 				r = 1;
@@ -10819,6 +11540,15 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win ||
 	    kvm_xen_has_interrupt(vcpu)) {
 		++vcpu->stat.req_event;
+		/*
+		 * 在以下使用kvm_apic_accept_events():
+		 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+		 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+		 *
+		 * 主要针对INIT和SIPI
+		 */
 		r = kvm_apic_accept_events(vcpu);
 		if (r < 0) {
 			r = 0;
@@ -10829,16 +11559,36 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			goto out;
 		}
 
+		/*
+		 * 只在这里使用
+		 */
 		r = kvm_check_and_inject_events(vcpu, &req_immediate_exit);
 		if (r < 0) {
 			r = 0;
 			goto out;
 		}
+		/*
+		 * 在以下使用kvm_x86_ops->enable_irq_window:
+		 *   - arch/x86/kvm/svm/svm.c|5349| <<global>> .enable_irq_window = svm_enable_irq_window,
+		 *   - arch/x86/kvm/vmx/main.c|981| <<global>> .enable_irq_window = vt_op(enable_irq_window),
+		 *   - arch/x86/kvm/x86.c|10882| <<kvm_check_and_inject_events>> kvm_x86_call(enable_irq_window)(vcpu);
+		 *   - arch/x86/kvm/x86.c|11568| <<vcpu_enter_guest>> kvm_x86_call(enable_irq_window)(vcpu);
+		 *
+		 * vmx_enable_irq_window()
+		 * svm_enable_irq_window()
+		 */
 		if (req_int_win)
 			kvm_x86_call(enable_irq_window)(vcpu);
 
 		if (kvm_lapic_enabled(vcpu)) {
+			/*
+			 * 它存储的是TPR(Task Priority Register)的值
+			 * 值范围为0~15
+			 */
 			update_cr8_intercept(vcpu);
+			/*
+			 * 只在这里调用
+			 */
 			kvm_lapic_sync_to_vapic(vcpu);
 		}
 	}
@@ -10850,6 +11600,14 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 
 	preempt_disable();
 
+	/*
+	 * 在以下使用kvm_x86_ops->prepare_switch_to_guest:
+	 *   - arch/x86/kvm/svm/svm.c|5193| <<global>> .prepare_switch_to_guest = svm_prepare_switch_to_guest,
+	 *   - arch/x86/kvm/vmx/main.c|920| <<global>> .prepare_switch_to_guest = vt_op(prepare_switch_to_guest),
+	 *   - arch/x86/kvm/x86.c|11515| <<vcpu_enter_guest>> kvm_x86_call(prepare_switch_to_guest)(vcpu);
+	 *
+	 * vmx_prepare_switch_to_guest()
+	 */
 	kvm_x86_call(prepare_switch_to_guest)(vcpu);
 
 	/*
@@ -11005,6 +11763,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.xfd_no_write_intercept)
 		fpu_sync_guest_vmexit_xfd_state();
 
+	/*
+	 * vmx_handle_exit_irqoff
+	 * svm_handle_exit_irqoff
+	 */
 	kvm_x86_call(handle_exit_irqoff)(vcpu);
 
 	if (vcpu->arch.guest_fpu.xfd_err)
@@ -11026,10 +11788,24 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	 * interrupts on processors that implement an interrupt shadow, the
 	 * stat.exits increment will do nicely.
 	 */
+	/*
+	 * 在以下使用kvm_before_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+	 *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+	 */
 	kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
 	local_irq_enable();
 	++vcpu->stat.exits;
 	local_irq_disable();
+	/*
+	 * 在以下使用kvm_after_interrupt():
+	 *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+	 *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+	 */
 	kvm_after_interrupt(vcpu);
 
 	/*
@@ -11064,12 +11840,31 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (unlikely(vcpu->arch.tsc_always_catchup))
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apic_attention:
+	 *   - arch/x86/kvm/lapic.c|1095| <<pv_eoi_set_pending>> __set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|1115| <<pv_eoi_test_and_clr_pending>> __clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|3394| <<kvm_lapic_reset>> vcpu->arch.apic_attention = 0;
+	 *   - arch/x86/kvm/lapic.c|3922| <<kvm_lapic_sync_from_vapic>> if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|3925| <<kvm_lapic_sync_from_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4011| <<kvm_lapic_sync_to_vapic>> if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
+	 *   - arch/x86/kvm/lapic.c|4055| <<kvm_lapic_set_vapic_addr>> __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/lapic.c|4057| <<kvm_lapic_set_vapic_addr>> __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
+	 *   - arch/x86/kvm/x86.c|11456| <<vcpu_enter_guest>> if (vcpu->arch.apic_attention)
+	 *   - arch/x86/kvm/x86.c|11469| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.apic_attention))
+	 */
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
 	if (unlikely(exit_fastpath == EXIT_FASTPATH_EXIT_USERSPACE))
 		return 0;
 
+	/*
+	 * 在以下使用kvm_x86_ops->handle_exit:
+	 *   - arch/x86/kvm/svm/svm.c|5227| <<global>> .handle_exit = svm_handle_exit,
+	 *   - arch/x86/kvm/vmx/main.c|954| <<global>> .handle_exit = vt_op(handle_exit),
+	 *   - arch/x86/kvm/x86.c|11734| <<vcpu_enter_guest>> r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
+	 */
 	r = kvm_x86_call(handle_exit)(vcpu, exit_fastpath);
 	return r;
 
@@ -11119,12 +11914,38 @@ bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 	if (kvm_test_request(KVM_REQ_UPDATE_PROTECTED_GUEST_STATE, vcpu))
 		return true;
 
+	/*
+	 * 在以下使用kvm_cpu_has_interrupt():
+	 *   - arch/x86/kvm/svm/nested.c|1594| <<svm_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !svm_interrupt_blocked(vcpu)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4348| <<vmx_check_nested_events>> if (kvm_cpu_has_interrupt(vcpu) && !vmx_interrupt_blocked(vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11338| <<kvm_vcpu_has_events>> if (kvm_arch_interrupt_allowed(vcpu) && kvm_cpu_has_interrupt(vcpu))
+	 *
+	 * 先判断有没有PIC或者userspace的interrupt
+	 * 然后:
+	 *     1. 判断kvm_apic_present()
+	 *     2. 根据TPR和ISR更新PPR
+	 *        TPR定义"可交付的最低优先级",而PPR表示"当前处理中或刚完成的优先级".
+	 *     3. 选出符合当前PPR的最高的irr, 没有就返回-1
+	 *
+	 * check if there is pending interrupt without
+	 * intack.
+	 */
 	if (kvm_arch_interrupt_allowed(vcpu) && kvm_cpu_has_interrupt(vcpu))
 		return true;
 
 	if (kvm_hv_has_stimer_pending(vcpu))
 		return true;
 
+	/*
+	 * 在以下使用kvm_x86_nested_ops->has_events:
+	 *   - arch/x86/kvm/vmx/nested.c|8276| <<global>> .has_events = vmx_has_nested_events,
+	 *   - arch/x86/kvm/x86.c|10811| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|10812| <<kvm_check_and_inject_events>> kvm_x86_ops.nested_ops->has_events(vcpu, true))
+	 *   - arch/x86/kvm/x86.c|11812| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events &&
+	 *   - arch/x86/kvm/x86.c|11813| <<kvm_vcpu_has_events>> kvm_x86_ops.nested_ops->has_events(vcpu, false))
+	 *
+	 * vmx_has_nested_events()
+	 */
 	if (is_guest_mode(vcpu) &&
 	    kvm_x86_ops.nested_ops->has_events &&
 	    kvm_x86_ops.nested_ops->has_events(vcpu, false))
@@ -11186,6 +12007,12 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 	 * causes a spurious wakeup from HLT).
 	 */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用kvm_check_nested_events():
+		 *   - arch/x86/kvm/lapic.c|4835| <<kvm_apic_accept_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|10630| <<kvm_check_and_inject_events>> r = kvm_check_nested_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11907| <<vcpu_block>> int r = kvm_check_nested_events(vcpu);
+		 */
 		int r = kvm_check_nested_events(vcpu);
 
 		WARN_ON_ONCE(r == -EBUSY);
@@ -11193,6 +12020,15 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 			return 0;
 	}
 
+	/*
+	 * 在以下使用kvm_apic_accept_events():
+	 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+	 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+	 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+	 *
+	 * 主要针对INIT和SIPI
+	 */
 	if (kvm_apic_accept_events(vcpu) < 0)
 		return 0;
 	switch(vcpu->arch.mp_state) {
@@ -11226,6 +12062,13 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 		 * use a stale page translation. Assume that any code after
 		 * this point can start executing an instruction.
 		 */
+		/*
+		 * 在以下使用kvm_vcpu_arch->at_instruction_boundary:
+		 *   - arch/x86/kvm/svm/svm.c|4666| <<svm_handle_exit_irqoff>> vcpu->arch.at_instruction_boundary = true;
+		 *   - arch/x86/kvm/vmx/vmx.c|6972| <<handle_external_interrupt_irqoff>> vcpu->arch.at_instruction_boundary = true;
+		 *   - arch/x86/kvm/x86.c|5127| <<kvm_steal_time_set_preempted>> if (!vcpu->arch.at_instruction_boundary) {
+		 *   - arch/x86/kvm/x86.c|11585| <<vcpu_run>> vcpu->arch.at_instruction_boundary = false;
+		 */
 		vcpu->arch.at_instruction_boundary = false;
 		if (kvm_vcpu_running(vcpu)) {
 			r = vcpu_enter_guest(vcpu);
@@ -11243,6 +12086,20 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 		if (kvm_cpu_has_pending_timer(vcpu))
 			kvm_inject_pending_timer_irqs(vcpu);
 
+		/*
+		 * 在以下使用dm_request_for_irq_injection():
+		 *   - arch/x86/kvm/x86.c|10863| <<vcpu_enter_guest>> dm_request_for_irq_injection(vcpu) &&
+		 *   - arch/x86/kvm/x86.c|11478| <<vcpu_run>> if (dm_request_for_irq_injection(vcpu) &&
+		 *
+		 * 满足:
+		 * 1. vcpu->run->request_interrupt_window设置了
+		 * 2. 不在KVM模拟PIC (split??)
+		 *
+		 *
+		 * 在以下使用kvm_vcpu_ready_for_interrupt_injection():
+		 *   - arch/x86/kvm/x86.c|10251| <<post_kvm_run_save>> kvm_vcpu_ready_for_interrupt_injection(vcpu);
+		 *   - arch/x86/kvm/x86.c|11479| <<vcpu_run>> kvm_vcpu_ready_for_interrupt_injection(vcpu)) {
+		 */
 		if (dm_request_for_irq_injection(vcpu) &&
 			kvm_vcpu_ready_for_interrupt_injection(vcpu)) {
 			r = 0;
@@ -11253,6 +12110,13 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 
 		if (__xfer_to_guest_mode_work_pending()) {
 			kvm_vcpu_srcu_read_unlock(vcpu);
+			/*
+			 * 在以下使用xfer_to_guest_mode_handle_work():
+			 *   - arch/arm64/kvm/arm.c|1180| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/loongarch/kvm/vcpu.c|254| <<kvm_enter_guest_check>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/riscv/kvm/vcpu.c|913| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+			 *   - arch/x86/kvm/x86.c|11796| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+			 */
 			r = xfer_to_guest_mode_handle_work(vcpu);
 			kvm_vcpu_srcu_read_lock(vcpu);
 			if (r)
@@ -11504,6 +12368,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		kvm_vcpu_block(vcpu);
 		kvm_vcpu_srcu_read_lock(vcpu);
 
+		/*
+		 * 在以下使用kvm_apic_accept_events():
+		 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+		 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+		 *
+		 * 主要针对INIT和SIPI
+		 */
 		if (kvm_apic_accept_events(vcpu) < 0) {
 			r = 0;
 			goto out;
@@ -11710,6 +12583,18 @@ static void __get_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 skip_protected_regs:
 	sregs->cr0 = kvm_read_cr0(vcpu);
 	sregs->cr4 = kvm_read_cr4(vcpu);
+	/*
+	 * 在以下使用kvm_get_cr8():
+	 *   - arch/x86/kvm/svm/svm.c|2590| <<cr_interception>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2681| <<cr8_write_interception>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|2686| <<cr8_write_interception>> if (cr8_prev <= kvm_get_cr8(vcpu))
+	 *   - arch/x86/kvm/svm/svm.c|4058| <<sync_lapic_to_cr8>> cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5444| <<handle_cr>> u8 cr8_prev = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|5475| <<handle_cr>> val = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|8387| <<emulator_get_cr>> value = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|10319| <<post_kvm_run_save>> kvm_run->cr8 = kvm_get_cr8(vcpu);
+	 *   - arch/x86/kvm/x86.c|12262| <<__get_sregs_common>> sregs->cr8 = kvm_get_cr8(vcpu);
+	 */
 	sregs->cr8 = kvm_get_cr8(vcpu);
 	sregs->efer = vcpu->arch.efer;
 	sregs->apic_base = vcpu->arch.apic_base;
@@ -11767,6 +12652,15 @@ int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 
 	kvm_vcpu_srcu_read_lock(vcpu);
 
+	/*
+	 * 在以下使用kvm_apic_accept_events():
+	 *   - arch/x86/kvm/x86.c|11038| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+	 *   - arch/x86/kvm/x86.c|11428| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+	 *   - arch/x86/kvm/x86.c|11739| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+	 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+	 *
+	 * 主要针对INIT和SIPI
+	 */
 	r = kvm_apic_accept_events(vcpu);
 	if (r < 0)
 		goto out;
@@ -11885,6 +12779,11 @@ static bool kvm_is_valid_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 	       kvm_is_valid_cr0(vcpu, sregs->cr0);
 }
 
+/*
+ * 在以下使用__set_sregs_common():
+ *   - arch/x86/kvm/x86.c|12732| <<__set_sregs>> int ret = __set_sregs_common(vcpu, sregs, &mmu_reset_needed, true);
+ *   - arch/x86/kvm/x86.c|12777| <<__set_sregs2>> ret = __set_sregs_common(vcpu, (struct kvm_sregs *)sregs2,
+ */
 static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 		int *mmu_reset_needed, bool update_pdptrs)
 {
@@ -11894,6 +12793,33 @@ static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 	if (!kvm_is_valid_sregs(vcpu, sregs))
 		return -EINVAL;
 
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 *
+	 * 在以下使用kvm_apic_set_base():
+	 *   - arch/x86/kvm/vmx/tdx.c|3139| <<tdx_vcpu_init>> if (kvm_apic_set_base(vcpu, apic_base, true))
+	 *   - arch/x86/kvm/x86.c|3947| <<kvm_set_msr_common(MSR_IA32_APICBASE)>> return kvm_apic_set_base(vcpu, data,
+	 *                         msr_info->host_initiated);
+	 *   - arch/x86/kvm/x86.c|12510| <<__set_sregs_common>> if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
+	 */
 	if (kvm_apic_set_base(vcpu, sregs->apic_base, true))
 		return -EINVAL;
 
@@ -11954,6 +12880,11 @@ static int __set_sregs_common(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs,
 	return 0;
 }
 
+/*
+ * 在以下使用__set_sregs():
+ *   - arch/x86/kvm/x86.c|12807| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+ *   - arch/x86/kvm/x86.c|13003| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+ */
 static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 {
 	int pending_vec, max_bits;
@@ -11973,6 +12904,15 @@ static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 		(const unsigned long *)sregs->interrupt_bitmap, max_bits);
 
 	if (pending_vec < max_bits) {
+		/*
+		 * 在以下使用kvm_queue_interrupt():
+		 *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+		 *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+		 *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+		 *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+		 *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+		 *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+		 */
 		kvm_queue_interrupt(vcpu, pending_vec, false);
 		pr_debug("Set back pending irq %d\n", pending_vec);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
@@ -12024,6 +12964,11 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 		return -EINVAL;
 
 	vcpu_load(vcpu);
+	/*
+	 * 在以下使用__set_sregs():
+	 *   - arch/x86/kvm/x86.c|12807| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+	 *   - arch/x86/kvm/x86.c|13003| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+	 */
 	ret = __set_sregs(vcpu, sregs);
 	vcpu_put(vcpu);
 	return ret;
@@ -12038,6 +12983,19 @@ static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9928| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10514| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10539| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10580| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10622| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10624| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12088| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
@@ -12197,6 +13155,10 @@ static void store_regs(struct kvm_vcpu *vcpu)
 				vcpu, &vcpu->run->s.regs.events);
 }
 
+/*
+ * 在以下使用sync_regs():
+ *   - arch/x86/kvm/x86.c|12251| <<kvm_arch_vcpu_ioctl_run>> r = if (kvm_run->kvm_dirty_regs) { sync_regs(vcpu);
+ */
 static int sync_regs(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {
@@ -12207,6 +13169,11 @@ static int sync_regs(struct kvm_vcpu *vcpu)
 	if (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_SREGS) {
 		struct kvm_sregs sregs = vcpu->run->s.regs.sregs;
 
+		/*
+		 * 在以下使用__set_sregs():
+		 *   - arch/x86/kvm/x86.c|12807| <<kvm_arch_vcpu_ioctl_set_sregs>> ret = __set_sregs(vcpu, sregs);
+		 *   - arch/x86/kvm/x86.c|13003| <<sync_regs>> if (__set_sregs(vcpu, &sregs))
+		 */
 		if (__set_sregs(vcpu, &sregs))
 			return -EINVAL;
 
@@ -12247,6 +13214,15 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 
 	vcpu->arch.last_vmentry_cpu = -1;
 	vcpu->arch.regs_avail = ~0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->regs_dirty:
+	 *   - arch/x86/kvm/kvm_cache_regs.h|76| <<kvm_register_is_dirty>> return test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/kvm_cache_regs.h|91| <<kvm_register_mark_dirty>> __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
+	 *   - arch/x86/kvm/svm/svm.c|4402| <<svm_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|324| <<vmx_switch_vmcs>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7537| <<vmx_vcpu_run>> vcpu->arch.regs_dirty = 0;
+	 *   - arch/x86/kvm/x86.c|12987| <<kvm_arch_vcpu_create>> vcpu->arch.regs_dirty = ~0;
+	 */
 	vcpu->arch.regs_dirty = ~0;
 
 	kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm);
@@ -12300,6 +13276,15 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	}
 	kvm_pmu_init(vcpu);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pending_external_vector:
+	 *   - arch/x86/kvm/irq.c|44| <<pending_userspace_extint>> return v->arch.pending_external_vector != -1;
+	 *   - arch/x86/kvm/irq.c|49| <<get_userspace_extint>> int vector = vcpu->arch.pending_external_vector;
+	 *   - arch/x86/kvm/irq.c|51| <<get_userspace_extint>> vcpu->arch.pending_external_vector = -1;
+	 *   - arch/x86/kvm/x86.c|5265| <<kvm_vcpu_ioctl_interrupt>> if (vcpu->arch.pending_external_vector != -1)
+	 *   - arch/x86/kvm/x86.c|5268| <<kvm_vcpu_ioctl_interrupt>> vcpu->arch.pending_external_vector = irq->irq;
+	 *   - arch/x86/kvm/x86.c|12419| <<kvm_arch_vcpu_create>> vcpu->arch.pending_external_vector = -1;
+	 */
 	vcpu->arch.pending_external_vector = -1;
 	vcpu->arch.preempted_in_kernel = false;
 
@@ -12315,6 +13300,13 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	vcpu_load(vcpu);
 	kvm_vcpu_after_set_cpuid(vcpu);
 	kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	/*
+	 * 在以下使用kvm_vcpu_reset():
+	 *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+	 *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+	 *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+	 *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+	 */
 	kvm_vcpu_reset(vcpu, false);
 	kvm_init_mmu(vcpu);
 	vcpu_put(vcpu);
@@ -12388,6 +13380,13 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kvfree(vcpu->arch.cpuid_entries);
 }
 
+/*
+ * 在以下使用kvm_vcpu_reset():
+ *   - arch/x86/kvm/lapic.c|4826| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2108| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|11385| <<vcpu_enter_guest>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|13089| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -12404,6 +13403,12 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	WARN_ON_ONCE(!init_event &&
 		     (old_cr0 || kvm_read_cr3(vcpu) || kvm_read_cr4(vcpu)));
 
+	/*
+	 * 在以下使用kvm_leave_nested():
+	 *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+	 *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+	 */
 	/*
 	 * SVM doesn't unconditionally VM-Exit on INIT and SHUTDOWN, thus it's
 	 * possible to INIT the vCPU while L2 is active.  Force the vCPU back
@@ -12489,6 +13494,12 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	cpuid_0x1 = kvm_find_cpuid_entry(vcpu, 1);
 	kvm_rdx_write(vcpu, cpuid_0x1 ? cpuid_0x1->eax : 0x600);
 
+	/*
+	 * 在以下使用kvm_x86_ops->vcpu_reset:
+	 *   - arch/x86/kvm/svm/svm.c|5162| <<global>> .vcpu_reset = svm_vcpu_reset,
+	 *   - arch/x86/kvm/vmx/main.c|906| <<global>> .vcpu_reset = vt_op(vcpu_reset),
+	 *   - arch/x86/kvm/x86.c|13263| <<kvm_vcpu_reset>> kvm_x86_call(vcpu_reset)(vcpu, init_event);
+	 */
 	kvm_x86_call(vcpu_reset)(vcpu, init_event);
 
 	kvm_set_rflags(vcpu, X86_EFLAGS_FIXED);
@@ -12672,6 +13683,27 @@ EXPORT_SYMBOL_GPL(kvm_vcpu_is_reset_bsp);
 
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下设置kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/lapic.c|3215| <<__kvm_apic_set_base>> vcpu->arch.apic_base = value;
+	 *   - arch/x86/kvm/lapic.c|3782| <<kvm_create_lapic>> vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	 * 在以下使用kvm_vcpu_arch->apic_base:
+	 *   - arch/x86/kvm/cpuid.c|287| <<kvm_update_cpuid_runtime>> kvm_update_feature_runtime(vcpu,
+	 *                     best, X86_FEATURE_APIC, vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);
+	 *   - arch/x86/kvm/lapic.c|3139| <<kvm_free_lapic>> if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))
+	 *   - arch/x86/kvm/lapic.c|3212| <<__kvm_apic_set_base>> u64 old_value = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/lapic.c|3276| <<__kvm_apic_set_base>> apic->base_address = apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_BASE;
+	 *   - arch/x86/kvm/lapic.c|3298| <<kvm_apic_set_base>> if (vcpu->arch.apic_base == value)
+	 *   - arch/x86/kvm/lapic.h|287| <<kvm_apic_hw_enabled>> return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|319| <<apic_x2apic_mode>> return apic->vcpu->arch.apic_base & X2APIC_ENABLE;
+	 *   - arch/x86/kvm/lapic.h|380| <<kvm_get_apic_mode>> return kvm_apic_mode(vcpu->arch.apic_base);
+	 *   - arch/x86/kvm/vmx/nested.c|938| <<nested_vmx_msr_check_common>> if (CC(vcpu->arch.apic_base &
+	 *                                                            X2APIC_ENABLE && e->index >> 8 == 0x8))
+	 *   - arch/x86/kvm/x86.c|4381| <<kvm_get_msr_common(MSR_IA32_APICBASE)>> msr_info->data = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|10363| <<post_kvm_run_save>> kvm_run->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|12326| <<__get_sregs_common>> sregs->apic_base = vcpu->arch.apic_base;
+	 *   - arch/x86/kvm/x86.c|13324| <<kvm_vcpu_is_bsp>> return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
+	 */
 	return (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;
 }
 
@@ -12715,6 +13747,14 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	atomic_set(&kvm->arch.noncoherent_dma_count, 0);
 
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->apic_map_lock:
+	 *   - arch/x86/kvm/lapic.c|412| <<kvm_recalculate_apic_map>> mutex_lock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|425| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/lapic.c|492| <<kvm_recalculate_apic_map>> lockdep_is_held(&kvm->arch.apic_map_lock));
+	 *   - arch/x86/kvm/lapic.c|500| <<kvm_recalculate_apic_map>> mutex_unlock(&kvm->arch.apic_map_lock);
+	 *   - arch/x86/kvm/x86.c|13686| <<kvm_arch_init_vm>> mutex_init(&kvm->arch.apic_map_lock);
+	 */
 	mutex_init(&kvm->arch.apic_map_lock);
 	seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
@@ -12862,6 +13902,15 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		 * or fd copying.
 		 */
 		mutex_lock(&kvm->slots_lock);
+		/*
+		 * 在以下使用APIC_ACCESS_PAGE_PRIVATE_MEMSLOT:
+		 *   - arch/x86/kvm/lapic.c|2671| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 *   - arch/x86/kvm/lapic.c|2697| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+		 *   - arch/x86/kvm/mmu/mmu.c|1667| <<kvm_unmap_gfn_range>> range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)
+		 *   - arch/x86/kvm/mmu/mmu.c|4657| <<kvm_mmu_faultin_pfn>> if (slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT) {
+		 *   - arch/x86/kvm/vmx/vmx.c|6716| <<vmx_set_apic_access_page_addr>> slot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);
+		 *   - arch/x86/kvm/x86.c|12908| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+		 */
 		__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
 					0, 0);
 		__x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
@@ -12875,6 +13924,17 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvm_pic_destroy(kvm);
 	kvm_ioapic_destroy(kvm);
 #endif
+	/*
+	 * 在以下使用kvm_arch->apic_map:
+	 *   - arch/x86/kvm/lapic.c|590| <<kvm_recalculate_apic_map>> old = rcu_dereference_protected(kvm->arch.apic_map,
+	 *   - arch/x86/kvm/lapic.c|592| <<kvm_recalculate_apic_map>> rcu_assign_pointer(kvm->arch.apic_map, new);
+	 *   - arch/x86/kvm/lapic.c|1354| <<kvm_pv_send_ipi>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1910| <<kvm_irq_delivery_to_apic_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|1952| <<kvm_intr_is_single_vcpu_fast>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/lapic.c|2097| <<kvm_bitmap_or_dest_vcpus>> map = rcu_dereference(kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|10234| <<kvm_sched_yield>> map = rcu_dereference(vcpu->kvm->arch.apic_map);
+	 *   - arch/x86/kvm/x86.c|13880| <<kvm_arch_destroy_vm>> kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
+	 */
 	kvfree(rcu_dereference_check(kvm->arch.apic_map, 1));
 	kfree(srcu_dereference_check(kvm->arch.pmu_event_filter, &kvm->srcu, 1));
 	kvm_mmu_uninit_vm(kvm);
@@ -13462,6 +14522,19 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 	    kvm_pv_async_pf_enabled(vcpu) &&
 	    !apf_put_user_ready(vcpu, work->arch.token)) {
 		vcpu->arch.apf.pageready_pending = true;
+		/*
+		 * 在以下使用kvm_apic_set_irq():
+		 *   - arch/x86/kvm/hyperv.c|858| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/hyperv.c|2247| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *   - arch/x86/kvm/irq.c|469| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/irq.c|491| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|1359| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+		 *   - arch/x86/kvm/lapic.c|1994| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/lapic.c|2024| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+		 *   - arch/x86/kvm/x86.c|14494| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+		 *
+		 * 调用__apic_accept_irq()
+		 */
 		kvm_apic_set_irq(vcpu, &irq, NULL);
 	}
 
@@ -13942,6 +15015,15 @@ module_init(kvm_x86_init);
 
 static void __exit kvm_x86_exit(void)
 {
+	/*
+	 * 在以下使用kvm_has_noapic_vcpu:
+	 *   - arch/x86/kvm/lapic.c|126| <<global>> __read_mostly DEFINE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|273| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3186| <<kvm_free_lapic>> static_branch_dec(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.c|3919| <<kvm_create_lapic>> static_branch_inc(&kvm_has_noapic_vcpu);
+	 *   - arch/x86/kvm/lapic.h|277| <<lapic_in_kernel>> if (static_branch_unlikely(&kvm_has_noapic_vcpu))
+	 *   - arch/x86/kvm/x86.c|14686| <<kvm_x86_exit>> WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
+	 */
 	WARN_ON_ONCE(static_branch_unlikely(&kvm_has_noapic_vcpu));
 }
 module_exit(kvm_x86_exit);
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index bcfd9b719..1ad501750 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -137,9 +137,21 @@ static inline unsigned int __shrink_ple_window(unsigned int val,
 void kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu);
 int kvm_check_nested_events(struct kvm_vcpu *vcpu);
 
+/*
+ * 在以下使用kvm_leave_nested():
+ *   - arch/x86/kvm/smm.c|653| <<emulator_leave_smm>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|5746| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_leave_nested(vcpu);
+ *   - arch/x86/kvm/x86.c|13330| <<kvm_vcpu_reset>> kvm_leave_nested(vcpu);
+ */
 /* Forcibly leave the nested mode in cases like a vCPU reset */
 static inline void kvm_leave_nested(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_x86_nested_ops->leave_nested:
+	 *   - arch/x86/kvm/svm/nested.c|2093| <<global>> .leave_nested = svm_leave_nested,
+	 *   - arch/x86/kvm/vmx/nested.c|8987| <<global>> .leave_nested = vmx_leave_nested,
+	 *   - arch/x86/kvm/x86.h|143| <<kvm_leave_nested>> kvm_x86_ops.nested_ops->leave_nested(vcpu);
+	 */
 	kvm_x86_ops.nested_ops->leave_nested(vcpu);
 }
 
@@ -166,6 +178,24 @@ static inline bool kvm_vcpu_has_run(struct kvm_vcpu *vcpu)
 	return vcpu->arch.last_vmentry_cpu != -1;
 }
 
+/*
+ * 在以下使用kvm_set_mp_state():
+ *   - arch/x86/kvm/lapic.c|4828| <<kvm_apic_accept_events>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/lapic.c|4830| <<kvm_apic_accept_events>> kvm_set_mp_state(vcpu, KVM_MP_STATE_INIT_RECEIVED);
+ *   - arch/x86/kvm/lapic.c|4847| <<kvm_apic_accept_events>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/svm/nested.c|1091| <<nested_svm_vmexit>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/svm/sev.c|3903| <<sev_snp_init_protected_guest_state>> kvm_set_mp_state(vcpu, KVM_MP_STATE_HALTED);
+ *   - arch/x86/kvm/svm/sev.c|3946| <<sev_snp_init_protected_guest_state>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/vmx/nested.c|3813| <<nested_vmx_run>> kvm_set_mp_state(vcpu, KVM_MP_STATE_INIT_RECEIVED);
+ *   - arch/x86/kvm/vmx/nested.c|5129| <<__nested_vmx_vmexit>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|11823| <<vcpu_block>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|11929| <<__kvm_emulate_halt>> kvm_set_mp_state(vcpu, state);
+ *   - arch/x86/kvm/x86.c|12505| <<kvm_arch_vcpu_ioctl_set_mpstate>> kvm_set_mp_state(vcpu, mp_state->mp_state);
+ *   - arch/x86/kvm/x86.c|12659| <<__set_sregs_common>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|12984| <<kvm_arch_vcpu_create>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ *   - arch/x86/kvm/x86.c|12986| <<kvm_arch_vcpu_create>> kvm_set_mp_state(vcpu, KVM_MP_STATE_UNINITIALIZED);
+ *   - arch/x86/kvm/x86.c|14237| <<kvm_arch_async_page_present>> kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
+ */
 static inline void kvm_set_mp_state(struct kvm_vcpu *vcpu, int mp_state)
 {
 	vcpu->arch.mp_state = mp_state;
@@ -187,6 +217,15 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.exception_vmexit.pending = false;
 }
 
+/*
+ * 在以下使用kvm_queue_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4150| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+ *   - arch/x86/kvm/svm/svm.c|4153| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+ *   - arch/x86/kvm/vmx/vmx.c|7069| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+ *   - arch/x86/kvm/x86.c|5279| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+ *   - arch/x86/kvm/x86.c|10581| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+ *   - arch/x86/kvm/x86.c|12347| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+ */
 static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 	bool soft)
 {
@@ -200,6 +239,16 @@ static inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.interrupt.injected = false;
 }
 
+/*
+ * 在以下使用kvm_event_needs_reinjection():
+ *   - arch/x86/kvm/mmu/mmu.c|6292| <<kvm_mmu_write_protect_fault>> if (... (!direct && kvm_event_needs_reinjection(vcpu))) &&
+ *   - arch/x86/kvm/svm/nested.c|1549| <<svm_check_nested_events>> bool block_nested_events = block_nested_exceptions ||
+ *                                        kvm_event_needs_reinjection(vcpu);
+ *   - arch/x86/kvm/vmx/nested.c|4232| <<vmx_check_nested_events>> bool block_non_injected_events = kvm_event_needs_reinjection(vcpu);
+ *   - arch/x86/kvm/x86.c|5249| <<kvm_vcpu_ready_for_interrupt_injection>> if (... !kvm_event_needs_reinjection(vcpu) &&
+ *   - arch/x86/kvm/x86.c|10435| <<kvm_check_and_inject_events>> can_inject = !kvm_event_needs_reinjection(vcpu);
+ *   - arch/x86/kvm/x86.c|13664| <<kvm_can_do_async_pf>> if (... kvm_event_needs_reinjection(vcpu) ||
+ */
 static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.exception.injected || vcpu->arch.interrupt.injected ||
@@ -556,12 +605,26 @@ static inline bool kvm_notify_vmexit_enabled(struct kvm *kvm)
 	return kvm->arch.notify_vmexit_flags & KVM_X86_NOTIFY_VMEXIT_ENABLED;
 }
 
+/*
+ * 在以下使用kvm_before_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4329| <<svm_vcpu_run>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/vmx/vmx.c|6965| <<handle_external_interrupt_irqoff>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ *   - arch/x86/kvm/vmx/vmx.c|7267| <<vmx_handle_nmi>> kvm_before_interrupt(vcpu, KVM_HANDLING_NMI);
+ *   - arch/x86/kvm/x86.c|11360| <<vcpu_enter_guest>> kvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);
+ */
 static __always_inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,
 						 enum kvm_intr_type intr)
 {
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
 }
 
+/*
+ * 在以下使用kvm_after_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4535| <<svm_vcpu_run>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7605| <<handle_external_interrupt_irqoff>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|7948| <<vmx_handle_nmi>> kvm_after_interrupt(vcpu);
+ *   - arch/x86/kvm/x86.c|11789| <<vcpu_enter_guest>> kvm_after_interrupt(vcpu);
+ */
 static __always_inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 {
 	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index d6b2a665b..c9421f827 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -626,6 +626,16 @@ void kvm_xen_inject_vcpu_vector(struct kvm_vcpu *v)
 	irq.delivery_mode = APIC_DM_FIXED;
 	irq.level = 1;
 
+	/*
+	 * 在以下使用kvm_irq_delivery_to_apic():
+	 *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+	 *   - arch/x86/kvm/ioapic.c|558| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+	 *   - arch/x86/kvm/ioapic.c|562| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+	 *   - arch/x86/kvm/irq.c|554| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+	 *   - arch/x86/kvm/lapic.c|2469| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+	 *   - arch/x86/kvm/x86.c|10160| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+	 *   - arch/x86/kvm/xen.c|629| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+	 */
 	kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
 }
 
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 975bdc5da..1a4a74624 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -843,6 +843,11 @@ static struct sk_buff *virtnet_build_skb(void *buf, unsigned int buflen,
 	return skb;
 }
 
+/*
+ * 在以下使用page_to_skb():
+ *   - drivers/net/virtio_net.c|2111| <<receive_big>> page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
+ *   - drivers/net/virtio_net.c|2496| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
+ */
 /* Called from bottom half context */
 static struct sk_buff *page_to_skb(struct virtnet_info *vi,
 				   struct receive_queue *rq,
@@ -2107,6 +2112,11 @@ static struct sk_buff *receive_big(struct net_device *dev,
 				   struct virtnet_rq_stats *stats)
 {
 	struct page *page = buf;
+	/*
+	 * 在以下使用page_to_skb():
+	 *   - drivers/net/virtio_net.c|2111| <<receive_big>> page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
+	 *   - drivers/net/virtio_net.c|2496| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
+	 */
 	struct sk_buff *skb =
 		page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
 
@@ -2493,6 +2503,11 @@ static struct sk_buff *receive_mergeable(struct net_device *dev,
 		rcu_read_unlock();
 	}
 
+	/*
+	 * 在以下使用page_to_skb():
+	 *   - drivers/net/virtio_net.c|2111| <<receive_big>> page_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);
+	 *   - drivers/net/virtio_net.c|2496| <<receive_mergeable>> head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
+	 */
 	head_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);
 	curr_skb = head_skb;
 
diff --git a/drivers/target/target_core_device.c b/drivers/target/target_core_device.c
index 7bb711b24..1cfab675a 100644
--- a/drivers/target/target_core_device.c
+++ b/drivers/target/target_core_device.c
@@ -1105,6 +1105,11 @@ void core_dev_release_virtual_lun0(void)
 /*
  * Common CDB parsing for kernel and user passthrough.
  */
+/*
+ * 在以下使用passthrough_parse_cdb():
+ *   - drivers/target/target_core_pscsi.c|928| <<pscsi_parse_cdb>> return passthrough_parse_cdb(cmd, pscsi_execute_cmd);
+ *   - drivers/target/target_core_user.c|2670| <<tcmu_parse_cdb>> return passthrough_parse_cdb(cmd, tcmu_queue_cmd);
+ */
 sense_reason_t
 passthrough_parse_cdb(struct se_cmd *cmd,
 	sense_reason_t (*exec_cmd)(struct se_cmd *cmd))
@@ -1144,6 +1149,14 @@ passthrough_parse_cdb(struct se_cmd *cmd,
 		if (cdb[0] == PERSISTENT_RESERVE_IN) {
 			cmd->execute_cmd = target_scsi3_emulate_pr_in;
 			size = get_unaligned_be16(&cdb[7]);
+			/*
+			 * 在以下使用target_cmd_size_check():
+			 *   - drivers/target/target_core_device.c|1147| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_device.c|1152| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_device.c|1161| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_device.c|1169| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 *   - drivers/target/target_core_sbc.c|1067| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+			 */
 			return target_cmd_size_check(cmd, size);
 		}
 		if (cdb[0] == PERSISTENT_RESERVE_OUT) {
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index fe8beb7db..e612a229e 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -764,6 +764,12 @@ sbc_check_dpofua(struct se_device *dev, struct se_cmd *cmd, unsigned char *cdb)
 	return 0;
 }
 
+/*
+ * 在以下使用sbc_parse_cdb():
+ *   - drivers/target/target_core_file.c|909| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_exec_cmd_ops);
+ *   - drivers/target/target_core_iblock.c|1162| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_exec_cmd_ops);
+ *   - drivers/target/target_core_rd.c|653| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_exec_cmd_ops);
+ */
 sense_reason_t
 sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 {
@@ -1064,6 +1070,14 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 			size = sbc_get_size(cmd, sectors);
 	}
 
+	/*
+	 * 在以下使用target_cmd_size_check():
+	 *   - drivers/target/target_core_device.c|1147| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_device.c|1152| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_device.c|1161| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_device.c|1169| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 *   - drivers/target/target_core_sbc.c|1067| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+	 */
 	return target_cmd_size_check(cmd, size);
 }
 EXPORT_SYMBOL(sbc_parse_cdb);
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 0a76bdfe5..6cb80901d 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -1382,6 +1382,14 @@ target_check_max_data_sg_nents(struct se_cmd *cmd, struct se_device *dev,
  *
  * Return: TCM_NO_SENSE
  */
+/*
+ * 在以下使用target_cmd_size_check():
+ *   - drivers/target/target_core_device.c|1147| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1152| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1161| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_device.c|1169| <<passthrough_parse_cdb>> return target_cmd_size_check(cmd, size);
+ *   - drivers/target/target_core_sbc.c|1067| <<sbc_parse_cdb>> return target_cmd_size_check(cmd, size);
+ */
 sense_reason_t
 target_cmd_size_check(struct se_cmd *cmd, unsigned int size)
 {
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index e299e1834..78124b2ea 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -58,6 +58,12 @@ struct virtio_balloon {
 
 	/* Balloon's own wq for cpu-intensive work items */
 	struct workqueue_struct *balloon_wq;
+	/*
+	 * 在以下使用virtio_balloon->report_free_page_work:
+	 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+	 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+	 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+	 */
 	/* The free page reporting work item submitted to the balloon wq */
 	struct work_struct report_free_page_work;
 
@@ -497,6 +503,10 @@ static unsigned long return_free_pages_to_mm(struct virtio_balloon *vb,
 	return num_returned;
 }
 
+/*
+ * 在以下使用virtio_balloon_queue_free_page_work():
+ *   - drivers/virtio/virtio_balloon.c|527| <<virtballoon_changed>> virtio_balloon_queue_free_page_work(vb);
+ */
 static void virtio_balloon_queue_free_page_work(struct virtio_balloon *vb)
 {
 	if (!virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT))
@@ -507,6 +517,12 @@ static void virtio_balloon_queue_free_page_work(struct virtio_balloon *vb)
 			     &vb->config_read_bitmap))
 		return;
 
+	/*
+	 * 在以下使用virtio_balloon->report_free_page_work:
+	 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+	 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+	 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+	 */
 	queue_work(vb->balloon_wq, &vb->report_free_page_work);
 }
 
@@ -516,6 +532,12 @@ static void start_update_balloon_size(struct virtio_balloon *vb)
 	queue_work(system_freezable_wq, &vb->update_balloon_size_work);
 }
 
+/*
+ * 在以下使用virtballoon_changed():
+ *   - struct virtio_driver virtio_balloon_driver.
+ *   - drivers/virtio/virtio_balloon.c|1084| <<virtballoon_probe>> virtballoon_changed(vdev);
+ *   - drivers/virtio/virtio_balloon.c|1171| <<virtballoon_restore>> virtballoon_changed(vdev);
+ */
 static void virtballoon_changed(struct virtio_device *vdev)
 {
 	struct virtio_balloon *vb = vdev->priv;
@@ -739,6 +761,10 @@ static int get_free_page_and_send(struct virtio_balloon *vb)
 	return 0;
 }
 
+/*
+ * 在以下使用send_free_pages():
+ *   - drivers/virtio/virtio_balloon.c|781| <<virtio_balloon_report_free_page>> err = send_free_pages(vb);
+ */
 static int send_free_pages(struct virtio_balloon *vb)
 {
 	int err;
@@ -768,6 +794,15 @@ static int send_free_pages(struct virtio_balloon *vb)
 	return 0;
 }
 
+/*
+ * 在以下使用virtio_balloon->report_free_page_work:
+ *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+ *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+ *
+ * 在以下使用virtio_balloon_report_free_page():
+ *   - drivers/virtio/virtio_balloon.c|804| <<report_free_page_func>> virtio_balloon_report_free_page(vb);
+ */
 static void virtio_balloon_report_free_page(struct virtio_balloon *vb)
 {
 	int err;
@@ -788,6 +823,10 @@ static void virtio_balloon_report_free_page(struct virtio_balloon *vb)
 		dev_err(dev, "Failed to send a stop id, err = %d\n", err);
 }
 
+/*
+ * 在以下使用report_free_page_func():
+ *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ */
 static void report_free_page_func(struct work_struct *work)
 {
 	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
@@ -990,6 +1029,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 			err = -ENOMEM;
 			goto out_del_vqs;
 		}
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work:
+		 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 */
 		INIT_WORK(&vb->report_free_page_work, report_free_page_func);
 		vb->cmd_id_received_cache = VIRTIO_BALLOON_CMD_ID_STOP;
 		vb->cmd_id_active = cpu_to_virtio32(vb->vdev,
@@ -1135,6 +1180,12 @@ static void virtballoon_remove(struct virtio_device *vdev)
 	cancel_work_sync(&vb->update_balloon_stats_work);
 
 	if (virtio_has_feature(vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT)) {
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work:
+		 *   - drivers/virtio/virtio_balloon.c|510| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|993| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1138| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 */
 		cancel_work_sync(&vb->report_free_page_work);
 		destroy_workqueue(vb->balloon_wq);
 	}
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 15656b7fb..41fcf4626 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -380,6 +380,21 @@ struct kvm_vcpu {
 		bool dy_eligible;
 	} spin_loop;
 #endif
+	/*
+	 * 在以下使用kvm_vcpu->wants_to_run:
+	 *   - arch/arm64/kvm/arm.c|1166| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - arch/loongarch/kvm/vcpu.c|1795| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/mips/kvm/mips.c|436| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/powerpc/kvm/powerpc.c|1849| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/riscv/kvm/vcpu.c|900| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - arch/s390/kvm/kvm-s390.c|5328| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+	 *   - arch/x86/kvm/vmx/vmx.c|6876| <<vmx_hwapic_isr_update>> WARN_ON_ONCE(vcpu->wants_to_run &&
+	 *   - arch/x86/kvm/x86.c|12138| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+	 *   - virt/kvm/kvm_main.c|4465| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
+	 *   - virt/kvm/kvm_main.c|4467| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = false;
+	 *   - virt/kvm/kvm_main.c|6383| <<kvm_sched_out>> if (task_is_runnable(current) && vcpu->wants_to_run) {
+	 */
 	bool wants_to_run;
 	bool preempted;
 	bool ready;
@@ -777,6 +792,17 @@ struct kvm {
 	struct kvm_memslots __memslots[KVM_MAX_NR_ADDRESS_SPACES][2];
 	/* The current active memslot set for each address space */
 	struct kvm_memslots __rcu *memslots[KVM_MAX_NR_ADDRESS_SPACES];
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	struct xarray vcpu_array;
 	/*
 	 * Protected by slots_lock, but can be read outside if an
@@ -987,11 +1013,33 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 	i = array_index_nospec(i, num_vcpus);
 
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu.  */
 	smp_rmb();
 	return xa_load(&kvm->vcpu_array, i);
 }
 
+/*
+ * 在以下使用kvm->vcpu_array:
+ *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+ *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+ *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+ *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+ *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+ *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+ *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+ *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+ */
 #define kvm_for_each_vcpu(idx, vcpup, kvm)				\
 	if (atomic_read(&kvm->online_vcpus))				\
 		xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0,	\
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index f0f0d49d2..3d116e278 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -209,6 +209,13 @@ struct kvm_xen_exit {
 /* for KVM_RUN, returned by mmap(vcpu_fd, offset=0) */
 struct kvm_run {
 	/* in */
+	/*
+	 * 这个是在userspace的QEMU设置的.
+	 * If we have an interrupt but the guest is not ready to receive an
+	 * interrupt, request an interrupt window exit.  This will
+	 * cause a return to userspace as soon as the guest is ready to
+	 * receive interrupts.
+	 */
 	__u8 request_interrupt_window;
 	__u8 HINT_UNSAFE_IN_KVM(immediate_exit);
 	__u8 padding1[6];
diff --git a/kernel/entry/kvm.c b/kernel/entry/kvm.c
index 8485f6386..025c3422e 100644
--- a/kernel/entry/kvm.c
+++ b/kernel/entry/kvm.c
@@ -3,6 +3,10 @@
 #include <linux/entry-kvm.h>
 #include <linux/kvm_host.h>
 
+/*
+ * 在以下使用xfer_to_guest_mode_work():
+ *   - kernel/entry/kvm.c|47| <<xfer_to_guest_mode_handle_work>> return xfer_to_guest_mode_work(vcpu, ti_work);
+ */
 static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 {
 	do {
@@ -28,6 +32,13 @@ static int xfer_to_guest_mode_work(struct kvm_vcpu *vcpu, unsigned long ti_work)
 	return 0;
 }
 
+/*
+ * 在以下使用xfer_to_guest_mode_handle_work():
+ *   - arch/arm64/kvm/arm.c|1180| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/loongarch/kvm/vcpu.c|254| <<kvm_enter_guest_check>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|913| <<kvm_arch_vcpu_ioctl_run>> ret = xfer_to_guest_mode_handle_work(vcpu);
+ *   - arch/x86/kvm/x86.c|11796| <<vcpu_run>> r = xfer_to_guest_mode_handle_work(vcpu);
+ */
 int xfer_to_guest_mode_handle_work(struct kvm_vcpu *vcpu)
 {
 	unsigned long ti_work;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 6c07dd423..6852844bc 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -486,8 +486,30 @@ void kvm_destroy_vcpus(struct kvm *kvm)
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		kvm_vcpu_destroy(vcpu);
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		xa_erase(&kvm->vcpu_array, i);
 
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		/*
 		 * Assert that the vCPU isn't visible in any way, to ensure KVM
 		 * doesn't trigger a use-after-free if destroying vCPUs results
@@ -1122,6 +1144,17 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	mutex_init(&kvm->slots_arch_lock);
 	spin_lock_init(&kvm->mn_invalidate_lock);
 	rcuwait_init(&kvm->mn_memslots_update_rcuwait);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_init(&kvm->vcpu_array);
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 	xa_init(&kvm->mem_attr_array);
@@ -3994,6 +4027,17 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 		if (idx == me->vcpu_idx)
 			continue;
 
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		vcpu = xa_load(&kvm->vcpu_array, idx);
 		if (!READ_ONCE(vcpu->ready))
 			continue;
@@ -4213,6 +4257,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 	}
 
 	vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
 	WARN_ON_ONCE(r == -EBUSY);
 	if (r)
@@ -4248,6 +4303,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 kvm_put_xa_erase:
 	mutex_unlock(&vcpu->mutex);
 	kvm_put_kvm_no_destroy(kvm);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1007| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1012| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|489| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|497| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1125| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|3997| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4216| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4251| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
 unlock_vcpu_destroy:
 	mutex_unlock(&kvm->lock);
@@ -4462,6 +4528,21 @@ static long kvm_vcpu_ioctl(struct file *filp,
 
 			put_pid(oldpid);
 		}
+		/*
+		 * 在以下使用kvm_vcpu->wants_to_run:
+		 *   - arch/arm64/kvm/arm.c|1166| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/loongarch/kvm/vcpu.c|1795| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/mips/kvm/mips.c|436| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/powerpc/kvm/powerpc.c|1849| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/riscv/kvm/vcpu.c|900| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/s390/kvm/kvm-s390.c|5328| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run)
+		 *   - arch/x86/kvm/vmx/vmx.c|6876| <<vmx_hwapic_isr_update>> WARN_ON_ONCE(vcpu->wants_to_run &&
+		 *   - arch/x86/kvm/x86.c|12138| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - arch/x86/kvm/x86.c|12226| <<kvm_arch_vcpu_ioctl_run>> if (!vcpu->wants_to_run) {
+		 *   - virt/kvm/kvm_main.c|4465| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
+		 *   - virt/kvm/kvm_main.c|4467| <<kvm_vcpu_ioctl>> vcpu->wants_to_run = false;
+		 *   - virt/kvm/kvm_main.c|6383| <<kvm_sched_out>> if (task_is_runnable(current) && vcpu->wants_to_run) {
+		 */
 		vcpu->wants_to_run = !READ_ONCE(vcpu->run->immediate_exit__unsafe);
 		r = kvm_arch_vcpu_ioctl_run(vcpu);
 		vcpu->wants_to_run = false;
@@ -5932,6 +6013,28 @@ int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 }
 EXPORT_SYMBOL_GPL(kvm_io_bus_read);
 
+/*
+ * 在以下使用kvm_io_bus_register_dev():
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1824| <<vgic_register_its_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|823| <<vgic_register_redist_iodev>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, rd_base,
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|1101| <<vgic_register_dist_iodev>> return kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, dist_base_address,
+ *   - arch/loongarch/kvm/intc/eiointc.c|647| <<kvm_eiointc_create>> ret = kvm_io_bus_register_dev(kvm, KVM_IOCSR_BUS,
+ *   - arch/loongarch/kvm/intc/eiointc.c|657| <<kvm_eiointc_create>> ret = kvm_io_bus_register_dev(kvm, KVM_IOCSR_BUS,
+ *   - arch/loongarch/kvm/intc/ipi.c|419| <<kvm_ipi_create>> ret = kvm_io_bus_register_dev(kvm, KVM_IOCSR_BUS, IOCSR_IPI_BASE, IOCSR_IPI_SIZE, device);
+ *   - arch/loongarch/kvm/intc/pch_pic.c|340| <<kvm_pch_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, addr, PCH_PIC_SIZE, device);
+ *   - arch/mips/kvm/loongson_ipi.c|211| <<kvm_init_loongson_ipi>> kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, addr, 0x400, device);
+ *   - arch/powerpc/kvm/mpic.c|1449| <<map_mmio>> kvm_io_bus_register_dev(opp->kvm, KVM_MMIO_BUS,
+ *   - arch/riscv/kvm/aia_aplic.c|603| <<kvm_riscv_aia_aplic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS,
+ *   - arch/riscv/kvm/aia_imsic.c|1103| <<kvm_riscv_vcpu_aia_imsic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/i8254.c|774| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, KVM_PIT_BASE_ADDRESS,
+ *   - arch/x86/kvm/i8254.c|781| <<kvm_create_pit>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS,
+ *   - arch/x86/kvm/i8259.c|607| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x20, 2,
+ *   - arch/x86/kvm/i8259.c|612| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0xa0, 2, &s->dev_slave);
+ *   - arch/x86/kvm/i8259.c|616| <<kvm_pic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_PIO_BUS, 0x4d0, 2, &s->dev_elcr);
+ *   - arch/x86/kvm/ioapic.c|755| <<kvm_ioapic_init>> ret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, ioapic->base_address,
+ *   - virt/kvm/coalesced_mmio.c|141| <<kvm_vm_ioctl_register_coalesced_mmio>> ret = kvm_io_bus_register_dev(kvm,
+ *   - virt/kvm/eventfd.c|901| <<kvm_assign_ioeventfd_idx>> ret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,
+ */
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev)
 {
-- 
2.39.5 (Apple Git-154)

