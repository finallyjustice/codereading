From 62263c836a23ecc3e2c1e4bd2981ef02ebdc9016 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 15 Oct 2019 00:04:21 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/badblocks.c                   |  26 ++
 block/bio-integrity.c               |   8 +
 block/bio.c                         |  43 +++
 block/blk-cgroup.c                  |  11 +
 block/blk-flush.c                   |  97 +++++
 block/blk-integrity.c               |  13 +
 block/blk-iolatency.c               |   4 +
 block/blk-map.c                     |  62 ++++
 block/blk-merge.c                   |  55 +++
 block/blk-mq-cpumap.c               |  41 +++
 block/blk-mq-pci.c                  |  32 ++
 block/blk-mq-rdma.c                 |  18 +
 block/blk-mq-sched.c                |   4 +
 block/blk-mq-tag.c                  |  77 ++++
 block/blk-mq-tag.h                  |  23 ++
 block/blk-mq-virtio.c               |  31 ++
 block/blk-mq.c                      | 507 ++++++++++++++++++++++++++
 block/blk-mq.h                      |  27 ++
 block/blk-rq-qos.c                  |  15 +
 block/blk-rq-qos.h                  |   4 +
 block/blk-settings.c                |  17 +
 block/blk-softirq.c                 |  46 +++
 block/blk-stat.c                    |  14 +
 block/blk-timeout.c                 |  42 +++
 block/blk-wbt.c                     |  10 +
 block/blk.h                         |  16 +
 block/bounce.c                      |  12 +
 block/partitions/check.c            |   4 +
 drivers/nvme/host/core.c            |  16 +
 drivers/nvme/host/fabrics.c         |  11 +
 drivers/nvme/host/fabrics.h         |   9 +
 drivers/nvme/host/fault_inject.c    |   4 +
 drivers/nvme/host/nvme.h            |  27 ++
 drivers/nvme/host/pci.c             |   6 +
 drivers/nvme/host/rdma.c            | 687 ++++++++++++++++++++++++++++++++++++
 drivers/nvme/target/core.c          |  67 ++++
 drivers/nvme/target/io-cmd-file.c   |  35 ++
 drivers/nvme/target/loop.c          |  56 +++
 drivers/nvme/target/nvmet.h         |  11 +
 drivers/scsi/fcoe/fcoe.c            |  10 +
 drivers/scsi/fcoe/fcoe_transport.c  |   5 +
 drivers/scsi/scsi_transport_fc.c    |  22 ++
 drivers/scsi/scsi_transport_iscsi.c |  10 +
 fs/block_dev.c                      |  14 +
 fs/direct-io.c                      |  21 ++
 fs/iomap/direct-io.c                |   7 +
 include/linux/bio.h                 | 179 ++++++++++
 include/linux/blk-mq.h              | 129 +++++++
 include/linux/blk_types.h           |  99 ++++++
 include/linux/blkdev.h              | 170 +++++++++
 include/linux/bvec.h                | 136 +++++++
 include/linux/fs.h                  |  12 +
 include/linux/percpu-refcount.h     |  63 ++++
 include/linux/uio.h                 |  26 ++
 include/rdma/rdma_cm.h              |  12 +
 lib/iov_iter.c                      |  27 ++
 lib/sbitmap.c                       |  14 +
 57 files changed, 3144 insertions(+)

diff --git a/block/badblocks.c b/block/badblocks.c
index 2e5f569..e1c1eac 100644
--- a/block/badblocks.c
+++ b/block/badblocks.c
@@ -50,6 +50,13 @@
  * -1: there are bad blocks which have not yet been acknowledged in metadata.
  * plus the start/length of the first bad section we overlap.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1176| <<null_handle_cmd>> if (!is_flush && badblocks_check(&nullb->dev->badblocks, sector,
+ *   - drivers/md/md.h|214| <<is_badblock>> int rv = badblocks_check(&rdev->badblocks, rdev->data_offset + s,
+ *   - drivers/nvdimm/nd.h|425| <<is_bad_pmem>> return !!badblocks_check(bb, sector, len / 512, &first_bad,
+ *   - drivers/nvdimm/pfn_devs.c|383| <<nd_pfn_clear_memmap_errors>> bb_present = badblocks_check(&nd_region->bb, meta_start,
+ */
 int badblocks_check(struct badblocks *bb, sector_t s, int sectors,
 			sector_t *first_bad, int *bad_sectors)
 {
@@ -160,6 +167,14 @@ static void badblocks_update_acked(struct badblocks *bb)
  *  0: success
  *  1: failed to set badblocks (out of space)
  */
+/*
+ * called by:
+ *   - block/badblocks.c|537| <<badblocks_store>> if (badblocks_set(bb, sector, length, !unack))
+ *   - drivers/block/null_blk_main.c|384| <<nullb_device_badblocks_store>> ret = badblocks_set(&t_dev->badblocks, start,
+ *   - drivers/md/md.c|1637| <<super_1_load>> if (badblocks_set(&rdev->badblocks, sector, count, 1))
+ *   - drivers/md/md.c|9137| <<rdev_set_badblocks>> rv = badblocks_set(&rdev->badblocks, s, sectors, 0);
+ *   - drivers/nvdimm/badrange.c|170| <<set_badblock>> if (badblocks_set(bb, s, num, 1))
+ */
 int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 			int acknowledged)
 {
@@ -514,6 +529,12 @@ EXPORT_SYMBOL_GPL(badblocks_show);
  * Return:
  *  Length of the buffer processed or -ve error.
  */
+/*
+ * called by:
+ *   - block/genhd.c|851| <<disk_badblocks_store>> return badblocks_store(disk->bb, page, len, 0);
+ *   - drivers/md/md.c|3322| <<bb_store>> int rv = badblocks_store(&rdev->badblocks, page, len, 0);
+ *   - drivers/md/md.c|3337| <<ubb_store>> return badblocks_store(&rdev->badblocks, page, len, 1);
+ */
 ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 			int unack)
 {
@@ -572,6 +593,11 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
  *  0: success
  *  -ve errno: on error
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|514| <<null_alloc_dev>> if (badblocks_init(&dev->badblocks, 0)) {
+ *   - drivers/md/md.c|3503| <<md_rdev_init>> return badblocks_init(&rdev->badblocks, 0);
+ */
 int badblocks_init(struct badblocks *bb, int enable)
 {
 	return __badblocks_init(NULL, bb, enable);
diff --git a/block/bio-integrity.c b/block/bio-integrity.c
index fb95dbb..6bb2874 100644
--- a/block/bio-integrity.c
+++ b/block/bio-integrity.c
@@ -34,6 +34,14 @@ void blk_flush_integrity(void)
  * metadata.  nr_vecs specifies the maximum number of pages containing
  * integrity metadata that can be attached.
  */
+/*
+ * called by:
+ *   - block/bio-integrity.c|249| <<bio_integrity_prep>> bip = bio_integrity_alloc(bio, GFP_NOIO, nr_pages);
+ *   - block/bio-integrity.c|414| <<bio_integrity_clone>> bip = bio_integrity_alloc(bio, gfp_mask, bip_src->bip_vcnt);
+ *   - drivers/md/dm-crypt.c|1003| <<dm_crypt_integrity_io_alloc>> bip = bio_integrity_alloc(bio, GFP_NOIO, 1);
+ *   - drivers/nvme/host/core.c|838| <<nvme_add_user_metadata>> bip = bio_integrity_alloc(bio, GFP_KERNEL, 1);
+ *   - drivers/target/target_core_iblock.c|641| <<iblock_alloc_bip>> bip = bio_integrity_alloc(bio, GFP_NOIO,
+ */
 struct bio_integrity_payload *bio_integrity_alloc(struct bio *bio,
 						  gfp_t gfp_mask,
 						  unsigned int nr_vecs)
diff --git a/block/bio.c b/block/bio.c
index 299a0e7..9d766b1 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -298,6 +298,11 @@ void bio_reset(struct bio *bio)
 }
 EXPORT_SYMBOL(bio_reset);
 
+/*
+ * called by:
+ *   - block/bio.c|313| <<bio_chain_endio>> bio_endio(__bio_chain_endio(bio));
+ *   - block/bio.c|1816| <<bio_endio>> bio = __bio_chain_endio(bio);
+ */
 static struct bio *__bio_chain_endio(struct bio *bio)
 {
 	struct bio *parent = bio->bi_private;
@@ -570,6 +575,9 @@ EXPORT_SYMBOL(bio_put);
  *
  * 	Caller must ensure that @bio_src is not freed before @bio.
  */
+/*
+ * cloned bio must not modify vec list
+ */
 void __bio_clone_fast(struct bio *bio, struct bio *bio_src)
 {
 	BUG_ON(bio->bi_pool && BVEC_POOL_IDX(bio));
@@ -586,7 +594,13 @@ void __bio_clone_fast(struct bio *bio, struct bio *bio_src)
 	bio->bi_opf = bio_src->bi_opf;
 	bio->bi_ioprio = bio_src->bi_ioprio;
 	bio->bi_write_hint = bio_src->bi_write_hint;
+	/*
+	 * 类型是struct bvec_iter bi_iter;
+	 */
 	bio->bi_iter = bio_src->bi_iter;
+	/*
+	 * 类型是struct bio_vec *bi_io_vec;
+	 */
 	bio->bi_io_vec = bio_src->bi_io_vec;
 
 	bio_clone_blkg_association(bio, bio_src);
@@ -602,10 +616,17 @@ EXPORT_SYMBOL(__bio_clone_fast);
  *
  * 	Like __bio_clone_fast, only also allocates the returned bio
  */
+/*
+ * cloned bio must not modify vec list
+ */
 struct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)
 {
 	struct bio *b;
 
+	/*
+	 * If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is
+	 *   backed by the @bs's mempool
+	 */
 	b = bio_alloc_bioset(gfp_mask, 0, bs);
 	if (!b)
 		return NULL;
@@ -1348,6 +1369,10 @@ struct bio *bio_copy_user_iov(struct request_queue *q,
  *	Map the user space address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|75| <<__blk_rq_map_user_iov>> bio = bio_map_user_iov(q, iter, gfp_mask);
+ */
 struct bio *bio_map_user_iov(struct request_queue *q,
 			     struct iov_iter *iter,
 			     gfp_t gfp_mask)
@@ -1356,6 +1381,9 @@ struct bio *bio_map_user_iov(struct request_queue *q,
 	struct bio *bio;
 	int ret;
 
+	/*
+	 * 返回iov_iter->count
+	 */
 	if (!iov_iter_count(iter))
 		return ERR_PTR(-EINVAL);
 
@@ -1363,6 +1391,9 @@ struct bio *bio_map_user_iov(struct request_queue *q,
 	if (!bio)
 		return ERR_PTR(-ENOMEM);
 
+	/*
+	 * 只要iov_iter->count不为0
+	 */
 	while (iov_iter_count(iter)) {
 		struct page **pages;
 		ssize_t bytes;
@@ -1409,6 +1440,12 @@ struct bio *bio_map_user_iov(struct request_queue *q,
 			break;
 	}
 
+	/*
+	 * 在以下使用BIO_USER_MAPPED:
+	 *   - block/bio.c|1424| <<bio_map_user_iov>> bio_set_flag(bio, BIO_USER_MAPPED);
+	 *   - block/blk-map.c|64| <<__blk_rq_unmap_user>> if (bio_flagged(bio, BIO_USER_MAPPED))
+	 *   - block/blk-map.c|168| <<blk_rq_map_user_iov>> if (!bio_flagged(bio, BIO_USER_MAPPED))
+	 */
 	bio_set_flag(bio, BIO_USER_MAPPED);
 
 	/*
@@ -1845,6 +1882,12 @@ EXPORT_SYMBOL(bio_endio);
  * to @bio's bi_io_vec; it is the caller's responsibility to ensure that
  * @bio is not freed before the split.
  */
+/*
+ * 制作一个新的bio (split)
+ * split是从start到参数的sectors
+ * bio则变成从参数的sectors到最后
+ * 返回split
+ */
 struct bio *bio_split(struct bio *bio, int sectors,
 		      gfp_t gfp, struct bio_set *bs)
 {
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index 55a7dc2..aa7fa95 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -1165,6 +1165,10 @@ blkcg_css_alloc(struct cgroup_subsys_state *parent_css)
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|535| <<blk_alloc_queue_node>> if (blkcg_init_queue(q))
+ */
 int blkcg_init_queue(struct request_queue *q)
 {
 	struct blkcg_gq *new_blkg, *blkg;
@@ -1436,6 +1440,13 @@ EXPORT_SYMBOL_GPL(blkcg_deactivate_policy);
  * Register @pol with blkcg core.  Might sleep and @pol may be modified on
  * successful registration.  Returns 0 on success and -errno on failure.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1513| <<global>> EXPORT_SYMBOL_GPL(blkcg_policy_register);
+ *   - block/bfq-iosched.c|6762| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+ *   - block/blk-iolatency.c|1045| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-throttle.c|2482| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+ */
 int blkcg_policy_register(struct blkcg_policy *pol)
 {
 	struct blkcg *blkcg;
diff --git a/block/blk-flush.c b/block/blk-flush.c
index aedd932..513f347 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -75,6 +75,37 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ */
+
+/*
+ * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request?
+ *
+ * 双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.队列自
+ * 带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自上层的
+ * fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷requst可以再
+ * 次被执行时,pending1队列不再接收新的fs_request(fs_request被加入到pending2队列),
+ * 冲刷request执行完毕后,pending1队列所有的fs_request的PREFLUSH/POSTFLUSH执行完毕.
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
@@ -95,13 +126,37 @@ enum {
 static void blk_kick_flush(struct request_queue *q,
 			   struct blk_flush_queue *fq, unsigned int flags);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|358| <<blk_insert_flush>> unsigned int policy = blk_flush_policy(fflags, rq);
+ */
 static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 {
+	/*
+	 * If a request doesn't have data, only REQ_PREFLUSH makes sense, which
+	 * indicates a simple flush request.  If there is data, REQ_PREFLUSH indicates
+	 * that the device cache should be flushed before the data is executed, and
+	 * REQ_FUA means that the data must be on non-volatile media on request
+	 * completion.
+	 *
+	 * If the device doesn't have writeback cache, PREFLUSH and FUA don't make any
+	 * difference.  The requests are either completed immediately if there's no data
+	 * or executed as normal requests otherwise.
+	 *
+	 * If the device has writeback cache and supports FUA, REQ_PREFLUSH is
+	 * translated to PREFLUSH but REQ_FUA is passed down directly with DATA.
+	 *
+	 * If the device has writeback cache and doesn't support FUA, REQ_PREFLUSH
+	 * is translated to PREFLUSH and REQ_FUA to POSTFLUSH.
+	 */
 	unsigned int policy = 0;
 
 	if (blk_rq_sectors(rq))
 		policy |= REQ_FSEQ_DATA;
 
+	/*
+	 * QUEUE_FLAG_WC: Write back caching
+	 */
 	if (fflags & (1UL << QUEUE_FLAG_WC)) {
 		if (rq->cmd_flags & REQ_PREFLUSH)
 			policy |= REQ_FSEQ_PREFLUSH;
@@ -313,6 +368,10 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * 在以下使用:
+ *   - block/blk-flush.c|409| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -346,6 +405,11 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2001| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -419,6 +483,31 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|2536| <<bitmap_flush_work>> blkdev_issue_flush(ic->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|660| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|701| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|770| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1040| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|221| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|689| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|157| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1427| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5142| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/fat/file.c|207| <<fat_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|242| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|417| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|781| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|885| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1040| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|197| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|168| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|658| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -461,6 +550,10 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2563| <<blk_mq_alloc_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -490,6 +583,10 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|43| <<blk_mq_hw_sysfs_release>> blk_free_flush_queue(hctx->fq);
+ */
 void blk_free_flush_queue(struct blk_flush_queue *fq)
 {
 	/* bio based request queue hasn't flush queue */
diff --git a/block/blk-integrity.c b/block/blk-integrity.c
index ca39b46..5dbeee2 100644
--- a/block/blk-integrity.c
+++ b/block/blk-integrity.c
@@ -385,6 +385,15 @@ static const struct blk_integrity_profile nop_profile = {
  * struct with values appropriate for the underlying hardware. See
  * Documentation/block/data-integrity.rst.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|3140| <<dm_integrity_set>> blk_integrity_register(disk, &bi);
+ *   - drivers/md/dm-table.c|1265| <<dm_table_register_integrity>> blk_integrity_register(dm_disk(md),
+ *   - drivers/md/md.c|2185| <<md_integrity_register>> blk_integrity_register(mddev->gendisk,
+ *   - drivers/nvdimm/core.c|412| <<nd_integrity_init>> blk_integrity_register(disk, &bi);
+ *   - drivers/nvme/host/core.c|1537| <<nvme_init_integrity>> blk_integrity_register(disk, &integrity);
+ *   - drivers/scsi/sd_dif.c|81| <<sd_dif_config_host>> blk_integrity_register(disk, &bi);
+ */
 void blk_integrity_register(struct gendisk *disk, struct blk_integrity *template)
 {
 	struct blk_integrity *bi = &disk->queue->integrity;
@@ -415,6 +424,10 @@ void blk_integrity_unregister(struct gendisk *disk)
 }
 EXPORT_SYMBOL(blk_integrity_unregister);
 
+/*
+ * called by:
+ *   - block/genhd.c|747| <<__device_add_disk>> blk_integrity_add(disk);
+ */
 void blk_integrity_add(struct gendisk *disk)
 {
 	if (kobject_init_and_add(&disk->integrity_kobj, &integrity_ktype,
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index 0fff7b5..7a1394e 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -714,6 +714,10 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	rcu_read_unlock();
 }
 
+/*
+ * called by only:
+ *   - block/blk-cgroup.c|1193| <<blkcg_init_queue>> ret = blk_iolatency_init(q);
+ */
 int blk_iolatency_init(struct request_queue *q)
 {
 	struct blk_iolatency *blkiolat;
diff --git a/block/blk-map.c b/block/blk-map.c
index 3a62e47..e56b496 100644
--- a/block/blk-map.c
+++ b/block/blk-map.c
@@ -15,6 +15,14 @@
  * Append a bio to a passthrough request.  Only works if the bio can be merged
  * into the request based on the driver constraints.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|89| <<__blk_rq_map_user_iov>> ret = blk_rq_append_bio(rq, &bio);
+ *   - block/blk-map.c|251| <<blk_rq_map_kern>> ret = blk_rq_append_bio(rq, &bio);
+ *   - drivers/nvme/host/lightnvm.c|663| <<nvme_nvm_alloc_request>> blk_rq_append_bio(rq, &rqd->bio);
+ *   - drivers/target/target_core_pscsi.c|914| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, &bio);
+ *   - drivers/target/target_core_pscsi.c|933| <<pscsi_map_sg>> rc = blk_rq_append_bio(req, &bio);
+ */
 int blk_rq_append_bio(struct request *rq, struct bio **bio)
 {
 	struct bio *orig_bio = *bio;
@@ -24,6 +32,7 @@ int blk_rq_append_bio(struct request *rq, struct bio **bio)
 
 	blk_queue_bounce(rq->q, bio);
 
+	/* 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page */
 	bio_for_each_bvec(bv, *bio, iter)
 		nr_segs++;
 
@@ -61,6 +70,10 @@ static int __blk_rq_unmap_user(struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|142| <<blk_rq_map_user_iov>> ret =__blk_rq_map_user_iov(rq, map_data, &i, gfp_mask, copy);
+ */
 static int __blk_rq_map_user_iov(struct request *rq,
 		struct rq_map_data *map_data, struct iov_iter *iter,
 		gfp_t gfp_mask, bool copy)
@@ -117,6 +130,12 @@ static int __blk_rq_map_user_iov(struct request *rq,
  *    original bio must be passed back in to blk_rq_unmap_user() for proper
  *    unmapping.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|172| <<blk_rq_map_user>> return blk_rq_map_user_iov(q, rq, map_data, &i, gfp_mask);
+ *   - block/scsi_ioctl.c|339| <<sg_io>> ret = blk_rq_map_user_iov(q, rq, NULL, &i, GFP_KERNEL);
+ *   - drivers/scsi/sg.c|1810| <<sg_start_req>> res = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);
+ */
 int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 			struct rq_map_data *map_data,
 			const struct iov_iter *iter, gfp_t gfp_mask)
@@ -127,6 +146,7 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 	struct iov_iter i;
 	int ret = -EINVAL;
 
+	/* 判断type是不是ITER_IOVEC */
 	if (!iter_is_iovec(iter))
 		goto fail;
 
@@ -146,6 +166,12 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 			bio = rq->bio;
 	} while (iov_iter_count(&i));
 
+	/*
+	 * 在以下使用BIO_USER_MAPPED:
+	 *   - block/bio.c|1424| <<bio_map_user_iov>> bio_set_flag(bio, BIO_USER_MAPPED);
+	 *   - block/blk-map.c|64| <<__blk_rq_unmap_user>> if (bio_flagged(bio, BIO_USER_MAPPED))
+	 *   - block/blk-map.c|168| <<blk_rq_map_user_iov>> if (!bio_flagged(bio, BIO_USER_MAPPED))
+	 */
 	if (!bio_flagged(bio, BIO_USER_MAPPED))
 		rq->rq_flags |= RQF_COPY_USER;
 	return 0;
@@ -158,12 +184,34 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 }
 EXPORT_SYMBOL(blk_rq_map_user_iov);
 
+/*
+ * called by:
+ *   - lock/bsg-lib.c|54| <<bsg_transport_fill_hdr>> ret = blk_rq_map_user(rq->q, job->bidi_rq, NULL,
+ *   - block/bsg.c|172| <<bsg_sg_io>> ret = blk_rq_map_user(q, rq, NULL, uptr64(hdr.dout_xferp),
+ *   - block/bsg.c|175| <<bsg_sg_io>> ret = blk_rq_map_user(q, rq, NULL, uptr64(hdr.din_xferp),
+ *   - block/scsi_ioctl.c|342| <<sg_io>> ret = blk_rq_map_user(q, rq, NULL, hdr->dxferp, hdr->dxfer_len,
+ *   - drivers/cdrom/cdrom.c|2203| <<cdrom_read_cdda_bpc>> ret = blk_rq_map_user(q, rq, NULL, ubuf, len, GFP_KERNEL);
+ *   - drivers/nvme/host/core.c|878| <<nvme_submit_user_cmd>> ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
+ *   - drivers/nvme/host/lightnvm.c|811| <<nvme_nvm_submit_user_cmd>> ret = blk_rq_map_user(q, rq, NULL, ubuf, bufflen, GFP_KERNEL);
+ *   - drivers/scsi/sg.c|1813| <<sg_start_req>> res = blk_rq_map_user(q, rq, md, hp->dxferp,
+ *   - drivers/scsi/st.c|559| <<st_scsi_execute>> err = blk_rq_map_user(req->q, req, mdata, NULL, bufflen,
+ */
 int blk_rq_map_user(struct request_queue *q, struct request *rq,
 		    struct rq_map_data *map_data, void __user *ubuf,
 		    unsigned long len, gfp_t gfp_mask)
 {
+	/*
+	 * struct iovec
+	 * {
+	 *     void __user *iov_base;
+	 *     __kernel_size_t iov_len;
+	 * };
+	 */
 	struct iovec iov;
 	struct iov_iter i;
+	/*
+	 * iovec的type会mask上ITER_KVEC或者ITER_IOVEC
+	 */
 	int ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);
 
 	if (unlikely(ret < 0))
@@ -182,6 +230,20 @@ EXPORT_SYMBOL(blk_rq_map_user);
  *    supply the original rq->bio from the blk_rq_map_user() return, since
  *    the I/O completion may have changed rq->bio.
  */
+/*
+ * called by:
+ *   - block/scsi_ioctl.c|488| <<sg_scsi_ioctl>> if (bytes && blk_rq_map_kern(q, rq, buffer, bytes, GFP_NOIO)) {
+ *   - drivers/block/pktcdvd.c|712| <<pkt_generic_packet>> ret = blk_rq_map_kern(q, rq, cgc->buffer, cgc->buflen,
+ *   - drivers/block/virtio_blk.c|373| <<virtblk_get_id>> err = blk_rq_map_kern(q, req, id_str, VIRTIO_BLK_ID_BYTES, GFP_KERNEL);
+ *   - drivers/ide/ide-atapi.c|101| <<ide_queue_pc_tail>> error = blk_rq_map_kern(drive->queue, rq, buf, bufflen,
+ *   - drivers/ide/ide-atapi.c|212| <<ide_prep_sense>> err = blk_rq_map_kern(drive->queue, sense_rq, sense, sense_len,
+ *   - drivers/ide/ide-cd.c|461| <<ide_cd_queue_pc>> error = blk_rq_map_kern(drive->queue, rq, buffer,
+ *   - drivers/ide/ide-tape.c|864| <<idetape_queue_rw_tail>> ret = blk_rq_map_kern(drive->queue, rq, tape->buf, size,
+ *   - drivers/ide/ide-taskfile.c|438| <<ide_raw_taskfile>> error = blk_rq_map_kern(drive->queue, rq, buf,
+ *   - drivers/nvme/host/core.c|794| <<__nvme_submit_sync_cmd>> ret = blk_rq_map_kern(q, req, buffer, bufflen, GFP_KERNEL);
+ *   - drivers/scsi/scsi_lib.c|265| <<__scsi_execute>> if (bufflen && blk_rq_map_kern(sdev->request_queue, req,
+ *   - fs/nfsd/blocklayout.c|245| <<nfsd4_scsi_identify_device>> error = blk_rq_map_kern(q, rq, buf, bufflen, GFP_KERNEL);
+ */
 int blk_rq_unmap_user(struct bio *bio)
 {
 	struct bio *mapped_bio;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 57f7990..6afb995 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -12,11 +12,17 @@
 
 #include "blk.h"
 
+/*
+ * called by:
+ *   - block/blk-merge.c|53| <<req_gap_back_merge>> return bio_will_gap(req->q, req, req->biotail, bio);
+ *   - block/blk-merge.c|58| <<req_gap_front_merge>> return bio_will_gap(req->q, NULL, bio, req->bio);
+ */
 static inline bool bio_will_gap(struct request_queue *q,
 		struct request *prev_rq, struct bio *prev, struct bio *next)
 {
 	struct bio_vec pb, nb;
 
+	/* 如果没设置virt_boundary就一定返回false */
 	if (!bio_has_data(prev) || !queue_virt_boundary(q))
 		return false;
 
@@ -135,6 +141,10 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
 static inline unsigned get_max_io_size(struct request_queue *q,
 				       struct bio *bio)
 {
+	/*
+	 * Return maximum size of a request at given offset. Only valid for
+	 * file system requests.
+	 */
 	unsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);
 	unsigned mask = queue_logical_block_size(q) - 1;
 
@@ -161,6 +171,12 @@ static unsigned get_max_segment_size(struct request_queue *q,
  * Split the bvec @bv into segments, and update all kinds of
  * variables.
  */
+/*
+ * called by:
+ *   - block/blk-merge.c|231| <<blk_bio_segment_split>> bvec_split_segs(q, &bv, &nsegs,
+ *   - block/blk-merge.c|246| <<blk_bio_segment_split>> } else if (bvec_split_segs(q, &bv, &nsegs, &sectors,
+ *   - block/blk-merge.c|330| <<blk_recalc_rq_segments>> bvec_split_segs(rq->q, &bv, &nr_phys_segs, NULL, UINT_MAX);
+ */
 static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 		unsigned *nsegs, unsigned *sectors, unsigned max_segs)
 {
@@ -194,6 +210,10 @@ static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 	return !!len;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|278| <<__blk_queue_split>> split = blk_bio_segment_split(q, *bio, &q->bio_split, nr_segs);
+ */
 static struct bio *blk_bio_segment_split(struct request_queue *q,
 					 struct bio *bio,
 					 struct bio_set *bs,
@@ -205,6 +225,9 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	const unsigned max_sectors = get_max_io_size(q, bio);
 	const unsigned max_segs = queue_max_segments(q);
 
+	/*
+	 * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+	 */
 	bio_for_each_bvec(bv, bio, iter) {
 		/*
 		 * If the queue doesn't support SG gaps and adding this
@@ -247,9 +270,20 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return NULL;
 split:
 	*segs = nsegs;
+	/*
+	 * 制作一个新的bio (split)
+	 * split是从start到参数的sectors
+	 * bio则变成从参数的sectors到最后
+	 * 返回split
+	 */
 	return bio_split(bio, sectors, GFP_NOIO, bs);
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|307| <<blk_queue_split>> __blk_queue_split(q, bio, &nr_segs);
+ *   - block/blk-mq.c|1999| <<blk_mq_make_request>> __blk_queue_split(q, &bio, &nr_segs);
+ */
 void __blk_queue_split(struct request_queue *q, struct bio **bio,
 		unsigned int *nr_segs)
 {
@@ -269,6 +303,13 @@ void __blk_queue_split(struct request_queue *q, struct bio **bio,
 				nr_segs);
 		break;
 	default:
+		/*
+		 * 如果需要split, 则调用bio_split()
+		 *     制作一个新的bio (split)
+		 *     split是从start到参数的sectors
+		 *     bio则变成从参数的sectors到最后
+		 *     返回split
+		 */
 		split = blk_bio_segment_split(q, *bio, &q->bio_split, nr_segs);
 		break;
 	}
@@ -285,8 +326,22 @@ void __blk_queue_split(struct request_queue *q, struct bio **bio,
 		 * that will never happen, as we're already holding a
 		 * reference to it.
 		 */
+		/*
+		 * 在以下使用BIO_QUEUE_ENTERED:
+		 *   - block/blk-merge.c|294| <<__blk_queue_split>> bio_set_flag(*bio, BIO_QUEUE_ENTERED);
+		 *   - include/linux/blk-cgroup.h|751| <<blkcg_bio_issue_check>> if (!bio_flagged(bio, BIO_QUEUE_ENTERED))
+		 */
 		bio_set_flag(*bio, BIO_QUEUE_ENTERED);
 
+		/*
+		 * The caller won't have a bi_end_io called when @bio completes - instead,
+		 * @parent's bi_end_io won't be called until both @parent and @bio have
+		 * completed; the chained bio will also be freed when it completes.
+		 *
+		 * *bio是parent
+		 *
+		 * 查看bio_endio()
+		 */
 		bio_chain(split, *bio);
 		trace_block_split(q, split, (*bio)->bi_iter.bi_sector);
 		generic_make_request(*bio);
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index f945621..baae24a 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,12 +15,21 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|49| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ *   - block/blk-mq-cpumap.c|53| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ */
 static int cpu_to_queue_index(struct blk_mq_queue_map *qmap,
 			      unsigned int nr_queues, const int cpu)
 {
 	return qmap->queue_offset + (cpu % nr_queues);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|51| <<blk_mq_map_queues>> first_sibling = get_first_sibling(cpu);
+ */
 static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
@@ -32,9 +41,35 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3049| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3321| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|453| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|2394| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2151| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2152| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7124| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1766| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	unsigned int nr_queues = qmap->nr_queues;
 	unsigned int cpu, first_sibling;
 
@@ -68,6 +103,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2116| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2172| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2815| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94..e38ddf4 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -23,17 +23,49 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|451| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7126| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5806| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
 		mask = pci_irq_get_affinity(pdev, queue + offset);
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e..b96945e 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2382| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|2384| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
@@ -32,6 +37,19 @@ int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 */
 		for_each_cpu(cpu, mask)
 			map->mq_map[cpu] = map->queue_offset + queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index c9d183d..6b6bff3 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -167,6 +167,10 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1394| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index da19f0b..8e01c6d 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -14,6 +14,12 @@
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
 
+/*
+ * called by:
+ *   - block/blk-mq.c|280| <<blk_mq_can_queue>> return blk_mq_has_free_tags(hctx->tags);
+ *
+ * 但是没人调用blk_mq_can_queue()
+ */
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
 	if (!tags)
@@ -28,8 +34,19 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|61| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 修改和使用active_queues的地方 (比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发):
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -40,6 +57,15 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|91| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|275| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ *
+ * 猜测唤醒的是
+ * blk_mq_get_tag()
+ *  -> sbitmap_prepare_to_wait()
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -51,6 +77,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|78| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -58,8 +88,20 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		return;
 
+	/*
+	 * 修改和使用active_queues的地方 (比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发):
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 */
 	atomic_dec(&tags->active_queues);
 
+	/*
+	 * 猜测唤醒的是
+	 * blk_mq_get_tag()
+	 *  -> sbitmap_prepare_to_wait()
+	 */
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
@@ -67,6 +109,10 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|111| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -94,6 +140,12 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|176| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|198| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|204| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
@@ -106,6 +158,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|395| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1064| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
@@ -456,6 +513,15 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3186| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3189| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ *
+ * queue_requests_store()
+ *  -> blk_mq_update_nr_requests()
+ *      -> blk_mq_tag_update_depth()
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -521,6 +587,17 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/nbd.c|170| <<nbd_cmd_handle>> u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/nvme/host/nvme.h|134| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|693| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|859| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|3698| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5620| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5291| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|487| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..24827d1 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -11,6 +11,15 @@ struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 修改和使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *
+	 * 比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -36,6 +45,10 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1123| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -53,6 +66,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|385| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1056| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -61,6 +79,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|955| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2291| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 4883416..a4a0c54 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,6 +21,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|697| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|658| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
@@ -30,11 +35,37 @@ int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 	if (!vdev->config->get_vq_affinity)
 		goto fallback;
 
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
 		mask = vdev->config->get_vq_affinity(vdev, first_vec + queue);
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 0835f4d..2be1ebc 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -142,9 +142,28 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|317| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|187| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3801| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|328| <<pmem_pagemap_kill>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3971| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ *   - drivers/nvme/host/multipath.c|42| <<nvme_mpath_start_freeze>> blk_freeze_queue_start(h->disk->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	mutex_lock(&q->mq_freeze_lock);
+	/*
+	 * 在以下修改mq_freeze_depth:
+	 *   - block/blk-mq.c|148| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 * 在以下使用mq_freeze_depth:
+	 *   - block/blk-core.c|434| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|205| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|206| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	if (++q->mq_freeze_depth == 1) {
 		percpu_ref_kill(&q->q_usage_counter);
 		mutex_unlock(&q->mq_freeze_lock);
@@ -265,6 +284,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|320| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -275,6 +298,9 @@ void blk_mq_wake_waiters(struct request_queue *q)
 			blk_mq_tag_wakeup_all(hctx->tags, true);
 }
 
+/*
+ * 没人调用!
+ */
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 {
 	return blk_mq_has_free_tags(hctx->tags);
@@ -408,6 +434,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|581| <<blk_get_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|985| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/block/sx8.c|511| <<carm_array_info>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/block/sx8.c|564| <<carm_send_special>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> sense_rq = blk_mq_alloc_request(drive->queue, REQ_OP_DRV_IN,
+ *   - drivers/nvme/host/core.c|436| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2296| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -533,6 +569,11 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 	if (blk_mq_need_time_stamp(rq))
 		now = ktime_get_ns();
 
+	/*
+	 * 在以下使用RQF_STATS:
+	 *   - block/blk-mq.c|543| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+	 *   - block/blk-mq.c|706| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+	 */
 	if (rq->rq_flags & RQF_STATS) {
 		blk_mq_poll_stats_start(rq->q);
 		blk_stat_add(rq, now);
@@ -568,13 +609,29 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|657| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
+	/*
+	 * 在以下设置mq_ctx:
+	 *   - block/blk-flush.c|351| <<blk_kick_flush>> flush_rq->mq_ctx = first_rq->mq_ctx;
+	 *   - block/blk-mq.c|341| <<blk_mq_rq_ctx_init>> rq->mq_ctx = data->ctx;
+	 */
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
 	struct request_queue *q = rq->q;
 	bool shared = false;
 	int cpu;
 
+	/*
+	 * 在以下使用MQ_RQ_COMPLETE:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|594| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|689| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3756| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
 	/*
 	 * Most of single queue controllers, there is only one irq vector
@@ -594,6 +651,16 @@ static void __blk_mq_complete_request(struct request *rq)
 	 * For a polled request, always complete locallly, it's pointless
 	 * to redirect the completion.
 	 */
+	/*
+	 * 在以下使用QUEUE_FLAG_SAME_COMP:
+	 *   - block/blk-mq.c|643| <<__blk_mq_complete_request>> !test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags)) {
+	 *   - block/blk-softirq.c|130| <<__blk_complete_request>> if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) && ccpu != -1) {
+	 *   - block/blk-sysfs.c|328| <<queue_rq_affinity_show>> bool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);
+	 *   - block/blk-sysfs.c|346| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+	 *   - block/blk-sysfs.c|349| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+	 *   - block/blk-sysfs.c|352| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_COMP, q);
+	 *   - include/linux/blkdev.h|758| <<QUEUE_FLAG_MQ_DEFAULT>> (1 << QUEUE_FLAG_SAME_COMP))
+	 */
 	if ((rq->cmd_flags & REQ_HIPRI) ||
 	    !test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags)) {
 		q->mq_ops->complete(rq);
@@ -601,9 +668,22 @@ static void __blk_mq_complete_request(struct request *rq)
 	}
 
 	cpu = get_cpu();
+	/*
+	 * 在以下使用QUEUE_FLAG_SAME_FORCE:
+	 *   - block/blk-mq.c|666| <<__blk_mq_complete_request>> if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+	 *   - block/blk-softirq.c|141| <<__blk_complete_request>> if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+	 *   - block/blk-sysfs.c|329| <<queue_rq_affinity_show>> bool force = test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags);
+	 *   - block/blk-sysfs.c|347| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_FORCE, q);
+	 *   - block/blk-sysfs.c|350| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+	 *   - block/blk-sysfs.c|353| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+	 */
 	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
 		shared = cpus_share_cache(cpu, ctx->cpu);
 
+	/*
+	 * 只在一处设置cpu:
+	 *   - block/blk-mq.c|2607| <<blk_mq_init_cpu_queues>> __ctx->cpu = i;
+	 */
 	if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
 		rq->csd.func = __blk_mq_complete_request_remote;
 		rq->csd.info = rq;
@@ -643,6 +723,22 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 调用blk_mq_complete_request()的部分例子:
+ *   - block/bsg-lib.c|186| <<bsg_job_done>> blk_mq_complete_request(blk_mq_rq_from_pdu(job));
+ *   - drivers/block/loop.c|497| <<lo_rw_aio_do_completion>> blk_mq_complete_request(rq);
+ *   - drivers/block/loop.c|1933| <<loop_handle_cmd>> blk_mq_complete_request(rq);
+ *   - drivers/block/nbd.c|402| <<nbd_xmit_timeout>> blk_mq_complete_request(req);
+ *   - drivers/block/nbd.c|744| <<recv_work>> blk_mq_complete_request(blk_mq_rq_from_pdu(cmd));
+ *   - drivers/block/nbd.c|757| <<nbd_clear_req>> blk_mq_complete_request(req);
+ *   - drivers/block/null_blk_main.c|1224| <<null_handle_cmd>> blk_mq_complete_request(cmd->rq);
+ *   - drivers/block/null_blk_main.c|1315| <<null_timeout_rq>> blk_mq_complete_request(rq);
+ *   - drivers/block/virtio_blk.c|244| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/md/dm-rq.c|291| <<dm_complete_request>> blk_mq_complete_request(rq);
+ *   - drivers/nvme/host/nvme.h|434| <<nvme_end_request>> blk_mq_complete_request(req);
+ *   - drivers/scsi/scsi_lib.c|1607| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -652,6 +748,10 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * called only by:
+ *   -  drivers/nvme/host/core.c|292| <<nvme_cancel_request>> blk_mq_complete_request_sync(req);
+ */
 void blk_mq_complete_request_sync(struct request *rq)
 {
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
@@ -676,6 +776,11 @@ void blk_mq_start_request(struct request *rq)
 #ifdef CONFIG_BLK_DEV_THROTTLING_LOW
 		rq->throtl_size = blk_rq_sectors(rq);
 #endif
+		/*
+		 * 在以下使用RQF_STATS:
+		 *   - block/blk-mq.c|543| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+		 *   - block/blk-mq.c|706| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+		 */
 		rq->rq_flags |= RQF_STATS;
 		rq_qos_issue(q, rq);
 	}
@@ -1388,10 +1493,32 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1498| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+ *   - block/blk-mq.c|2719| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+ *
+ * 有可能返回>= nr_cpu_ids
+ */
 static inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * return the first cpu from *srcp1 & *srcp2
+	 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
+	 *
+	 * 设置cpumask的地方:
+	 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *
+	 * 表示这个hctx都map了那些sw cpu
+	 */
 	int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
 
+	/*
+	 * 关于cpumask_first():
+	 * get the first cpu in a cpumask
+	 * Returns >= nr_cpu_ids if no cpus set.
+	 */
 	if (cpu >= nr_cpu_ids)
 		cpu = cpumask_first(hctx->cpumask);
 	return cpu;
@@ -1403,20 +1530,57 @@ static inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx *hctx)
  * For now we just round-robin here, switching for every
  * BLK_MQ_CPU_WORK_BATCH queued items.
  */
+/*
+ * used by:
+ *   - block/blk-mq.c|1500| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ */
 static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下设置next_cpu:
+	 *   - block/blk-mq.c|1474| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|1479| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|2626| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 * 在以下使用next_cpu:
+	 *   - block/blk-mq.c|1408| <<__blk_mq_run_hw_queue>> cpu_online(hctx->next_cpu)) {
+	 *   - block/blk-mq.c|1446| <<blk_mq_hctx_next_cpu>> int next_cpu = hctx->next_cpu;
+	 *   - block/blk-mq.c|1453| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1455| <<blk_mq_hctx_next_cpu>> if (next_cpu >= nr_cpu_ids)
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 *   - block/blk-mq.c|1464| <<blk_mq_hctx_next_cpu>> if (!cpu_online(next_cpu)) {
+	 *   - block/blk-mq.c|1480| <<blk_mq_hctx_next_cpu>> return next_cpu;
+	 */
 	bool tried = false;
 	int next_cpu = hctx->next_cpu;
 
 	if (hctx->queue->nr_hw_queues == 1)
 		return WORK_CPU_UNBOUND;
 
+	/*
+	 * 在以下使用next_cpu_batch:
+	 *   - block/blk-mq.c|1469| <<blk_mq_hctx_next_cpu>> if (--hctx->next_cpu_batch <= 0) {
+	 *   - block/blk-mq.c|1475| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|1493| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = 1;
+	 *   - block/blk-mq.c|2645| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	if (--hctx->next_cpu_batch <= 0) {
 select_cpu:
+		/*
+		 * 设置cpumask的地方:
+		 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+		 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+		 *
+		 * 表示这个hctx都map了那些sw cpu
+		 */
 		next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
 				cpu_online_mask);
 		if (next_cpu >= nr_cpu_ids)
 			next_cpu = blk_mq_first_mapped_cpu(hctx);
+		/*
+		 * 在以下使用BLK_MQ_CPU_WORK_BATCH = 8:
+		 *   - block/blk-mq.c|1499| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+		 *   - block/blk-mq.c|2674| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+		 */
 		hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
 	}
 
@@ -1443,6 +1607,11 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1553| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1576| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1464,12 +1633,34 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1369| <<blk_mq_dispatch_rq_list>> blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
+ *   - drivers/ide/ide-io.c|453| <<ide_requeue_and_plug>> blk_mq_delay_run_hw_queue(q->queue_hw_ctx[0], 3);
+ *   - drivers/scsi/scsi_lib.c|1628| <<scsi_mq_get_budget>> blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|80| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|414| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|448| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|203| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1124| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1367| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1593| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1658| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1678| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1758| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2096| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2356| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1497,6 +1688,20 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL(blk_mq_run_hw_queue);
 
+/*
+ * 部分调用blk_mq_run_hw_queues()的例子:
+ *   - block/bfq-iosched.c|425| <<bfq_schedule_dispatch>> blk_mq_run_hw_queues(bfqd->queue, true);
+ *   - block/blk-mq-debugfs.c|164| <<queue_state_write>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|152| <<blk_freeze_queue_start>> blk_mq_run_hw_queues(q, false);
+ *   - block/blk-mq.c|264| <<blk_mq_unquiesce_queue>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|799| <<blk_mq_requeue_work>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/md/dm-table.c|2138| <<dm_table_run_md_queue_async>> blk_mq_run_hw_queues(queue, true);
+ *   - drivers/scsi/scsi_lib.c|362| <<scsi_kick_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|523| <<scsi_run_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|619| <<scsi_end_request>> blk_mq_run_hw_queues(q, true);
+ *   - drivers/scsi/scsi_sysfs.c|785| <<store_state_field>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_transport_fc.c|3697| <<fc_bsg_goose_queue>> blk_mq_run_hw_queues(q, true);
+ */
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1652,6 +1857,16 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|391| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|751| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1850| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1866| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ *
+ * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1851,6 +2066,11 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2045| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2050| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1884,6 +2104,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|437| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1897,6 +2121,9 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		if (ret != BLK_STS_OK) {
 			if (ret == BLK_STS_RESOURCE ||
 					ret == BLK_STS_DEV_RESOURCE) {
+				/*
+				 * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+				 */
 				blk_mq_request_bypass_insert(rq,
 							list_empty(list));
 				break;
@@ -1914,6 +2141,13 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		hctx->queue->mq_ops->commit_rqs(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1998| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *   - block/blk-mq.c|2013| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *
+ * 核心思想是把request插入到plug->mq_list
+ */
 static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 {
 	list_add_tail(&rq->queuelist, &plug->mq_list);
@@ -1974,6 +2208,11 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	plug = blk_mq_plug(q, bio);
 	if (unlikely(is_flush_fua)) {
 		/* bypass scheduler for flush rq */
+		/*
+		 * called by:
+		 *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+		 *   - block/blk-mq.c|2001| <<blk_mq_make_request>> blk_insert_flush(rq);
+		 */
 		blk_insert_flush(rq);
 		blk_mq_run_hw_queue(data.hctx, true);
 	} else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs)) {
@@ -1995,6 +2234,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			trace_block_plug(q);
 		}
 
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 	} else if (plug && !blk_queue_nomerges(q)) {
 		/*
@@ -2010,6 +2250,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			list_del_init(&same_queue_rq->queuelist);
 			plug->rq_count--;
 		}
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 		trace_block_plug(q);
 
@@ -2446,6 +2687,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3087| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3477| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2458,7 +2704,15 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/*
+	 * q->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 设置cpumask的地方:
+		 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+		 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+		 */
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
 		hctx->dispatch_from = NULL;
@@ -2484,13 +2738,61 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		}
 
 		ctx = per_cpu_ptr(q->queue_ctx, i);
+		/*
+		 * 设置nr_maps的地方:
+		 *   - block/blk-mq.c|2899| <<blk_mq_init_sq_queue>> set->nr_maps = 1; 
+		 *   - block/blk-mq.c|3234| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3245| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+		 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+		 *   - drivers/nvme/host/pci.c|2257| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2259| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 *   - drivers/nvme/host/rdma.c|1076| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+		 *   - drivers/nvme/host/tcp.c|1472| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+		 *
+		 * struct blk_mq_tag_set
+		 *   - struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+		 *       - unsigned int *mq_map;
+		 *       - unsigned int nr_queues;
+		 *       - unsigned int queue_offset;
+		 *   - unsigned int nr_maps;
+		 */
 		for (j = 0; j < set->nr_maps; j++) {
+			/*
+			 * 在以下修改nr_queues:
+			 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+			 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+			 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+			 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+			 */
 			if (!set->map[j].nr_queues) {
+				/*
+				 * struct blk_mq_ctx:
+				 *   - unsigned short          index_hw[HCTX_MAX_TYPES];
+				 *   - struct blk_mq_hw_ctx    *hctxs[HCTX_MAX_TYPES];
+				 *
+				 * blk_mq_map_queue_type()
+				 * 返回q->queue_hw_ctx[q->tag_set->map[HCTX_TYPE_DEFAULT].mq_map[i]]
+				 */
 				ctx->hctxs[j] = blk_mq_map_queue_type(q,
 						HCTX_TYPE_DEFAULT, i);
 				continue;
 			}
 
+			/*
+			 * j是以下之一:
+			 *   - HCTX_TYPE_DEFAULT
+			 *   - HCTX_TYPE_READ
+			 *   - HCTX_TYPE_POLL
+			 *
+			 * i是cpu
+			 */
 			hctx = blk_mq_map_queue_type(q, j, i);
 			ctx->hctxs[j] = hctx;
 			/*
@@ -2514,12 +2816,23 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		}
 
 		for (; j < HCTX_MAX_TYPES; j++)
+			/*
+			 * struct blk_mq_ctx:
+			 *   - unsigned short          index_hw[HCTX_MAX_TYPES];
+			 *   - struct blk_mq_hw_ctx    *hctxs[HCTX_MAX_TYPES];
+			 *
+			 * blk_mq_map_queue_type()
+			 * 返回q->queue_hw_ctx[q->tag_set->map[HCTX_TYPE_DEFAULT].mq_map[i]]
+			 */
 			ctx->hctxs[j] = blk_mq_map_queue_type(q,
 					HCTX_TYPE_DEFAULT, i);
 	}
 
 	mutex_unlock(&q->sysfs_lock);
 
+	/*
+	 * q->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		/*
 		 * If no software queues are mapped to this hardware queue,
@@ -2550,7 +2863,28 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		/*
 		 * Initialize batch roundrobin counts
 		 */
+		/*
+		 * 在以下设置next_cpu:
+		 *   - block/blk-mq.c|1474| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+		 *   - block/blk-mq.c|1479| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+		 *   - block/blk-mq.c|2626| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+		 * 在以下使用next_cpu:
+		 *   - block/blk-mq.c|1408| <<__blk_mq_run_hw_queue>> cpu_online(hctx->next_cpu)) {
+		 *   - block/blk-mq.c|1446| <<blk_mq_hctx_next_cpu>> int next_cpu = hctx->next_cpu;
+		 *   - block/blk-mq.c|1453| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+		 *   - block/blk-mq.c|1455| <<blk_mq_hctx_next_cpu>> if (next_cpu >= nr_cpu_ids)
+		 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+		 *   - block/blk-mq.c|1464| <<blk_mq_hctx_next_cpu>> if (!cpu_online(next_cpu)) {
+		 *   - block/blk-mq.c|1480| <<blk_mq_hctx_next_cpu>> return next_cpu;
+		 */
 		hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+		/*
+		 * 在以下使用next_cpu_batch:
+		 *   - block/blk-mq.c|1469| <<blk_mq_hctx_next_cpu>> if (--hctx->next_cpu_batch <= 0) {
+		 *   - block/blk-mq.c|1475| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+		 *   - block/blk-mq.c|1493| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = 1;
+		 *   - block/blk-mq.c|2645| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+		 */
 		hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
 	}
 }
@@ -2559,6 +2893,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2835| <<blk_mq_update_tag_set_depth>> queue_set_hctx_shared(q, shared);
+ *   - block/blk-mq.c|2871| <<blk_mq_add_queue_tag_set>> queue_set_hctx_shared(q, true);
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2875,6 +3214,16 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	q->tag_set = set;
 
 	q->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL:
+	 *   - block/blk-core.c|914| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3151| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-mq.c|3766| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - drivers/nvme/host/core.c|763| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 */
 	if (set->nr_maps > HCTX_TYPE_POLL &&
 	    set->map[HCTX_TYPE_POLL].nr_queues)
 		blk_queue_flag_set(QUEUE_FLAG_POLL, q);
@@ -3123,6 +3472,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3238,6 +3591,10 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3496| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3301,6 +3658,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1168| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2531| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2276| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|1233| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1657| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|508| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3310,8 +3677,19 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3660| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL_STATS:
+	 *   - block/blk-mq.c|3618| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-mq.c|3619| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+	 *   - block/blk-mq.c|3635| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+	 *   - block/blk-sysfs.c|889| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+	 */
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
 	    blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
 		return true;
@@ -3319,6 +3697,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|549| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3329,9 +3711,19 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	    blk_stat_is_active(q->poll_cb))
 		return;
 
+	/*
+	 * Gather block statistics during a time window in milliseconds.
+	 * The timer callback will be called when the window expires.
+	 *
+	 * 就是mod_timer()
+	 */
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|3118| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3343,6 +3735,10 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3692| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, hctx, rq);
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3376,6 +3772,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3745| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, hctx, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3385,6 +3785,11 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	unsigned int nsecs;
 	ktime_t kt;
 
+	/*
+	 * 在以下使用RQF_MQ_POLL_SLEPT:
+	 *   - block/blk-mq.c|3714| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+	 *   - block/blk-mq.c|3731| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+	 */
 	if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
 		return false;
 
@@ -3414,8 +3819,18 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	hrtimer_init_on_stack(&hs.timer, CLOCK_MONOTONIC, mode);
 	hrtimer_set_expires(&hs.timer, kt);
 
+	/*
+	 * 设置timer的function为hrtimer_wakeup()
+	 */
 	hrtimer_init_sleeper(&hs, current);
 	do {
+		/*
+		 * 在以下使用MQ_RQ_COMPLETE:
+		 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+		 *   - block/blk-mq.c|594| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+		 *   - block/blk-mq.c|689| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+		 *   - block/blk-mq.c|3756| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+		 */
 		if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
 			break;
 		set_current_state(TASK_UNINTERRUPTIBLE);
@@ -3431,14 +3846,48 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * 在以下patch加入的函数
+ * commit 06426adf072bca62ac31ea396ff2159a34f276c2
+ * Author: Jens Axboe <axboe@fb.com>
+ * Date:   Mon Nov 14 13:01:59 2016 -0700
+ *
+ * blk-mq: implement hybrid poll mode for sync O_DIRECT
+ *
+ * This patch enables a hybrid polling mode. Instead of polling after IO
+ * submission, we can induce an artificial delay, and then poll after that.
+ * For example, if the IO is presumed to complete in 8 usecs from now, we
+ * can sleep for 4 usecs, wake up, and then do our polling. This still puts
+ * a sleep/wakeup cycle in the IO path, but instead of the wakeup happening
+ * after the IO has completed, it'll happen before. With this hybrid
+ * scheme, we can achieve big latency reductions while still using the same
+ * (or less) amount of CPU.
+ *
+ * Signed-off-by: Jens Axboe <axboe@fb.com>
+ * Tested-By: Stephen Bates <sbates@raithlin.com>
+ * Reviewed-By: Stephen Bates <sbates@raithlin.com>
+ *
+ * called by:
+ *   - block/blk-mq.c|3781| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
 	struct request *rq;
 
+	/*
+	 * poll_nsec设置的地方:
+	 *   - block/blk-mq.c|3179| <<blk_mq_init_allocated_queue>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|384| <<queue_poll_delay_store>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|386| <<queue_poll_delay_store>> q->poll_nsec = val * 1000;
+	 */
 	if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
 		return false;
 
+	/*
+	 * blk_qc_t_to_tag():
+	 * 获取cookie的0-15位
+	 */
 	if (!blk_qc_t_is_internal(cookie))
 		rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
 	else {
@@ -3468,11 +3917,57 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * 根据一位网友的测试:
+ * 从上述测试结果来看,IO-Polling对于sync模式的direct-io的延迟有较好的提升,
+ * sync模式下,无论4K随机读或者随机写IO压力下,延迟平均大约减少5μs,而这5μs
+ * 几乎就是中断模式下,处理中断时,上下文切换的时间差.
+ * 相比随机读,对随机写的延迟降低约20%,这对延迟敏感的IO请求来说是极大的性能提升.
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|770| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|257| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|295| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|451| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|522| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap/direct-io.c|57| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap/direct-io.c|549| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|414| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ *
+ * poll是可以有专门的hctx的. 用cookie来标记用的哪个hctx甚至哪个tag.
+ *
+ * blk_qc_t_to_queue_num()把cookie转换成q->queue_hw_ctx[]的index.
+ *
+ * blk_qc_t_to_tag()把cookie转换成tag.
+ *
+ * ext4_direct_IO_write()
+ *  -> __blockdev_direct_IO()
+ *      -> do_blockdev_direct_IO
+ *          -> dio_bio_submit
+ *              -> submit_bio()
+ *          -> dio_await_completion()
+ *              -> dio_await_one()
+ *                  -> blk_poll()
+ *
+ * cookie一直通过submit_bio()返回, 然后通过blk_poll()去poll()查看IO是否完成.
+ *
+ * slides: I/O Latency Optimization with Polling
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
 	long state;
 
+	/*
+	 * 在以下使用QUEUE_FLAG_POLL:
+	 *   - block/blk-core.c|914| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-mq.c|3151| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-mq.c|3766| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+	 *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+	 *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+	 *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+	 *   - drivers/nvme/host/core.c|763| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+	 */
 	if (!blk_qc_t_valid(cookie) ||
 	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
 		return 0;
@@ -3500,6 +3995,9 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 
 		hctx->poll_invoked++;
 
+		/*
+		 * nvme pci的例子是nvme_poll()
+		 */
 		ret = q->mq_ops->poll(hctx);
 		if (ret > 0) {
 			hctx->poll_success++;
@@ -3522,8 +4020,17 @@ int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 }
 EXPORT_SYMBOL_GPL(blk_poll);
 
+/*
+ * called by:
+ *   - drivers/scsi/bnx2i/bnx2i_hwi.c|1918| <<bnx2i_queue_scsi_cmd_resp>> p = &per_cpu(bnx2i_percpu, blk_mq_rq_cpu(sc->request));
+ *   - drivers/scsi/csiostor/csio_scsi.c|1789| <<csio_queuecommand>> sqset = &hw->sqset[ln->portid][blk_mq_rq_cpu(cmnd->request)];
+ */
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
+	/*
+	 * 只在以下修改:
+	 *   - block/blk-mq.c|2574| <<blk_mq_init_cpu_queues>> __ctx->cpu = i;
+	 */
 	return rq->mq_ctx->cpu;
 }
 EXPORT_SYMBOL(blk_mq_rq_cpu);
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 32c62c6..ba573d2 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -21,6 +21,10 @@ struct blk_mq_ctx {
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
+	/*
+	 * 只在一处设置cpu:
+	 *   - block/blk-mq.c|2607| <<blk_mq_init_cpu_queues>> __ctx->cpu = i;
+	 */
 	unsigned int		cpu;
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
@@ -86,10 +90,24 @@ extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
  * @type: the hctx type index
  * @cpu: CPU
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2586| <<blk_mq_init_cpu_queues>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2683| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ *   - block/blk-mq.c|2688| <<blk_mq_map_swqueue>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2711| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
 							  enum hctx_type type,
 							  unsigned int cpu)
 {
+	/*
+	 * struct blk_mq_tag_set
+	 *   - struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+	 *       - unsigned int *mq_map;
+	 *       - unsigned int nr_queues;
+	 *       - unsigned int queue_offset;
+	 */
 	return q->queue_hw_ctx[q->tag_set->map[type].mq_map[cpu]];
 }
 
@@ -225,6 +243,11 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|64| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3202| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
@@ -251,6 +274,10 @@ static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
  *
  * Return current->plug if the bio can be plugged and NULL otherwise
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2003| <<blk_mq_make_request>> plug = blk_mq_plug(q, bio);
+ */
 static inline struct blk_plug *blk_mq_plug(struct request_queue *q,
 					   struct bio *bio)
 {
diff --git a/block/blk-rq-qos.c b/block/blk-rq-qos.c
index 3954c0d..8bf8e19 100644
--- a/block/blk-rq-qos.c
+++ b/block/blk-rq-qos.c
@@ -224,6 +224,11 @@ static int rq_qos_wake_function(struct wait_queue_entry *curr,
  * cleanup_cb is in case that we race with a waker and need to cleanup the
  * inflight count accordingly.
  */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|303| <<__blkcg_iolatency_throttle>> rq_qos_wait(rqw, iolat, iolat_acquire_inflight, iolat_cleanup_cb);
+ *   - block/blk-wbt.c|524| <<__wbt_wait>> rq_qos_wait(rqw, &data, wbt_inflight_cb, wbt_cleanup_cb);
+ */
 void rq_qos_wait(struct rq_wait *rqw, void *private_data,
 		 acquire_inflight_cb_t *acquire_inflight_cb,
 		 cleanup_cb_t *cleanup_cb)
@@ -270,10 +275,20 @@ void rq_qos_wait(struct rq_wait *rqw, void *private_data,
 	finish_wait(&rqw->wait, &data.wq);
 }
 
+/*
+ * called by only:
+ *   - block/blk-core.c|351| <<blk_cleanup_queue>> rq_qos_exit(q);
+ */
 void rq_qos_exit(struct request_queue *q)
 {
 	blk_mq_debugfs_unregister_queue_rqos(q);
 
+	/*
+	 * 设置request_queue->rq_qos的地方:
+	 *   - block/blk-rq-qos.c|279| <<rq_qos_exit>> q->rq_qos = rqos->next;
+	 *   - block/blk-rq-qos.h|98| <<rq_qos_add>> q->rq_qos = rqos;
+	 *   - block/blk-rq-qos.h|112| <<rq_qos_del>> q->rq_qos = cur;
+	 */
 	while (q->rq_qos) {
 		struct rq_qos *rqos = q->rq_qos;
 		q->rq_qos = rqos->next;
diff --git a/block/blk-rq-qos.h b/block/blk-rq-qos.h
index 2300e03..2fc9d5a 100644
--- a/block/blk-rq-qos.h
+++ b/block/blk-rq-qos.h
@@ -92,6 +92,10 @@ static inline void rq_wait_init(struct rq_wait *rq_wait)
 	init_waitqueue_head(&rq_wait->wait);
 }
 
+/*
+ * called by:
+ *   - block/blk-rq-qos.h|95| <<rq_qos_add>> static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
+ */
 static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
 {
 	rqos->next = q->rq_qos;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 2c18312..302c8a4 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -132,6 +132,23 @@ EXPORT_SYMBOL(blk_queue_make_request);
  *    blk_queue_bounce_limit to have lower memory pages allocated as bounce
  *    buffers for doing I/O to pages residing above @max_addr.
  **/
+/*
+ * called by:
+ *   - drivers/block/floppy.c|4569| <<do_floppy_init>> blk_queue_bounce_limit(disks[drive]->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/paride/pcd.c|324| <<pcd_init_units>> blk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/paride/pd.c|925| <<pd_probe_drive>> blk_queue_bounce_limit(p->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/paride/pf.c|311| <<pf_init_units>> blk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/swim.c|849| <<swim_floppy_init>> blk_queue_bounce_limit(swd->unit[drive].disk->queue,
+ *   - drivers/block/swim3.c|1204| <<swim3_attach>> blk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/xsysace.c|1020| <<ace_setup>> blk_queue_bounce_limit(ace->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/cdrom/gdrom.c|789| <<probe_gdrom>> blk_queue_bounce_limit(gd.gdrom_rq, BLK_BOUNCE_HIGH);
+ *   - drivers/mmc/core/queue.c|362| <<mmc_setup_queue>> blk_queue_bounce_limit(mq->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/aha152x.c|2894| <<aha152x_adjust_queue>> blk_queue_bounce_limit(device->request_queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/imm.c|1099| <<imm_adjust_queue>> blk_queue_bounce_limit(device->request_queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/ppa.c|966| <<ppa_adjust_queue>> blk_queue_bounce_limit(device->request_queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/scsi_lib.c|1793| <<__scsi_init_queue>> blk_queue_bounce_limit(q, BLK_BOUNCE_ISA);
+ *   - drivers/usb/storage/scsiglue.c|149| <<slave_configure>> blk_queue_bounce_limit(sdev->request_queue, BLK_BOUNCE_HIGH);
+ */
 void blk_queue_bounce_limit(struct request_queue *q, u64 max_addr)
 {
 	unsigned long b_pfn = max_addr >> PAGE_SHIFT;
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 457d9ba..8e7e008 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -14,6 +14,15 @@
 
 #include "blk.h"
 
+/*
+ * 在以下使用blk_cpu_done:
+ *   - block/blk-softirq.c|28| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|49| <<trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|90| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+ *   - block/blk-softirq.c|91| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+ *   - block/blk-softirq.c|130| <<__blk_complete_request>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|152| <<blk_softirq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+ */
 static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
 
 /*
@@ -58,6 +67,10 @@ static void trigger_softirq(void *data)
 /*
  * Setup and invoke a run of 'trigger_softirq' on the given cpu.
  */
+/*
+ * called by:
+ *   - block/blk-softirq.c|141| <<__blk_complete_request>> } else if (raise_blk_irq(ccpu, req))
+ */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
@@ -95,6 +108,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|596| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -110,7 +127,26 @@ void __blk_complete_request(struct request *req)
 	/*
 	 * Select completion CPU
 	 */
+	/*
+	 * 在以下使用QUEUE_FLAG_SAME_COMP:
+	 *   - block/blk-mq.c|643| <<__blk_mq_complete_request>> !test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags)) {
+	 *   - block/blk-softirq.c|130| <<__blk_complete_request>> if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) && ccpu != -1) {
+	 *   - block/blk-sysfs.c|328| <<queue_rq_affinity_show>> bool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);
+	 *   - block/blk-sysfs.c|346| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+	 *   - block/blk-sysfs.c|349| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+	 *   - block/blk-sysfs.c|352| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_COMP, q);
+	 *   - include/linux/blkdev.h|758| <<QUEUE_FLAG_MQ_DEFAULT>> (1 << QUEUE_FLAG_SAME_COMP))
+	 */
 	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) && ccpu != -1) {
+		/*
+		 * 在以下使用QUEUE_FLAG_SAME_FORCE:
+		 *   - block/blk-mq.c|666| <<__blk_mq_complete_request>> if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+		 *   - block/blk-softirq.c|141| <<__blk_complete_request>> if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+		 *   - block/blk-sysfs.c|329| <<queue_rq_affinity_show>> bool force = test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags);
+		 *   - block/blk-sysfs.c|347| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_FORCE, q);
+		 *   - block/blk-sysfs.c|350| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+		 *   - block/blk-sysfs.c|353| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+		 */
 		if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
 			shared = cpus_share_cache(cpu, ccpu);
 	} else
@@ -148,9 +184,19 @@ static __init int blk_softirq_init(void)
 {
 	int i;
 
+	/*
+	 * 在上面声明
+	 * static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
+	 */
 	for_each_possible_cpu(i)
 		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
 
+	/*
+	 * 触发BLOCK_SOFTIRQ的地方:
+	 *   - block/blk-softirq.c|53| <<trigger_softirq>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-softirq.c|92| <<blk_softirq_cpu_dead>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-softirq.c|140| <<__blk_complete_request>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 */
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
 				  "block/softirq:dead", NULL,
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 940f15d..2774730 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -48,6 +48,10 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|545| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -132,6 +136,11 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3385| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|862| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -180,6 +189,11 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2447| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 8aa68fa..77b5f9e 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -12,6 +12,12 @@
 
 #ifdef CONFIG_FAIL_IO_TIMEOUT
 
+/*
+ * called by:
+ *   - block/blk-timeout.c|19| <<setup_fail_io_timeout>> return setup_fault_attr(&fail_io_timeout, str);
+ *   - block/blk-timeout.c|28| <<blk_should_fake_timeout>> return should_fail(&fail_io_timeout, 1);
+ *   - block/blk-timeout.c|34| <<fail_io_timeout_debugfs>> NULL, &fail_io_timeout);
+ */
 static DECLARE_FAULT_ATTR(fail_io_timeout);
 
 static int __init setup_fail_io_timeout(char *str)
@@ -22,6 +28,13 @@ __setup("fail_io_timeout=", setup_fail_io_timeout);
 
 int blk_should_fake_timeout(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_FAIL_IO:
+	 *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+	 *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+	 *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+	 *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+	 */
 	if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
 		return 0;
 
@@ -38,6 +51,10 @@ static int __init fail_io_timeout_debugfs(void)
 
 late_initcall(fail_io_timeout_debugfs);
 
+/*
+ * 在block/genhd.c用做:
+ * __ATTR(io-timeout-fail, 0644, part_timeout_show, part_timeout_store);
+ */
 ssize_t part_timeout_show(struct device *dev, struct device_attribute *attr,
 			  char *buf)
 {
@@ -78,6 +95,15 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
  * LLDDs who implement their own error recovery MAY ignore the timeout
  * event if they generated blk_abort_request.
  */
+/*
+ * called by:
+ *   - drivers/ata/libata-eh.c|916| <<ata_qc_schedule_eh>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2617| <<mtip_queue_cmd>> blk_abort_request(req);
+ *   - drivers/s390/block/dasd_ioctl.c|168| <<dasd_ioctl_abortio>> blk_abort_request(cqr->callback_data);
+ *   - drivers/scsi/libsas/sas_ata.c|588| <<sas_ata_task_abort>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/scsi/libsas/sas_scsi_host.c|911| <<sas_task_abort>> blk_abort_request(sc->request);
+ *   - drivers/scsi/scsi_debug.c|4390| <<schedule_resp>> blk_abort_request(cmnd->request);
+ */
 void blk_abort_request(struct request *req)
 {
 	/*
@@ -90,6 +116,10 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
+/*
+ * called by:
+ *   - block/blk-timeout.c|134| <<blk_add_timer>> expiry = blk_rq_timeout(round_jiffies_up(expiry));
+ */
 unsigned long blk_rq_timeout(unsigned long timeout)
 {
 	unsigned long maxt;
@@ -109,6 +139,11 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|685| <<blk_mq_start_request>> blk_add_timer(rq);
+ *   - block/blk-mq.c|853| <<blk_mq_rq_timed_out>> blk_add_timer(req);
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -121,6 +156,13 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
+	/*
+	 * 在以下使用RQF_TIMED_OUT (timeout has been called, don't expire again):
+	 *   - block/blk-mq.c|710| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+	 *   - block/blk-mq.c|843| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+	 *   - block/blk-mq.c|862| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+	 *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+	 */
 	req->rq_flags &= ~RQF_TIMED_OUT;
 
 	expiry = jiffies + req->timeout;
diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index 313f45a..55003f3 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -648,6 +648,11 @@ void wbt_set_write_cache(struct request_queue *q, bool write_cache_on)
 /*
  * Enable wbt if defaults are configured that way
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|995| <<blk_register_queue>> wbt_enable_default(q);
+ *   - block/elevator.c|507| <<elv_unregister_queue>> wbt_enable_default(q);
+ */
 void wbt_enable_default(struct request_queue *q)
 {
 	struct rq_qos *rqos = wbt_rq_qos(q);
@@ -817,6 +822,11 @@ static struct rq_qos_ops wbt_rqos_ops = {
 #endif
 };
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|463| <<queue_wb_lat_store>> ret = wbt_init(q);
+ *   - block/blk-wbt.c|663| <<wbt_enable_default>> wbt_init(q);
+ */
 int wbt_init(struct request_queue *q)
 {
 	struct rq_wb *rwb;
diff --git a/block/blk.h b/block/blk.h
index de6b2e1..45d77a5 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -80,9 +80,17 @@ static inline bool biovec_phys_mergeable(struct request_queue *q,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|54| <<bio_will_gap>> return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
+ *   - block/blk.h|107| <<bvec_gap_to_prev>> return __bvec_gap_to_prev(q, bprv, offset);
+ */
 static inline bool __bvec_gap_to_prev(struct request_queue *q,
 		struct bio_vec *bprv, unsigned int offset)
 {
+	/*
+	 * 比如offset是下一个bvec的offset, bprv是上一个的bvec
+	 */
 	return (offset & queue_virt_boundary(q)) ||
 		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
 }
@@ -91,6 +99,14 @@ static inline bool __bvec_gap_to_prev(struct request_queue *q,
  * Check if adding a bio_vec after bprv with offset would create a gap in
  * the SG list. Most drivers don't care about this, but some do.
  */
+/*
+ * called by:
+ *   - block/bio-integrity.c|142| <<bio_integrity_add_page>> bvec_gap_to_prev(bio->bi_disk->queue,
+ *   - block/bio.c|717| <<__bio_add_pc_page>> if (bvec_gap_to_prev(q, bvec, offset))
+ *   - block/blk-merge.c|219| <<blk_bio_segment_split>> if (bvprvp && bvec_gap_to_prev(q, bvprvp, bv.bv_offset))
+ *   - block/blk.h|130| <<integrity_req_gap_back_merge>> return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+ *   - block/blk.h|140| <<integrity_req_gap_front_merge>> return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+ */
 static inline bool bvec_gap_to_prev(struct request_queue *q,
 		struct bio_vec *bprv, unsigned int offset)
 {
diff --git a/block/bounce.c b/block/bounce.c
index f8ed677..38832b1 100644
--- a/block/bounce.c
+++ b/block/bounce.c
@@ -283,6 +283,10 @@ static struct bio *bounce_clone_bio(struct bio *bio_src, gfp_t gfp_mask,
 	return bio;
 }
 
+/*
+ * called by:
+ *   - block/bounce.c|392| <<blk_queue_bounce>> __blk_queue_bounce(q, bio_orig, pool);
+ */
 static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 			       mempool_t *pool)
 {
@@ -295,6 +299,9 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	int sectors = 0;
 	bool passthrough = bio_is_passthrough(*bio_orig);
 
+	/*
+	 * 从bio->bi_iter开始, 遍历每一个最大1个page的bvec
+	 */
 	bio_for_each_segment(from, *bio_orig, iter) {
 		if (i++ < BIO_MAX_PAGES)
 			sectors += from.bv_len >> 9;
@@ -357,6 +364,11 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	*bio_orig = bio;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|25| <<blk_rq_append_bio>> blk_queue_bounce(rq->q, bio);
+ *   - block/blk-mq.c|1998| <<blk_mq_make_request>> blk_queue_bounce(q, &bio);
+ */
 void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 {
 	mempool_t *pool;
diff --git a/block/partitions/check.c b/block/partitions/check.c
index ffe408f..2b602f9 100644
--- a/block/partitions/check.c
+++ b/block/partitions/check.c
@@ -139,6 +139,10 @@ void free_partitions(struct parsed_partitions *state)
 	kfree(state);
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|531| <<rescan_partitions>> if (!get_capacity(disk) || !(state = check_partition(disk, bdev)))
+ */
 struct parsed_partitions *
 check_partition(struct gendisk *hd, struct block_device *bdev)
 {
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index d3d6b7b..244fccf 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -702,6 +702,14 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2319| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|892| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|2229| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2063| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|145| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
@@ -747,6 +755,10 @@ static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
 	complete(waiting);
 }
 
+/*
+ * calle by:
+ *   - drivers/nvme/host/core.c|800| <<__nvme_submit_sync_cmd>> nvme_execute_rq_polled(req->q, NULL, req, at_head);
+ */
 static void nvme_execute_rq_polled(struct request_queue *q,
 		struct gendisk *bd_disk, struct request *rq, int at_head)
 {
@@ -1500,6 +1512,10 @@ static int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo)
 }
 
 #ifdef CONFIG_BLK_DEV_INTEGRITY
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1690| <<nvme_update_disk_info>> nvme_init_integrity(disk, ns->ms, ns->pi_type);
+ */
 static void nvme_init_integrity(struct gendisk *disk, u16 ms, u8 pi_type)
 {
 	struct blk_integrity integrity;
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 1994d5b..432bcc1 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -490,6 +490,13 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3449| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|2328| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|691| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
@@ -977,6 +984,10 @@ EXPORT_SYMBOL_GPL(nvmf_free_options);
 				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
 				 NVMF_OPT_DISABLE_SQFLOW)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1083| <<nvmf_dev_write>> ctrl = nvmf_create_ctrl(nvmf_device, buf);
+ */
 static struct nvme_ctrl *
 nvmf_create_ctrl(struct device *dev, const char *buf)
 {
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index 3044d8b..055e773 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -99,6 +99,15 @@ struct nvmf_ctrl_options {
 	unsigned int		nr_io_queues;
 	unsigned int		reconnect_delay;
 	bool			discovery_nqn;
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {
+	 */
 	bool			duplicate_connect;
 	unsigned int		kato;
 	struct nvmf_host	*host;
diff --git a/drivers/nvme/host/fault_inject.c b/drivers/nvme/host/fault_inject.c
index 1352159..84ece48 100644
--- a/drivers/nvme/host/fault_inject.c
+++ b/drivers/nvme/host/fault_inject.c
@@ -54,6 +54,10 @@ void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inject)
 	debugfs_remove_recursive(fault_inject->parent);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/nvme.h|433| <<nvme_end_request>> nvme_should_fail(req);
+ */
 void nvme_should_fail(struct request *req)
 {
 	struct gendisk *disk = req->rq_disk;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 2d678fb..f41d166 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -179,6 +179,23 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 
@@ -405,6 +422,16 @@ static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
 	return (sector >> (ns->lba_shift - 9));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|975| <<nvme_handle_cqe>> nvme_end_request(req, cqe->status, cqe->result);
+ *   - drivers/nvme/host/rdma.c|1519| <<nvme_rdma_inv_rkey_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1762| <<nvme_rdma_send_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1926| <<nvme_rdma_process_nvme_rsp>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/tcp.c|440| <<nvme_tcp_process_nvme_cqe>> nvme_end_request(rq, cqe->status, cqe->result);
+ *   - drivers/nvme/host/tcp.c|634| <<nvme_tcp_end_request>> nvme_end_request(rq, cpu_to_le16(status << 1), res);
+ *   - drivers/nvme/target/loop.c|131| <<nvme_loop_queue_response>> nvme_end_request(rq, cqe->status, cqe->result);
+ */
 static inline void nvme_end_request(struct request *req, __le16 status,
 		union nvme_result result)
 {
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 732d5b6..8bd0582 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -1074,6 +1074,12 @@ static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
 	return found;
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|3917| <<blk_poll>> ret = q->mq_ops->poll(hctx);
+ *
+ * struct blk_mq_ops nvme_mq_ops.poll = nvme_poll()
+ */
 static int nvme_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_queue *nvmeq = hctx->driver_data;
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 1a6449b..08f9e7d 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -27,6 +27,9 @@
 #include "nvme.h"
 #include "fabrics.h"
 
+/*
+ * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+ */
 
 #define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
 
@@ -39,6 +42,12 @@ struct nvme_rdma_device {
 	struct ib_pd		*pd;
 	struct kref		ref;
 	struct list_head	entry;
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	unsigned int		num_inline_segments;
 };
 
@@ -73,8 +82,27 @@ enum nvme_rdma_queue_flags {
 };
 
 struct nvme_rdma_queue {
+	/*
+	 * 分配, 释放和使用rsp_ring的地方:
+	 *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|595| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|597| <<nvme_rdma_create_queue_ib>> if (!queue->rsp_ring) {
+	 *   - drivers/nvme/host/rdma.c|618| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|1756| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+	 *
+	 * 是一个数组, 有queue->queue_size个struct nvme_rdma_qe
+	 */
 	struct nvme_rdma_qe	*rsp_ring;
+	/*
+	 * 设置queue_size的地方:
+	 *   - drivers/nvme/host/rdma.c|659| <<nvme_rdma_alloc_queue>> queue->queue_size = queue_size;
+	 */
 	int			queue_size;
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	size_t			cmnd_capsule_len;
 	struct nvme_rdma_ctrl	*ctrl;
 	struct nvme_rdma_device	*device;
@@ -84,19 +112,56 @@ struct nvme_rdma_queue {
 	unsigned long		flags;
 	struct rdma_cm_id	*cm_id;
 	int			cm_error;
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	struct completion	cm_done;
 };
 
 struct nvme_rdma_ctrl {
 	/* read only in the hot path */
+	/*
+	 * nvme_rdma_create_ctrl()分配的queues
+	 * 2402         ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+	 * 2403                                 GFP_KERNEL);
+	 */
 	struct nvme_rdma_queue	*queues;
 
 	/* other member variables */
 	struct blk_mq_tag_set	tag_set;
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	struct work_struct	err_work;
 
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	struct nvme_rdma_qe	async_event_sqe;
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	struct delayed_work	reconnect_work;
 
 	struct list_head	list;
@@ -110,18 +175,49 @@ struct nvme_rdma_ctrl {
 	struct sockaddr_storage src_addr;
 
 	struct nvme_ctrl	ctrl;
+	/*
+	 * 在以下修改和使用use_inline_data:
+	 *   - drivers/nvme/host/rdma.c|1205| <<nvme_rdma_setup_ctrl>> ctrl->use_inline_data = true;
+	 *   - drivers/nvme/host/rdma.c|1516| <<nvme_rdma_map_data>> queue->ctrl->use_inline_data &&
+	 */
 	bool			use_inline_data;
+	/*
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	u32			io_queues[HCTX_MAX_TYPES];
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|729| <<nvme_rdma_alloc_tagset>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|951| <<nvme_rdma_free_ctrl>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+ *   - drivers/nvme/host/rdma.c|1922| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ */
 static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|373| <<nvme_rdma_find_get_device>> list_for_each_entry(ndev, &device_list, entry) {
+ *   - drivers/nvme/host/rdma.c|400| <<nvme_rdma_find_get_device>> list_add(&ndev->entry, &device_list);
+ *   - drivers/nvme/host/rdma.c|2119| <<nvme_rdma_remove_one>> list_for_each_entry(ndev, &device_list, entry) {
+ */
 static LIST_HEAD(device_list);
 static DEFINE_MUTEX(device_list_mutex);
 
+/*
+ * 在以下使用nvme_rdma_ctrl_list:
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+ *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ */
 static LIST_HEAD(nvme_rdma_ctrl_list);
 static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
 
@@ -130,6 +226,10 @@ static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
  * unsafe.  With it turned off we will have to register a global rkey that
  * allows read and write access to all physical memory.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|387| <<nvme_rdma_find_get_device>> register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+ */
 static bool register_always = true;
 module_param(register_always, bool, 0444);
 MODULE_PARM_DESC(register_always,
@@ -139,10 +239,24 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *event);
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops;
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
 
 /* XXX: really should move to a generic header sooner or later.. */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1223| <<nvme_rdma_set_sg_null>> put_unaligned_le24(0, sg->length);
+ *   - drivers/nvme/host/rdma.c|1260| <<nvme_rdma_map_sg_single>> put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
+ *   - drivers/nvme/host/rdma.c|1304| <<nvme_rdma_map_sg_fr>> put_unaligned_le24(req->mr->length, sg->length);
+ */
 static inline void put_unaligned_le24(u32 val, u8 *p)
 {
 	*p++ = val;
@@ -152,28 +266,76 @@ static inline void put_unaligned_le24(u32 val, u8 *p)
 
 static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctrl
+	 */
 	return queue - queue->ctrl->queues;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|471| <<nvme_rdma_create_queue_ib>> if (nvme_rdma_poll_queue(queue))
+ *   - drivers/nvme/host/rdma.c|620| <<nvme_rdma_start_queue>> bool poll = nvme_rdma_poll_queue(queue);
+ */
 static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctr
+	 *
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	return nvme_rdma_queue_idx(queue) >
 		queue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +
 		queue->ctrl->io_queues[HCTX_TYPE_READ];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1540| <<nvme_rdma_map_data>> nvme_rdma_inline_data_size(queue)) {
+ */
 static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|201| <<nvme_rdma_free_ring>> nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+ */
 static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
+	/*
+	 * struct nvme_rdma_qe {
+	 *         struct ib_cqe           cqe;
+	 *         void                    *data;
+	 *         u64                     dma;
+	 * };
+	 */
 	ib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);
 	kfree(qe->data);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|257| <<nvme_rdma_alloc_ring>> if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+ *   - drivers/nvme/host/rdma.c|842| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+ */
 static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -191,17 +353,31 @@ static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|264| <<nvme_rdma_alloc_ring>> nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|474| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ *   - drivers/nvme/host/rdma.c|551| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ */
 static void nvme_rdma_free_ring(struct ib_device *ibdev,
 		struct nvme_rdma_qe *ring, size_t ib_queue_size,
 		size_t capsule_size, enum dma_data_direction dir)
 {
 	int i;
 
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+	 */
 	for (i = 0; i < ib_queue_size; i++)
 		nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
 	kfree(ring);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+ */
 static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 		size_t ib_queue_size, size_t capsule_size,
 		enum dma_data_direction dir)
@@ -218,6 +394,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	 * lifetime. It's safe, since any chage in the underlying RDMA device
 	 * will issue error recovery and queue re-creation.
 	 */
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+	 */
 	for (i = 0; i < ib_queue_size; i++) {
 		if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
 			goto out_free_ring;
@@ -230,6 +410,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|300| <<nvme_rdma_create_qp>> init_attr.event_handler = nvme_rdma_qp_event;
+ */
 static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 {
 	pr_debug("QP event %s (%d)\n",
@@ -237,10 +421,24 @@ static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|634| <<nvme_rdma_alloc_queue>> ret = nvme_rdma_wait_for_cm(queue);
+ *
+ * 核心思想是等待nvme_rdma_queue->cm_done被complete
+ * 只在nvme_rdma_cm_handler被唤醒
+ */
 static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 {
 	int ret;
 
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
 			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
 	if (ret < 0)
@@ -251,6 +449,10 @@ static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 	return queue->cm_error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|485| <<nvme_rdma_create_queue_ib>> ret = nvme_rdma_create_qp(queue, send_wr_factor);
+ */
 static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 {
 	struct nvme_rdma_device *dev = queue->device;
@@ -270,20 +472,42 @@ static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 	init_attr.send_cq = queue->ib_cq;
 	init_attr.recv_cq = queue->ib_cq;
 
+	/*
+	 * dev->pd是struct ib_pd
+	 * queue->cm_id是struct rdma_cm_id
+	 */
 	ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
 
+	/*
+	 * queue->qp是struct ib_qp
+	 */
 	queue->qp = queue->cm_id->qp;
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.exit_request = nvme_rdma_exit_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.exit_request = nvme_rdma_exit_request()
+ */
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	kfree(req->sqe.data);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_request = nvme_rdma_init_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_request = nvme_rdma_init_request()
+ */
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -294,6 +518,13 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
 
 	nvme_req(rq)->ctrl = &ctrl->ctrl;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.data = kzalloc(sizeof(struct nvme_command), GFP_KERNEL);
 	if (!req->sqe.data)
 		return -ENOMEM;
@@ -303,6 +534,9 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_hctx = nvme_rdma_init_hctx()
+ */
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -315,6 +549,9 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_hctx = nvme_rdma_init_admin_hctx()
+ */
 static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -350,6 +587,17 @@ static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
 	return kref_get_unless_zero(&dev->ref);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|530| <<nvme_rdma_create_queue_ib>> queue->device = nvme_rdma_find_get_device(queue->cm_id);
+ *
+ * nvme_rdma_cm_handler()
+ *  -> nvme_rdma_addr_resolved()
+ *      -> nvme_rdma_create_queue_ib()
+ *          -> nvme_rdma_find_get_device()
+ *
+ * 惨是的rdma_cm_id->qp是struct ib_qp
+ */
 static struct nvme_rdma_device *
 nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 {
@@ -366,9 +614,22 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	if (!ndev)
 		goto out_err;
 
+	/* 类型是struct ib_device */
 	ndev->dev = cm_id->device;
 	kref_init(&ndev->ref);
 
+	/*
+	 * ndev->pd是struct ib_pd
+	 *
+	 * ib_alloc_pd - Allocates an unused protection domain.
+	 * @device: The device on which to allocate the protection domain.
+	 *
+	 * A protection domain object provides an association between QPs, shared
+	 * receive queues, address handles, memory regions, and memory windows.
+	 *
+	 * Every PD has a local_dma_lkey which can be used as the lkey value for local
+	 * memory operations.
+	 */
 	ndev->pd = ib_alloc_pd(ndev->dev,
 		register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
 	if (IS_ERR(ndev->pd))
@@ -397,6 +658,16 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_alloc_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|799| <<nvme_rdma_free_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1846| <<nvme_rdma_conn_established>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1899| <<nvme_rdma_addr_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1953| <<nvme_rdma_route_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2008| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2014| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ */
 static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_device *dev;
@@ -424,12 +695,30 @@ static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 	nvme_rdma_dev_put(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|687| <<nvme_rdma_create_queue_ib>> nvme_rdma_get_max_fr_pages(ibdev), 0);
+ *   - drivers/nvme/host/rdma.c|1016| <<nvme_rdma_configure_admin_queue>> ctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev);
+ */
 static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
 {
 	return min_t(u32, NVME_RDMA_MAX_SEGMENTS,
 		     ibdev->attrs.max_fast_reg_page_list_len);
 }
 
+/*
+ * [0] nvme_rdma_create_queue_ib
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1699| <<nvme_rdma_addr_resolved>> ret = nvme_rdma_create_queue_ib(queue);
+ */
 static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct ib_device *ibdev;
@@ -506,6 +795,20 @@ static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_alloc_queue [nvme_rdma]
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_io_queues>> ret = nvme_rdma_alloc_queue(ctrl, i,
+ *   - drivers/nvme/host/rdma.c|828| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ */
 static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		int idx, size_t queue_size)
 {
@@ -600,6 +903,11 @@ static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
 		nvme_rdma_stop_queue(&ctrl->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|715| <<nvme_rdma_start_io_queues>> ret = nvme_rdma_start_queue(ctrl, i);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_start_queue(ctrl, 0);
+ */
 static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 {
 	struct nvme_rdma_queue *queue = &ctrl->queues[idx];
@@ -621,6 +929,10 @@ static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1203| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_start_io_queues(ctrl);
+ */
 static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	int i, ret = 0;
@@ -639,6 +951,10 @@ static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_alloc_io_queues(ctrl);
+ */
 static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -709,6 +1025,21 @@ static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|848| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ *   - drivers/nvme/host/rdma.c|925| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ */
 static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
 		bool admin)
 {
@@ -767,6 +1098,10 @@ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_queue(&ctrl->queues[0]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1035| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_admin_queue(ctrl, new);
+ */
 static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 		bool new)
 {
@@ -786,6 +1121,16 @@ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	 * It's safe, since any chage in the underlying RDMA device will issue
 	 * error recovery and queue re-creation.
 	 */
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
 			sizeof(struct nvme_command), DMA_TO_DEVICE);
 	if (error)
@@ -860,6 +1205,10 @@ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_io_queues(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1180| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_io_queues(ctrl, new);
+ */
 static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret;
@@ -929,6 +1278,9 @@ static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
 	}
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.free_ctrl = nvme_rdma_free_ctrl()
+ */
 static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
@@ -946,6 +1298,12 @@ static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 	kfree(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1178| <<nvme_rdma_reconnect_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|1197| <<nvme_rdma_error_recovery_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|2114| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ */
 static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 {
 	/* If we are resetting/deleting then do nothing */
@@ -965,6 +1323,30 @@ static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 	}
 }
 
+/*
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1054| <<nvme_rdma_reconnect_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|1939| <<nvme_rdma_reset_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|2071| <<nvme_rdma_create_ctrl>> ret = nvme_rdma_setup_ctrl(ctrl, true);
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ */
 static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret = -EINVAL;
@@ -1027,6 +1409,13 @@ static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 	return ret;
 }
 
+/*
+ * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+ *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+ *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+ */
 static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
@@ -1050,6 +1439,14 @@ static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+ *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+ *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+ */
 static void nvme_rdma_error_recovery_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(work,
@@ -1069,6 +1466,16 @@ static void nvme_rdma_error_recovery_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1251| <<nvme_rdma_wr_error>> nvme_rdma_error_recovery(ctrl);
+ *   - drivers/nvme/host/rdma.c|1627| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1640| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1650| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1900| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1947| <<nvme_rdma_timeout>> nvme_rdma_error_recovery(ctrl);
+ */
 static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
@@ -1201,6 +1608,10 @@ static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1688| <<nvme_rdma_map_data>> ret = nvme_rdma_map_sg_fr(queue, req, c, count);
+ */
 static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_request *req, struct nvme_command *c,
 		int count)
@@ -1208,6 +1619,9 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
 	int nr;
 
+	/*
+	 * req->mr是struct ib_mr
+	 */
 	req->mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);
 	if (WARN_ON_ONCE(!req->mr))
 		return -EAGAIN;
@@ -1247,6 +1661,10 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1828| <<nvme_rdma_queue_rq>> err = nvme_rdma_map_data(queue, rq, c);
+ */
 static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		struct request *rq, struct nvme_command *c)
 {
@@ -1279,6 +1697,12 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		goto out_free_table;
 	}
 
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	if (count <= dev->num_inline_segments) {
 		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
 		    queue->ctrl->use_inline_data &&
@@ -1310,6 +1734,17 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_send_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1327,6 +1762,11 @@ static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1835| <<nvme_rdma_submit_async_event>> ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
+ *   - drivers/nvme/host/rdma.c|2248| <<nvme_rdma_queue_rq>> err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
+ */
 static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
 		struct ib_send_wr *first)
@@ -1358,6 +1798,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1641| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1653| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1376,6 +1821,10 @@ static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 	wr.sg_list  = &list;
 	wr.num_sge  = 1;
 
+	/*
+	 * Posts a list of work requests to the receive queue of
+	 *   the specified QP.
+	 */
 	ret = ib_post_recv(queue->qp, &wr, NULL);
 	if (unlikely(ret)) {
 		dev_err(queue->ctrl->ctrl.device,
@@ -1399,6 +1848,9 @@ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_rdma_wr_error(cq, wc, "ASYNC");
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.submit_async_event = nvme_rdma_submit_async_event()
+ */
 static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
@@ -1426,6 +1878,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1564| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1470,6 +1926,20 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * [0] nvme_rdma_recv_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1779| <<nvme_rdma_post_recv>> qe->cqe.done = nvme_rdma_recv_done;
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1502,6 +1972,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1712| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -1545,6 +2019,10 @@ static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
 	return -ECONNRESET;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1706| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_addr_resolved(queue);
+ */
 static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -1568,6 +2046,10 @@ static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1709| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_route_resolved(queue);
+ */
 static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
@@ -1618,6 +2100,34 @@ static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_ib_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|580| <<nvme_rdma_alloc_queue>> queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+ */
 static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *ev)
 {
@@ -1629,6 +2139,18 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		ev->status, cm_id);
 
 	switch (ev->event) {
+		/*
+		 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 */
 	case RDMA_CM_EVENT_ADDR_RESOLVED:
 		cm_error = nvme_rdma_addr_resolved(queue);
 		break;
@@ -1679,6 +2201,10 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.timeout = nvme_rdma_timeout()
+ * struct blk_mq_ops nvme_rdma_mq_ops.timeout = nvme_rdma_timeout()
+ */
 static enum blk_eh_timer_return
 nvme_rdma_timeout(struct request *rq, bool reserved)
 {
@@ -1707,6 +2233,10 @@ nvme_rdma_timeout(struct request *rq, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ */
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1715,6 +2245,15 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct request *rq = bd->rq;
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 	struct nvme_rdma_qe *sqe = &req->sqe;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 *
+	 * 注意, c指向的req->sqe.data!!!!!!!!
+	 */
 	struct nvme_command *c = sqe->data;
 	struct ib_device *dev;
 	bool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);
@@ -1728,6 +2267,17 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	dev = queue->device->dev;
 
+	/*
+	 * req是struct nvme_rdma_request类型
+	 * 来自blk_mq_rq_to_pdu(rq)
+	 *
+	 * 这里req->sqe.dma对应的req->sqe.data (存储的nvme_command c)
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.dma = ib_dma_map_single(dev, req->sqe.data,
 					 sizeof(struct nvme_command),
 					 DMA_TO_DEVICE);
@@ -1777,6 +2327,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.poll = nvme_rdma_poll()
+ */
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1784,6 +2337,10 @@ static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 	return ib_process_cq_direct(queue->ib_cq, -1);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.complete = nvme_rdma_complete_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.complete = nvme_rdma_complete_rq()
+ */
 static void nvme_rdma_complete_rq(struct request *rq)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
@@ -1796,6 +2353,9 @@ static void nvme_rdma_complete_rq(struct request *rq)
 	nvme_complete_rq(rq);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.map_queues = nvme_rdma_map_queues()
+ */
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
@@ -1843,6 +2403,10 @@ static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1854,6 +2418,10 @@ static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.poll		= nvme_rdma_poll,
 };
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1863,6 +2431,11 @@ static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.timeout	= nvme_rdma_timeout,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1975| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ *   - drivers/nvme/host/rdma.c|1988| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_shutdown_ctrl(ctrl, false);
+ */
 static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 {
 	cancel_work_sync(&ctrl->err_work);
@@ -1876,11 +2449,18 @@ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.delete_ctrl = nvme_rdma_delete_ctrl()
+ */
 static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|2050| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ */
 static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl =
@@ -1930,6 +2510,10 @@ static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
  * The ports don't need to be compared as they are intrinsically
  * already matched by the port pointers supplied.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2021| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+ */
 static bool
 nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 {
@@ -1947,9 +2531,25 @@ nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 	return found;
 }
 
+/*
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1039| <<nvmf_create_ctrl>> ctrl = ops->create_ctrl(dev, opts);
+ *
+ * struct nvmf_transport_ops nvme_rdma_transport.create_ctrl = nvme_ctrl_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
+	/*
+	 * 包含struct nvme_ctrl和struct blk_mq_tag_set (不是指针)
+	 */
 	struct nvme_rdma_ctrl *ctrl;
 	int ret;
 	bool changed;
@@ -1970,6 +2570,9 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		opts->mask |= NVMF_OPT_TRSVCID;
 	}
 
+	/*
+	 * convert an IPv4/IPv6 and port to socket address
+	 */
 	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
 			opts->traddr, opts->trsvcid, &ctrl->addr);
 	if (ret) {
@@ -1988,14 +2591,60 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		}
 	}
 
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts))
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts))
+	 *
+	 * nvme_rdma_existing_controller()"
+	 * Fails a connection request if it matches an existing controller
+	 * (association) with the same tuple:
+	 * <Host NQN, Host ID, local address, remote address, remote port, SUBSYS NQN>
+	 */
 	if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
 		ret = -EALREADY;
 		goto out_free_ctrl;
 	}
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	INIT_DELAYED_WORK(&ctrl->reconnect_work,
 			nvme_rdma_reconnect_ctrl_work);
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
 
 	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
@@ -2009,6 +2658,11 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	if (!ctrl->queues)
 		goto out_free_ctrl;
 
+	/*
+	 * 下面的命令到这里的时候queue_count是5
+	 * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+	 */
+
 	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
 				0 /* no quirks, we're perfect! */);
 	if (ret)
@@ -2024,9 +2678,16 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs\n",
 		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
 
+	/* 不知道在哪里put */
 	nvme_get_ctrl(&ctrl->ctrl);
 
 	mutex_lock(&nvme_rdma_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_rdma_ctrl_list:
+	 *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+	 *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 */
 	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
 	mutex_unlock(&nvme_rdma_ctrl_mutex);
 
@@ -2055,6 +2716,20 @@ static struct nvmf_transport_ops nvme_rdma_transport = {
 	.create_ctrl	= nvme_rdma_create_ctrl,
 };
 
+/*
+ * 在nvme_rdma_init_module()被用于ib_register_client - Register an IB client
+ *
+ * Upper level users of the IB drivers can use ib_register_client() to
+ * register callbacks for IB device addition and removal.  When an IB
+ * device is added, each registered client's add method will be called
+ * (in the order the clients were registered), and when a device is
+ * removed, each client's remove method will be called (in the reverse
+ * order that clients were registered).  In addition, when
+ * ib_register_client() is called, the client will receive an add
+ * callback for all devices already registered.
+ *
+ * struct ib_client nvme_rdma_ib_client.remove = nvme_rdma_remove_one()
+ */
 static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
 {
 	struct nvme_rdma_ctrl *ctrl;
@@ -2094,6 +2769,18 @@ static int __init nvme_rdma_init_module(void)
 {
 	int ret;
 
+	/*
+	 * ib_register_client - Register an IB client
+	 *
+	 * Upper level users of the IB drivers can use ib_register_client() to
+	 * register callbacks for IB device addition and removal.  When an IB
+	 * device is added, each registered client's add method will be called
+	 * (in the order the clients were registered), and when a device is
+	 * removed, each client's remove method will be called (in the reverse
+	 * order that clients were registered).  In addition, when
+	 * ib_register_client() is called, the client will receive an add
+	 * callback for all devices already registered.
+	 */
 	ret = ib_register_client(&nvme_rdma_ib_client);
 	if (ret)
 		return ret;
diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 3a67e24..4165919 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -260,6 +260,13 @@ void nvmet_port_send_ana_event(struct nvmet_port *port)
 	up_read(&nvmet_config_sem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2575| <<nvmet_fc_init_module>> return nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);
+ *   - drivers/nvme/target/loop.c|687| <<nvme_loop_init_module>> ret = nvmet_register_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/rdma.c|1667| <<nvmet_rdma_init>> ret = nvmet_register_transport(&nvmet_rdma_ops);
+ *   - drivers/nvme/target/tcp.c|1719| <<nvmet_tcp_init>> ret = nvmet_register_transport(&nvmet_tcp_ops);
+ */
 int nvmet_register_transport(const struct nvmet_fabrics_ops *ops)
 {
 	int ret = 0;
@@ -822,6 +829,10 @@ static inline u16 nvmet_io_cmd_check_access(struct nvmet_req *req)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|905| <<nvmet_req_init>> status = nvmet_parse_io_cmd(req);
+ */
 static u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
@@ -853,6 +864,14 @@ static u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 		return nvmet_bdev_parse_io_cmd(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2196| <<nvmet_fc_handle_fcp_rqst>> ret = nvmet_req_init(&fod->req,
+ *   - drivers/nvme/target/loop.c|159| <<nvme_loop_queue_rq>> if (!nvmet_req_init(&iod->req, &queue->nvme_cq,
+ *   - drivers/nvme/target/loop.c|194| <<nvme_loop_submit_async_event>> if (!nvmet_req_init(&iod->req, &queue->nvme_cq, &queue->nvme_sq,
+ *   - drivers/nvme/target/rdma.c|767| <<nvmet_rdma_handle_command>> if (!nvmet_req_init(&cmd->req, &queue->nvme_cq,
+ *   - drivers/nvme/target/tcp.c|902| <<nvmet_tcp_done_recv_pdu>> if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
+ */
 bool nvmet_req_init(struct nvmet_req *req, struct nvmet_cq *cq,
 		struct nvmet_sq *sq, const struct nvmet_fabrics_ops *ops)
 {
@@ -930,6 +949,17 @@ void nvmet_req_uninit(struct nvmet_req *req)
 }
 EXPORT_SYMBOL_GPL(nvmet_req_uninit);
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2018| <<nvmet_fc_fod_op_done>> nvmet_req_execute(&fod->req);
+ *   - drivers/nvme/target/fc.c|2234| <<nvmet_fc_handle_fcp_rqst>> nvmet_req_execute(&fod->req);
+ *   - drivers/nvme/target/loop.c|133| <<nvme_loop_execute_work>> nvmet_req_execute(&iod->req);
+ *   - drivers/nvme/target/rdma.c|606| <<nvmet_rdma_read_data_done>> nvmet_req_execute(&rsp->req);
+ *   - drivers/nvme/target/rdma.c|749| <<nvmet_rdma_execute_command>> nvmet_req_execute(&rsp->req);
+ *   - drivers/nvme/target/tcp.c|935| <<nvmet_tcp_done_recv_pdu>> nvmet_req_execute(&queue->cmd->req);
+ *   - drivers/nvme/target/tcp.c|1055| <<nvmet_tcp_try_recv_data>> nvmet_req_execute(&cmd->req);
+ *   - drivers/nvme/target/tcp.c|1095| <<nvmet_tcp_try_recv_ddgst>> nvmet_req_execute(&cmd->req);
+ */
 void nvmet_req_execute(struct nvmet_req *req)
 {
 	if (unlikely(req->data_len != req->transfer_len)) {
@@ -937,6 +967,43 @@ void nvmet_req_execute(struct nvmet_req *req)
 		nvmet_req_complete(req, NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR);
 	} else
 		req->execute(req);
+
+	/*
+	 * execute的设置:
+	 *   - drivers/nvme/target/admin-cmd.c|826| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_error;
+	 *   - drivers/nvme/target/admin-cmd.c|829| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_smart;
+	 *   - drivers/nvme/target/admin-cmd.c|838| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_noop;
+	 *   - drivers/nvme/target/admin-cmd.c|841| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_changed_ns;
+	 *   - drivers/nvme/target/admin-cmd.c|844| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_cmd_effects_ns;
+	 *   - drivers/nvme/target/admin-cmd.c|847| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_ana;
+	 *   - drivers/nvme/target/admin-cmd.c|855| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_ns;
+	 *   - drivers/nvme/target/admin-cmd.c|858| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_ctrl;
+	 *   - drivers/nvme/target/admin-cmd.c|861| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_nslist;
+	 *   - drivers/nvme/target/admin-cmd.c|864| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_desclist;
+	 *   - drivers/nvme/target/admin-cmd.c|869| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_abort;
+	 *   - drivers/nvme/target/admin-cmd.c|873| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_set_features;
+	 *   - drivers/nvme/target/admin-cmd.c|877| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_features;
+	 *   - drivers/nvme/target/admin-cmd.c|881| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_async_event;
+	 *   - drivers/nvme/target/admin-cmd.c|885| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_keep_alive;
+	 *   - drivers/nvme/target/discovery.c|330| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_disc_set_features;
+	 *   - drivers/nvme/target/discovery.c|334| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_disc_get_features;
+	 *   - drivers/nvme/target/discovery.c|338| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_async_event;
+	 *   - drivers/nvme/target/discovery.c|342| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_keep_alive;
+	 *   - drivers/nvme/target/discovery.c|350| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_get_disc_log_page;
+	 *   - drivers/nvme/target/discovery.c|363| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_identify_disc_ctrl;
+	 *   - drivers/nvme/target/fabrics-cmd.c|86| <<nvmet_parse_fabrics_cmd>> req->execute = nvmet_execute_prop_set;
+	 *   - drivers/nvme/target/fabrics-cmd.c|90| <<nvmet_parse_fabrics_cmd>> req->execute = nvmet_execute_prop_get;
+	 *   - drivers/nvme/target/fabrics-cmd.c|286| <<nvmet_parse_connect_cmd>> req->execute = nvmet_execute_admin_connect;
+	 *   - drivers/nvme/target/fabrics-cmd.c|288| <<nvmet_parse_connect_cmd>> req->execute = nvmet_execute_io_connect;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|321| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_rw;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|325| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_flush;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|329| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_dsm;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|334| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_write_zeroes;
+	 *   - drivers/nvme/target/io-cmd-file.c|373| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_rw;
+	 *   - drivers/nvme/target/io-cmd-file.c|377| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_flush;
+	 *   - drivers/nvme/target/io-cmd-file.c|381| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_dsm;
+	 *   - drivers/nvme/target/io-cmd-file.c|386| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_write_zeroes;
+	 */
 }
 EXPORT_SYMBOL_GPL(nvmet_req_execute);
 
diff --git a/drivers/nvme/target/io-cmd-file.c b/drivers/nvme/target/io-cmd-file.c
index 05453f5..ebbca3f 100644
--- a/drivers/nvme/target/io-cmd-file.c
+++ b/drivers/nvme/target/io-cmd-file.c
@@ -87,6 +87,11 @@ static void nvmet_file_init_bvec(struct bio_vec *bv, struct scatterlist *sg)
 	bv->bv_len = sg->length;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|165| <<nvmet_file_execute_io>> ret = nvmet_file_submit_bvec(req, pos, bv_cnt, len, 0);
+ *   - drivers/nvme/target/io-cmd-file.c|193| <<nvmet_file_execute_io>> ret = nvmet_file_submit_bvec(req, pos, bv_cnt, total_len, ki_flags);
+ */
 static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 		unsigned long nr_segs, size_t count, int ki_flags)
 {
@@ -114,6 +119,11 @@ static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 	return call_iter(iocb, &iter);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|191| <<nvmet_file_execute_io>> req->f.iocb.ki_complete = nvmet_file_io_done;
+ *   - drivers/nvme/target/io-cmd-file.c|214| <<nvmet_file_execute_io>> nvmet_file_io_done(&req->f.iocb, ret, 0);
+ */
 static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 {
 	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
@@ -131,6 +141,12 @@ static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 	nvmet_req_complete(req, status);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|222| <<nvmet_file_buffered_io_work>> nvmet_file_execute_io(req, 0);
+ *   - drivers/nvme/target/io-cmd-file.c|270| <<nvmet_file_execute_rw>> nvmet_file_execute_io(req, IOCB_NOWAIT))
+ *   - drivers/nvme/target/io-cmd-file.c|274| <<nvmet_file_execute_rw>> nvmet_file_execute_io(req, 0);
+ */
 static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 {
 	ssize_t nr_bvec = req->sg_cnt;
@@ -250,6 +266,21 @@ static void nvmet_file_execute_rw(struct nvmet_req *req)
 	} else
 		req->f.mpool_alloc = false;
 
+	/*
+	 * 在以下设置和使用buffered_io:
+	 *   - drivers/nvme/target/configfs.c|541| <<nvmet_ns_buffered_io_store>> ns->buffered_io = val;
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/configfs.c|546| <<global>> CONFIGFS_ATTR(nvmet_ns_, buffered_io);
+	 *   - drivers/nvme/target/configfs.c|522| <<nvmet_ns_buffered_io_show>> return sprintf(page, "%d\n", to_nvmet_ns(item)->buffered_io);
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false; 
+	 *   - drivers/nvme/target/io-cmd-file.c|19| <<nvmet_file_ns_disable>> if (ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|36| <<nvmet_file_ns_enable>> if (!ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|253| <<nvmet_file_execute_rw>> if (req->ns->buffered_io) {
+	 *
+	 * struct nvmet_req *req
+	 *  -> struct nvmet_ns *ns;
+	 *      -> bool buffered_io;
+	 */
 	if (req->ns->buffered_io) {
 		if (likely(!req->f.mpool_alloc) &&
 				nvmet_file_execute_io(req, IOCB_NOWAIT))
@@ -363,6 +394,10 @@ static void nvmet_file_execute_write_zeroes(struct nvmet_req *req)
 	schedule_work(&req->f.work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|858| <<nvmet_parse_io_cmd>> return nvmet_file_parse_io_cmd(req);
+ */
 u16 nvmet_file_parse_io_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index 0940c50..395e2a5 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -56,9 +56,20 @@ struct nvme_loop_queue {
 	unsigned long		flags;
 };
 
+/*
+ * 在以下添加和使用nvme_loop_ports:
+ *   - drivers/nvme/target/loop.c|667| <<nvme_loop_add_port>> list_add_tail(&port->entry, &nvme_loop_ports);
+ *   - drivers/nvme/target/loop.c|572| <<nvme_loop_find_port>> list_for_each_entry(p, &nvme_loop_ports, entry) {
+ */
 static LIST_HEAD(nvme_loop_ports);
 static DEFINE_MUTEX(nvme_loop_ports_mutex);
 
+/*
+ * 在以下添加删除和使用nvme_loop_ctrl_list:
+ *   - drivers/nvme/target/loop.c|453| <<nvme_loop_delete_ctrl>> list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+ *   - drivers/nvme/target/loop.c|644| <<nvme_loop_create_ctrl>> list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
+ *   - drivers/nvme/target/loop.c|726| <<nvme_loop_cleanup_module>> list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
+ */
 static LIST_HEAD(nvme_loop_ctrl_list);
 static DEFINE_MUTEX(nvme_loop_ctrl_mutex);
 
@@ -121,6 +132,15 @@ static void nvme_loop_queue_response(struct nvmet_req *req)
 	}
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/target/loop.c|198| <<nvme_loop_init_iod>> INIT_WORK(&iod->work, nvme_loop_execute_work);
+ *
+ * 在nvme_loop_queue_rq()设置了如下作为输入:
+ *   - iod->req.sg = iod->sg_table.sgl;
+ *   - iod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);
+ *   - iod->req.transfer_len = blk_rq_payload_bytes(req);
+ */
 static void nvme_loop_execute_work(struct work_struct *work)
 {
 	struct nvme_loop_iod *iod =
@@ -142,6 +162,9 @@ static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (!nvmf_check_ready(&queue->ctrl->ctrl, req, queue_ready))
 		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, req);
 
+	/*
+	 * iod->cmd是struct nvme_command
+	 */
 	ret = nvme_setup_cmd(ns, req, &iod->cmd);
 	if (ret)
 		return ret;
@@ -154,17 +177,29 @@ static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		return BLK_STS_OK;
 
 	if (blk_rq_nr_phys_segments(req)) {
+		/*
+		 * iod->first_sgl因为是struct scatterlist first_sgl[];
+		 * 所以分配的时候就指向iod后面的sg们了
+		 */
 		iod->sg_table.sgl = iod->first_sgl;
 		if (sg_alloc_table_chained(&iod->sg_table,
 				blk_rq_nr_phys_segments(req),
 				iod->sg_table.sgl, SG_CHUNK_SIZE))
 			return BLK_STS_RESOURCE;
 
+		/*
+		 * struct nvme_loop_iod
+		 *  -> struct nvmet_req req
+		 */
 		iod->req.sg = iod->sg_table.sgl;
 		iod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);
 		iod->req.transfer_len = blk_rq_payload_bytes(req);
 	}
 
+	/*
+	 * 设置的地方:
+	 *   - drivers/nvme/target/loop.c|198| <<nvme_loop_init_iod>> INIT_WORK(&iod->work, nvme_loop_execute_work);
+	 */
 	schedule_work(&iod->work);
 	return BLK_STS_OK;
 }
@@ -189,6 +224,11 @@ static void nvme_loop_submit_async_event(struct nvme_ctrl *arg)
 	schedule_work(&iod->work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|220| <<nvme_loop_init_request>> return nvme_loop_init_iod(ctrl, blk_mq_rq_to_pdu(req),
+ *   - drivers/nvme/target/loop.c|624| <<nvme_loop_create_ctrl>> nvme_loop_init_iod(ctrl, &ctrl->async_event_iod, 0);
+ */
 static int nvme_loop_init_iod(struct nvme_loop_ctrl *ctrl,
 		struct nvme_loop_iod *iod, unsigned int queue_idx)
 {
@@ -544,6 +584,10 @@ static int nvme_loop_create_io_queues(struct nvme_loop_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|608| <<nvme_loop_create_ctrl>> ctrl->port = nvme_loop_find_port(&ctrl->ctrl);
+ */
 static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 {
 	struct nvmet_port *p, *found = NULL;
@@ -561,6 +605,9 @@ static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 	return found;
 }
 
+/*
+ * struct nvmf_transport_ops nvme_loop_transport.create_ctrl = nvme_loop_crete_ctrl()
+ */
 static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
@@ -585,6 +632,7 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 
 	ctrl->ctrl.sqsize = opts->queue_size - 1;
 	ctrl->ctrl.kato = opts->kato;
+	/* 返回的是struct nvmet_port */
 	ctrl->port = nvme_loop_find_port(&ctrl->ctrl);
 
 	ctrl->queues = kcalloc(opts->nr_io_queues + 1, sizeof(*ctrl->queues),
@@ -610,6 +658,10 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 			goto out_remove_admin_queue;
 	}
 
+	/*
+	 * struct nvme_loop_ctrl
+	 *  -> struct nvme_loop_iod async_event_iod
+	 */
 	nvme_loop_init_iod(ctrl, &ctrl->async_event_iod, 0);
 
 	dev_info(ctrl->ctrl.device,
@@ -641,6 +693,10 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|328| <<nvmet_enable_port>> ret = ops->add_port(port);
+ */
 static int nvme_loop_add_port(struct nvmet_port *port)
 {
 	mutex_lock(&nvme_loop_ports_mutex);
diff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h
index c51f8dd..f1c4931 100644
--- a/drivers/nvme/target/nvmet.h
+++ b/drivers/nvme/target/nvmet.h
@@ -62,6 +62,17 @@ struct nvmet_ns {
 	uuid_t			uuid;
 	u32			anagrpid;
 
+	/*
+	 * 在以下设置和使用buffered_io:
+	 *   - drivers/nvme/target/configfs.c|541| <<nvmet_ns_buffered_io_store>> ns->buffered_io = val;
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/configfs.c|546| <<global>> CONFIGFS_ATTR(nvmet_ns_, buffered_io);
+	 *   - drivers/nvme/target/configfs.c|522| <<nvmet_ns_buffered_io_show>> return sprintf(page, "%d\n", to_nvmet_ns(item)->buffered_io);
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/io-cmd-file.c|19| <<nvmet_file_ns_disable>> if (ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|36| <<nvmet_file_ns_enable>> if (!ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|253| <<nvmet_file_execute_rw>> if (req->ns->buffered_io) {
+	 */
 	bool			buffered_io;
 	bool			enabled;
 	struct nvmet_subsys	*subsys;
diff --git a/drivers/scsi/fcoe/fcoe.c b/drivers/scsi/fcoe/fcoe.c
index 00dd47b..d7442c1 100644
--- a/drivers/scsi/fcoe/fcoe.c
+++ b/drivers/scsi/fcoe/fcoe.c
@@ -1120,6 +1120,11 @@ static int fcoe_ddp_done(struct fc_lport *lport, u16 xid)
  *
  * Returns: The allocated fc_lport or an error pointer
  */
+/*
+ * called by:
+ *   - drivers/scsi/fcoe/fcoe.c|2221| <<_fcoe_create>> lport = fcoe_if_create(fcoe, &ctlr_dev->dev, 0);
+ *   - drivers/scsi/fcoe/fcoe.c|2675| <<fcoe_vport_create>> vn_port = fcoe_if_create(fcoe, &vport->dev, 1);
+ */
 static struct fc_lport *fcoe_if_create(struct fcoe_interface *fcoe,
 				       struct device *parent, int npiv)
 {
@@ -2192,6 +2197,11 @@ enum fcoe_create_link_state {
  * consolidation of code can be done when that interface is
  * removed.
  */
+/*
+ * called by:
+ *   - drivers/scsi/fcoe/fcoe.c|2288| <<fcoe_create>> return _fcoe_create(netdev, fip_mode, FCOE_CREATE_LINK_UP);
+ *   - drivers/scsi/fcoe/fcoe.c|2304| <<fcoe_ctlr_alloc>> return _fcoe_create(netdev, FIP_MODE_FABRIC,
+ */
 static int _fcoe_create(struct net_device *netdev, enum fip_mode fip_mode,
 			enum fcoe_create_link_state link_state)
 {
diff --git a/drivers/scsi/fcoe/fcoe_transport.c b/drivers/scsi/fcoe/fcoe_transport.c
index ba4603d..a1bf4eb 100644
--- a/drivers/scsi/fcoe/fcoe_transport.c
+++ b/drivers/scsi/fcoe/fcoe_transport.c
@@ -523,6 +523,11 @@ static struct fcoe_transport *fcoe_transport_lookup(struct net_device *netdev)
  *
  * Returns : 0 for success
  */
+/*
+ * called by:
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|2699| <<bnx2fc_mod_init>> rc = fcoe_transport_attach(&bnx2fc_transport);
+ *   - drivers/scsi/fcoe/fcoe.c|2478| <<fcoe_init>> rc = fcoe_transport_attach(&fcoe_sw_transport);
+ */
 int fcoe_transport_attach(struct fcoe_transport *ft)
 {
 	int rc = 0;
diff --git a/drivers/scsi/scsi_transport_fc.c b/drivers/scsi/scsi_transport_fc.c
index 2732fa6..0227fd0 100644
--- a/drivers/scsi/scsi_transport_fc.c
+++ b/drivers/scsi/scsi_transport_fc.c
@@ -2149,6 +2149,28 @@ fc_user_scan(struct Scsi_Host *shost, uint channel, uint id, u64 lun)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/message/fusion/mptfc.c|1466| <<mptfc_init>> fc_attach_transport(&mptfc_transport_functions);
+ *   - drivers/s390/scsi/zfcp_aux.c|141| <<zfcp_module_init>> fc_attach_transport(&zfcp_transport_functions);
+ *   - drivers/scsi/bfa/bfad_im.c|842| <<bfad_im_module_init>> fc_attach_transport(&bfad_im_fc_function_template);
+ *   - drivers/scsi/bfa/bfad_im.c|847| <<bfad_im_module_init>> fc_attach_transport(&bfad_im_vport_fc_function_template);
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|1283| <<bnx2fc_attach_transport>> fc_attach_transport(&bnx2fc_transport_function);
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|1291| <<bnx2fc_attach_transport>> fc_attach_transport(&bnx2fc_vport_xport_function);
+ *   - drivers/scsi/csiostor/csio_init.c|1213| <<csio_init>> csio_fcoe_transport = fc_attach_transport(&csio_fc_transport_funcs);
+ *   - drivers/scsi/csiostor/csio_init.c|1218| <<csio_init>> fc_attach_transport(&csio_fc_transport_vport_funcs);
+ *   - drivers/scsi/fcoe/fcoe.c|1252| <<fcoe_if_init>> fc_attach_transport(&fcoe_nport_fc_functions);
+ *   - drivers/scsi/fcoe/fcoe.c|1254| <<fcoe_if_init>> fc_attach_transport(&fcoe_vport_fc_functions);
+ *   - drivers/scsi/fnic/fnic_main.c|1111| <<fnic_init_module>> fnic_fc_transport = fc_attach_transport(&fnic_fc_functions);
+ *   - drivers/scsi/ibmvscsi/ibmvfc.c|4986| <<ibmvfc_module_init>> ibmvfc_transport_template = fc_attach_transport(&ibmvfc_transport_functions);
+ *   - drivers/scsi/lpfc/lpfc_init.c|13495| <<lpfc_init>> fc_attach_transport(&lpfc_transport_functions);
+ *   - drivers/scsi/lpfc/lpfc_init.c|13499| <<lpfc_init>> fc_attach_transport(&lpfc_vport_transport_functions);
+ *   - drivers/scsi/qedf/qedf_main.c|3786| <<qedf_init>> fc_attach_transport(&qedf_fc_transport_fn);
+ *   - drivers/scsi/qedf/qedf_main.c|3793| <<qedf_init>> fc_attach_transport(&qedf_fc_vport_transport_fn);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7285| <<qla2x00_module_init>> fc_attach_transport(&qla2xxx_transport_functions);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7300| <<qla2x00_module_init>> fc_attach_transport(&qla2xxx_transport_vport_functions);
+ *   - drivers/scsi/storvsc_drv.c|1973| <<storvsc_drv_init>> fc_transport_template = fc_attach_transport(&fc_transport_functions);
+ */
 struct scsi_transport_template *
 fc_attach_transport(struct fc_function_template *ft)
 {
diff --git a/drivers/scsi/scsi_transport_iscsi.c b/drivers/scsi/scsi_transport_iscsi.c
index 417b868..5ad4da9 100644
--- a/drivers/scsi/scsi_transport_iscsi.c
+++ b/drivers/scsi/scsi_transport_iscsi.c
@@ -4407,6 +4407,16 @@ static int iscsi_host_match(struct attribute_container *cont,
         return &priv->t.host_attrs.ac == cont;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iscsi_iser.c|1050| <<iser_init>> iscsi_iser_scsi_transport = iscsi_register_transport(
+ *   - drivers/scsi/be2iscsi/be_main.c|5849| <<beiscsi_module_init>> iscsi_register_transport(&beiscsi_iscsi_transport);
+ *   - drivers/scsi/bnx2i/bnx2i_init.c|480| <<bnx2i_mod_init>> iscsi_register_transport(&bnx2i_iscsi_transport);
+ *   - drivers/scsi/cxgbi/libcxgbi.c|2691| <<cxgbi_iscsi_init>> *stt = iscsi_register_transport(itp);
+ *   - drivers/scsi/iscsi_tcp.c|1024| <<iscsi_sw_tcp_init>> iscsi_sw_tcp_scsi_transport = iscsi_register_transport(
+ *   - drivers/scsi/qedi/qedi_main.c|2717| <<qedi_init>> qedi_scsi_transport = iscsi_register_transport(&qedi_iscsi_transport);
+ *   - drivers/scsi/qla4xxx/ql4_os.c|9900| <<qla4xxx_module_init>> iscsi_register_transport(&qla4xxx_iscsi_transport);
+ */
 struct scsi_transport_template *
 iscsi_register_transport(struct iscsi_transport *tt)
 {
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 677cb36..a8dad39 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -198,6 +198,10 @@ static void blkdev_bio_end_io_simple(struct bio *bio)
 	blk_wake_io_task(waiter);
 }
 
+/*
+ * called by:
+ *   - fs/block_dev.c|481| <<blkdev_direct_IO>> return __blkdev_direct_IO_simple(iocb, iter, nr_pages);
+ */
 static ssize_t
 __blkdev_direct_IO_simple(struct kiocb *iocb, struct iov_iter *iter,
 		int nr_pages)
@@ -287,6 +291,9 @@ struct blkdev_dio {
 
 static struct bio_set blkdev_dio_pool;
 
+/*
+ * struct file_operations def_blk_fops.iopoll = blkdev_iopoll()
+ */
 static int blkdev_iopoll(struct kiocb *kiocb, bool wait)
 {
 	struct block_device *bdev = I_BDEV(kiocb->ki_filp->f_mapping->host);
@@ -334,6 +341,10 @@ static void blkdev_bio_end_io(struct bio *bio)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/block_dev.c|476| <<blkdev_direct_IO>> return __blkdev_direct_IO(iocb, iter, min(nr_pages, BIO_MAX_PAGES));
+ */
 static ssize_t
 __blkdev_direct_IO(struct kiocb *iocb, struct iov_iter *iter, int nr_pages)
 {
@@ -462,6 +473,9 @@ __blkdev_direct_IO(struct kiocb *iocb, struct iov_iter *iter, int nr_pages)
 	return ret;
 }
 
+/*
+ * struct address_space_operations def_blk_aops.direct_IO = blkdev_direct_IO()
+ */
 static ssize_t
 blkdev_direct_IO(struct kiocb *iocb, struct iov_iter *iter)
 {
diff --git a/fs/direct-io.c b/fs/direct-io.c
index ae19678..04f3a71 100644
--- a/fs/direct-io.c
+++ b/fs/direct-io.c
@@ -501,6 +501,10 @@ static inline void dio_cleanup(struct dio *dio, struct dio_submit *sdio)
  * all bios have been issued so that dio->refcount can only decrease.  This
  * requires that that the caller hold a reference on the dio.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|571| <<dio_await_completion>> bio = dio_await_one(dio);
+ */
 static struct bio *dio_await_one(struct dio *dio)
 {
 	unsigned long flags;
@@ -564,6 +568,10 @@ static blk_status_t dio_bio_complete(struct dio *dio, struct bio *bio)
  * errors are propagated through dio->io_error and should be propagated via
  * dio_complete().
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1384| <<do_blockdev_direct_IO>> dio_await_completion(dio);
+ */
 static void dio_await_completion(struct dio *dio)
 {
 	struct bio *bio;
@@ -1161,6 +1169,10 @@ static inline int drop_refcount(struct dio *dio)
  * individual fields and will generate much worse code. This is important
  * for the whole file.
  */
+/*
+ * called by:
+ *   - fs/direct-io.c|1413| <<__blockdev_direct_IO>> return do_blockdev_direct_IO(iocb, inode, bdev, iter, get_block,
+ */
 static inline ssize_t
 do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 		      struct block_device *bdev, struct iov_iter *iter,
@@ -1392,6 +1404,15 @@ do_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 	return retval;
 }
 
+/*
+ * called by:
+ *   - fs/btrfs/inode.c|8657| <<btrfs_direct_IO>> ret = __blockdev_direct_IO(iocb, inode,
+ *   - fs/ext4/inode.c|3788| <<ext4_direct_IO_write>> ret = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ *   - fs/ext4/inode.c|3881| <<ext4_direct_IO_read>> ret = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/f2fs/data.c|2781| <<f2fs_direct_IO>> err = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - fs/ocfs2/aops.c|2443| <<ocfs2_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
+ *   - include/linux/fs.h|3115| <<blockdev_direct_IO>> return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
+ */
 ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     struct block_device *bdev, struct iov_iter *iter,
 			     get_block_t get_block,
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index 10517ce..43f23ec 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -394,6 +394,13 @@ iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
  * may be pure data writes. In that case, we still need to do a full data sync
  * completion.
  */
+/*
+ * called by:
+ *   - fs/gfs2/file.c|735| <<gfs2_file_direct_read>> ret = iomap_dio_rw(iocb, to, &gfs2_iomap_ops, NULL);
+ *   - fs/gfs2/file.c|770| <<gfs2_file_direct_write>> ret = iomap_dio_rw(iocb, from, &gfs2_iomap_ops, NULL);
+ *   - fs/xfs/xfs_file.c|190| <<xfs_file_dio_aio_read>> ret = iomap_dio_rw(iocb, to, &xfs_iomap_ops, NULL);
+ *   - fs/xfs/xfs_file.c|543| <<xfs_file_dio_aio_write>> ret = iomap_dio_rw(iocb, from, &xfs_iomap_ops, xfs_dio_write_end_io);
+ */
 ssize_t
 iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, iomap_dio_end_io_t end_io)
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 3cdb84c..442b6b0 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -23,30 +23,99 @@
 
 #define BIO_MAX_PAGES		256
 
+/*
+ * 设置bi_ioprio的地方:
+ *   - block/bio.c|587| <<__bio_clone_fast>> bio->bi_ioprio = bio_src->bi_ioprio;
+ *   - block/bounce.c|251| <<bounce_clone_bio>> bio->bi_ioprio = bio_src->bi_ioprio;
+ *   - fs/block_dev.c|233| <<__blkdev_direct_IO_simple>> bio.bi_ioprio = iocb->ki_ioprio;
+ *   - fs/block_dev.c|384| <<__blkdev_direct_IO>> bio->bi_ioprio = iocb->ki_ioprio;
+ *   - fs/iomap/direct-io.c|271| <<iomap_dio_bio_actor>> bio->bi_ioprio = dio->iocb->ki_ioprio;
+ *   - include/linux/bio.h|27| <<bio_set_prio>> #define bio_set_prio(bio, prio) ((bio)->bi_ioprio = prio)
+ */
 #define bio_prio(bio)			(bio)->bi_ioprio
+/*
+ * called by:
+ *   - drivers/md/bcache/movinggc.c|85| <<moving_init>> bio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));
+ *   - drivers/md/bcache/writeback.c|256| <<dirty_init>> bio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));
+ */
 #define bio_set_prio(bio, prio)		((bio)->bi_ioprio = prio)
 
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bio_iter_iovec(bio, iter)				\
 	bvec_iter_bvec((bio)->bi_io_vec, (iter))
 
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bio_iter_page(bio, iter)				\
 	bvec_iter_page((bio)->bi_io_vec, (iter))
+/*
+ * 二个进行比较:
+ * 1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * 2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ */
 #define bio_iter_len(bio, iter)					\
 	bvec_iter_len((bio)->bi_io_vec, (iter))
+/*
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
 #define bio_iter_offset(bio, iter)				\
 	bvec_iter_offset((bio)->bi_io_vec, (iter))
 
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bio_page(bio)		bio_iter_page((bio), (bio)->bi_iter)
+/*
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
 #define bio_offset(bio)		bio_iter_offset((bio), (bio)->bi_iter)
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bio_iovec(bio)		bio_iter_iovec((bio), (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|315| <<bio_get_last_bvec>> if (unlikely(!bio_multiple_segments(bio))) {
+ *
+ * 判断是否有多个segment (bvec???)
+ */
 #define bio_multiple_segments(bio)				\
 	((bio)->bi_iter.bi_size != bio_iovec(bio).bv_len)
 
+/*
+ * bio的iter所表示的sector的数量
+ */
 #define bvec_iter_sectors(iter)	((iter).bi_size >> 9)
+/*
+ * bio的iter表示的最后一个vector
+ */
 #define bvec_iter_end_sector(iter) ((iter).bi_sector + bvec_iter_sectors((iter)))
 
+/*
+ * bio的iter所表示的sector的数量
+ */
 #define bio_sectors(bio)	bvec_iter_sectors((bio)->bi_iter)
+/*
+ * bio的iter表示的最后一个vector
+ */
 #define bio_end_sector(bio)	bvec_iter_end_sector((bio)->bi_iter)
 
 /*
@@ -70,6 +139,10 @@ static inline bool bio_has_data(struct bio *bio)
 	return false;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|204| <<bio_advance_iter>> if (bio_no_advance_iter(bio))
+ */
 static inline bool bio_no_advance_iter(struct bio *bio)
 {
 	return bio_op(bio) == REQ_OP_DISCARD ||
@@ -78,6 +151,10 @@ static inline bool bio_no_advance_iter(struct bio *bio)
 	       bio_op(bio) == REQ_OP_WRITE_ZEROES;
 }
 
+/*
+ * 测试bio->bi_opf是否设置了REQ_NOMERGE_FLAGS
+ * 定义是(REQ_NOMERGE | REQ_PREFLUSH | REQ_FUA)
+ */
 static inline bool bio_mergeable(struct bio *bio)
 {
 	if (bio->bi_opf & REQ_NOMERGE_FLAGS)
@@ -110,6 +187,9 @@ static inline void *bio_data(struct bio *bio)
  * Return true if @bio is full and one segment with @len bytes can't be
  * added to the bio, otherwise return false
  */
+/*
+ * 不能再添加bio_vec了
+ */
 static inline bool bio_full(struct bio *bio, unsigned len)
 {
 	if (bio->bi_vcnt >= bio->bi_max_vecs)
@@ -121,6 +201,10 @@ static inline bool bio_full(struct bio *bio, unsigned len)
 	return false;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|153| <<bio_for_each_segment_all>> for (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )
+ */
 static inline bool bio_next_segment(const struct bio *bio,
 				    struct bvec_iter_all *iter)
 {
@@ -150,15 +234,57 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 		/* TODO: It is reasonable to complete bio with error here. */
 }
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|178| <<bio_integrity_process>> __bio_for_each_segment(bv, bio, bviter, *proc_iter) {
+ *   - block/bio.c|529| <<zero_fill_bio_iter>> __bio_for_each_segment(bv, bio, iter, start) {
+ *   - drivers/block/aoe/aoecmd.c|302| <<skb_fillup>> __bio_for_each_segment(bv, bio, iter, iter)
+ *   - drivers/block/aoe/aoecmd.c|1030| <<bvcpy>> __bio_for_each_segment(bv, bio, iter, iter) {
+ *   - drivers/md/dm-integrity.c|1530| <<integrity_metadata>> __bio_for_each_segment(bv, bio, iter, dio->orig_bi_iter) {
+ *   - include/linux/bio.h|259| <<bio_for_each_segment>> __bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
+ *   - include/linux/ceph/messenger.h|125| <<ceph_bio_iter_advance_step>> __bio_for_each_segment(bv, (it)->bio, __cur_iter, __cur_iter) \
+ *  
+ * bio_iter_iovec():
+ *
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ *
+ *
+ * 从start开始, 遍历每一个最大1个page的bvec
+ *
+ * iter和start是struct bvec_iter
+ */
 #define __bio_for_each_segment(bvl, bio, iter, start)			\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
 		((bvl = bio_iter_iovec((bio), (iter))), 1);		\
 	     bio_advance_iter((bio), &(iter), (bvl).bv_len))
 
+/*
+ * 从bio->bi_iter开始, 遍历每一个最大1个page的bvec
+ */
 #define bio_for_each_segment(bvl, bio, iter)				\
 	__bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|279| <<bio_for_each_bvec>> __bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)
+ *
+ * mp_bvec_iter_bvec():
+ * 构造一个bvec:
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * bv_offset : 所以返回的是当前bvec"完成到"的offset
+ *
+ *
+ * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+ */
 #define __bio_for_each_bvec(bvl, bio, iter, start)		\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -166,11 +292,41 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 	     bio_advance_iter((bio), &(iter), (bvl).bv_len))
 
 /* iterate over multi-page bvec */
+/*
+ * called by:
+ *   - block/blk-map.c|35| <<blk_rq_append_bio>> bio_for_each_bvec(bv, *bio, iter)
+ *   - block/blk-merge.c|231| <<blk_bio_segment_split>> bio_for_each_bvec(bv, bio, iter) {
+ *   - block/blk-merge.c|474| <<__blk_bios_map_sg>> bio_for_each_bvec(bvec, bio, iter) {
+ *   - include/linux/blkdev.h|848| <<rq_for_each_bvec>> bio_for_each_bvec(bvl, _iter.bio, _iter.iter)
+ *
+ * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+ */
 #define bio_for_each_bvec(bvl, bio, iter)			\
 	__bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_main.c|1600| <<_drbd_send_bio>> bio_iter_last(bvec, iter)
+ *   - drivers/block/drbd/drbd_main.c|1622| <<_drbd_send_zc_bio>> bio_iter_last(bvec, iter) ? 0 : MSG_MORE);
+ *   - drivers/block/nbd.c|574| <<nbd_send_cmd>> bool is_last = !next && bio_iter_last(bvec, iter);
+ *   - include/linux/blkdev.h|832| <<rq_iter_last>> bio_iter_last(bvec, _iter.iter))
+ *
+ * 如果bvec的长度等于全部bvec剩余的长度, 说明是最后一个bvec
+ */
 #define bio_iter_last(bvec, iter) ((iter).bi_size == (bvec).bv_len)
 
+/*
+ * called by:
+ *   - block/bounce.c|246| <<bounce_clone_bio>> bio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);
+ *   - drivers/md/bcache/debug.c|114| <<bch_data_verify>> check = bio_kmalloc(GFP_NOIO, bio_segments(bio));
+ *   - drivers/md/dm-log-writes.c|702| <<log_writes_map>> alloc_size = struct_size(block, vecs, bio_segments(bio));
+ *   - drivers/nvme/host/tcp.c|228| <<nvme_tcp_init_iter>> nsegs = bio_segments(bio);
+ *   - drivers/target/target_core_pscsi.c|908| <<pscsi_map_sg>> bio_segments(bio), nr_vecs);
+ *   - fs/btrfs/check-integrity.c|2811| <<__btrfsic_submit_bio>> unsigned int segs = bio_segments(bio);
+ *   - fs/btrfs/inode.c|7890| <<dio_read_error>> segs = bio_segments(failed_bio);
+ *
+ * 似乎是计算bio一共多少page (一个bvec可能有多个page)
+ */
 static inline unsigned bio_segments(struct bio *bio)
 {
 	unsigned segs = 0;
@@ -193,6 +349,9 @@ static inline unsigned bio_segments(struct bio *bio)
 		break;
 	}
 
+	/*
+	 * 从bio->bi_iter开始, 遍历每一个最大1个page的bvec
+	 */
 	bio_for_each_segment(bv, bio, iter)
 		segs++;
 
@@ -244,8 +403,28 @@ static inline void bio_clear_flag(struct bio *bio, unsigned int bit)
 	bio->bi_flags &= ~(1U << bit);
 }
 
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 static inline void bio_get_first_bvec(struct bio *bio, struct bio_vec *bv)
 {
+	/*
+	 * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+	 * bv_len    : 二个进行比较:
+	 *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+	 *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+	 * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+	 *
+	 * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+	 * 不像mp_bvec_iter_bvec()是基于多个page的
+	 */
 	*bv = bio_iovec(bio);
 }
 
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3fa1fa5..e04b512 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -20,8 +20,50 @@ struct blk_mq_hw_ctx {
 	} ____cacheline_aligned_in_smp;
 
 	struct delayed_work	run_work;
+	/*
+	 * 设置cpumask的地方:
+	 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 * 测试cpumask的地方:
+	 *   - block/blk-mq-sysfs.c|172| <<blk_mq_hw_sysfs_cpus_show>> for_each_cpu(i, hctx->cpumask) {
+	 *   - block/blk-mq.c|475| <<blk_mq_alloc_request_hctx>> cpu = cpumask_first_and(alloc_data.hctx->cpumask, cpu_online_mask);
+	 *   - block/blk-mq.c|1407| <<__blk_mq_run_hw_queue>> if (!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask) &&
+	 *   - block/blk-mq.c|1411| <<__blk_mq_run_hw_queue>> cpumask_empty(hctx->cpumask) ? "inactive": "active");
+	 *   - block/blk-mq.c|1430| <<blk_mq_first_mapped_cpu>> int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
+	 *   - block/blk-mq.c|1433| <<blk_mq_first_mapped_cpu>> cpu = cpumask_first(hctx->cpumask);
+	 *   - block/blk-mq.c|1478| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1516| <<__blk_mq_delay_run_hw_queue>> if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+	 *   - block/blk-mq.c|2599| <<blk_mq_map_swqueue>> if (cpumask_test_cpu(i, hctx->cpumask))
+	 * 其他使用cpumask的地方:
+	 *   - block/blk-mq-sysfs.c|45| <<blk_mq_hw_sysfs_release>> free_cpumask_var(hctx->cpumask);
+	 *   - block/blk-mq.c|2431| <<blk_mq_alloc_hctx>> if (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))
+	 *   - block/blk-mq.c|2481| <<blk_mq_alloc_hctx>> free_cpumask_var(hctx->cpumask);
+	 *
+	 * 表示这个hctx都map了那些sw cpu
+	 */
 	cpumask_var_t		cpumask;
+	/*
+	 * 在以下设置next_cpu:
+	 *   - block/blk-mq.c|1474| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|1479| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|2626| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 * 在以下使用next_cpu:
+	 *   - block/blk-mq.c|1408| <<__blk_mq_run_hw_queue>> cpu_online(hctx->next_cpu)) {
+	 *   - block/blk-mq.c|1446| <<blk_mq_hctx_next_cpu>> int next_cpu = hctx->next_cpu;
+	 *   - block/blk-mq.c|1453| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1455| <<blk_mq_hctx_next_cpu>> if (next_cpu >= nr_cpu_ids)
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 *   - block/blk-mq.c|1464| <<blk_mq_hctx_next_cpu>> if (!cpu_online(next_cpu)) {
+	 *   - block/blk-mq.c|1480| <<blk_mq_hctx_next_cpu>> return next_cpu;
+	 */
 	int			next_cpu;
+	/*
+	 * 在以下使用next_cpu_batch:
+	 *   - block/blk-mq.c|1469| <<blk_mq_hctx_next_cpu>> if (--hctx->next_cpu_batch <= 0) {
+	 *   - block/blk-mq.c|1475| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|1493| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = 1;
+	 *   - block/blk-mq.c|2645| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	int			next_cpu_batch;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
@@ -61,6 +103,12 @@ struct blk_mq_hw_ctx {
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;
 
+	/*
+	 * 在以下使用poll_considered:
+	 *   - block/blk-mq-debugfs.c|532| <<hctx_io_poll_show>> seq_printf(m, "considered=%lu\n", hctx->poll_considered);
+	 *   - block/blk-mq-debugfs.c|543| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3839| <<blk_poll>> hctx->poll_considered++;
+	 */
 	unsigned long		poll_considered;
 	unsigned long		poll_invoked;
 	unsigned long		poll_success;
@@ -78,7 +126,33 @@ struct blk_mq_hw_ctx {
 
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	unsigned int nr_queues;
+	/*
+	 * 设置queue_offset的地方:
+	 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+	 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 */
 	unsigned int queue_offset;
 };
 
@@ -98,6 +172,35 @@ struct blk_mq_tag_set {
 	 * share maps between types.
 	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 设置nr_maps的地方:
+	 *   - block/blk-mq.c|2899| <<blk_mq_init_sq_queue>> set->nr_maps = 1; 
+	 *   - block/blk-mq.c|3234| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3245| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+	 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2257| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2259| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 *   - drivers/nvme/host/rdma.c|1076| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 *   - drivers/nvme/host/tcp.c|1472| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+	 *
+	 * 使用nr_maps的地方:
+	 *   - block/blk-mq.c|2585| <<blk_mq_init_cpu_queues>> for (j = 0; j < set->nr_maps; j++) {
+	 *   - block/blk-mq.c|2672| <<blk_mq_map_swqueue>> for (j = 0; j < set->nr_maps; j++) {
+	 *   - block/blk-mq.c|3020| <<nr_hw_queues>> if (set->nr_maps == 1)
+	 *   - block/blk-mq.c|3063| <<blk_mq_init_allocated_queue>> if (set->nr_maps > HCTX_TYPE_POLL &&
+	 *   - block/blk-mq.c|3192| <<blk_mq_update_queue_map>> for (i = 0; i < set->nr_maps; i++)
+	 *   - block/blk-mq.c|3197| <<blk_mq_update_queue_map>> BUG_ON(set->nr_maps > 1);
+	 *   - block/blk-mq.c|3233| <<blk_mq_alloc_tag_set>> if (!set->nr_maps)
+	 *   - block/blk-mq.c|3235| <<blk_mq_alloc_tag_set>> else if (set->nr_maps > HCTX_MAX_TYPES)
+	 *   - block/blk-mq.c|3252| <<blk_mq_alloc_tag_set>> if (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)
+	 *   - block/blk-mq.c|3261| <<blk_mq_alloc_tag_set>> for (i = 0; i < set->nr_maps; i++) {
+	 *   - block/blk-mq.c|3284| <<blk_mq_alloc_tag_set>> for (i = 0; i < set->nr_maps; i++) {
+	 *   - block/blk-mq.c|3301| <<blk_mq_free_tag_set>> for (j = 0; j < set->nr_maps; j++) {
+	 *   - block/blk-mq.c|3439| <<__blk_mq_update_nr_hw_queues>> if (set->nr_maps == 1 && nr_hw_queues > nr_cpu_ids)
+	 *   - block/blk-sysfs.c|404| <<queue_poll_store>> if (!q->tag_set || q->tag_set->nr_maps <= HCTX_TYPE_POLL ||
+	 *   - drivers/nvme/host/pci.c|436| <<nvme_pci_map_queues>> for (i = 0, qoff = 0; i < set->nr_maps; i++) {
+	 */
 	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
@@ -174,6 +277,10 @@ struct blk_mq_ops {
 	/*
 	 * Called to poll for completion of a specific tag.
 	 */
+	/*
+	 * called by only:
+	 *   - block/blk-mq.c|3917| <<blk_poll>> ret = q->mq_ops->poll(hctx);
+	 */
 	poll_fn			*poll;
 
 	complete_fn		*complete;
@@ -220,6 +327,17 @@ enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 设置和使用BLK_MQ_F_NO_SCHED的地方:
+	 *   - block/blk-mq.c|2947| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
+	 *   - block/bsg-lib.c|381| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - block/elevator.c|690| <<elv_support_iosched>> if (q->tag_set && (q->tag_set->flags & BLK_MQ_F_NO_SCHED))
+	 *   - drivers/block/null_blk_main.c|1556| <<null_init_tag_set>> set->flags |= BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/fc.c|3105| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/pci.c|1616| <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/rdma.c|1062| <<nvme_rdma_alloc_tagset>> set->flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/target/loop.c|367| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
@@ -230,6 +348,11 @@ enum {
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 
+	/*
+	 * 在以下使用BLK_MQ_CPU_WORK_BATCH = 8:
+	 *   - block/blk-mq.c|1499| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|2674| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
 #define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
@@ -356,6 +479,12 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1933| <<__blk_mq_issue_directly>> new_cookie = request_to_qc_t(hctx, rq);
+ *   - block/blk-mq.c|2136| <<blk_mq_make_request>> cookie = request_to_qc_t(data.hctx, rq);
+ *   - drivers/nvme/host/core.c|774| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ */
 static inline blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx,
 		struct request *rq)
 {
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index feff3fe..9e5feb9 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -149,6 +149,15 @@ struct bio {
 						 * accessors.
 						 */
 	unsigned short		bi_flags;	/* status, etc and bvec pool number */
+	/*
+	 * 设置bi_ioprio的地方:
+	 *   - block/bio.c|587| <<__bio_clone_fast>> bio->bi_ioprio = bio_src->bi_ioprio;
+	 *   - block/bounce.c|251| <<bounce_clone_bio>> bio->bi_ioprio = bio_src->bi_ioprio;
+	 *   - fs/block_dev.c|233| <<__blkdev_direct_IO_simple>> bio.bi_ioprio = iocb->ki_ioprio;
+	 *   - fs/block_dev.c|384| <<__blkdev_direct_IO>> bio->bi_ioprio = iocb->ki_ioprio;
+	 *   - fs/iomap/direct-io.c|271| <<iomap_dio_bio_actor>> bio->bi_ioprio = dio->iocb->ki_ioprio;
+	 *   - include/linux/bio.h|27| <<bio_set_prio>> #define bio_set_prio(bio, prio) ((bio)->bi_ioprio = prio)
+	 */
 	unsigned short		bi_ioprio;
 	unsigned short		bi_write_hint;
 	blk_status_t		bi_status;
@@ -156,6 +165,14 @@ struct bio {
 
 	struct bvec_iter	bi_iter;
 
+	/*
+	 * 在以下使用__bi_remaining:
+	 *   - block/bio.c|271| <<bio_init>> atomic_set(&bio->__bi_remaining, 1);
+	 *   - block/bio.c|297| <<bio_reset>> atomic_set(&bio->__bi_remaining, 1);
+	 *   - block/bio.c|1772| <<bio_remaining_done>> BUG_ON(atomic_read(&bio->__bi_remaining) <= 0);
+	 *   - block/bio.c|1774| <<bio_remaining_done>> if (atomic_dec_and_test(&bio->__bi_remaining)) {
+	 *   - include/linux/bio.h|861| <<bio_inc_remaining>> atomic_inc(&bio->__bi_remaining);
+	 */
 	atomic_t		__bi_remaining;
 	bio_end_io_t		*bi_end_io;
 
@@ -205,17 +222,54 @@ struct bio {
  */
 enum {
 	BIO_NO_PAGE_REF,	/* don't put release vec pages */
+	/*
+	 * 在以下使用BIO_CLONED:
+	 *   - block/bio.c|588| <<__bio_clone_fast>> bio_set_flag(bio, BIO_CLONED);
+	 *   - block/bio.c|701| <<__bio_add_pc_page>> if (unlikely(bio_flagged(bio, BIO_CLONED)))
+	 *   - block/bio.c|774| <<__bio_try_merge_page>> if (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))
+	 *   - block/bio.c|805| <<__bio_add_page>> WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/compression.c|192| <<end_compressed_bio_read>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/disk-io.c|848| <<btree_csum_one_bio>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/extent_io.c|2648| <<end_bio_extent_writepage>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/extent_io.c|2719| <<end_bio_extent_readpage>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/extent_io.c|3813| <<end_bio_extent_buffer_writepage>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/inode.c|7939| <<btrfs_retry_endio_nocsum>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/inode.c|8032| <<btrfs_retry_endio>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - fs/btrfs/raid56.c|1159| <<index_rbio_pages>> if (bio_flagged(bio, BIO_CLONED))
+	 *   - fs/btrfs/raid56.c|1447| <<set_bio_pages_uptodate>> ASSERT(!bio_flagged(bio, BIO_CLONED));
+	 *   - include/linux/bio.h|460| <<bio_first_bvec_all>> WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
+	 *   - include/linux/bio.h|471| <<bio_last_bvec_all>> WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));
+	 *
+	 * cloned bio must not modify vec list
+	 */
 	BIO_CLONED,		/* doesn't own data */
 	BIO_BOUNCED,		/* bio is a bounce bio */
+	/*
+	 * 在以下使用BIO_USER_MAPPED:
+	 *   - block/bio.c|1424| <<bio_map_user_iov>> bio_set_flag(bio, BIO_USER_MAPPED);
+	 *   - block/blk-map.c|64| <<__blk_rq_unmap_user>> if (bio_flagged(bio, BIO_USER_MAPPED))
+	 *   - block/blk-map.c|168| <<blk_rq_map_user_iov>> if (!bio_flagged(bio, BIO_USER_MAPPED))
+	 */
 	BIO_USER_MAPPED,	/* contains user pages */
 	BIO_NULL_MAPPED,	/* contains invalid user pages */
 	BIO_QUIET,		/* Make BIO Quiet */
+	/*
+	 * 在以下使用BIO_CHAIN:
+	 *   - block/bio.c|1769| <<bio_remaining_done>> if (!bio_flagged(bio, BIO_CHAIN))
+	 *   - block/bio.c|1775| <<bio_remaining_done>> bio_clear_flag(bio, BIO_CHAIN);
+	 *   - include/linux/bio.h|859| <<bio_inc_remaining>> bio_set_flag(bio, BIO_CHAIN);
+	 */
 	BIO_CHAIN,		/* chained bio, ->bi_remaining in effect */
 	BIO_REFFED,		/* bio has elevated ->bi_cnt */
 	BIO_THROTTLED,		/* This bio has already been subjected to
 				 * throttling rules. Don't do it again. */
 	BIO_TRACE_COMPLETION,	/* bio_endio() should trace the final completion
 				 * of this bio. */
+	/*
+	 * 在以下使用BIO_QUEUE_ENTERED:
+	 *   - block/blk-merge.c|294| <<__blk_queue_split>> bio_set_flag(*bio, BIO_QUEUE_ENTERED);
+	 *   - include/linux/blk-cgroup.h|751| <<blkcg_bio_issue_check>> if (!bio_flagged(bio, BIO_QUEUE_ENTERED))
+	 */
 	BIO_QUEUE_ENTERED,	/* can use blk_queue_enter_live() */
 	BIO_TRACKED,		/* set if bio goes through the rq_qos path */
 	BIO_FLAG_LAST
@@ -348,6 +402,16 @@ enum req_flag_bits {
 #define REQ_CGROUP_PUNT		(1ULL << __REQ_CGROUP_PUNT)
 
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 使用REQ_HIPRI的地方:
+ *   - block/blk-core.c|915| <<generic_make_request_checks>> bio->bi_opf &= ~REQ_HIPRI;
+ *   - block/blk-mq.c|613| <<__blk_mq_complete_request>> if ((rq->cmd_flags & REQ_HIPRI) ||
+ *   - block/blk-mq.h|125| <<blk_mq_map_queue>> if (flags & REQ_HIPRI)
+ *   - drivers/nvme/host/core.c|765| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1263| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|1010| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|402| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
@@ -356,6 +420,11 @@ enum req_flag_bits {
 #define REQ_FAILFAST_MASK \
 	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | REQ_FAILFAST_DRIVER)
 
+/*
+ * 在以下使用REQ_NOMERGE_FLAGS:
+ *   - include/linux/bio.h|137| <<bio_mergeable>> if (bio->bi_opf & REQ_NOMERGE_FLAGS)
+ *   - include/linux/blkdev.h|752| <<rq_mergeable>> if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
+ */
 #define REQ_NOMERGE_FLAGS \
 	(REQ_NOMERGE | REQ_PREFLUSH | REQ_FUA)
 
@@ -418,7 +487,20 @@ static inline int op_stat_group(unsigned int op)
 
 typedef unsigned int blk_qc_t;
 #define BLK_QC_T_NONE		-1U
+/*
+ * 在以下使用BLK_QC_T_SHIFT:
+ *   - include/linux/blk-mq.h|482| <<request_to_qc_t>> return rq->tag | (hctx->queue_num << BLK_QC_T_SHIFT);
+ *   - include/linux/blk-mq.h|484| <<request_to_qc_t>> return rq->internal_tag | (hctx->queue_num << BLK_QC_T_SHIFT) |
+ *   - include/linux/blk_types.h|474| <<blk_qc_t_to_queue_num>> return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
+ *   - include/linux/blk_types.h|479| <<blk_qc_t_to_tag>> return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
+ */
 #define BLK_QC_T_SHIFT		16
+/*
+ * 在以下使用BLK_QC_T_INTERNAL:
+ *   - include/linux/blk-mq.h|485| <<request_to_qc_t>> BLK_QC_T_INTERNAL;
+ *   - include/linux/blk_types.h|474| <<blk_qc_t_to_queue_num>> return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
+ *   - include/linux/blk_types.h|484| <<blk_qc_t_is_internal>> return (cookie & BLK_QC_T_INTERNAL) != 0;
+ */
 #define BLK_QC_T_INTERNAL	(1U << 31)
 
 static inline bool blk_qc_t_valid(blk_qc_t cookie)
@@ -426,11 +508,28 @@ static inline bool blk_qc_t_valid(blk_qc_t cookie)
 	return cookie != BLK_QC_T_NONE;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3827| <<blk_poll>> hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
+ *
+ * 清空cookie的第31位, 然后向右移动16位
+ */
 static inline unsigned int blk_qc_t_to_queue_num(blk_qc_t cookie)
 {
+	/*
+	 * BLK_QC_T_INTERNAL是1往左移动31位
+	 * ~就是从0数除了第31位是0, 其他都是1
+	 */
 	return (cookie & ~BLK_QC_T_INTERNAL) >> BLK_QC_T_SHIFT;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3766| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
+ *   - block/blk-mq.c|3768| <<blk_mq_poll_hybrid>> rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
+ *
+ * 获取cookie的0-15位
+ */
 static inline unsigned int blk_qc_t_to_tag(blk_qc_t cookie)
 {
 	return cookie & ((1u << BLK_QC_T_SHIFT) - 1);
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ef375d..6637513 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,6 +99,11 @@ typedef __u32 __bitwise req_flags_t;
 /* on IO scheduler merge hash */
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
 /* track IO completion time */
+/*
+ * 在以下使用RQF_STATS:
+ *   - block/blk-mq.c|543| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+ *   - block/blk-mq.c|706| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+ */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
@@ -106,8 +111,20 @@ typedef __u32 __bitwise req_flags_t;
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* already slept for hybrid poll */
+/*
+ * 在以下使用RQF_MQ_POLL_SLEPT:
+ *   - block/blk-mq.c|3714| <<blk_mq_poll_hybrid_sleep>> if (rq->rq_flags & RQF_MQ_POLL_SLEPT)
+ *   - block/blk-mq.c|3731| <<blk_mq_poll_hybrid_sleep>> rq->rq_flags |= RQF_MQ_POLL_SLEPT;
+ */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
+/*
+ * 在以下使用RQF_TIMED_OUT (timeout has been called, don't expire again):
+ *   - block/blk-mq.c|710| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-mq.c|843| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+ *   - block/blk-mq.c|862| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+ *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+ */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
@@ -118,8 +135,31 @@ typedef __u32 __bitwise req_flags_t;
  * Request state for blk-mq.
  */
 enum mq_rq_state {
+	/*
+	 * 在以下使用MQ_RQ_IDLE:
+	 *   - block/blk-mq-debugfs.c|311| <<global>> [MQ_RQ_IDLE] = "idle",
+	 *   - block/blk-mq.c|530| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|696| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *   - block/blk-mq.c|720| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|746| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2298| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 在以下使用MQ_RQ_IN_FLIGHT:
+	 *   - block/blk-mq-debugfs.c|312| <<global>> [MQ_RQ_IN_FLIGHT] = "in_flight",
+	 *   - block/blk-mq.c|723| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *   - block/blk-mq.c|859| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|897| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 在以下使用MQ_RQ_COMPLETE:
+	 *   - block/blk-mq-debugfs.c|313| <<global>> [MQ_RQ_COMPLETE] = "complete",
+	 *   - block/blk-mq.c|594| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|689| <<blk_mq_complete_request_sync>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *   - block/blk-mq.c|3756| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -131,6 +171,11 @@ enum mq_rq_state {
  */
 struct request {
 	struct request_queue *q;
+	/*
+	 * 在以下设置mq_ctx:
+	 *   - block/blk-flush.c|351| <<blk_kick_flush>> flush_rq->mq_ctx = first_rq->mq_ctx;
+	 *   - block/blk-mq.c|341| <<blk_mq_rq_ctx_init>> rq->mq_ctx = data->ctx;
+	 */
 	struct blk_mq_ctx *mq_ctx;
 	struct blk_mq_hw_ctx *mq_hctx;
 
@@ -311,6 +356,14 @@ enum blk_zoned_model {
 };
 
 struct queue_limits {
+	/*
+	 * 设置bounce_pfn的地方:
+	 *   - block/blk-settings.c|56| <<blk_set_default_limits>> lim->bounce_pfn = (unsigned long )(BLK_BOUNCE_ANY >> PAGE_SHIFT);
+	 *   - block/blk-settings.c|149| <<blk_queue_bounce_limit>> q->limits.bounce_pfn = max(max_low_pfn, b_pfn);
+	 *   - block/blk-settings.c|153| <<blk_queue_bounce_limit>> q->limits.bounce_pfn = b_pfn;
+	 *   - block/blk-settings.c|158| <<blk_queue_bounce_limit>> q->limits.bounce_pfn = b_pfn;
+	 *   - block/blk-settings.c|508| <<blk_stack_limits>> t->bounce_pfn = min_not_zero(t->bounce_pfn, b->bounce_pfn);
+	 */
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
 	unsigned long		virt_boundary_mask;
@@ -479,8 +532,31 @@ struct request_queue {
 	unsigned int		dma_alignment;
 
 	unsigned int		rq_timeout;
+	/*
+	 * poll_nsec使用的地方:
+	 *   - block/blk-mq.c|3179| <<blk_mq_init_allocated_queue>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|384| <<queue_poll_delay_store>> q->poll_nsec = BLK_MQ_POLL_CLASSIC;
+	 *   - block/blk-sysfs.c|386| <<queue_poll_delay_store>> q->poll_nsec = val * 1000;
+	 *   - block/blk-mq.c|3723| <<blk_mq_poll_hybrid_sleep>> if (q->poll_nsec > 0)
+	 *   - block/blk-mq.c|3724| <<blk_mq_poll_hybrid_sleep>> nsecs = q->poll_nsec;
+	 *   - block/blk-mq.c|3769| <<blk_mq_poll_hybrid>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|363| <<queue_poll_delay_show>> if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
+	 *   - block/blk-sysfs.c|366| <<queue_poll_delay_show>> val = q->poll_nsec / 1000;
+	 */
 	int			poll_nsec;
 
+	/*
+	 * 在以下使用poll_cb:
+	 *   - block/blk-mq.c|3118| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+	 *   - block/blk-mq.c|3121| <<blk_mq_init_allocated_queue>> if (!q->poll_cb)
+	 *   - block/blk-mq.c|3200| <<blk_mq_init_allocated_queue>> blk_stat_free_callback(q->poll_cb);
+	 *   - block/blk-mq.c|3201| <<blk_mq_init_allocated_queue>> q->poll_cb = NULL;
+	 *   - block/blk-mq.c|3621| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+	 *   - block/blk-mq.c|3636| <<blk_mq_poll_stats_start>> blk_stat_is_active(q->poll_cb))
+	 *   - block/blk-mq.c|3639| <<blk_mq_poll_stats_start>> blk_stat_activate_msecs(q->poll_cb, 100);
+	 *   - block/blk-sysfs.c|890| <<__blk_release_queue>> blk_stat_remove_callback(q, q->poll_cb);
+	 *   - block/blk-sysfs.c|891| <<__blk_release_queue>> blk_stat_free_callback(q->poll_cb);
+	 */
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
@@ -547,6 +623,15 @@ struct request_queue {
 	struct list_head	unused_hctx_list;
 	spinlock_t		unused_hctx_lock;
 
+	/*
+	 * 在以下修改mq_freeze_depth:
+	 *   - block/blk-mq.c|148| <<blk_freeze_queue_start>> if (++q->mq_freeze_depth == 1) {
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> q->mq_freeze_depth--;
+	 * 在以下使用mq_freeze_depth:
+	 *   - block/blk-core.c|434| <<blk_queue_enter>> (!q->mq_freeze_depth &&
+	 *   - block/blk-mq.c|205| <<blk_mq_unfreeze_queue>> WARN_ON_ONCE(q->mq_freeze_depth < 0);
+	 *   - block/blk-mq.c|206| <<blk_mq_unfreeze_queue>> if (!q->mq_freeze_depth) {
+	 */
 	int			mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -564,6 +649,37 @@ struct request_queue {
 	 * percpu_ref_kill() and percpu_ref_reinit().
 	 */
 	struct mutex		mq_freeze_lock;
+	/*
+	 * 在以下使用q_usage_counter:
+	 *   - block/blk-core.c|404| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-mq-sched.c|429| <<blk_mq_sched_insert_requests>> percpu_ref_get(&q->q_usage_counter);
+	 *   - block/blk-mq-tag.c|441| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|972| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk.h|64| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|611| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *
+	 *   - block/blk-core.c|413| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|445| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-mq-sched.c|450| <<blk_mq_sched_insert_requests>> percpu_ref_put(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|621| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 *
+	 *   - block/blk-core.c|530| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|378| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|541| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-mq.c|149| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *   - block/blk-mq.c|207| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-sysfs.c|964| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 
+	 *   - block/blk-mq.c|161| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|169| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *
+	 *   - block/blk-core.c|451| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *   - drivers/nvdimm/pmem.c|313| <<pmem_pagemap_cleanup>> container_of(pgmap->ref, struct request_queue, q_usage_counter);
+	 *   - drivers/nvdimm/pmem.c|326| <<pmem_pagemap_kill>> container_of(pgmap->ref, struct request_queue, q_usage_counter);
+	 *   - drivers/nvdimm/pmem.c|406| <<pmem_attach_disk>> pmem->pgmap.ref = &q->q_usage_counter;
+	 */
 	struct percpu_ref	q_usage_counter;
 
 	struct blk_mq_tag_set	*tag_set;
@@ -589,7 +705,24 @@ struct request_queue {
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
+/*
+ * 在以下使用QUEUE_FLAG_SAME_COMP:
+ *   - block/blk-mq.c|643| <<__blk_mq_complete_request>> !test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags)) {
+ *   - block/blk-softirq.c|130| <<__blk_complete_request>> if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) && ccpu != -1) {
+ *   - block/blk-sysfs.c|328| <<queue_rq_affinity_show>> bool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);
+ *   - block/blk-sysfs.c|346| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+ *   - block/blk-sysfs.c|349| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+ *   - block/blk-sysfs.c|352| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_COMP, q);
+ *   - include/linux/blkdev.h|758| <<QUEUE_FLAG_MQ_DEFAULT>> (1 << QUEUE_FLAG_SAME_COMP))
+ */
 #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
+/*
+ * 在以下使用QUEUE_FLAG_FAIL_IO:
+ *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+ *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+ *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+ *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+ */
 #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
 #define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
@@ -598,20 +731,57 @@ struct request_queue {
 #define QUEUE_FLAG_NOXMERGES	9	/* No extended merges */
 #define QUEUE_FLAG_ADD_RANDOM	10	/* Contributes to random pool */
 #define QUEUE_FLAG_SECERASE	11	/* supports secure erase */
+/*
+ * 在以下使用QUEUE_FLAG_SAME_FORCE:
+ *   - block/blk-mq.c|666| <<__blk_mq_complete_request>> if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+ *   - block/blk-softirq.c|141| <<__blk_complete_request>> if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+ *   - block/blk-sysfs.c|329| <<queue_rq_affinity_show>> bool force = test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags);
+ *   - block/blk-sysfs.c|347| <<queue_rq_affinity_store>> blk_queue_flag_set(QUEUE_FLAG_SAME_FORCE, q);
+ *   - block/blk-sysfs.c|350| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+ *   - block/blk-sysfs.c|353| <<queue_rq_affinity_store>> blk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+ */
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
+/*
+ * 在以下使用QUEUE_FLAG_POLL:
+ *   - block/blk-core.c|914| <<generic_make_request_checks>> if (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-mq.c|3151| <<blk_mq_init_allocated_queue>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-mq.c|3766| <<blk_poll>> !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
+ *   - block/blk-sysfs.c|395| <<queue_poll_show>> return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
+ *   - block/blk-sysfs.c|413| <<queue_poll_store>> blk_queue_flag_set(QUEUE_FLAG_POLL, q);
+ *   - block/blk-sysfs.c|415| <<queue_poll_store>> blk_queue_flag_clear(QUEUE_FLAG_POLL, q);
+ *   - drivers/nvme/host/core.c|763| <<nvme_execute_rq_polled>> WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+ */
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|701| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|151| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|161| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|187| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
+/*
+ * 在以下使用QUEUE_FLAG_POLL_STATS:
+ *   - block/blk-mq.c|3618| <<blk_poll_stats_enable>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-mq.c|3619| <<blk_poll_stats_enable>> blk_queue_flag_test_and_set(QUEUE_FLAG_POLL_STATS, q))
+ *   - block/blk-mq.c|3635| <<blk_mq_poll_stats_start>> if (!test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
+ *   - block/blk-sysfs.c|889| <<__blk_release_queue>> if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
+ */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
+/*
+ * 在以下使用QUEUE_FLAG_MQ_DEFAULT:
+ *   - block/blk-mq.c|3181| <<blk_mq_init_allocated_queue>> q->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;
+ */
 #define QUEUE_FLAG_MQ_DEFAULT	((1 << QUEUE_FLAG_IO_STAT) |		\
 				 (1 << QUEUE_FLAG_SAME_COMP))
 
diff --git a/include/linux/bvec.h b/include/linux/bvec.h
index a032f01..0b51092 100644
--- a/include/linux/bvec.h
+++ b/include/linux/bvec.h
@@ -42,22 +42,56 @@ struct bvec_iter_all {
  * various member access, note that bio_data should of course not be used
  * on highmem page vectors
  */
+/*
+ * 返回bio_vec数组中的bvec_iter指向的(bvec_iter->bi_idx)当前正处理的元素
+ */
 #define __bvec_iter_bvec(bvec, iter)	(&(bvec)[(iter).bi_idx])
 
 /* multi-page (mp_bvec) helpers */
+/*
+ * called by:
+ *   - include/linux/bvec.h|63| <<mp_bvec_iter_bvec>> .bv_page = mp_bvec_iter_page((bvec), (iter)), \
+ *   - include/linux/bvec.h|77| <<bvec_iter_page>> (mp_bvec_iter_page((bvec), (iter)) + \
+ *
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ */
 #define mp_bvec_iter_page(bvec, iter)				\
 	(__bvec_iter_bvec((bvec), (iter))->bv_page)
 
+/*
+ * 这里二个选最小的:
+ * 1. (iter).bi_size: 所有bvec剩下的
+ * 2. __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done: 当前bvec剩下的
+ *
+ * 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ */
 #define mp_bvec_iter_len(bvec, iter)				\
 	min((iter).bi_size,					\
 	    __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done)
 
+/*
+ * __bvec_iter_bvec((bvec), (iter))->bv_offset: 当前bvec起始的offset
+ * (iter).bi_bvec_done: 当前bvec已经完成的
+ *
+ * 所以返回的是当前bvec"完成到"的offset
+ */
 #define mp_bvec_iter_offset(bvec, iter)				\
 	(__bvec_iter_bvec((bvec), (iter))->bv_offset + (iter).bi_bvec_done)
 
+/* 返回当前bvec中已经完成到的offset所在的以PAGE_SIZE为单位的index */
 #define mp_bvec_iter_page_idx(bvec, iter)			\
 	(mp_bvec_iter_offset((bvec), (iter)) / PAGE_SIZE)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|165| <<__bio_for_each_bvec>> ((bvl = mp_bvec_iter_bvec((bio)->bi_io_vec, (iter))), 1); \
+ *   - include/linux/blkdev.h|981| <<req_bvec>> return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
+ *
+ * 构造一个bvec:
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * bv_offset : 所以返回的是当前bvec"完成到"的offset
+ */
 #define mp_bvec_iter_bvec(bvec, iter)				\
 ((struct bio_vec) {						\
 	.bv_page	= mp_bvec_iter_page((bvec), (iter)),	\
@@ -66,17 +100,64 @@ struct bvec_iter_all {
 })
 
 /* For building single-page bvec in flight */
+/*
+ * called by:
+ *   - include/linux/bio.h|51| <<bio_iter_offset>> bvec_iter_offset((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|108| <<bvec_iter_len>> PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
+ *   - include/linux/bvec.h|118| <<bvec_iter_bvec>> .bv_offset = bvec_iter_offset((bvec), (iter)), \
+ *
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
  #define bvec_iter_offset(bvec, iter)				\
 	(mp_bvec_iter_offset((bvec), (iter)) % PAGE_SIZE)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|49| <<bio_iter_len>> bvec_iter_len((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|117| <<bvec_iter_bvec>> .bv_len = bvec_iter_len((bvec), (iter)), \
+ *   - net/ceph/messenger.c|885| <<ceph_msg_data_bvecs_cursor_init>> BUG_ON(cursor->resid < bvec_iter_len(bvecs, cursor->bvec_iter));
+ *   - net/ceph/messenger.c|887| <<ceph_msg_data_bvecs_cursor_init>> cursor->resid == bvec_iter_len(bvecs, cursor->bvec_iter);
+ *   - net/ceph/messenger.c|909| <<ceph_msg_data_bvecs_advance>> BUG_ON(bytes > bvec_iter_len(bvecs, cursor->bvec_iter));
+ *   - net/ceph/messenger.c|923| <<ceph_msg_data_bvecs_advance>> BUG_ON(cursor->resid < bvec_iter_len(bvecs, cursor->bvec_iter));
+ *   - net/ceph/messenger.c|925| <<ceph_msg_data_bvecs_advance>> cursor->resid == bvec_iter_len(bvecs, cursor->bvec_iter);
+ *
+ * 二个进行比较:
+ * 1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * 2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ */
 #define bvec_iter_len(bvec, iter)				\
 	min_t(unsigned, mp_bvec_iter_len((bvec), (iter)),		\
 	      PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
 
+/*
+ * mp_bvec_iter_page((bvec), (iter)): 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * mp_bvec_iter_page_idx((bvec), (iter)): 返回当前bvec中已经完成到的offset所在的以PAGE_SIZE为单位的index
+ *
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bvec_iter_page(bvec, iter)				\
 	(mp_bvec_iter_page((bvec), (iter)) +			\
 	 mp_bvec_iter_page_idx((bvec), (iter)))
 
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|1758| <<__journal_read_write>> struct bio_vec biv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - drivers/md/dm-io.c|211| <<bio_get_page>> struct bio_vec bvec = bvec_iter_bvec((struct bio_vec *)dp->context_ptr,
+ *   - drivers/nvdimm/blk.c|84| <<nd_blk_rw_integrity>> bv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - drivers/nvdimm/btt.c|1158| <<btt_rw_integrity>> bv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - include/linux/bio.h|44| <<bio_iter_iovec>> bvec_iter_bvec((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|150| <<for_each_bvec>> ((bvl = bvec_iter_bvec((bio_vec), (iter))), 1); \
+ *   - net/ceph/messenger.c|894| <<ceph_msg_data_bvecs_next>> struct bio_vec bv = bvec_iter_bvec(cursor->data->bvec_pos.bvecs,
+ *
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较: 
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bvec_iter_bvec(bvec, iter)				\
 ((struct bio_vec) {						\
 	.bv_page	= bvec_iter_page((bvec), (iter)),	\
@@ -84,6 +165,19 @@ struct bvec_iter_all {
 	.bv_offset	= bvec_iter_offset((bvec), (iter)),	\
 })
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|388| <<bio_integrity_advance>> bvec_iter_advance(bip->bip_vec, &bip->bip_iter, bytes);
+ *   - drivers/md/dm-integrity.c|1767| <<__journal_read_write>> bvec_iter_advance(bip->bip_vec, &bip->bip_iter, tag_now);
+ *   - drivers/md/dm-io.c|226| <<bio_next_page>> bvec_iter_advance((struct bio_vec *)dp->context_ptr,
+ *   - drivers/nvdimm/blk.c|101| <<nd_blk_rw_integrity>> if (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))
+ *   - drivers/nvdimm/btt.c|1182| <<btt_rw_integrity>> if (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))
+ *   - include/linux/bio.h|163| <<bio_advance_iter>> bvec_iter_advance(bio->bi_io_vec, iter, bytes);
+ *   - include/linux/bvec.h|151| <<for_each_bvec>> bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
+ *   - include/linux/ceph/messenger.h|139| <<__ceph_bvec_iter_advance_step>> bvec_iter_advance((it)->bvecs, &(it)->iter, (n)); \
+ *   - net/ceph/messenger.c|911| <<ceph_msg_data_bvecs_advance>> bvec_iter_advance(bvecs, &cursor->bvec_iter, bytes);
+ *   - net/sunrpc/xprtsock.c|390| <<xs_flush_bvec>> bvec_iter_advance(bvec, &bi, seek & PAGE_MASK);
+ */
 static inline bool bvec_iter_advance(const struct bio_vec *bv,
 		struct bvec_iter *iter, unsigned bytes)
 {
@@ -95,6 +189,12 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 
 	while (bytes) {
 		const struct bio_vec *cur = bv + iter->bi_idx;
+		/*
+		 * 三个里面选择最小值:
+		 * 1. bytes                           : 想要advance的
+		 * 2. iter->bi_size                   : 所有bvec剩下的
+		 * 3. cur->bv_len - iter->bi_bvec_done: 当前bvec剩下的
+		 */
 		unsigned len = min3(bytes, iter->bi_size,
 				    cur->bv_len - iter->bi_bvec_done);
 
@@ -102,6 +202,11 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 		iter->bi_size -= len;
 		iter->bi_bvec_done += len;
 
+		/*
+		 * 如果当前bio_vec已经用完了, 则要进入下一个bio_vec (iter->bi_idx++)
+		 * 清空iter->bi_bvec_done因为是number of bytes completed in current bvec
+		 * 现在要进入下一个了
+		 */
 		if (iter->bi_bvec_done == cur->bv_len) {
 			iter->bi_bvec_done = 0;
 			iter->bi_idx++;
@@ -110,6 +215,15 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	return true;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|757| <<bip_for_each_vec>> for_each_bvec(bvl, (bip)->bip_vec, iter, (bip)->bip_iter)
+ *   - include/linux/ceph/messenger.h|158| <<ceph_bvec_iter_advance_step>> for_each_bvec(bv, (it)->bvecs, __cur_iter, __cur_iter) \
+ *   - lib/iov_iter.c|70| <<iterate_bvec>> for_each_bvec(__v, i->bvec, __bi, __start) { \
+ *   - net/sunrpc/xprtsock.c|391| <<xs_flush_bvec>> for_each_bvec(bv, bvec, bi, bi)
+ *
+ * 这里第一个参数bvl是基于一个page的bvec
+ */
 #define for_each_bvec(bvl, bio_vec, iter, start)			\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -117,6 +231,10 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	     bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
 
 /* for iterating one bio from start to end */
+/*
+ * called by:
+ *   - block/bounce.c|142| <<copy_to_high_bio_irq>> struct bvec_iter from_iter = BVEC_ITER_ALL_INIT;
+ */
 #define BVEC_ITER_ALL_INIT (struct bvec_iter)				\
 {									\
 	.bi_sector	= 0,						\
@@ -125,6 +243,10 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	.bi_bvec_done	= 0,						\
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|153| <<bio_for_each_segment_all>> for (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )
+ */
 static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
 {
 	iter_all->done = 0;
@@ -133,6 +255,10 @@ static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
 	return &iter_all->bv;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|144| <<bio_next_segment>> bvec_advance(&bio->bi_io_vec[iter->idx], iter);
+ */
 static inline void bvec_advance(const struct bio_vec *bvec,
 				struct bvec_iter_all *iter_all)
 {
@@ -149,6 +275,12 @@ static inline void bvec_advance(const struct bio_vec *bvec,
 			   bvec->bv_len - iter_all->done);
 	iter_all->done += bv->bv_len;
 
+	/*
+	 * 似乎iter_all->done是当前bvec完成的len?
+	 *
+	 * 所以这里当前bvec如果都完成了
+	 * 就要进入下一个bvec?? ---> iter_all->idx++
+	 */
 	if (iter_all->done == bvec->bv_len) {
 		iter_all->idx++;
 		iter_all->done = 0;
@@ -159,6 +291,10 @@ static inline void bvec_advance(const struct bio_vec *bvec,
  * Get the last single-page segment from the multi-page bvec and store it
  * in @seg
  */
+/*
+ * called by:
+ *   - fs/buffer.c|3042| <<guard_bio_eod>> mp_bvec_last_segment(bvec, &bv);
+ */
 static inline void mp_bvec_last_segment(const struct bio_vec *bvec,
 					struct bio_vec *seg)
 {
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 997a530..1956ef2 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -3107,6 +3107,18 @@ ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
 			     dio_iodone_t end_io, dio_submit_t submit_io,
 			     int flags);
 
+/*
+ * called by:
+ *   - fs/affs/file.c|409| <<affs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, affs_get_block);
+ *   - fs/ext2/inode.c|945| <<ext2_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, ext2_get_block);
+ *   - fs/fat/inode.c|283| <<fat_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, fat_get_block);
+ *   - fs/hfs/inode.c|137| <<hfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfs_get_block);
+ *   - fs/hfsplus/inode.c|134| <<hfsplus_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, hfsplus_get_block);
+ *   - fs/jfs/inode.c|342| <<jfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, jfs_get_block);
+ *   - fs/nilfs2/inode.c|303| <<nilfs_direct_IO>> return blockdev_direct_IO(iocb, inode, iter, nilfs_get_block);
+ *   - fs/reiserfs/inode.c|3262| <<reiserfs_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter,
+ *   - fs/udf/inode.c|217| <<udf_direct_IO>> ret = blockdev_direct_IO(iocb, inode, iter, udf_get_block);
+ */
 static inline ssize_t blockdev_direct_IO(struct kiocb *iocb,
 					 struct inode *inode,
 					 struct iov_iter *iter,
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index 7aef0ab..1679156 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -61,10 +61,41 @@ typedef void (percpu_ref_func_t)(struct percpu_ref *);
 
 /* flags set in the lower bits of percpu_ref->percpu_count_ptr */
 enum {
+	/*
+	 * 在以下使用__PERCPU_REF_ATOMIC:
+	 *   - include/linux/percpu-refcount.h|66| <<global>> __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
+	 *   - lib/percpu-refcount.c|76| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|175| <<__percpu_ref_switch_to_atomic>> if (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {
+	 *   - lib/percpu-refcount.c|182| <<__percpu_ref_switch_to_atomic>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|201| <<__percpu_ref_switch_to_percpu>> if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
+	 *   - lib/percpu-refcount.c|219| <<__percpu_ref_switch_to_percpu>> ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
+	 */
 	__PERCPU_REF_ATOMIC	= 1LU << 0,	/* operating in atomic mode */
+	/*
+	 * 在以下使用__PERCPU_REF_DEAD:
+	 *   - include/linux/percpu-refcount.h|66| <<global>> __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
+	 *   - include/linux/percpu-refcount.h|265| <<percpu_ref_tryget_live>> } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
+	 *   - include/linux/percpu-refcount.h|323| <<percpu_ref_is_dying>> return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|83| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+	 *   - lib/percpu-refcount.c|345| <<percpu_ref_kill_and_confirm>> WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
+	 *   - lib/percpu-refcount.c|348| <<percpu_ref_kill_and_confirm>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|396| <<percpu_ref_resurrect>> WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	 *   - lib/percpu-refcount.c|399| <<percpu_ref_resurrect>> ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
+	 */
 	__PERCPU_REF_DEAD	= 1LU << 1,	/* (being) killed */
+	/*
+	 * 在以下使用__PERCPU_REF_ATOMIC_DEAD:
+	 *   - include/linux/percpu-refcount.h|169| <<__ref_is_percpu>> if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
+	 *   - lib/percpu-refcount.c|43| <<percpu_count_ptr>> (ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);
+	 *   - lib/percpu-refcount.c|113| <<percpu_ref_exit>> ref->percpu_count_ptr = __PERCPU_REF_ATOMIC_DEAD;
+	 */
 	__PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
 
+	/*
+	 * 在以下使用__PERCPU_REF_FLAG_BITS:
+	 *   - lib/percpu-refcount.c|63| <<percpu_ref_init>> size_t align = max_t(size_t, 1 << __PERCPU_REF_FLAG_BITS,
+	 */
 	__PERCPU_REF_FLAG_BITS	= 2,
 };
 
@@ -77,6 +108,13 @@ enum {
 	 * percpu_ref_switch_to_percpu() is invoked on it.
 	 * Implies ALLOW_REINIT.
 	 */
+	/*
+	 * 在以下使用PERCPU_REF_INIT_ATOMIC:
+	 *   - block/blk-core.c|532| <<blk_alloc_queue_node>> PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+	 *   - drivers/infiniband/sw/rdmavt/mr.c|739| <<rvt_alloc_fmr>> PERCPU_REF_INIT_ATOMIC);
+	 *   - lib/percpu-refcount.c|72| <<percpu_ref_init>> ref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;
+	 *   - lib/percpu-refcount.c|75| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD)) {
+	 */
 	PERCPU_REF_INIT_ATOMIC	= 1 << 0,
 
 	/*
@@ -84,11 +122,22 @@ enum {
 	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC and
 	 * ALLOW_REINIT.
 	 */
+	/*
+	 * 在以下使用PERCPU_REF_INIT_DEAD:
+	 *   - lib/percpu-refcount.c|75| <<percpu_ref_init>> if (flags & (PERCPU_REF_INIT_ATOMIC | PERCPU_REF_INIT_DEAD)) {
+	 *   - lib/percpu-refcount.c|82| <<percpu_ref_init>> if (flags & PERCPU_REF_INIT_DEAD)
+	 */
 	PERCPU_REF_INIT_DEAD	= 1 << 1,
 
 	/*
 	 * Allow switching from atomic mode to percpu mode.
 	 */
+	/*
+	 * 在以下PERCPU_REF_ALLOW_REINIT使用:
+	 *   - drivers/md/md.c|5320| <<mddev_init_writes_pending>> PERCPU_REF_ALLOW_REINIT, GFP_KERNEL) < 0)
+	 *   - fs/io_uring.c|404| <<io_ring_ctx_alloc>> PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {
+	 *   - lib/percpu-refcount.c|73| <<percpu_ref_init>> ref->allow_reinit = flags & PERCPU_REF_ALLOW_REINIT;
+	 */
 	PERCPU_REF_ALLOW_REINIT	= 1 << 2,
 };
 
@@ -101,7 +150,21 @@ struct percpu_ref {
 	unsigned long		percpu_count_ptr;
 	percpu_ref_func_t	*release;
 	percpu_ref_func_t	*confirm_switch;
+	/*
+	 * 在以下使用force_atomic:
+	 *   - lib/percpu-refcount.c|72| <<percpu_ref_init>> ref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;
+	 *   - lib/percpu-refcount.c|235| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+	 *   - lib/percpu-refcount.c|268| <<percpu_ref_switch_to_atomic>> ref->force_atomic = true;
+	 *   - lib/percpu-refcount.c|314| <<percpu_ref_switch_to_percpu>> ref->force_atomic = false;
+	 */
 	bool			force_atomic:1;
+	/*
+	 * 在以下使用allow_reinit:
+	 *   - lib/percpu-refcount.c|73| <<percpu_ref_init>> ref->allow_reinit = flags & PERCPU_REF_ALLOW_REINIT;
+	 *   - lib/percpu-refcount.c|77| <<percpu_ref_init>> ref->allow_reinit = true;
+	 *   - lib/percpu-refcount.c|126| <<percpu_ref_call_confirm_rcu>> if (!ref->allow_reinit)
+	 *   - lib/percpu-refcount.c|204| <<__percpu_ref_switch_to_percpu>> if (WARN_ON_ONCE(!ref->allow_reinit))
+	 */
 	bool			allow_reinit:1;
 	struct rcu_head		rcu;
 };
diff --git a/include/linux/uio.h b/include/linux/uio.h
index ab5f523..7198512 100644
--- a/include/linux/uio.h
+++ b/include/linux/uio.h
@@ -20,10 +20,36 @@ struct kvec {
 
 enum iter_type {
 	/* iter types */
+	/*
+	 * 在以下使用ITER_IOVEC:
+	 *   - include/linux/uio.h|61| <<iter_is_iovec>> return iov_iter_type(i) == ITER_IOVEC;
+	 *   - lib/iov_iter.c|448| <<iov_iter_init>> i->type = ITER_IOVEC | direction;
+	 */
 	ITER_IOVEC = 4,
+	/*
+	 * 设置ITER_KVEC的地方:
+	 *   - lib/iov_iter.c|445| <<iov_iter_init>> i->type = ITER_KVEC | direction;
+	 *   - lib/iov_iter.c|1136| <<iov_iter_kvec>> i->type = ITER_KVEC | (direction & (READ | WRITE));
+	 */
 	ITER_KVEC = 8,
+	/*
+	 * 设置ITER_BVEC的地方:
+	 *   - lib/iov_iter.c|1149| <<iov_iter_bvec>> i->type = ITER_BVEC | (direction & (READ | WRITE));
+	 */
 	ITER_BVEC = 16,
+	/*
+	 * 在以下使用ITER_PIPE:
+	 *   - include/linux/uio.h|76| <<iov_iter_is_pipe>> return iov_iter_type(i) == ITER_PIPE;
+	 *   - lib/iov_iter.c|1163| <<iov_iter_pipe>> i->type = ITER_PIPE | READ;
+	 */
 	ITER_PIPE = 32,
+	/*
+	 * 在以下使用ITER_DISCARD:
+	 *   - lib/iov_iter.c|1184| <<iov_iter_discard>> i->type = ITER_DISCARD | READ;
+	 *   - include/linux/uio.h|81| <<iov_iter_is_discard>> return iov_iter_type(i) == ITER_DISCARD;
+	 *   - lib/iov_iter.c|88| <<iterate_all_kinds>> } else if (unlikely(i->type & ITER_DISCARD)) { \
+	 *   - lib/iov_iter.c|120| <<iterate_and_advance>> } else if (unlikely(i->type & ITER_DISCARD)) { \
+	 */
 	ITER_DISCARD = 64,
 };
 
diff --git a/include/rdma/rdma_cm.h b/include/rdma/rdma_cm.h
index 71f48cf..b0cb2a0 100644
--- a/include/rdma/rdma_cm.h
+++ b/include/rdma/rdma_cm.h
@@ -45,6 +45,18 @@
  * RDMA identifier and release all resources allocated with the device.
  */
 enum rdma_cm_event_type {
+	/*
+	 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 */
 	RDMA_CM_EVENT_ADDR_RESOLVED,
 	RDMA_CM_EVENT_ADDR_ERROR,
 	RDMA_CM_EVENT_ROUTE_RESOLVED,
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index f1e0569..08c9a45 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -433,6 +433,9 @@ int iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)
 }
 EXPORT_SYMBOL(iov_iter_fault_in_readable);
 
+/*
+ * iovec的type会mask上ITER_KVEC或者ITER_IOVEC
+ */
 void iov_iter_init(struct iov_iter *i, unsigned int direction,
 			const struct iovec *iov, unsigned long nr_segs,
 			size_t count)
@@ -1352,6 +1355,17 @@ static ssize_t pipe_get_pages_alloc(struct iov_iter *i,
 	return n;
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1384| <<bio_map_user_iov>> bytes = iov_iter_get_pages_alloc(iter, &pages, LONG_MAX, &offs);
+ *   - fs/ceph/file.c|625| <<ceph_sync_read>> ret = iov_iter_get_pages_alloc(to, &pages, len,
+ *   - fs/cifs/file.c|2755| <<cifs_write_from_iter>> result = iov_iter_get_pages_alloc(
+ *   - fs/cifs/file.c|3477| <<cifs_send_async_read>> result = iov_iter_get_pages_alloc(
+ *   - fs/nfs/direct.c|490| <<nfs_direct_read_schedule_iovec>> result = iov_iter_get_pages_alloc(iter, &pagevec,
+ *   - fs/nfs/direct.c|907| <<nfs_direct_write_schedule_iovec>> result = iov_iter_get_pages_alloc(iter, &pagevec,
+ *   - fs/splice.c|388| <<default_file_splice_read>> res = iov_iter_get_pages_alloc(&to, &pages, len + offset, &base);
+ *   - net/9p/trans_virtio.c|329| <<p9_get_mapped_pages>> n = iov_iter_get_pages_alloc(data, pages, count, offs);
+ */
 ssize_t iov_iter_get_pages_alloc(struct iov_iter *i,
 		   struct page ***pages, size_t maxsize,
 		   size_t *start)
@@ -1680,6 +1694,16 @@ ssize_t compat_import_iovec(int type,
 }
 #endif
 
+/*
+ * called by:
+ *   - block/blk-map.c|167| <<blk_rq_map_user>> int ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);
+ *   - fs/aio.c|1488| <<aio_setup_rw>> ssize_t ret = import_single_range(rw, buf, len, *iovec, iter);
+ *   - net/socket.c|1931| <<__sys_sendto>> err = import_single_range(WRITE, buff, len, &iov, &msg.msg_iter);
+ *   - net/socket.c|1992| <<__sys_recvfrom>> err = import_single_range(READ, ubuf, size, &iov, &msg.msg_iter);
+ *   - security/keys/keyctl.c|1186| <<keyctl_instantiate_key>> ret = import_single_range(WRITE, (void __user *)_payload, plen,
+ *
+ * iovec的type会mask上ITER_KVEC或者ITER_IOVEC
+ */
 int import_single_range(int rw, void __user *buf, size_t len,
 		 struct iovec *iov, struct iov_iter *i)
 {
@@ -1690,6 +1714,9 @@ int import_single_range(int rw, void __user *buf, size_t len,
 
 	iov->iov_base = buf;
 	iov->iov_len = len;
+	/*
+	 * iovec的type会mask上ITER_KVEC或者ITER_IOVEC
+	 */
 	iov_iter_init(i, rw, iov, 1, len);
 	return 0;
 }
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 969e540..9ebff44 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -603,6 +603,11 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|62| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->bitmap_tags);
+ *   - block/blk-mq-tag.c|64| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->breserved_tags);
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -660,6 +665,10 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
 
+/*
+ * called by:
+ *   - block/kyber-iosched.c|723| <<kyber_get_domain_token>> sbitmap_add_wait_queue(domain_tokens, ws, wait);
+ */
 void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
 			    struct sbq_wait_state *ws,
 			    struct sbq_wait *sbq_wait)
@@ -682,6 +691,11 @@ void sbitmap_del_wait_queue(struct sbq_wait *sbq_wait)
 }
 EXPORT_SYMBOL_GPL(sbitmap_del_wait_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|188| <<blk_mq_get_tag>> sbitmap_prepare_to_wait(bt, ws, &wait, TASK_UNINTERRUPTIBLE);
+ *   - drivers/target/iscsi/iscsi_target_util.c|155| <<iscsit_wait_for_tag>> sbitmap_prepare_to_wait(sbq, ws, &wait, state);
+ */
 void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
 			     struct sbq_wait_state *ws,
 			     struct sbq_wait *sbq_wait, int state)
-- 
2.7.4

