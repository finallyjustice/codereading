From 32151a77a6d02a1066f724bf8df1e1aef1078b5a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Fri, 20 Sep 2019 06:37:38 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-mq.c              |  26 ++
 drivers/nvme/host/core.c    |   8 +
 drivers/nvme/host/fabrics.c |  11 +
 drivers/nvme/host/fabrics.h |   9 +
 drivers/nvme/host/nvme.h    |  17 ++
 drivers/nvme/host/rdma.c    | 687 ++++++++++++++++++++++++++++++++++++++++++++
 drivers/nvme/target/core.c  |   7 +
 include/rdma/rdma_cm.h      |  12 +
 8 files changed, 777 insertions(+)

diff --git a/block/blk-mq.c b/block/blk-mq.c
index 0835f4d..3f4cd53 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -1652,6 +1652,16 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|391| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|751| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1850| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1866| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ *
+ * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1884,6 +1894,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|437| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1897,6 +1911,9 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		if (ret != BLK_STS_OK) {
 			if (ret == BLK_STS_RESOURCE ||
 					ret == BLK_STS_DEV_RESOURCE) {
+				/*
+				 * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+				 */
 				blk_mq_request_bypass_insert(rq,
 							list_empty(list));
 				break;
@@ -1914,6 +1931,13 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		hctx->queue->mq_ops->commit_rqs(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1998| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *   - block/blk-mq.c|2013| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *
+ * 核心思想是把request插入到plug->mq_list
+ */
 static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 {
 	list_add_tail(&rq->queuelist, &plug->mq_list);
@@ -1995,6 +2019,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			trace_block_plug(q);
 		}
 
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 	} else if (plug && !blk_queue_nomerges(q)) {
 		/*
@@ -2010,6 +2035,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			list_del_init(&same_queue_rq->queuelist);
 			plug->rq_count--;
 		}
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 		trace_block_plug(q);
 
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index d3d6b7b..9b91b56 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -702,6 +702,14 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2319| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|892| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|2229| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2063| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|145| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 1994d5b..432bcc1 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -490,6 +490,13 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3449| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|2328| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|691| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
@@ -977,6 +984,10 @@ EXPORT_SYMBOL_GPL(nvmf_free_options);
 				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
 				 NVMF_OPT_DISABLE_SQFLOW)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1083| <<nvmf_dev_write>> ctrl = nvmf_create_ctrl(nvmf_device, buf);
+ */
 static struct nvme_ctrl *
 nvmf_create_ctrl(struct device *dev, const char *buf)
 {
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index 3044d8b..055e773 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -99,6 +99,15 @@ struct nvmf_ctrl_options {
 	unsigned int		nr_io_queues;
 	unsigned int		reconnect_delay;
 	bool			discovery_nqn;
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {
+	 */
 	bool			duplicate_connect;
 	unsigned int		kato;
 	struct nvmf_host	*host;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 2d678fb..58c4078 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -179,6 +179,23 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 1a6449b..08f9e7d 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -27,6 +27,9 @@
 #include "nvme.h"
 #include "fabrics.h"
 
+/*
+ * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+ */
 
 #define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
 
@@ -39,6 +42,12 @@ struct nvme_rdma_device {
 	struct ib_pd		*pd;
 	struct kref		ref;
 	struct list_head	entry;
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	unsigned int		num_inline_segments;
 };
 
@@ -73,8 +82,27 @@ enum nvme_rdma_queue_flags {
 };
 
 struct nvme_rdma_queue {
+	/*
+	 * 分配, 释放和使用rsp_ring的地方:
+	 *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|595| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|597| <<nvme_rdma_create_queue_ib>> if (!queue->rsp_ring) {
+	 *   - drivers/nvme/host/rdma.c|618| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|1756| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+	 *
+	 * 是一个数组, 有queue->queue_size个struct nvme_rdma_qe
+	 */
 	struct nvme_rdma_qe	*rsp_ring;
+	/*
+	 * 设置queue_size的地方:
+	 *   - drivers/nvme/host/rdma.c|659| <<nvme_rdma_alloc_queue>> queue->queue_size = queue_size;
+	 */
 	int			queue_size;
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	size_t			cmnd_capsule_len;
 	struct nvme_rdma_ctrl	*ctrl;
 	struct nvme_rdma_device	*device;
@@ -84,19 +112,56 @@ struct nvme_rdma_queue {
 	unsigned long		flags;
 	struct rdma_cm_id	*cm_id;
 	int			cm_error;
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	struct completion	cm_done;
 };
 
 struct nvme_rdma_ctrl {
 	/* read only in the hot path */
+	/*
+	 * nvme_rdma_create_ctrl()分配的queues
+	 * 2402         ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+	 * 2403                                 GFP_KERNEL);
+	 */
 	struct nvme_rdma_queue	*queues;
 
 	/* other member variables */
 	struct blk_mq_tag_set	tag_set;
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	struct work_struct	err_work;
 
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	struct nvme_rdma_qe	async_event_sqe;
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	struct delayed_work	reconnect_work;
 
 	struct list_head	list;
@@ -110,18 +175,49 @@ struct nvme_rdma_ctrl {
 	struct sockaddr_storage src_addr;
 
 	struct nvme_ctrl	ctrl;
+	/*
+	 * 在以下修改和使用use_inline_data:
+	 *   - drivers/nvme/host/rdma.c|1205| <<nvme_rdma_setup_ctrl>> ctrl->use_inline_data = true;
+	 *   - drivers/nvme/host/rdma.c|1516| <<nvme_rdma_map_data>> queue->ctrl->use_inline_data &&
+	 */
 	bool			use_inline_data;
+	/*
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	u32			io_queues[HCTX_MAX_TYPES];
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|729| <<nvme_rdma_alloc_tagset>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|951| <<nvme_rdma_free_ctrl>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+ *   - drivers/nvme/host/rdma.c|1922| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ */
 static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|373| <<nvme_rdma_find_get_device>> list_for_each_entry(ndev, &device_list, entry) {
+ *   - drivers/nvme/host/rdma.c|400| <<nvme_rdma_find_get_device>> list_add(&ndev->entry, &device_list);
+ *   - drivers/nvme/host/rdma.c|2119| <<nvme_rdma_remove_one>> list_for_each_entry(ndev, &device_list, entry) {
+ */
 static LIST_HEAD(device_list);
 static DEFINE_MUTEX(device_list_mutex);
 
+/*
+ * 在以下使用nvme_rdma_ctrl_list:
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+ *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ */
 static LIST_HEAD(nvme_rdma_ctrl_list);
 static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
 
@@ -130,6 +226,10 @@ static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
  * unsafe.  With it turned off we will have to register a global rkey that
  * allows read and write access to all physical memory.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|387| <<nvme_rdma_find_get_device>> register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+ */
 static bool register_always = true;
 module_param(register_always, bool, 0444);
 MODULE_PARM_DESC(register_always,
@@ -139,10 +239,24 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *event);
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops;
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
 
 /* XXX: really should move to a generic header sooner or later.. */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1223| <<nvme_rdma_set_sg_null>> put_unaligned_le24(0, sg->length);
+ *   - drivers/nvme/host/rdma.c|1260| <<nvme_rdma_map_sg_single>> put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
+ *   - drivers/nvme/host/rdma.c|1304| <<nvme_rdma_map_sg_fr>> put_unaligned_le24(req->mr->length, sg->length);
+ */
 static inline void put_unaligned_le24(u32 val, u8 *p)
 {
 	*p++ = val;
@@ -152,28 +266,76 @@ static inline void put_unaligned_le24(u32 val, u8 *p)
 
 static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctrl
+	 */
 	return queue - queue->ctrl->queues;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|471| <<nvme_rdma_create_queue_ib>> if (nvme_rdma_poll_queue(queue))
+ *   - drivers/nvme/host/rdma.c|620| <<nvme_rdma_start_queue>> bool poll = nvme_rdma_poll_queue(queue);
+ */
 static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctr
+	 *
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	return nvme_rdma_queue_idx(queue) >
 		queue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +
 		queue->ctrl->io_queues[HCTX_TYPE_READ];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1540| <<nvme_rdma_map_data>> nvme_rdma_inline_data_size(queue)) {
+ */
 static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|201| <<nvme_rdma_free_ring>> nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+ */
 static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
+	/*
+	 * struct nvme_rdma_qe {
+	 *         struct ib_cqe           cqe;
+	 *         void                    *data;
+	 *         u64                     dma;
+	 * };
+	 */
 	ib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);
 	kfree(qe->data);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|257| <<nvme_rdma_alloc_ring>> if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+ *   - drivers/nvme/host/rdma.c|842| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+ */
 static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -191,17 +353,31 @@ static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|264| <<nvme_rdma_alloc_ring>> nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|474| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ *   - drivers/nvme/host/rdma.c|551| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ */
 static void nvme_rdma_free_ring(struct ib_device *ibdev,
 		struct nvme_rdma_qe *ring, size_t ib_queue_size,
 		size_t capsule_size, enum dma_data_direction dir)
 {
 	int i;
 
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+	 */
 	for (i = 0; i < ib_queue_size; i++)
 		nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
 	kfree(ring);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+ */
 static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 		size_t ib_queue_size, size_t capsule_size,
 		enum dma_data_direction dir)
@@ -218,6 +394,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	 * lifetime. It's safe, since any chage in the underlying RDMA device
 	 * will issue error recovery and queue re-creation.
 	 */
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+	 */
 	for (i = 0; i < ib_queue_size; i++) {
 		if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
 			goto out_free_ring;
@@ -230,6 +410,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|300| <<nvme_rdma_create_qp>> init_attr.event_handler = nvme_rdma_qp_event;
+ */
 static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 {
 	pr_debug("QP event %s (%d)\n",
@@ -237,10 +421,24 @@ static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|634| <<nvme_rdma_alloc_queue>> ret = nvme_rdma_wait_for_cm(queue);
+ *
+ * 核心思想是等待nvme_rdma_queue->cm_done被complete
+ * 只在nvme_rdma_cm_handler被唤醒
+ */
 static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 {
 	int ret;
 
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
 			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
 	if (ret < 0)
@@ -251,6 +449,10 @@ static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 	return queue->cm_error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|485| <<nvme_rdma_create_queue_ib>> ret = nvme_rdma_create_qp(queue, send_wr_factor);
+ */
 static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 {
 	struct nvme_rdma_device *dev = queue->device;
@@ -270,20 +472,42 @@ static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 	init_attr.send_cq = queue->ib_cq;
 	init_attr.recv_cq = queue->ib_cq;
 
+	/*
+	 * dev->pd是struct ib_pd
+	 * queue->cm_id是struct rdma_cm_id
+	 */
 	ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
 
+	/*
+	 * queue->qp是struct ib_qp
+	 */
 	queue->qp = queue->cm_id->qp;
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.exit_request = nvme_rdma_exit_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.exit_request = nvme_rdma_exit_request()
+ */
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	kfree(req->sqe.data);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_request = nvme_rdma_init_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_request = nvme_rdma_init_request()
+ */
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -294,6 +518,13 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
 
 	nvme_req(rq)->ctrl = &ctrl->ctrl;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.data = kzalloc(sizeof(struct nvme_command), GFP_KERNEL);
 	if (!req->sqe.data)
 		return -ENOMEM;
@@ -303,6 +534,9 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_hctx = nvme_rdma_init_hctx()
+ */
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -315,6 +549,9 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_hctx = nvme_rdma_init_admin_hctx()
+ */
 static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -350,6 +587,17 @@ static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
 	return kref_get_unless_zero(&dev->ref);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|530| <<nvme_rdma_create_queue_ib>> queue->device = nvme_rdma_find_get_device(queue->cm_id);
+ *
+ * nvme_rdma_cm_handler()
+ *  -> nvme_rdma_addr_resolved()
+ *      -> nvme_rdma_create_queue_ib()
+ *          -> nvme_rdma_find_get_device()
+ *
+ * 惨是的rdma_cm_id->qp是struct ib_qp
+ */
 static struct nvme_rdma_device *
 nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 {
@@ -366,9 +614,22 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	if (!ndev)
 		goto out_err;
 
+	/* 类型是struct ib_device */
 	ndev->dev = cm_id->device;
 	kref_init(&ndev->ref);
 
+	/*
+	 * ndev->pd是struct ib_pd
+	 *
+	 * ib_alloc_pd - Allocates an unused protection domain.
+	 * @device: The device on which to allocate the protection domain.
+	 *
+	 * A protection domain object provides an association between QPs, shared
+	 * receive queues, address handles, memory regions, and memory windows.
+	 *
+	 * Every PD has a local_dma_lkey which can be used as the lkey value for local
+	 * memory operations.
+	 */
 	ndev->pd = ib_alloc_pd(ndev->dev,
 		register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
 	if (IS_ERR(ndev->pd))
@@ -397,6 +658,16 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_alloc_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|799| <<nvme_rdma_free_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1846| <<nvme_rdma_conn_established>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1899| <<nvme_rdma_addr_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1953| <<nvme_rdma_route_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2008| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2014| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ */
 static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_device *dev;
@@ -424,12 +695,30 @@ static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 	nvme_rdma_dev_put(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|687| <<nvme_rdma_create_queue_ib>> nvme_rdma_get_max_fr_pages(ibdev), 0);
+ *   - drivers/nvme/host/rdma.c|1016| <<nvme_rdma_configure_admin_queue>> ctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev);
+ */
 static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
 {
 	return min_t(u32, NVME_RDMA_MAX_SEGMENTS,
 		     ibdev->attrs.max_fast_reg_page_list_len);
 }
 
+/*
+ * [0] nvme_rdma_create_queue_ib
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1699| <<nvme_rdma_addr_resolved>> ret = nvme_rdma_create_queue_ib(queue);
+ */
 static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct ib_device *ibdev;
@@ -506,6 +795,20 @@ static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_alloc_queue [nvme_rdma]
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_io_queues>> ret = nvme_rdma_alloc_queue(ctrl, i,
+ *   - drivers/nvme/host/rdma.c|828| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ */
 static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		int idx, size_t queue_size)
 {
@@ -600,6 +903,11 @@ static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
 		nvme_rdma_stop_queue(&ctrl->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|715| <<nvme_rdma_start_io_queues>> ret = nvme_rdma_start_queue(ctrl, i);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_start_queue(ctrl, 0);
+ */
 static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 {
 	struct nvme_rdma_queue *queue = &ctrl->queues[idx];
@@ -621,6 +929,10 @@ static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1203| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_start_io_queues(ctrl);
+ */
 static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	int i, ret = 0;
@@ -639,6 +951,10 @@ static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_alloc_io_queues(ctrl);
+ */
 static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -709,6 +1025,21 @@ static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|848| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ *   - drivers/nvme/host/rdma.c|925| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ */
 static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
 		bool admin)
 {
@@ -767,6 +1098,10 @@ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_queue(&ctrl->queues[0]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1035| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_admin_queue(ctrl, new);
+ */
 static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 		bool new)
 {
@@ -786,6 +1121,16 @@ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	 * It's safe, since any chage in the underlying RDMA device will issue
 	 * error recovery and queue re-creation.
 	 */
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
 			sizeof(struct nvme_command), DMA_TO_DEVICE);
 	if (error)
@@ -860,6 +1205,10 @@ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_io_queues(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1180| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_io_queues(ctrl, new);
+ */
 static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret;
@@ -929,6 +1278,9 @@ static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
 	}
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.free_ctrl = nvme_rdma_free_ctrl()
+ */
 static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
@@ -946,6 +1298,12 @@ static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 	kfree(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1178| <<nvme_rdma_reconnect_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|1197| <<nvme_rdma_error_recovery_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|2114| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ */
 static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 {
 	/* If we are resetting/deleting then do nothing */
@@ -965,6 +1323,30 @@ static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 	}
 }
 
+/*
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1054| <<nvme_rdma_reconnect_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|1939| <<nvme_rdma_reset_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|2071| <<nvme_rdma_create_ctrl>> ret = nvme_rdma_setup_ctrl(ctrl, true);
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ */
 static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret = -EINVAL;
@@ -1027,6 +1409,13 @@ static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 	return ret;
 }
 
+/*
+ * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+ *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+ *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+ */
 static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
@@ -1050,6 +1439,14 @@ static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+ *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+ *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+ */
 static void nvme_rdma_error_recovery_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(work,
@@ -1069,6 +1466,16 @@ static void nvme_rdma_error_recovery_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1251| <<nvme_rdma_wr_error>> nvme_rdma_error_recovery(ctrl);
+ *   - drivers/nvme/host/rdma.c|1627| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1640| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1650| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1900| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1947| <<nvme_rdma_timeout>> nvme_rdma_error_recovery(ctrl);
+ */
 static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
@@ -1201,6 +1608,10 @@ static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1688| <<nvme_rdma_map_data>> ret = nvme_rdma_map_sg_fr(queue, req, c, count);
+ */
 static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_request *req, struct nvme_command *c,
 		int count)
@@ -1208,6 +1619,9 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
 	int nr;
 
+	/*
+	 * req->mr是struct ib_mr
+	 */
 	req->mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);
 	if (WARN_ON_ONCE(!req->mr))
 		return -EAGAIN;
@@ -1247,6 +1661,10 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1828| <<nvme_rdma_queue_rq>> err = nvme_rdma_map_data(queue, rq, c);
+ */
 static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		struct request *rq, struct nvme_command *c)
 {
@@ -1279,6 +1697,12 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		goto out_free_table;
 	}
 
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	if (count <= dev->num_inline_segments) {
 		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
 		    queue->ctrl->use_inline_data &&
@@ -1310,6 +1734,17 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_send_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1327,6 +1762,11 @@ static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1835| <<nvme_rdma_submit_async_event>> ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
+ *   - drivers/nvme/host/rdma.c|2248| <<nvme_rdma_queue_rq>> err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
+ */
 static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
 		struct ib_send_wr *first)
@@ -1358,6 +1798,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1641| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1653| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1376,6 +1821,10 @@ static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 	wr.sg_list  = &list;
 	wr.num_sge  = 1;
 
+	/*
+	 * Posts a list of work requests to the receive queue of
+	 *   the specified QP.
+	 */
 	ret = ib_post_recv(queue->qp, &wr, NULL);
 	if (unlikely(ret)) {
 		dev_err(queue->ctrl->ctrl.device,
@@ -1399,6 +1848,9 @@ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_rdma_wr_error(cq, wc, "ASYNC");
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.submit_async_event = nvme_rdma_submit_async_event()
+ */
 static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
@@ -1426,6 +1878,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1564| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1470,6 +1926,20 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * [0] nvme_rdma_recv_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1779| <<nvme_rdma_post_recv>> qe->cqe.done = nvme_rdma_recv_done;
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1502,6 +1972,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1712| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -1545,6 +2019,10 @@ static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
 	return -ECONNRESET;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1706| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_addr_resolved(queue);
+ */
 static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -1568,6 +2046,10 @@ static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1709| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_route_resolved(queue);
+ */
 static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
@@ -1618,6 +2100,34 @@ static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_ib_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|580| <<nvme_rdma_alloc_queue>> queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+ */
 static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *ev)
 {
@@ -1629,6 +2139,18 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		ev->status, cm_id);
 
 	switch (ev->event) {
+		/*
+		 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 */
 	case RDMA_CM_EVENT_ADDR_RESOLVED:
 		cm_error = nvme_rdma_addr_resolved(queue);
 		break;
@@ -1679,6 +2201,10 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.timeout = nvme_rdma_timeout()
+ * struct blk_mq_ops nvme_rdma_mq_ops.timeout = nvme_rdma_timeout()
+ */
 static enum blk_eh_timer_return
 nvme_rdma_timeout(struct request *rq, bool reserved)
 {
@@ -1707,6 +2233,10 @@ nvme_rdma_timeout(struct request *rq, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ */
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1715,6 +2245,15 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct request *rq = bd->rq;
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 	struct nvme_rdma_qe *sqe = &req->sqe;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 *
+	 * 注意, c指向的req->sqe.data!!!!!!!!
+	 */
 	struct nvme_command *c = sqe->data;
 	struct ib_device *dev;
 	bool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);
@@ -1728,6 +2267,17 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	dev = queue->device->dev;
 
+	/*
+	 * req是struct nvme_rdma_request类型
+	 * 来自blk_mq_rq_to_pdu(rq)
+	 *
+	 * 这里req->sqe.dma对应的req->sqe.data (存储的nvme_command c)
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.dma = ib_dma_map_single(dev, req->sqe.data,
 					 sizeof(struct nvme_command),
 					 DMA_TO_DEVICE);
@@ -1777,6 +2327,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.poll = nvme_rdma_poll()
+ */
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1784,6 +2337,10 @@ static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 	return ib_process_cq_direct(queue->ib_cq, -1);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.complete = nvme_rdma_complete_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.complete = nvme_rdma_complete_rq()
+ */
 static void nvme_rdma_complete_rq(struct request *rq)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
@@ -1796,6 +2353,9 @@ static void nvme_rdma_complete_rq(struct request *rq)
 	nvme_complete_rq(rq);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.map_queues = nvme_rdma_map_queues()
+ */
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
@@ -1843,6 +2403,10 @@ static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1854,6 +2418,10 @@ static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.poll		= nvme_rdma_poll,
 };
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1863,6 +2431,11 @@ static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.timeout	= nvme_rdma_timeout,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1975| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ *   - drivers/nvme/host/rdma.c|1988| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_shutdown_ctrl(ctrl, false);
+ */
 static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 {
 	cancel_work_sync(&ctrl->err_work);
@@ -1876,11 +2449,18 @@ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.delete_ctrl = nvme_rdma_delete_ctrl()
+ */
 static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|2050| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ */
 static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl =
@@ -1930,6 +2510,10 @@ static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
  * The ports don't need to be compared as they are intrinsically
  * already matched by the port pointers supplied.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2021| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+ */
 static bool
 nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 {
@@ -1947,9 +2531,25 @@ nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 	return found;
 }
 
+/*
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1039| <<nvmf_create_ctrl>> ctrl = ops->create_ctrl(dev, opts);
+ *
+ * struct nvmf_transport_ops nvme_rdma_transport.create_ctrl = nvme_ctrl_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
+	/*
+	 * 包含struct nvme_ctrl和struct blk_mq_tag_set (不是指针)
+	 */
 	struct nvme_rdma_ctrl *ctrl;
 	int ret;
 	bool changed;
@@ -1970,6 +2570,9 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		opts->mask |= NVMF_OPT_TRSVCID;
 	}
 
+	/*
+	 * convert an IPv4/IPv6 and port to socket address
+	 */
 	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
 			opts->traddr, opts->trsvcid, &ctrl->addr);
 	if (ret) {
@@ -1988,14 +2591,60 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		}
 	}
 
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts))
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts))
+	 *
+	 * nvme_rdma_existing_controller()"
+	 * Fails a connection request if it matches an existing controller
+	 * (association) with the same tuple:
+	 * <Host NQN, Host ID, local address, remote address, remote port, SUBSYS NQN>
+	 */
 	if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
 		ret = -EALREADY;
 		goto out_free_ctrl;
 	}
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	INIT_DELAYED_WORK(&ctrl->reconnect_work,
 			nvme_rdma_reconnect_ctrl_work);
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
 
 	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
@@ -2009,6 +2658,11 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	if (!ctrl->queues)
 		goto out_free_ctrl;
 
+	/*
+	 * 下面的命令到这里的时候queue_count是5
+	 * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+	 */
+
 	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
 				0 /* no quirks, we're perfect! */);
 	if (ret)
@@ -2024,9 +2678,16 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs\n",
 		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
 
+	/* 不知道在哪里put */
 	nvme_get_ctrl(&ctrl->ctrl);
 
 	mutex_lock(&nvme_rdma_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_rdma_ctrl_list:
+	 *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+	 *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 */
 	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
 	mutex_unlock(&nvme_rdma_ctrl_mutex);
 
@@ -2055,6 +2716,20 @@ static struct nvmf_transport_ops nvme_rdma_transport = {
 	.create_ctrl	= nvme_rdma_create_ctrl,
 };
 
+/*
+ * 在nvme_rdma_init_module()被用于ib_register_client - Register an IB client
+ *
+ * Upper level users of the IB drivers can use ib_register_client() to
+ * register callbacks for IB device addition and removal.  When an IB
+ * device is added, each registered client's add method will be called
+ * (in the order the clients were registered), and when a device is
+ * removed, each client's remove method will be called (in the reverse
+ * order that clients were registered).  In addition, when
+ * ib_register_client() is called, the client will receive an add
+ * callback for all devices already registered.
+ *
+ * struct ib_client nvme_rdma_ib_client.remove = nvme_rdma_remove_one()
+ */
 static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
 {
 	struct nvme_rdma_ctrl *ctrl;
@@ -2094,6 +2769,18 @@ static int __init nvme_rdma_init_module(void)
 {
 	int ret;
 
+	/*
+	 * ib_register_client - Register an IB client
+	 *
+	 * Upper level users of the IB drivers can use ib_register_client() to
+	 * register callbacks for IB device addition and removal.  When an IB
+	 * device is added, each registered client's add method will be called
+	 * (in the order the clients were registered), and when a device is
+	 * removed, each client's remove method will be called (in the reverse
+	 * order that clients were registered).  In addition, when
+	 * ib_register_client() is called, the client will receive an add
+	 * callback for all devices already registered.
+	 */
 	ret = ib_register_client(&nvme_rdma_ib_client);
 	if (ret)
 		return ret;
diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 3a67e24..bc0f841 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -260,6 +260,13 @@ void nvmet_port_send_ana_event(struct nvmet_port *port)
 	up_read(&nvmet_config_sem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2575| <<nvmet_fc_init_module>> return nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);
+ *   - drivers/nvme/target/loop.c|687| <<nvme_loop_init_module>> ret = nvmet_register_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/rdma.c|1667| <<nvmet_rdma_init>> ret = nvmet_register_transport(&nvmet_rdma_ops);
+ *   - drivers/nvme/target/tcp.c|1719| <<nvmet_tcp_init>> ret = nvmet_register_transport(&nvmet_tcp_ops);
+ */
 int nvmet_register_transport(const struct nvmet_fabrics_ops *ops)
 {
 	int ret = 0;
diff --git a/include/rdma/rdma_cm.h b/include/rdma/rdma_cm.h
index 71f48cf..b0cb2a0 100644
--- a/include/rdma/rdma_cm.h
+++ b/include/rdma/rdma_cm.h
@@ -45,6 +45,18 @@
  * RDMA identifier and release all resources allocated with the device.
  */
 enum rdma_cm_event_type {
+	/*
+	 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 */
 	RDMA_CM_EVENT_ADDR_RESOLVED,
 	RDMA_CM_EVENT_ADDR_ERROR,
 	RDMA_CM_EVENT_ROUTE_RESOLVED,
-- 
2.7.4

