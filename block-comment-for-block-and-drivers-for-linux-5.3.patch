From 3ccabeb9202b38e5717c910d8831eaa49460c165 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 9 Oct 2019 16:18:22 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/badblocks.c                   |  26 ++
 block/bio-integrity.c               |   8 +
 block/bio.c                         |  18 +
 block/blk-cgroup.c                  |  11 +
 block/blk-flush.c                   |   5 +
 block/blk-iolatency.c               |   4 +
 block/blk-merge.c                   |  55 +++
 block/blk-mq-cpumap.c               |  41 +++
 block/blk-mq-pci.c                  |  32 ++
 block/blk-mq-rdma.c                 |  18 +
 block/blk-mq-sched.c                |   4 +
 block/blk-mq-tag.c                  |  77 ++++
 block/blk-mq-tag.h                  |  23 ++
 block/blk-mq-virtio.c               |  31 ++
 block/blk-mq.c                      | 244 +++++++++++++
 block/blk-mq.h                      |  16 +
 block/blk-rq-qos.c                  |  15 +
 block/blk-rq-qos.h                  |   4 +
 block/blk-settings.c                |  17 +
 block/blk-softirq.c                 |  27 ++
 block/blk-stat.c                    |  14 +
 block/blk-timeout.c                 |  42 +++
 block/blk-wbt.c                     |  10 +
 block/blk.h                         |  16 +
 block/bounce.c                      |  12 +
 block/partitions/check.c            |   4 +
 drivers/nvme/host/core.c            |   8 +
 drivers/nvme/host/fabrics.c         |  11 +
 drivers/nvme/host/fabrics.h         |   9 +
 drivers/nvme/host/fault_inject.c    |   4 +
 drivers/nvme/host/nvme.h            |  27 ++
 drivers/nvme/host/rdma.c            | 687 ++++++++++++++++++++++++++++++++++++
 drivers/nvme/target/core.c          |  67 ++++
 drivers/nvme/target/io-cmd-file.c   |  35 ++
 drivers/nvme/target/loop.c          |  56 +++
 drivers/nvme/target/nvmet.h         |  11 +
 drivers/scsi/fcoe/fcoe.c            |  10 +
 drivers/scsi/fcoe/fcoe_transport.c  |   5 +
 drivers/scsi/scsi_transport_fc.c    |  22 ++
 drivers/scsi/scsi_transport_iscsi.c |  10 +
 include/linux/bio.h                 | 170 +++++++++
 include/linux/blk-mq.h              | 113 ++++++
 include/linux/blk_types.h           |  33 ++
 include/linux/blkdev.h              |  34 ++
 include/linux/bvec.h                | 136 +++++++
 include/rdma/rdma_cm.h              |  12 +
 lib/sbitmap.c                       |  14 +
 47 files changed, 2248 insertions(+)

diff --git a/block/badblocks.c b/block/badblocks.c
index 2e5f569..e1c1eac 100644
--- a/block/badblocks.c
+++ b/block/badblocks.c
@@ -50,6 +50,13 @@
  * -1: there are bad blocks which have not yet been acknowledged in metadata.
  * plus the start/length of the first bad section we overlap.
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|1176| <<null_handle_cmd>> if (!is_flush && badblocks_check(&nullb->dev->badblocks, sector,
+ *   - drivers/md/md.h|214| <<is_badblock>> int rv = badblocks_check(&rdev->badblocks, rdev->data_offset + s,
+ *   - drivers/nvdimm/nd.h|425| <<is_bad_pmem>> return !!badblocks_check(bb, sector, len / 512, &first_bad,
+ *   - drivers/nvdimm/pfn_devs.c|383| <<nd_pfn_clear_memmap_errors>> bb_present = badblocks_check(&nd_region->bb, meta_start,
+ */
 int badblocks_check(struct badblocks *bb, sector_t s, int sectors,
 			sector_t *first_bad, int *bad_sectors)
 {
@@ -160,6 +167,14 @@ static void badblocks_update_acked(struct badblocks *bb)
  *  0: success
  *  1: failed to set badblocks (out of space)
  */
+/*
+ * called by:
+ *   - block/badblocks.c|537| <<badblocks_store>> if (badblocks_set(bb, sector, length, !unack))
+ *   - drivers/block/null_blk_main.c|384| <<nullb_device_badblocks_store>> ret = badblocks_set(&t_dev->badblocks, start,
+ *   - drivers/md/md.c|1637| <<super_1_load>> if (badblocks_set(&rdev->badblocks, sector, count, 1))
+ *   - drivers/md/md.c|9137| <<rdev_set_badblocks>> rv = badblocks_set(&rdev->badblocks, s, sectors, 0);
+ *   - drivers/nvdimm/badrange.c|170| <<set_badblock>> if (badblocks_set(bb, s, num, 1))
+ */
 int badblocks_set(struct badblocks *bb, sector_t s, int sectors,
 			int acknowledged)
 {
@@ -514,6 +529,12 @@ EXPORT_SYMBOL_GPL(badblocks_show);
  * Return:
  *  Length of the buffer processed or -ve error.
  */
+/*
+ * called by:
+ *   - block/genhd.c|851| <<disk_badblocks_store>> return badblocks_store(disk->bb, page, len, 0);
+ *   - drivers/md/md.c|3322| <<bb_store>> int rv = badblocks_store(&rdev->badblocks, page, len, 0);
+ *   - drivers/md/md.c|3337| <<ubb_store>> return badblocks_store(&rdev->badblocks, page, len, 1);
+ */
 ssize_t badblocks_store(struct badblocks *bb, const char *page, size_t len,
 			int unack)
 {
@@ -572,6 +593,11 @@ static int __badblocks_init(struct device *dev, struct badblocks *bb,
  *  0: success
  *  -ve errno: on error
  */
+/*
+ * called by:
+ *   - drivers/block/null_blk_main.c|514| <<null_alloc_dev>> if (badblocks_init(&dev->badblocks, 0)) {
+ *   - drivers/md/md.c|3503| <<md_rdev_init>> return badblocks_init(&rdev->badblocks, 0);
+ */
 int badblocks_init(struct badblocks *bb, int enable)
 {
 	return __badblocks_init(NULL, bb, enable);
diff --git a/block/bio-integrity.c b/block/bio-integrity.c
index fb95dbb..6bb2874 100644
--- a/block/bio-integrity.c
+++ b/block/bio-integrity.c
@@ -34,6 +34,14 @@ void blk_flush_integrity(void)
  * metadata.  nr_vecs specifies the maximum number of pages containing
  * integrity metadata that can be attached.
  */
+/*
+ * called by:
+ *   - block/bio-integrity.c|249| <<bio_integrity_prep>> bip = bio_integrity_alloc(bio, GFP_NOIO, nr_pages);
+ *   - block/bio-integrity.c|414| <<bio_integrity_clone>> bip = bio_integrity_alloc(bio, gfp_mask, bip_src->bip_vcnt);
+ *   - drivers/md/dm-crypt.c|1003| <<dm_crypt_integrity_io_alloc>> bip = bio_integrity_alloc(bio, GFP_NOIO, 1);
+ *   - drivers/nvme/host/core.c|838| <<nvme_add_user_metadata>> bip = bio_integrity_alloc(bio, GFP_KERNEL, 1);
+ *   - drivers/target/target_core_iblock.c|641| <<iblock_alloc_bip>> bip = bio_integrity_alloc(bio, GFP_NOIO,
+ */
 struct bio_integrity_payload *bio_integrity_alloc(struct bio *bio,
 						  gfp_t gfp_mask,
 						  unsigned int nr_vecs)
diff --git a/block/bio.c b/block/bio.c
index 299a0e7..53e7ec7 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -298,6 +298,11 @@ void bio_reset(struct bio *bio)
 }
 EXPORT_SYMBOL(bio_reset);
 
+/*
+ * called by:
+ *   - block/bio.c|313| <<bio_chain_endio>> bio_endio(__bio_chain_endio(bio));
+ *   - block/bio.c|1816| <<bio_endio>> bio = __bio_chain_endio(bio);
+ */
 static struct bio *__bio_chain_endio(struct bio *bio)
 {
 	struct bio *parent = bio->bi_private;
@@ -1348,6 +1353,10 @@ struct bio *bio_copy_user_iov(struct request_queue *q,
  *	Map the user space address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
+/*
+ * called by:
+ *   - block/blk-map.c|75| <<__blk_rq_map_user_iov>> bio = bio_map_user_iov(q, iter, gfp_mask);
+ */
 struct bio *bio_map_user_iov(struct request_queue *q,
 			     struct iov_iter *iter,
 			     gfp_t gfp_mask)
@@ -1356,6 +1365,9 @@ struct bio *bio_map_user_iov(struct request_queue *q,
 	struct bio *bio;
 	int ret;
 
+	/*
+	 * 返回iov_iter->count
+	 */
 	if (!iov_iter_count(iter))
 		return ERR_PTR(-EINVAL);
 
@@ -1845,6 +1857,12 @@ EXPORT_SYMBOL(bio_endio);
  * to @bio's bi_io_vec; it is the caller's responsibility to ensure that
  * @bio is not freed before the split.
  */
+/*
+ * 制作一个新的bio (split)
+ * split是从start到参数的sectors
+ * bio则变成从参数的sectors到最后
+ * 返回split
+ */
 struct bio *bio_split(struct bio *bio, int sectors,
 		      gfp_t gfp, struct bio_set *bs)
 {
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index 55a7dc2..aa7fa95 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -1165,6 +1165,10 @@ blkcg_css_alloc(struct cgroup_subsys_state *parent_css)
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|535| <<blk_alloc_queue_node>> if (blkcg_init_queue(q))
+ */
 int blkcg_init_queue(struct request_queue *q)
 {
 	struct blkcg_gq *new_blkg, *blkg;
@@ -1436,6 +1440,13 @@ EXPORT_SYMBOL_GPL(blkcg_deactivate_policy);
  * Register @pol with blkcg core.  Might sleep and @pol may be modified on
  * successful registration.  Returns 0 on success and -errno on failure.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1513| <<global>> EXPORT_SYMBOL_GPL(blkcg_policy_register);
+ *   - block/bfq-iosched.c|6762| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+ *   - block/blk-iolatency.c|1045| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-throttle.c|2482| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+ */
 int blkcg_policy_register(struct blkcg_policy *pol)
 {
 	struct blkcg *blkcg;
diff --git a/block/blk-flush.c b/block/blk-flush.c
index aedd932..33c3f92 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -346,6 +346,11 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2001| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index 0fff7b5..7a1394e 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -714,6 +714,10 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	rcu_read_unlock();
 }
 
+/*
+ * called by only:
+ *   - block/blk-cgroup.c|1193| <<blkcg_init_queue>> ret = blk_iolatency_init(q);
+ */
 int blk_iolatency_init(struct request_queue *q)
 {
 	struct blk_iolatency *blkiolat;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 57f7990..6afb995 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -12,11 +12,17 @@
 
 #include "blk.h"
 
+/*
+ * called by:
+ *   - block/blk-merge.c|53| <<req_gap_back_merge>> return bio_will_gap(req->q, req, req->biotail, bio);
+ *   - block/blk-merge.c|58| <<req_gap_front_merge>> return bio_will_gap(req->q, NULL, bio, req->bio);
+ */
 static inline bool bio_will_gap(struct request_queue *q,
 		struct request *prev_rq, struct bio *prev, struct bio *next)
 {
 	struct bio_vec pb, nb;
 
+	/* 如果没设置virt_boundary就一定返回false */
 	if (!bio_has_data(prev) || !queue_virt_boundary(q))
 		return false;
 
@@ -135,6 +141,10 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
 static inline unsigned get_max_io_size(struct request_queue *q,
 				       struct bio *bio)
 {
+	/*
+	 * Return maximum size of a request at given offset. Only valid for
+	 * file system requests.
+	 */
 	unsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);
 	unsigned mask = queue_logical_block_size(q) - 1;
 
@@ -161,6 +171,12 @@ static unsigned get_max_segment_size(struct request_queue *q,
  * Split the bvec @bv into segments, and update all kinds of
  * variables.
  */
+/*
+ * called by:
+ *   - block/blk-merge.c|231| <<blk_bio_segment_split>> bvec_split_segs(q, &bv, &nsegs,
+ *   - block/blk-merge.c|246| <<blk_bio_segment_split>> } else if (bvec_split_segs(q, &bv, &nsegs, &sectors,
+ *   - block/blk-merge.c|330| <<blk_recalc_rq_segments>> bvec_split_segs(rq->q, &bv, &nr_phys_segs, NULL, UINT_MAX);
+ */
 static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 		unsigned *nsegs, unsigned *sectors, unsigned max_segs)
 {
@@ -194,6 +210,10 @@ static bool bvec_split_segs(struct request_queue *q, struct bio_vec *bv,
 	return !!len;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|278| <<__blk_queue_split>> split = blk_bio_segment_split(q, *bio, &q->bio_split, nr_segs);
+ */
 static struct bio *blk_bio_segment_split(struct request_queue *q,
 					 struct bio *bio,
 					 struct bio_set *bs,
@@ -205,6 +225,9 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	const unsigned max_sectors = get_max_io_size(q, bio);
 	const unsigned max_segs = queue_max_segments(q);
 
+	/*
+	 * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+	 */
 	bio_for_each_bvec(bv, bio, iter) {
 		/*
 		 * If the queue doesn't support SG gaps and adding this
@@ -247,9 +270,20 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return NULL;
 split:
 	*segs = nsegs;
+	/*
+	 * 制作一个新的bio (split)
+	 * split是从start到参数的sectors
+	 * bio则变成从参数的sectors到最后
+	 * 返回split
+	 */
 	return bio_split(bio, sectors, GFP_NOIO, bs);
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|307| <<blk_queue_split>> __blk_queue_split(q, bio, &nr_segs);
+ *   - block/blk-mq.c|1999| <<blk_mq_make_request>> __blk_queue_split(q, &bio, &nr_segs);
+ */
 void __blk_queue_split(struct request_queue *q, struct bio **bio,
 		unsigned int *nr_segs)
 {
@@ -269,6 +303,13 @@ void __blk_queue_split(struct request_queue *q, struct bio **bio,
 				nr_segs);
 		break;
 	default:
+		/*
+		 * 如果需要split, 则调用bio_split()
+		 *     制作一个新的bio (split)
+		 *     split是从start到参数的sectors
+		 *     bio则变成从参数的sectors到最后
+		 *     返回split
+		 */
 		split = blk_bio_segment_split(q, *bio, &q->bio_split, nr_segs);
 		break;
 	}
@@ -285,8 +326,22 @@ void __blk_queue_split(struct request_queue *q, struct bio **bio,
 		 * that will never happen, as we're already holding a
 		 * reference to it.
 		 */
+		/*
+		 * 在以下使用BIO_QUEUE_ENTERED:
+		 *   - block/blk-merge.c|294| <<__blk_queue_split>> bio_set_flag(*bio, BIO_QUEUE_ENTERED);
+		 *   - include/linux/blk-cgroup.h|751| <<blkcg_bio_issue_check>> if (!bio_flagged(bio, BIO_QUEUE_ENTERED))
+		 */
 		bio_set_flag(*bio, BIO_QUEUE_ENTERED);
 
+		/*
+		 * The caller won't have a bi_end_io called when @bio completes - instead,
+		 * @parent's bi_end_io won't be called until both @parent and @bio have
+		 * completed; the chained bio will also be freed when it completes.
+		 *
+		 * *bio是parent
+		 *
+		 * 查看bio_endio()
+		 */
 		bio_chain(split, *bio);
 		trace_block_split(q, split, (*bio)->bi_iter.bi_sector);
 		generic_make_request(*bio);
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index f945621..baae24a 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,12 +15,21 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|49| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ *   - block/blk-mq-cpumap.c|53| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ */
 static int cpu_to_queue_index(struct blk_mq_queue_map *qmap,
 			      unsigned int nr_queues, const int cpu)
 {
 	return qmap->queue_offset + (cpu % nr_queues);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|51| <<blk_mq_map_queues>> first_sibling = get_first_sibling(cpu);
+ */
 static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
@@ -32,9 +41,35 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3049| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3321| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|453| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|2394| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2151| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2152| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7124| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1766| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	unsigned int nr_queues = qmap->nr_queues;
 	unsigned int cpu, first_sibling;
 
@@ -68,6 +103,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2116| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2172| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2815| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94..e38ddf4 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -23,17 +23,49 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|451| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7126| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5806| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
 		mask = pci_irq_get_affinity(pdev, queue + offset);
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e..b96945e 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2382| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|2384| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
@@ -32,6 +37,19 @@ int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 */
 		for_each_cpu(cpu, mask)
 			map->mq_map[cpu] = map->queue_offset + queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index c9d183d..6b6bff3 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -167,6 +167,10 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1394| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index da19f0b..8e01c6d 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -14,6 +14,12 @@
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
 
+/*
+ * called by:
+ *   - block/blk-mq.c|280| <<blk_mq_can_queue>> return blk_mq_has_free_tags(hctx->tags);
+ *
+ * 但是没人调用blk_mq_can_queue()
+ */
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
 	if (!tags)
@@ -28,8 +34,19 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|61| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 修改和使用active_queues的地方 (比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发):
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -40,6 +57,15 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|91| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|275| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ *
+ * 猜测唤醒的是
+ * blk_mq_get_tag()
+ *  -> sbitmap_prepare_to_wait()
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -51,6 +77,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|78| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -58,8 +88,20 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		return;
 
+	/*
+	 * 修改和使用active_queues的地方 (比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发):
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 */
 	atomic_dec(&tags->active_queues);
 
+	/*
+	 * 猜测唤醒的是
+	 * blk_mq_get_tag()
+	 *  -> sbitmap_prepare_to_wait()
+	 */
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
@@ -67,6 +109,10 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|111| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -94,6 +140,12 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|176| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|198| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|204| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
@@ -106,6 +158,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|395| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1064| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
@@ -456,6 +513,15 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3186| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3189| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ *
+ * queue_requests_store()
+ *  -> blk_mq_update_nr_requests()
+ *      -> blk_mq_tag_update_depth()
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -521,6 +587,17 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/nbd.c|170| <<nbd_cmd_handle>> u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/nvme/host/nvme.h|134| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|693| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|859| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|3698| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5620| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5291| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|487| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..24827d1 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -11,6 +11,15 @@ struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 修改和使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *
+	 * 比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -36,6 +45,10 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1123| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -53,6 +66,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|385| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1056| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -61,6 +79,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|955| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2291| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 4883416..a4a0c54 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,6 +21,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|697| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|658| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
@@ -30,11 +35,37 @@ int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 	if (!vdev->config->get_vq_affinity)
 		goto fallback;
 
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	for (queue = 0; queue < qmap->nr_queues; queue++) {
 		mask = vdev->config->get_vq_affinity(vdev, first_vec + queue);
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 0835f4d..11f47a8 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -265,6 +265,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|320| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -275,6 +279,9 @@ void blk_mq_wake_waiters(struct request_queue *q)
 			blk_mq_tag_wakeup_all(hctx->tags, true);
 }
 
+/*
+ * 没人调用!
+ */
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 {
 	return blk_mq_has_free_tags(hctx->tags);
@@ -533,6 +540,11 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 	if (blk_mq_need_time_stamp(rq))
 		now = ktime_get_ns();
 
+	/*
+	 * 在以下使用RQF_STATS:
+	 *   - block/blk-mq.c|543| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+	 *   - block/blk-mq.c|706| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+	 */
 	if (rq->rq_flags & RQF_STATS) {
 		blk_mq_poll_stats_start(rq->q);
 		blk_stat_add(rq, now);
@@ -568,6 +580,10 @@ static void __blk_mq_complete_request_remote(void *data)
 	q->mq_ops->complete(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|657| <<blk_mq_complete_request>> __blk_mq_complete_request(rq);
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -643,6 +659,22 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 调用blk_mq_complete_request()的部分例子:
+ *   - block/bsg-lib.c|186| <<bsg_job_done>> blk_mq_complete_request(blk_mq_rq_from_pdu(job));
+ *   - drivers/block/loop.c|497| <<lo_rw_aio_do_completion>> blk_mq_complete_request(rq);
+ *   - drivers/block/loop.c|1933| <<loop_handle_cmd>> blk_mq_complete_request(rq);
+ *   - drivers/block/nbd.c|402| <<nbd_xmit_timeout>> blk_mq_complete_request(req);
+ *   - drivers/block/nbd.c|744| <<recv_work>> blk_mq_complete_request(blk_mq_rq_from_pdu(cmd));
+ *   - drivers/block/nbd.c|757| <<nbd_clear_req>> blk_mq_complete_request(req);
+ *   - drivers/block/null_blk_main.c|1224| <<null_handle_cmd>> blk_mq_complete_request(cmd->rq);
+ *   - drivers/block/null_blk_main.c|1315| <<null_timeout_rq>> blk_mq_complete_request(rq);
+ *   - drivers/block/virtio_blk.c|244| <<virtblk_done>> blk_mq_complete_request(req);
+ *   - drivers/block/xen-blkfront.c|1648| <<blkif_interrupt>> blk_mq_complete_request(req);
+ *   - drivers/md/dm-rq.c|291| <<dm_complete_request>> blk_mq_complete_request(rq);
+ *   - drivers/nvme/host/nvme.h|434| <<nvme_end_request>> blk_mq_complete_request(req);
+ *   - drivers/scsi/scsi_lib.c|1607| <<scsi_mq_done>> if (unlikely(!blk_mq_complete_request(cmd->request)))
+ */
 bool blk_mq_complete_request(struct request *rq)
 {
 	if (unlikely(blk_should_fake_timeout(rq->q)))
@@ -676,6 +708,11 @@ void blk_mq_start_request(struct request *rq)
 #ifdef CONFIG_BLK_DEV_THROTTLING_LOW
 		rq->throtl_size = blk_rq_sectors(rq);
 #endif
+		/*
+		 * 在以下使用RQF_STATS:
+		 *   - block/blk-mq.c|543| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+		 *   - block/blk-mq.c|706| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+		 */
 		rq->rq_flags |= RQF_STATS;
 		rq_qos_issue(q, rq);
 	}
@@ -1388,10 +1425,32 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1498| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+ *   - block/blk-mq.c|2719| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+ *
+ * 有可能返回>= nr_cpu_ids
+ */
 static inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * return the first cpu from *srcp1 & *srcp2
+	 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
+	 *
+	 * 设置cpumask的地方:
+	 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *
+	 * 表示这个hctx都map了那些sw cpu
+	 */
 	int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
 
+	/*
+	 * 关于cpumask_first():
+	 * get the first cpu in a cpumask
+	 * Returns >= nr_cpu_ids if no cpus set.
+	 */
 	if (cpu >= nr_cpu_ids)
 		cpu = cpumask_first(hctx->cpumask);
 	return cpu;
@@ -1403,20 +1462,57 @@ static inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx *hctx)
  * For now we just round-robin here, switching for every
  * BLK_MQ_CPU_WORK_BATCH queued items.
  */
+/*
+ * used by:
+ *   - block/blk-mq.c|1500| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ */
 static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下设置next_cpu:
+	 *   - block/blk-mq.c|1474| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|1479| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|2626| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 * 在以下使用next_cpu:
+	 *   - block/blk-mq.c|1408| <<__blk_mq_run_hw_queue>> cpu_online(hctx->next_cpu)) {
+	 *   - block/blk-mq.c|1446| <<blk_mq_hctx_next_cpu>> int next_cpu = hctx->next_cpu;
+	 *   - block/blk-mq.c|1453| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1455| <<blk_mq_hctx_next_cpu>> if (next_cpu >= nr_cpu_ids)
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 *   - block/blk-mq.c|1464| <<blk_mq_hctx_next_cpu>> if (!cpu_online(next_cpu)) {
+	 *   - block/blk-mq.c|1480| <<blk_mq_hctx_next_cpu>> return next_cpu;
+	 */
 	bool tried = false;
 	int next_cpu = hctx->next_cpu;
 
 	if (hctx->queue->nr_hw_queues == 1)
 		return WORK_CPU_UNBOUND;
 
+	/*
+	 * 在以下使用next_cpu_batch:
+	 *   - block/blk-mq.c|1469| <<blk_mq_hctx_next_cpu>> if (--hctx->next_cpu_batch <= 0) {
+	 *   - block/blk-mq.c|1475| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|1493| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = 1;
+	 *   - block/blk-mq.c|2645| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	if (--hctx->next_cpu_batch <= 0) {
 select_cpu:
+		/*
+		 * 设置cpumask的地方:
+		 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+		 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+		 *
+		 * 表示这个hctx都map了那些sw cpu
+		 */
 		next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
 				cpu_online_mask);
 		if (next_cpu >= nr_cpu_ids)
 			next_cpu = blk_mq_first_mapped_cpu(hctx);
+		/*
+		 * 在以下使用BLK_MQ_CPU_WORK_BATCH = 8:
+		 *   - block/blk-mq.c|1499| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+		 *   - block/blk-mq.c|2674| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+		 */
 		hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
 	}
 
@@ -1443,6 +1539,11 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1553| <<blk_mq_delay_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, true, msecs);
+ *   - block/blk-mq.c|1576| <<blk_mq_run_hw_queue>> __blk_mq_delay_run_hw_queue(hctx, async, 0);
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
@@ -1464,12 +1565,34 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 				    msecs_to_jiffies(msecs));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1369| <<blk_mq_dispatch_rq_list>> blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);
+ *   - drivers/ide/ide-io.c|453| <<ide_requeue_and_plug>> blk_mq_delay_run_hw_queue(q->queue_hw_ctx[0], 3);
+ *   - drivers/scsi/scsi_lib.c|1628| <<scsi_mq_get_budget>> blk_mq_delay_run_hw_queue(hctx, SCSI_QUEUE_DELAY);
+ */
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|80| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|414| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|448| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|203| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1124| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1367| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1593| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1658| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1678| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1758| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2096| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2356| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1497,6 +1620,20 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 }
 EXPORT_SYMBOL(blk_mq_run_hw_queue);
 
+/*
+ * 部分调用blk_mq_run_hw_queues()的例子:
+ *   - block/bfq-iosched.c|425| <<bfq_schedule_dispatch>> blk_mq_run_hw_queues(bfqd->queue, true);
+ *   - block/blk-mq-debugfs.c|164| <<queue_state_write>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|152| <<blk_freeze_queue_start>> blk_mq_run_hw_queues(q, false);
+ *   - block/blk-mq.c|264| <<blk_mq_unquiesce_queue>> blk_mq_run_hw_queues(q, true);
+ *   - block/blk-mq.c|799| <<blk_mq_requeue_work>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/md/dm-table.c|2138| <<dm_table_run_md_queue_async>> blk_mq_run_hw_queues(queue, true);
+ *   - drivers/scsi/scsi_lib.c|362| <<scsi_kick_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|523| <<scsi_run_queue>> blk_mq_run_hw_queues(q, false);
+ *   - drivers/scsi/scsi_lib.c|619| <<scsi_end_request>> blk_mq_run_hw_queues(q, true);
+ *   - drivers/scsi/scsi_sysfs.c|785| <<store_state_field>> blk_mq_run_hw_queues(sdev->request_queue, true);
+ *   - drivers/scsi/scsi_transport_fc.c|3697| <<fc_bsg_goose_queue>> blk_mq_run_hw_queues(q, true);
+ */
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1652,6 +1789,16 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|391| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|751| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1850| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1866| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ *
+ * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1851,6 +1998,11 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2045| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2050| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1884,6 +2036,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|437| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1897,6 +2053,9 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		if (ret != BLK_STS_OK) {
 			if (ret == BLK_STS_RESOURCE ||
 					ret == BLK_STS_DEV_RESOURCE) {
+				/*
+				 * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+				 */
 				blk_mq_request_bypass_insert(rq,
 							list_empty(list));
 				break;
@@ -1914,6 +2073,13 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		hctx->queue->mq_ops->commit_rqs(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1998| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *   - block/blk-mq.c|2013| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *
+ * 核心思想是把request插入到plug->mq_list
+ */
 static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 {
 	list_add_tail(&rq->queuelist, &plug->mq_list);
@@ -1974,6 +2140,11 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	plug = blk_mq_plug(q, bio);
 	if (unlikely(is_flush_fua)) {
 		/* bypass scheduler for flush rq */
+		/*
+		 * called by:
+		 *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+		 *   - block/blk-mq.c|2001| <<blk_mq_make_request>> blk_insert_flush(rq);
+		 */
 		blk_insert_flush(rq);
 		blk_mq_run_hw_queue(data.hctx, true);
 	} else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs)) {
@@ -1995,6 +2166,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			trace_block_plug(q);
 		}
 
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 	} else if (plug && !blk_queue_nomerges(q)) {
 		/*
@@ -2010,6 +2182,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			list_del_init(&same_queue_rq->queuelist);
 			plug->rq_count--;
 		}
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 		trace_block_plug(q);
 
@@ -2446,6 +2619,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3087| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3477| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2458,7 +2636,15 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/*
+	 * q->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 设置cpumask的地方:
+		 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+		 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+		 */
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
 		hctx->dispatch_from = NULL;
@@ -2484,8 +2670,48 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		}
 
 		ctx = per_cpu_ptr(q->queue_ctx, i);
+		/*
+		 * 设置nr_maps的地方:
+		 *   - block/blk-mq.c|2899| <<blk_mq_init_sq_queue>> set->nr_maps = 1; 
+		 *   - block/blk-mq.c|3234| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3245| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+		 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+		 *   - drivers/nvme/host/pci.c|2257| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2259| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 *   - drivers/nvme/host/rdma.c|1076| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+		 *   - drivers/nvme/host/tcp.c|1472| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+		 *
+		 * struct blk_mq_tag_set
+		 *   - struct blk_mq_queue_map map[HCTX_MAX_TYPES];
+		 *       - unsigned int *mq_map;
+		 *       - unsigned int nr_queues;
+		 *       - unsigned int queue_offset;
+		 *   - unsigned int nr_maps;
+		 */
 		for (j = 0; j < set->nr_maps; j++) {
+			/*
+			 * 在以下修改nr_queues:
+			 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+			 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+			 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+			 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+			 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+			 */
 			if (!set->map[j].nr_queues) {
+				/*
+				 * struct blk_mq_ctx:
+				 *   - unsigned short          index_hw[HCTX_MAX_TYPES];
+				 *   - struct blk_mq_hw_ctx    *hctxs[HCTX_MAX_TYPES];
+				 *
+				 * blk_mq_map_queue_type()
+				 * 返回q->queue_hw_ctx[q->tag_set->map[HCTX_TYPE_DEFAULT].mq_map[i]]
+				 */
 				ctx->hctxs[j] = blk_mq_map_queue_type(q,
 						HCTX_TYPE_DEFAULT, i);
 				continue;
@@ -3123,6 +3349,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3238,6 +3468,10 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3496| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3301,6 +3535,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1168| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2531| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2276| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|1233| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1657| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|508| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 32c62c6..7ae44c7 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -86,6 +86,13 @@ extern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);
  * @type: the hctx type index
  * @cpu: CPU
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2586| <<blk_mq_init_cpu_queues>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2683| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ *   - block/blk-mq.c|2688| <<blk_mq_map_swqueue>> hctx = blk_mq_map_queue_type(q, j, i);
+ *   - block/blk-mq.c|2711| <<blk_mq_map_swqueue>> ctx->hctxs[j] = blk_mq_map_queue_type(q,
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,
 							  enum hctx_type type,
 							  unsigned int cpu)
@@ -225,6 +232,11 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|64| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3202| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
@@ -251,6 +263,10 @@ static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
  *
  * Return current->plug if the bio can be plugged and NULL otherwise
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2003| <<blk_mq_make_request>> plug = blk_mq_plug(q, bio);
+ */
 static inline struct blk_plug *blk_mq_plug(struct request_queue *q,
 					   struct bio *bio)
 {
diff --git a/block/blk-rq-qos.c b/block/blk-rq-qos.c
index 3954c0d..8bf8e19 100644
--- a/block/blk-rq-qos.c
+++ b/block/blk-rq-qos.c
@@ -224,6 +224,11 @@ static int rq_qos_wake_function(struct wait_queue_entry *curr,
  * cleanup_cb is in case that we race with a waker and need to cleanup the
  * inflight count accordingly.
  */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|303| <<__blkcg_iolatency_throttle>> rq_qos_wait(rqw, iolat, iolat_acquire_inflight, iolat_cleanup_cb);
+ *   - block/blk-wbt.c|524| <<__wbt_wait>> rq_qos_wait(rqw, &data, wbt_inflight_cb, wbt_cleanup_cb);
+ */
 void rq_qos_wait(struct rq_wait *rqw, void *private_data,
 		 acquire_inflight_cb_t *acquire_inflight_cb,
 		 cleanup_cb_t *cleanup_cb)
@@ -270,10 +275,20 @@ void rq_qos_wait(struct rq_wait *rqw, void *private_data,
 	finish_wait(&rqw->wait, &data.wq);
 }
 
+/*
+ * called by only:
+ *   - block/blk-core.c|351| <<blk_cleanup_queue>> rq_qos_exit(q);
+ */
 void rq_qos_exit(struct request_queue *q)
 {
 	blk_mq_debugfs_unregister_queue_rqos(q);
 
+	/*
+	 * 设置request_queue->rq_qos的地方:
+	 *   - block/blk-rq-qos.c|279| <<rq_qos_exit>> q->rq_qos = rqos->next;
+	 *   - block/blk-rq-qos.h|98| <<rq_qos_add>> q->rq_qos = rqos;
+	 *   - block/blk-rq-qos.h|112| <<rq_qos_del>> q->rq_qos = cur;
+	 */
 	while (q->rq_qos) {
 		struct rq_qos *rqos = q->rq_qos;
 		q->rq_qos = rqos->next;
diff --git a/block/blk-rq-qos.h b/block/blk-rq-qos.h
index 2300e03..2fc9d5a 100644
--- a/block/blk-rq-qos.h
+++ b/block/blk-rq-qos.h
@@ -92,6 +92,10 @@ static inline void rq_wait_init(struct rq_wait *rq_wait)
 	init_waitqueue_head(&rq_wait->wait);
 }
 
+/*
+ * called by:
+ *   - block/blk-rq-qos.h|95| <<rq_qos_add>> static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
+ */
 static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
 {
 	rqos->next = q->rq_qos;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 2c18312..302c8a4 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -132,6 +132,23 @@ EXPORT_SYMBOL(blk_queue_make_request);
  *    blk_queue_bounce_limit to have lower memory pages allocated as bounce
  *    buffers for doing I/O to pages residing above @max_addr.
  **/
+/*
+ * called by:
+ *   - drivers/block/floppy.c|4569| <<do_floppy_init>> blk_queue_bounce_limit(disks[drive]->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/paride/pcd.c|324| <<pcd_init_units>> blk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/paride/pd.c|925| <<pd_probe_drive>> blk_queue_bounce_limit(p->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/paride/pf.c|311| <<pf_init_units>> blk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/swim.c|849| <<swim_floppy_init>> blk_queue_bounce_limit(swd->unit[drive].disk->queue,
+ *   - drivers/block/swim3.c|1204| <<swim3_attach>> blk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/block/xsysace.c|1020| <<ace_setup>> blk_queue_bounce_limit(ace->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/cdrom/gdrom.c|789| <<probe_gdrom>> blk_queue_bounce_limit(gd.gdrom_rq, BLK_BOUNCE_HIGH);
+ *   - drivers/mmc/core/queue.c|362| <<mmc_setup_queue>> blk_queue_bounce_limit(mq->queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/aha152x.c|2894| <<aha152x_adjust_queue>> blk_queue_bounce_limit(device->request_queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/imm.c|1099| <<imm_adjust_queue>> blk_queue_bounce_limit(device->request_queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/ppa.c|966| <<ppa_adjust_queue>> blk_queue_bounce_limit(device->request_queue, BLK_BOUNCE_HIGH);
+ *   - drivers/scsi/scsi_lib.c|1793| <<__scsi_init_queue>> blk_queue_bounce_limit(q, BLK_BOUNCE_ISA);
+ *   - drivers/usb/storage/scsiglue.c|149| <<slave_configure>> blk_queue_bounce_limit(sdev->request_queue, BLK_BOUNCE_HIGH);
+ */
 void blk_queue_bounce_limit(struct request_queue *q, u64 max_addr)
 {
 	unsigned long b_pfn = max_addr >> PAGE_SHIFT;
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 457d9ba..094ea75 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -14,6 +14,15 @@
 
 #include "blk.h"
 
+/*
+ * 在以下使用blk_cpu_done:
+ *   - block/blk-softirq.c|28| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|49| <<trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|90| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+ *   - block/blk-softirq.c|91| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+ *   - block/blk-softirq.c|130| <<__blk_complete_request>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-softirq.c|152| <<blk_softirq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+ */
 static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
 
 /*
@@ -58,6 +67,10 @@ static void trigger_softirq(void *data)
 /*
  * Setup and invoke a run of 'trigger_softirq' on the given cpu.
  */
+/*
+ * called by:
+ *   - block/blk-softirq.c|141| <<__blk_complete_request>> } else if (raise_blk_irq(ccpu, req))
+ */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
@@ -95,6 +108,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|596| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -148,9 +165,19 @@ static __init int blk_softirq_init(void)
 {
 	int i;
 
+	/*
+	 * 在上面声明
+	 * static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
+	 */
 	for_each_possible_cpu(i)
 		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
 
+	/*
+	 * 触发BLOCK_SOFTIRQ的地方:
+	 *   - block/blk-softirq.c|53| <<trigger_softirq>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-softirq.c|92| <<blk_softirq_cpu_dead>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-softirq.c|140| <<__blk_complete_request>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 */
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
 				  "block/softirq:dead", NULL,
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 940f15d..2774730 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -48,6 +48,10 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|545| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -132,6 +136,11 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3385| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|862| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -180,6 +189,11 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2447| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|431| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 8aa68fa..77b5f9e 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -12,6 +12,12 @@
 
 #ifdef CONFIG_FAIL_IO_TIMEOUT
 
+/*
+ * called by:
+ *   - block/blk-timeout.c|19| <<setup_fail_io_timeout>> return setup_fault_attr(&fail_io_timeout, str);
+ *   - block/blk-timeout.c|28| <<blk_should_fake_timeout>> return should_fail(&fail_io_timeout, 1);
+ *   - block/blk-timeout.c|34| <<fail_io_timeout_debugfs>> NULL, &fail_io_timeout);
+ */
 static DECLARE_FAULT_ATTR(fail_io_timeout);
 
 static int __init setup_fail_io_timeout(char *str)
@@ -22,6 +28,13 @@ __setup("fail_io_timeout=", setup_fail_io_timeout);
 
 int blk_should_fake_timeout(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_FAIL_IO:
+	 *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+	 *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+	 *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+	 *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+	 */
 	if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
 		return 0;
 
@@ -38,6 +51,10 @@ static int __init fail_io_timeout_debugfs(void)
 
 late_initcall(fail_io_timeout_debugfs);
 
+/*
+ * 在block/genhd.c用做:
+ * __ATTR(io-timeout-fail, 0644, part_timeout_show, part_timeout_store);
+ */
 ssize_t part_timeout_show(struct device *dev, struct device_attribute *attr,
 			  char *buf)
 {
@@ -78,6 +95,15 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
  * LLDDs who implement their own error recovery MAY ignore the timeout
  * event if they generated blk_abort_request.
  */
+/*
+ * called by:
+ *   - drivers/ata/libata-eh.c|916| <<ata_qc_schedule_eh>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2617| <<mtip_queue_cmd>> blk_abort_request(req);
+ *   - drivers/s390/block/dasd_ioctl.c|168| <<dasd_ioctl_abortio>> blk_abort_request(cqr->callback_data);
+ *   - drivers/scsi/libsas/sas_ata.c|588| <<sas_ata_task_abort>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/scsi/libsas/sas_scsi_host.c|911| <<sas_task_abort>> blk_abort_request(sc->request);
+ *   - drivers/scsi/scsi_debug.c|4390| <<schedule_resp>> blk_abort_request(cmnd->request);
+ */
 void blk_abort_request(struct request *req)
 {
 	/*
@@ -90,6 +116,10 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
+/*
+ * called by:
+ *   - block/blk-timeout.c|134| <<blk_add_timer>> expiry = blk_rq_timeout(round_jiffies_up(expiry));
+ */
 unsigned long blk_rq_timeout(unsigned long timeout)
 {
 	unsigned long maxt;
@@ -109,6 +139,11 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|685| <<blk_mq_start_request>> blk_add_timer(rq);
+ *   - block/blk-mq.c|853| <<blk_mq_rq_timed_out>> blk_add_timer(req);
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -121,6 +156,13 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
+	/*
+	 * 在以下使用RQF_TIMED_OUT (timeout has been called, don't expire again):
+	 *   - block/blk-mq.c|710| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+	 *   - block/blk-mq.c|843| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+	 *   - block/blk-mq.c|862| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+	 *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+	 */
 	req->rq_flags &= ~RQF_TIMED_OUT;
 
 	expiry = jiffies + req->timeout;
diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index 313f45a..55003f3 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -648,6 +648,11 @@ void wbt_set_write_cache(struct request_queue *q, bool write_cache_on)
 /*
  * Enable wbt if defaults are configured that way
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|995| <<blk_register_queue>> wbt_enable_default(q);
+ *   - block/elevator.c|507| <<elv_unregister_queue>> wbt_enable_default(q);
+ */
 void wbt_enable_default(struct request_queue *q)
 {
 	struct rq_qos *rqos = wbt_rq_qos(q);
@@ -817,6 +822,11 @@ static struct rq_qos_ops wbt_rqos_ops = {
 #endif
 };
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|463| <<queue_wb_lat_store>> ret = wbt_init(q);
+ *   - block/blk-wbt.c|663| <<wbt_enable_default>> wbt_init(q);
+ */
 int wbt_init(struct request_queue *q)
 {
 	struct rq_wb *rwb;
diff --git a/block/blk.h b/block/blk.h
index de6b2e1..45d77a5 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -80,9 +80,17 @@ static inline bool biovec_phys_mergeable(struct request_queue *q,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-merge.c|54| <<bio_will_gap>> return __bvec_gap_to_prev(q, &pb, nb.bv_offset);
+ *   - block/blk.h|107| <<bvec_gap_to_prev>> return __bvec_gap_to_prev(q, bprv, offset);
+ */
 static inline bool __bvec_gap_to_prev(struct request_queue *q,
 		struct bio_vec *bprv, unsigned int offset)
 {
+	/*
+	 * 比如offset是下一个bvec的offset, bprv是上一个的bvec
+	 */
 	return (offset & queue_virt_boundary(q)) ||
 		((bprv->bv_offset + bprv->bv_len) & queue_virt_boundary(q));
 }
@@ -91,6 +99,14 @@ static inline bool __bvec_gap_to_prev(struct request_queue *q,
  * Check if adding a bio_vec after bprv with offset would create a gap in
  * the SG list. Most drivers don't care about this, but some do.
  */
+/*
+ * called by:
+ *   - block/bio-integrity.c|142| <<bio_integrity_add_page>> bvec_gap_to_prev(bio->bi_disk->queue,
+ *   - block/bio.c|717| <<__bio_add_pc_page>> if (bvec_gap_to_prev(q, bvec, offset))
+ *   - block/blk-merge.c|219| <<blk_bio_segment_split>> if (bvprvp && bvec_gap_to_prev(q, bvprvp, bv.bv_offset))
+ *   - block/blk.h|130| <<integrity_req_gap_back_merge>> return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+ *   - block/blk.h|140| <<integrity_req_gap_front_merge>> return bvec_gap_to_prev(req->q, &bip->bip_vec[bip->bip_vcnt - 1],
+ */
 static inline bool bvec_gap_to_prev(struct request_queue *q,
 		struct bio_vec *bprv, unsigned int offset)
 {
diff --git a/block/bounce.c b/block/bounce.c
index f8ed677..38832b1 100644
--- a/block/bounce.c
+++ b/block/bounce.c
@@ -283,6 +283,10 @@ static struct bio *bounce_clone_bio(struct bio *bio_src, gfp_t gfp_mask,
 	return bio;
 }
 
+/*
+ * called by:
+ *   - block/bounce.c|392| <<blk_queue_bounce>> __blk_queue_bounce(q, bio_orig, pool);
+ */
 static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 			       mempool_t *pool)
 {
@@ -295,6 +299,9 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	int sectors = 0;
 	bool passthrough = bio_is_passthrough(*bio_orig);
 
+	/*
+	 * 从bio->bi_iter开始, 遍历每一个最大1个page的bvec
+	 */
 	bio_for_each_segment(from, *bio_orig, iter) {
 		if (i++ < BIO_MAX_PAGES)
 			sectors += from.bv_len >> 9;
@@ -357,6 +364,11 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	*bio_orig = bio;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|25| <<blk_rq_append_bio>> blk_queue_bounce(rq->q, bio);
+ *   - block/blk-mq.c|1998| <<blk_mq_make_request>> blk_queue_bounce(q, &bio);
+ */
 void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 {
 	mempool_t *pool;
diff --git a/block/partitions/check.c b/block/partitions/check.c
index ffe408f..2b602f9 100644
--- a/block/partitions/check.c
+++ b/block/partitions/check.c
@@ -139,6 +139,10 @@ void free_partitions(struct parsed_partitions *state)
 	kfree(state);
 }
 
+/*
+ * called by:
+ *   - block/partition-generic.c|531| <<rescan_partitions>> if (!get_capacity(disk) || !(state = check_partition(disk, bdev)))
+ */
 struct parsed_partitions *
 check_partition(struct gendisk *hd, struct block_device *bdev)
 {
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index d3d6b7b..9b91b56 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -702,6 +702,14 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2319| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|892| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|2229| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2063| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|145| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 1994d5b..432bcc1 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -490,6 +490,13 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3449| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|2328| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|691| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
@@ -977,6 +984,10 @@ EXPORT_SYMBOL_GPL(nvmf_free_options);
 				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
 				 NVMF_OPT_DISABLE_SQFLOW)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1083| <<nvmf_dev_write>> ctrl = nvmf_create_ctrl(nvmf_device, buf);
+ */
 static struct nvme_ctrl *
 nvmf_create_ctrl(struct device *dev, const char *buf)
 {
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index 3044d8b..055e773 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -99,6 +99,15 @@ struct nvmf_ctrl_options {
 	unsigned int		nr_io_queues;
 	unsigned int		reconnect_delay;
 	bool			discovery_nqn;
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {
+	 */
 	bool			duplicate_connect;
 	unsigned int		kato;
 	struct nvmf_host	*host;
diff --git a/drivers/nvme/host/fault_inject.c b/drivers/nvme/host/fault_inject.c
index 1352159..84ece48 100644
--- a/drivers/nvme/host/fault_inject.c
+++ b/drivers/nvme/host/fault_inject.c
@@ -54,6 +54,10 @@ void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inject)
 	debugfs_remove_recursive(fault_inject->parent);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/nvme.h|433| <<nvme_end_request>> nvme_should_fail(req);
+ */
 void nvme_should_fail(struct request *req)
 {
 	struct gendisk *disk = req->rq_disk;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 2d678fb..f41d166 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -179,6 +179,23 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 
@@ -405,6 +422,16 @@ static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
 	return (sector >> (ns->lba_shift - 9));
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|975| <<nvme_handle_cqe>> nvme_end_request(req, cqe->status, cqe->result);
+ *   - drivers/nvme/host/rdma.c|1519| <<nvme_rdma_inv_rkey_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1762| <<nvme_rdma_send_done>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/rdma.c|1926| <<nvme_rdma_process_nvme_rsp>> nvme_end_request(rq, req->status, req->result);
+ *   - drivers/nvme/host/tcp.c|440| <<nvme_tcp_process_nvme_cqe>> nvme_end_request(rq, cqe->status, cqe->result);
+ *   - drivers/nvme/host/tcp.c|634| <<nvme_tcp_end_request>> nvme_end_request(rq, cpu_to_le16(status << 1), res);
+ *   - drivers/nvme/target/loop.c|131| <<nvme_loop_queue_response>> nvme_end_request(rq, cqe->status, cqe->result);
+ */
 static inline void nvme_end_request(struct request *req, __le16 status,
 		union nvme_result result)
 {
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 1a6449b..08f9e7d 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -27,6 +27,9 @@
 #include "nvme.h"
 #include "fabrics.h"
 
+/*
+ * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+ */
 
 #define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
 
@@ -39,6 +42,12 @@ struct nvme_rdma_device {
 	struct ib_pd		*pd;
 	struct kref		ref;
 	struct list_head	entry;
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	unsigned int		num_inline_segments;
 };
 
@@ -73,8 +82,27 @@ enum nvme_rdma_queue_flags {
 };
 
 struct nvme_rdma_queue {
+	/*
+	 * 分配, 释放和使用rsp_ring的地方:
+	 *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|595| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|597| <<nvme_rdma_create_queue_ib>> if (!queue->rsp_ring) {
+	 *   - drivers/nvme/host/rdma.c|618| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|1756| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+	 *
+	 * 是一个数组, 有queue->queue_size个struct nvme_rdma_qe
+	 */
 	struct nvme_rdma_qe	*rsp_ring;
+	/*
+	 * 设置queue_size的地方:
+	 *   - drivers/nvme/host/rdma.c|659| <<nvme_rdma_alloc_queue>> queue->queue_size = queue_size;
+	 */
 	int			queue_size;
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	size_t			cmnd_capsule_len;
 	struct nvme_rdma_ctrl	*ctrl;
 	struct nvme_rdma_device	*device;
@@ -84,19 +112,56 @@ struct nvme_rdma_queue {
 	unsigned long		flags;
 	struct rdma_cm_id	*cm_id;
 	int			cm_error;
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	struct completion	cm_done;
 };
 
 struct nvme_rdma_ctrl {
 	/* read only in the hot path */
+	/*
+	 * nvme_rdma_create_ctrl()分配的queues
+	 * 2402         ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+	 * 2403                                 GFP_KERNEL);
+	 */
 	struct nvme_rdma_queue	*queues;
 
 	/* other member variables */
 	struct blk_mq_tag_set	tag_set;
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	struct work_struct	err_work;
 
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	struct nvme_rdma_qe	async_event_sqe;
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	struct delayed_work	reconnect_work;
 
 	struct list_head	list;
@@ -110,18 +175,49 @@ struct nvme_rdma_ctrl {
 	struct sockaddr_storage src_addr;
 
 	struct nvme_ctrl	ctrl;
+	/*
+	 * 在以下修改和使用use_inline_data:
+	 *   - drivers/nvme/host/rdma.c|1205| <<nvme_rdma_setup_ctrl>> ctrl->use_inline_data = true;
+	 *   - drivers/nvme/host/rdma.c|1516| <<nvme_rdma_map_data>> queue->ctrl->use_inline_data &&
+	 */
 	bool			use_inline_data;
+	/*
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	u32			io_queues[HCTX_MAX_TYPES];
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|729| <<nvme_rdma_alloc_tagset>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|951| <<nvme_rdma_free_ctrl>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+ *   - drivers/nvme/host/rdma.c|1922| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ */
 static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|373| <<nvme_rdma_find_get_device>> list_for_each_entry(ndev, &device_list, entry) {
+ *   - drivers/nvme/host/rdma.c|400| <<nvme_rdma_find_get_device>> list_add(&ndev->entry, &device_list);
+ *   - drivers/nvme/host/rdma.c|2119| <<nvme_rdma_remove_one>> list_for_each_entry(ndev, &device_list, entry) {
+ */
 static LIST_HEAD(device_list);
 static DEFINE_MUTEX(device_list_mutex);
 
+/*
+ * 在以下使用nvme_rdma_ctrl_list:
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+ *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ */
 static LIST_HEAD(nvme_rdma_ctrl_list);
 static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
 
@@ -130,6 +226,10 @@ static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
  * unsafe.  With it turned off we will have to register a global rkey that
  * allows read and write access to all physical memory.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|387| <<nvme_rdma_find_get_device>> register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+ */
 static bool register_always = true;
 module_param(register_always, bool, 0444);
 MODULE_PARM_DESC(register_always,
@@ -139,10 +239,24 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *event);
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops;
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
 
 /* XXX: really should move to a generic header sooner or later.. */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1223| <<nvme_rdma_set_sg_null>> put_unaligned_le24(0, sg->length);
+ *   - drivers/nvme/host/rdma.c|1260| <<nvme_rdma_map_sg_single>> put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
+ *   - drivers/nvme/host/rdma.c|1304| <<nvme_rdma_map_sg_fr>> put_unaligned_le24(req->mr->length, sg->length);
+ */
 static inline void put_unaligned_le24(u32 val, u8 *p)
 {
 	*p++ = val;
@@ -152,28 +266,76 @@ static inline void put_unaligned_le24(u32 val, u8 *p)
 
 static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctrl
+	 */
 	return queue - queue->ctrl->queues;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|471| <<nvme_rdma_create_queue_ib>> if (nvme_rdma_poll_queue(queue))
+ *   - drivers/nvme/host/rdma.c|620| <<nvme_rdma_start_queue>> bool poll = nvme_rdma_poll_queue(queue);
+ */
 static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctr
+	 *
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	return nvme_rdma_queue_idx(queue) >
 		queue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +
 		queue->ctrl->io_queues[HCTX_TYPE_READ];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1540| <<nvme_rdma_map_data>> nvme_rdma_inline_data_size(queue)) {
+ */
 static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|201| <<nvme_rdma_free_ring>> nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+ */
 static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
+	/*
+	 * struct nvme_rdma_qe {
+	 *         struct ib_cqe           cqe;
+	 *         void                    *data;
+	 *         u64                     dma;
+	 * };
+	 */
 	ib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);
 	kfree(qe->data);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|257| <<nvme_rdma_alloc_ring>> if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+ *   - drivers/nvme/host/rdma.c|842| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+ */
 static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -191,17 +353,31 @@ static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|264| <<nvme_rdma_alloc_ring>> nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|474| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ *   - drivers/nvme/host/rdma.c|551| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ */
 static void nvme_rdma_free_ring(struct ib_device *ibdev,
 		struct nvme_rdma_qe *ring, size_t ib_queue_size,
 		size_t capsule_size, enum dma_data_direction dir)
 {
 	int i;
 
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+	 */
 	for (i = 0; i < ib_queue_size; i++)
 		nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
 	kfree(ring);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+ */
 static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 		size_t ib_queue_size, size_t capsule_size,
 		enum dma_data_direction dir)
@@ -218,6 +394,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	 * lifetime. It's safe, since any chage in the underlying RDMA device
 	 * will issue error recovery and queue re-creation.
 	 */
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+	 */
 	for (i = 0; i < ib_queue_size; i++) {
 		if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
 			goto out_free_ring;
@@ -230,6 +410,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|300| <<nvme_rdma_create_qp>> init_attr.event_handler = nvme_rdma_qp_event;
+ */
 static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 {
 	pr_debug("QP event %s (%d)\n",
@@ -237,10 +421,24 @@ static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|634| <<nvme_rdma_alloc_queue>> ret = nvme_rdma_wait_for_cm(queue);
+ *
+ * 核心思想是等待nvme_rdma_queue->cm_done被complete
+ * 只在nvme_rdma_cm_handler被唤醒
+ */
 static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 {
 	int ret;
 
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
 			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
 	if (ret < 0)
@@ -251,6 +449,10 @@ static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 	return queue->cm_error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|485| <<nvme_rdma_create_queue_ib>> ret = nvme_rdma_create_qp(queue, send_wr_factor);
+ */
 static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 {
 	struct nvme_rdma_device *dev = queue->device;
@@ -270,20 +472,42 @@ static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 	init_attr.send_cq = queue->ib_cq;
 	init_attr.recv_cq = queue->ib_cq;
 
+	/*
+	 * dev->pd是struct ib_pd
+	 * queue->cm_id是struct rdma_cm_id
+	 */
 	ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
 
+	/*
+	 * queue->qp是struct ib_qp
+	 */
 	queue->qp = queue->cm_id->qp;
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.exit_request = nvme_rdma_exit_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.exit_request = nvme_rdma_exit_request()
+ */
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	kfree(req->sqe.data);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_request = nvme_rdma_init_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_request = nvme_rdma_init_request()
+ */
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -294,6 +518,13 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
 
 	nvme_req(rq)->ctrl = &ctrl->ctrl;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.data = kzalloc(sizeof(struct nvme_command), GFP_KERNEL);
 	if (!req->sqe.data)
 		return -ENOMEM;
@@ -303,6 +534,9 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_hctx = nvme_rdma_init_hctx()
+ */
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -315,6 +549,9 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_hctx = nvme_rdma_init_admin_hctx()
+ */
 static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -350,6 +587,17 @@ static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
 	return kref_get_unless_zero(&dev->ref);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|530| <<nvme_rdma_create_queue_ib>> queue->device = nvme_rdma_find_get_device(queue->cm_id);
+ *
+ * nvme_rdma_cm_handler()
+ *  -> nvme_rdma_addr_resolved()
+ *      -> nvme_rdma_create_queue_ib()
+ *          -> nvme_rdma_find_get_device()
+ *
+ * 惨是的rdma_cm_id->qp是struct ib_qp
+ */
 static struct nvme_rdma_device *
 nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 {
@@ -366,9 +614,22 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	if (!ndev)
 		goto out_err;
 
+	/* 类型是struct ib_device */
 	ndev->dev = cm_id->device;
 	kref_init(&ndev->ref);
 
+	/*
+	 * ndev->pd是struct ib_pd
+	 *
+	 * ib_alloc_pd - Allocates an unused protection domain.
+	 * @device: The device on which to allocate the protection domain.
+	 *
+	 * A protection domain object provides an association between QPs, shared
+	 * receive queues, address handles, memory regions, and memory windows.
+	 *
+	 * Every PD has a local_dma_lkey which can be used as the lkey value for local
+	 * memory operations.
+	 */
 	ndev->pd = ib_alloc_pd(ndev->dev,
 		register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
 	if (IS_ERR(ndev->pd))
@@ -397,6 +658,16 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_alloc_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|799| <<nvme_rdma_free_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1846| <<nvme_rdma_conn_established>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1899| <<nvme_rdma_addr_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1953| <<nvme_rdma_route_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2008| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2014| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ */
 static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_device *dev;
@@ -424,12 +695,30 @@ static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 	nvme_rdma_dev_put(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|687| <<nvme_rdma_create_queue_ib>> nvme_rdma_get_max_fr_pages(ibdev), 0);
+ *   - drivers/nvme/host/rdma.c|1016| <<nvme_rdma_configure_admin_queue>> ctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev);
+ */
 static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
 {
 	return min_t(u32, NVME_RDMA_MAX_SEGMENTS,
 		     ibdev->attrs.max_fast_reg_page_list_len);
 }
 
+/*
+ * [0] nvme_rdma_create_queue_ib
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1699| <<nvme_rdma_addr_resolved>> ret = nvme_rdma_create_queue_ib(queue);
+ */
 static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct ib_device *ibdev;
@@ -506,6 +795,20 @@ static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_alloc_queue [nvme_rdma]
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_io_queues>> ret = nvme_rdma_alloc_queue(ctrl, i,
+ *   - drivers/nvme/host/rdma.c|828| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ */
 static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		int idx, size_t queue_size)
 {
@@ -600,6 +903,11 @@ static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
 		nvme_rdma_stop_queue(&ctrl->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|715| <<nvme_rdma_start_io_queues>> ret = nvme_rdma_start_queue(ctrl, i);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_start_queue(ctrl, 0);
+ */
 static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 {
 	struct nvme_rdma_queue *queue = &ctrl->queues[idx];
@@ -621,6 +929,10 @@ static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1203| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_start_io_queues(ctrl);
+ */
 static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	int i, ret = 0;
@@ -639,6 +951,10 @@ static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_alloc_io_queues(ctrl);
+ */
 static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -709,6 +1025,21 @@ static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|848| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ *   - drivers/nvme/host/rdma.c|925| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ */
 static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
 		bool admin)
 {
@@ -767,6 +1098,10 @@ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_queue(&ctrl->queues[0]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1035| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_admin_queue(ctrl, new);
+ */
 static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 		bool new)
 {
@@ -786,6 +1121,16 @@ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	 * It's safe, since any chage in the underlying RDMA device will issue
 	 * error recovery and queue re-creation.
 	 */
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
 			sizeof(struct nvme_command), DMA_TO_DEVICE);
 	if (error)
@@ -860,6 +1205,10 @@ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_io_queues(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1180| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_io_queues(ctrl, new);
+ */
 static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret;
@@ -929,6 +1278,9 @@ static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
 	}
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.free_ctrl = nvme_rdma_free_ctrl()
+ */
 static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
@@ -946,6 +1298,12 @@ static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 	kfree(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1178| <<nvme_rdma_reconnect_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|1197| <<nvme_rdma_error_recovery_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|2114| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ */
 static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 {
 	/* If we are resetting/deleting then do nothing */
@@ -965,6 +1323,30 @@ static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 	}
 }
 
+/*
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1054| <<nvme_rdma_reconnect_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|1939| <<nvme_rdma_reset_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|2071| <<nvme_rdma_create_ctrl>> ret = nvme_rdma_setup_ctrl(ctrl, true);
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ */
 static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret = -EINVAL;
@@ -1027,6 +1409,13 @@ static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 	return ret;
 }
 
+/*
+ * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+ *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+ *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+ */
 static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
@@ -1050,6 +1439,14 @@ static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+ *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+ *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+ */
 static void nvme_rdma_error_recovery_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(work,
@@ -1069,6 +1466,16 @@ static void nvme_rdma_error_recovery_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1251| <<nvme_rdma_wr_error>> nvme_rdma_error_recovery(ctrl);
+ *   - drivers/nvme/host/rdma.c|1627| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1640| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1650| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1900| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1947| <<nvme_rdma_timeout>> nvme_rdma_error_recovery(ctrl);
+ */
 static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
@@ -1201,6 +1608,10 @@ static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1688| <<nvme_rdma_map_data>> ret = nvme_rdma_map_sg_fr(queue, req, c, count);
+ */
 static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_request *req, struct nvme_command *c,
 		int count)
@@ -1208,6 +1619,9 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
 	int nr;
 
+	/*
+	 * req->mr是struct ib_mr
+	 */
 	req->mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);
 	if (WARN_ON_ONCE(!req->mr))
 		return -EAGAIN;
@@ -1247,6 +1661,10 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1828| <<nvme_rdma_queue_rq>> err = nvme_rdma_map_data(queue, rq, c);
+ */
 static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		struct request *rq, struct nvme_command *c)
 {
@@ -1279,6 +1697,12 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		goto out_free_table;
 	}
 
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	if (count <= dev->num_inline_segments) {
 		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
 		    queue->ctrl->use_inline_data &&
@@ -1310,6 +1734,17 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_send_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1327,6 +1762,11 @@ static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1835| <<nvme_rdma_submit_async_event>> ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
+ *   - drivers/nvme/host/rdma.c|2248| <<nvme_rdma_queue_rq>> err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
+ */
 static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
 		struct ib_send_wr *first)
@@ -1358,6 +1798,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1641| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1653| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1376,6 +1821,10 @@ static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 	wr.sg_list  = &list;
 	wr.num_sge  = 1;
 
+	/*
+	 * Posts a list of work requests to the receive queue of
+	 *   the specified QP.
+	 */
 	ret = ib_post_recv(queue->qp, &wr, NULL);
 	if (unlikely(ret)) {
 		dev_err(queue->ctrl->ctrl.device,
@@ -1399,6 +1848,9 @@ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_rdma_wr_error(cq, wc, "ASYNC");
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.submit_async_event = nvme_rdma_submit_async_event()
+ */
 static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
@@ -1426,6 +1878,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1564| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1470,6 +1926,20 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * [0] nvme_rdma_recv_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1779| <<nvme_rdma_post_recv>> qe->cqe.done = nvme_rdma_recv_done;
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1502,6 +1972,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1712| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -1545,6 +2019,10 @@ static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
 	return -ECONNRESET;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1706| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_addr_resolved(queue);
+ */
 static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -1568,6 +2046,10 @@ static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1709| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_route_resolved(queue);
+ */
 static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
@@ -1618,6 +2100,34 @@ static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_ib_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|580| <<nvme_rdma_alloc_queue>> queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+ */
 static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *ev)
 {
@@ -1629,6 +2139,18 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		ev->status, cm_id);
 
 	switch (ev->event) {
+		/*
+		 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 */
 	case RDMA_CM_EVENT_ADDR_RESOLVED:
 		cm_error = nvme_rdma_addr_resolved(queue);
 		break;
@@ -1679,6 +2201,10 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.timeout = nvme_rdma_timeout()
+ * struct blk_mq_ops nvme_rdma_mq_ops.timeout = nvme_rdma_timeout()
+ */
 static enum blk_eh_timer_return
 nvme_rdma_timeout(struct request *rq, bool reserved)
 {
@@ -1707,6 +2233,10 @@ nvme_rdma_timeout(struct request *rq, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ */
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1715,6 +2245,15 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct request *rq = bd->rq;
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 	struct nvme_rdma_qe *sqe = &req->sqe;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 *
+	 * 注意, c指向的req->sqe.data!!!!!!!!
+	 */
 	struct nvme_command *c = sqe->data;
 	struct ib_device *dev;
 	bool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);
@@ -1728,6 +2267,17 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	dev = queue->device->dev;
 
+	/*
+	 * req是struct nvme_rdma_request类型
+	 * 来自blk_mq_rq_to_pdu(rq)
+	 *
+	 * 这里req->sqe.dma对应的req->sqe.data (存储的nvme_command c)
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.dma = ib_dma_map_single(dev, req->sqe.data,
 					 sizeof(struct nvme_command),
 					 DMA_TO_DEVICE);
@@ -1777,6 +2327,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.poll = nvme_rdma_poll()
+ */
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1784,6 +2337,10 @@ static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 	return ib_process_cq_direct(queue->ib_cq, -1);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.complete = nvme_rdma_complete_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.complete = nvme_rdma_complete_rq()
+ */
 static void nvme_rdma_complete_rq(struct request *rq)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
@@ -1796,6 +2353,9 @@ static void nvme_rdma_complete_rq(struct request *rq)
 	nvme_complete_rq(rq);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.map_queues = nvme_rdma_map_queues()
+ */
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
@@ -1843,6 +2403,10 @@ static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1854,6 +2418,10 @@ static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.poll		= nvme_rdma_poll,
 };
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1863,6 +2431,11 @@ static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.timeout	= nvme_rdma_timeout,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1975| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ *   - drivers/nvme/host/rdma.c|1988| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_shutdown_ctrl(ctrl, false);
+ */
 static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 {
 	cancel_work_sync(&ctrl->err_work);
@@ -1876,11 +2449,18 @@ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.delete_ctrl = nvme_rdma_delete_ctrl()
+ */
 static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|2050| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ */
 static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl =
@@ -1930,6 +2510,10 @@ static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
  * The ports don't need to be compared as they are intrinsically
  * already matched by the port pointers supplied.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2021| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+ */
 static bool
 nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 {
@@ -1947,9 +2531,25 @@ nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 	return found;
 }
 
+/*
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1039| <<nvmf_create_ctrl>> ctrl = ops->create_ctrl(dev, opts);
+ *
+ * struct nvmf_transport_ops nvme_rdma_transport.create_ctrl = nvme_ctrl_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
+	/*
+	 * 包含struct nvme_ctrl和struct blk_mq_tag_set (不是指针)
+	 */
 	struct nvme_rdma_ctrl *ctrl;
 	int ret;
 	bool changed;
@@ -1970,6 +2570,9 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		opts->mask |= NVMF_OPT_TRSVCID;
 	}
 
+	/*
+	 * convert an IPv4/IPv6 and port to socket address
+	 */
 	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
 			opts->traddr, opts->trsvcid, &ctrl->addr);
 	if (ret) {
@@ -1988,14 +2591,60 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		}
 	}
 
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts))
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts))
+	 *
+	 * nvme_rdma_existing_controller()"
+	 * Fails a connection request if it matches an existing controller
+	 * (association) with the same tuple:
+	 * <Host NQN, Host ID, local address, remote address, remote port, SUBSYS NQN>
+	 */
 	if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
 		ret = -EALREADY;
 		goto out_free_ctrl;
 	}
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	INIT_DELAYED_WORK(&ctrl->reconnect_work,
 			nvme_rdma_reconnect_ctrl_work);
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
 
 	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
@@ -2009,6 +2658,11 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	if (!ctrl->queues)
 		goto out_free_ctrl;
 
+	/*
+	 * 下面的命令到这里的时候queue_count是5
+	 * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+	 */
+
 	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
 				0 /* no quirks, we're perfect! */);
 	if (ret)
@@ -2024,9 +2678,16 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs\n",
 		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
 
+	/* 不知道在哪里put */
 	nvme_get_ctrl(&ctrl->ctrl);
 
 	mutex_lock(&nvme_rdma_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_rdma_ctrl_list:
+	 *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+	 *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 */
 	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
 	mutex_unlock(&nvme_rdma_ctrl_mutex);
 
@@ -2055,6 +2716,20 @@ static struct nvmf_transport_ops nvme_rdma_transport = {
 	.create_ctrl	= nvme_rdma_create_ctrl,
 };
 
+/*
+ * 在nvme_rdma_init_module()被用于ib_register_client - Register an IB client
+ *
+ * Upper level users of the IB drivers can use ib_register_client() to
+ * register callbacks for IB device addition and removal.  When an IB
+ * device is added, each registered client's add method will be called
+ * (in the order the clients were registered), and when a device is
+ * removed, each client's remove method will be called (in the reverse
+ * order that clients were registered).  In addition, when
+ * ib_register_client() is called, the client will receive an add
+ * callback for all devices already registered.
+ *
+ * struct ib_client nvme_rdma_ib_client.remove = nvme_rdma_remove_one()
+ */
 static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
 {
 	struct nvme_rdma_ctrl *ctrl;
@@ -2094,6 +2769,18 @@ static int __init nvme_rdma_init_module(void)
 {
 	int ret;
 
+	/*
+	 * ib_register_client - Register an IB client
+	 *
+	 * Upper level users of the IB drivers can use ib_register_client() to
+	 * register callbacks for IB device addition and removal.  When an IB
+	 * device is added, each registered client's add method will be called
+	 * (in the order the clients were registered), and when a device is
+	 * removed, each client's remove method will be called (in the reverse
+	 * order that clients were registered).  In addition, when
+	 * ib_register_client() is called, the client will receive an add
+	 * callback for all devices already registered.
+	 */
 	ret = ib_register_client(&nvme_rdma_ib_client);
 	if (ret)
 		return ret;
diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 3a67e24..4165919 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -260,6 +260,13 @@ void nvmet_port_send_ana_event(struct nvmet_port *port)
 	up_read(&nvmet_config_sem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2575| <<nvmet_fc_init_module>> return nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);
+ *   - drivers/nvme/target/loop.c|687| <<nvme_loop_init_module>> ret = nvmet_register_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/rdma.c|1667| <<nvmet_rdma_init>> ret = nvmet_register_transport(&nvmet_rdma_ops);
+ *   - drivers/nvme/target/tcp.c|1719| <<nvmet_tcp_init>> ret = nvmet_register_transport(&nvmet_tcp_ops);
+ */
 int nvmet_register_transport(const struct nvmet_fabrics_ops *ops)
 {
 	int ret = 0;
@@ -822,6 +829,10 @@ static inline u16 nvmet_io_cmd_check_access(struct nvmet_req *req)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|905| <<nvmet_req_init>> status = nvmet_parse_io_cmd(req);
+ */
 static u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
@@ -853,6 +864,14 @@ static u16 nvmet_parse_io_cmd(struct nvmet_req *req)
 		return nvmet_bdev_parse_io_cmd(req);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2196| <<nvmet_fc_handle_fcp_rqst>> ret = nvmet_req_init(&fod->req,
+ *   - drivers/nvme/target/loop.c|159| <<nvme_loop_queue_rq>> if (!nvmet_req_init(&iod->req, &queue->nvme_cq,
+ *   - drivers/nvme/target/loop.c|194| <<nvme_loop_submit_async_event>> if (!nvmet_req_init(&iod->req, &queue->nvme_cq, &queue->nvme_sq,
+ *   - drivers/nvme/target/rdma.c|767| <<nvmet_rdma_handle_command>> if (!nvmet_req_init(&cmd->req, &queue->nvme_cq,
+ *   - drivers/nvme/target/tcp.c|902| <<nvmet_tcp_done_recv_pdu>> if (unlikely(!nvmet_req_init(req, &queue->nvme_cq,
+ */
 bool nvmet_req_init(struct nvmet_req *req, struct nvmet_cq *cq,
 		struct nvmet_sq *sq, const struct nvmet_fabrics_ops *ops)
 {
@@ -930,6 +949,17 @@ void nvmet_req_uninit(struct nvmet_req *req)
 }
 EXPORT_SYMBOL_GPL(nvmet_req_uninit);
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2018| <<nvmet_fc_fod_op_done>> nvmet_req_execute(&fod->req);
+ *   - drivers/nvme/target/fc.c|2234| <<nvmet_fc_handle_fcp_rqst>> nvmet_req_execute(&fod->req);
+ *   - drivers/nvme/target/loop.c|133| <<nvme_loop_execute_work>> nvmet_req_execute(&iod->req);
+ *   - drivers/nvme/target/rdma.c|606| <<nvmet_rdma_read_data_done>> nvmet_req_execute(&rsp->req);
+ *   - drivers/nvme/target/rdma.c|749| <<nvmet_rdma_execute_command>> nvmet_req_execute(&rsp->req);
+ *   - drivers/nvme/target/tcp.c|935| <<nvmet_tcp_done_recv_pdu>> nvmet_req_execute(&queue->cmd->req);
+ *   - drivers/nvme/target/tcp.c|1055| <<nvmet_tcp_try_recv_data>> nvmet_req_execute(&cmd->req);
+ *   - drivers/nvme/target/tcp.c|1095| <<nvmet_tcp_try_recv_ddgst>> nvmet_req_execute(&cmd->req);
+ */
 void nvmet_req_execute(struct nvmet_req *req)
 {
 	if (unlikely(req->data_len != req->transfer_len)) {
@@ -937,6 +967,43 @@ void nvmet_req_execute(struct nvmet_req *req)
 		nvmet_req_complete(req, NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR);
 	} else
 		req->execute(req);
+
+	/*
+	 * execute的设置:
+	 *   - drivers/nvme/target/admin-cmd.c|826| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_error;
+	 *   - drivers/nvme/target/admin-cmd.c|829| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_smart;
+	 *   - drivers/nvme/target/admin-cmd.c|838| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_noop;
+	 *   - drivers/nvme/target/admin-cmd.c|841| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_changed_ns;
+	 *   - drivers/nvme/target/admin-cmd.c|844| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_cmd_effects_ns;
+	 *   - drivers/nvme/target/admin-cmd.c|847| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_log_page_ana;
+	 *   - drivers/nvme/target/admin-cmd.c|855| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_ns;
+	 *   - drivers/nvme/target/admin-cmd.c|858| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_ctrl;
+	 *   - drivers/nvme/target/admin-cmd.c|861| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_nslist;
+	 *   - drivers/nvme/target/admin-cmd.c|864| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_identify_desclist;
+	 *   - drivers/nvme/target/admin-cmd.c|869| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_abort;
+	 *   - drivers/nvme/target/admin-cmd.c|873| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_set_features;
+	 *   - drivers/nvme/target/admin-cmd.c|877| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_get_features;
+	 *   - drivers/nvme/target/admin-cmd.c|881| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_async_event;
+	 *   - drivers/nvme/target/admin-cmd.c|885| <<nvmet_parse_admin_cmd>> req->execute = nvmet_execute_keep_alive;
+	 *   - drivers/nvme/target/discovery.c|330| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_disc_set_features;
+	 *   - drivers/nvme/target/discovery.c|334| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_disc_get_features;
+	 *   - drivers/nvme/target/discovery.c|338| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_async_event;
+	 *   - drivers/nvme/target/discovery.c|342| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_keep_alive;
+	 *   - drivers/nvme/target/discovery.c|350| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_get_disc_log_page;
+	 *   - drivers/nvme/target/discovery.c|363| <<nvmet_parse_discovery_cmd>> req->execute = nvmet_execute_identify_disc_ctrl;
+	 *   - drivers/nvme/target/fabrics-cmd.c|86| <<nvmet_parse_fabrics_cmd>> req->execute = nvmet_execute_prop_set;
+	 *   - drivers/nvme/target/fabrics-cmd.c|90| <<nvmet_parse_fabrics_cmd>> req->execute = nvmet_execute_prop_get;
+	 *   - drivers/nvme/target/fabrics-cmd.c|286| <<nvmet_parse_connect_cmd>> req->execute = nvmet_execute_admin_connect;
+	 *   - drivers/nvme/target/fabrics-cmd.c|288| <<nvmet_parse_connect_cmd>> req->execute = nvmet_execute_io_connect;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|321| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_rw;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|325| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_flush;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|329| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_dsm;
+	 *   - drivers/nvme/target/io-cmd-bdev.c|334| <<nvmet_bdev_parse_io_cmd>> req->execute = nvmet_bdev_execute_write_zeroes;
+	 *   - drivers/nvme/target/io-cmd-file.c|373| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_rw;
+	 *   - drivers/nvme/target/io-cmd-file.c|377| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_flush;
+	 *   - drivers/nvme/target/io-cmd-file.c|381| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_dsm;
+	 *   - drivers/nvme/target/io-cmd-file.c|386| <<nvmet_file_parse_io_cmd>> req->execute = nvmet_file_execute_write_zeroes;
+	 */
 }
 EXPORT_SYMBOL_GPL(nvmet_req_execute);
 
diff --git a/drivers/nvme/target/io-cmd-file.c b/drivers/nvme/target/io-cmd-file.c
index 05453f5..ebbca3f 100644
--- a/drivers/nvme/target/io-cmd-file.c
+++ b/drivers/nvme/target/io-cmd-file.c
@@ -87,6 +87,11 @@ static void nvmet_file_init_bvec(struct bio_vec *bv, struct scatterlist *sg)
 	bv->bv_len = sg->length;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|165| <<nvmet_file_execute_io>> ret = nvmet_file_submit_bvec(req, pos, bv_cnt, len, 0);
+ *   - drivers/nvme/target/io-cmd-file.c|193| <<nvmet_file_execute_io>> ret = nvmet_file_submit_bvec(req, pos, bv_cnt, total_len, ki_flags);
+ */
 static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 		unsigned long nr_segs, size_t count, int ki_flags)
 {
@@ -114,6 +119,11 @@ static ssize_t nvmet_file_submit_bvec(struct nvmet_req *req, loff_t pos,
 	return call_iter(iocb, &iter);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|191| <<nvmet_file_execute_io>> req->f.iocb.ki_complete = nvmet_file_io_done;
+ *   - drivers/nvme/target/io-cmd-file.c|214| <<nvmet_file_execute_io>> nvmet_file_io_done(&req->f.iocb, ret, 0);
+ */
 static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 {
 	struct nvmet_req *req = container_of(iocb, struct nvmet_req, f.iocb);
@@ -131,6 +141,12 @@ static void nvmet_file_io_done(struct kiocb *iocb, long ret, long ret2)
 	nvmet_req_complete(req, status);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd-file.c|222| <<nvmet_file_buffered_io_work>> nvmet_file_execute_io(req, 0);
+ *   - drivers/nvme/target/io-cmd-file.c|270| <<nvmet_file_execute_rw>> nvmet_file_execute_io(req, IOCB_NOWAIT))
+ *   - drivers/nvme/target/io-cmd-file.c|274| <<nvmet_file_execute_rw>> nvmet_file_execute_io(req, 0);
+ */
 static bool nvmet_file_execute_io(struct nvmet_req *req, int ki_flags)
 {
 	ssize_t nr_bvec = req->sg_cnt;
@@ -250,6 +266,21 @@ static void nvmet_file_execute_rw(struct nvmet_req *req)
 	} else
 		req->f.mpool_alloc = false;
 
+	/*
+	 * 在以下设置和使用buffered_io:
+	 *   - drivers/nvme/target/configfs.c|541| <<nvmet_ns_buffered_io_store>> ns->buffered_io = val;
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/configfs.c|546| <<global>> CONFIGFS_ATTR(nvmet_ns_, buffered_io);
+	 *   - drivers/nvme/target/configfs.c|522| <<nvmet_ns_buffered_io_show>> return sprintf(page, "%d\n", to_nvmet_ns(item)->buffered_io);
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false; 
+	 *   - drivers/nvme/target/io-cmd-file.c|19| <<nvmet_file_ns_disable>> if (ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|36| <<nvmet_file_ns_enable>> if (!ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|253| <<nvmet_file_execute_rw>> if (req->ns->buffered_io) {
+	 *
+	 * struct nvmet_req *req
+	 *  -> struct nvmet_ns *ns;
+	 *      -> bool buffered_io;
+	 */
 	if (req->ns->buffered_io) {
 		if (likely(!req->f.mpool_alloc) &&
 				nvmet_file_execute_io(req, IOCB_NOWAIT))
@@ -363,6 +394,10 @@ static void nvmet_file_execute_write_zeroes(struct nvmet_req *req)
 	schedule_work(&req->f.work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|858| <<nvmet_parse_io_cmd>> return nvmet_file_parse_io_cmd(req);
+ */
 u16 nvmet_file_parse_io_cmd(struct nvmet_req *req)
 {
 	struct nvme_command *cmd = req->cmd;
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index 0940c50..395e2a5 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -56,9 +56,20 @@ struct nvme_loop_queue {
 	unsigned long		flags;
 };
 
+/*
+ * 在以下添加和使用nvme_loop_ports:
+ *   - drivers/nvme/target/loop.c|667| <<nvme_loop_add_port>> list_add_tail(&port->entry, &nvme_loop_ports);
+ *   - drivers/nvme/target/loop.c|572| <<nvme_loop_find_port>> list_for_each_entry(p, &nvme_loop_ports, entry) {
+ */
 static LIST_HEAD(nvme_loop_ports);
 static DEFINE_MUTEX(nvme_loop_ports_mutex);
 
+/*
+ * 在以下添加删除和使用nvme_loop_ctrl_list:
+ *   - drivers/nvme/target/loop.c|453| <<nvme_loop_delete_ctrl>> list_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {
+ *   - drivers/nvme/target/loop.c|644| <<nvme_loop_create_ctrl>> list_add_tail(&ctrl->list, &nvme_loop_ctrl_list);
+ *   - drivers/nvme/target/loop.c|726| <<nvme_loop_cleanup_module>> list_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)
+ */
 static LIST_HEAD(nvme_loop_ctrl_list);
 static DEFINE_MUTEX(nvme_loop_ctrl_mutex);
 
@@ -121,6 +132,15 @@ static void nvme_loop_queue_response(struct nvmet_req *req)
 	}
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/target/loop.c|198| <<nvme_loop_init_iod>> INIT_WORK(&iod->work, nvme_loop_execute_work);
+ *
+ * 在nvme_loop_queue_rq()设置了如下作为输入:
+ *   - iod->req.sg = iod->sg_table.sgl;
+ *   - iod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);
+ *   - iod->req.transfer_len = blk_rq_payload_bytes(req);
+ */
 static void nvme_loop_execute_work(struct work_struct *work)
 {
 	struct nvme_loop_iod *iod =
@@ -142,6 +162,9 @@ static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (!nvmf_check_ready(&queue->ctrl->ctrl, req, queue_ready))
 		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, req);
 
+	/*
+	 * iod->cmd是struct nvme_command
+	 */
 	ret = nvme_setup_cmd(ns, req, &iod->cmd);
 	if (ret)
 		return ret;
@@ -154,17 +177,29 @@ static blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 		return BLK_STS_OK;
 
 	if (blk_rq_nr_phys_segments(req)) {
+		/*
+		 * iod->first_sgl因为是struct scatterlist first_sgl[];
+		 * 所以分配的时候就指向iod后面的sg们了
+		 */
 		iod->sg_table.sgl = iod->first_sgl;
 		if (sg_alloc_table_chained(&iod->sg_table,
 				blk_rq_nr_phys_segments(req),
 				iod->sg_table.sgl, SG_CHUNK_SIZE))
 			return BLK_STS_RESOURCE;
 
+		/*
+		 * struct nvme_loop_iod
+		 *  -> struct nvmet_req req
+		 */
 		iod->req.sg = iod->sg_table.sgl;
 		iod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);
 		iod->req.transfer_len = blk_rq_payload_bytes(req);
 	}
 
+	/*
+	 * 设置的地方:
+	 *   - drivers/nvme/target/loop.c|198| <<nvme_loop_init_iod>> INIT_WORK(&iod->work, nvme_loop_execute_work);
+	 */
 	schedule_work(&iod->work);
 	return BLK_STS_OK;
 }
@@ -189,6 +224,11 @@ static void nvme_loop_submit_async_event(struct nvme_ctrl *arg)
 	schedule_work(&iod->work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|220| <<nvme_loop_init_request>> return nvme_loop_init_iod(ctrl, blk_mq_rq_to_pdu(req),
+ *   - drivers/nvme/target/loop.c|624| <<nvme_loop_create_ctrl>> nvme_loop_init_iod(ctrl, &ctrl->async_event_iod, 0);
+ */
 static int nvme_loop_init_iod(struct nvme_loop_ctrl *ctrl,
 		struct nvme_loop_iod *iod, unsigned int queue_idx)
 {
@@ -544,6 +584,10 @@ static int nvme_loop_create_io_queues(struct nvme_loop_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/loop.c|608| <<nvme_loop_create_ctrl>> ctrl->port = nvme_loop_find_port(&ctrl->ctrl);
+ */
 static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 {
 	struct nvmet_port *p, *found = NULL;
@@ -561,6 +605,9 @@ static struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)
 	return found;
 }
 
+/*
+ * struct nvmf_transport_ops nvme_loop_transport.create_ctrl = nvme_loop_crete_ctrl()
+ */
 static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
@@ -585,6 +632,7 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 
 	ctrl->ctrl.sqsize = opts->queue_size - 1;
 	ctrl->ctrl.kato = opts->kato;
+	/* 返回的是struct nvmet_port */
 	ctrl->port = nvme_loop_find_port(&ctrl->ctrl);
 
 	ctrl->queues = kcalloc(opts->nr_io_queues + 1, sizeof(*ctrl->queues),
@@ -610,6 +658,10 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 			goto out_remove_admin_queue;
 	}
 
+	/*
+	 * struct nvme_loop_ctrl
+	 *  -> struct nvme_loop_iod async_event_iod
+	 */
 	nvme_loop_init_iod(ctrl, &ctrl->async_event_iod, 0);
 
 	dev_info(ctrl->ctrl.device,
@@ -641,6 +693,10 @@ static struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|328| <<nvmet_enable_port>> ret = ops->add_port(port);
+ */
 static int nvme_loop_add_port(struct nvmet_port *port)
 {
 	mutex_lock(&nvme_loop_ports_mutex);
diff --git a/drivers/nvme/target/nvmet.h b/drivers/nvme/target/nvmet.h
index c51f8dd..f1c4931 100644
--- a/drivers/nvme/target/nvmet.h
+++ b/drivers/nvme/target/nvmet.h
@@ -62,6 +62,17 @@ struct nvmet_ns {
 	uuid_t			uuid;
 	u32			anagrpid;
 
+	/*
+	 * 在以下设置和使用buffered_io:
+	 *   - drivers/nvme/target/configfs.c|541| <<nvmet_ns_buffered_io_store>> ns->buffered_io = val;
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/configfs.c|546| <<global>> CONFIGFS_ATTR(nvmet_ns_, buffered_io);
+	 *   - drivers/nvme/target/configfs.c|522| <<nvmet_ns_buffered_io_show>> return sprintf(page, "%d\n", to_nvmet_ns(item)->buffered_io);
+	 *   - drivers/nvme/target/core.c|662| <<nvmet_ns_alloc>> ns->buffered_io = false;
+	 *   - drivers/nvme/target/io-cmd-file.c|19| <<nvmet_file_ns_disable>> if (ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|36| <<nvmet_file_ns_enable>> if (!ns->buffered_io)
+	 *   - drivers/nvme/target/io-cmd-file.c|253| <<nvmet_file_execute_rw>> if (req->ns->buffered_io) {
+	 */
 	bool			buffered_io;
 	bool			enabled;
 	struct nvmet_subsys	*subsys;
diff --git a/drivers/scsi/fcoe/fcoe.c b/drivers/scsi/fcoe/fcoe.c
index 00dd47b..d7442c1 100644
--- a/drivers/scsi/fcoe/fcoe.c
+++ b/drivers/scsi/fcoe/fcoe.c
@@ -1120,6 +1120,11 @@ static int fcoe_ddp_done(struct fc_lport *lport, u16 xid)
  *
  * Returns: The allocated fc_lport or an error pointer
  */
+/*
+ * called by:
+ *   - drivers/scsi/fcoe/fcoe.c|2221| <<_fcoe_create>> lport = fcoe_if_create(fcoe, &ctlr_dev->dev, 0);
+ *   - drivers/scsi/fcoe/fcoe.c|2675| <<fcoe_vport_create>> vn_port = fcoe_if_create(fcoe, &vport->dev, 1);
+ */
 static struct fc_lport *fcoe_if_create(struct fcoe_interface *fcoe,
 				       struct device *parent, int npiv)
 {
@@ -2192,6 +2197,11 @@ enum fcoe_create_link_state {
  * consolidation of code can be done when that interface is
  * removed.
  */
+/*
+ * called by:
+ *   - drivers/scsi/fcoe/fcoe.c|2288| <<fcoe_create>> return _fcoe_create(netdev, fip_mode, FCOE_CREATE_LINK_UP);
+ *   - drivers/scsi/fcoe/fcoe.c|2304| <<fcoe_ctlr_alloc>> return _fcoe_create(netdev, FIP_MODE_FABRIC,
+ */
 static int _fcoe_create(struct net_device *netdev, enum fip_mode fip_mode,
 			enum fcoe_create_link_state link_state)
 {
diff --git a/drivers/scsi/fcoe/fcoe_transport.c b/drivers/scsi/fcoe/fcoe_transport.c
index ba4603d..a1bf4eb 100644
--- a/drivers/scsi/fcoe/fcoe_transport.c
+++ b/drivers/scsi/fcoe/fcoe_transport.c
@@ -523,6 +523,11 @@ static struct fcoe_transport *fcoe_transport_lookup(struct net_device *netdev)
  *
  * Returns : 0 for success
  */
+/*
+ * called by:
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|2699| <<bnx2fc_mod_init>> rc = fcoe_transport_attach(&bnx2fc_transport);
+ *   - drivers/scsi/fcoe/fcoe.c|2478| <<fcoe_init>> rc = fcoe_transport_attach(&fcoe_sw_transport);
+ */
 int fcoe_transport_attach(struct fcoe_transport *ft)
 {
 	int rc = 0;
diff --git a/drivers/scsi/scsi_transport_fc.c b/drivers/scsi/scsi_transport_fc.c
index 2732fa6..0227fd0 100644
--- a/drivers/scsi/scsi_transport_fc.c
+++ b/drivers/scsi/scsi_transport_fc.c
@@ -2149,6 +2149,28 @@ fc_user_scan(struct Scsi_Host *shost, uint channel, uint id, u64 lun)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/message/fusion/mptfc.c|1466| <<mptfc_init>> fc_attach_transport(&mptfc_transport_functions);
+ *   - drivers/s390/scsi/zfcp_aux.c|141| <<zfcp_module_init>> fc_attach_transport(&zfcp_transport_functions);
+ *   - drivers/scsi/bfa/bfad_im.c|842| <<bfad_im_module_init>> fc_attach_transport(&bfad_im_fc_function_template);
+ *   - drivers/scsi/bfa/bfad_im.c|847| <<bfad_im_module_init>> fc_attach_transport(&bfad_im_vport_fc_function_template);
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|1283| <<bnx2fc_attach_transport>> fc_attach_transport(&bnx2fc_transport_function);
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|1291| <<bnx2fc_attach_transport>> fc_attach_transport(&bnx2fc_vport_xport_function);
+ *   - drivers/scsi/csiostor/csio_init.c|1213| <<csio_init>> csio_fcoe_transport = fc_attach_transport(&csio_fc_transport_funcs);
+ *   - drivers/scsi/csiostor/csio_init.c|1218| <<csio_init>> fc_attach_transport(&csio_fc_transport_vport_funcs);
+ *   - drivers/scsi/fcoe/fcoe.c|1252| <<fcoe_if_init>> fc_attach_transport(&fcoe_nport_fc_functions);
+ *   - drivers/scsi/fcoe/fcoe.c|1254| <<fcoe_if_init>> fc_attach_transport(&fcoe_vport_fc_functions);
+ *   - drivers/scsi/fnic/fnic_main.c|1111| <<fnic_init_module>> fnic_fc_transport = fc_attach_transport(&fnic_fc_functions);
+ *   - drivers/scsi/ibmvscsi/ibmvfc.c|4986| <<ibmvfc_module_init>> ibmvfc_transport_template = fc_attach_transport(&ibmvfc_transport_functions);
+ *   - drivers/scsi/lpfc/lpfc_init.c|13495| <<lpfc_init>> fc_attach_transport(&lpfc_transport_functions);
+ *   - drivers/scsi/lpfc/lpfc_init.c|13499| <<lpfc_init>> fc_attach_transport(&lpfc_vport_transport_functions);
+ *   - drivers/scsi/qedf/qedf_main.c|3786| <<qedf_init>> fc_attach_transport(&qedf_fc_transport_fn);
+ *   - drivers/scsi/qedf/qedf_main.c|3793| <<qedf_init>> fc_attach_transport(&qedf_fc_vport_transport_fn);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7285| <<qla2x00_module_init>> fc_attach_transport(&qla2xxx_transport_functions);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7300| <<qla2x00_module_init>> fc_attach_transport(&qla2xxx_transport_vport_functions);
+ *   - drivers/scsi/storvsc_drv.c|1973| <<storvsc_drv_init>> fc_transport_template = fc_attach_transport(&fc_transport_functions);
+ */
 struct scsi_transport_template *
 fc_attach_transport(struct fc_function_template *ft)
 {
diff --git a/drivers/scsi/scsi_transport_iscsi.c b/drivers/scsi/scsi_transport_iscsi.c
index 417b868..5ad4da9 100644
--- a/drivers/scsi/scsi_transport_iscsi.c
+++ b/drivers/scsi/scsi_transport_iscsi.c
@@ -4407,6 +4407,16 @@ static int iscsi_host_match(struct attribute_container *cont,
         return &priv->t.host_attrs.ac == cont;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iscsi_iser.c|1050| <<iser_init>> iscsi_iser_scsi_transport = iscsi_register_transport(
+ *   - drivers/scsi/be2iscsi/be_main.c|5849| <<beiscsi_module_init>> iscsi_register_transport(&beiscsi_iscsi_transport);
+ *   - drivers/scsi/bnx2i/bnx2i_init.c|480| <<bnx2i_mod_init>> iscsi_register_transport(&bnx2i_iscsi_transport);
+ *   - drivers/scsi/cxgbi/libcxgbi.c|2691| <<cxgbi_iscsi_init>> *stt = iscsi_register_transport(itp);
+ *   - drivers/scsi/iscsi_tcp.c|1024| <<iscsi_sw_tcp_init>> iscsi_sw_tcp_scsi_transport = iscsi_register_transport(
+ *   - drivers/scsi/qedi/qedi_main.c|2717| <<qedi_init>> qedi_scsi_transport = iscsi_register_transport(&qedi_iscsi_transport);
+ *   - drivers/scsi/qla4xxx/ql4_os.c|9900| <<qla4xxx_module_init>> iscsi_register_transport(&qla4xxx_iscsi_transport);
+ */
 struct scsi_transport_template *
 iscsi_register_transport(struct iscsi_transport *tt)
 {
diff --git a/include/linux/bio.h b/include/linux/bio.h
index 3cdb84c..1543c81 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -23,30 +23,99 @@
 
 #define BIO_MAX_PAGES		256
 
+/*
+ * 设置bi_ioprio的地方:
+ *   - block/bio.c|587| <<__bio_clone_fast>> bio->bi_ioprio = bio_src->bi_ioprio;
+ *   - block/bounce.c|251| <<bounce_clone_bio>> bio->bi_ioprio = bio_src->bi_ioprio;
+ *   - fs/block_dev.c|233| <<__blkdev_direct_IO_simple>> bio.bi_ioprio = iocb->ki_ioprio;
+ *   - fs/block_dev.c|384| <<__blkdev_direct_IO>> bio->bi_ioprio = iocb->ki_ioprio;
+ *   - fs/iomap/direct-io.c|271| <<iomap_dio_bio_actor>> bio->bi_ioprio = dio->iocb->ki_ioprio;
+ *   - include/linux/bio.h|27| <<bio_set_prio>> #define bio_set_prio(bio, prio) ((bio)->bi_ioprio = prio)
+ */
 #define bio_prio(bio)			(bio)->bi_ioprio
+/*
+ * called by:
+ *   - drivers/md/bcache/movinggc.c|85| <<moving_init>> bio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));
+ *   - drivers/md/bcache/writeback.c|256| <<dirty_init>> bio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));
+ */
 #define bio_set_prio(bio, prio)		((bio)->bi_ioprio = prio)
 
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bio_iter_iovec(bio, iter)				\
 	bvec_iter_bvec((bio)->bi_io_vec, (iter))
 
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bio_iter_page(bio, iter)				\
 	bvec_iter_page((bio)->bi_io_vec, (iter))
+/*
+ * 二个进行比较:
+ * 1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * 2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ */
 #define bio_iter_len(bio, iter)					\
 	bvec_iter_len((bio)->bi_io_vec, (iter))
+/*
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
 #define bio_iter_offset(bio, iter)				\
 	bvec_iter_offset((bio)->bi_io_vec, (iter))
 
+/*
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bio_page(bio)		bio_iter_page((bio), (bio)->bi_iter)
+/*
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
 #define bio_offset(bio)		bio_iter_offset((bio), (bio)->bi_iter)
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bio_iovec(bio)		bio_iter_iovec((bio), (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|315| <<bio_get_last_bvec>> if (unlikely(!bio_multiple_segments(bio))) {
+ *
+ * 判断是否有多个segment (bvec???)
+ */
 #define bio_multiple_segments(bio)				\
 	((bio)->bi_iter.bi_size != bio_iovec(bio).bv_len)
 
+/*
+ * bio的iter所表示的sector的数量
+ */
 #define bvec_iter_sectors(iter)	((iter).bi_size >> 9)
+/*
+ * bio的iter表示的最后一个vector
+ */
 #define bvec_iter_end_sector(iter) ((iter).bi_sector + bvec_iter_sectors((iter)))
 
+/*
+ * bio的iter所表示的sector的数量
+ */
 #define bio_sectors(bio)	bvec_iter_sectors((bio)->bi_iter)
+/*
+ * bio的iter表示的最后一个vector
+ */
 #define bio_end_sector(bio)	bvec_iter_end_sector((bio)->bi_iter)
 
 /*
@@ -70,6 +139,10 @@ static inline bool bio_has_data(struct bio *bio)
 	return false;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|204| <<bio_advance_iter>> if (bio_no_advance_iter(bio))
+ */
 static inline bool bio_no_advance_iter(struct bio *bio)
 {
 	return bio_op(bio) == REQ_OP_DISCARD ||
@@ -78,6 +151,10 @@ static inline bool bio_no_advance_iter(struct bio *bio)
 	       bio_op(bio) == REQ_OP_WRITE_ZEROES;
 }
 
+/*
+ * 测试bio->bi_opf是否设置了REQ_NOMERGE_FLAGS
+ * 定义是(REQ_NOMERGE | REQ_PREFLUSH | REQ_FUA)
+ */
 static inline bool bio_mergeable(struct bio *bio)
 {
 	if (bio->bi_opf & REQ_NOMERGE_FLAGS)
@@ -110,6 +187,9 @@ static inline void *bio_data(struct bio *bio)
  * Return true if @bio is full and one segment with @len bytes can't be
  * added to the bio, otherwise return false
  */
+/*
+ * 不能再添加bio_vec了
+ */
 static inline bool bio_full(struct bio *bio, unsigned len)
 {
 	if (bio->bi_vcnt >= bio->bi_max_vecs)
@@ -121,6 +201,10 @@ static inline bool bio_full(struct bio *bio, unsigned len)
 	return false;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|153| <<bio_for_each_segment_all>> for (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )
+ */
 static inline bool bio_next_segment(const struct bio *bio,
 				    struct bvec_iter_all *iter)
 {
@@ -150,15 +234,57 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 		/* TODO: It is reasonable to complete bio with error here. */
 }
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|178| <<bio_integrity_process>> __bio_for_each_segment(bv, bio, bviter, *proc_iter) {
+ *   - block/bio.c|529| <<zero_fill_bio_iter>> __bio_for_each_segment(bv, bio, iter, start) {
+ *   - drivers/block/aoe/aoecmd.c|302| <<skb_fillup>> __bio_for_each_segment(bv, bio, iter, iter)
+ *   - drivers/block/aoe/aoecmd.c|1030| <<bvcpy>> __bio_for_each_segment(bv, bio, iter, iter) {
+ *   - drivers/md/dm-integrity.c|1530| <<integrity_metadata>> __bio_for_each_segment(bv, bio, iter, dio->orig_bi_iter) {
+ *   - include/linux/bio.h|259| <<bio_for_each_segment>> __bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
+ *   - include/linux/ceph/messenger.h|125| <<ceph_bio_iter_advance_step>> __bio_for_each_segment(bv, (it)->bio, __cur_iter, __cur_iter) \
+ *  
+ * bio_iter_iovec():
+ *
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ *
+ *
+ * 从start开始, 遍历每一个最大1个page的bvec
+ *
+ * iter和start是struct bvec_iter
+ */
 #define __bio_for_each_segment(bvl, bio, iter, start)			\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
 		((bvl = bio_iter_iovec((bio), (iter))), 1);		\
 	     bio_advance_iter((bio), &(iter), (bvl).bv_len))
 
+/*
+ * 从bio->bi_iter开始, 遍历每一个最大1个page的bvec
+ */
 #define bio_for_each_segment(bvl, bio, iter)				\
 	__bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|279| <<bio_for_each_bvec>> __bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)
+ *
+ * mp_bvec_iter_bvec():
+ * 构造一个bvec:
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * bv_offset : 所以返回的是当前bvec"完成到"的offset
+ *
+ *
+ * 这里相当于以bvec(不是segment或者1个page)来遍历, 可以是多个page
+ */
 #define __bio_for_each_bvec(bvl, bio, iter, start)		\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -169,8 +295,29 @@ static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
 #define bio_for_each_bvec(bvl, bio, iter)			\
 	__bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)
 
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_main.c|1600| <<_drbd_send_bio>> bio_iter_last(bvec, iter)
+ *   - drivers/block/drbd/drbd_main.c|1622| <<_drbd_send_zc_bio>> bio_iter_last(bvec, iter) ? 0 : MSG_MORE);
+ *   - drivers/block/nbd.c|574| <<nbd_send_cmd>> bool is_last = !next && bio_iter_last(bvec, iter);
+ *   - include/linux/blkdev.h|832| <<rq_iter_last>> bio_iter_last(bvec, _iter.iter))
+ *
+ * 如果bvec的长度等于全部bvec剩余的长度, 说明是最后一个bvec
+ */
 #define bio_iter_last(bvec, iter) ((iter).bi_size == (bvec).bv_len)
 
+/*
+ * called by:
+ *   - block/bounce.c|246| <<bounce_clone_bio>> bio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);
+ *   - drivers/md/bcache/debug.c|114| <<bch_data_verify>> check = bio_kmalloc(GFP_NOIO, bio_segments(bio));
+ *   - drivers/md/dm-log-writes.c|702| <<log_writes_map>> alloc_size = struct_size(block, vecs, bio_segments(bio));
+ *   - drivers/nvme/host/tcp.c|228| <<nvme_tcp_init_iter>> nsegs = bio_segments(bio);
+ *   - drivers/target/target_core_pscsi.c|908| <<pscsi_map_sg>> bio_segments(bio), nr_vecs);
+ *   - fs/btrfs/check-integrity.c|2811| <<__btrfsic_submit_bio>> unsigned int segs = bio_segments(bio);
+ *   - fs/btrfs/inode.c|7890| <<dio_read_error>> segs = bio_segments(failed_bio);
+ *
+ * 似乎是计算bio一共多少page (一个bvec可能有多个page)
+ */
 static inline unsigned bio_segments(struct bio *bio)
 {
 	unsigned segs = 0;
@@ -193,6 +340,9 @@ static inline unsigned bio_segments(struct bio *bio)
 		break;
 	}
 
+	/*
+	 * 从bio->bi_iter开始, 遍历每一个最大1个page的bvec
+	 */
 	bio_for_each_segment(bv, bio, iter)
 		segs++;
 
@@ -244,8 +394,28 @@ static inline void bio_clear_flag(struct bio *bio, unsigned int bit)
 	bio->bi_flags &= ~(1U << bit);
 }
 
+/*
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较:
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 static inline void bio_get_first_bvec(struct bio *bio, struct bio_vec *bv)
 {
+	/*
+	 * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+	 * bv_len    : 二个进行比较:
+	 *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+	 *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+	 * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+	 *
+	 * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+	 * 不像mp_bvec_iter_bvec()是基于多个page的
+	 */
 	*bv = bio_iovec(bio);
 }
 
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3fa1fa5..ac2ff31 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -20,8 +20,50 @@ struct blk_mq_hw_ctx {
 	} ____cacheline_aligned_in_smp;
 
 	struct delayed_work	run_work;
+	/*
+	 * 设置cpumask的地方:
+	 *   - block/blk-mq.c|2602| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 *   - block/blk-mq.c|2560| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 * 测试cpumask的地方:
+	 *   - block/blk-mq-sysfs.c|172| <<blk_mq_hw_sysfs_cpus_show>> for_each_cpu(i, hctx->cpumask) {
+	 *   - block/blk-mq.c|475| <<blk_mq_alloc_request_hctx>> cpu = cpumask_first_and(alloc_data.hctx->cpumask, cpu_online_mask);
+	 *   - block/blk-mq.c|1407| <<__blk_mq_run_hw_queue>> if (!cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask) &&
+	 *   - block/blk-mq.c|1411| <<__blk_mq_run_hw_queue>> cpumask_empty(hctx->cpumask) ? "inactive": "active");
+	 *   - block/blk-mq.c|1430| <<blk_mq_first_mapped_cpu>> int cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);
+	 *   - block/blk-mq.c|1433| <<blk_mq_first_mapped_cpu>> cpu = cpumask_first(hctx->cpumask);
+	 *   - block/blk-mq.c|1478| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1516| <<__blk_mq_delay_run_hw_queue>> if (cpumask_test_cpu(cpu, hctx->cpumask)) {
+	 *   - block/blk-mq.c|2599| <<blk_mq_map_swqueue>> if (cpumask_test_cpu(i, hctx->cpumask))
+	 * 其他使用cpumask的地方:
+	 *   - block/blk-mq-sysfs.c|45| <<blk_mq_hw_sysfs_release>> free_cpumask_var(hctx->cpumask);
+	 *   - block/blk-mq.c|2431| <<blk_mq_alloc_hctx>> if (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))
+	 *   - block/blk-mq.c|2481| <<blk_mq_alloc_hctx>> free_cpumask_var(hctx->cpumask);
+	 *
+	 * 表示这个hctx都map了那些sw cpu
+	 */
 	cpumask_var_t		cpumask;
+	/*
+	 * 在以下设置next_cpu:
+	 *   - block/blk-mq.c|1474| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|1479| <<blk_mq_hctx_next_cpu>> hctx->next_cpu = next_cpu;
+	 *   - block/blk-mq.c|2626| <<blk_mq_map_swqueue>> hctx->next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 * 在以下使用next_cpu:
+	 *   - block/blk-mq.c|1408| <<__blk_mq_run_hw_queue>> cpu_online(hctx->next_cpu)) {
+	 *   - block/blk-mq.c|1446| <<blk_mq_hctx_next_cpu>> int next_cpu = hctx->next_cpu;
+	 *   - block/blk-mq.c|1453| <<blk_mq_hctx_next_cpu>> next_cpu = cpumask_next_and(next_cpu, hctx->cpumask,
+	 *   - block/blk-mq.c|1455| <<blk_mq_hctx_next_cpu>> if (next_cpu >= nr_cpu_ids)
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> next_cpu = blk_mq_first_mapped_cpu(hctx);
+	 *   - block/blk-mq.c|1464| <<blk_mq_hctx_next_cpu>> if (!cpu_online(next_cpu)) {
+	 *   - block/blk-mq.c|1480| <<blk_mq_hctx_next_cpu>> return next_cpu;
+	 */
 	int			next_cpu;
+	/*
+	 * 在以下使用next_cpu_batch:
+	 *   - block/blk-mq.c|1469| <<blk_mq_hctx_next_cpu>> if (--hctx->next_cpu_batch <= 0) {
+	 *   - block/blk-mq.c|1475| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|1493| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = 1;
+	 *   - block/blk-mq.c|2645| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	int			next_cpu_batch;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
@@ -78,7 +120,33 @@ struct blk_mq_hw_ctx {
 
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
+	/*
+	 * 在以下修改nr_queues:
+	 *   - block/blk-mq.c|3276| <<blk_mq_alloc_tag_set>> set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
+	 *   - drivers/nvme/host/rdma.c|2366| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2369| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2375| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2378| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 *   - drivers/nvme/host/rdma.c|2389| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2135| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2138| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues = 
+	 *   - drivers/nvme/host/tcp.c|2144| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].nr_queues =
+	 *   - drivers/nvme/host/tcp.c|2147| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].nr_queues =
+	 */
 	unsigned int nr_queues;
+	/*
+	 * 设置queue_offset的地方:
+	 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+	 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 */
 	unsigned int queue_offset;
 };
 
@@ -98,6 +166,35 @@ struct blk_mq_tag_set {
 	 * share maps between types.
 	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 设置nr_maps的地方:
+	 *   - block/blk-mq.c|2899| <<blk_mq_init_sq_queue>> set->nr_maps = 1; 
+	 *   - block/blk-mq.c|3234| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3245| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/block/sx8.c|1463| <<carm_init_one>> host->tag_set.nr_maps = 1;
+	 *   - drivers/block/paride/pd.c|908| <<pd_probe_drive>> disk->tag_set.nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2257| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2259| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 *   - drivers/nvme/host/rdma.c|1076| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	 *   - drivers/nvme/host/tcp.c|1472| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+	 *
+	 * 使用nr_maps的地方:
+	 *   - block/blk-mq.c|2585| <<blk_mq_init_cpu_queues>> for (j = 0; j < set->nr_maps; j++) {
+	 *   - block/blk-mq.c|2672| <<blk_mq_map_swqueue>> for (j = 0; j < set->nr_maps; j++) {
+	 *   - block/blk-mq.c|3020| <<nr_hw_queues>> if (set->nr_maps == 1)
+	 *   - block/blk-mq.c|3063| <<blk_mq_init_allocated_queue>> if (set->nr_maps > HCTX_TYPE_POLL &&
+	 *   - block/blk-mq.c|3192| <<blk_mq_update_queue_map>> for (i = 0; i < set->nr_maps; i++)
+	 *   - block/blk-mq.c|3197| <<blk_mq_update_queue_map>> BUG_ON(set->nr_maps > 1);
+	 *   - block/blk-mq.c|3233| <<blk_mq_alloc_tag_set>> if (!set->nr_maps)
+	 *   - block/blk-mq.c|3235| <<blk_mq_alloc_tag_set>> else if (set->nr_maps > HCTX_MAX_TYPES)
+	 *   - block/blk-mq.c|3252| <<blk_mq_alloc_tag_set>> if (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)
+	 *   - block/blk-mq.c|3261| <<blk_mq_alloc_tag_set>> for (i = 0; i < set->nr_maps; i++) {
+	 *   - block/blk-mq.c|3284| <<blk_mq_alloc_tag_set>> for (i = 0; i < set->nr_maps; i++) {
+	 *   - block/blk-mq.c|3301| <<blk_mq_free_tag_set>> for (j = 0; j < set->nr_maps; j++) {
+	 *   - block/blk-mq.c|3439| <<__blk_mq_update_nr_hw_queues>> if (set->nr_maps == 1 && nr_hw_queues > nr_cpu_ids)
+	 *   - block/blk-sysfs.c|404| <<queue_poll_store>> if (!q->tag_set || q->tag_set->nr_maps <= HCTX_TYPE_POLL ||
+	 *   - drivers/nvme/host/pci.c|436| <<nvme_pci_map_queues>> for (i = 0, qoff = 0; i < set->nr_maps; i++) {
+	 */
 	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
@@ -220,6 +317,17 @@ enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 设置和使用BLK_MQ_F_NO_SCHED的地方:
+	 *   - block/blk-mq.c|2947| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
+	 *   - block/bsg-lib.c|381| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - block/elevator.c|690| <<elv_support_iosched>> if (q->tag_set && (q->tag_set->flags & BLK_MQ_F_NO_SCHED))
+	 *   - drivers/block/null_blk_main.c|1556| <<null_init_tag_set>> set->flags |= BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/fc.c|3105| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/pci.c|1616| <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/rdma.c|1062| <<nvme_rdma_alloc_tagset>> set->flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/target/loop.c|367| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
@@ -230,6 +338,11 @@ enum {
 
 	BLK_MQ_MAX_DEPTH	= 10240,
 
+	/*
+	 * 在以下使用BLK_MQ_CPU_WORK_BATCH = 8:
+	 *   - block/blk-mq.c|1499| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|2674| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
 #define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index feff3fe..132b656 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -149,6 +149,15 @@ struct bio {
 						 * accessors.
 						 */
 	unsigned short		bi_flags;	/* status, etc and bvec pool number */
+	/*
+	 * 设置bi_ioprio的地方:
+	 *   - block/bio.c|587| <<__bio_clone_fast>> bio->bi_ioprio = bio_src->bi_ioprio;
+	 *   - block/bounce.c|251| <<bounce_clone_bio>> bio->bi_ioprio = bio_src->bi_ioprio;
+	 *   - fs/block_dev.c|233| <<__blkdev_direct_IO_simple>> bio.bi_ioprio = iocb->ki_ioprio;
+	 *   - fs/block_dev.c|384| <<__blkdev_direct_IO>> bio->bi_ioprio = iocb->ki_ioprio;
+	 *   - fs/iomap/direct-io.c|271| <<iomap_dio_bio_actor>> bio->bi_ioprio = dio->iocb->ki_ioprio;
+	 *   - include/linux/bio.h|27| <<bio_set_prio>> #define bio_set_prio(bio, prio) ((bio)->bi_ioprio = prio)
+	 */
 	unsigned short		bi_ioprio;
 	unsigned short		bi_write_hint;
 	blk_status_t		bi_status;
@@ -156,6 +165,14 @@ struct bio {
 
 	struct bvec_iter	bi_iter;
 
+	/*
+	 * 在以下使用__bi_remaining:
+	 *   - block/bio.c|271| <<bio_init>> atomic_set(&bio->__bi_remaining, 1);
+	 *   - block/bio.c|297| <<bio_reset>> atomic_set(&bio->__bi_remaining, 1);
+	 *   - block/bio.c|1772| <<bio_remaining_done>> BUG_ON(atomic_read(&bio->__bi_remaining) <= 0);
+	 *   - block/bio.c|1774| <<bio_remaining_done>> if (atomic_dec_and_test(&bio->__bi_remaining)) {
+	 *   - include/linux/bio.h|861| <<bio_inc_remaining>> atomic_inc(&bio->__bi_remaining);
+	 */
 	atomic_t		__bi_remaining;
 	bio_end_io_t		*bi_end_io;
 
@@ -210,12 +227,23 @@ enum {
 	BIO_USER_MAPPED,	/* contains user pages */
 	BIO_NULL_MAPPED,	/* contains invalid user pages */
 	BIO_QUIET,		/* Make BIO Quiet */
+	/*
+	 * 在以下使用BIO_CHAIN:
+	 *   - block/bio.c|1769| <<bio_remaining_done>> if (!bio_flagged(bio, BIO_CHAIN))
+	 *   - block/bio.c|1775| <<bio_remaining_done>> bio_clear_flag(bio, BIO_CHAIN);
+	 *   - include/linux/bio.h|859| <<bio_inc_remaining>> bio_set_flag(bio, BIO_CHAIN);
+	 */
 	BIO_CHAIN,		/* chained bio, ->bi_remaining in effect */
 	BIO_REFFED,		/* bio has elevated ->bi_cnt */
 	BIO_THROTTLED,		/* This bio has already been subjected to
 				 * throttling rules. Don't do it again. */
 	BIO_TRACE_COMPLETION,	/* bio_endio() should trace the final completion
 				 * of this bio. */
+	/*
+	 * 在以下使用BIO_QUEUE_ENTERED:
+	 *   - block/blk-merge.c|294| <<__blk_queue_split>> bio_set_flag(*bio, BIO_QUEUE_ENTERED);
+	 *   - include/linux/blk-cgroup.h|751| <<blkcg_bio_issue_check>> if (!bio_flagged(bio, BIO_QUEUE_ENTERED))
+	 */
 	BIO_QUEUE_ENTERED,	/* can use blk_queue_enter_live() */
 	BIO_TRACKED,		/* set if bio goes through the rq_qos path */
 	BIO_FLAG_LAST
@@ -356,6 +384,11 @@ enum req_flag_bits {
 #define REQ_FAILFAST_MASK \
 	(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT | REQ_FAILFAST_DRIVER)
 
+/*
+ * 在以下使用REQ_NOMERGE_FLAGS:
+ *   - include/linux/bio.h|137| <<bio_mergeable>> if (bio->bi_opf & REQ_NOMERGE_FLAGS)
+ *   - include/linux/blkdev.h|752| <<rq_mergeable>> if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
+ */
 #define REQ_NOMERGE_FLAGS \
 	(REQ_NOMERGE | REQ_PREFLUSH | REQ_FUA)
 
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ef375d..a37b025 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -99,6 +99,11 @@ typedef __u32 __bitwise req_flags_t;
 /* on IO scheduler merge hash */
 #define RQF_HASHED		((__force req_flags_t)(1 << 16))
 /* track IO completion time */
+/*
+ * 在以下使用RQF_STATS:
+ *   - block/blk-mq.c|543| <<__blk_mq_end_request>> if (rq->rq_flags & RQF_STATS) {
+ *   - block/blk-mq.c|706| <<blk_mq_start_request>> rq->rq_flags |= RQF_STATS;
+ */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
    bio chain. */
@@ -108,6 +113,13 @@ typedef __u32 __bitwise req_flags_t;
 /* already slept for hybrid poll */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
+/*
+ * 在以下使用RQF_TIMED_OUT (timeout has been called, don't expire again):
+ *   - block/blk-mq.c|710| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-mq.c|843| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+ *   - block/blk-mq.c|862| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+ *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+ */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
@@ -311,6 +323,14 @@ enum blk_zoned_model {
 };
 
 struct queue_limits {
+	/*
+	 * 设置bounce_pfn的地方:
+	 *   - block/blk-settings.c|56| <<blk_set_default_limits>> lim->bounce_pfn = (unsigned long )(BLK_BOUNCE_ANY >> PAGE_SHIFT);
+	 *   - block/blk-settings.c|149| <<blk_queue_bounce_limit>> q->limits.bounce_pfn = max(max_low_pfn, b_pfn);
+	 *   - block/blk-settings.c|153| <<blk_queue_bounce_limit>> q->limits.bounce_pfn = b_pfn;
+	 *   - block/blk-settings.c|158| <<blk_queue_bounce_limit>> q->limits.bounce_pfn = b_pfn;
+	 *   - block/blk-settings.c|508| <<blk_stack_limits>> t->bounce_pfn = min_not_zero(t->bounce_pfn, b->bounce_pfn);
+	 */
 	unsigned long		bounce_pfn;
 	unsigned long		seg_boundary_mask;
 	unsigned long		virt_boundary_mask;
@@ -590,6 +610,13 @@ struct request_queue {
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
+/*
+ * 在以下使用QUEUE_FLAG_FAIL_IO:
+ *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+ *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+ *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+ *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+ */
 #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
 #define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
@@ -605,6 +632,13 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 在以下使用QUEUE_FLAG_STATS:
+ *   - block/blk-mq.c|701| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|151| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|161| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|187| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
diff --git a/include/linux/bvec.h b/include/linux/bvec.h
index a032f01..0b51092 100644
--- a/include/linux/bvec.h
+++ b/include/linux/bvec.h
@@ -42,22 +42,56 @@ struct bvec_iter_all {
  * various member access, note that bio_data should of course not be used
  * on highmem page vectors
  */
+/*
+ * 返回bio_vec数组中的bvec_iter指向的(bvec_iter->bi_idx)当前正处理的元素
+ */
 #define __bvec_iter_bvec(bvec, iter)	(&(bvec)[(iter).bi_idx])
 
 /* multi-page (mp_bvec) helpers */
+/*
+ * called by:
+ *   - include/linux/bvec.h|63| <<mp_bvec_iter_bvec>> .bv_page = mp_bvec_iter_page((bvec), (iter)), \
+ *   - include/linux/bvec.h|77| <<bvec_iter_page>> (mp_bvec_iter_page((bvec), (iter)) + \
+ *
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ */
 #define mp_bvec_iter_page(bvec, iter)				\
 	(__bvec_iter_bvec((bvec), (iter))->bv_page)
 
+/*
+ * 这里二个选最小的:
+ * 1. (iter).bi_size: 所有bvec剩下的
+ * 2. __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done: 当前bvec剩下的
+ *
+ * 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ */
 #define mp_bvec_iter_len(bvec, iter)				\
 	min((iter).bi_size,					\
 	    __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done)
 
+/*
+ * __bvec_iter_bvec((bvec), (iter))->bv_offset: 当前bvec起始的offset
+ * (iter).bi_bvec_done: 当前bvec已经完成的
+ *
+ * 所以返回的是当前bvec"完成到"的offset
+ */
 #define mp_bvec_iter_offset(bvec, iter)				\
 	(__bvec_iter_bvec((bvec), (iter))->bv_offset + (iter).bi_bvec_done)
 
+/* 返回当前bvec中已经完成到的offset所在的以PAGE_SIZE为单位的index */
 #define mp_bvec_iter_page_idx(bvec, iter)			\
 	(mp_bvec_iter_offset((bvec), (iter)) / PAGE_SIZE)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|165| <<__bio_for_each_bvec>> ((bvl = mp_bvec_iter_bvec((bio)->bi_io_vec, (iter))), 1); \
+ *   - include/linux/blkdev.h|981| <<req_bvec>> return mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);
+ *
+ * 构造一个bvec:
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * bv_len    : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * bv_offset : 所以返回的是当前bvec"完成到"的offset
+ */
 #define mp_bvec_iter_bvec(bvec, iter)				\
 ((struct bio_vec) {						\
 	.bv_page	= mp_bvec_iter_page((bvec), (iter)),	\
@@ -66,17 +100,64 @@ struct bvec_iter_all {
 })
 
 /* For building single-page bvec in flight */
+/*
+ * called by:
+ *   - include/linux/bio.h|51| <<bio_iter_offset>> bvec_iter_offset((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|108| <<bvec_iter_len>> PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
+ *   - include/linux/bvec.h|118| <<bvec_iter_bvec>> .bv_offset = bvec_iter_offset((bvec), (iter)), \
+ *
+ * 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ */
  #define bvec_iter_offset(bvec, iter)				\
 	(mp_bvec_iter_offset((bvec), (iter)) % PAGE_SIZE)
 
+/*
+ * called by:
+ *   - include/linux/bio.h|49| <<bio_iter_len>> bvec_iter_len((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|117| <<bvec_iter_bvec>> .bv_len = bvec_iter_len((bvec), (iter)), \
+ *   - net/ceph/messenger.c|885| <<ceph_msg_data_bvecs_cursor_init>> BUG_ON(cursor->resid < bvec_iter_len(bvecs, cursor->bvec_iter));
+ *   - net/ceph/messenger.c|887| <<ceph_msg_data_bvecs_cursor_init>> cursor->resid == bvec_iter_len(bvecs, cursor->bvec_iter);
+ *   - net/ceph/messenger.c|909| <<ceph_msg_data_bvecs_advance>> BUG_ON(bytes > bvec_iter_len(bvecs, cursor->bvec_iter));
+ *   - net/ceph/messenger.c|923| <<ceph_msg_data_bvecs_advance>> BUG_ON(cursor->resid < bvec_iter_len(bvecs, cursor->bvec_iter));
+ *   - net/ceph/messenger.c|925| <<ceph_msg_data_bvecs_advance>> cursor->resid == bvec_iter_len(bvecs, cursor->bvec_iter);
+ *
+ * 二个进行比较:
+ * 1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ * 2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ */
 #define bvec_iter_len(bvec, iter)				\
 	min_t(unsigned, mp_bvec_iter_len((bvec), (iter)),		\
 	      PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
 
+/*
+ * mp_bvec_iter_page((bvec), (iter)): 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page (struct page指针)
+ * mp_bvec_iter_page_idx((bvec), (iter)): 返回当前bvec中已经完成到的offset所在的以PAGE_SIZE为单位的index
+ *
+ * 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ */
 #define bvec_iter_page(bvec, iter)				\
 	(mp_bvec_iter_page((bvec), (iter)) +			\
 	 mp_bvec_iter_page_idx((bvec), (iter)))
 
+/*
+ * called by:
+ *   - drivers/md/dm-integrity.c|1758| <<__journal_read_write>> struct bio_vec biv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - drivers/md/dm-io.c|211| <<bio_get_page>> struct bio_vec bvec = bvec_iter_bvec((struct bio_vec *)dp->context_ptr,
+ *   - drivers/nvdimm/blk.c|84| <<nd_blk_rw_integrity>> bv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - drivers/nvdimm/btt.c|1158| <<btt_rw_integrity>> bv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);
+ *   - include/linux/bio.h|44| <<bio_iter_iovec>> bvec_iter_bvec((bio)->bi_io_vec, (iter))
+ *   - include/linux/bvec.h|150| <<for_each_bvec>> ((bvl = bvec_iter_bvec((bio_vec), (iter))), 1); \
+ *   - net/ceph/messenger.c|894| <<ceph_msg_data_bvecs_next>> struct bio_vec bv = bvec_iter_bvec(cursor->data->bvec_pos.bvecs,
+ *
+ * bv_page   : 返回当前bvec_iter指向的bio_vec数组处理到的bio_vec的bv_page中的page
+ * bv_len    : 二个进行比较: 
+ *             1. mp_bvec_iter_len((bvec), (iter))             : 所以返回的是所有bvec剩下的和当前bvec剩下的size最小值
+ *             2. PAGE_SIZE - bvec_iter_offset((bvec), (iter)) : 当前bvec在一个PAGE_SIZE内还没完成的部分
+ * bv_offset : 返回的是当前bvec"完成到"的offset在一个PAGE_SIZE内的offset
+ *
+ * 这里相当于生成了一个bio_vec, 是限定在某"一"个page内的
+ * 不像mp_bvec_iter_bvec()是基于多个page的
+ */
 #define bvec_iter_bvec(bvec, iter)				\
 ((struct bio_vec) {						\
 	.bv_page	= bvec_iter_page((bvec), (iter)),	\
@@ -84,6 +165,19 @@ struct bvec_iter_all {
 	.bv_offset	= bvec_iter_offset((bvec), (iter)),	\
 })
 
+/*
+ * called by:
+ *   - block/bio-integrity.c|388| <<bio_integrity_advance>> bvec_iter_advance(bip->bip_vec, &bip->bip_iter, bytes);
+ *   - drivers/md/dm-integrity.c|1767| <<__journal_read_write>> bvec_iter_advance(bip->bip_vec, &bip->bip_iter, tag_now);
+ *   - drivers/md/dm-io.c|226| <<bio_next_page>> bvec_iter_advance((struct bio_vec *)dp->context_ptr,
+ *   - drivers/nvdimm/blk.c|101| <<nd_blk_rw_integrity>> if (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))
+ *   - drivers/nvdimm/btt.c|1182| <<btt_rw_integrity>> if (!bvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len))
+ *   - include/linux/bio.h|163| <<bio_advance_iter>> bvec_iter_advance(bio->bi_io_vec, iter, bytes);
+ *   - include/linux/bvec.h|151| <<for_each_bvec>> bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
+ *   - include/linux/ceph/messenger.h|139| <<__ceph_bvec_iter_advance_step>> bvec_iter_advance((it)->bvecs, &(it)->iter, (n)); \
+ *   - net/ceph/messenger.c|911| <<ceph_msg_data_bvecs_advance>> bvec_iter_advance(bvecs, &cursor->bvec_iter, bytes);
+ *   - net/sunrpc/xprtsock.c|390| <<xs_flush_bvec>> bvec_iter_advance(bvec, &bi, seek & PAGE_MASK);
+ */
 static inline bool bvec_iter_advance(const struct bio_vec *bv,
 		struct bvec_iter *iter, unsigned bytes)
 {
@@ -95,6 +189,12 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 
 	while (bytes) {
 		const struct bio_vec *cur = bv + iter->bi_idx;
+		/*
+		 * 三个里面选择最小值:
+		 * 1. bytes                           : 想要advance的
+		 * 2. iter->bi_size                   : 所有bvec剩下的
+		 * 3. cur->bv_len - iter->bi_bvec_done: 当前bvec剩下的
+		 */
 		unsigned len = min3(bytes, iter->bi_size,
 				    cur->bv_len - iter->bi_bvec_done);
 
@@ -102,6 +202,11 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 		iter->bi_size -= len;
 		iter->bi_bvec_done += len;
 
+		/*
+		 * 如果当前bio_vec已经用完了, 则要进入下一个bio_vec (iter->bi_idx++)
+		 * 清空iter->bi_bvec_done因为是number of bytes completed in current bvec
+		 * 现在要进入下一个了
+		 */
 		if (iter->bi_bvec_done == cur->bv_len) {
 			iter->bi_bvec_done = 0;
 			iter->bi_idx++;
@@ -110,6 +215,15 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	return true;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|757| <<bip_for_each_vec>> for_each_bvec(bvl, (bip)->bip_vec, iter, (bip)->bip_iter)
+ *   - include/linux/ceph/messenger.h|158| <<ceph_bvec_iter_advance_step>> for_each_bvec(bv, (it)->bvecs, __cur_iter, __cur_iter) \
+ *   - lib/iov_iter.c|70| <<iterate_bvec>> for_each_bvec(__v, i->bvec, __bi, __start) { \
+ *   - net/sunrpc/xprtsock.c|391| <<xs_flush_bvec>> for_each_bvec(bv, bvec, bi, bi)
+ *
+ * 这里第一个参数bvl是基于一个page的bvec
+ */
 #define for_each_bvec(bvl, bio_vec, iter, start)			\
 	for (iter = (start);						\
 	     (iter).bi_size &&						\
@@ -117,6 +231,10 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	     bvec_iter_advance((bio_vec), &(iter), (bvl).bv_len))
 
 /* for iterating one bio from start to end */
+/*
+ * called by:
+ *   - block/bounce.c|142| <<copy_to_high_bio_irq>> struct bvec_iter from_iter = BVEC_ITER_ALL_INIT;
+ */
 #define BVEC_ITER_ALL_INIT (struct bvec_iter)				\
 {									\
 	.bi_sector	= 0,						\
@@ -125,6 +243,10 @@ static inline bool bvec_iter_advance(const struct bio_vec *bv,
 	.bi_bvec_done	= 0,						\
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|153| <<bio_for_each_segment_all>> for (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )
+ */
 static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
 {
 	iter_all->done = 0;
@@ -133,6 +255,10 @@ static inline struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
 	return &iter_all->bv;
 }
 
+/*
+ * called by:
+ *   - include/linux/bio.h|144| <<bio_next_segment>> bvec_advance(&bio->bi_io_vec[iter->idx], iter);
+ */
 static inline void bvec_advance(const struct bio_vec *bvec,
 				struct bvec_iter_all *iter_all)
 {
@@ -149,6 +275,12 @@ static inline void bvec_advance(const struct bio_vec *bvec,
 			   bvec->bv_len - iter_all->done);
 	iter_all->done += bv->bv_len;
 
+	/*
+	 * 似乎iter_all->done是当前bvec完成的len?
+	 *
+	 * 所以这里当前bvec如果都完成了
+	 * 就要进入下一个bvec?? ---> iter_all->idx++
+	 */
 	if (iter_all->done == bvec->bv_len) {
 		iter_all->idx++;
 		iter_all->done = 0;
@@ -159,6 +291,10 @@ static inline void bvec_advance(const struct bio_vec *bvec,
  * Get the last single-page segment from the multi-page bvec and store it
  * in @seg
  */
+/*
+ * called by:
+ *   - fs/buffer.c|3042| <<guard_bio_eod>> mp_bvec_last_segment(bvec, &bv);
+ */
 static inline void mp_bvec_last_segment(const struct bio_vec *bvec,
 					struct bio_vec *seg)
 {
diff --git a/include/rdma/rdma_cm.h b/include/rdma/rdma_cm.h
index 71f48cf..b0cb2a0 100644
--- a/include/rdma/rdma_cm.h
+++ b/include/rdma/rdma_cm.h
@@ -45,6 +45,18 @@
  * RDMA identifier and release all resources allocated with the device.
  */
 enum rdma_cm_event_type {
+	/*
+	 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 */
 	RDMA_CM_EVENT_ADDR_RESOLVED,
 	RDMA_CM_EVENT_ADDR_ERROR,
 	RDMA_CM_EVENT_ROUTE_RESOLVED,
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 969e540..9ebff44 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -603,6 +603,11 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|62| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->bitmap_tags);
+ *   - block/blk-mq-tag.c|64| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->breserved_tags);
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -660,6 +665,10 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
 
+/*
+ * called by:
+ *   - block/kyber-iosched.c|723| <<kyber_get_domain_token>> sbitmap_add_wait_queue(domain_tokens, ws, wait);
+ */
 void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
 			    struct sbq_wait_state *ws,
 			    struct sbq_wait *sbq_wait)
@@ -682,6 +691,11 @@ void sbitmap_del_wait_queue(struct sbq_wait *sbq_wait)
 }
 EXPORT_SYMBOL_GPL(sbitmap_del_wait_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|188| <<blk_mq_get_tag>> sbitmap_prepare_to_wait(bt, ws, &wait, TASK_UNINTERRUPTIBLE);
+ *   - drivers/target/iscsi/iscsi_target_util.c|155| <<iscsit_wait_for_tag>> sbitmap_prepare_to_wait(sbq, ws, &wait, state);
+ */
 void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
 			     struct sbq_wait_state *ws,
 			     struct sbq_wait *sbq_wait, int state)
-- 
2.7.4

