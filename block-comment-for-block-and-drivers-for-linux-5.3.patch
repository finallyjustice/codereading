From 83c8ee5aeb62809733c2182fe8c7a83ba7d4e659 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 17 Sep 2019 16:12:02 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 drivers/nvme/host/fabrics.c |  11 ++
 drivers/nvme/host/fabrics.h |   9 +
 drivers/nvme/host/nvme.h    |  17 ++
 drivers/nvme/host/rdma.c    | 414 ++++++++++++++++++++++++++++++++++++++++++++
 drivers/nvme/target/core.c  |   7 +
 5 files changed, 458 insertions(+)

diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 1994d5b..432bcc1 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -490,6 +490,13 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3449| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|2328| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|691| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
@@ -977,6 +984,10 @@ EXPORT_SYMBOL_GPL(nvmf_free_options);
 				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
 				 NVMF_OPT_DISABLE_SQFLOW)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1083| <<nvmf_dev_write>> ctrl = nvmf_create_ctrl(nvmf_device, buf);
+ */
 static struct nvme_ctrl *
 nvmf_create_ctrl(struct device *dev, const char *buf)
 {
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index 3044d8b..055e773 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -99,6 +99,15 @@ struct nvmf_ctrl_options {
 	unsigned int		nr_io_queues;
 	unsigned int		reconnect_delay;
 	bool			discovery_nqn;
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {
+	 */
 	bool			duplicate_connect;
 	unsigned int		kato;
 	struct nvmf_host	*host;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 2d678fb..58c4078 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -179,6 +179,23 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 1a6449b..1dd2503 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -27,6 +27,9 @@
 #include "nvme.h"
 #include "fabrics.h"
 
+/*
+ * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+ */
 
 #define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
 
@@ -93,10 +96,25 @@ struct nvme_rdma_ctrl {
 
 	/* other member variables */
 	struct blk_mq_tag_set	tag_set;
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	struct work_struct	err_work;
 
 	struct nvme_rdma_qe	async_event_sqe;
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	struct delayed_work	reconnect_work;
 
 	struct list_head	list;
@@ -114,14 +132,33 @@ struct nvme_rdma_ctrl {
 	u32			io_queues[HCTX_MAX_TYPES];
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|729| <<nvme_rdma_alloc_tagset>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|951| <<nvme_rdma_free_ctrl>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+ *   - drivers/nvme/host/rdma.c|1922| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ */
 static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|373| <<nvme_rdma_find_get_device>> list_for_each_entry(ndev, &device_list, entry) {
+ *   - drivers/nvme/host/rdma.c|400| <<nvme_rdma_find_get_device>> list_add(&ndev->entry, &device_list);
+ *   - drivers/nvme/host/rdma.c|2119| <<nvme_rdma_remove_one>> list_for_each_entry(ndev, &device_list, entry) {
+ */
 static LIST_HEAD(device_list);
 static DEFINE_MUTEX(device_list_mutex);
 
+/*
+ * 在以下使用nvme_rdma_ctrl_list:
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+ *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ */
 static LIST_HEAD(nvme_rdma_ctrl_list);
 static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
 
@@ -130,6 +167,10 @@ static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
  * unsafe.  With it turned off we will have to register a global rkey that
  * allows read and write access to all physical memory.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|387| <<nvme_rdma_find_get_device>> register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+ */
 static bool register_always = true;
 module_param(register_always, bool, 0444);
 MODULE_PARM_DESC(register_always,
@@ -143,6 +184,12 @@ static const struct blk_mq_ops nvme_rdma_mq_ops;
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
 
 /* XXX: really should move to a generic header sooner or later.. */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1223| <<nvme_rdma_set_sg_null>> put_unaligned_le24(0, sg->length);
+ *   - drivers/nvme/host/rdma.c|1260| <<nvme_rdma_map_sg_single>> put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
+ *   - drivers/nvme/host/rdma.c|1304| <<nvme_rdma_map_sg_fr>> put_unaligned_le24(req->mr->length, sg->length);
+ */
 static inline void put_unaligned_le24(u32 val, u8 *p)
 {
 	*p++ = val;
@@ -152,11 +199,20 @@ static inline void put_unaligned_le24(u32 val, u8 *p)
 
 static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctrl
+	 */
 	return queue - queue->ctrl->queues;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|471| <<nvme_rdma_create_queue_ib>> if (nvme_rdma_poll_queue(queue))
+ *   - drivers/nvme/host/rdma.c|620| <<nvme_rdma_start_queue>> bool poll = nvme_rdma_poll_queue(queue);
+ */
 static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
 {
+	/* queue->ctrl是struct nvme_rdma_ctr * */
 	return nvme_rdma_queue_idx(queue) >
 		queue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +
 		queue->ctrl->io_queues[HCTX_TYPE_READ];
@@ -167,6 +223,12 @@ static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
 	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|201| <<nvme_rdma_free_ring>> nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ */
 static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -174,6 +236,11 @@ static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	kfree(qe->data);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|257| <<nvme_rdma_alloc_ring>> if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+ *   - drivers/nvme/host/rdma.c|842| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ */
 static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -191,6 +258,12 @@ static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|264| <<nvme_rdma_alloc_ring>> nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|474| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ *   - drivers/nvme/host/rdma.c|551| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ */
 static void nvme_rdma_free_ring(struct ib_device *ibdev,
 		struct nvme_rdma_qe *ring, size_t ib_queue_size,
 		size_t capsule_size, enum dma_data_direction dir)
@@ -202,6 +275,10 @@ static void nvme_rdma_free_ring(struct ib_device *ibdev,
 	kfree(ring);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+ */
 static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 		size_t ib_queue_size, size_t capsule_size,
 		enum dma_data_direction dir)
@@ -230,6 +307,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|300| <<nvme_rdma_create_qp>> init_attr.event_handler = nvme_rdma_qp_event;
+ */
 static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 {
 	pr_debug("QP event %s (%d)\n",
@@ -237,6 +318,10 @@ static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|634| <<nvme_rdma_alloc_queue>> ret = nvme_rdma_wait_for_cm(queue);
+ */
 static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -251,6 +336,10 @@ static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 	return queue->cm_error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|485| <<nvme_rdma_create_queue_ib>> ret = nvme_rdma_create_qp(queue, send_wr_factor);
+ */
 static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 {
 	struct nvme_rdma_device *dev = queue->device;
@@ -276,6 +365,10 @@ static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.exit_request = nvme_rdma_exit_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.exit_request = nvme_rdma_exit_request()
+ */
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
 {
@@ -284,6 +377,10 @@ static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 	kfree(req->sqe.data);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_request = nvme_rdma_init_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_request = nvme_rdma_init_request()
+ */
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -303,6 +400,9 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_hctx = nvme_rdma_init_hctx()
+ */
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -315,6 +415,9 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_hctx = nvme_rdma_init_admin_hctx()
+ */
 static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -350,6 +453,10 @@ static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
 	return kref_get_unless_zero(&dev->ref);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|530| <<nvme_rdma_create_queue_ib>> queue->device = nvme_rdma_find_get_device(queue->cm_id);
+ */
 static struct nvme_rdma_device *
 nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 {
@@ -430,6 +537,19 @@ static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
 		     ibdev->attrs.max_fast_reg_page_list_len);
 }
 
+/*
+ * [0] nvme_rdma_create_queue_ib
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1699| <<nvme_rdma_addr_resolved>> ret = nvme_rdma_create_queue_ib(queue);
+ */
 static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct ib_device *ibdev;
@@ -506,6 +626,20 @@ static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_alloc_queue [nvme_rdma]
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_io_queues>> ret = nvme_rdma_alloc_queue(ctrl, i,
+ *   - drivers/nvme/host/rdma.c|828| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ */
 static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		int idx, size_t queue_size)
 {
@@ -600,6 +734,11 @@ static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
 		nvme_rdma_stop_queue(&ctrl->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|715| <<nvme_rdma_start_io_queues>> ret = nvme_rdma_start_queue(ctrl, i);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_start_queue(ctrl, 0);
+ */
 static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 {
 	struct nvme_rdma_queue *queue = &ctrl->queues[idx];
@@ -639,6 +778,10 @@ static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_alloc_io_queues(ctrl);
+ */
 static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -709,6 +852,11 @@ static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|848| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ *   - drivers/nvme/host/rdma.c|925| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ */
 static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
 		bool admin)
 {
@@ -767,6 +915,10 @@ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_queue(&ctrl->queues[0]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1035| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_admin_queue(ctrl, new);
+ */
 static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 		bool new)
 {
@@ -860,6 +1012,10 @@ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_io_queues(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1180| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_io_queues(ctrl, new);
+ */
 static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret;
@@ -929,6 +1085,9 @@ static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
 	}
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.free_ctrl = nvme_rdma_free_ctrl()
+ */
 static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
@@ -946,6 +1105,12 @@ static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 	kfree(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1178| <<nvme_rdma_reconnect_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|1197| <<nvme_rdma_error_recovery_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|2114| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ */
 static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 {
 	/* If we are resetting/deleting then do nothing */
@@ -965,6 +1130,20 @@ static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 	}
 }
 
+/*
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1054| <<nvme_rdma_reconnect_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|1939| <<nvme_rdma_reset_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|2071| <<nvme_rdma_create_ctrl>> ret = nvme_rdma_setup_ctrl(ctrl, true);
+ */
 static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret = -EINVAL;
@@ -1027,6 +1206,13 @@ static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 	return ret;
 }
 
+/*
+ * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+ *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+ *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+ */
 static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
@@ -1050,6 +1236,14 @@ static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+ *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+ *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+ */
 static void nvme_rdma_error_recovery_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(work,
@@ -1069,6 +1263,16 @@ static void nvme_rdma_error_recovery_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1251| <<nvme_rdma_wr_error>> nvme_rdma_error_recovery(ctrl);
+ *   - drivers/nvme/host/rdma.c|1627| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1640| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1650| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1900| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1947| <<nvme_rdma_timeout>> nvme_rdma_error_recovery(ctrl);
+ */
 static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
@@ -1247,6 +1451,10 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1828| <<nvme_rdma_queue_rq>> err = nvme_rdma_map_data(queue, rq, c);
+ */
 static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		struct request *rq, struct nvme_command *c)
 {
@@ -1310,6 +1518,17 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_send_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1358,6 +1577,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1641| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1653| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1399,6 +1623,9 @@ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_rdma_wr_error(cq, wc, "ASYNC");
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.submit_async_event = nvme_rdma_submit_async_event()
+ */
 static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
@@ -1426,6 +1653,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1564| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1470,6 +1701,17 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * [0] nvme_rdma_recv_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1502,6 +1744,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1712| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -1545,6 +1791,10 @@ static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
 	return -ECONNRESET;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1706| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_addr_resolved(queue);
+ */
 static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -1568,6 +1818,10 @@ static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1709| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_route_resolved(queue);
+ */
 static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
@@ -1618,6 +1872,34 @@ static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_ib_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|580| <<nvme_rdma_alloc_queue>> queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+ */
 static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *ev)
 {
@@ -1679,6 +1961,10 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.timeout = nvme_rdma_timeout()
+ * struct blk_mq_ops nvme_rdma_mq_ops.timeout = nvme_rdma_timeout()
+ */
 static enum blk_eh_timer_return
 nvme_rdma_timeout(struct request *rq, bool reserved)
 {
@@ -1707,6 +1993,10 @@ nvme_rdma_timeout(struct request *rq, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ */
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1777,6 +2067,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.poll = nvme_rdma_poll()
+ */
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1784,6 +2077,10 @@ static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 	return ib_process_cq_direct(queue->ib_cq, -1);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.complete = nvme_rdma_complete_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.complete = nvme_rdma_complete_rq()
+ */
 static void nvme_rdma_complete_rq(struct request *rq)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
@@ -1796,6 +2093,9 @@ static void nvme_rdma_complete_rq(struct request *rq)
 	nvme_complete_rq(rq);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.map_queues = nvme_rdma_map_queues()
+ */
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
@@ -1863,6 +2163,11 @@ static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.timeout	= nvme_rdma_timeout,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1975| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ *   - drivers/nvme/host/rdma.c|1988| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_shutdown_ctrl(ctrl, false);
+ */
 static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 {
 	cancel_work_sync(&ctrl->err_work);
@@ -1876,11 +2181,18 @@ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.delete_ctrl = nvme_rdma_delete_ctrl()
+ */
 static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|2050| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ */
 static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl =
@@ -1930,6 +2242,10 @@ static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
  * The ports don't need to be compared as they are intrinsically
  * already matched by the port pointers supplied.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2021| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+ */
 static bool
 nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 {
@@ -1947,9 +2263,25 @@ nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 	return found;
 }
 
+/*
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1039| <<nvmf_create_ctrl>> ctrl = ops->create_ctrl(dev, opts);
+ *
+ * struct nvmf_transport_ops nvme_rdma_transport.create_ctrl = nvme_ctrl_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
+	/*
+	 * 包含struct nvme_ctrl和struct blk_mq_tag_set (不是指针)
+	 */
 	struct nvme_rdma_ctrl *ctrl;
 	int ret;
 	bool changed;
@@ -1970,6 +2302,9 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		opts->mask |= NVMF_OPT_TRSVCID;
 	}
 
+	/*
+	 * convert an IPv4/IPv6 and port to socket address
+	 */
 	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
 			opts->traddr, opts->trsvcid, &ctrl->addr);
 	if (ret) {
@@ -1988,14 +2323,55 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		}
 	}
 
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts))
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts))
+	 */
 	if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
 		ret = -EALREADY;
 		goto out_free_ctrl;
 	}
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	INIT_DELAYED_WORK(&ctrl->reconnect_work,
 			nvme_rdma_reconnect_ctrl_work);
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
 
 	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
@@ -2009,6 +2385,11 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	if (!ctrl->queues)
 		goto out_free_ctrl;
 
+	/*
+	 * 下面的命令到这里的时候queue_count是5
+	 * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+	 */
+
 	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
 				0 /* no quirks, we're perfect! */);
 	if (ret)
@@ -2024,9 +2405,16 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs\n",
 		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
 
+	/* 不知道在哪里put */
 	nvme_get_ctrl(&ctrl->ctrl);
 
 	mutex_lock(&nvme_rdma_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_rdma_ctrl_list:
+	 *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+	 *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 */
 	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
 	mutex_unlock(&nvme_rdma_ctrl_mutex);
 
@@ -2055,6 +2443,20 @@ static struct nvmf_transport_ops nvme_rdma_transport = {
 	.create_ctrl	= nvme_rdma_create_ctrl,
 };
 
+/*
+ * 在nvme_rdma_init_module()被用于ib_register_client - Register an IB client
+ *
+ * Upper level users of the IB drivers can use ib_register_client() to
+ * register callbacks for IB device addition and removal.  When an IB
+ * device is added, each registered client's add method will be called
+ * (in the order the clients were registered), and when a device is
+ * removed, each client's remove method will be called (in the reverse
+ * order that clients were registered).  In addition, when
+ * ib_register_client() is called, the client will receive an add
+ * callback for all devices already registered.
+ *
+ * struct ib_client nvme_rdma_ib_client.remove = nvme_rdma_remove_one()
+ */
 static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
 {
 	struct nvme_rdma_ctrl *ctrl;
@@ -2094,6 +2496,18 @@ static int __init nvme_rdma_init_module(void)
 {
 	int ret;
 
+	/*
+	 * ib_register_client - Register an IB client
+	 *
+	 * Upper level users of the IB drivers can use ib_register_client() to
+	 * register callbacks for IB device addition and removal.  When an IB
+	 * device is added, each registered client's add method will be called
+	 * (in the order the clients were registered), and when a device is
+	 * removed, each client's remove method will be called (in the reverse
+	 * order that clients were registered).  In addition, when
+	 * ib_register_client() is called, the client will receive an add
+	 * callback for all devices already registered.
+	 */
 	ret = ib_register_client(&nvme_rdma_ib_client);
 	if (ret)
 		return ret;
diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 3a67e24..bc0f841 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -260,6 +260,13 @@ void nvmet_port_send_ana_event(struct nvmet_port *port)
 	up_read(&nvmet_config_sem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2575| <<nvmet_fc_init_module>> return nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);
+ *   - drivers/nvme/target/loop.c|687| <<nvme_loop_init_module>> ret = nvmet_register_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/rdma.c|1667| <<nvmet_rdma_init>> ret = nvmet_register_transport(&nvmet_rdma_ops);
+ *   - drivers/nvme/target/tcp.c|1719| <<nvmet_tcp_init>> ret = nvmet_register_transport(&nvmet_tcp_ops);
+ */
 int nvmet_register_transport(const struct nvmet_fabrics_ops *ops)
 {
 	int ret = 0;
-- 
2.7.4

