From 1708625679e97b1d91b4d40c5272337181b04f81 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 24 Sep 2019 13:46:25 +0800
Subject: [PATCH 1/1] block comment for block and drivers for linux-5.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-flush.c                   |   5 +
 block/blk-mq-cpumap.c               |  28 ++
 block/blk-mq-pci.c                  |  19 +
 block/blk-mq-rdma.c                 |  18 +
 block/blk-mq-sched.c                |   4 +
 block/blk-mq-tag.c                  |  77 ++++
 block/blk-mq-tag.h                  |  23 ++
 block/blk-mq-virtio.c               |  18 +
 block/blk-mq.c                      |  47 +++
 block/blk-mq.h                      |   4 +
 block/blk-timeout.c                 |  42 +++
 drivers/nvme/host/core.c            |   8 +
 drivers/nvme/host/fabrics.c         |  11 +
 drivers/nvme/host/fabrics.h         |   9 +
 drivers/nvme/host/nvme.h            |  17 +
 drivers/nvme/host/rdma.c            | 687 ++++++++++++++++++++++++++++++++++++
 drivers/nvme/target/core.c          |   7 +
 drivers/nvme/target/loop.c          |   4 +
 drivers/scsi/fcoe/fcoe.c            |  10 +
 drivers/scsi/fcoe/fcoe_transport.c  |   5 +
 drivers/scsi/scsi_transport_fc.c    |  22 ++
 drivers/scsi/scsi_transport_iscsi.c |  10 +
 include/linux/blk-mq.h              |  13 +
 include/linux/blkdev.h              |  14 +
 include/rdma/rdma_cm.h              |  12 +
 lib/sbitmap.c                       |  14 +
 26 files changed, 1128 insertions(+)

diff --git a/block/blk-flush.c b/block/blk-flush.c
index aedd932..33c3f92 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -346,6 +346,11 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2001| <<blk_mq_make_request>> blk_insert_flush(rq);
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index f945621..ccbd7f6 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -15,12 +15,21 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|49| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ *   - block/blk-mq-cpumap.c|53| <<blk_mq_map_queues>> map[cpu] = cpu_to_queue_index(qmap, nr_queues, cpu);
+ */
 static int cpu_to_queue_index(struct blk_mq_queue_map *qmap,
 			      unsigned int nr_queues, const int cpu)
 {
 	return qmap->queue_offset + (cpu % nr_queues);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-cpumap.c|51| <<blk_mq_map_queues>> first_sibling = get_first_sibling(cpu);
+ */
 static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
@@ -32,6 +41,19 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3049| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3321| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|453| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|2394| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2151| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2152| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7124| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1766| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -68,6 +90,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2116| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2172| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2815| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index b595a94..287a653 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -23,6 +23,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|451| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7126| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5806| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[HCTX_TYPE_DEFAULT],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
@@ -34,6 +40,19 @@ int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-rdma.c b/block/blk-mq-rdma.c
index 14f968e..b96945e 100644
--- a/block/blk-mq-rdma.c
+++ b/block/blk-mq-rdma.c
@@ -21,6 +21,11 @@
  * @set->nr_hw_queues, or @dev does not provide an affinity mask for a
  * vector, we fallback to the naive mapping.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2382| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/nvme/host/rdma.c|2384| <<nvme_rdma_map_queues>> blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+ */
 int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		struct ib_device *dev, int first_vec)
 {
@@ -32,6 +37,19 @@ int blk_mq_rdma_map_queues(struct blk_mq_queue_map *map,
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 */
 		for_each_cpu(cpu, mask)
 			map->mq_map[cpu] = map->queue_offset + queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index c9d183d..6b6bff3 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -167,6 +167,10 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1394| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index da19f0b..8e01c6d 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -14,6 +14,12 @@
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
 
+/*
+ * called by:
+ *   - block/blk-mq.c|280| <<blk_mq_can_queue>> return blk_mq_has_free_tags(hctx->tags);
+ *
+ * 但是没人调用blk_mq_can_queue()
+ */
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
 	if (!tags)
@@ -28,8 +34,19 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|61| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 修改和使用active_queues的地方 (比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发):
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -40,6 +57,15 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|91| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|275| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ *
+ * 猜测唤醒的是
+ * blk_mq_get_tag()
+ *  -> sbitmap_prepare_to_wait()
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -51,6 +77,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|78| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -58,8 +88,20 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		return;
 
+	/*
+	 * 修改和使用active_queues的地方 (比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发):
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 */
 	atomic_dec(&tags->active_queues);
 
+	/*
+	 * 猜测唤醒的是
+	 * blk_mq_get_tag()
+	 *  -> sbitmap_prepare_to_wait()
+	 */
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
@@ -67,6 +109,10 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|111| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -94,6 +140,12 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|176| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|198| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|204| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
@@ -106,6 +158,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|395| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1064| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
@@ -456,6 +513,15 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3186| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3189| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ *
+ * queue_requests_store()
+ *  -> blk_mq_update_nr_requests()
+ *      -> blk_mq_tag_update_depth()
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -521,6 +587,17 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 部分调用的例子:
+ *   - drivers/block/nbd.c|170| <<nbd_cmd_handle>> u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/nvme/host/nvme.h|134| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|693| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|859| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|3698| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5620| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5291| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|487| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..24827d1 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -11,6 +11,15 @@ struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 修改和使用active_queues的地方:
+	 *   - block/blk-mq-tag.c|45| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|71| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|96| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 *   - block/blk-mq-debugfs.c|449| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *
+	 * 比如用在hctx_may_queue()用来判断scsi某个lun或者nvme某个namespace是否可以下发
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -36,6 +45,10 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1123| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -53,6 +66,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|385| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1056| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -61,6 +79,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|955| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2291| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 4883416..fc95d95 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,6 +21,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|697| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|658| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
@@ -35,6 +40,19 @@ int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		if (!mask)
 			goto fallback;
 
+		/*
+		 * 设置queue_offset的地方:
+		 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+		 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+		 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+		 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+		 */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 0835f4d..49f3e2f 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -265,6 +265,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|320| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -275,6 +279,9 @@ void blk_mq_wake_waiters(struct request_queue *q)
 			blk_mq_tag_wakeup_all(hctx->tags, true);
 }
 
+/*
+ * 没人调用!
+ */
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 {
 	return blk_mq_has_free_tags(hctx->tags);
@@ -1652,6 +1659,16 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|391| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|751| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false);
+ *   - block/blk-mq.c|1850| <<__blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, run_queue);
+ *   - block/blk-mq.c|1866| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, true);
+ *   - block/blk-mq.c|1900| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq,
+ *
+ * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1851,6 +1868,11 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2045| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2050| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie);
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1884,6 +1906,10 @@ blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|437| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1897,6 +1923,9 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		if (ret != BLK_STS_OK) {
 			if (ret == BLK_STS_RESOURCE ||
 					ret == BLK_STS_DEV_RESOURCE) {
+				/*
+				 * 把request插入hctx->dispatch, 如果参数的run_queue是true, 调用blk_mq_run_hw_queue(hctx, false)
+				 */
 				blk_mq_request_bypass_insert(rq,
 							list_empty(list));
 				break;
@@ -1914,6 +1943,13 @@ void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		hctx->queue->mq_ops->commit_rqs(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1998| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *   - block/blk-mq.c|2013| <<blk_mq_make_request>> blk_add_rq_to_plug(plug, rq);
+ *
+ * 核心思想是把request插入到plug->mq_list
+ */
 static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 {
 	list_add_tail(&rq->queuelist, &plug->mq_list);
@@ -1974,6 +2010,11 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 	plug = blk_mq_plug(q, bio);
 	if (unlikely(is_flush_fua)) {
 		/* bypass scheduler for flush rq */
+		/*
+		 * called by:
+		 *   - block/blk-mq-sched.c|388| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+		 *   - block/blk-mq.c|2001| <<blk_mq_make_request>> blk_insert_flush(rq);
+		 */
 		blk_insert_flush(rq);
 		blk_mq_run_hw_queue(data.hctx, true);
 	} else if (plug && (q->nr_hw_queues == 1 || q->mq_ops->commit_rqs)) {
@@ -1995,6 +2036,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			trace_block_plug(q);
 		}
 
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 	} else if (plug && !blk_queue_nomerges(q)) {
 		/*
@@ -2010,6 +2052,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 			list_del_init(&same_queue_rq->queuelist);
 			plug->rq_count--;
 		}
+		/* 核心思想是把request插入到plug->mq_list */
 		blk_add_rq_to_plug(plug, rq);
 		trace_block_plug(q);
 
@@ -3123,6 +3166,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 32c62c6..26cccd3 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -251,6 +251,10 @@ static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
  *
  * Return current->plug if the bio can be plugged and NULL otherwise
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2003| <<blk_mq_make_request>> plug = blk_mq_plug(q, bio);
+ */
 static inline struct blk_plug *blk_mq_plug(struct request_queue *q,
 					   struct bio *bio)
 {
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 8aa68fa..77b5f9e 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -12,6 +12,12 @@
 
 #ifdef CONFIG_FAIL_IO_TIMEOUT
 
+/*
+ * called by:
+ *   - block/blk-timeout.c|19| <<setup_fail_io_timeout>> return setup_fault_attr(&fail_io_timeout, str);
+ *   - block/blk-timeout.c|28| <<blk_should_fake_timeout>> return should_fail(&fail_io_timeout, 1);
+ *   - block/blk-timeout.c|34| <<fail_io_timeout_debugfs>> NULL, &fail_io_timeout);
+ */
 static DECLARE_FAULT_ATTR(fail_io_timeout);
 
 static int __init setup_fail_io_timeout(char *str)
@@ -22,6 +28,13 @@ __setup("fail_io_timeout=", setup_fail_io_timeout);
 
 int blk_should_fake_timeout(struct request_queue *q)
 {
+	/*
+	 * 在以下使用QUEUE_FLAG_FAIL_IO:
+	 *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+	 *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+	 *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+	 *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+	 */
 	if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
 		return 0;
 
@@ -38,6 +51,10 @@ static int __init fail_io_timeout_debugfs(void)
 
 late_initcall(fail_io_timeout_debugfs);
 
+/*
+ * 在block/genhd.c用做:
+ * __ATTR(io-timeout-fail, 0644, part_timeout_show, part_timeout_store);
+ */
 ssize_t part_timeout_show(struct device *dev, struct device_attribute *attr,
 			  char *buf)
 {
@@ -78,6 +95,15 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
  * LLDDs who implement their own error recovery MAY ignore the timeout
  * event if they generated blk_abort_request.
  */
+/*
+ * called by:
+ *   - drivers/ata/libata-eh.c|916| <<ata_qc_schedule_eh>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2617| <<mtip_queue_cmd>> blk_abort_request(req);
+ *   - drivers/s390/block/dasd_ioctl.c|168| <<dasd_ioctl_abortio>> blk_abort_request(cqr->callback_data);
+ *   - drivers/scsi/libsas/sas_ata.c|588| <<sas_ata_task_abort>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/scsi/libsas/sas_scsi_host.c|911| <<sas_task_abort>> blk_abort_request(sc->request);
+ *   - drivers/scsi/scsi_debug.c|4390| <<schedule_resp>> blk_abort_request(cmnd->request);
+ */
 void blk_abort_request(struct request *req)
 {
 	/*
@@ -90,6 +116,10 @@ void blk_abort_request(struct request *req)
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
 
+/*
+ * called by:
+ *   - block/blk-timeout.c|134| <<blk_add_timer>> expiry = blk_rq_timeout(round_jiffies_up(expiry));
+ */
 unsigned long blk_rq_timeout(unsigned long timeout)
 {
 	unsigned long maxt;
@@ -109,6 +139,11 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|685| <<blk_mq_start_request>> blk_add_timer(rq);
+ *   - block/blk-mq.c|853| <<blk_mq_rq_timed_out>> blk_add_timer(req);
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -121,6 +156,13 @@ void blk_add_timer(struct request *req)
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
+	/*
+	 * 在以下使用RQF_TIMED_OUT (timeout has been called, don't expire again):
+	 *   - block/blk-mq.c|710| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+	 *   - block/blk-mq.c|843| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+	 *   - block/blk-mq.c|862| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+	 *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+	 */
 	req->rq_flags &= ~RQF_TIMED_OUT;
 
 	expiry = jiffies + req->timeout;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index d3d6b7b..9b91b56 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -702,6 +702,14 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2319| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|892| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|2229| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|2063| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|145| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 1994d5b..432bcc1 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -490,6 +490,13 @@ EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
  * being implemented to the common NVMe fabrics library. Part of
  * the overall init sequence of starting up a fabrics driver.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3449| <<nvme_fc_init_module>> ret = nvmf_register_transport(&nvme_fc_transport);
+ *   - drivers/nvme/host/rdma.c|2101| <<nvme_rdma_init_module>> ret = nvmf_register_transport(&nvme_rdma_transport);
+ *   - drivers/nvme/host/tcp.c|2328| <<nvme_tcp_init_module>> nvmf_register_transport(&nvme_tcp_transport);
+ *   - drivers/nvme/target/loop.c|691| <<nvme_loop_init_module>> ret = nvmf_register_transport(&nvme_loop_transport);
+ */
 int nvmf_register_transport(struct nvmf_transport_ops *ops)
 {
 	if (!ops->create_ctrl)
@@ -977,6 +984,10 @@ EXPORT_SYMBOL_GPL(nvmf_free_options);
 				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
 				 NVMF_OPT_DISABLE_SQFLOW)
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1083| <<nvmf_dev_write>> ctrl = nvmf_create_ctrl(nvmf_device, buf);
+ */
 static struct nvme_ctrl *
 nvmf_create_ctrl(struct device *dev, const char *buf)
 {
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index 3044d8b..055e773 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -99,6 +99,15 @@ struct nvmf_ctrl_options {
 	unsigned int		nr_io_queues;
 	unsigned int		reconnect_delay;
 	bool			discovery_nqn;
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {
+	 */
 	bool			duplicate_connect;
 	unsigned int		kato;
 	struct nvmf_host	*host;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 2d678fb..58c4078 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -179,6 +179,23 @@ struct nvme_ctrl {
 	struct device ctrl_device;
 	struct device *device;	/* char device */
 	struct cdev cdev;
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	struct work_struct reset_work;
 	struct work_struct delete_work;
 
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 1a6449b..08f9e7d 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -27,6 +27,9 @@
 #include "nvme.h"
 #include "fabrics.h"
 
+/*
+ * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+ */
 
 #define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
 
@@ -39,6 +42,12 @@ struct nvme_rdma_device {
 	struct ib_pd		*pd;
 	struct kref		ref;
 	struct list_head	entry;
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	unsigned int		num_inline_segments;
 };
 
@@ -73,8 +82,27 @@ enum nvme_rdma_queue_flags {
 };
 
 struct nvme_rdma_queue {
+	/*
+	 * 分配, 释放和使用rsp_ring的地方:
+	 *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|595| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|597| <<nvme_rdma_create_queue_ib>> if (!queue->rsp_ring) {
+	 *   - drivers/nvme/host/rdma.c|618| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|1756| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+	 *
+	 * 是一个数组, 有queue->queue_size个struct nvme_rdma_qe
+	 */
 	struct nvme_rdma_qe	*rsp_ring;
+	/*
+	 * 设置queue_size的地方:
+	 *   - drivers/nvme/host/rdma.c|659| <<nvme_rdma_alloc_queue>> queue->queue_size = queue_size;
+	 */
 	int			queue_size;
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	size_t			cmnd_capsule_len;
 	struct nvme_rdma_ctrl	*ctrl;
 	struct nvme_rdma_device	*device;
@@ -84,19 +112,56 @@ struct nvme_rdma_queue {
 	unsigned long		flags;
 	struct rdma_cm_id	*cm_id;
 	int			cm_error;
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	struct completion	cm_done;
 };
 
 struct nvme_rdma_ctrl {
 	/* read only in the hot path */
+	/*
+	 * nvme_rdma_create_ctrl()分配的queues
+	 * 2402         ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+	 * 2403                                 GFP_KERNEL);
+	 */
 	struct nvme_rdma_queue	*queues;
 
 	/* other member variables */
 	struct blk_mq_tag_set	tag_set;
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	struct work_struct	err_work;
 
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	struct nvme_rdma_qe	async_event_sqe;
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	struct delayed_work	reconnect_work;
 
 	struct list_head	list;
@@ -110,18 +175,49 @@ struct nvme_rdma_ctrl {
 	struct sockaddr_storage src_addr;
 
 	struct nvme_ctrl	ctrl;
+	/*
+	 * 在以下修改和使用use_inline_data:
+	 *   - drivers/nvme/host/rdma.c|1205| <<nvme_rdma_setup_ctrl>> ctrl->use_inline_data = true;
+	 *   - drivers/nvme/host/rdma.c|1516| <<nvme_rdma_map_data>> queue->ctrl->use_inline_data &&
+	 */
 	bool			use_inline_data;
+	/*
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	u32			io_queues[HCTX_MAX_TYPES];
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|729| <<nvme_rdma_alloc_tagset>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|951| <<nvme_rdma_free_ctrl>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|1424| <<nvme_rdma_submit_async_event>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+ *   - drivers/nvme/host/rdma.c|1922| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ */
 static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|373| <<nvme_rdma_find_get_device>> list_for_each_entry(ndev, &device_list, entry) {
+ *   - drivers/nvme/host/rdma.c|400| <<nvme_rdma_find_get_device>> list_add(&ndev->entry, &device_list);
+ *   - drivers/nvme/host/rdma.c|2119| <<nvme_rdma_remove_one>> list_for_each_entry(ndev, &device_list, entry) {
+ */
 static LIST_HEAD(device_list);
 static DEFINE_MUTEX(device_list_mutex);
 
+/*
+ * 在以下使用nvme_rdma_ctrl_list:
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+ *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ */
 static LIST_HEAD(nvme_rdma_ctrl_list);
 static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
 
@@ -130,6 +226,10 @@ static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
  * unsafe.  With it turned off we will have to register a global rkey that
  * allows read and write access to all physical memory.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|387| <<nvme_rdma_find_get_device>> register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+ */
 static bool register_always = true;
 module_param(register_always, bool, 0444);
 MODULE_PARM_DESC(register_always,
@@ -139,10 +239,24 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *event);
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops;
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
 
 /* XXX: really should move to a generic header sooner or later.. */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1223| <<nvme_rdma_set_sg_null>> put_unaligned_le24(0, sg->length);
+ *   - drivers/nvme/host/rdma.c|1260| <<nvme_rdma_map_sg_single>> put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
+ *   - drivers/nvme/host/rdma.c|1304| <<nvme_rdma_map_sg_fr>> put_unaligned_le24(req->mr->length, sg->length);
+ */
 static inline void put_unaligned_le24(u32 val, u8 *p)
 {
 	*p++ = val;
@@ -152,28 +266,76 @@ static inline void put_unaligned_le24(u32 val, u8 *p)
 
 static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctrl
+	 */
 	return queue - queue->ctrl->queues;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|471| <<nvme_rdma_create_queue_ib>> if (nvme_rdma_poll_queue(queue))
+ *   - drivers/nvme/host/rdma.c|620| <<nvme_rdma_start_queue>> bool poll = nvme_rdma_poll_queue(queue);
+ */
 static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * queue->ctrl是struct nvme_rdma_ctr
+	 *
+	 * io_queues被设置的地方:
+	 *   - drivers/nvme/host/rdma.c|841| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	 *   - drivers/nvme/host/rdma.c|843| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|852| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+	 *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_alloc_io_queues>> ctrl->io_queues[HCTX_TYPE_POLL] =
+	 */
 	return nvme_rdma_queue_idx(queue) >
 		queue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +
 		queue->ctrl->io_queues[HCTX_TYPE_READ];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1540| <<nvme_rdma_map_data>> nvme_rdma_inline_data_size(queue)) {
+ */
 static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
 {
+	/*
+	 * 设置cmnd_capsule_len的地方:
+	 *   - drivers/nvme/host/rdma.c|655| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	 *   - drivers/nvme/host/rdma.c|657| <<nvme_rdma_alloc_queue>> queue->cmnd_capsule_len = sizeof(struct nvme_command);
+	 */
 	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|201| <<nvme_rdma_free_ring>> nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *   - drivers/nvme/host/rdma.c|859| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+ */
 static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
+	/*
+	 * struct nvme_rdma_qe {
+	 *         struct ib_cqe           cqe;
+	 *         void                    *data;
+	 *         u64                     dma;
+	 * };
+	 */
 	ib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);
 	kfree(qe->data);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|257| <<nvme_rdma_alloc_ring>> if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+ *   - drivers/nvme/host/rdma.c|842| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *
+ * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+ */
 static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -191,17 +353,31 @@ static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|264| <<nvme_rdma_alloc_ring>> nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|474| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ *   - drivers/nvme/host/rdma.c|551| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+ */
 static void nvme_rdma_free_ring(struct ib_device *ibdev,
 		struct nvme_rdma_qe *ring, size_t ib_queue_size,
 		size_t capsule_size, enum dma_data_direction dir)
 {
 	int i;
 
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 把nvme_rdma_qe->dma指向的nvme_rdma_qe->data给dmp unmap了并free
+	 */
 	for (i = 0; i < ib_queue_size; i++)
 		nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
 	kfree(ring);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|528| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+ */
 static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 		size_t ib_queue_size, size_t capsule_size,
 		enum dma_data_direction dir)
@@ -218,6 +394,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	 * lifetime. It's safe, since any chage in the underlying RDMA device
 	 * will issue error recovery and queue re-creation.
 	 */
+	/*
+	 * nvme_rdma_alloc_qe():
+	 * 核心思想就是分配惨是的capsule_size大小的内存到nvme_rdma_qe->data, 然后用dma map了到qe->dma
+	 */
 	for (i = 0; i < ib_queue_size; i++) {
 		if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
 			goto out_free_ring;
@@ -230,6 +410,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|300| <<nvme_rdma_create_qp>> init_attr.event_handler = nvme_rdma_qp_event;
+ */
 static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 {
 	pr_debug("QP event %s (%d)\n",
@@ -237,10 +421,24 @@ static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|634| <<nvme_rdma_alloc_queue>> ret = nvme_rdma_wait_for_cm(queue);
+ *
+ * 核心思想是等待nvme_rdma_queue->cm_done被complete
+ * 只在nvme_rdma_cm_handler被唤醒
+ */
 static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 {
 	int ret;
 
+	/*
+	 * cm_done在以下使用:
+	 *   - drivers/nvme/host/rdma.c|411| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|734| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2005| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|2040| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
 			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
 	if (ret < 0)
@@ -251,6 +449,10 @@ static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 	return queue->cm_error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|485| <<nvme_rdma_create_queue_ib>> ret = nvme_rdma_create_qp(queue, send_wr_factor);
+ */
 static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 {
 	struct nvme_rdma_device *dev = queue->device;
@@ -270,20 +472,42 @@ static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 	init_attr.send_cq = queue->ib_cq;
 	init_attr.recv_cq = queue->ib_cq;
 
+	/*
+	 * dev->pd是struct ib_pd
+	 * queue->cm_id是struct rdma_cm_id
+	 */
 	ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
 
+	/*
+	 * queue->qp是struct ib_qp
+	 */
 	queue->qp = queue->cm_id->qp;
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.exit_request = nvme_rdma_exit_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.exit_request = nvme_rdma_exit_request()
+ */
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	kfree(req->sqe.data);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_request = nvme_rdma_init_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_request = nvme_rdma_init_request()
+ */
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -294,6 +518,13 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
 
 	nvme_req(rq)->ctrl = &ctrl->ctrl;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.data = kzalloc(sizeof(struct nvme_command), GFP_KERNEL);
 	if (!req->sqe.data)
 		return -ENOMEM;
@@ -303,6 +534,9 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_hctx = nvme_rdma_init_hctx()
+ */
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -315,6 +549,9 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_hctx = nvme_rdma_init_admin_hctx()
+ */
 static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -350,6 +587,17 @@ static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
 	return kref_get_unless_zero(&dev->ref);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|530| <<nvme_rdma_create_queue_ib>> queue->device = nvme_rdma_find_get_device(queue->cm_id);
+ *
+ * nvme_rdma_cm_handler()
+ *  -> nvme_rdma_addr_resolved()
+ *      -> nvme_rdma_create_queue_ib()
+ *          -> nvme_rdma_find_get_device()
+ *
+ * 惨是的rdma_cm_id->qp是struct ib_qp
+ */
 static struct nvme_rdma_device *
 nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 {
@@ -366,9 +614,22 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	if (!ndev)
 		goto out_err;
 
+	/* 类型是struct ib_device */
 	ndev->dev = cm_id->device;
 	kref_init(&ndev->ref);
 
+	/*
+	 * ndev->pd是struct ib_pd
+	 *
+	 * ib_alloc_pd - Allocates an unused protection domain.
+	 * @device: The device on which to allocate the protection domain.
+	 *
+	 * A protection domain object provides an association between QPs, shared
+	 * receive queues, address handles, memory regions, and memory windows.
+	 *
+	 * Every PD has a local_dma_lkey which can be used as the lkey value for local
+	 * memory operations.
+	 */
 	ndev->pd = ib_alloc_pd(ndev->dev,
 		register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
 	if (IS_ERR(ndev->pd))
@@ -397,6 +658,16 @@ nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_alloc_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|799| <<nvme_rdma_free_queue>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1846| <<nvme_rdma_conn_established>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1899| <<nvme_rdma_addr_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|1953| <<nvme_rdma_route_resolved>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2008| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ *   - drivers/nvme/host/rdma.c|2014| <<nvme_rdma_cm_handler>> nvme_rdma_destroy_queue_ib(queue);
+ */
 static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_device *dev;
@@ -424,12 +695,30 @@ static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
 	nvme_rdma_dev_put(dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|687| <<nvme_rdma_create_queue_ib>> nvme_rdma_get_max_fr_pages(ibdev), 0);
+ *   - drivers/nvme/host/rdma.c|1016| <<nvme_rdma_configure_admin_queue>> ctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev);
+ */
 static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
 {
 	return min_t(u32, NVME_RDMA_MAX_SEGMENTS,
 		     ibdev->attrs.max_fast_reg_page_list_len);
 }
 
+/*
+ * [0] nvme_rdma_create_queue_ib
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1699| <<nvme_rdma_addr_resolved>> ret = nvme_rdma_create_queue_ib(queue);
+ */
 static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct ib_device *ibdev;
@@ -506,6 +795,20 @@ static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_alloc_queue [nvme_rdma]
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|750| <<nvme_rdma_alloc_io_queues>> ret = nvme_rdma_alloc_queue(ctrl, i,
+ *   - drivers/nvme/host/rdma.c|828| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ */
 static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		int idx, size_t queue_size)
 {
@@ -600,6 +903,11 @@ static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
 		nvme_rdma_stop_queue(&ctrl->queues[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|715| <<nvme_rdma_start_io_queues>> ret = nvme_rdma_start_queue(ctrl, i);
+ *   - drivers/nvme/host/rdma.c|907| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_start_queue(ctrl, 0);
+ */
 static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 {
 	struct nvme_rdma_queue *queue = &ctrl->queues[idx];
@@ -621,6 +929,10 @@ static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1203| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_start_io_queues(ctrl);
+ */
 static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	int i, ret = 0;
@@ -639,6 +951,10 @@ static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|920| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_alloc_io_queues(ctrl);
+ */
 static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -709,6 +1025,21 @@ static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|848| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+ *   - drivers/nvme/host/rdma.c|925| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+ */
 static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
 		bool admin)
 {
@@ -767,6 +1098,10 @@ static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_queue(&ctrl->queues[0]);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1035| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_admin_queue(ctrl, new);
+ */
 static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 		bool new)
 {
@@ -786,6 +1121,16 @@ static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
 	 * It's safe, since any chage in the underlying RDMA device will issue
 	 * error recovery and queue re-creation.
 	 */
+	/*
+	 * 在以下使用async_event_sqe:
+	 *   - drivers/nvme/host/rdma.c|934| <<nvme_rdma_destroy_admin_queue>> if (ctrl->async_event_sqe.data) {
+	 *   - drivers/nvme/host/rdma.c|935| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_destroy_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|965| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1021| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+	 *   - drivers/nvme/host/rdma.c|1023| <<nvme_rdma_configure_admin_queue>> ctrl->async_event_sqe.data = NULL;
+	 *   - drivers/nvme/host/rdma.c|1658| <<nvme_rdma_submit_async_event>> struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	 */
 	error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
 			sizeof(struct nvme_command), DMA_TO_DEVICE);
 	if (error)
@@ -860,6 +1205,10 @@ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_io_queues(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1180| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_io_queues(ctrl, new);
+ */
 static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret;
@@ -929,6 +1278,9 @@ static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
 	}
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.free_ctrl = nvme_rdma_free_ctrl()
+ */
 static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
@@ -946,6 +1298,12 @@ static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 	kfree(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1178| <<nvme_rdma_reconnect_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|1197| <<nvme_rdma_error_recovery_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ *   - drivers/nvme/host/rdma.c|2114| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_reconnect_or_remove(ctrl);
+ */
 static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 {
 	/* If we are resetting/deleting then do nothing */
@@ -965,6 +1323,30 @@ static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 	}
 }
 
+/*
+ * [0] nvme_rdma_setup_ctrl [nvme_rdma]
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1054| <<nvme_rdma_reconnect_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|1939| <<nvme_rdma_reset_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|2071| <<nvme_rdma_create_ctrl>> ret = nvme_rdma_setup_ctrl(ctrl, true);
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_admin_queue()
+ *          -> nvme_rdma_alloc_tagset()
+ *
+ * nvme_rdma_create_ctrl()或者nvme_rdma_reset_ctrl_work()或者nvme_rdma_reconnect_ctrl_work()
+ *  -> nvme_rdma_setup_ctrl()
+ *      -> nvme_rdma_configure_io_queues()
+ *          -> nvme_rdma_alloc_tagset()
+ */
 static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret = -EINVAL;
@@ -1027,6 +1409,13 @@ static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 	return ret;
 }
 
+/*
+ * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+ *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+ *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+ *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+ */
 static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
@@ -1050,6 +1439,14 @@ static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+ *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+ *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+ *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+ */
 static void nvme_rdma_error_recovery_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl = container_of(work,
@@ -1069,6 +1466,16 @@ static void nvme_rdma_error_recovery_work(struct work_struct *work)
 	nvme_rdma_reconnect_or_remove(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1251| <<nvme_rdma_wr_error>> nvme_rdma_error_recovery(ctrl);
+ *   - drivers/nvme/host/rdma.c|1627| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1640| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1650| <<nvme_rdma_process_nvme_rsp>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1900| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1908| <<nvme_rdma_cm_handler>> nvme_rdma_error_recovery(queue->ctrl);
+ *   - drivers/nvme/host/rdma.c|1947| <<nvme_rdma_timeout>> nvme_rdma_error_recovery(ctrl);
+ */
 static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
@@ -1201,6 +1608,10 @@ static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1688| <<nvme_rdma_map_data>> ret = nvme_rdma_map_sg_fr(queue, req, c, count);
+ */
 static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_request *req, struct nvme_command *c,
 		int count)
@@ -1208,6 +1619,9 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
 	int nr;
 
+	/*
+	 * req->mr是struct ib_mr
+	 */
 	req->mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);
 	if (WARN_ON_ONCE(!req->mr))
 		return -EAGAIN;
@@ -1247,6 +1661,10 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1828| <<nvme_rdma_queue_rq>> err = nvme_rdma_map_data(queue, rq, c);
+ */
 static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		struct request *rq, struct nvme_command *c)
 {
@@ -1279,6 +1697,12 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		goto out_free_table;
 	}
 
+	/*
+	 * num_inline_segments在以下使用:
+	 *   - drivers/nvme/host/rdma.c|463| <<nvme_rdma_create_qp>> init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	 *   - drivers/nvme/host/rdma.c|625| <<nvme_rdma_find_get_device>> ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+	 *   - drivers/nvme/host/rdma.c|1673| <<nvme_rdma_map_data>> if (count <= dev->num_inline_segments) {
+	 */
 	if (count <= dev->num_inline_segments) {
 		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
 		    queue->ctrl->use_inline_data &&
@@ -1310,6 +1734,17 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_send_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1327,6 +1762,11 @@ static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1835| <<nvme_rdma_submit_async_event>> ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
+ *   - drivers/nvme/host/rdma.c|2248| <<nvme_rdma_queue_rq>> err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
+ */
 static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
 		struct ib_send_wr *first)
@@ -1358,6 +1798,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1641| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1653| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1376,6 +1821,10 @@ static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 	wr.sg_list  = &list;
 	wr.num_sge  = 1;
 
+	/*
+	 * Posts a list of work requests to the receive queue of
+	 *   the specified QP.
+	 */
 	ret = ib_post_recv(queue->qp, &wr, NULL);
 	if (unlikely(ret)) {
 		dev_err(queue->ctrl->ctrl.device,
@@ -1399,6 +1848,9 @@ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_rdma_wr_error(cq, wc, "ASYNC");
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.submit_async_event = nvme_rdma_submit_async_event()
+ */
 static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
@@ -1426,6 +1878,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1564| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1470,6 +1926,20 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * [0] nvme_rdma_recv_done [nvme_rdma]
+ * [0] __ib_process_cq [ib_core]
+ * [0] ib_poll_handler [ib_core]
+ * [0] irq_poll_softirq
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1779| <<nvme_rdma_post_recv>> qe->cqe.done = nvme_rdma_recv_done;
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1502,6 +1972,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1712| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -1545,6 +2019,10 @@ static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
 	return -ECONNRESET;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1706| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_addr_resolved(queue);
+ */
 static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -1568,6 +2046,10 @@ static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1709| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_route_resolved(queue);
+ */
 static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
@@ -1618,6 +2100,34 @@ static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] addr_handler [rdma_cm]
+ * [0] process_one_req [ib_core]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] nvme_rdma_cm_handler [nvme_rdma]
+ * [0] cma_ib_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/nvme/host/rdma.c|580| <<nvme_rdma_alloc_queue>> queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+ */
 static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *ev)
 {
@@ -1629,6 +2139,18 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		ev->status, cm_id);
 
 	switch (ev->event) {
+		/*
+		 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+		 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+		 */
 	case RDMA_CM_EVENT_ADDR_RESOLVED:
 		cm_error = nvme_rdma_addr_resolved(queue);
 		break;
@@ -1679,6 +2201,10 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.timeout = nvme_rdma_timeout()
+ * struct blk_mq_ops nvme_rdma_mq_ops.timeout = nvme_rdma_timeout()
+ */
 static enum blk_eh_timer_return
 nvme_rdma_timeout(struct request *rq, bool reserved)
 {
@@ -1707,6 +2233,10 @@ nvme_rdma_timeout(struct request *rq, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ */
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1715,6 +2245,15 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct request *rq = bd->rq;
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
 	struct nvme_rdma_qe *sqe = &req->sqe;
+	/*
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 *
+	 * 注意, c指向的req->sqe.data!!!!!!!!
+	 */
 	struct nvme_command *c = sqe->data;
 	struct ib_device *dev;
 	bool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);
@@ -1728,6 +2267,17 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	dev = queue->device->dev;
 
+	/*
+	 * req是struct nvme_rdma_request类型
+	 * 来自blk_mq_rq_to_pdu(rq)
+	 *
+	 * 这里req->sqe.dma对应的req->sqe.data (存储的nvme_command c)
+	 * struct nvme_rdma_request *req:
+	 *  -> struct nvme_rdma_qe sqe:
+	 *      -> struct ib_cqe  cqe;
+	 *      -> void           *data;
+	 *      -> u64            dma;
+	 */
 	req->sqe.dma = ib_dma_map_single(dev, req->sqe.data,
 					 sizeof(struct nvme_command),
 					 DMA_TO_DEVICE);
@@ -1777,6 +2327,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.poll = nvme_rdma_poll()
+ */
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1784,6 +2337,10 @@ static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 	return ib_process_cq_direct(queue->ib_cq, -1);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.complete = nvme_rdma_complete_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.complete = nvme_rdma_complete_rq()
+ */
 static void nvme_rdma_complete_rq(struct request *rq)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
@@ -1796,6 +2353,9 @@ static void nvme_rdma_complete_rq(struct request *rq)
 	nvme_complete_rq(rq);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.map_queues = nvme_rdma_map_queues()
+ */
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
@@ -1843,6 +2403,10 @@ static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1024| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1854,6 +2418,10 @@ static const struct blk_mq_ops nvme_rdma_mq_ops = {
 	.poll		= nvme_rdma_poll,
 };
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1011| <<nvme_rdma_alloc_tagset>> set->ops = &nvme_rdma_admin_mq_ops;
+ */
 static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.queue_rq	= nvme_rdma_queue_rq,
 	.complete	= nvme_rdma_complete_rq,
@@ -1863,6 +2431,11 @@ static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
 	.timeout	= nvme_rdma_timeout,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1975| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ *   - drivers/nvme/host/rdma.c|1988| <<nvme_rdma_reset_ctrl_work>> nvme_rdma_shutdown_ctrl(ctrl, false);
+ */
 static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 {
 	cancel_work_sync(&ctrl->err_work);
@@ -1876,11 +2449,18 @@ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.delete_ctrl = nvme_rdma_delete_ctrl()
+ */
 static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|2050| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ */
 static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl =
@@ -1930,6 +2510,10 @@ static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
  * The ports don't need to be compared as they are intrinsically
  * already matched by the port pointers supplied.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|2021| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+ */
 static bool
 nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 {
@@ -1947,9 +2531,25 @@ nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 	return found;
 }
 
+/*
+ * [0] nvme_rdma_create_ctrl [nvme_rdma]
+ * [0] nvmf_dev_write [nvme_fabrics]
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1039| <<nvmf_create_ctrl>> ctrl = ops->create_ctrl(dev, opts);
+ *
+ * struct nvmf_transport_ops nvme_rdma_transport.create_ctrl = nvme_ctrl_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
+	/*
+	 * 包含struct nvme_ctrl和struct blk_mq_tag_set (不是指针)
+	 */
 	struct nvme_rdma_ctrl *ctrl;
 	int ret;
 	bool changed;
@@ -1970,6 +2570,9 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		opts->mask |= NVMF_OPT_TRSVCID;
 	}
 
+	/*
+	 * convert an IPv4/IPv6 and port to socket address
+	 */
 	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
 			opts->traddr, opts->trsvcid, &ctrl->addr);
 	if (ret) {
@@ -1988,14 +2591,60 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		}
 	}
 
+	/*
+	 * 在以下使用和修改duplicate_connect:
+	 *   - drivers/nvme/host/fabrics.c|639| <<nvmf_parse_options>> opts->duplicate_connect = false;
+	 *   - drivers/nvme/host/fabrics.c|831| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fabrics.c|879| <<nvmf_parse_options>> opts->duplicate_connect = true;
+	 *   - drivers/nvme/host/fc.c|3034| <<nvme_fc_init_ctrl>> if (!opts->duplicate_connect &&
+	 *   - drivers/nvme/host/rdma.c|2276| <<nvme_rdma_create_ctrl>> if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts))
+	 *   - drivers/nvme/host/tcp.c|2260| <<nvme_tcp_create_ctrl>> if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts))
+	 *
+	 * nvme_rdma_existing_controller()"
+	 * Fails a connection request if it matches an existing controller
+	 * (association) with the same tuple:
+	 * <Host NQN, Host ID, local address, remote address, remote port, SUBSYS NQN>
+	 */
 	if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
 		ret = -EALREADY;
 		goto out_free_ctrl;
 	}
 
+	/*
+	 * 在以下使用ctrl->reconnect_work = nvme_rdma_reconnect_ctrl_work()
+	 *   - drivers/nvme/host/rdma.c|1107| <<nvme_rdma_reconnect_or_remove>> queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+	 *   - drivers/nvme/host/rdma.c|1193| <<nvme_rdma_reconnect_ctrl_work>> struct nvme_rdma_ctrl, reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2130| <<nvme_rdma_shutdown_ctrl>> cancel_delayed_work_sync(&ctrl->reconnect_work);
+	 *   - drivers/nvme/host/rdma.c|2281| <<nvme_rdma_create_ctrl>> INIT_DELAYED_WORK(&ctrl->reconnect_work, nvme_rdma_reconnect_ctrl_work);
+	 */
 	INIT_DELAYED_WORK(&ctrl->reconnect_work,
 			nvme_rdma_reconnect_ctrl_work);
+	/*
+	 * 在以下使用ctrl->err_work = nvme_rdma_error_recovery_work()
+	 *   - drivers/nvme/host/rdma.c|1216| <<nvme_rdma_error_recovery_work>> struct nvme_rdma_ctrl, err_work);
+	 *   - drivers/nvme/host/rdma.c|1237| <<nvme_rdma_error_recovery>> queue_work(nvme_wq, &ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|1940| <<nvme_rdma_timeout>> flush_work(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2129| <<nvme_rdma_shutdown_ctrl>> cancel_work_sync(&ctrl->err_work);
+	 *   - drivers/nvme/host/rdma.c|2283| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	 */
 	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	/*
+	 * 在以下调用reset_work:
+	 *   - drivers/nvme/host/core.c|125| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+	 *   - drivers/nvme/host/core.c|137| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/core.c|152| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+	 *   - drivers/nvme/host/fc.c|3162| <<nvme_fc_init_ctrl>> cancel_work_sync(&ctrl->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2698| <<nvme_async_probe>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2820| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+	 *   - drivers/nvme/host/pci.c|2989| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+	 *
+	 * 在以下设置reset_work:
+	 *   - drivers/nvme/host/fc.c|3071| <<nvme_fc_init_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	 *   - drivers/nvme/host/pci.c|2730| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	 *   - drivers/nvme/host/rdma.c|2354| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+	 *   - drivers/nvme/host/tcp.c|2230| <<nvme_tcp_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+	 *   - drivers/nvme/target/loop.c|577| <<nvme_loop_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);
+	 */
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
 
 	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
@@ -2009,6 +2658,11 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	if (!ctrl->queues)
 		goto out_free_ctrl;
 
+	/*
+	 * 下面的命令到这里的时候queue_count是5
+	 * echo "host_traddr=10.0.2.15,traddr=10.0.2.15,transport=rdma,trsvcid=4420,nqn=testhost,hostnqn=nqn.host2,nr_io_queues=4,queue_size=16" > /dev/nvme-fabric
+	 */
+
 	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
 				0 /* no quirks, we're perfect! */);
 	if (ret)
@@ -2024,9 +2678,16 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs\n",
 		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
 
+	/* 不知道在哪里put */
 	nvme_get_ctrl(&ctrl->ctrl);
 
 	mutex_lock(&nvme_rdma_ctrl_mutex);
+	/*
+	 * 在以下使用nvme_rdma_ctrl_list:
+	 *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 *   - drivers/nvme/host/rdma.c|2081| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+	 *   - drivers/nvme/host/rdma.c|2132| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+	 */
 	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
 	mutex_unlock(&nvme_rdma_ctrl_mutex);
 
@@ -2055,6 +2716,20 @@ static struct nvmf_transport_ops nvme_rdma_transport = {
 	.create_ctrl	= nvme_rdma_create_ctrl,
 };
 
+/*
+ * 在nvme_rdma_init_module()被用于ib_register_client - Register an IB client
+ *
+ * Upper level users of the IB drivers can use ib_register_client() to
+ * register callbacks for IB device addition and removal.  When an IB
+ * device is added, each registered client's add method will be called
+ * (in the order the clients were registered), and when a device is
+ * removed, each client's remove method will be called (in the reverse
+ * order that clients were registered).  In addition, when
+ * ib_register_client() is called, the client will receive an add
+ * callback for all devices already registered.
+ *
+ * struct ib_client nvme_rdma_ib_client.remove = nvme_rdma_remove_one()
+ */
 static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
 {
 	struct nvme_rdma_ctrl *ctrl;
@@ -2094,6 +2769,18 @@ static int __init nvme_rdma_init_module(void)
 {
 	int ret;
 
+	/*
+	 * ib_register_client - Register an IB client
+	 *
+	 * Upper level users of the IB drivers can use ib_register_client() to
+	 * register callbacks for IB device addition and removal.  When an IB
+	 * device is added, each registered client's add method will be called
+	 * (in the order the clients were registered), and when a device is
+	 * removed, each client's remove method will be called (in the reverse
+	 * order that clients were registered).  In addition, when
+	 * ib_register_client() is called, the client will receive an add
+	 * callback for all devices already registered.
+	 */
 	ret = ib_register_client(&nvme_rdma_ib_client);
 	if (ret)
 		return ret;
diff --git a/drivers/nvme/target/core.c b/drivers/nvme/target/core.c
index 3a67e24..bc0f841 100644
--- a/drivers/nvme/target/core.c
+++ b/drivers/nvme/target/core.c
@@ -260,6 +260,13 @@ void nvmet_port_send_ana_event(struct nvmet_port *port)
 	up_read(&nvmet_config_sem);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/target/fc.c|2575| <<nvmet_fc_init_module>> return nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);
+ *   - drivers/nvme/target/loop.c|687| <<nvme_loop_init_module>> ret = nvmet_register_transport(&nvme_loop_ops);
+ *   - drivers/nvme/target/rdma.c|1667| <<nvmet_rdma_init>> ret = nvmet_register_transport(&nvmet_rdma_ops);
+ *   - drivers/nvme/target/tcp.c|1719| <<nvmet_tcp_init>> ret = nvmet_register_transport(&nvmet_tcp_ops);
+ */
 int nvmet_register_transport(const struct nvmet_fabrics_ops *ops)
 {
 	int ret = 0;
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index 0940c50..e4b12b6 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -121,6 +121,10 @@ static void nvme_loop_queue_response(struct nvmet_req *req)
 	}
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/target/loop.c|198| <<nvme_loop_init_iod>> INIT_WORK(&iod->work, nvme_loop_execute_work);
+ */
 static void nvme_loop_execute_work(struct work_struct *work)
 {
 	struct nvme_loop_iod *iod =
diff --git a/drivers/scsi/fcoe/fcoe.c b/drivers/scsi/fcoe/fcoe.c
index 00dd47b..d7442c1 100644
--- a/drivers/scsi/fcoe/fcoe.c
+++ b/drivers/scsi/fcoe/fcoe.c
@@ -1120,6 +1120,11 @@ static int fcoe_ddp_done(struct fc_lport *lport, u16 xid)
  *
  * Returns: The allocated fc_lport or an error pointer
  */
+/*
+ * called by:
+ *   - drivers/scsi/fcoe/fcoe.c|2221| <<_fcoe_create>> lport = fcoe_if_create(fcoe, &ctlr_dev->dev, 0);
+ *   - drivers/scsi/fcoe/fcoe.c|2675| <<fcoe_vport_create>> vn_port = fcoe_if_create(fcoe, &vport->dev, 1);
+ */
 static struct fc_lport *fcoe_if_create(struct fcoe_interface *fcoe,
 				       struct device *parent, int npiv)
 {
@@ -2192,6 +2197,11 @@ enum fcoe_create_link_state {
  * consolidation of code can be done when that interface is
  * removed.
  */
+/*
+ * called by:
+ *   - drivers/scsi/fcoe/fcoe.c|2288| <<fcoe_create>> return _fcoe_create(netdev, fip_mode, FCOE_CREATE_LINK_UP);
+ *   - drivers/scsi/fcoe/fcoe.c|2304| <<fcoe_ctlr_alloc>> return _fcoe_create(netdev, FIP_MODE_FABRIC,
+ */
 static int _fcoe_create(struct net_device *netdev, enum fip_mode fip_mode,
 			enum fcoe_create_link_state link_state)
 {
diff --git a/drivers/scsi/fcoe/fcoe_transport.c b/drivers/scsi/fcoe/fcoe_transport.c
index ba4603d..a1bf4eb 100644
--- a/drivers/scsi/fcoe/fcoe_transport.c
+++ b/drivers/scsi/fcoe/fcoe_transport.c
@@ -523,6 +523,11 @@ static struct fcoe_transport *fcoe_transport_lookup(struct net_device *netdev)
  *
  * Returns : 0 for success
  */
+/*
+ * called by:
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|2699| <<bnx2fc_mod_init>> rc = fcoe_transport_attach(&bnx2fc_transport);
+ *   - drivers/scsi/fcoe/fcoe.c|2478| <<fcoe_init>> rc = fcoe_transport_attach(&fcoe_sw_transport);
+ */
 int fcoe_transport_attach(struct fcoe_transport *ft)
 {
 	int rc = 0;
diff --git a/drivers/scsi/scsi_transport_fc.c b/drivers/scsi/scsi_transport_fc.c
index 2732fa6..0227fd0 100644
--- a/drivers/scsi/scsi_transport_fc.c
+++ b/drivers/scsi/scsi_transport_fc.c
@@ -2149,6 +2149,28 @@ fc_user_scan(struct Scsi_Host *shost, uint channel, uint id, u64 lun)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/message/fusion/mptfc.c|1466| <<mptfc_init>> fc_attach_transport(&mptfc_transport_functions);
+ *   - drivers/s390/scsi/zfcp_aux.c|141| <<zfcp_module_init>> fc_attach_transport(&zfcp_transport_functions);
+ *   - drivers/scsi/bfa/bfad_im.c|842| <<bfad_im_module_init>> fc_attach_transport(&bfad_im_fc_function_template);
+ *   - drivers/scsi/bfa/bfad_im.c|847| <<bfad_im_module_init>> fc_attach_transport(&bfad_im_vport_fc_function_template);
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|1283| <<bnx2fc_attach_transport>> fc_attach_transport(&bnx2fc_transport_function);
+ *   - drivers/scsi/bnx2fc/bnx2fc_fcoe.c|1291| <<bnx2fc_attach_transport>> fc_attach_transport(&bnx2fc_vport_xport_function);
+ *   - drivers/scsi/csiostor/csio_init.c|1213| <<csio_init>> csio_fcoe_transport = fc_attach_transport(&csio_fc_transport_funcs);
+ *   - drivers/scsi/csiostor/csio_init.c|1218| <<csio_init>> fc_attach_transport(&csio_fc_transport_vport_funcs);
+ *   - drivers/scsi/fcoe/fcoe.c|1252| <<fcoe_if_init>> fc_attach_transport(&fcoe_nport_fc_functions);
+ *   - drivers/scsi/fcoe/fcoe.c|1254| <<fcoe_if_init>> fc_attach_transport(&fcoe_vport_fc_functions);
+ *   - drivers/scsi/fnic/fnic_main.c|1111| <<fnic_init_module>> fnic_fc_transport = fc_attach_transport(&fnic_fc_functions);
+ *   - drivers/scsi/ibmvscsi/ibmvfc.c|4986| <<ibmvfc_module_init>> ibmvfc_transport_template = fc_attach_transport(&ibmvfc_transport_functions);
+ *   - drivers/scsi/lpfc/lpfc_init.c|13495| <<lpfc_init>> fc_attach_transport(&lpfc_transport_functions);
+ *   - drivers/scsi/lpfc/lpfc_init.c|13499| <<lpfc_init>> fc_attach_transport(&lpfc_vport_transport_functions);
+ *   - drivers/scsi/qedf/qedf_main.c|3786| <<qedf_init>> fc_attach_transport(&qedf_fc_transport_fn);
+ *   - drivers/scsi/qedf/qedf_main.c|3793| <<qedf_init>> fc_attach_transport(&qedf_fc_vport_transport_fn);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7285| <<qla2x00_module_init>> fc_attach_transport(&qla2xxx_transport_functions);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7300| <<qla2x00_module_init>> fc_attach_transport(&qla2xxx_transport_vport_functions);
+ *   - drivers/scsi/storvsc_drv.c|1973| <<storvsc_drv_init>> fc_transport_template = fc_attach_transport(&fc_transport_functions);
+ */
 struct scsi_transport_template *
 fc_attach_transport(struct fc_function_template *ft)
 {
diff --git a/drivers/scsi/scsi_transport_iscsi.c b/drivers/scsi/scsi_transport_iscsi.c
index 417b868..5ad4da9 100644
--- a/drivers/scsi/scsi_transport_iscsi.c
+++ b/drivers/scsi/scsi_transport_iscsi.c
@@ -4407,6 +4407,16 @@ static int iscsi_host_match(struct attribute_container *cont,
         return &priv->t.host_attrs.ac == cont;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iscsi_iser.c|1050| <<iser_init>> iscsi_iser_scsi_transport = iscsi_register_transport(
+ *   - drivers/scsi/be2iscsi/be_main.c|5849| <<beiscsi_module_init>> iscsi_register_transport(&beiscsi_iscsi_transport);
+ *   - drivers/scsi/bnx2i/bnx2i_init.c|480| <<bnx2i_mod_init>> iscsi_register_transport(&bnx2i_iscsi_transport);
+ *   - drivers/scsi/cxgbi/libcxgbi.c|2691| <<cxgbi_iscsi_init>> *stt = iscsi_register_transport(itp);
+ *   - drivers/scsi/iscsi_tcp.c|1024| <<iscsi_sw_tcp_init>> iscsi_sw_tcp_scsi_transport = iscsi_register_transport(
+ *   - drivers/scsi/qedi/qedi_main.c|2717| <<qedi_init>> qedi_scsi_transport = iscsi_register_transport(&qedi_iscsi_transport);
+ *   - drivers/scsi/qla4xxx/ql4_os.c|9900| <<qla4xxx_module_init>> iscsi_register_transport(&qla4xxx_iscsi_transport);
+ */
 struct scsi_transport_template *
 iscsi_register_transport(struct iscsi_transport *tt)
 {
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 3fa1fa5..f9f304b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -79,6 +79,19 @@ struct blk_mq_hw_ctx {
 struct blk_mq_queue_map {
 	unsigned int *mq_map;
 	unsigned int nr_queues;
+	/*
+	 * 设置queue_offset的地方:
+	 *   - drivers/nvme/host/pci.c|449| <<nvme_pci_map_queues>> map->queue_offset = qoff;
+	 *   - drivers/nvme/host/rdma.c|2368| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2371| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/rdma.c|2377| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2380| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 *   - drivers/nvme/host/rdma.c|2391| <<nvme_rdma_map_queues>> set->map[HCTX_TYPE_POLL].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2137| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2140| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset =
+	 *   - drivers/nvme/host/tcp.c|2146| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	 *   - drivers/nvme/host/tcp.c|2149| <<nvme_tcp_map_queues>> set->map[HCTX_TYPE_READ].queue_offset = 0;
+	 */
 	unsigned int queue_offset;
 };
 
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ef375d..8da731e 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -108,6 +108,13 @@ typedef __u32 __bitwise req_flags_t;
 /* already slept for hybrid poll */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
 /* ->timeout has been called, don't expire again */
+/*
+ * 在以下使用RQF_TIMED_OUT (timeout has been called, don't expire again):
+ *   - block/blk-mq.c|710| <<__blk_mq_requeue_request>> rq->rq_flags &= ~RQF_TIMED_OUT;
+ *   - block/blk-mq.c|843| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_TIMED_OUT;
+ *   - block/blk-mq.c|862| <<blk_mq_req_expired>> if (rq->rq_flags & RQF_TIMED_OUT)
+ *   - block/blk-timeout.c|124| <<blk_add_timer>> req->rq_flags &= ~RQF_TIMED_OUT;
+ */
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 
 /* flags that prevent us from merging requests: */
@@ -590,6 +597,13 @@ struct request_queue {
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
 #define QUEUE_FLAG_SAME_COMP	4	/* complete on same CPU-group */
+/*
+ * 在以下使用QUEUE_FLAG_FAIL_IO:
+ *   - block/blk-timeout.c|25| <<blk_should_fake_timeout>> if (!test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))
+ *   - block/blk-timeout.c|45| <<part_timeout_show>> int set = test_bit(QUEUE_FLAG_FAIL_IO, &disk->queue->queue_flags);
+ *   - block/blk-timeout.c|62| <<part_timeout_store>> blk_queue_flag_set(QUEUE_FLAG_FAIL_IO, q);
+ *   - block/blk-timeout.c|64| <<part_timeout_store>> blk_queue_flag_clear(QUEUE_FLAG_FAIL_IO, q);
+ */
 #define QUEUE_FLAG_FAIL_IO	5	/* fake timeout */
 #define QUEUE_FLAG_NONROT	6	/* non-rotational device (SSD) */
 #define QUEUE_FLAG_VIRT		QUEUE_FLAG_NONROT /* paravirt device */
diff --git a/include/rdma/rdma_cm.h b/include/rdma/rdma_cm.h
index 71f48cf..b0cb2a0 100644
--- a/include/rdma/rdma_cm.h
+++ b/include/rdma/rdma_cm.h
@@ -45,6 +45,18 @@
  * RDMA identifier and release all resources allocated with the device.
  */
 enum rdma_cm_event_type {
+	/*
+	 * 在以下使用RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/core/cma.c|2691| <<cma_init_resolve_addr_work>> work->event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/core/cma.c|3062| <<addr_handler>> event.event = RDMA_CM_EVENT_ADDR_RESOLVED;
+	 *   - drivers/infiniband/ulp/iser/iser_verbs.c|847| <<iser_cma_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/infiniband/ulp/srp/ib_srp.c|2822| <<srp_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - drivers/nvme/host/rdma.c|2055| <<nvme_rdma_cm_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - fs/cifs/smbdirect.c|184| <<smbd_conn_upcall>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/9p/trans_rdma.c|244| <<p9_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/rds/rdma_transport.c|88| <<rds_rdma_cm_event_handler_cmn>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 *   - net/sunrpc/xprtrdma/verbs.c|228| <<rpcrdma_cm_event_handler>> case RDMA_CM_EVENT_ADDR_RESOLVED:
+	 */
 	RDMA_CM_EVENT_ADDR_RESOLVED,
 	RDMA_CM_EVENT_ADDR_ERROR,
 	RDMA_CM_EVENT_ROUTE_RESOLVED,
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 969e540..9ebff44 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -603,6 +603,11 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|62| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->bitmap_tags);
+ *   - block/blk-mq-tag.c|64| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->breserved_tags);
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -660,6 +665,10 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
 
+/*
+ * called by:
+ *   - block/kyber-iosched.c|723| <<kyber_get_domain_token>> sbitmap_add_wait_queue(domain_tokens, ws, wait);
+ */
 void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
 			    struct sbq_wait_state *ws,
 			    struct sbq_wait *sbq_wait)
@@ -682,6 +691,11 @@ void sbitmap_del_wait_queue(struct sbq_wait *sbq_wait)
 }
 EXPORT_SYMBOL_GPL(sbitmap_del_wait_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|188| <<blk_mq_get_tag>> sbitmap_prepare_to_wait(bt, ws, &wait, TASK_UNINTERRUPTIBLE);
+ *   - drivers/target/iscsi/iscsi_target_util.c|155| <<iscsit_wait_for_tag>> sbitmap_prepare_to_wait(sbq, ws, &wait, state);
+ */
 void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
 			     struct sbq_wait_state *ws,
 			     struct sbq_wait *sbq_wait, int state)
-- 
2.7.4

