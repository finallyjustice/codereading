From 9ed2c91a2e5a8faf74792423f8a633c1f5c2594f Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmailcom>
Date: Mon, 2 Oct 2023 23:56:20 -0700
Subject: [PATCH 1/1] linux-v6.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arch_gicv3.h           |   4 +
 arch/arm64/include/asm/kvm_emulate.h          |  21 +
 arch/arm64/include/asm/kvm_host.h             |  27 +
 arch/arm64/include/asm/kvm_mmu.h              |  29 +
 arch/arm64/kvm/arch_timer.c                   | 709 ++++++++++++++++
 arch/arm64/kvm/arm.c                          |  56 ++
 arch/arm64/kvm/guest.c                        |   4 +
 arch/arm64/kvm/handle_exit.c                  |  27 +
 arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h    |  18 +
 arch/arm64/kvm/hyp/nvhe/mem_protect.c         |   4 +
 arch/arm64/kvm/hyp/pgtable.c                  |  13 +
 arch/arm64/kvm/hyp/vgic-v3-sr.c               |  78 ++
 arch/arm64/kvm/hyp/vhe/sysreg-sr.c            |   8 +
 arch/arm64/kvm/hyp/vhe/timer-sr.c             |   4 +
 arch/arm64/kvm/mmio.c                         |   4 +
 arch/arm64/kvm/mmu.c                          |  39 +
 arch/arm64/kvm/pmu-emul.c                     |  11 +
 arch/arm64/kvm/pvtime.c                       |  35 +
 arch/arm64/kvm/sys_regs.c                     |  49 ++
 arch/arm64/kvm/vgic-sys-reg-v3.c              |  17 +
 arch/arm64/kvm/vgic/vgic-init.c               |  34 +
 arch/arm64/kvm/vgic/vgic-irqfd.c              |  32 +
 arch/arm64/kvm/vgic/vgic-its.c                | 134 +++
 arch/arm64/kvm/vgic/vgic-mmio-v3.c            |   4 +
 arch/arm64/kvm/vgic/vgic-v3.c                 |  43 +
 arch/arm64/kvm/vgic/vgic.c                    | 224 +++++
 arch/x86/events/core.c                        |   6 +
 arch/x86/include/asm/kvm_host.h               | 173 ++++
 arch/x86/include/asm/x86_init.h               |   9 +
 arch/x86/include/uapi/asm/kvm.h               |  15 +
 arch/x86/kernel/apic/apic.c                   |   4 +
 arch/x86/kernel/kvmclock.c                    |   9 +
 arch/x86/kernel/paravirt.c                    |   7 +
 arch/x86/kernel/pvclock.c                     |  10 +
 arch/x86/kernel/smpboot.c                     |   4 +
 arch/x86/kernel/tsc.c                         |  20 +
 arch/x86/kvm/hyperv.c                         |  12 +
 arch/x86/kvm/lapic.c                          |  42 +
 arch/x86/kvm/mmu/mmu.c                        |   4 +
 arch/x86/kvm/mmu/tdp_mmu.c                    |   4 +
 arch/x86/kvm/pmu.c                            | 304 +++++++
 arch/x86/kvm/pmu.h                            | 135 +++
 arch/x86/kvm/vmx/pmu_intel.c                  |  35 +
 arch/x86/kvm/vmx/vmx.c                        |   7 +
 arch/x86/kvm/x86.c                            | 783 ++++++++++++++++++
 arch/x86/kvm/x86.h                            |   7 +
 arch/x86/kvm/xen.c                            |  12 +
 drivers/clocksource/arm_arch_timer.c          | 102 +++
 drivers/irqchip/irq-gic-v3-its-pci-msi.c      |   4 +
 drivers/irqchip/irq-gic-v3-its.c              |  25 +
 drivers/irqchip/irq-gic-v3.c                  | 221 +++++
 drivers/irqchip/irq-gic.c                     |  20 +
 drivers/net/tap.c                             |   5 +
 drivers/net/virtio_net.c                      |   5 +
 .../net/wireless/intel/iwlwifi/mvm/mac-ctxt.c |   4 +
 drivers/nvme/host/core.c                      |   6 +
 drivers/vhost/net.c                           | 150 ++++
 drivers/vhost/scsi.c                          |   5 +
 include/kvm/arm_arch_timer.h                  |  58 ++
 include/kvm/arm_vgic.h                        |  41 +
 include/linux/irqchip.h                       |  10 +
 include/linux/kvm_host.h                      |   4 +
 kernel/cpu.c                                  |  10 +
 kernel/events/core.c                          |  13 +
 kernel/irq/irqdomain.c                        |  73 ++
 kernel/time/clocksource.c                     |   6 +
 kernel/time/timekeeping.c                     |  39 +
 .../selftests/kvm/include/x86_64/processor.h  |   9 +
 virt/kvm/eventfd.c                            |  18 +
 virt/kvm/irqchip.c                            |   3 +
 virt/kvm/kvm_main.c                           |  59 ++
 71 files changed, 4120 insertions(+)

diff --git a/arch/arm64/include/asm/arch_gicv3.h b/arch/arm64/include/asm/arch_gicv3.h
index 01281a533..274273719 100644
--- a/arch/arm64/include/asm/arch_gicv3.h
+++ b/arch/arm64/include/asm/arch_gicv3.h
@@ -96,6 +96,10 @@ static inline void gic_write_grpen1(u32 val)
 	isb();
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|1383| <<gic_send_sgi>> gic_write_sgi1r(val);
+ */
 static inline void gic_write_sgi1r(u64 val)
 {
 	write_sysreg_s(val, SYS_ICC_SGI1R_EL1);
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 3d6725ff0..40ff9b339 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -66,6 +66,10 @@ static __always_inline bool vcpu_el1_is_32bit(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1315| <<kvm_arch_vcpu_ioctl_vcpu_init>> vcpu_reset_hcr(vcpu);
+ */
 static inline void vcpu_reset_hcr(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hcr_el2 = HCR_GUEST_FLAGS;
@@ -103,6 +107,15 @@ static inline void vcpu_reset_hcr(struct kvm_vcpu *vcpu)
 		vcpu->arch.hcr_el2 |= HCR_ATA;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|558| <<kvm_arch_vcpu_runnable>> bool irq_lines = *vcpu_hcr(v) & (HCR_VI | HCR_VF);
+ *   - arch/arm64/kvm/arm.c|1103| <<vcpu_interrupt_line>> hcr = vcpu_hcr(vcpu);
+ *   - arch/arm64/kvm/inject_fault.c|236| <<kvm_set_sei_esr>> *vcpu_hcr(vcpu) |= HCR_VSE;
+ *   - arch/arm64/kvm/mmu.c|2061| <<kvm_set_way_flush>> unsigned long hcr = *vcpu_hcr(vcpu);
+ *   - arch/arm64/kvm/mmu.c|2076| <<kvm_set_way_flush>> *vcpu_hcr(vcpu) = hcr | HCR_TVM;
+ *   - arch/arm64/kvm/mmu.c|2094| <<kvm_toggle_cache>> *vcpu_hcr(vcpu) &= ~HCR_TVM;
+ */
 static inline unsigned long *vcpu_hcr(struct kvm_vcpu *vcpu)
 {
 	return (unsigned long *)&vcpu->arch.hcr_el2;
@@ -463,6 +476,14 @@ static inline bool kvm_is_write_fault(struct kvm_vcpu *vcpu)
 	return kvm_vcpu_dabt_iswrite(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2386| <<kvm_mpidr_to_vcpu>> if (mpidr == kvm_vcpu_get_mpidr_aff(vcpu))
+ *   - arch/arm64/kvm/psci.c|150| <<kvm_psci_vcpu_affinity_info>> mpidr = kvm_vcpu_get_mpidr_aff(tmp);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|295| <<vgic_init>> irq->mpidr = kvm_vcpu_get_mpidr_aff(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|322| <<vgic_mmio_read_v3r_typer>> unsigned long mpidr = kvm_vcpu_get_mpidr_aff(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1031| <<match_mpidr>> affinity = kvm_vcpu_get_mpidr_aff(vcpu);
+ */
 static inline unsigned long kvm_vcpu_get_mpidr_aff(struct kvm_vcpu *vcpu)
 {
 	return vcpu_read_sys_reg(vcpu, MPIDR_EL1) & MPIDR_HWID_BITMASK;
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d3dd05bbf..ff455d5f1 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -43,6 +43,17 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_IRQ_PENDING:
+ *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ */
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
@@ -933,6 +944,22 @@ void kvm_arm_resume_guest(struct kvm *kvm);
 		res.a1;							\
 	})
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|494| <<set_cntvoff>> kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
+ *   - arch/arm64/kvm/arm.c|441| <<kvm_arch_vcpu_load>> kvm_call_hyp(__kvm_flush_cpu_context, mmu);
+ *   - arch/arm64/kvm/arm.c|1086| <<kvm_arch_vcpu_ioctl_run>> kvm_call_hyp(__kvm_adjust_pc, vcpu);
+ *   - arch/arm64/kvm/hyp/pgtable.c|789| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid, mmu);
+ *   - arch/arm64/kvm/hyp/pgtable.c|791| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ctx->addr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|822| <<stage2_put_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ctx->addr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1271| <<kvm_pgtable_stage2_relax_perms>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa_nsh, pgt->mmu, addr, level);
+ *   - arch/arm64/kvm/mmu.c|172| <<kvm_flush_remote_tlbs>> kvm_call_hyp(__kvm_tlb_flush_vmid, &kvm->arch.mmu);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|544| <<kvm_vgic_init_cpu_hardware>> kvm_call_hyp(__vgic_v3_init_lrs);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|730| <<vgic_v3_load>> kvm_call_hyp(__vgic_v3_write_vmcr, cpu_if->vgic_vmcr);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|732| <<vgic_v3_load>> kvm_call_hyp(__vgic_v3_restore_aprs, cpu_if);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|756| <<vgic_v3_put>> kvm_call_hyp(__vgic_v3_save_aprs, cpu_if);
+ *   - arch/arm64/kvm/vmid.c|69| <<flush_context>> kvm_call_hyp(__kvm_flush_vm_context);
+ */
 /*
  * The couple of isb() below are there to guarantee the same behaviour
  * on VHE as on !VHE, where the eret to EL1 acts as a context
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 0e1e1ab17..90228e492 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -223,6 +223,11 @@ static inline void __clean_dcache_guest_page(void *va, size_t size)
 	kvm_flush_dcache_to_poc(va, size);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|228| <<invalidate_icache_guest_page>> __invalidate_icache_guest_page(hyp_fixmap_map(__hyp_pa(va)), size);
+ *   - arch/arm64/kvm/mmu.c|272| <<invalidate_icache_guest_page>> __invalidate_icache_guest_page(va, size);
+ */
 static inline void __invalidate_icache_guest_page(void *va, size_t size)
 {
 	if (icache_is_aliasing()) {
@@ -279,6 +284,11 @@ static inline int kvm_write_guest_lock(struct kvm *kvm, gpa_t gpa,
  * path, we rely on a previously issued DSB so that page table updates
  * and VMID reads are correctly ordered.
  */
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_mmu.h|307| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|297| <<__pkvm_prot_finalize>> params->vttbr = kvm_get_vttbr(mmu);
+ */
 static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
 {
 	struct kvm_vmid *vmid = &mmu->vmid;
@@ -295,9 +305,28 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
  * Must be called from hyp code running at EL2 with an updated VTTBR
  * and interrupts disabled.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|89| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|310| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|306| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|65| <<__tlb_switch_to_guest>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|183| <<__kvm_vcpu_run_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|56| <<__tlb_switch_to_guest>> __load_stage2(mmu, mmu->arch);
+ *
+ * 一个例子.
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_s2_mmu *hw_mmu;
+ */
 static __always_inline void __load_stage2(struct kvm_s2_mmu *mmu,
 					  struct kvm_arch *arch)
 {
+	/*
+	 * The VTCR_EL2 controls the translation table walks required for
+	 * the stage 2 translation of memory accesses from
+	 * Non-secure EL0 and EL1.
+	 */
 	write_sysreg(arch->vtcr, vtcr_el2);
 	write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
 
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index 6dcdae4d3..03cd2fb14 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -23,12 +23,79 @@
 
 #include "trace.h"
 
+/*
+ * struct kvm:
+ * -> struct kvm_arch arch;
+ *    -> struct arch_timer_vm_data timer_data;
+ *       -> u64 voffset;
+ *       -> u64 poffset;
+ *       -> ppi[NR_KVM_TIMERS];
+ *
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_cpu_context ctxt;
+ *       -> struct user_pt_regs regs;
+ *       -> u64 sys_regs[NR_SYS_REGS];
+ *    -> struct arch_timer_cpu timer_cpu;
+ *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> struct arch_timer_offset offset;
+ *             -> u64 *vm_offset;
+ *             -> u64 *vcpu_offset;
+ *       -> struct hrtimer bg_timer;
+ *       -> bool enabled;
+ *
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
+
+/*
+ * 在以下使用timecounter:
+ *   - arch/arm64/kvm/arch_timer.c|211| <<kvm_phys_timer_read>> return timecounter->cc->read(timecounter->cc);
+ *   - arch/arm64/kvm/arch_timer.c|305| <<kvm_counter_compute_delta>> ns = cyclecounter_cyc2ns(timecounter->cc,
+ *   - arch/arm64/kvm/arch_timer.c|307| <<kvm_counter_compute_delta>> timecounter->mask,
+ *   - arch/arm64/kvm/arch_timer.c|1464| <<kvm_timer_hyp_init>> timecounter = &info->timecounter;
+ *   - arch/arm64/kvm/arch_timer.c|1466| <<kvm_timer_hyp_init>> if (!timecounter->cc) {
+ */
 static struct timecounter *timecounter;
+/*
+ * 在以下设置host_vtimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1643| <<kvm_irq_init>> host_vtimer_irq = info->virtual_irq;
+ */
 static unsigned int host_vtimer_irq;
+/*
+ * 在以下设置host_ptimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1670| <<kvm_irq_init>> host_ptimer_irq = info->physical_irq;
+ */
 static unsigned int host_ptimer_irq;
+/*
+ * 在以下啊使用host_vtimer_irq_flags:
+ *   - arch/arm64/kvm/arch_timer.c|961| <<kvm_timer_vcpu_load_nogic>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1207| <<unmask_vtimer_irq_user>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1322| <<kvm_timer_cpu_up>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1644| <<kvm_irq_init>> kvm_irq_fixup_flags(host_vtimer_irq, &host_vtimer_irq_flags);
+ */
 static u32 host_vtimer_irq_flags;
+/*
+ * 在以下使用host_ptimer_irq_flags:
+ *   - arch/arm64/kvm/arch_timer.c|1324| <<kvm_timer_cpu_up>> enable_percpu_irq(host_ptimer_irq, host_ptimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1671| <<kvm_irq_init>> kvm_irq_fixup_flags(host_ptimer_irq, &host_ptimer_irq_flags);
+ */
 static u32 host_ptimer_irq_flags;
 
+/*
+ * 在以下使用has_gic_active_state:
+ *   - arch/arm64/kvm/arch_timer.c|32| <<global>> static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
+ *   - arch/arm64/kvm/arch_timer.c|255| <<kvm_arch_timer_handler>> !static_branch_unlikely(&has_gic_active_state))
+ *   - arch/arm64/kvm/arch_timer.c|856| <<kvm_timer_vcpu_load>> if (static_branch_likely(&has_gic_active_state)) {
+ *   - arch/arm64/kvm/arch_timer.c|940| <<unmask_vtimer_irq_user>> if (static_branch_likely(&has_gic_active_state))
+ *   - arch/arm64/kvm/arch_timer.c|1424| <<kvm_timer_hyp_init>> static_branch_enable(&has_gic_active_state);
+ */
 static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
 
 static const u8 default_ppi[] = {
@@ -51,15 +118,39 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      enum kvm_arch_timer_regs treg);
 static bool kvm_arch_timer_get_input_level(int vintid);
 
+/*
+ * 在以下使用arch_timer_irq_ops:
+ *   - arch/arm64/kvm/arch_timer.c|800| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq, timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|805| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq, timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1437| <<kvm_irq_init>> arch_timer_irq_ops.flags |= VGIC_IRQ_SW_RESAMPLE;
+ *   - arch/arm64/kvm/arch_timer.c|1624| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1632| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+ */
 static struct irq_ops arch_timer_irq_ops = {
 	.get_input_level = kvm_arch_timer_get_input_level,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|545| <<set_cntpoff>> if (has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|597| <<timer_save_state>> if (!has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|692| <<timer_restore_state>> if (!has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|854| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ */
 static bool has_cntpoff(void)
 {
 	return (has_vhe() && cpus_have_final_cap(ARM64_HAS_ECV_CNTPOFF));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|354| <<kvm_timer_earliest_exp>> for (i = 0; i < nr_timers(vcpu); i++) {
+ *   - arch/arm64/kvm/arch_timer.c|1012| <<kvm_timer_vcpu_reset>> for (int i = 0; i < nr_timers(vcpu); i++)
+ *   - arch/arm64/kvm/arch_timer.c|1028| <<kvm_timer_vcpu_reset>> for (int i = 0; i < nr_timers(vcpu); i++)
+ *   - arch/arm64/kvm/arch_timer.c|1550| <<timer_irqs_are_valid>> for (int i = 0; i < nr_timers(vcpu); i++) {
+ *   - arch/arm64/kvm/arch_timer.c|1566| <<timer_irqs_are_valid>> valid = hweight32(ppis) == nr_timers(vcpu);
+ *   - arch/arm64/kvm/arch_timer.c|1583| <<kvm_arch_timer_get_input_level>> for (int i = 0; i < nr_timers(vcpu); i++) {
+ */
 static int nr_timers(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu_has_nv(vcpu))
@@ -68,6 +159,15 @@ static int nr_timers(struct kvm_vcpu *vcpu)
 	return NR_KVM_TIMERS;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|324| <<kvm_timer_irq_can_fire>> ((timer_get_ctl(timer_ctx) &
+ *   - arch/arm64/kvm/arch_timer.c|685| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTV_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|696| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTP_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1162| <<read_timer_ctl>> u32 ctl = timer_get_ctl(timer);
+ *   - arch/arm64/kvm/trace_arm.h|243| <<__field>> __entry->ctl = timer_get_ctl(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|265| <<__field>> __entry->ctl = timer_get_ctl(ctx);
+ */
 u32 timer_get_ctl(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -87,6 +187,17 @@ u32 timer_get_ctl(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|458| <<kvm_timer_should_fire>> cval = timer_get_cval(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|683| <<timer_restore_state>> write_sysreg_el0(timer_get_cval(ctx), SYS_CNTV_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|689| <<timer_restore_state>> cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1214| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1223| <<kvm_arm_timer_read>> val = timer_get_cval(timer);
+ *   - arch/arm64/kvm/trace_arm.h|244| <<__field>> __entry->cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|266| <<__field>> __entry->cval = timer_get_cval(ctx);
+ */
 u64 timer_get_cval(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -106,6 +217,18 @@ u64 timer_get_cval(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|447| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|586| <<timer_save_state>> cval -= timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|670| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|678| <<timer_restore_state>> offset = timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|842| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|1191| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1204| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1250| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 static u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
 	u64 offset = 0;
@@ -113,6 +236,12 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	if (!ctxt)
 		return 0;
 
+	/*
+	 * struct arch_timer_context *ctxt:
+	 * -> struct arch_timer_offset offset;
+	 *    -> u64 *vm_offset;
+	 *    -> u64 *vcpu_offset;
+	 */
 	if (ctxt->offset.vm_offset)
 		offset += *ctxt->offset.vm_offset;
 	if (ctxt->offset.vcpu_offset)
@@ -121,6 +250,13 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	return offset;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|568| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|594| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTP_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|1013| <<kvm_timer_vcpu_reset>> timer_set_ctl(vcpu_get_timer(vcpu, i), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1290| <<kvm_arm_timer_write>> timer_set_ctl(timer, val & ~ARCH_TIMER_CTRL_IT_STAT);
+ */
 static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -143,6 +279,13 @@ static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|569| <<timer_save_state>> timer_set_cval(ctx, read_sysreg_el0(SYS_CNTV_CVAL));
+ *   - arch/arm64/kvm/arch_timer.c|600| <<timer_save_state>> timer_set_cval(ctx, cval);
+ *   - arch/arm64/kvm/arch_timer.c|1286| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ *   - arch/arm64/kvm/arch_timer.c|1294| <<kvm_arm_timer_write>> timer_set_cval(timer, val);
+ */
 static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -165,6 +308,13 @@ static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1083| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1084| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_ptimer(vcpu), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1124| <<kvm_arm_timer_set_reg(KVM_REG_ARM_TIMER_CNT)>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1139| <<kvm_arm_timer_set_reg(KVM_REG_ARM_PTIMER_CNT)>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ */
 static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 {
 	if (!ctxt->offset.vm_offset) {
@@ -175,11 +325,40 @@ static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 	WRITE_ONCE(*ctxt->offset.vm_offset, offset);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|264| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|423| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1042| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1083| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1098| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1162| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1175| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1221| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 u64 kvm_phys_timer_read(void)
 {
 	return timecounter->cc->read(timecounter->cc);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|280| <<kvm_arch_timer_handler>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|629| <<kvm_timer_blocking>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|895| <<kvm_timer_vcpu_load>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|946| <<kvm_timer_vcpu_put>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1004| <<kvm_timer_vcpu_reset>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1253| <<kvm_arm_timer_read_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1314| <<kvm_arm_timer_write_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1619| <<kvm_timer_enable>> get_timer_map(vcpu, &map);
+ *
+ * struct timer_map {
+ *     struct arch_timer_context *direct_vtimer;
+ *     struct arch_timer_context *direct_ptimer;
+ *     struct arch_timer_context *emul_vtimer;
+ *     struct arch_timer_context *emul_ptimer;
+ * };
+ */
 static void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	if (vcpu_has_nv(vcpu)) {
@@ -195,6 +374,9 @@ static void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 			map->emul_ptimer = vcpu_hptimer(vcpu);
 		}
 	} else if (has_vhe()) {
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+		 */
 		map->direct_vtimer = vcpu_vtimer(vcpu);
 		map->direct_ptimer = vcpu_ptimer(vcpu);
 		map->emul_vtimer = NULL;
@@ -215,17 +397,37 @@ static inline bool userspace_irqchip(struct kvm *kvm)
 		unlikely(!irqchip_in_kernel(kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static void soft_timer_start(struct hrtimer *hrt, u64 ns)
 {
 	hrtimer_start(hrt, ktime_add_ns(ktime_get(), ns),
 		      HRTIMER_MODE_ABS_HARD);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|653| <<kvm_timer_unblocking>> soft_timer_cancel(&timer->bg_timer);
+ *   - arch/arm64/kvm/arch_timer.c|962| <<kvm_timer_vcpu_put>> soft_timer_cancel(&map.emul_vtimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|964| <<kvm_timer_vcpu_put>> soft_timer_cancel(&map.emul_ptimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1040| <<kvm_timer_vcpu_reset>> soft_timer_cancel(&map.emul_vtimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1042| <<kvm_timer_vcpu_reset>> soft_timer_cancel(&map.emul_ptimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1317| <<kvm_arm_timer_write_sysreg>> soft_timer_cancel(&timer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1540| <<kvm_timer_vcpu_terminate>> soft_timer_cancel(&timer->bg_timer);
+ */
 static void soft_timer_cancel(struct hrtimer *hrt)
 {
 	hrtimer_cancel(hrt);
 }
 
+/*
+ * 在以下使用kvm_arch_timer_handler():
+ *   - arch/arm64/kvm/arch_timer.c|1408| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_vtimer_irq, kvm_arch_timer_handler, "kvm guest vtimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1432| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_ptimer_irq, kvm_arch_timer_handler, "kvm guest ptimer", kvm_get_running_vcpus());
+ */
 static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 {
 	struct kvm_vcpu *vcpu = *(struct kvm_vcpu **)dev_id;
@@ -258,9 +460,36 @@ static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|342| <<wfit_delay_ns>> return kvm_counter_compute_delta(ctx, val);
+ */
 static u64 kvm_counter_compute_delta(struct arch_timer_context *timer_ctx,
 				     u64 val)
 {
+	/*
+	 * struct kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct arch_timer_vm_data timer_data;
+	 *       -> u64 voffset;
+	 *       -> u64 poffset;
+	 *       -> ppi[NR_KVM_TIMERS];
+	 *
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_cpu_context ctxt;
+	 *       -> struct user_pt_regs regs;
+	 *       -> u64 sys_regs[NR_SYS_REGS];
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *          -> struct kvm_vcpu *vcpu;
+	 *          -> struct arch_timer_offset offset;
+	 *             -> u64 *vm_offset;
+	 *             -> u64 *vcpu_offset;
+	 *       -> struct hrtimer bg_timer;
+	 *       -> bool enabled;
+	 */
 	u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
 
 	if (now < val) {
@@ -276,11 +505,33 @@ static u64 kvm_counter_compute_delta(struct arch_timer_context *timer_ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|359| <<kvm_timer_earliest_exp>> min_delta = min(min_delta, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|412| <<kvm_hrtimer_expire>> ns = kvm_timer_compute_delta(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|1164| <<read_timer_ctl>> if (!kvm_timer_compute_delta(timer))
+ */
 static u64 kvm_timer_compute_delta(struct arch_timer_context *timer_ctx)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|342| <<wfit_delay_ns>> return kvm_counter_compute_delta(ctx, val);
+	 */
 	return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|358| <<kvm_timer_earliest_exp>> if (kvm_timer_irq_can_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_should_fire>> if (!kvm_timer_irq_can_fire(timer_ctx))
+ *   - arch/arm64/kvm/arch_timer.c|522| <<timer_emulate>> if (should_fire || !kvm_timer_irq_can_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|635| <<kvm_timer_blocking>> if (!kvm_timer_irq_can_fire(map.direct_vtimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|636| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.direct_ptimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|637| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.emul_vtimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|638| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.emul_ptimer) &&
+ */
 static bool kvm_timer_irq_can_fire(struct arch_timer_context *timer_ctx)
 {
 	WARN_ON(timer_ctx && timer_ctx->loaded);
@@ -310,6 +561,11 @@ static u64 wfit_delay_ns(struct kvm_vcpu *vcpu)
  * Returns the earliest expiration time in ns among guest timers.
  * Note that it will return 0 if none of timers can fire.
  */
+/*
+ * 在以下使用kvm_timer_earliest_exp():
+ *   - arch/arm64/kvm/arch_timer.c|386| <<kvm_bg_timer_expire>> ns = kvm_timer_earliest_exp(vcpu);
+ *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static u64 kvm_timer_earliest_exp(struct kvm_vcpu *vcpu)
 {
 	u64 min_delta = ULLONG_MAX;
@@ -357,6 +613,15 @@ static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
 static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_context *ctx;
@@ -383,6 +648,19 @@ static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|287| <<kvm_arch_timer_handler>> if (kvm_timer_should_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|481| <<kvm_timer_update_run>> if (kvm_timer_should_fire(vtimer))
+ *   - arch/arm64/kvm/arch_timer.c|483| <<kvm_timer_update_run>> if (kvm_timer_should_fire(ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|508| <<timer_emulate>> bool should_fire = kvm_timer_should_fire(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|727| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|747| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|934| <<kvm_timer_should_notify_user>> return kvm_timer_should_fire(vtimer) != vlevel ||
+ *   - arch/arm64/kvm/arch_timer.c|935| <<kvm_timer_should_notify_user>> kvm_timer_should_fire(ptimer) != plevel;
+ *   - arch/arm64/kvm/arch_timer.c|979| <<unmask_vtimer_irq_user>> if (!kvm_timer_should_fire(vtimer)) {
+ *   - arch/arm64/kvm/arch_timer.c|1588| <<kvm_arch_timer_get_input_level>> return kvm_timer_should_fire(ctx);
+ */
 static bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx)
 {
 	enum kvm_arch_timers index;
@@ -419,6 +697,18 @@ static bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx)
 	if (!kvm_timer_irq_can_fire(timer_ctx))
 		return false;
 
+	/*
+	 * 在以下使用timer_get_offset():
+	 *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|447| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|586| <<timer_save_state>> cval -= timer_get_offset(ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|670| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|678| <<timer_restore_state>> offset = timer_get_offset(ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|842| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+	 *   - arch/arm64/kvm/arch_timer.c|1191| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1204| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1250| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+	 */
 	cval = timer_get_cval(timer_ctx);
 	now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
 
@@ -448,16 +738,44 @@ void kvm_timer_update_run(struct kvm_vcpu *vcpu)
 		regs->device_irq_level |= KVM_ARM_DEV_EL1_PTIMER;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_arch_timer_handler>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_hrtimer_expire>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|513| <<timer_emulate>> kvm_timer_update_irq(ctx->vcpu, should_fire, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|727| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|747| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|980| <<unmask_vtimer_irq_user>> kvm_timer_update_irq(vcpu, false, vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1029| <<kvm_timer_vcpu_reset>> kvm_timer_update_irq(vcpu, false,
+ *   - arch/arm64/kvm/trace_arm.h|181| <<__field>> TRACE_EVENT(kvm_timer_update_irq,
+ *
+ * 核心思想是调用kvm_vgic_inject_irq()
+ */
 static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				 struct arch_timer_context *timer_ctx)
 {
 	int ret;
 
+	/*
+	 * struct arch_timer_context *timer_ctx:
+	 *   // Output level of the timer IRQ
+	 *   struct {
+	 *       bool                    level;
+	 *   } irq;
+	 */
 	timer_ctx->irq.level = new_level;
 	trace_kvm_timer_update_irq(vcpu->vcpu_id, timer_irq(timer_ctx),
 				   timer_ctx->irq.level);
 
 	if (!userspace_irqchip(vcpu->kvm)) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|497| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/arm.c|1172| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1180| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|346| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+		 */
 		ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
 					  timer_irq(timer_ctx),
 					  timer_ctx->irq.level,
@@ -466,6 +784,12 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1137| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1139| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1542| <<kvm_arm_timer_write_sysreg>> timer_emulate(timer);
+ */
 /* Only called for a fully emulated timer */
 static void timer_emulate(struct arch_timer_context *ctx)
 {
@@ -489,17 +813,43 @@ static void timer_emulate(struct arch_timer_context *ctx)
 	soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|544| <<timer_save_state>> set_cntvoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|629| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void set_cntvoff(u64 cntvoff)
 {
 	kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|565| <<timer_save_state>> set_cntpoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|650| <<timer_restore_state>> set_cntpoff(offset);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void set_cntpoff(u64 cntpoff)
 {
 	if (has_cntpoff())
 		write_sysreg_s(cntpoff, SYS_CNTPOFF_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1135| <<kvm_timer_vcpu_put>> timer_save_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1137| <<kvm_timer_vcpu_put>> timer_save_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1447| <<kvm_arm_timer_read_sysreg>> timer_save_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1509| <<kvm_arm_timer_write_sysreg>> timer_save_state(timer);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void timer_save_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -519,6 +869,10 @@ static void timer_save_state(struct arch_timer_context *ctx)
 
 	case TIMER_VTIMER:
 	case TIMER_HVTIMER:
+		/*
+		 * 因为寄存器像是x86的ibrs一样是共享的
+		 * 所以在退出guest mode的时候要保存
+		 */
 		timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
 		timer_set_cval(ctx, read_sysreg_el0(SYS_CNTV_CVAL));
 
@@ -575,8 +929,27 @@ static void timer_save_state(struct arch_timer_context *ctx)
  * thread is removed from its waitqueue and made runnable when there's a timer
  * interrupt to handle.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1190| <<kvm_timer_vcpu_put>> kvm_timer_blocking(vcpu);
+ */
 static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct arch_timer_cpu {
+	 *     struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *
+	 *     // Background timer used when the guest is not running
+	 *     struct hrtimer                  bg_timer;
+	 *
+	 *     // Is the timer enabled
+	 *     bool                    enabled;
+	 * };
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 */
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
@@ -593,6 +966,13 @@ static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 	    !vcpu_has_wfit_active(vcpu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+	 *
+	 * Background timer used when the guest is not running
+	 */
 	/*
 	 * At least one guest time will expire. Schedule a background timer.
 	 * Set the earliest expiration time among the guest timers.
@@ -600,6 +980,10 @@ static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 	soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1110| <<kvm_timer_vcpu_load>> kvm_timer_unblocking(vcpu);
+ */
 static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -607,6 +991,16 @@ static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|857| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|859| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1196| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1247| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void timer_restore_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -653,13 +1047,35 @@ static void timer_restore_state(struct arch_timer_context *ctx)
 	local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|933| <<kvm_timer_vcpu_load_gic>> set_timer_irq_phys_active(ctx, phys_active);
+ *   - arch/arm64/kvm/arch_timer.c|1205| <<unmask_vtimer_irq_user>> set_timer_irq_phys_active(vtimer, false);
+ */
 static inline void set_timer_irq_phys_active(struct arch_timer_context *ctx, bool active)
 {
 	int r;
+	/*
+	 * irq_set_irqchip_state - set the state of a forwarded interrupt.
+	 *      @irq: Interrupt line that is forwarded to a VM
+	 *      @which: State to be restored (one of IRQCHIP_STATE_*)
+	 *      @val: Value corresponding to @which
+	 *
+	 *      This call sets the internal irqchip state of an interrupt,
+	 *      depending on the value of @which.
+	 *
+	 *      This function should be called with migration disabled if the
+	 *      interrupt controller has per-cpu registers.
+	 */
 	r = irq_set_irqchip_state(ctx->host_timer_irq, IRQCHIP_STATE_ACTIVE, active);
 	WARN_ON(r);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1192| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_gic(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1194| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_gic(map.direct_ptimer);
+ */
 static void kvm_timer_vcpu_load_gic(struct arch_timer_context *ctx)
 {
 	struct kvm_vcpu *vcpu = ctx->vcpu;
@@ -681,6 +1097,10 @@ static void kvm_timer_vcpu_load_gic(struct arch_timer_context *ctx)
 	set_timer_irq_phys_active(ctx, phys_active);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1196| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_nogic(vcpu);
+ */
 static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -709,6 +1129,11 @@ static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 		enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1168| <<timer_set_traps>> assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
+ *   - arch/arm64/kvm/arch_timer.c|1169| <<timer_set_traps>> assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
+ */
 /* If _pred is true, set bit in _set, otherwise set it in _clr */
 #define assign_clear_set_bit(_pred, _bit, _clr, _set)			\
 	do {								\
@@ -718,6 +1143,10 @@ static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 			(_clr) |= (_bit);				\
 	} while (0)
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1190| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_nested_switch(vcpu, &map);
+ */
 static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 					      struct timer_map *map)
 {
@@ -769,6 +1198,10 @@ static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1230| <<kvm_timer_vcpu_load>> timer_set_traps(vcpu, &map);
+ */
 static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	bool tpt, tpc;
@@ -827,12 +1260,26 @@ static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 	assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
 	assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
 
+	/*
+	 * 注释
+	 * Controls the generation of an event stream from the physical
+	 * counter, and access from EL1 to the physical counter and the EL1
+	 * physical timer.
+	 * 控制哪些timer的counter access会trap!
+	 */
 	/* This only happens on VHE, so use the CNTHCTL_EL2 accessor. */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|448| <<kvm_arch_vcpu_load>> kvm_timer_vcpu_load(vcpu);
+ */
 void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 返回&(v)->arch.timer_cpu
+	 */
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
@@ -852,8 +1299,32 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 		kvm_timer_vcpu_load_nogic(vcpu);
 	}
 
+	/*
+	 * 只在此处调用
+	 */
 	kvm_timer_unblocking(vcpu);
 
+	/*
+	 * vcpu_vtimer(vcpu);
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *          -> struct arch_timer_offset offset;
+	 *             -> u64 *vm_offset;
+	 *             -> u64 *vcpu_offset;
+	 *
+	 *
+	 * struct timer_map {
+	 *     struct arch_timer_context *direct_vtimer;
+	 *     struct arch_timer_context *direct_ptimer;
+	 *     struct arch_timer_context *emul_vtimer;
+	 *     struct arch_timer_context *emul_ptimer;
+	 * };
+	 *
+	 * direct_vtimer: 比如(&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+	 */
 	timer_restore_state(map.direct_vtimer);
 	if (map.direct_ptimer)
 		timer_restore_state(map.direct_ptimer);
@@ -865,6 +1336,12 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 	timer_set_traps(vcpu, &map);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|850| <<kvm_vcpu_exit_request>> if (kvm_timer_should_notify_user(vcpu) ||
+ *
+ * 如果irqchip_in_kernel就返回false
+ */
 bool kvm_timer_should_notify_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -882,6 +1359,10 @@ bool kvm_timer_should_notify_user(struct kvm_vcpu *vcpu)
 	       kvm_timer_should_fire(ptimer) != plevel;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|475| <<kvm_arch_vcpu_put>> kvm_timer_vcpu_put(vcpu);
+ */
 void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -919,6 +1400,12 @@ void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
  * timer and if so, unmask the timer irq signal on the host interrupt
  * controller to ensure that we see future timer signals.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1312| <<kvm_timer_sync_user>> unmask_vtimer_irq_user(vcpu);
+ *
+ * 只在irqchip_in_kernel为false的时候调用
+ */
 static void unmask_vtimer_irq_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -932,6 +1419,11 @@ static void unmask_vtimer_irq_user(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|969| <<kvm_arch_vcpu_ioctl_run>> kvm_timer_sync_user(vcpu);
+ *   - arch/arm64/kvm/arm.c|1015| <<kvm_arch_vcpu_ioctl_run>> kvm_timer_sync_user(vcpu);
+ */
 void kvm_timer_sync_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -943,11 +1435,23 @@ void kvm_timer_sync_user(struct kvm_vcpu *vcpu)
 		unmask_vtimer_irq_user(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/reset.c|302| <<kvm_reset_vcpu>> ret = kvm_timer_vcpu_reset(vcpu);
+ */
 int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
+	/*
+	 * struct timer_map {
+	 *     struct arch_timer_context *direct_vtimer;
+	 *     struct arch_timer_context *direct_ptimer;
+	 *     struct arch_timer_context *emul_vtimer;
+	 *     struct arch_timer_context *emul_ptimer;
+	 * };
+	 */
 	get_timer_map(vcpu, &map);
 
 	/*
@@ -977,6 +1481,15 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 					     vcpu_get_timer(vcpu, i));
 
 		if (irqchip_in_kernel(vcpu->kvm)) {
+			/*
+			 * kvm_vgic_reset_mapped_irq - Reset a mapped IRQ
+			 * @vcpu: The VCPU pointer
+			 * @vintid: The INTID of the interrupt
+			 *
+			 * Reset the active and pending states of a mapped interrupt.  Kernel
+			 * subsystems injecting mapped interrupts should reset their interrupt lines
+			 * when we are doing a reset of the VM.
+			 */
 			kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_vtimer));
 			if (map.direct_ptimer)
 				kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_ptimer));
@@ -991,6 +1504,10 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1448| <<kvm_timer_vcpu_init>> timer_context_init(vcpu, i);
+ */
 static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 {
 	struct arch_timer_context *ctxt = vcpu_get_timer(vcpu, timerid);
@@ -1003,6 +1520,15 @@ static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 	else
 		ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
 
+	/*
+	 * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+	 * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+	 * 然后再次进入guest时,就有中断了.
+	 *
+	 * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+	 * timer到期的时候(还在host mode)唤醒guest并inject irq.
+	 * KVM注册了一个hrtimer来处理这种情况.
+	 */
 	hrtimer_init(&ctxt->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
 	ctxt->hrtimer.function = kvm_hrtimer_expire;
 
@@ -1018,6 +1544,10 @@ static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|380| <<kvm_arch_vcpu_create>> kvm_timer_vcpu_init(vcpu);
+ */
 void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1035,12 +1565,20 @@ void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 	timer->bg_timer.function = kvm_bg_timer_expire;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|169| <<kvm_arch_init_vm>> kvm_timer_init_vm(kvm);
+ */
 void kvm_timer_init_vm(struct kvm *kvm)
 {
 	for (int i = 0; i < NR_KVM_TIMERS; i++)
 		kvm->arch.timer_data.ppi[i] = default_ppi[i];
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1906| <<kvm_arch_hardware_enable>> kvm_timer_cpu_up();
+ */
 void kvm_timer_cpu_up(void)
 {
 	enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
@@ -1048,6 +1586,10 @@ void kvm_timer_cpu_up(void)
 		enable_percpu_irq(host_ptimer_irq, host_ptimer_irq_flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1915| <<kvm_arch_hardware_disable>> kvm_timer_cpu_down();
+ */
 void kvm_timer_cpu_down(void)
 {
 	disable_percpu_irq(host_vtimer_irq);
@@ -1055,6 +1597,10 @@ void kvm_timer_cpu_down(void)
 		disable_percpu_irq(host_ptimer_irq);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|639| <<set_timer_reg>> return kvm_arm_timer_set_reg(vcpu, reg->id, val);
+ */
 int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 {
 	struct arch_timer_context *timer;
@@ -1098,6 +1644,10 @@ int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1588| <<kvm_arm_timer_read(TIMER_REG_CTL)>> val = read_timer_ctl(timer);
+ */
 static u64 read_timer_ctl(struct arch_timer_context *timer)
 {
 	/*
@@ -1114,6 +1664,11 @@ static u64 read_timer_ctl(struct arch_timer_context *timer)
 	return ctl;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|647| <<get_timer_reg>> val = kvm_arm_timer_get_reg(vcpu, reg->id);
+ *   - arch/arm64/kvm/handle_exit.c|130| <<kvm_handle_wfx>> now = kvm_arm_timer_get_reg(vcpu, KVM_REG_ARM_TIMER_CNT);
+ */
 u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 {
 	switch (regid) {
@@ -1139,6 +1694,17 @@ u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 	return (u64)-1;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1174| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1177| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1180| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1183| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1186| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1189| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1242| <<kvm_arm_timer_read_sysreg>> return kvm_arm_timer_read(vcpu, timer, treg);
+ *   - arch/arm64/kvm/arch_timer.c|1247| <<kvm_arm_timer_read_sysreg>> val = kvm_arm_timer_read(vcpu, timer, treg);
+ */
 static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      struct arch_timer_context *timer,
 			      enum kvm_arch_timer_regs treg)
@@ -1160,6 +1726,28 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 		break;
 
 	case TIMER_REG_CNT:
+		/*
+		 * struct kvm:
+		 * -> struct kvm_arch arch;
+		 *    -> struct arch_timer_vm_data timer_data;
+		 *       -> u64 voffset;
+		 *       -> u64 poffset;
+		 *       -> ppi[NR_KVM_TIMERS];
+		 *
+		 * struct kvm_vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct kvm_cpu_context ctxt;
+		 *       -> struct user_pt_regs regs;
+		 *       -> u64 sys_regs[NR_SYS_REGS];
+		 *    -> struct arch_timer_cpu timer_cpu;
+		 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+		 *          -> struct kvm_vcpu *vcpu;
+		 *          -> struct arch_timer_offset offset;
+		 *             -> u64 *vm_offset;
+		 *             -> u64 *vcpu_offset;
+		 *       -> struct hrtimer bg_timer;
+		 *       -> bool enabled;
+		 */
 		val = kvm_phys_timer_read() - timer_get_offset(timer);
 		break;
 
@@ -1174,6 +1762,22 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1206| <<access_arch_timer>> p->regval = kvm_arm_timer_read_sysreg(vcpu, tmr, treg);
+ */
 u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 			      enum kvm_arch_timers tmr,
 			      enum kvm_arch_timer_regs treg)
@@ -1199,6 +1803,15 @@ u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1118| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1129| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1133| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1144| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1294| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ *   - arch/arm64/kvm/arch_timer.c|1299| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ */
 static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 				struct arch_timer_context *timer,
 				enum kvm_arch_timer_regs treg,
@@ -1226,6 +1839,22 @@ static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1204| <<access_arch_timer>> kvm_arm_timer_write_sysreg(vcpu, tmr, treg, p->regval);
+ */
 void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 				enum kvm_arch_timers tmr,
 				enum kvm_arch_timer_regs treg,
@@ -1249,6 +1878,9 @@ void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * struct irq_chip timer_chip.irq_set_vcpu_affinity = timer_irq_set_vcpu_affinity()
+ */
 static int timer_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu)
 {
 	if (vcpu)
@@ -1259,6 +1891,9 @@ static int timer_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu)
 	return 0;
 }
 
+/*
+ * struct irq_chip timer_chip.irq_set_irqchip_state = timer_irq_set_irqchip_state()
+ */
 static int timer_irq_set_irqchip_state(struct irq_data *d,
 				       enum irqchip_irq_state which, bool val)
 {
@@ -1273,12 +1908,18 @@ static int timer_irq_set_irqchip_state(struct irq_data *d,
 	return 0;
 }
 
+/*
+ * struct irq_chip timer_chip.irq_eoi = timer_irq_eoi()
+ */
 static void timer_irq_eoi(struct irq_data *d)
 {
 	if (!irqd_is_forwarded_to_vcpu(d))
 		irq_chip_eoi_parent(d);
 }
 
+/*
+ * struct irq_chip timer_chip.irq_ack = timer_irq_ack()
+ */
 static void timer_irq_ack(struct irq_data *d)
 {
 	d = d->parent_data;
@@ -1286,6 +1927,10 @@ static void timer_irq_ack(struct irq_data *d)
 		d->chip->irq_ack(d);
 }
 
+/*
+ * 在以下使用timer_chip:
+ *   - arch/arm64/kvm/arch_timer.c|1383| <<timer_irq_domain_alloc>> return irq_domain_set_hwirq_and_chip(domain, virq, hwirq, &timer_chip, NULL);
+ */
 static struct irq_chip timer_chip = {
 	.name			= "KVM",
 	.irq_ack		= timer_irq_ack,
@@ -1311,11 +1956,20 @@ static void timer_irq_domain_free(struct irq_domain *domain, unsigned int virq,
 {
 }
 
+/*
+ * 在以下使用timer_domain_ops:
+ *   - arch/arm64/kvm/arch_timer.c|1853| <<kvm_irq_init>> domain = irq_domain_create_hierarchy(data->domain, 0, NR_KVM_TIMERS, fwnode, &timer_domain_ops, NULL);
+ */
 static const struct irq_domain_ops timer_domain_ops = {
 	.alloc	= timer_irq_domain_alloc,
 	.free	= timer_irq_domain_free,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1839| <<kvm_irq_init>> kvm_irq_fixup_flags(host_vtimer_irq, &host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1866| <<kvm_irq_init>> kvm_irq_fixup_flags(host_ptimer_irq, &host_ptimer_irq_flags);
+ */
 static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 {
 	*flags = irq_get_trigger_type(virq);
@@ -1326,6 +1980,10 @@ static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1906| <<kvm_timer_hyp_init>> err = kvm_irq_init(info);
+ */
 static int kvm_irq_init(struct arch_timer_kvm_info *info)
 {
 	struct irq_domain *domain = NULL;
@@ -1374,11 +2032,28 @@ static int kvm_irq_init(struct arch_timer_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2044| <<init_subsystems>> err = kvm_timer_hyp_init(vgic_present);
+ */
 int __init kvm_timer_hyp_init(bool has_gic)
 {
 	struct arch_timer_kvm_info *info;
 	int err;
 
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 *
+	 * 在以下使用arch_timer_kvm_info:
+	 *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+	 *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+	 *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+	 *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+	 */
 	info = arch_timer_get_kvm_info();
 	timecounter = &info->timecounter;
 
@@ -1452,6 +2127,10 @@ int __init kvm_timer_hyp_init(bool has_gic)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|407| <<kvm_arch_vcpu_destroy>> kvm_timer_vcpu_terminate(vcpu);
+ */
 void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1459,6 +2138,10 @@ void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|2053| <<kvm_timer_enable>> if (!timer_irqs_are_valid(vcpu)) {
+ */
 static bool timer_irqs_are_valid(struct kvm_vcpu *vcpu)
 {
 	u32 ppis = 0;
@@ -1492,6 +2175,9 @@ static bool timer_irqs_are_valid(struct kvm_vcpu *vcpu)
 	return valid;
 }
 
+/*
+ * struct irq_ops arch_timer_irq_ops..get_input_level = kvm_arch_timer_get_input_level,
+ */
 static bool kvm_arch_timer_get_input_level(int vintid)
 {
 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
@@ -1513,6 +2199,10 @@ static bool kvm_arch_timer_get_input_level(int vintid)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|615| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_timer_enable(vcpu);
+ */
 int kvm_timer_enable(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1537,6 +2227,17 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 
 	get_timer_map(vcpu, &map);
 
+	/*
+	 * 在以下使用arch_timer_irq_ops:
+	 *   - arch/arm64/kvm/arch_timer.c|800| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq,
+	 *                                                                                      timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|805| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq,
+	 *                                                                                      timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|1437| <<kvm_irq_init>> arch_timer_irq_ops.flags |= VGIC_IRQ_SW_RESAMPLE;
+	 *   - arch/arm64/kvm/arch_timer.c|1624| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|1632| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+	 */
+
 	ret = kvm_vgic_map_phys_irq(vcpu,
 				    map.direct_vtimer->host_timer_irq,
 				    timer_irq(map.direct_vtimer),
@@ -1559,6 +2260,10 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1864| <<cpu_hyp_init_features>> kvm_timer_init_vhe();
+ */
 /* If we have CNTPOFF, permanently set ECV to enable it */
 void kvm_timer_init_vhe(void)
 {
@@ -1658,6 +2363,10 @@ int kvm_arm_timer_has_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1619| <<kvm_arch_vm_ioctl(KVM_ARM_SET_COUNTER_OFFSET)>> return kvm_vm_ioctl_set_counter_offset(kvm, &offset);
+ */
 int kvm_vm_ioctl_set_counter_offset(struct kvm *kvm,
 				    struct kvm_arm_counter_offset *offset)
 {
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index d1cb298a5..cbdb300a0 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -420,6 +420,11 @@ void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5985| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_s2_mmu *mmu;
@@ -704,6 +709,12 @@ static void kvm_vcpu_sleep(struct kvm_vcpu *vcpu)
  * the vCPU is runnable.  The vCPU may or may not be scheduled out, depending
  * on when a wake event arrives, e.g. there may already be a pending wake event.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|739| <<kvm_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|147| <<kvm_handle_wfx>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/psci.c|49| <<kvm_psci_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ */
 void kvm_vcpu_wfi(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -773,6 +784,10 @@ static int kvm_vcpu_suspend(struct kvm_vcpu *vcpu)
  *	   < 0 if we should exit to userspace, where the return value indicates
  *	   an error
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|936| <<kvm_arch_vcpu_ioctl_run>> ret = check_vcpu_requests(vcpu);
+ */
 static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 {
 	if (kvm_request_pending(vcpu)) {
@@ -895,6 +910,10 @@ static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
  * return with return value 0 and with the kvm_run structure filled in with the
  * required data for the requested emulation.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4128| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -906,6 +925,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 			return ret;
 	}
 
+	/*
+	 * 除了调度
+	 * 核心是kvm_arch_vcpu_load()
+	 */
 	vcpu_load(vcpu);
 
 	if (run->immediate_exit) {
@@ -913,6 +936,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|922| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/riscv/kvm/vcpu.c|1173| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/s390/kvm/kvm-s390.c|4992| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/x86/kvm/x86.c|11710| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 */
 	kvm_sigset_activate(vcpu);
 
 	ret = 1;
@@ -922,6 +954,14 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		/*
 		 * Check conditions before entering the guest
 		 */
+		/*
+		 * 注释.
+		 * xfer_to_guest_mode_handle_work - Check and handle pending work which needs
+		 *                                  to be handled before going to guest mode
+		 * @vcpu:       Pointer to current's VCPU data
+		 *
+		 * Returns: 0 or an error code
+		 */
 		ret = xfer_to_guest_mode_handle_work(vcpu);
 		if (!ret)
 			ret = 1;
@@ -1653,6 +1693,16 @@ void unlock_all_vcpus(struct kvm *kvm)
 	unlock_vcpus(kvm, atomic_read(&kvm->online_vcpus) - 1);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1756| <<kvm_vm_ioctl_set_counter_offset>> if (lock_all_vcpus(kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|91| <<kvm_vgic_create>> if (!lock_all_vcpus(kvm))
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2064| <<vgic_its_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2772| <<vgic_its_ctrl>> if (!lock_all_vcpus(kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|265| <<vgic_set_common_attr>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|385| <<vgic_v2_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|546| <<vgic_v3_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ */
 /* Returns true if all vcpus were locked, false otherwise */
 bool lock_all_vcpus(struct kvm *kvm)
 {
@@ -2360,6 +2410,12 @@ static int __init init_hyp_mode(void)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/psci.c|72| <<kvm_psci_vcpu_on>> vcpu = kvm_mpidr_to_vcpu(kvm, cpu_id);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|491| <<vgic_v3_parse_attr>> reg_attr->vcpu = kvm_mpidr_to_vcpu(dev->kvm, mpidr_reg);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|232| <<vgic_mmio_write_irouter>> irq->target_vcpu = kvm_mpidr_to_vcpu(vcpu->kvm, irq->mpidr);
+ */
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr)
 {
 	struct kvm_vcpu *vcpu;
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 20280a523..3eaf2d3d9 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -759,6 +759,10 @@ int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
 	return kvm_arm_copy_sys_reg_indices(vcpu, uindices);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1443| <<kvm_arch_vcpu_ioctl(KVM_GET_ONE_REG)>> r = kvm_arm_get_reg(vcpu, &reg);
+ */
 int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	/* We currently use nothing arch-specific in upper 32 bits */
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 6dcd6604b..a2c1029c1 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -111,6 +111,9 @@ static int handle_no_fpsimd(struct kvm_vcpu *vcpu)
  *
  * WF{I,E}T can immediately return if the deadline has already expired.
  */
+/*
+ * static exit_handle_fn arm_exit_handlers[ESR_ELx_EC_WFx] = kvm_handle_wfx()
+ */
 static int kvm_handle_wfx(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
@@ -226,6 +229,10 @@ static int kvm_handle_eret(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用arm_exit_handlers[]:
+ *   - arch/arm64/kvm/handle_exit.c|261| <<kvm_get_exit_handler>> return arm_exit_handlers[esr_ec];
+ */
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -253,6 +260,10 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|284| <<handle_trap_exceptions>> exit_handler = kvm_get_exit_handler(vcpu);
+ */
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
@@ -267,6 +278,10 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
  * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
  * emulation first.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|315| <<handle_exit>> return handle_trap_exceptions(vcpu);
+ */
 static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
 {
 	int handled;
@@ -292,6 +307,10 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1065| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
@@ -335,6 +354,10 @@ int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1042| <<kvm_arch_vcpu_ioctl_run>> handle_exit_early(vcpu, ret);
+ */
 /* For exit types that need handling before we can be preempted */
 void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 {
@@ -356,6 +379,10 @@ void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 		kvm_handle_guest_serror(vcpu, kvm_vcpu_get_esr(vcpu));
 }
 
+/*
+ * 在以下使用nvhe_hyp_panic_handler():
+ *   - arch/arm64/kernel/image-vars.h|62| <<global>> KVM_NVHE_ALIAS(nvhe_hyp_panic_handler);
+ */
 void __noreturn __cold nvhe_hyp_panic_handler(u64 esr, u64 spsr,
 					      u64 elr_virt, u64 elr_phys,
 					      u64 par, uintptr_t vcpu,
diff --git a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
index bb6b571ec..4306880c5 100644
--- a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
+++ b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
@@ -88,6 +88,10 @@ static inline void __sysreg_save_el2_return_state(struct kvm_cpu_context *ctxt)
 
 static inline void __sysreg_restore_common_state(struct kvm_cpu_context *ctxt)
 {
+	/*
+	 * MDSCR_EL1 is a 32-bit register, and is part of the Debug registers
+	 * functional group.
+	 */
 	write_sysreg(ctxt_sys_reg(ctxt, MDSCR_EL1),  mdscr_el1);
 }
 
@@ -97,6 +101,20 @@ static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
 	write_sysreg(ctxt_sys_reg(ctxt, TPIDRRO_EL0),	tpidrro_el0);
 }
 
+/*
+ * vcpu_load() or kvm_sched_in()
+ * -> kvm_arch_vcpu_load()
+ *    -> kvm_vcpu_load_sysregs_vhe()
+ *       -> __sysreg32_restore_state(vcpu);
+ *       -> __sysreg_restore_user_state(guest_ctxt);
+ *       -> __sysreg_restore_el1_state(guest_ctxt);
+ *          -> write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1), vmpidr_el2);
+ *          -> write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR0_EL1), SYS_TTBR0);
+ *          -> write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR1_EL1), SYS_TTBR1);
+ *
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|96| <<kvm_vcpu_load_sysregs_vhe>> __sysreg_restore_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 {
 	write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1),	vmpidr_el2);
diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 9d7034412..2f298b14d 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
@@ -407,6 +407,10 @@ static bool range_is_memory(u64 start, u64 end)
 	return is_in_mem_range(end - 1, &r);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|477| <<host_stage2_idmap_locked>> return host_stage2_try(__host_stage2_idmap, addr, addr + size, prot);
+ */
 static inline int __host_stage2_idmap(u64 start, u64 end,
 				      enum kvm_pgtable_prot prot)
 {
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index f7a93ef29..cf8e9c779 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -990,6 +990,15 @@ static int stage2_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|413| <<__host_stage2_idmap>> return kvm_pgtable_stage2_map(&host_mmu.pgt, start, end - start, start,
+ *   - arch/arm64/kvm/mmu.c|1026| <<kvm_phys_addr_ioremap>> ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
+ *   - arch/arm64/kvm/mmu.c|1530| <<user_mem_abort>> ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
+ *   - arch/arm64/kvm/mmu.c|1754| <<kvm_set_spte_gfn>> kvm_pgtable_stage2_map(kvm->arch.mmu.pgt, range->start << PAGE_SHIFT,
+ *
+ * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
+ */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
 			   void *mc, enum kvm_pgtable_walk_flags flags)
@@ -1245,6 +1254,10 @@ bool kvm_pgtable_stage2_test_clear_young(struct kvm_pgtable *pgt, u64 addr,
 	return data.young;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1538| <<user_mem_abort>> ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
+ */
 int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
 				   enum kvm_pgtable_prot prot)
 {
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..085852df7 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -220,6 +220,22 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 
 		write_gicreg(cpu_if->vgic_hcr & ~ICH_HCR_EN, ICH_HCR_EL2);
 
+		/*
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct vgic_cpu vgic_cpu;
+		 *       -> struct vgic_v3_cpu_if vgic_v3;
+		 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+		 *
+		 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+		 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+		 */
 		for (i = 0; i < used_lrs; i++) {
 			if (elrsr & (1 << i))
 				cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
@@ -231,6 +247,49 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * The virtual GIC works by holding a prioritized list of pending virtual
+ * interrupts for each PE. In GICv3 this list is compiled in software and a
+ * number of the top entries are held in List registers in hardware. For LPIs,
+ * this list can be compiled using tables for each vPE. These tables are
+ * controlled by the GICR_* registers.
+ *
+ * A hypervisor uses a System register interface that is accessible at EL2 to
+ * switch context and to control multiple VMs. The context held in the ICH_*
+ * System registers is the context for the scheduled vPE. A vPE is scheduled
+ *
+ * when:
+ * x ICH_HCR_EL2.En == 1.
+ * x HCR_EL2.FMO == 1, when virtualizing Group 0 interrupts.
+ * x HCR_EL2.IMO == 1, when virtualizing Group 1 interrupts.
+ * x The PE is executing at EL1 and either:
+ *    - SCR_EL3.NS == 1.
+ *    — SCR_EL3.EEL2 == 1.
+ *
+ * When a vPE is scheduled, the ICH_*_EL2 registers affect software executing at EL1.
+ *
+ * The ICH_*_EL2 registers control and maintain a vPE as follows:
+ * x ICH_HCR_EL2 is used for the top-level configuration and control of virtual interrupts.
+ * x Information about the implementation, such as the size of the supported
+ * virtual INTIDs and the number of levels of prioritization is read from
+ * ICH_VTR_EL2.
+ * x A hypervisor can monitor and provide context for ICV_CTLR_EL1 using ICH_VMCR_EL2.
+ * x A set of List registers, ICH_LR<n>_EL2, are used by the hypervisor to
+ * forward a queue of pending interrupts to the PE, see Usage model for the
+ * List registers on page 6-155. The status of free locations in ICH_LR<n>_EL2
+ * is held in ICH_ELRSR_EL2.
+ * x The end of interrupt status for the List registers is held in ICH_EISR_EL2.
+ * x The VM maintenance interrupt status is held in ICH_MISR_EL2.
+ * x The active priority status is held in:
+ *   - ICH_AP0R<n>_EL2, where n = 0-3.
+ *   - ICH_AP1R<n>_EL2, where n = 0-3.
+ */
+
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|129| <<__hyp_vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic.c|969| <<vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ */
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
@@ -239,6 +298,25 @@ void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	if (used_lrs || cpu_if->its_vpe.its_vm) {
 		write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
 
+		/*
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct vgic_cpu vgic_cpu;
+		 *       -> struct vgic_v3_cpu_if vgic_v3;
+		 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+		 *
+		 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+		 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+		 *
+		 * ICH_LR<n>_EL2, Interrupt Controller List Registers
+		 * Provides interrupt context information for the virtual CPU interface.
+		 */
 		for (i = 0; i < used_lrs; i++)
 			__gic_v3_set_lr(cpu_if->vgic_lr[i], i);
 	}
diff --git a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
index b35a178e7..1b7e93028 100644
--- a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
@@ -25,6 +25,10 @@
  * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.
  */
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|170| <<__kvm_vcpu_run_vhe>> sysreg_save_host_state_vhe(host_ctxt);
+ */
 void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt)
 {
 	__sysreg_save_common_state(ctxt);
@@ -62,6 +66,10 @@ NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
  * and loading system register state early avoids having to load them on
  * every entry to the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|450| <<kvm_arch_vcpu_load>> kvm_vcpu_load_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/timer-sr.c b/arch/arm64/kvm/hyp/vhe/timer-sr.c
index 4cda674a8..51aa2d3bd 100644
--- a/arch/arm64/kvm/hyp/vhe/timer-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/timer-sr.c
@@ -6,6 +6,10 @@
 
 #include <asm/kvm_hyp.h>
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|826| <<set_cntvoff>> kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
+ */
 void __kvm_timer_set_cntvoff(u64 cntvoff)
 {
 	write_sysreg(cntvoff, cntvoff_el2);
diff --git a/arch/arm64/kvm/mmio.c b/arch/arm64/kvm/mmio.c
index 3dd38a151..d822016de 100644
--- a/arch/arm64/kvm/mmio.c
+++ b/arch/arm64/kvm/mmio.c
@@ -120,6 +120,10 @@ int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1699| <<kvm_handle_guest_abort>> ret = io_mem_abort(vcpu, fault_ipa);
+ */
 int io_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index d3b4feed4..632058cd3 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -267,6 +267,9 @@ static void clean_dcache_guest_page(void *va, size_t size)
 	__clean_dcache_guest_page(va, size);
 }
 
+/*
+ * struct kvm_pgtable_mm_ops kvm_s2_mm_ops.icache_inval_pou = invalidate_icache_guest_page()
+ */
 static void invalidate_icache_guest_page(void *va, size_t size)
 {
 	__invalidate_icache_guest_page(va, size);
@@ -603,6 +606,11 @@ int create_hyp_mappings(void *from, void *to, enum kvm_pgtable_prot prot)
  *
  * Return: 0 on success or negative error code on failure.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2302| <<init_hyp_mode>> err = hyp_alloc_private_va_range(PAGE_SIZE * 2, &hyp_addr);
+ *   - arch/arm64/kvm/mmu.c|659| <<__create_hyp_private_mapping>> ret = hyp_alloc_private_va_range(size, &addr);
+ */
 int hyp_alloc_private_va_range(size_t size, unsigned long *haddr)
 {
 	unsigned long base;
@@ -638,6 +646,11 @@ int hyp_alloc_private_va_range(size_t size, unsigned long *haddr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|697| <<create_hyp_io_mappings>> ret = __create_hyp_private_mapping(phys_addr, size,
+ *   - arch/arm64/kvm/mmu.c|724| <<create_hyp_exec_mappings>> ret = __create_hyp_private_mapping(phys_addr, size,
+ */
 static int __create_hyp_private_mapping(phys_addr_t phys_addr, size_t size,
 					unsigned long *haddr,
 					enum kvm_pgtable_prot prot)
@@ -999,6 +1012,10 @@ int topup_hyp_memcache(struct kvm_hyp_memcache *mc, unsigned long min_pages)
  * @size:	The size of the mapping
  * @writable:   Whether or not to create a writable mapping
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|316| <<vgic_v2_map_resources>> ret = kvm_phys_addr_ioremap(kvm, dist->vgic_cpu_base, kvm_vgic_global_state.vcpu_base, KVM_VGIC_V2_CPU_SIZE, true);
+ */
 int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
 			  phys_addr_t pa, unsigned long size, bool writable)
 {
@@ -1329,6 +1346,10 @@ static bool kvm_vma_mte_allowed(struct vm_area_struct *vma)
 	return vma->vm_flags & VM_MTE_ALLOWED;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1712| <<kvm_handle_guest_abort>> ret = user_mem_abort(vcpu, fault_ipa, memslot, hva, fault_status);
+ */
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  struct kvm_memory_slot *memslot, unsigned long hva,
 			  unsigned long fault_status)
@@ -1519,6 +1540,15 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	else if (cpus_have_const_cap(ARM64_HAS_CACHE_DIC))
 		prot |= KVM_PGTABLE_PROT_X;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|413| <<__host_stage2_idmap>> return kvm_pgtable_stage2_map(&host_mmu.pgt, start, end - start, start,
+	 *   - arch/arm64/kvm/mmu.c|1026| <<kvm_phys_addr_ioremap>> ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
+	 *   - arch/arm64/kvm/mmu.c|1530| <<user_mem_abort>> ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
+	 *   - arch/arm64/kvm/mmu.c|1754| <<kvm_set_spte_gfn>> kvm_pgtable_stage2_map(kvm->arch.mmu.pgt, range->start << PAGE_SHIFT,
+	 *
+	 * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
+	 */
 	/*
 	 * Under the premise of getting a FSC_PERM fault, we just need to relax
 	 * permissions only if vma_pagesize equals fault_granule. Otherwise,
@@ -1574,6 +1604,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * 在以下使用kvm_handle_guest_abort():
+ *   - arch/arm64/kvm/handle_exit.c|245| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|246| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long fault_status;
@@ -1719,6 +1754,10 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	return false;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|687| <<kvm_change_spte_gfn>> return kvm_set_spte_gfn(kvm, range);
+ */
 bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	kvm_pfn_t pfn = pte_pfn(range->pte);
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 560650972..9efe94e7a 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -501,6 +501,17 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 					  ARMV8_PMUV3_PERFCTR_CHAIN);
 
 	if (kvm_pmu_overflow_status(vcpu)) {
+		/*
+		 * 在以下使用KVM_REQ_IRQ_PENDING:
+		 *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 */
 		kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 
 		if (!in_nmi())
diff --git a/arch/arm64/kvm/pvtime.c b/arch/arm64/kvm/pvtime.c
index 4ceabaa4c..df44b075e 100644
--- a/arch/arm64/kvm/pvtime.c
+++ b/arch/arm64/kvm/pvtime.c
@@ -10,6 +10,14 @@
 
 #include <kvm/arm_hypercalls.h>
 
+/*
+ * 在以下使用KVM_REQ_RECORD_STEAL:
+ *   - arch/arm64/kvm/arm.c|454| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_RECORD_STEAL, vcpu);
+ *   - arch/arm64/kvm/arm.c|791| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RECORD_STEAL, vcpu))
+ *
+ * 处理KVM_REQ_RECORD_STEAL:
+ *   - arch/arm64/kvm/arm.c|792| <<check_vcpu_requests>> kvm_update_stolen_time(vcpu);
+ */
 void kvm_update_stolen_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -32,6 +40,10 @@ void kvm_update_stolen_time(struct kvm_vcpu *vcpu)
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hypercalls.c|338| <<kvm_smccc_call_handler>> val[0] = kvm_hypercall_pv_features(vcpu);
+ */
 long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu)
 {
 	u32 feature = smccc_get_arg1(vcpu);
@@ -48,6 +60,10 @@ long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hypercalls.c|341| <<kvm_smccc_call_handler>> gpa = kvm_init_stolen_time(vcpu);
+ */
 gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu)
 {
 	struct pvclock_vcpu_stolen_time init_values = {};
@@ -67,11 +83,22 @@ gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu)
 	return base;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|283| <<kvm_vm_ioctl_check_extension>> r = kvm_arm_pvtime_supported();
+ *   - arch/arm64/kvm/pvtime.c|84| <<kvm_arm_pvtime_set_attr>> if (!kvm_arm_pvtime_supported() ||
+ *   - arch/arm64/kvm/pvtime.c|113| <<kvm_arm_pvtime_get_attr>> if (!kvm_arm_pvtime_supported() ||
+ *   - arch/arm64/kvm/pvtime.c|129| <<kvm_arm_pvtime_has_attr>> if (kvm_arm_pvtime_supported())
+ */
 bool kvm_arm_pvtime_supported(void)
 {
 	return !!sched_info_on();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|973| <<kvm_arm_vcpu_arch_set_attr>> ret = kvm_arm_pvtime_set_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
@@ -104,6 +131,10 @@ int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|996| <<kvm_arm_vcpu_arch_get_attr>> ret = kvm_arm_pvtime_get_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
@@ -121,6 +152,10 @@ int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|1019| <<kvm_arm_vcpu_arch_has_attr>> ret = kvm_arm_pvtime_has_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_has_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 2ca2973ab..488bd8323 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -35,6 +35,17 @@
 
 #include "trace.h"
 
+/*
+ * 根据manual, GIC的寄存器似乎不支持trap.
+ *
+ * The GIC does not provide additional mechanisms for the virtualization of the
+ * GICD_*, GICR_*, and GITS_* registers. To virtualize VM accesses to these registers,
+ * the hypervisor must set stage 2 Data Aborts to those memory locations so
+ * that the hypervisor can emulate these effects. For more information about
+ * stage 2 Data Aborts, see Arm Architecture Reference Manual, Armv8,
+ * for Armv8-A architecture profile.
+ */
+
 /*
  * For AArch32, we only take care of what is being trapped. Anything
  * that has to do with init and userspace access has to go via the
@@ -295,6 +306,15 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
  * The cp15_64 code makes sure this automatically works
  * for both AArch64 and AArch32 accesses.
  */
+/*
+ * 在以下使用access_gic_sgi():
+ *   - arch/arm64/kvm/sys_regs.c|2156| <<global>> { SYS_DESC(SYS_ICC_SGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2157| <<global>> { SYS_DESC(SYS_ICC_ASGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2158| <<global>> { SYS_DESC(SYS_ICC_SGI0R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2749| <<global>> { Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2752| <<global>> { Op1( 1), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2753| <<global>> { Op1( 2), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ */
 static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
@@ -1164,6 +1184,19 @@ static unsigned int ptrauth_visibility(const struct kvm_vcpu *vcpu,
 	__PTRAUTH_KEY(k ## KEYLO_EL1),					\
 	__PTRAUTH_KEY(k ## KEYHI_EL1)
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ */
 static bool access_arch_timer(struct kvm_vcpu *vcpu,
 			      struct sys_reg_params *p,
 			      const struct sys_reg_desc *r)
@@ -1916,6 +1949,18 @@ static bool access_spsr(struct kvm_vcpu *vcpu,
  * This should be revisited if we ever encounter a more demanding
  * guest...
  */
+/*
+ * 在以下使用sys_reg_descs[]:
+ *   - arch/arm64/kvm/sys_regs.c|3130| <<emulate_sys_reg>> r = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|3183| <<kvm_reset_sys_regs>> for (i = 0; i < ARRAY_SIZE(sys_reg_descs); i++) {
+ *   - arch/arm64/kvm/sys_regs.c|3184| <<kvm_reset_sys_regs>> const struct sys_reg_desc *r = &sys_reg_descs[i];
+ *   - arch/arm64/kvm/sys_regs.c|3440| <<kvm_arm_sys_reg_get_reg>> sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|3484| <<kvm_arm_sys_reg_set_reg>> sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|3558| <<walk_sys_regs>> i2 = sys_reg_descs;
+ *   - arch/arm64/kvm/sys_regs.c|3559| <<walk_sys_regs>> end2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);
+ *   - arch/arm64/kvm/sys_regs.c|3603| <<kvm_sys_reg_table_init>> valid &= check_sysreg_table(sys_reg_descs, ARRAY_SIZE(sys_reg_descs), false);
+ *   - arch/arm64/kvm/sys_regs.c|3619| <<kvm_sys_reg_table_init>> first_idreg = find_reg(&params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ */
 static const struct sys_reg_desc sys_reg_descs[] = {
 	{ SYS_DESC(SYS_DC_ISW), access_dcsw },
 	{ SYS_DESC(SYS_DC_IGSW), access_dcgsw },
@@ -3435,6 +3480,10 @@ int kvm_sys_reg_set_user(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|803| <<kvm_arm_set_reg>> return kvm_arm_sys_reg_set_reg(vcpu, reg);
+ */
 int kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	void __user *uaddr = (void __user *)(unsigned long)reg->addr;
diff --git a/arch/arm64/kvm/vgic-sys-reg-v3.c b/arch/arm64/kvm/vgic-sys-reg-v3.c
index 9e7c486b4..499bd5544 100644
--- a/arch/arm64/kvm/vgic-sys-reg-v3.c
+++ b/arch/arm64/kvm/vgic-sys-reg-v3.c
@@ -10,6 +10,10 @@
 #include "vgic/vgic.h"
 #include "sys_regs.h"
 
+/*
+ * ICC_CTLR_EL1 controls aspects of the behavior of the GIC CPU interface and
+ * provides information about the features implemented.
+ */
 static int set_gic_ctlr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 			u64 val)
 {
@@ -297,6 +301,15 @@ static int get_gic_sre(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 	return 0;
 }
 
+/*
+ * 在以下使用gic_v3_icc_reg_descs[]:
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|344| <<vgic_v3_has_cpu_sysregs_attr>> if (get_reg_by_id(attr_to_id(attr->attr), gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|345| <<vgic_v3_has_cpu_sysregs_attr>> ARRAY_SIZE(gic_v3_icc_reg_descs)))
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|361| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_set_user(vcpu, &reg, gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|362| <<vgic_v3_cpu_sysregs_uaccess>> ARRAY_SIZE(gic_v3_icc_reg_descs));
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|364| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_get_user(vcpu, &reg, gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|365| <<vgic_v3_cpu_sysregs_uaccess>> ARRAY_SIZE(gic_v3_icc_reg_descs));
+ */
 static const struct sys_reg_desc gic_v3_icc_reg_descs[] = {
 	{ SYS_DESC(SYS_ICC_PMR_EL1),
 	  .set_user = set_gic_pmr, .get_user = get_gic_pmr, },
@@ -348,6 +361,10 @@ int vgic_v3_has_cpu_sysregs_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|566| <<vgic_v3_attr_regs_access>> ret = vgic_v3_cpu_sysregs_uaccess(vcpu, attr, is_write);
+ */
 int vgic_v3_cpu_sysregs_uaccess(struct kvm_vcpu *vcpu,
 				struct kvm_device_attr *attr,
 				bool is_write)
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index c8c3cb812..91fa38905 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -515,8 +515,33 @@ static irqreturn_t vgic_maintenance_handler(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
+/*
+ * 在以下使用gic_kvm_info:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|522| <<vgic_set_kvm_info>> BUG_ON(gic_kvm_info != NULL);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|523| <<vgic_set_kvm_info>> gic_kvm_info = kmalloc(sizeof(*info), GFP_KERNEL);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|524| <<vgic_set_kvm_info>> if (gic_kvm_info)
+ *   - arch/arm64/kvm/vgic/vgic-init.c|525| <<vgic_set_kvm_info>> *gic_kvm_info = *info;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|558| <<kvm_vgic_hyp_init>> if (!gic_kvm_info)
+ *   - arch/arm64/kvm/vgic/vgic-init.c|561| <<kvm_vgic_hyp_init>> has_mask = !gic_kvm_info->no_maint_irq_mask;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|563| <<kvm_vgic_hyp_init>> if (has_mask && !gic_kvm_info->maint_irq) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|572| <<kvm_vgic_hyp_init>> if (gic_kvm_info->no_hw_deactivation) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|578| <<kvm_vgic_hyp_init>> switch (gic_kvm_info->type) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|580| <<kvm_vgic_hyp_init>> ret = vgic_v2_probe(gic_kvm_info);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|592| <<kvm_vgic_hyp_init>> ret = vgic_v3_probe(gic_kvm_info);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|602| <<kvm_vgic_hyp_init>> kvm_vgic_global_state.maint_irq = gic_kvm_info->maint_irq;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|604| <<kvm_vgic_hyp_init>> kfree(gic_kvm_info);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|605| <<kvm_vgic_hyp_init>> gic_kvm_info = NULL;
+ */
 static struct gic_kvm_info *gic_kvm_info;
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-apple-aic.c|1063| <<aic_of_ic_init>> vgic_set_kvm_info(&vgic_info);
+ *   - drivers/irqchip/irq-gic-v3.c|2245| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+ *   - drivers/irqchip/irq-gic-v3.c|2587| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+ *   - drivers/irqchip/irq-gic.c|1466| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+ *   - drivers/irqchip/irq-gic.c|1627| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+ */
 void __init vgic_set_kvm_info(const struct gic_kvm_info *info)
 {
 	BUG_ON(gic_kvm_info != NULL);
@@ -580,6 +605,15 @@ int kvm_vgic_hyp_init(void)
 		ret = vgic_v2_probe(gic_kvm_info);
 		break;
 	case GIC_V3:
+		/*
+		 * 例子.
+		 * [   14.829157] kvm [1]: IPA Size Limit: 48 bits
+		 * [   14.833671] kvm [1]: GICv3: no GICV resource entry
+		 * [   14.838458] kvm [1]: disabling GICv2 emulation
+		 * [   14.842929] kvm [1]: GIC system register CPU interface enabled
+		 * [   14.848777] kvm [1]: vgic interrupt IRQ9
+		 * [   14.852918] kvm [1]: VHE mode initialized successfully
+		 */
 		ret = vgic_v3_probe(gic_kvm_info);
 		if (!ret) {
 			static_branch_enable(&kvm_vgic_global_state.gicv3_cpuif);
diff --git a/arch/arm64/kvm/vgic/vgic-irqfd.c b/arch/arm64/kvm/vgic/vgic-irqfd.c
index 475059bac..e0577c366 100644
--- a/arch/arm64/kvm/vgic/vgic-irqfd.c
+++ b/arch/arm64/kvm/vgic/vgic-irqfd.c
@@ -15,6 +15,11 @@
  *
  * This is the entry point for irqfd IRQ injection
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|46| <<kvm_set_routing_entry>> e->set = vgic_irqfd_set_irq;
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|142| <<kvm_arch_set_irq_inatomic>> return vgic_irqfd_set_irq(e, kvm, irq_source_id, 1, line_status);
+ */
 static int vgic_irqfd_set_irq(struct kvm_kernel_irq_routing_entry *e,
 			struct kvm *kvm, int irq_source_id,
 			int level, bool line_status)
@@ -66,6 +71,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+ */
 static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
 			     struct kvm_msi *msi)
 {
@@ -94,13 +104,25 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 	if (!level)
 		return -1;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+	 */
 	kvm_populate_msi(e, &msi);
+	/*
+	 * 只在这里
+	 */
 	return vgic_its_inject_msi(kvm, &msi);
 }
 
 /**
  * kvm_arch_set_irq_inatomic: fast-path for irqfd injection
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|215| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -115,7 +137,17 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (!vgic_has_its(kvm))
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+		 */
 		kvm_populate_msi(e, &msi);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-its.c|816| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+		 */
 		return vgic_its_inject_cached_translation(kvm, &msi);
 	}
 
diff --git a/arch/arm64/kvm/vgic/vgic-its.c b/arch/arm64/kvm/vgic/vgic-its.c
index 5fe2365a6..e5e1b0a27 100644
--- a/arch/arm64/kvm/vgic/vgic-its.c
+++ b/arch/arm64/kvm/vgic/vgic-its.c
@@ -223,6 +223,15 @@ static struct its_device *find_its_device(struct vgic_its *its, u32 device_id)
  * Device ID/Event ID pair on an ITS.
  * Must be called with the its_lock mutex held.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|721| <<vgic_its_resolve_lpi>> ite = find_ite(its, devid, eventid);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|946| <<vgic_its_cmd_handle_discard>> ite = find_ite(its, device_id, event_id);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|976| <<vgic_its_cmd_handle_movi>> ite = find_ite(its, device_id, event_id);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1198| <<vgic_its_cmd_handle_mapi>> if (find_ite(its, device_id, event_id))
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1393| <<vgic_its_cmd_handle_clear>> ite = find_ite(its, device_id, event_id);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1423| <<vgic_its_cmd_handle_inv>> ite = find_ite(its, device_id, event_id);
+ */
 static struct its_ite *find_ite(struct vgic_its *its, u32 device_id,
 				  u32 event_id)
 {
@@ -545,12 +554,36 @@ static unsigned long vgic_mmio_read_its_idregs(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|587| <<vgic_its_check_cache>> irq = __vgic_its_check_cache(dist, db, devid, eventid);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|617| <<vgic_its_cache_translation>> if (__vgic_its_check_cache(dist, db, devid, eventid))
+ */
 static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 					       phys_addr_t db,
 					       u32 devid, u32 eventid)
 {
 	struct vgic_translation_cache_entry *cte;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1929| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1943| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1955| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 *
+	 * struct kvm *kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct vgic_dist vgic;
+	 *       -> struct list_head lpi_translation_cache;
+	 */
 	list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
 		/*
 		 * If we hit a NULL entry, there is nothing after this
@@ -559,6 +592,9 @@ static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 		if (!cte->irq)
 			break;
 
+		/*
+		 * db, devid, eventid都要match
+		 */
 		if (cte->db != db || cte->devid != devid ||
 		    cte->eventid != eventid)
 			continue;
@@ -576,9 +612,19 @@ static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|818| <<vgic_its_inject_cached_translation>> irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
+ */
 static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 					     u32 devid, u32 eventid)
 {
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct vgic_dist vgic;
+	 *       -> struct list_head lpi_translation_cache;
+	 */
 	struct vgic_dist *dist = &kvm->arch.vgic;
 	struct vgic_irq *irq;
 	unsigned long flags;
@@ -590,6 +636,10 @@ static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 	return irq;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|737| <<vgic_its_resolve_lpi>> vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
+ */
 static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 				       u32 devid, u32 eventid,
 				       struct vgic_irq *irq)
@@ -605,6 +655,20 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 
 	raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1929| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1943| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1955| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	if (unlikely(list_empty(&dist->lpi_translation_cache)))
 		goto out;
 
@@ -666,6 +730,12 @@ void vgic_its_invalidate_cache(struct kvm *kvm)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|795| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|434| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|506| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ */
 int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 			 u32 devid, u32 eventid, struct vgic_irq **irq)
 {
@@ -679,6 +749,11 @@ int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 	if (!ite || !its_is_collection_mapped(ite->collection))
 		return E_ITS_INT_UNMAPPED_INTERRUPT;
 
+	/*
+	 * struct its_ite *ite;
+	 * -> struct its_collection *collection;
+	 *    -> u32 target_addr;
+	 */
 	vcpu = kvm_get_vcpu(kvm, ite->collection->target_addr);
 	if (!vcpu)
 		return E_ITS_INT_UNMAPPED_INTERRUPT;
@@ -686,12 +761,20 @@ int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 	if (!vgic_lpis_enabled(vcpu))
 		return -EBUSY;
 
+	/*
+	 * 只在这里调用
+	 */
 	vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
 
 	*irq = ite->irq;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|861| <<vgic_its_inject_msi>> its = vgic_msi_to_its(kvm, msi);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|408| <<vgic_get_its>> return vgic_msi_to_its(kvm, &msi);
+ */
 struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
 {
 	u64 address;
@@ -727,6 +810,22 @@ struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
  * Returns 0 on success, a positive error value for any ITS mapping
  * related errors and negative error values for generic errors.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|789| <<vgic_its_inject_msi>> ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1447| <<vgic_its_cmd_handle_int>> return vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);
+ *
+ * The configuration of LPIs is very different to the other interrupt types,
+ * and involves the following:
+ * - Redistributors
+ * - ITSs (Interrupt Translation Service)
+ *
+ * LPIs are always message–based interrupts, and they can be supported by an
+ * ITS. An ITS is responsible for receiving interrupts from peripherals and
+ * forwarding them to the appropriate Redistributor as LPIs. A system might
+ * include more than one ITS, in which case each ITS must be configured
+ * individually.
+ */
 static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 				u32 devid, u32 eventid)
 {
@@ -749,12 +848,27 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|816| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+ */
 int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_irq *irq;
 	unsigned long flags;
 	phys_addr_t db;
 
+	/*
+	 * struct kvm_msi {
+	 *     __u32 address_lo;
+	 *     __u32 address_hi;
+	 *     __u32 data;
+	 *     __u32 flags;
+	 *     __u32 devid;
+	 *     __u8  pad[12];
+	 * };
+	 */
 	db = (u64)msi->address_hi << 32 | msi->address_lo;
 	irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
 	if (!irq)
@@ -773,6 +887,20 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
  * We then call vgic_its_trigger_msi() with the decoded data.
  * According to the KVM_SIGNAL_MSI API description returns 1 on success.
  */
+/*
+ * generic不分架构
+ * struct kvm_msi {
+ *     __u32 address_lo;
+ *     __u32 address_hi;
+ *     __u32 data;
+ *     __u32 flags;
+ *     __u32 devid;
+ *     __u8  pad[12];
+ * };
+ *
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|98| <<kvm_set_msi>> return vgic_its_inject_msi(kvm, &msi);
+ */
 int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_its *its;
@@ -781,11 +909,17 @@ int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 	if (!vgic_its_inject_cached_translation(kvm, msi))
 		return 1;
 
+	/*
+	 * 用address寻找设备和its
+	 */
 	its = vgic_msi_to_its(kvm, msi);
 	if (IS_ERR(its))
 		return PTR_ERR(its);
 
 	mutex_lock(&its->its_lock);
+	/*
+	 * 用data转换成中断
+	 */
 	ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
 	mutex_unlock(&its->its_lock);
 
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index 188d2187e..36eeeb17d 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -1066,6 +1066,10 @@ static int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)
  * check for matching ones. If this bit is set, we signal all, but not the
  * calling VCPU.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|338| <<access_gic_sgi>> vgic_v3_dispatch_sgi(vcpu, p->regval, g1);
+ */
 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/arm64/kvm/vgic/vgic-v3.c b/arch/arm64/kvm/vgic/vgic-v3.c
index 3dfc8b84e..e1ab1125e 100644
--- a/arch/arm64/kvm/vgic/vgic-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-v3.c
@@ -32,6 +32,10 @@ static bool lr_signals_eoi_mi(u64 lr_val)
 	       !(lr_val & ICH_LR_HW);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|899| <<vgic_fold_lr_state>> vgic_v3_fold_lr_state(vcpu);
+ */
 void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -103,6 +107,16 @@ void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 	cpuif->used_lrs = 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|819| <<vgic_populate_lr>> vgic_v3_populate_lr(vcpu, irq, lr);
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> kvm_vgic_flush_hwstate()
+ *    -> vgic_flush_lr_state()
+ *       -> vgic_populate_lr()
+ *          -> vgic_v3_populate_lr()
+ */
 /* Requires the irq to be locked already */
 void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 {
@@ -183,6 +197,22 @@ void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 
 	val |= (u64)irq->priority << ICH_LR_PRIORITY_SHIFT;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct vgic_cpu vgic_cpu;
+	 *       -> struct vgic_v3_cpu_if vgic_v3;
+	 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+	 *
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
 }
 
@@ -629,6 +659,10 @@ static bool vgic_v3_broken_seis(void)
  * Returns 0 if the VGICv3 has been probed successfully, returns an error code
  * otherwise
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|592| <<kvm_vgic_hyp_init>> ret = vgic_v3_probe(gic_kvm_info);
+ */
 int vgic_v3_probe(const struct gic_kvm_info *info)
 {
 	u64 ich_vtr_el2 = kvm_call_hyp_ret(__vgic_v3_get_gic_config);
@@ -657,6 +691,11 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 
 	kvm_vgic_global_state.vcpu_base = 0;
 
+	/*
+	 * struct gic_kvm_info *info:
+	 * -> struct resource vcpu;
+	 *    -> resource_size_t start;
+	 */
 	if (!info->vcpu.start) {
 		kvm_info("GICv3: no GICV resource entry\n");
 	} else if (!has_v2) {
@@ -717,6 +756,10 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1127| <<kvm_vgic_load>> vgic_v3_load(vcpu);
+ */
 void vgic_v3_load(struct kvm_vcpu *vcpu)
 {
 	struct vgic_v3_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v3;
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index 8be4c1ebd..11f548493 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -17,6 +17,74 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * SPI
+ * PPI
+ * SGI
+ * LPI
+ *
+ * GIC(Generic Interrupt Controller)是ARM公司提供的中断控制器统一架构,它详细定义了ARM平台上,
+ * 中断控制器的内部分发逻辑(Distributor)和CPU接口(CPU Interface).
+ *
+ * 目前GICv3引入了对ITS(Interrupt Translation Service)的支持,而GICv4则引入了
+ * LPIs(Locality-specific Peripheral Interrupts)中断的透传功能.
+ *
+ * 从GICv2开始,支持中断虚拟化硬件扩展.vGIC为每个CPU引入了一个vGIC CPU接口和相应的hypervisor控制接口,
+ * 虚拟机可以被配置成直接使用vGIC CPU接口.当非安全物理中断发送给某个CPU CORE后,可以触发该CPU CORE
+ * 陷入到EL2模式,并允许VMM中的中断处理程序,中断处理程序会根据该物理中断对应的VMID等信息,
+ * 通过List Registers,配置并发送虚拟中断到vGIC CPU接口,并路由到相应的vCPU CORE上.
+ */
+/*
+ * https://www.zhihu.com/column/c_1520029500636696576
+ *
+ * 1. 外设触发irq中断,gic的distributor接收到该中断后,通过distributor的仲裁器确定其应该被发送的cpu.
+ *
+ * 2. 在target cpu确定以后,将中断发送给该cpu相关的redistributor
+ *
+ * 3. redistributor将中断发送到其对应的cpu interface上
+ *
+ * 4. 该中断被路由到EL2的hypervisor中断处理入口
+ *
+ * 5. hypervisor退出到host,然后重新使能中断,使中断再次触发,并由host中断处理函数执行中断处理
+ *
+ * 6. host通过写lr的方式向guest注入虚拟中断,并切入guest执行
+ *
+ * 7. 虚拟中断通过virq触发,并进入guest的中断入口函数
+ *
+ * 8. guest执行中断处理流程,并通过虚拟cpu interface执行中断应答以及EOI等操作
+ *
+ * 9. 若该虚拟中断与物理中断关联,则EOI操作将会作用到实际的物理中断上
+ *
+ * 10. 中断处理完成
+ *
+ *
+ * With GICv3, the GIC forwards a packet to the core describing the interrupt
+ * (ID, Group, Priority).  When, within the interrupt handler, software reads
+ * ICC_IARx_EL1 to acknowledge the interrupt, the core already has all the
+ * information it needs to return the interrupt ID (INTID).
+ *
+ * With GICv2, the equivalent step requires a read of an MMIO register.  That
+ * read has to get from the core to the GIC, and then back, before software knows
+ * the INTID.  How long will that take?  It depends on the specific system design
+ * and can also depend on how much contention there is on the memory system at
+ * the time.
+ *
+ * Another advantage is to software - in the interrupt handling path you don't
+ * need to know the address of the interrupt controller to perform the
+ * acknowledge.
+ *
+ * 在gicv2中,cpu interface的寄存器,是实现在gic内部的,因此当core收到一个中断时,
+ * 会通过axi总线(假设memory总线是axi总线,去访问cpu interface的寄存器.而中断在
+ * 一个soc系统中,是会频繁的产生的,这就意味着,core会频繁的去访问gic的寄存器,这样
+ * 会占用axi总线的带宽,总而会影响中断的实时响应.而且core通过axi总线去访问cpu
+ * interface寄存器,延迟,也比较大.
+ *
+ * 在gicv3中,将cpu interface从gic中抽离出来,实现在core内部,而不实现在gic中.
+ * core对cpu interface的访问,通过系统寄存器方式访问,也就是使用msr,mrs访问,那么core对
+ * cpu interface的寄存器访问,就加速了,而且还不占用axi总线带宽.这样core对中断的处理,
+ * 就加速了。
+ */
+
 struct vgic_global kvm_vgic_global_state __ro_after_init = {
 	.gicv3_cpuif = STATIC_KEY_FALSE_INIT,
 };
@@ -149,6 +217,11 @@ void vgic_put_irq(struct kvm *kvm, struct vgic_irq *irq)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|379| <<kvm_vgic_vcpu_destroy>> vgic_flush_pending_lpis(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|279| <<vgic_mmio_write_v3r_ctlr>> vgic_flush_pending_lpis(vcpu);
+ */
 void vgic_flush_pending_lpis(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -172,6 +245,18 @@ void vgic_flush_pending_lpis(struct kvm_vcpu *vcpu)
 
 void vgic_irq_set_phys_pending(struct vgic_irq *irq, bool pending)
 {
+	/*
+	 * irq_set_irqchip_state - set the state of a forwarded interrupt.
+	 * @irq: Interrupt line that is forwarded to a VM
+	 * @which: State to be restored (one of IRQCHIP_STATE_*)
+	 * @val: Value corresponding to @which
+	 *
+	 * This call sets the internal irqchip state of an interrupt,
+	 * depending on the value of @which.
+	 *
+	 * This function should be called with migration disabled if the
+	 * interrupt controller has per-cpu registers.
+	 */
 	WARN_ON(irq_set_irqchip_state(irq->host_irq,
 				      IRQCHIP_STATE_PENDING,
 				      pending));
@@ -213,6 +298,14 @@ void vgic_irq_set_phys_active(struct vgic_irq *irq, bool active)
  *
  * Requires the IRQ lock to be held.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|407| <<vgic_queue_irq_unlock>> vcpu = vgic_target_oracle(irq);
+ *   - arch/arm64/kvm/vgic/vgic.c|459| <<vgic_queue_irq_unlock>> if (unlikely(irq->vcpu || vcpu != vgic_target_oracle(irq))) {
+ *   - arch/arm64/kvm/vgic/vgic.c|742| <<vgic_prune_ap_list>> target_vcpu = vgic_target_oracle(irq);
+ *   - arch/arm64/kvm/vgic/vgic.c|801| <<vgic_prune_ap_list>> if (target_vcpu == vgic_target_oracle(irq)) {
+ *   - arch/arm64/kvm/vgic/vgic.c|928| <<vgic_flush_lr_state>> if (likely(vgic_target_oracle(irq) == vcpu)) {
+ */
 static struct kvm_vcpu *vgic_target_oracle(struct vgic_irq *irq)
 {
 	lockdep_assert_held(&irq->irq_lock);
@@ -333,6 +426,27 @@ static bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owne
  * Needs to be entered with the IRQ lock already held, but will return
  * with all locks dropped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|301| <<update_lpi_config>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|467| <<its_sync_lpi_pending_table>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|747| <<vgic_its_trigger_msi>> vgic_queue_irq_unlock(kvm, irq, flags); 
+ *   - arch/arm64/kvm/vgic/vgic-its.c|765| <<vgic_its_inject_cached_translation>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|157| <<vgic_mmio_write_sgir>> vgic_queue_irq_unlock(source_vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|264| <<vgic_mmio_write_sgipends>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|375| <<vgic_v3_uaccess_write_pending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1127| <<vgic_v3_dispatch_sgi>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|85| <<vgic_mmio_write_group>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|159| <<vgic_mmio_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|200| <<vgic_uaccess_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|341| <<vgic_mmio_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|368| <<vgic_uaccess_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|603| <<vgic_mmio_change_active>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|828| <<vgic_write_irq_line_level_info>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|358| <<vgic_v3_lpi_sync_pending_status>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|476| <<kvm_vgic_v4_set_forwarding>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *    - arch/arm64/kvm/vgic/vgic.c|552| <<kvm_vgic_inject_irq>> vgic_queue_irq_unlock(kvm, irq, flags);
+ */
 bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 			   unsigned long flags)
 {
@@ -402,6 +516,26 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 		goto retry;
 	}
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *
+	 * kvm_arch_vcpu_ioctl_run()
+	 */
+
 	/*
 	 * Grab a reference to the irq to reflect the fact that it is
 	 * now in the ap_list.
@@ -413,6 +547,17 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 	raw_spin_unlock(&irq->irq_lock);
 	raw_spin_unlock_irqrestore(&vcpu->arch.vgic_cpu.ap_list_lock, flags);
 
+	/*
+	 * 在以下使用KVM_REQ_IRQ_PENDING:
+	 *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 */
 	kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 	kvm_vcpu_kick(vcpu);
 
@@ -436,6 +581,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|497| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/arm.c|1172| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1180| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|346| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 			bool level, void *owner)
 {
@@ -454,6 +607,12 @@ int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 	if (!vcpu && intid < VGIC_NR_PRIVATE_IRQS)
 		return -EINVAL;
 
+	/*
+	 * 注释
+	 * This looks up the virtual interrupt ID to get the corresponding
+	 * struct vgic_irq. It also increases the refcount, so any caller is expected
+	 * to call vgic_put_irq() once it's finished with this IRQ.
+	 */
 	irq = vgic_get_irq(kvm, vcpu, intid);
 	if (!irq)
 		return -EINVAL;
@@ -513,6 +672,13 @@ static inline void kvm_vgic_unmap_irq(struct vgic_irq *irq)
 	irq->ops = NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|797| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq, timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|802| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq, timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1621| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1629| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+ */
 int kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, unsigned int host_irq,
 			  u32 vintid, struct irq_ops *ops)
 {
@@ -539,6 +705,11 @@ int kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, unsigned int host_irq,
  * subsystems injecting mapped interrupts should reset their interrupt lines
  * when we are doing a reset of the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1402| <<kvm_timer_vcpu_reset>> kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_vtimer));
+ *   - arch/arm64/kvm/arch_timer.c|1404| <<kvm_timer_vcpu_reset>> kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_ptimer));
+ */
 void kvm_vgic_reset_mapped_irq(struct kvm_vcpu *vcpu, u32 vintid)
 {
 	struct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, vintid);
@@ -742,6 +913,10 @@ static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 }
 
 /* Requires the irq_lock to be held. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|903| <<vgic_flush_lr_state>> vgic_populate_lr(vcpu, irq, count++);
+ */
 static inline void vgic_populate_lr(struct kvm_vcpu *vcpu,
 				    struct vgic_irq *irq, int lr)
 {
@@ -795,6 +970,10 @@ static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1004| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+ */
 /* Requires the VCPU's ap_list_lock to be held. */
 static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
@@ -855,6 +1034,11 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 		vcpu->arch.vgic_cpu.vgic_v3.used_lrs = count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|956| <<kvm_vgic_sync_hwstate>> if (can_access_vgic_from_kernel())
+ *   - arch/arm64/kvm/vgic/vgic.c|1008| <<kvm_vgic_flush_hwstate>> if (can_access_vgic_from_kernel())
+ */
 static inline bool can_access_vgic_from_kernel(void)
 {
 	/*
@@ -895,6 +1079,10 @@ void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 	vgic_prune_ap_list(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1009| <<kvm_vgic_flush_hwstate>> vgic_restore_state(vcpu);
+ */
 static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 {
 	if (!static_branch_unlikely(&kvm_vgic_global_state.gicv3_cpuif))
@@ -903,6 +1091,10 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|952| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ */
 /* Flush our emulation state into the GIC hardware before entering the guest. */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -924,6 +1116,23 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 
 	DEBUG_SPINLOCK_BUG_ON(!irqs_disabled());
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
 		raw_spin_lock(&vcpu->arch.vgic_cpu.ap_list_lock);
 		vgic_flush_lr_state(vcpu);
@@ -937,6 +1146,10 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 		vgic_v4_commit(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|447| <<kvm_arch_vcpu_load>> kvm_vgic_load(vcpu);
+ */
 void kvm_vgic_load(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!vgic_initialized(vcpu->kvm)))
@@ -1015,6 +1228,17 @@ void vgic_kick_vcpus(struct kvm *kvm)
 	 */
 	kvm_for_each_vcpu(c, vcpu, kvm) {
 		if (kvm_vgic_vcpu_pending_irq(vcpu)) {
+			/*
+			 * 在以下使用KVM_REQ_IRQ_PENDING:
+			 *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 */
 			kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 			kvm_vcpu_kick(vcpu);
 		}
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 9d248703c..d1f235579 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1716,6 +1716,12 @@ int x86_pmu_handle_irq(struct pt_regs *regs)
 	return handled;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1352| <<x86_pmu_enable>> perf_events_lapic_init();
+ *   - arch/x86/events/core.c|2103| <<init_hw_perf_events>> perf_events_lapic_init();
+ *   - arch/x86/kernel/apic/apic.c|1685| <<setup_local_APIC>> perf_events_lapic_init();
+ */
 void perf_events_lapic_init(void)
 {
 	if (!x86_pmu.apic || !x86_pmu_initialized())
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3bc146dfd..60513845c 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -74,22 +74,81 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * kvm_guest_time_update()
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+ *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *
+ * 处理函数是kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+ *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理函数是kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
 #endif
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+ *   - kvm_update_masterclock()
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * kvm_gen_kvmclock_update()
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -493,8 +552,43 @@ struct kvm_pmc {
 	bool is_paused;
 	bool intr;
 	u64 counter;
+	/*
+	 * 在以下使用kvm_pmc->prev_counter:
+	 *   - arch/x86/kvm/pmu.c|471| <<reprogram_counter>> if (pmc->counter < pmc->prev_counter)
+	 *   - arch/x86/kvm/pmu.c|511| <<reprogram_counter>> pmc->prev_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|800| <<kvm_pmu_incr_counter>> pmc->prev_counter = pmc->counter;
+	 *   - arch/x86/kvm/svm/pmu.c|245| <<amd_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|614| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|621| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = 0;
+	 */
 	u64 prev_counter;
 	u64 eventsel;
+	/*
+	 * 在以下设置kvm_pmc->perf_event:
+	 *   - arch/x86/kvm/pmu.c|318| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|81| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 * 在以下使用kvm_pmc->perf_event:
+	 *   -  arch/x86/kvm/pmu.c|166| <<__kvm_perf_overflow>> if (pmc->perf_event && pmc->perf_event->attr.precise_ip) {
+	 *   - arch/x86/kvm/pmu.c|222| <<kvm_perf_overflow>> struct kvm_pmc *pmc = perf_event->overflow_handler_context;
+	 *   - arch/x86/kvm/pmu.c|333| <<pmc_pause_counter>> if (!pmc->perf_event || pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.c|337| <<pmc_pause_counter>> counter += perf_event_pause(pmc->perf_event, true);
+	 *   - arch/x86/kvm/pmu.c|348| <<pmc_resume_counter>> if (!pmc->perf_event)
+	 *   - arch/x86/kvm/pmu.c|352| <<pmc_resume_counter>> if (is_sampling_event(pmc->perf_event) &&
+	 *   - arch/x86/kvm/pmu.c|353| <<pmc_resume_counter>> perf_event_period(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.c|358| <<pmc_resume_counter>> (!!pmc->perf_event->attr.precise_ip))
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+	 *   - arch/x86/kvm/pmu.c|827| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+	 *   - arch/x86/kvm/pmu.h|70| <<pmc_read_counter>> if (pmc->perf_event && !pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.h|79| <<pmc_release_perf_event>> if (pmc->perf_event) {
+	 *   - arch/x86/kvm/pmu.h|80| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+	 *   - arch/x86/kvm/pmu.h|89| <<pmc_stop_counter>> if (pmc->perf_event) {
+	 *   - arch/x86/kvm/pmu.h|154| <<pmc_update_sample_period>> if (!pmc->perf_event || pmc->is_paused ||
+	 *   - arch/x86/kvm/pmu.h|155| <<pmc_update_sample_period>> !is_sampling_event(pmc->perf_event))
+	 *   - arch/x86/kvm/pmu.h|158| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|756| <<intel_pmu_cross_mapped_check>> !pmc_is_globally_enabled(pmc) || !pmc->perf_event)
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|763| <<intel_pmu_cross_mapped_check>> hw_idx = pmc->perf_event->hw.idx;
+	 */
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
 	/*
@@ -528,6 +622,12 @@ struct kvm_pmu {
 	u64 raw_event_mask;
 	struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+	/*
+	 * 在以下使用kvm_pmu->irq_work:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+	 *   - arch/x86/kvm/pmu.c|737| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+	 *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+	 */
 	struct irq_work irq_work;
 
 	/*
@@ -536,7 +636,23 @@ struct kvm_pmu {
 	 * filter changes.
 	 */
 	union {
+		/*
+		 * 在以下使用kvm_pmu->reprogram_pmi:
+		 *   - arch/x86/include/asm/kvm_host.h|539| <<global>> DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+		 *   - arch/x86/kvm/pmu.c|155| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+		 *   - arch/x86/kvm/pmu.c|455| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|464| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+		 *   - arch/x86/kvm/pmu.c|468| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|909| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+		 *   - arch/x86/kvm/pmu.h|219| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.h|231| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+		 */
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+		/*
+		 * 在以下使用kvm_pmu->_reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|910| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+		 *   - arch/x86/kvm/pmu.c|913| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+		 */
 		atomic64_t __reprogram_pmi;
 	};
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
@@ -863,6 +979,12 @@ struct kvm_vcpu_arch {
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2521| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3563| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5305| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
@@ -878,6 +1000,13 @@ struct kvm_vcpu_arch {
 	unsigned int nmi_pending;
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 	bool smi_pending;    /* SMI queued after currently running handler */
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1861| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|441| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|446| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|451| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	u8 handling_intr_from_guest;
 
 	struct kvm_mtrr mtrr_state;
@@ -1287,6 +1416,15 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 
 	/*
@@ -1306,11 +1444,46 @@ struct kvm_arch {
 
 	u32 default_tsc_khz;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|3077| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3094| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3201| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3203| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3301| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3314| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|12741| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_t pvclock_sc;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3218| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched && !ka->backwards_tsc_observed && !ka->boot_vcpu_runs_old_kvmclock;
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3033| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3157| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3279| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6953| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|12539| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
diff --git a/arch/x86/include/asm/x86_init.h b/arch/x86/include/asm/x86_init.h
index 5240d88db..14bd5da92 100644
--- a/arch/x86/include/asm/x86_init.h
+++ b/arch/x86/include/asm/x86_init.h
@@ -298,6 +298,15 @@ struct x86_hyper_runtime {
  */
 struct x86_platform_ops {
 	unsigned long (*calibrate_cpu)(void);
+	/*
+	 * 几个calibrate_tsc():
+	 *   - arch/x86/kernel/x86_init.c|142| <<global>> .calibrate_tsc = native_calibrate_tsc,
+	 *   - arch/x86/kernel/cpu/acrn.c|32| <<acrn_init_platform>> x86_platform.calibrate_tsc = acrn_get_tsc_khz;
+	 *   - arch/x86/kernel/cpu/mshyperv.c|390| <<ms_hyperv_init_platform>> x86_platform.calibrate_tsc = hv_get_tsc_khz;
+	 *   - arch/x86/kernel/cpu/vmware.c|411| <<vmware_platform_setup>> x86_platform.calibrate_tsc = vmware_get_tsc_khz;
+	 *   - arch/x86/kernel/jailhouse.c|212| <<jailhouse_init_platform>> x86_platform.calibrate_tsc = jailhouse_get_tsc;
+	 *   - arch/x86/xen/time.c|569| <<xen_init_time_common>> x86_platform.calibrate_tsc = xen_tsc_khz;
+	 */
 	unsigned long (*calibrate_tsc)(void);
 	void (*get_wallclock)(struct timespec64 *ts);
 	int (*set_wallclock)(const struct timespec64 *ts);
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index 1a6a1f987..58aa1920f 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -318,6 +318,21 @@ struct kvm_reinject_control {
 	__u8 reserved[31];
 };
 
+/*
+ * 在QEMU设置KVM_VCPUEVENT_VALID_SMM的地方
+ *
+ * 4426 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
+ * 4427 {
+ * ... ...
+ * 4474          * Stop SMI delivery on old machine types to avoid a reboot
+ * 4475          * on an inward migration of an old VM.
+ * 4476          *
+ * 4477         if (!cpu->kvm_no_smi_migration) {
+ * 4478             events.flags |= KVM_VCPUEVENT_VALID_SMM;
+ * 4479         }
+ * 4480     }
+ */
+
 /* When set in flags, include corresponding fields on KVM_SET_VCPU_EVENTS */
 #define KVM_VCPUEVENT_VALID_NMI_PENDING	0x00000001
 #define KVM_VCPUEVENT_VALID_SIPI_VECTOR	0x00000002
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index af49e24b4..4a7ed7dae 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -482,6 +482,10 @@ static int lapic_next_event(unsigned long delta,
 	return 0;
 }
 
+/*
+ * 在以下使用lapic_next_deadline():
+ *   - arch/x86/kernel/apic/apic.c|642| <<setup_APIC_timer>> levt->set_next_event = lapic_next_deadline;
+ */
 static int lapic_next_deadline(unsigned long delta,
 			       struct clock_event_device *evt)
 {
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index fb8f52149..db9df46a2 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -60,6 +60,15 @@ EXPORT_PER_CPU_SYMBOL_GPL(hv_clock_per_cpu);
  */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
+	/*
+	 * static struct pvclock_wall_clock wall_clock __bss_decrypted;
+	 *
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|28| <<global>> static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|296| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|307| <<kvmclock_init>> msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
 	preempt_disable();
 	pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index ac10b46c5..79c707348 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -112,6 +112,13 @@ static u64 native_steal_clock(int cpu)
 DEFINE_STATIC_CALL(pv_steal_clock, native_steal_clock);
 DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+ *   - arch/x86/kernel/kvmclock.c|108| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ *   - arch/x86/xen/time.c|567| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+ *   - drivers/clocksource/hyperv_timer.c|514| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+ */
 void paravirt_set_sched_clock(u64 (*func)(void))
 {
 	static_call_update(pv_sched_clock, func);
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index b3f81379c..bbe6a9bfe 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -64,6 +64,11 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/pvclock.c|115| <<pvclock_clocksource_read>> return __pvclock_clocksource_read(src, true);
+ *   - arch/x86/kernel/pvclock.c|120| <<pvclock_clocksource_read_nowd>> return __pvclock_clocksource_read(src, false);
+ */
 static __always_inline
 u64 __pvclock_clocksource_read(struct pvclock_vcpu_time_info *src, bool dowd)
 {
@@ -120,6 +125,11 @@ noinstr u64 pvclock_clocksource_read_nowd(struct pvclock_vcpu_time_info *src)
 	return __pvclock_clocksource_read(src, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|65| <<kvm_get_wallclock>> pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
+ *   - arch/x86/xen/time.c|83| <<xen_read_wallclock>> pvclock_read_wallclock(wall_clock, vcpu_time, ts);
+ */
 void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 			    struct pvclock_vcpu_time_info *vcpu_time,
 			    struct timespec64 *ts)
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e1aa2cd77..1298b687c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1129,6 +1129,10 @@ int native_kick_ap(unsigned int cpu, struct task_struct *tidle)
 	return err;
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|785| <<cpuhp_kick_ap_alive>> return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));
+ */
 int arch_cpuhp_kick_ap_alive(unsigned int cpu, struct task_struct *tidle)
 {
 	return smp_ops.kick_ap_alive(cpu, tidle);
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3425c6a94..bd2d7a915 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -33,6 +33,18 @@
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
 
+/*
+ * 在以下修改tsc_khz:
+ *   - arch/x86/hyperv/hv_init.c|148| <<hyperv_stop_tsc_emulation>> tsc_khz = div64_u64(freq, 1000);
+ *   - arch/x86/kernel/tsc.c|942| <<recalibrate_cpu_khz>> tsc_khz = x86_platform.calibrate_tsc();
+ *   - arch/x86/kernel/tsc.c|944| <<recalibrate_cpu_khz>> tsc_khz = cpu_khz;
+ *   - arch/x86/kernel/tsc.c|1039| <<time_cpufreq_notifier>> tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);
+ *   - arch/x86/kernel/tsc.c|1441| <<tsc_refine_calibration_work>> tsc_khz = freq;
+ *   - arch/x86/kernel/tsc.c|1509| <<determine_cpu_tsc_frequencies>> tsc_khz = tsc_early_khz;
+ *   - arch/x86/kernel/tsc.c|1511| <<determine_cpu_tsc_frequencies>> tsc_khz = x86_platform.calibrate_tsc();
+ *   - arch/x86/kernel/tsc.c|1523| <<determine_cpu_tsc_frequencies>> if (tsc_khz == 0)
+ *   - arch/x86/kernel/tsc.c|1524| <<determine_cpu_tsc_frequencies>> tsc_khz = cpu_khz;
+ */
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
@@ -1489,6 +1501,9 @@ static int __init init_tsc_clocksource(void)
 			return 0;
 	}
 
+	/*
+	 * tsc_refine_calibration_work()
+	 */
 	schedule_delayed_work(&tsc_irqwork, 0);
 	return 0;
 }
@@ -1498,6 +1513,11 @@ static int __init init_tsc_clocksource(void)
  */
 device_initcall(init_tsc_clocksource);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1569| <<tsc_early_init>> if (!determine_cpu_tsc_frequencies(true))
+ *   - arch/x86/kernel/tsc.c|1590| <<tsc_init>> if (!determine_cpu_tsc_frequencies(false)) {
+ */
 static bool __init determine_cpu_tsc_frequencies(bool early)
 {
 	/* Make sure that cpu and tsc are not already calibrated */
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index b28fd0200..eb2cf8e19 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1385,6 +1385,18 @@ static int kvm_hv_set_msr_pw(struct kvm_vcpu *vcpu, u32 msr, u64 data,
 				hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
 			else
 				hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+			/*
+			 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+			 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+			 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+			 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+			 *   - kvm_update_masterclock()
+			 */
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 		} else {
 			hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a983a1616..21317597c 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1301,6 +1301,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,
 						       trig_mode, vector);
 		break;
@@ -1853,6 +1857,12 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1882| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|1897| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2859| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1866,6 +1876,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1935| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2047| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2108| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2157| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2770| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1966,6 +1984,10 @@ static void update_target_expiration(struct kvm_lapic *apic, uint32_t old_diviso
 	apic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2198| <<__start_apic_timer>> && !set_target_expiration(apic, count_reg))
+ */
 static bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)
 {
 	ktime_t now;
@@ -2086,6 +2108,9 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (!ktimer->tscdeadline)
 		return false;
 
+	/*
+	 * vmx_set_hv_timer()
+	 */
 	if (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
@@ -2201,6 +2226,11 @@ static void __start_apic_timer(struct kvm_lapic *apic, u32 count_reg)
 	restart_apic_timer(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2358| <<kvm_lapic_reg_write(APIC_TMICT)>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2515| <<kvm_set_lapic_tscdeadline_msr>> start_apic_timer(apic);
+ */
 static void start_apic_timer(struct kvm_lapic *apic)
 {
 	__start_apic_timer(apic, APIC_TMICT);
@@ -2468,6 +2498,10 @@ void kvm_free_lapic(struct kvm_vcpu *vcpu)
  * LAPIC interface
  *----------------------------------------------------------------------
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4167| <<kvm_get_msr_common(MSR_IA32_TSC_DEADLINE)>> msr_info->data = kvm_get_lapic_tscdeadline_msr(vcpu);
+ */
 u64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2478,6 +2512,11 @@ u64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)
 	return apic->lapic_timer.tscdeadline;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2177| <<handle_fastpath_set_tscdeadline>> kvm_set_lapic_tscdeadline_msr(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3737| <<kvm_set_msr_common>> kvm_set_lapic_tscdeadline_msr(vcpu, data);
+ */
 void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2981,6 +3020,9 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	kvm_lapic_set_reg(apic, APIC_TMCCT, 0);
 	kvm_apic_update_apicv(vcpu);
 	if (apic->apicv_active) {
+		/*
+		 * vmx_apicv_post_state_restore()
+		 */
 		static_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);
 		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
 		static_call_cond(kvm_x86_hwapic_isr_update)(apic_find_highest_isr(apic));
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index ec169f5c7..190bafb8c 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -1610,6 +1610,10 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	return flush;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|687| <<kvm_change_spte_gfn>> return kvm_set_spte_gfn(kvm, range);
+ */
 bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool flush = false;
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 512163d52..3d6c9fdef 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -915,6 +915,10 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
  * Zap all invalidated roots to ensure all SPTEs are dropped before the "fast
  * zap" completes.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6194| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	flush_workqueue(kvm->arch.tdp_mmu_zap_wq);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index bf653df86..390faf9fa 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,12 +23,55 @@
 #include "lapic.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - arch/x86/kvm/pmu.c|994| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ */
 /* This is enough to filter the vast majority of currently defined events. */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/pmu.c|29| <<global>> struct x86_pmu_capability __read_mostly kvm_pmu_cap;
+ *   - arch/x86/kvm/cpuid.c|958| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|959| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|960| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|961| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len;
+ *   - arch/x86/kvm/cpuid.c|962| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|963| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|965| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|971| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/cpuid.c|1251| <<__do_cpuid_func>> ebx.split.num_core_pmc = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/pmu.h|190| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|198| <<kvm_init_pmu_capability>> if (!kvm_pmu_cap.num_counters_gp || WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|199| <<kvm_init_pmu_capability>> WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|201| <<kvm_init_pmu_capability>> else if (is_intel && !kvm_pmu_cap.version)
+ *   - arch/x86/kvm/pmu.h|206| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|210| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|211| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_gp = min(kvm_pmu_cap.num_counters_gp, pmu_ops->MAX_NR_GP_COUNTERS);
+ *   - arch/x86/kvm/pmu.h|213| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed, KVM_PMC_MAX_FIXED);
+ *   - arch/x86/kvm/svm/pmu.c|204| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int, pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5015| <<svm_set_cpu_caps>> if (kvm_pmu_cap.num_counters_gp < AMD64_NUM_COUNTERS_CORE)
+ *   - arch/x86/kvm/svm/svm.c|5016| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5017| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5021| <<svm_set_cpu_caps>> if (kvm_pmu_cap.version != 2 || !kvm_cpu_cap_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/vmx/capabilities.h|395| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|499| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|501| <<intel_pmu_refresh>> eax.split.bit_width = min_t(int, eax.split.bit_width, kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> eax.split.mask_length = min_t(int, eax.split.mask_length, kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|514| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min3(ARRAY_SIZE(fixed_pmc_events), (size_t) edx.split.num_counters_fixed, (size_t)kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|516| <<intel_pmu_refresh>> edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed, kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|7194| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_PERFCTR0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7199| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_EVENTSEL0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7204| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_FIXED_CTR0 >= kvm_pmu_cap.num_counters_fixed)
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
+/*
+ * 在以下使用vmx_pebs_pdir_cpu[]:
+ *   - arch/x86/kvm/pmu.c|216| <<pmc_get_pebs_precise_level>> (pmc->idx == 32 && x86_match_cpu(vmx_pebs_pdir_cpu)))
+ */
 /* Precise Distribution of Instructions Retired (PDIR) */
 static const struct x86_cpu_id vmx_pebs_pdir_cpu[] = {
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D, NULL),
@@ -38,6 +81,10 @@ static const struct x86_cpu_id vmx_pebs_pdir_cpu[] = {
 	{}
 };
 
+/*
+ * 在以下使用vmx_pebs_pdist_cpu[]:
+ *   - arch/x86/kvm/pmu.c|215| <<pmc_get_pebs_precise_level>> if ((pmc->idx == 0 && x86_match_cpu(vmx_pebs_pdist_cpu)) ||
+ */
 /* Precise Distribution (PDist) */
 static const struct x86_cpu_id vmx_pebs_pdist_cpu[] = {
 	X86_MATCH_INTEL_FAM6_MODEL(SAPPHIRERAPIDS_X, NULL),
@@ -80,6 +127,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9450| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -93,14 +144,33 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * 在以下使用kvm_pmu->irq_work:
+ *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+ *   - arch/x86/kvm/pmu.c|737| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+ *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ *
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
 	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|101| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+	 *   - arch/x86/kvm/x86.c|10594| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+	 */
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|158| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|417| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -127,6 +197,20 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 	if (!pmc->intr || skip_pmi)
 		return;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_deliver_pmi()
+	 *
+	 * 在以下使用kvm_pmu->irq_work:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+	 *   - arch/x86/kvm/pmu.c|737| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+	 *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+	 */
 	/*
 	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
 	 * can be ejected on a guest mode re-entry. Otherwise we can't
@@ -139,8 +223,19 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
 	else
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|225| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -155,8 +250,25 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|158| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|417| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	__kvm_perf_overflow(pmc, true);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -182,6 +294,16 @@ static u64 pmc_get_pebs_precise_level(struct kvm_pmc *pmc)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|545| <<reprogram_counter>> if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ *
+ * 575         if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ * 576                                   (eventsel & pmu->raw_event_mask),
+ * 577                                   !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ * 578                                   !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ * 579                                   eventsel & ARCH_PERFMON_EVENTSEL_INT))
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
@@ -236,6 +358,10 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|420| <<reprogram_counter>> pmc_pause_counter(pmc);
+ */
 static void pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -249,6 +375,10 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|443| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -271,6 +401,11 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|415| <<filter_sort_cmp>> return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT |
+ *   - arch/x86/kvm/pmu.c|426| <<filter_event_cmp>> return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT));
+ */
 static int filter_cmp(const void *pa, const void *pb, u64 mask)
 {
 	u64 a = *(u64 *)pa & mask;
@@ -280,6 +415,10 @@ static int filter_cmp(const void *pa, const void *pb, u64 mask)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|995| <<prepare_filter_lists>> sort(&filter->events, filter->nevents, sizeof(filter->events[0]), filter_sort_cmp, NULL);
+ */
 static int filter_sort_cmp(const void *pa, const void *pb)
 {
 	return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT |
@@ -291,11 +430,21 @@ static int filter_sort_cmp(const void *pa, const void *pb)
  * 'excludes' list separately rather than on the 'events' list (which
  * has both).  As a result the exclude bit can be ignored.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|432| <<find_filter_index>> u64 *fe = bsearch(&key, events, nevents, sizeof(events[0]), filter_event_cmp);
+ *   - arch/x86/kvm/pmu.c|467| <<filter_contains_match>> if (filter_event_cmp(&events[i], &event_select))
+ *   - arch/x86/kvm/pmu.c|475| <<filter_contains_match>> if (filter_event_cmp(&events[i], &event_select))
+ */
 static int filter_event_cmp(const void *pa, const void *pb)
 {
 	return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|458| <<filter_contains_match>> index = find_filter_index(events, nevents, event_select);
+ */
 static int find_filter_index(u64 *events, u64 nevents, u64 key)
 {
 	u64 *fe = bsearch(&key, events, nevents, sizeof(events[0]),
@@ -307,6 +456,11 @@ static int find_filter_index(u64 *events, u64 nevents, u64 key)
 	return fe - events;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|470| <<filter_contains_match>> if (is_filter_entry_match(events[i], umask))
+ *   - arch/x86/kvm/pmu.c|478| <<filter_contains_match>> if (is_filter_entry_match(events[i], umask))
+ */
 static bool is_filter_entry_match(u64 filter_event, u64 umask)
 {
 	u64 mask = filter_event >> (KVM_PMU_MASKED_ENTRY_UMASK_MASK_SHIFT - 8);
@@ -319,6 +473,11 @@ static bool is_filter_entry_match(u64 filter_event, u64 umask)
 	return (umask & mask) == match;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|488| <<is_gp_event_allowed>> if (filter_contains_match(f->includes, f->nr_includes, eventsel) &&
+ *   - arch/x86/kvm/pmu.c|489| <<is_gp_event_allowed>> !filter_contains_match(f->excludes, f->nr_excludes, eventsel))
+ */
 static bool filter_contains_match(u64 *events, u64 nevents, u64 eventsel)
 {
 	u64 event_select = eventsel & kvm_pmu_ops.EVENTSEL_EVENT;
@@ -352,6 +511,10 @@ static bool filter_contains_match(u64 *events, u64 nevents, u64 eventsel)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|552| <<check_pmu_event_filter>> return is_gp_event_allowed(filter, pmc->eventsel);
+ */
 static bool is_gp_event_allowed(struct kvm_x86_pmu_event_filter *f,
 				u64 eventsel)
 {
@@ -362,6 +525,10 @@ static bool is_gp_event_allowed(struct kvm_x86_pmu_event_filter *f,
 	return f->action == KVM_PMU_EVENT_DENY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|554| <<check_pmu_event_filter>> return is_fixed_event_allowed(filter, pmc->idx);
+ */
 static bool is_fixed_event_allowed(struct kvm_x86_pmu_event_filter *filter,
 				   int idx)
 {
@@ -377,6 +544,10 @@ static bool is_fixed_event_allowed(struct kvm_x86_pmu_event_filter *filter,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|560| <<pmc_event_is_allowed>> check_pmu_event_filter(pmc);
+ */
 static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 {
 	struct kvm_x86_pmu_event_filter *filter;
@@ -395,12 +566,21 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 	return is_fixed_event_allowed(filter, pmc->idx);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|576| <<reprogram_counter>> if (!pmc_event_is_allowed(pmc))
+ *   - arch/x86/kvm/pmu.c|952| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_event_is_allowed(pmc))
+ */
 static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 {
 	return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
 	       check_pmu_event_filter(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|481| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ */
 static void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -456,6 +636,10 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	pmc->prev_counter = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10592| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -469,6 +653,9 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
+		/*
+		 * 只在这里调用
+		 */
 		reprogram_counter(pmc);
 	}
 
@@ -481,12 +668,21 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 		kvm_pmu_cleanup(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8248| <<emulator_check_pmc>> if (kvm_pmu_is_valid_rdpmc_ecx(emul_to_vcpu(ctxt), pmc))
+ */
 /* check if idx is a valid index to access PMU */
 bool kvm_pmu_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	return static_call(kvm_x86_pmu_is_valid_rdpmc_ecx)(vcpu, idx);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|3955| <<check_rdpmc>> if (enable_vmware_backdoor && is_vmware_backdoor_pmc(rcx))
+ *   - arch/x86/kvm/pmu.c|721| <<kvm_pmu_rdpmc>> if (is_vmware_backdoor_pmc(idx))
+ */
 bool is_vmware_backdoor_pmc(u32 pmc_idx)
 {
 	switch (pmc_idx) {
@@ -498,6 +694,10 @@ bool is_vmware_backdoor_pmc(u32 pmc_idx)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|722| <<kvm_pmu_rdpmc>> return kvm_pmu_rdpmc_vmware(vcpu, idx, data);
+ */
 static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 {
 	u64 ctr_val;
@@ -521,6 +721,11 @@ static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1423| <<kvm_emulate_rdpmc>> if (kvm_pmu_rdpmc(vcpu, ecx, &data)) {
+ *   - arch/x86/kvm/x86.c|8256| <<emulator_read_pmc>> return kvm_pmu_rdpmc(emul_to_vcpu(ctxt), pmc, pdata);
+ */
 int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 {
 	bool fast_mode = idx & (1u << 31);
@@ -547,14 +752,29 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|101| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10594| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3929| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|4010| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|4114| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ *   - arch/x86/kvm/x86.c|4366| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ */
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	switch (msr) {
@@ -569,6 +789,10 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|853| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL)>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -578,6 +802,11 @@ static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4115| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4367| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ */
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -603,6 +832,11 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3930| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4011| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -666,6 +900,12 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|342| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|890| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3699| <<kvm_set_msr_common>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
@@ -675,6 +915,11 @@ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|924| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|12129| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -683,6 +928,10 @@ void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_reset)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11958| <<kvm_arch_vcpu_create>> kvm_pmu_init(vcpu);
+ */
 void kvm_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -695,6 +944,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	kvm_pmu_refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|586| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
@@ -720,11 +973,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12041| <<kvm_arch_vcpu_destroy>> kvm_pmu_destroy(vcpu);
+ */
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|985| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
 	pmc->prev_counter = pmc->counter;
@@ -732,6 +993,10 @@ static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 	kvm_pmu_request_counter_reprogram(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|984| <<kvm_pmu_trigger_event>> if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))
+ */
 static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
 	unsigned int perf_hw_id)
 {
@@ -739,6 +1004,10 @@ static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
 		AMD64_RAW_EVENT_MASK_NB);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|973| <<kvm_pmu_trigger_event>> if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))
+ */
 static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 {
 	bool select_os, select_user;
@@ -758,6 +1027,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3567| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8758| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9065| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9067| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -777,6 +1053,10 @@ void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 }
 EXPORT_SYMBOL_GPL(kvm_pmu_trigger_event);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|1030| <<prepare_filter_lists>> else if (!is_masked_filter_valid(filter))
+ */
 static bool is_masked_filter_valid(const struct kvm_x86_pmu_event_filter *filter)
 {
 	u64 mask = kvm_pmu_ops.EVENTSEL_EVENT |
@@ -793,6 +1073,10 @@ static bool is_masked_filter_valid(const struct kvm_x86_pmu_event_filter *filter
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|1029| <<prepare_filter_lists>> convert_to_masked_filter(filter);
+ */
 static void convert_to_masked_filter(struct kvm_x86_pmu_event_filter *filter)
 {
 	int i, j;
@@ -822,6 +1106,10 @@ static void convert_to_masked_filter(struct kvm_x86_pmu_event_filter *filter)
 	filter->nevents = j;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|1102| <<kvm_vm_ioctl_set_pmu_event_filter>> r = prepare_filter_lists(filter);
+ */
 static int prepare_filter_lists(struct kvm_x86_pmu_event_filter *filter)
 {
 	int i;
@@ -858,6 +1146,10 @@ static int prepare_filter_lists(struct kvm_x86_pmu_event_filter *filter)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7113| <<kvm_arch_vm_ioctl(KVM_SET_PMU_EVENT_FILTER)>> r = kvm_vm_ioctl_set_pmu_event_filter(kvm, argp);
+ */
 int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_pmu_event_filter __user *user_filter = argp;
@@ -912,6 +1204,18 @@ int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
 
 	r = 0;
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 7d9ba301c..c02cd470f 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -41,6 +41,13 @@ struct kvm_pmu_ops {
 
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.h|279| <<pmc_is_globally_enabled>> if (!kvm_pmu_has_perf_global_ctrl(pmu))
+ *   - arch/x86/kvm/vmx/nested.c|2651| <<prepare_vmcs02>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)) &&
+ *   - arch/x86/kvm/vmx/nested.c|4526| <<load_vmcs12_host_state>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|178| <<intel_is_valid_msr>> return kvm_pmu_has_perf_global_ctrl(pmu);
+ */
 static inline bool kvm_pmu_has_perf_global_ctrl(struct kvm_pmu *pmu)
 {
 	/*
@@ -55,6 +62,14 @@ static inline bool kvm_pmu_has_perf_global_ctrl(struct kvm_pmu *pmu)
 	return pmu->version > 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|374| <<pmc_pause_counter>> pmc->counter = counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.c|992| <<kvm_pmu_incr_counter>> pmc->counter = (pmc->counter + 1) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|81| <<pmc_read_counter>> return counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|164| <<get_sample_period>> u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|167| <<get_sample_period>> sample_period = pmc_bitmask(pmc) + 1;
+ */
 static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -62,6 +77,17 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|751| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/pmu.h|109| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|163| <<amd_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|340| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|345| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|409| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|413| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
@@ -74,6 +100,11 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|535| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|91| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -84,6 +115,13 @@ static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|828| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|244| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|613| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|620| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -92,22 +130,53 @@ static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|563| <<check_pmu_event_filter>> if (pmc_is_gp(pmc))
+ *   - arch/x86/kvm/pmu.c|1016| <<cpl_is_matched>> if (pmc_is_gp(pmc)) {
+ */
 static inline bool pmc_is_gp(struct kvm_pmc *pmc)
 {
 	return pmc->type == KVM_PMC_GP;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc_is_fixed(pmc)) {
+ *   - arch/x86/kvm/pmu.h|185| <<pmc_speculative_in_use>> if (pmc_is_fixed(pmc))
+ */
 static inline bool pmc_is_fixed(struct kvm_pmc *pmc)
 {
 	return pmc->type == KVM_PMC_FIXED;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|870| <<kvm_pmu_set_msr>> if (!kvm_valid_perf_global_ctrl(pmu, data))
+ *   - arch/x86/kvm/vmx/nested.c|2927| <<nested_vmx_check_host_state>> CC(!kvm_valid_perf_global_ctrl(vcpu_to_pmu(vcpu),
+ *   - arch/x86/kvm/vmx/nested.c|3046| <<nested_vmx_check_guest_state>> CC(!kvm_valid_perf_global_ctrl(vcpu_to_pmu(vcpu),
+ */
 static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
 						 u64 data)
 {
 	return !(pmu->global_ctrl_mask & data);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|67| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|149| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|191| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|207| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|208| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|338| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|339| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|349| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|400| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|401| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|416| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 /* returns general purpose PMC with the specified MSR. Note that it can be
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
@@ -125,6 +194,15 @@ static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|57| <<reprogram_fixed_counters>> pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|72| <<intel_pmc_idx_to_pmc>> return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|193| <<intel_is_valid_msr>> get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|206| <<intel_msr_idx_to_pmc>> pmc = get_fixed_pmc(pmu, msr);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|344| <<intel_pmu_get_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|412| <<intel_pmu_set_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ */
 /* returns fixed PMC with the specified MSR */
 static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 {
@@ -140,6 +218,12 @@ static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|325| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|390| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|178| <<pmc_update_sample_period>> get_sample_period(pmc, pmc->counter));
+ */
 static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
@@ -149,6 +233,12 @@ static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|164| <<amd_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|410| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|414| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ */
 static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event || pmc->is_paused ||
@@ -159,6 +249,12 @@ static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|576| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/pmu.c|967| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|755| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -172,6 +268,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9544| <<__kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -214,12 +314,35 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 					     KVM_PMC_MAX_FIXED);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|746| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - rch/x86/kvm/svm/pmu.c|173| <<amd_pmu_set_msr(PMU_TYPE_EVNTSEL)>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|426| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ */
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * called b:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|384| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
@@ -237,10 +360,22 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
  *
  * If the vPMU doesn't have global_ctrl MSR, all vPMCs are enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|576| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|756| <<intel_pmu_cross_mapped_check>> !pmc_is_globally_enabled(pmc) || !pmc->perf_event)
+ */
 static inline bool pmc_is_globally_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.h|279| <<pmc_is_globally_enabled>> if (!kvm_pmu_has_perf_global_ctrl(pmu))
+	 *   - arch/x86/kvm/vmx/nested.c|2651| <<prepare_vmcs02>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)) &&
+	 *   - arch/x86/kvm/vmx/nested.c|4526| <<load_vmcs12_host_state>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|178| <<intel_is_valid_msr>> return kvm_pmu_has_perf_global_ctrl(pmu);
+	 */
 	if (!kvm_pmu_has_perf_global_ctrl(pmu))
 		return true;
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 80c769c58..edb80565e 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -20,8 +20,22 @@
 #include "nested.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用MSR_PMC_FULL_WIDTH_BIT:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|402| <<intel_pmu_set_msr>> if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|407| <<intel_pmu_set_msr>> !(msr & MSR_PMC_FULL_WIDTH_BIT))
+ */
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * 在以下使用intel_arch_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|28| <<global>> } const intel_arch_events[] = {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|83| <<intel_hw_event_available>> for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|84| <<intel_hw_event_available>> if (intel_arch_events[i].eventsel != event_select ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|85| <<intel_hw_event_available>> intel_arch_events[i].unit_mask != unit_mask)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|449| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+ *   - arch/x86/kvm/vmx/pmu_intel.c|450| <<setup_fixed_pmc_eventsel>> intel_arch_events[event].eventsel;
+ */
 static struct {
 	u8 eventsel;
 	u8 unit_mask;
@@ -37,6 +51,13 @@ static struct {
 	[7] = { 0x00, 0x03 },
 };
 
+/*
+ * 在以下使用fixed_pmc_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|41| <<global>> static int fixed_pmc_events[] = {1, 0, 7};
+ *   - arch/x86/kvm/vmx/pmu_intel.c|441| <<setup_fixed_pmc_eventsel>> size_t size = ARRAY_SIZE(fixed_pmc_events);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|448| <<setup_fixed_pmc_eventsel>> event = fixed_pmc_events[array_index_nospec(i, size)];
+ *   - arch/x86/kvm/vmx/pmu_intel.c|512| <<intel_pmu_refresh>> min3(ARRAY_SIZE(fixed_pmc_events),
+ */
 /* mapping between fixed pmc index and intel_arch_events array */
 static int fixed_pmc_events[] = {1, 0, 7};
 
@@ -57,6 +78,13 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 		pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
 
 		__set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|746| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+		 *   - rch/x86/kvm/svm/pmu.c|173| <<amd_pmu_set_msr(PMU_TYPE_EVNTSEL)>> kvm_pmu_request_counter_reprogram(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|426| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+		 */
 		kvm_pmu_request_counter_reprogram(pmc);
 	}
 }
@@ -634,6 +662,10 @@ static void intel_pmu_reset(struct kvm_vcpu *vcpu)
  *
  * Guest needs to re-enable LBR to resume branches recording.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|655| <<intel_pmu_deliver_pmi>> intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);
+ */
 static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 {
 	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);
@@ -644,6 +676,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.deliver_pmi = intel_pmu_deliver_pmi()
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index df461f387..990ac55a2 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -8349,6 +8349,13 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c381770bc..36afe4cff 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -150,9 +150,23 @@ bool __read_mostly report_ignored_msrs = true;
 module_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);
 EXPORT_SYMBOL_GPL(report_ignored_msrs);
 
+/*
+ * 在以下使用min_timer_period_us:
+ *   - arch/x86/kvm/x86.c|153| <<global>> unsigned int min_timer_period_us = 200;
+ *   - arch/x86/kvm/x86.c|154| <<global>> module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/i8254.c|350| <<create_pit_timer>> s64 min_period = min_timer_period_us * 1000LL;
+ *   - arch/x86/kvm/lapic.c|1710| <<limit_periodic_timer_frequency>> s64 min_period = min_timer_period_us * 1000LL;
+ */
 unsigned int min_timer_period_us = 200;
 module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用kvmclock_periodic_sync:
+ *   - arch/x86/kvm/x86.c|156| <<global>> static bool __read_mostly kvmclock_periodic_sync = true;
+ *   - arch/x86/kvm/x86.c|157| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|3742| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+ *   - arch/x86/kvm/x86.c|12578| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+ */
 static bool __read_mostly kvmclock_periodic_sync = true;
 module_param(kvmclock_periodic_sync, bool, S_IRUGO);
 
@@ -2250,6 +2264,13 @@ struct pvclock_clock {
 	u64 mask;
 	u32 mult;
 	u32 shift;
+	/*
+	 * 在以下使用pvclock_clock->base_cycles:
+	 *   - arch/x86/kvm/x86.c|2299| <<update_pvclock_gtod>> vdata->clock.base_cycles = tk->tkr_mono.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|2307| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|3123| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+	 *   - arch/x86/kvm/x86.c|3147| <<do_realtime>> ns = gtod->clock.base_cycles;
+	 */
 	u64 base_cycles;
 	u64 offset;
 };
@@ -2266,6 +2287,10 @@ struct pvclock_gtod_data {
 
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9925| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
@@ -2309,6 +2334,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4230| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK_NEW)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|4237| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2337,6 +2367,12 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * 写完msr后wall_clock就是更新后的墙上时间,即guest启动的日期.
+	 *
+	 * get_kvmclock_ns(kvm)应该是虚拟机启动后到现在的时间
+	 * 所以wall_nsec是虚拟机启动时候的时间
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -2355,12 +2391,29 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4023| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|4029| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+		 *   - kvm_update_masterclock()
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
@@ -2368,6 +2421,14 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	}
 
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * kvm_gen_kvmclock_update()
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
@@ -2380,12 +2441,22 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2439| <<kvm_get_time_scale>> *pmultiplier = div_frac(scaled64, tps32);
+ */
 static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 {
 	do_shl32_div32(dividend, divisor);
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2514| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+ *   - arch/x86/kvm/x86.c|3532| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2415,12 +2486,39 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2430| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+ *   - arch/x86/kvm/x86.c|3027| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|9443| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|9487| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2453| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3303| <<get_cpu_tsc_khz>> return __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {
+ *   - arch/x86/kvm/x86.c|9590| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|9607| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|9626| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+/*
+ * 在以下使用max_tsc_khz:
+ *   - arch/x86/kvm/x86.c|9765| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|9775| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+ *   - arch/x86/kvm/x86.c|12922| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+ */
 static unsigned long max_tsc_khz;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2525| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2526| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 {
 	u64 v = (u64)khz * (1000000 + ppm);
@@ -2430,6 +2528,10 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2532| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
@@ -2466,6 +2568,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6353| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|12484| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2500,6 +2607,10 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
@@ -2510,12 +2621,25 @@ static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2586| <<kvm_track_tsc_matching>> (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
+ *   - arch/x86/kvm/x86.c|3000| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|3003| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ *   - arch/x86/kvm/x86.c|3017| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|3020| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ *   - arch/x86/kvm/x86.c|9815| <<pvclock_gtod_notify>> if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
+ */
 static inline int gtod_is_based_on_tsc(int mode)
 {
 	return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2725| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2534,6 +2658,18 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+	 *   - kvm_update_masterclock()
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2554,11 +2690,25 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
  *
  * N equals to kvm_caps.tsc_scaling_ratio_frac_bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2615| <<kvm_scale_tsc>> _tsc = __scale_tsc(ratio, tsc);
+ *   - arch/x86/kvm/x86.c|9982| <<__kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+ */
 static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 {
 	return mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2633| <<kvm_compute_l1_tsc_offset>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2641| <<kvm_read_l1_tsc>> kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2873| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc((u64) adjustment,
+ *   - arch/x86/kvm/x86.c|3449| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,
+ *   - arch/x86/kvm/x86.c|4468| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;
+ *   - arch/x86/kvm/x86.c|5882| <<kvm_arch_tsc_set_attr>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
+ */
 u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 {
 	u64 _tsc = tsc;
@@ -2569,6 +2719,15 @@ u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 	return _tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2767| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3788| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4841| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+ *
+ * 基于当前的rdtsc()计算
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2578,6 +2737,21 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|582| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1828| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1839| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1938| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2030| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|2054| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|961| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|8026| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|3424| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|10116| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|11288| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
 	return vcpu->arch.l1_tsc_offset +
@@ -2585,6 +2759,12 @@ u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|691| <<nested_vmcb02_prepare_control>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+ *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+ *   - arch/x86/kvm/x86.c|2684| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+ */
 u64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)
 {
 	u64 nested_offset;
@@ -2600,6 +2780,12 @@ u64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_offset);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|1538| <<nested_svm_update_tsc_ratio_msr>> kvm_calc_nested_tsc_multiplier(vcpu->arch.l1_tsc_scaling_ratio
+ *   - arch/x86/kvm/vmx/nested.c|2583| <<prepare_vmcs02>> vcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(
+ *   - arch/x86/kvm/x86.c|2700| <<kvm_vcpu_write_tsc_multiplier>> vcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(
+ */
 u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 {
 	if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
@@ -2610,6 +2796,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2747| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2862| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|5177| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
@@ -2634,6 +2826,12 @@ static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 	static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2471| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2497| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+ *   - arch/x86/kvm/x86.c|2509| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ */
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)
 {
 	vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
@@ -2651,6 +2849,14 @@ static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multipli
 			vcpu, vcpu->arch.tsc_scaling_ratio);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2837| <<kvm_synchronize_tsc>> if (!kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5168| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5174| <<kvm_arch_vcpu_load>> if (kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|12297| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && kvm->created_vcpus)
+ *   - arch/x86/kvm/x86.c|12667| <<kvm_arch_hardware_enable>> stable = !kvm_check_tsc_unstable();
+ */
 static inline bool kvm_check_tsc_unstable(void)
 {
 #ifdef CONFIG_X86_64
@@ -2669,6 +2875,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2847| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ *   - arch/x86/kvm/x86.c|5885| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched)
 {
@@ -2716,6 +2927,36 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	kvm_track_tsc_matching(vcpu);
 }
 
+/*
+ * 创建一个2-CPU的虚拟机:
+ *
+ * 先是post create, 后来是被QEMU的MSR_IA32_TSC调用(两次每个vcpu)
+ *
+ * 因为master clock是在run的时候check request调用, 所以每个cpu只调用一次
+ *
+ * [  197.631757] kvm: orabug: kvm_synchronize_tsc() post create()
+ * [  197.633896] kvm: orabug: kvm_synchronize_tsc() post create()
+ * [  197.654083] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=0
+ * [  197.654924] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=1
+ * [  197.663893] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=0
+ * [  197.664745] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=1
+ * [  197.667353] kvm: orabug: kvm_update_masterclock()
+ * [  197.711664] kvm: orabug: kvm_update_masterclock()
+ *
+ *
+ * 如果此时hot-add cpu:
+ *
+ * [  717.672590] kvm: orabug: kvm_synchronize_tsc() post create()
+ * [  717.673603] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=2
+ *
+ * 只有online cpu的时候才有下面的.
+ *
+ * [  774.337876] kvm: orabug: kvm_update_masterclock()
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|4025| <<kvm_set_msr_common(MSR_IA32_TSC,只能QEMU)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|12348| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2725,6 +2966,15 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool synchronizing = false;
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+	 *   - arch/x86/kvm/x86.c|3788| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4841| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 *
+	 * 基于当前的rdtsc()计算
+	 */
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
 	ns = get_kvmclock_base_ns();
 	elapsed = ns - kvm->arch.last_tsc_nsec;
@@ -2746,6 +2996,10 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 * of virtual cycle time against real time is
 			 * interpreted as an attempt to synchronize the CPU.
 			 */
+			/*
+			 * data < tsc_exp + tsc_hz
+			 * data > tsc_exp - tsc_hz
+			 */
 			synchronizing = data < tsc_exp + tsc_hz &&
 					data + tsc_hz > tsc_exp;
 		}
@@ -2773,6 +3027,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2859| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3344| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|3982| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|4028| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
@@ -2780,6 +3041,10 @@ static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5068| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+ */
 static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 {
 	if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
@@ -2791,6 +3056,10 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2905| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2811,6 +3080,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2929| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2949| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2846,6 +3120,10 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2966| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns, tsc_timestamp));
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2865,6 +3143,10 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2978| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2886,6 +3168,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3045| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+ */
 /* returns true if host is using TSC based clocksource */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
@@ -2897,6 +3183,11 @@ static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 						      tsc_timestamp));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3174| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+ *   - arch/x86/kvm/x86.c|10016| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+ */
 /* returns true if host is using TSC based clocksource */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
@@ -2950,6 +3241,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3099| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6919| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9398| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12636| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2973,6 +3271,13 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2430| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3027| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9443| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9487| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2982,17 +3287,32 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3082| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9444| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 static void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3085| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9457| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+ */
 static void __kvm_start_pvclock_update(struct kvm *kvm)
 {
 	raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
 	write_seqcount_begin(&kvm->arch.pvclock_sc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3138| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|6978| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+ */
 static void kvm_start_pvclock_update(struct kvm *kvm)
 {
 	kvm_make_mclock_inprogress_request(kvm);
@@ -3001,6 +3321,12 @@ static void kvm_start_pvclock_update(struct kvm *kvm)
 	__kvm_start_pvclock_update(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3140| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7003| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9459| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+ */
 static void kvm_end_pvclock_update(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3009,6 +3335,24 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 
 	write_seqcount_end(&ka->pvclock_sc);
 	raw_spin_unlock_irq(&ka->tsc_write_lock);
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *
+	 * kvm_guest_time_update()
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -3017,6 +3361,19 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10779| <<vcpu_enter_guest(KVM_REQ_MASTERCLOCK_UPDATE)>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_request_tsc_page_update(kvm);
@@ -3033,14 +3390,32 @@ static void kvm_update_masterclock(struct kvm *kvm)
  * notification when calibration completes, but practically speaking calibration
  * will complete before userspace is alive enough to create VMs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3184| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+ *   - arch/x86/kvm/x86.c|3318| <<kvm_guest_time_update>> tgt_tsc_khz = get_cpu_tsc_khz();
+ */
 static unsigned long get_cpu_tsc_khz(void)
 {
+	/*
+	 * 在以下使用percpu的cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2453| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|3303| <<get_cpu_tsc_khz>> return __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {
+	 *   - arch/x86/kvm/x86.c|9590| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|9607| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|9626| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 */
 	if (static_cpu_has(X86_FEATURE_CONSTANT_TSC))
 		return tsc_khz;
 	else
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3202| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3056,6 +3431,10 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 #ifdef CONFIG_X86_64
 		struct timespec64 ts;
 
+		/*
+		 * 返回a pair of ns and tsc
+		 * 都是realtime的
+		 */
 		if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
 			data->realtime = ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec;
 			data->flags |= KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC;
@@ -3064,6 +3443,11 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 		data->host_tsc = rdtsc();
 
 		data->flags |= KVM_CLOCK_TSC_STABLE;
+		/*
+		 * 下面的两个:
+		 *   - ka->master_cycle_now对应收集master的时候的host tsc
+		 *   - "ka->master_kernel_ns + ka->kvmclock_offset"对应收集master的时候VM启动了经历了的时间
+		 */
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 		kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
@@ -3071,12 +3455,28 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 				   &hv_clock.tsc_to_system_mul);
 		data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
 	} else {
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+		 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 *
+		 * get_kvmclock_base_ns()是host启动后一共花的时间
+		 */
 		data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
 	}
 
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3210| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6954| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3088,6 +3488,18 @@ static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|579| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2340| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|62| <<kvm_xen_shared_info_init>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|458| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|846| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|887| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|939| <<kvm_xen_vcpu_set_attr>> get_kvmclock_ns(vcpu->kvm));
+ *   - arch/x86/kvm/xen.c|1389| <<kvm_xen_hcall_vcpu_op>> delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|1419| <<kvm_xen_hcall_set_timer_op>> uint64_t guest_now = get_kvmclock_ns(vcpu->kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
@@ -3096,6 +3508,12 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return data.clock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3381| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3383| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|3386| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 				    struct gfn_to_pfn_cache *gpc,
 				    unsigned int offset)
@@ -3145,6 +3563,25 @@ static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 }
 
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10581| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3165,6 +3602,17 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 */
 	do {
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|3218| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 *                                                              && !ka->backwards_tsc_observed && !ka->boot_vcpu_runs_old_kvmclock;
+		 *
+		 * 在以下使用kvm_arch->master_kernel_ns:
+		 *   - arch/x86/kvm/x86.c|3033| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3157| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3279| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+		 *   - arch/x86/kvm/x86.c|6953| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+		 */
 		use_master_clock = ka->use_master_clock;
 		if (use_master_clock) {
 			host_tsc = ka->master_cycle_now;
@@ -3197,6 +3645,12 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2521| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3563| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5305| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -3221,6 +3675,10 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		kvm_xen_update_tsc_info(v);
 	}
 
+	/*
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
 	vcpu->last_guest_tsc = tsc_timestamp;
@@ -3257,8 +3715,19 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
  * by the delay we use to rate-limit the updates.
  */
 
+/*
+ * 在以下使用KVMCLOCK_UPDATE_DELAY:
+ *   - arch/x86/kvm/x86.c|3449| <<kvm_gen_kvmclock_update>> KVMCLOCK_UPDATE_DELAY);
+ */
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_update_work:
+ *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+ *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+ *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	unsigned long i;
@@ -3269,22 +3738,91 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *
+		 * kvm_guest_time_update()
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10635| <<vcpu_enter_guest(KVM_REQ_GLOBAL_CLOCK_UPDATE)>> kvm_gen_kvmclock_update(vcpu);
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *
+	 * kvm_guest_time_update()
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 *
+	 * kvmclock_update_fn()
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
+/*
+ * 在以下使用KVMCLOCK_SYNC_PERIOD:
+ *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ */
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_sync_work:
+ *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ *   - arch/x86/kvm/x86.c|12539| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+ *
+ * 在以下使用kvmclock_sync_fn():
+ *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ */
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -3292,10 +3830,35 @@ static void kvmclock_sync_fn(struct work_struct *work)
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|156| <<global>> static bool __read_mostly kvmclock_periodic_sync = true;
+	 *   - arch/x86/kvm/x86.c|157| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|3742| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|12578| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (!kvmclock_periodic_sync)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 *
+	 * 调用kvmclock_update_fn()
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|12539| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 *
+	 * 一共在两处schedue, kvm_arch_vcpu_postcreate()和这里.
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -4804,6 +5367,11 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5985| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -4936,6 +5504,11 @@ static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 	return kvm_apic_get_state(vcpu, s);
 }
 
+/*
+ * struct kvm_lapic_state {
+ *     char regs[KVM_APIC_REG_SIZE];
+ * };
+ */
 static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
@@ -5309,6 +5882,20 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	    lapic_in_kernel(vcpu))
 		vcpu->arch.apic->sipi_vector = events->sipi_vector;
 
+	/*
+	 * 在QEMU设置KVM_VCPUEVENT_VALID_SMM的地方
+	 *
+	 * 4426 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
+	 * 4427 {
+	 * ... ...
+	 * 4474          * Stop SMI delivery on old machine types to avoid a reboot
+	 * 4475          * on an inward migration of an old VM.
+	 * 4476          *
+	 * 4477         if (!cpu->kvm_no_smi_migration) {
+	 * 4478             events.flags |= KVM_VCPUEVENT_VALID_SMM;
+	 * 4479         }
+	 * 4480     }
+	 */
 	if (events->flags & KVM_VCPUEVENT_VALID_SMM) {
 #ifdef CONFIG_KVM_SMM
 		if (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {
@@ -9268,6 +9855,24 @@ static void __kvmclock_cpufreq_notifier(struct cpufreq_freqs *freq, int cpu)
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			if (vcpu->cpu != cpu)
 				continue;
+			/*
+			 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+			 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+			 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *
+			 * kvm_guest_time_update()
+			 */
 			kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 			if (vcpu->cpu != raw_smp_processor_id())
 				send_ipi = 1;
@@ -9346,20 +9951,57 @@ static void kvm_timer_init(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9403| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9640| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
 	struct kvm_vcpu *vcpu;
 	unsigned long i;
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+	 *   - kvm_update_masterclock()
+	 */
 	mutex_lock(&kvm_lock);
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2430| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3027| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9443| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9487| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
 
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9403| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9640| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
 
 /*
@@ -9367,16 +10009,34 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  * region to prevent possible deadlocks against time accessors which
  * are invoked with work related locks held.
  */
+/*
+ * 在以下使用pvclock_irq_work_fn():
+ *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9634| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9847| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9634| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9847| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * struct notifier_block pvclock_gtod_notifier.notifier_call = pvclock_gtod_notify()
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
@@ -9390,6 +10050,12 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|9634| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|9847| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
@@ -9668,6 +10334,10 @@ int kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);
 
 #ifdef CONFIG_X86_64
+/*
+ * 处理KVM_HC_CLOCK_PAIRING:
+ *   - arch/x86/kvm/x86.c|10471| <<kvm_emulate_hypercall(KVM_HC_CLOCK_PAIRING)>> ret = kvm_pv_clock_pairing(vcpu, a0, a1);
+ */
 static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 			        unsigned long clock_type)
 {
@@ -10524,10 +11194,48 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_mmu_free_obsolete_roots(vcpu);
 		if (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))
 			__kvm_migrate_timers(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+		 *   - kvm_update_masterclock()
+		 */
 		if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
 			kvm_update_masterclock(vcpu->kvm);
+		/*
+		 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+		 *
+		 * kvm_gen_kvmclock_update()
+		 */
 		if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
 			kvm_gen_kvmclock_update(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *
+		 * kvm_guest_time_update()
+		 */
 		if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
 			r = kvm_guest_time_update(vcpu);
 			if (unlikely(r))
@@ -10588,8 +11296,27 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 #endif
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+		 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 处理函数是kvm_pmu_handle_event()
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+		 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -10879,6 +11606,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 }
 
 /* Called within kvm->srcu read side.  */
+/*
+ * 对于没有hot-add的(曾经在但是remove了的)
+ * mp_state是3 (KVM_MP_STATE_HALTED)
+ */
 static inline int vcpu_block(struct kvm_vcpu *vcpu)
 {
 	bool hv_timer;
@@ -11946,6 +12677,14 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3994| <<kvm_vm_ioctl_create_vcpu>> kvm_arch_vcpu_postcreate(vcpu);
+ *
+ * kvm_vm_ioctl(KVM_CREATE_VCPU)
+ * -> kvm_vm_ioctl_create_vcpu()
+ *    -> kvm_arch_vcpu_postcreate()
+ */
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -11961,6 +12700,13 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 	mutex_unlock(&vcpu->mutex);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|156| <<global>> static bool __read_mostly kvmclock_periodic_sync = true;
+	 *   - arch/x86/kvm/x86.c|157| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|3742| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|12578| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
 		schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 						KVMCLOCK_SYNC_PERIOD);
@@ -12235,6 +12981,18 @@ int kvm_arch_hardware_enable(void)
 			kvm_for_each_vcpu(i, vcpu, kvm) {
 				vcpu->arch.tsc_offset_adjustment += delta_cyc;
 				vcpu->arch.last_host_tsc = local_tsc;
+				/*
+				 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+				 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+				 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+				 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+				 *   - kvm_update_masterclock()
+				 */
 				kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 			}
 
@@ -12278,6 +13036,18 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+		 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 处理函数是kvm_pmu_handle_event()
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 	static_call(kvm_x86_sched_in)(vcpu, cpu);
@@ -12323,6 +13093,15 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
 	mutex_init(&kvm->arch.apic_map_lock);
 	seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
@@ -12378,6 +13157,10 @@ static void kvm_unload_vcpu_mmus(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1283| <<kvm_destroy_vm>> kvm_arch_sync_events(kvm);
+ */
 void kvm_arch_sync_events(struct kvm *kvm)
 {
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 82e3dafc5..560a78761 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -448,6 +448,13 @@ static __always_inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_handling_nmi_from_guest(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1861| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|441| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|446| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|451| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
 }
 
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index 40edf4d19..495b354f5 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -106,6 +106,18 @@ static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 	wc->version = wc_version + 1;
 	read_unlock_irq(&gpc->lock);
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+	 *   - kvm_update_masterclock()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
 
 out:
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index e733a2a19..ff034f863 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -71,6 +71,23 @@ static struct arch_timer *arch_timer_mem __ro_after_init;
 #define to_arch_timer(e) container_of(e, struct arch_timer, evt)
 
 static u32 arch_timer_rate __ro_after_init;
+/*
+ * VM的数据
+ * crash> arch_timer_ppi
+ * arch_timer_ppi = $1 =
+ * {0, 9, 10, 11, 0}
+ *
+ *   irq_data = {
+ *       mask = 0,
+ *       irq = 10,
+ *       hwirq = 27,
+ *       common = 0xffff0000c0021800,
+ *       chip = 0xffff80000ad592f0 <gic_chip>,
+ *       domain = 0xffff0000c0145e00,
+ *       parent_data = 0x0,
+ *       chip_data = 0xffff80000a8e3b88 <gic_data>
+ *   },
+ */
 static int arch_timer_ppi[ARCH_TIMER_MAX_TIMER_PPI] __ro_after_init;
 
 static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
@@ -83,8 +100,25 @@ static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
 
 static struct clock_event_device __percpu *arch_timer_evt;
 
+/*
+ * 在以下设置arch_timer_uses_ppi:
+ *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ */
 static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
 static bool arch_timer_c3stop __ro_after_init;
+/*
+ * 在以下使用arch_timer_mem_use_virtual:
+ *   - drivers/clocksource/arm_arch_timer.c|88| <<global>> static bool arch_timer_mem_use_virtual __ro_after_init;
+ *   - drivers/clocksource/arm_arch_timer.c|887| <<__arch_timer_setup>> if (arch_timer_mem_use_virtual) {
+ *   - drivers/clocksource/arm_arch_timer.c|1062| <<arch_timer_banner>> arch_timer_mem_use_virtual ? "virt" : "phys" :
+ *   - drivers/clocksource/arm_arch_timer.c|1325| <<arch_timer_mem_register>> if (arch_timer_mem_use_virtual)
+ *   - drivers/clocksource/arm_arch_timer.c|1544| <<arch_timer_mem_find_best_frame>> arch_timer_mem_use_virtual = true;
+ *   - drivers/clocksource/arm_arch_timer.c|1565| <<arch_timer_mem_frame_register>> if (arch_timer_mem_use_virtual)
+ *   - drivers/clocksource/arm_arch_timer.c|1572| <<arch_timer_mem_frame_register>> arch_timer_mem_use_virtual ? "virt" : "phys");
+ */
 static bool arch_timer_mem_use_virtual __ro_after_init;
 static bool arch_counter_suspend_stop __ro_after_init;
 #ifdef CONFIG_GENERIC_GETTIMEOFDAY
@@ -1083,13 +1117,43 @@ static noinstr u64 arch_counter_get_cntvct_mem(void)
 	return arch_counter_get_cnt_mem(arch_timer_mem, CNTVCT_LO);
 }
 
+/*
+ * struct arch_timer_kvm_info {
+ *     struct timecounter timecounter;
+ *     int virtual_irq;         
+ *     int physical_irq;
+ * };
+ *
+ * 在以下使用arch_timer_kvm_info:
+ *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+ *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+ *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+ *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+ */
 static struct arch_timer_kvm_info arch_timer_kvm_info;
 
 struct arch_timer_kvm_info *arch_timer_get_kvm_info(void)
 {
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 *
+	 * 在以下使用arch_timer_kvm_info:
+	 *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+	 *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+	 *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+	 *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+	 */
 	return &arch_timer_kvm_info;
 }
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1348| <<arch_timer_common_init>> arch_counter_register(arch_timers_present);
+ */
 static void __init arch_counter_register(unsigned type)
 {
 	u64 (*scr)(void);
@@ -1100,12 +1164,22 @@ static void __init arch_counter_register(unsigned type)
 	if (type & ARCH_TIMER_TYPE_CP15) {
 		u64 (*rd)(void);
 
+		/*
+		 * 在以下设置arch_timer_uses_ppi:
+		 *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+		 *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+		 *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+		 *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+		 */
 		if ((IS_ENABLED(CONFIG_ARM64) && !is_hyp_mode_available()) ||
 		    arch_timer_uses_ppi == ARCH_TIMER_VIRT_PPI) {
 			if (arch_timer_counter_has_wa()) {
 				rd = arch_counter_get_cntvct_stable;
 				scr = raw_counter_get_cntvct_stable;
 			} else {
+				/*
+				 * kvm似乎是这里
+				 */
 				rd = arch_counter_get_cntvct;
 				scr = arch_counter_get_cntvct;
 			}
@@ -1206,6 +1280,11 @@ static void __init arch_timer_cpu_pm_deinit(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1477| <<arch_timer_of_init>> ret = arch_timer_register();
+ *   - drivers/clocksource/arm_arch_timer.c|1800| <<arch_timer_acpi_init>> ret = arch_timer_register();
+ */
 static int __init arch_timer_register(void)
 {
 	int err;
@@ -1217,6 +1296,29 @@ static int __init arch_timer_register(void)
 		goto out;
 	}
 
+	/*
+	 * 在以下设置arch_timer_uses_ppi:
+	 *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+	 *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+	 *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+	 *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+	 *
+	 * VM的数据
+	 * crash> arch_timer_ppi
+	 * arch_timer_ppi = $1 = 
+	 * {0, 9, 10, 11, 0}
+	 *
+	 *   irq_data = {
+	 *       mask = 0,
+	 *       irq = 10,
+	 *       hwirq = 27,
+	 *       common = 0xffff0000c0021800,
+	 *       chip = 0xffff80000ad592f0 <gic_chip>,
+	 *       domain = 0xffff0000c0145e00,
+	 *       parent_data = 0x0,
+	 *       chip_data = 0xffff80000a8e3b88 <gic_data>
+	 *   },
+	 */
 	ppi = arch_timer_ppi[arch_timer_uses_ppi];
 	switch (arch_timer_uses_ppi) {
 	case ARCH_TIMER_VIRT_PPI:
diff --git a/drivers/irqchip/irq-gic-v3-its-pci-msi.c b/drivers/irqchip/irq-gic-v3-its-pci-msi.c
index 93f77a819..3efae51ca 100644
--- a/drivers/irqchip/irq-gic-v3-its-pci-msi.c
+++ b/drivers/irqchip/irq-gic-v3-its-pci-msi.c
@@ -170,6 +170,10 @@ its_pci_msi_parse_madt(union acpi_subtable_headers *header,
 		goto out;
 	}
 
+	/*
+	 * [    0.010681] Platform MSI: ITS@0x8080000 domain created
+	 * [    0.010825] PCI/MSI: ITS@0x8080000 domain created
+	 */
 	err = its_pci_msi_init_one(dom_handle, node_name);
 	if (!err)
 		pr_info("PCI/MSI: %s domain created\n", node_name);
diff --git a/drivers/irqchip/irq-gic-v3-its.c b/drivers/irqchip/irq-gic-v3-its.c
index e0c2b10d1..027b40127 100644
--- a/drivers/irqchip/irq-gic-v3-its.c
+++ b/drivers/irqchip/irq-gic-v3-its.c
@@ -109,6 +109,13 @@ struct its_node {
 	struct its_baser	tables[GITS_BASER_NR_REGS];
 	struct its_collection	*collections;
 	struct fwnode_handle	*fwnode_handle;
+	/*
+	 * 在以下使用its_node->get_msi_base:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1726| <<its_irq_compose_msi_msg>> addr = its->get_msi_base(its_dev);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3599| <<its_irq_domain_alloc>> err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
+	 *   - drivers/irqchip/irq-gic-v3-its.c|4716| <<its_enable_quirk_socionext_synquacer>> its->get_msi_base = its_irq_get_msi_base_pre_its;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|5129| <<its_probe_one>> its->get_msi_base = its_irq_get_msi_base;
+	 */
 	u64			(*get_msi_base)(struct its_device *its_dev);
 	u64			typer;
 	u64			cbaser_save;
@@ -1723,6 +1730,13 @@ static void its_irq_compose_msi_msg(struct irq_data *d, struct msi_msg *msg)
 	u64 addr;
 
 	its = its_dev->its;
+	/*
+	 * 在以下使用its_node->get_msi_base:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1726| <<its_irq_compose_msi_msg>> addr = its->get_msi_base(its_dev);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3599| <<its_irq_domain_alloc>> err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
+	 *   - drivers/irqchip/irq-gic-v3-its.c|4716| <<its_enable_quirk_socionext_synquacer>> its->get_msi_base = its_irq_get_msi_base_pre_its;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|5129| <<its_probe_one>> its->get_msi_base = its_irq_get_msi_base;
+	 */
 	addr = its->get_msi_base(its_dev);
 
 	msg->address_lo		= lower_32_bits(addr);
@@ -3596,6 +3610,13 @@ static int its_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,
 	if (err)
 		return err;
 
+	/*
+	 * 在以下使用its_node->get_msi_base:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1726| <<its_irq_compose_msi_msg>> addr = its->get_msi_base(its_dev);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3599| <<its_irq_domain_alloc>> err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
+	 *   - drivers/irqchip/irq-gic-v3-its.c|4716| <<its_enable_quirk_socionext_synquacer>> its->get_msi_base = its_irq_get_msi_base_pre_its;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|5129| <<its_probe_one>> its->get_msi_base = its_irq_get_msi_base;
+	 */
 	err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
 	if (err)
 		return err;
@@ -4952,6 +4973,10 @@ static void __init __iomem *its_map_one(struct resource *res, int *err)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|5198| <<its_probe_one>> err = its_init_domain(handle, its);
+ */
 static int its_init_domain(struct fwnode_handle *handle, struct its_node *its)
 {
 	struct irq_domain *inner_domain;
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index eedfa8e9f..02e1b2d41 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -35,6 +35,121 @@
 
 #include "irq-gic-common.h"
 
+/*
+ * intid是针对gic的中断号, 也就是irq_data->hwirq
+ * 应该不是proc/interrupts的irq
+ *
+ *---------------------------
+ *
+ * virtio的中断
+ *
+ * # cat /sys/kernel/debug/irq/irqs/85
+ * handler:  handle_fasteoi_irq
+ * device:   0000:00:06.0
+ * status:   0x00000000
+ * istate:   0x00004000
+ * ddepth:   0
+ * wdepth:   0
+ * dstate:   0x31401200
+ *             IRQD_ACTIVATED
+ *             IRQD_IRQ_STARTED
+ *             IRQD_SINGLE_TARGET
+ *             IRQD_AFFINITY_SET
+ *             IRQD_AFFINITY_ON_ACTIVATE
+ *             IRQD_HANDLE_ENFORCE_IRQCTX
+ * node:     -1
+ * affinity: 0
+ * effectiv: 0
+ * domain:  irqchip@0x0000000008080000-3
+ *  hwirq:   0x18001
+ *  chip:    ITS-MSI
+ *   flags:   0x20
+ *              IRQCHIP_ONESHOT_SAFE
+ *  parent:
+ *     domain:  irqchip@0x0000000008080000-5
+ *      hwirq:   0x2025
+ *      chip:    ITS
+ *       flags:   0x0
+ *      parent:
+ *         domain:  irqchip@0x0000000008000000-1
+ *          hwirq:   0x2025
+ *          chip:    GICv3
+ *           flags:   0x15
+ *                      IRQCHIP_SET_TYPE_MASKED
+ *                      IRQCHIP_MASK_ON_SUSPEND
+ *                      IRQCHIP_SKIP_SET_WAKE
+ *
+ *---------------------------
+ *
+ * arch_timer的中断
+ *
+ * # cat /sys/kernel/debug/irq/irqs/10
+ * handler:  handle_percpu_devid_irq
+ * device:   (null)
+ * status:   0x00031704
+ *             _IRQ_NOPROBE
+ *             _IRQ_NOTHREAD
+ *             _IRQ_NOAUTOEN
+ *             _IRQ_PER_CPU_DEVID
+ * istate:   0x00004000
+ * ddepth:   1
+ * wdepth:   0
+ * dstate:   0x12032a04
+ *             IRQ_TYPE_LEVEL_HIGH
+ *             IRQD_LEVEL
+ *             IRQD_ACTIVATED
+ *             IRQD_IRQ_DISABLED
+ *             IRQD_IRQ_MASKED
+ *             IRQD_PER_CPU
+ *             IRQD_DEFAULT_TRIGGER_SET
+ *             IRQD_HANDLE_ENFORCE_IRQCTX
+ * node:     -1
+ * affinity: 0-3
+ * effectiv:
+ * domain:  irqchip@0x0000000008000000-1
+ *  hwirq:   0x1b
+ *  chip:    GICv3
+ *   flags:   0x15
+ *              IRQCHIP_SET_TYPE_MASKED
+ *              IRQCHIP_MASK_ON_SUSPEND
+ *              IRQCHIP_SKIP_SET_WAKE
+ *
+ *---------------------------
+ *
+ * pmu的中断
+ *
+ * # cat /sys/kernel/debug/irq/irqs/50
+ * handler:  handle_percpu_devid_irq
+ * device:   (null)
+ * status:   0x00031704
+ *             _IRQ_NOPROBE
+ *             _IRQ_NOTHREAD
+ *             _IRQ_NOAUTOEN
+ *             _IRQ_PER_CPU_DEVID
+ * istate:   0x00004000
+ * ddepth:   1
+ * wdepth:   0
+ * dstate:   0x12032a04
+ *             IRQ_TYPE_LEVEL_HIGH
+ *             IRQD_LEVEL
+ *             IRQD_ACTIVATED
+ *             IRQD_IRQ_DISABLED
+ *             IRQD_IRQ_MASKED
+ *             IRQD_PER_CPU
+ *             IRQD_DEFAULT_TRIGGER_SET
+ *             IRQD_HANDLE_ENFORCE_IRQCTX
+ * node:     -1
+ * affinity: 0-3
+ * effectiv: 
+ * domain:  irqchip@0x0000000008000000-1
+ *  hwirq:   0x17
+ *  chip:    GICv3
+ *   flags:   0x15
+ *              IRQCHIP_SET_TYPE_MASKED
+ *              IRQCHIP_MASK_ON_SUSPEND
+ *              IRQCHIP_SKIP_SET_WAKE
+ */
+
 #define GICD_INT_NMI_PRI	(GICD_INT_DEF_PRI & ~0x80)
 
 #define FLAGS_WORKAROUND_GICR_WAKER_MSM8996	(1ULL << 0)
@@ -131,6 +246,10 @@ static refcount_t *ppi_nmi_refs;
 static struct gic_kvm_info gic_v3_kvm_info __initdata;
 static DEFINE_PER_CPU(bool, has_rss);
 
+/*
+ * 每个核都有一个MPIDR寄存器,并且每个核的MPIDR寄存器的值都不相同,这可以作为每个核的唯一标志.
+ * 比如我希望某个中断由核1处理,那么GIC中的路由寄存器需要根据核1的MPIDR值去配置.
+ */
 #define MPIDR_RS(mpidr)			(((mpidr) & 0xF0UL) >> 4)
 #define gic_data_rdist()		(this_cpu_ptr(gic_data.rdists.rdist))
 #define gic_data_rdist_rd_base()	(gic_data_rdist()->rd_base)
@@ -149,6 +268,14 @@ enum gic_intid_range {
 	__INVALID_RANGE__
 };
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|302| <<get_intid_range>> return __get_intid_range(d->hwirq);
+ *   - drivers/irqchip/irq-gic-v3.c|339| <<gic_dist_base_alias>> switch (__get_intid_range(hwirq)) {
+ *   - drivers/irqchip/irq-gic-v3.c|650| <<__gic_get_ppi_index>> switch (__get_intid_range(hwirq)) {
+ *   - drivers/irqchip/irq-gic-v3.c|1676| <<gic_irq_domain_map>> switch (__get_intid_range(hw)) {
+ *   - drivers/irqchip/irq-gic-v3.c|1839| <<fwspec_is_partitioned_ppi>> range = __get_intid_range(hwirq);
+ */
 static enum gic_intid_range __get_intid_range(irq_hw_number_t hwirq)
 {
 	switch (hwirq) {
@@ -169,6 +296,15 @@ static enum gic_intid_range __get_intid_range(irq_hw_number_t hwirq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|184| <<gic_irq_in_rdist>> switch (get_intid_range(d)) {
+ *   - drivers/irqchip/irq-gic-v3.c|229| <<gic_dist_base>> switch (get_intid_range(d)) {
+ *   - drivers/irqchip/irq-gic-v3.c|326| <<convert_offset_index>> switch (get_intid_range(d)) {
+ *   - drivers/irqchip/irq-gic-v3.c|604| <<gic_arm64_erratum_2941627_needed>> range = get_intid_range(d);
+ *   - drivers/irqchip/irq-gic-v3.c|655| <<gic_set_type>> range = get_intid_range(d);
+ *   - drivers/irqchip/irq-gic-v3.c|685| <<gic_irq_set_vcpu_affinity>> if (get_intid_range(d) == SGI_RANGE)
+ */
 static enum gic_intid_range get_intid_range(struct irq_data *d)
 {
 	return __get_intid_range(d->hwirq);
@@ -272,6 +408,11 @@ static void gic_redist_wait_for_rwp(void)
 
 #ifdef CONFIG_ARM64
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|952| <<__gic_handle_irq_from_irqson>> irqnr = gic_read_iar();
+ *   - drivers/irqchip/irq-gic-v3.c|1001| <<__gic_handle_irq_from_irqsoff>> irqnr = gic_read_iar();
+ */
 static u64 __maybe_unused gic_read_iar(void)
 {
 	if (cpus_have_const_cap(ARM64_WORKAROUND_CAVIUM_23154))
@@ -405,6 +546,11 @@ static void gic_poke_irq(struct irq_data *d, u32 offset)
 
 static void gic_mask_irq(struct irq_data *d)
 {
+	/*
+	 * GICD_ICENABLER寄存器为GIC支持的每个中断提供一个clear-enable位,
+	 * 写1到clear-enable位将禁止相应的中断从分发器转发到CPU接口端.
+	 * 即禁止了所有中断转发.
+	 */
 	gic_poke_irq(d, GICD_ICENABLER);
 	if (gic_irq_in_rdist(d))
 		gic_redist_wait_for_rwp();
@@ -497,6 +643,11 @@ static int gic_irq_get_irqchip_state(struct irq_data *d,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|577| <<gic_irq_nmi_setup>> gic_irq_set_prio(d, GICD_INT_NMI_PRI);
+ *   - drivers/irqchip/irq-gic-v3.c|612| <<gic_irq_nmi_teardown>> gic_irq_set_prio(d, GICD_INT_DEF_PRI);
+ */
 static void gic_irq_set_prio(struct irq_data *d, u8 prio)
 {
 	void __iomem *base = gic_dist_base(d);
@@ -1336,6 +1487,10 @@ static u16 gic_compute_target_list(int *base_cpu, const struct cpumask *mask,
 	(MPIDR_AFFINITY_LEVEL(cluster_id, level) \
 		<< ICC_SGI1R_AFFINITY_## level ##_SHIFT)
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|1390| <<gic_ipi_send_mask>> gic_send_sgi(cluster_id, tlist, d->hwirq);
+ */
 static void gic_send_sgi(u64 cluster_id, u16 tlist, unsigned int irq)
 {
 	u64 val;
@@ -1351,6 +1506,10 @@ static void gic_send_sgi(u64 cluster_id, u16 tlist, unsigned int irq)
 	gic_write_sgi1r(val);
 }
 
+/*
+ * struct irq_chip gic_chip.ipi_send_mask = gic_ipi_send_mask()
+ * struct irq_chip gic_eoimode1_chip.ipi_send_mask = gic_ipi_send_mask()
+ */
 static void gic_ipi_send_mask(struct irq_data *d, const struct cpumask *mask)
 {
 	int cpu;
@@ -1514,6 +1673,10 @@ static struct irq_chip gic_eoimode1_chip = {
 				  IRQCHIP_MASK_ON_SUSPEND,
 };
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|1681| <<gic_irq_domain_alloc>> ret = gic_irq_domain_map(domain, virq + i, hwirq + i);
+ */
 static int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,
 			      irq_hw_number_t hw)
 {
@@ -1556,6 +1719,13 @@ static int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - struct irq_domain_ops gic_irq_domain_ops.translate = gic_irq_domain_translate()
+ *   - drivers/irqchip/irq-gic-v3.c|1676| <<gic_irq_domain_alloc>> ret = gic_irq_domain_translate(domain, fwspec, &hwirq, &type);
+ *   - drivers/irqchip/irq-gic-v3.c|1737| <<gic_irq_domain_select>> ret = gic_irq_domain_translate(d, fwspec, &hwirq, &type);
+ *   - drivers/irqchip/irq-gic-v3.c|1776| <<partition_domain_translate>> ret = gic_irq_domain_translate(d, fwspec, &ppi_intid, type);
+ */
 static int gic_irq_domain_translate(struct irq_domain *d,
 				    struct irq_fwspec *fwspec,
 				    unsigned long *hwirq,
@@ -1629,6 +1799,9 @@ static int gic_irq_domain_translate(struct irq_domain *d,
 	return -EINVAL;
 }
 
+/*
+ * struct irq_domain_ops gic_irq_domain_ops.alloc = gic_irq_domain_alloc()
+ */
 static int gic_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,
 				unsigned int nr_irqs, void *arg)
 {
@@ -2094,6 +2267,10 @@ static int __init gic_validate_dist_version(void __iomem *dist_base)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2355| <<gic_of_init>> gic_populate_ppi_partitions(node);
+ */
 /* Create all possible partitions at boot time */
 static void __init gic_populate_ppi_partitions(struct device_node *gic_node)
 {
@@ -2198,6 +2375,26 @@ static void __init gic_of_setup_kvm_info(struct device_node *node)
 	struct resource r;
 	u32 gicv_idx;
 
+	/*
+	 * struct gic_kvm_info {
+	 *     // GIC type
+	 *     enum gic_type   type;
+	 *     // Virtual CPU interface
+	 *     struct resource vcpu;
+	 *     // Interrupt number
+	 *     unsigned int    maint_irq;
+	 *     // No interrupt mask, no need to use the above field
+	 *     bool            no_maint_irq_mask;
+	 *     // Virtual control interface
+	 *     struct resource vctrl;
+	 *     // vlpi support
+	 *     bool            has_v4;
+	 *     // rvpeid support
+	 *     bool            has_v4_1;
+	 *     // Deactivation impared, subpar stuff
+	 *     bool            no_hw_deactivation;
+	 * };
+	 */
 	gic_v3_kvm_info.type = GIC_V3;
 
 	gic_v3_kvm_info.maint_irq = irq_of_parse_and_map(node, 0);
@@ -2242,6 +2439,10 @@ static void __iomem *gic_of_iomap(struct device_node *node, int idx,
 	return base ?: IOMEM_ERR_PTR(-ENOMEM);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2371| <<global>> IRQCHIP_DECLARE(gic_v3, "arm,gic-v3", gic_of_init);
+ */
 static int __init gic_of_init(struct device_node *node, struct device_node *parent)
 {
 	phys_addr_t dist_phys_base;
@@ -2557,6 +2758,14 @@ static void __init gic_acpi_setup_kvm_info(void)
 
 	gic_v3_kvm_info.has_v4 = gic_data.rdists.has_vlpis;
 	gic_v3_kvm_info.has_v4_1 = gic_data.rdists.has_rvpeid;
+	/*
+	 * called by:
+	 *   - drivers/irqchip/irq-apple-aic.c|1063| <<aic_of_ic_init>> vgic_set_kvm_info(&vgic_info);
+	 *   - drivers/irqchip/irq-gic-v3.c|2245| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+	 *   - drivers/irqchip/irq-gic-v3.c|2587| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+	 *   - drivers/irqchip/irq-gic.c|1466| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+	 *   - drivers/irqchip/irq-gic.c|1627| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+	 */
 	vgic_set_kvm_info(&gic_v3_kvm_info);
 }
 
@@ -2632,6 +2841,18 @@ gic_acpi_init(union acpi_subtable_headers *header, const unsigned long end)
 	iounmap(acpi_data.dist_base);
 	return err;
 }
+
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2687| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V3, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2690| <<global>> IRQCHIP_ACPI_DECLARE(gic_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V4, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2693| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3_or_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_NONE, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1703| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_V2, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1706| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2_maybe, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_NONE, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-loongarch-cpu.c|171| <<global>> IRQCHIP_ACPI_DECLARE(cpuintc_v1, ACPI_MADT_TYPE_CORE_PIC, NULL, ACPI_MADT_CORE_PIC_VERSION_V1, cpuintc_acpi_init);
+ *   - drivers/irqchip/irq-riscv-intc.c|194| <<global>> IRQCHIP_ACPI_DECLARE(riscv_intc, ACPI_MADT_TYPE_RINTC, NULL, ACPI_MADT_RINTC_VERSION_V1, riscv_intc_acpi_init);
+ */
+
 IRQCHIP_ACPI_DECLARE(gic_v3, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR,
 		     acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V3,
 		     gic_acpi_init);
diff --git a/drivers/irqchip/irq-gic.c b/drivers/irqchip/irq-gic.c
index 412196a7d..9f197b093 100644
--- a/drivers/irqchip/irq-gic.c
+++ b/drivers/irqchip/irq-gic.c
@@ -820,6 +820,10 @@ static int gic_set_affinity(struct irq_data *d, const struct cpumask *mask_val,
 	return IRQ_SET_MASK_OK_DONE;
 }
 
+/*
+ * struct irq_chip gic_chip.ipi_send_mask = gic_ipi_send_mask()
+ * struct irq_chip gic_chip_mode1.ipi_send_mask = gic_ipi_send_mask()
+ */
 static void gic_ipi_send_mask(struct irq_data *d, const struct cpumask *mask)
 {
 	int cpu;
@@ -920,6 +924,10 @@ static const struct irq_chip gic_chip_mode1 = {
  * cpu_id: the ID for the destination CPU interface
  * irq: the IPI number to send a SGI for
  */
+/*
+ * called by:
+ *   - arch/arm/common/bL_switcher.c|198| <<bL_switch_to>> gic_send_sgi(bL_gic_id[ib_cpu][ib_cluster], 0);
+ */
 void gic_send_sgi(unsigned int cpu_id, unsigned int irq)
 {
 	BUG_ON(cpu_id >= NR_GIC_CPU_IF);
@@ -1466,6 +1474,18 @@ static void __init gic_of_setup_kvm_info(struct device_node *node)
 		vgic_set_kvm_info(&gic_v2_kvm_info);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic.c|1524| <<global>> IRQCHIP_DECLARE(gic_400, "arm,gic-400", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1525| <<global>> IRQCHIP_DECLARE(arm11mp_gic, "arm,arm11mp-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1526| <<global>> IRQCHIP_DECLARE(arm1176jzf_dc_gic, "arm,arm1176jzf-devchip-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1527| <<global>> IRQCHIP_DECLARE(cortex_a15_gic, "arm,cortex-a15-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1528| <<global>> IRQCHIP_DECLARE(cortex_a9_gic, "arm,cortex-a9-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1529| <<global>> IRQCHIP_DECLARE(cortex_a7_gic, "arm,cortex-a7-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1530| <<global>> IRQCHIP_DECLARE(msm_8660_qgic, "qcom,msm-8660-qgic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1531| <<global>> IRQCHIP_DECLARE(msm_qgic2, "qcom,msm-qgic2", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1532| <<global>> IRQCHIP_DECLARE(pl390, "arm,pl390", gic_of_init);
+ */
 int __init
 gic_of_init(struct device_node *node, struct device_node *parent)
 {
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 49d1d6acf..66bff6e9a 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -842,6 +842,11 @@ static ssize_t tap_put_user(struct tap_queue *q,
 	return ret ? ret : total;
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|904| <<tap_read_iter>> ret = tap_do_read(q, to, noblock, NULL);
+ *   - drivers/net/tap.c|1259| <<tap_recvmsg>> ret = tap_do_read(q, &m->msg_iter, flags & MSG_DONTWAIT, skb);
+ */
 static ssize_t tap_do_read(struct tap_queue *q,
 			   struct iov_iter *to,
 			   int noblock, struct sk_buff *skb)
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 8e9f4cfe9..9ad37e99c 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -1570,6 +1570,11 @@ static void virtio_skb_set_hash(const struct virtio_net_hdr_v1_hash *hdr_hash,
 	skb_set_hash(skb, __le32_to_cpu(hdr_hash->hash_value), rss_hash_type);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1888| <<virtnet_receive>> receive_buf(vi, rq, buf, len, ctx, xdp_xmit, &stats);
+ *   - drivers/net/virtio_net.c|1894| <<virtnet_receive>> receive_buf(vi, rq, buf, len, NULL, xdp_xmit, &stats);
+ */
 static void receive_buf(struct virtnet_info *vi, struct receive_queue *rq,
 			void *buf, unsigned int len, void **ctx,
 			unsigned int *xdp_xmit,
diff --git a/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c b/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
index 7369a45f7..4d0abe89c 100644
--- a/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
+++ b/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
@@ -1554,6 +1554,10 @@ void iwl_mvm_rx_beacon_notif(struct iwl_mvm *mvm,
 	}
 }
 
+/*
+ * 在以下使用iwl_mvm_rx_missed_beacons_notif():
+ *   - drivers/net/wireless/intel/iwlwifi/mvm/ops.c|355| <<global>> RX_HANDLER(MISSED_BEACONS_NOTIFICATION, iwl_mvm_rx_missed_beacons_notif, RX_HANDLER_SYNC, struct iwl_missed_beacons_notif),
+ */
 void iwl_mvm_rx_missed_beacons_notif(struct iwl_mvm *mvm,
 				     struct iwl_rx_cmd_buffer *rxb)
 {
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index f3a01b791..f65bd44be 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -2167,6 +2167,12 @@ const struct block_device_operations nvme_bdev_ops = {
 	.pr_ops		= &nvme_pr_ops,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2212| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, NVME_CSTS_SHST_MASK,
+ *   - drivers/nvme/host/core.c|2218| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, NVME_CSTS_RDY, 0,
+ *   - drivers/nvme/host/core.c|2284| <<nvme_enable_ctrl>> return nvme_wait_ready(ctrl, NVME_CSTS_RDY, NVME_CSTS_RDY,
+ */
 static int nvme_wait_ready(struct nvme_ctrl *ctrl, u32 mask, u32 val,
 		u32 timeout, const char *op)
 {
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index f2ed7167c..c13245d1a 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -35,6 +35,14 @@
 
 #include "vhost.h"
 
+/*
+ * 在以下使用experimental_zcopytx:
+ *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+ *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+ *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+ *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+ *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+ */
 static int experimental_zcopytx = 0;
 module_param(experimental_zcopytx, int, 0444);
 MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
@@ -326,11 +334,19 @@ static void vhost_net_tx_packet(struct vhost_net *net)
 	net->tx_zcopy_err = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|369| <<vhost_zerocopy_signal_used>> vhost_net_tx_err(net);
+ */
 static void vhost_net_tx_err(struct vhost_net *net)
 {
 	++net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|922| <<handle_tx_zerocopy>> && vhost_net_tx_select_zcopy(net);
+ */
 static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 {
 	/* TX flush waits for outstanding DMAs to be done.
@@ -340,12 +356,32 @@ static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 		net->tx_packets / 64 >= net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|1002| <<handle_tx>> if (vhost_sock_zcopy(sock))
+ *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+ *
+ * experimental_zcopytx似乎一直是false
+ */
 static bool vhost_sock_zcopy(struct socket *sock)
 {
+	/*
+	 * 在以下使用experimental_zcopytx:
+	 *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+	 *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+	 *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+	 *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+	 *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+	 */
 	return unlikely(experimental_zcopytx) &&
 		sock_flag(sock->sk, SOCK_ZEROCOPY);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|724| <<vhost_net_build_xdp>> int headroom = vhost_sock_xdp(sock) ? XDP_PACKET_HEADROOM : 0;
+ */
 static bool vhost_sock_xdp(struct socket *sock)
 {
 	return sock_flag(sock->sk, SOCK_XDP);
@@ -459,6 +495,22 @@ static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 	nvq->done_idx = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|587| <<vhost_net_tx_get_vq_desc>> vhost_tx_batch(net, tnvq, vhost_vq_get_backend(tvq), msghdr);
+ *   - drivers/vhost/net.c|791| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|820| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|830| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|856| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *
+ * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+ *
+ * 2. 否则, 是下面的path
+ * handle_tx_copy() or handle_tx_zerocopy()
+ * -> get_tx_bufs()
+ *    -> vhost_net_tx_get_vq_desc()
+ *       -> vhost_tx_batch()
+ */
 static void vhost_tx_batch(struct vhost_net *net,
 			   struct vhost_net_virtqueue *nvq,
 			   struct socket *sock,
@@ -476,6 +528,9 @@ static void vhost_tx_batch(struct vhost_net *net,
 
 	msghdr->msg_control = &ctl;
 	msghdr->msg_controllen = sizeof(ctl);
+	/*
+	 * tap_sendmsg()
+	 */
 	err = sock->ops->sendmsg(sock, msghdr, 0);
 	if (unlikely(err < 0)) {
 		vq_err(&nvq->vq, "Fail to batch sending packets\n");
@@ -569,6 +624,10 @@ static void vhost_net_busy_poll(struct vhost_net *net,
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|630| <<get_tx_bufs>> ret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);
+ */
 static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				    struct vhost_net_virtqueue *tnvq,
 				    unsigned int *out_num, unsigned int *in_num,
@@ -582,6 +641,15 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				  out_num, in_num, NULL, NULL);
 
 	if (r == tvq->num && tvq->busyloop_timeout) {
+		/*
+		 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+		 *
+		 * 2. 否则, 是下面的path
+		 * handle_tx_copy() or handle_tx_zerocopy()
+		 * -> get_tx_bufs()
+		 *    -> vhost_net_tx_get_vq_desc()
+		 *       -> vhost_tx_batch()
+		 */
 		/* Flush batched packets first */
 		if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
 			vhost_tx_batch(net, tnvq,
@@ -618,6 +686,11 @@ static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
 	return iov_iter_count(iter);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|793| <<handle_tx_copy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ *   - drivers/vhost/net.c|887| <<handle_tx_zerocopy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ */
 static int get_tx_bufs(struct vhost_net *net,
 		       struct vhost_net_virtqueue *nvq,
 		       struct msghdr *msg,
@@ -766,6 +839,10 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1005| <<handle_tx>> handle_tx_copy(net, sock);
+ */
 static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -787,6 +864,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 	do {
 		bool busyloop_intr = false;
 
+		/*
+		 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+		 *
+		 * 2. 否则, 是下面的path
+		 * handle_tx_copy() or handle_tx_zerocopy()
+		 * -> get_tx_bufs()
+		 *    -> vhost_net_tx_get_vq_desc()
+		 *       -> vhost_tx_batch()
+		 */
 		if (nvq->done_idx == VHOST_NET_BATCH)
 			vhost_tx_batch(net, nvq, sock, &msg);
 
@@ -817,12 +903,30 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 			if (!err) {
 				goto done;
 			} else if (unlikely(err != -ENOSPC)) {
+				/*
+				 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+				 *
+				 * 2. 否则, 是下面的path
+				 * handle_tx_copy() or handle_tx_zerocopy()
+				 * -> get_tx_bufs()
+				 *    -> vhost_net_tx_get_vq_desc()
+				 *       -> vhost_tx_batch()
+				 */
 				vhost_tx_batch(net, nvq, sock, &msg);
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
 			}
 
+			/*
+			 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+			 *
+			 * 2. 否则, 是下面的path
+			 * handle_tx_copy() or handle_tx_zerocopy()
+			 * -> get_tx_bufs()
+			 *    -> vhost_net_tx_get_vq_desc()
+			 *       -> vhost_tx_batch()
+			 */
 			/* We can't build XDP buff, go for single
 			 * packet path but let's flush batched
 			 * packets.
@@ -836,6 +940,9 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 				msg.msg_flags &= ~MSG_MORE;
 		}
 
+		/*
+		 * tap_sendmsg()
+		 */
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			if (err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS) {
@@ -853,6 +960,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		++nvq->done_idx;
 	} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
 
+	/*
+	 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+	 *
+	 * 2. 否则, 是下面的path
+	 * handle_tx_copy() or handle_tx_zerocopy()
+	 * -> get_tx_bufs()
+	 *    -> vhost_net_tx_get_vq_desc()
+	 *       -> vhost_tx_batch()
+	 */
 	vhost_tx_batch(net, nvq, sock, &msg);
 }
 
@@ -933,6 +1049,9 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 			msg.msg_flags &= ~MSG_MORE;
 		}
 
+		/*
+		 * tap_sendmsg()
+		 */
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			bool retry = err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS;
@@ -965,6 +1084,11 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1284| <<handle_tx_kick>> handle_tx(net);
+ *   - drivers/vhost/net.c|1300| <<handle_tx_net>> handle_tx(net);
+ */
 static void handle_tx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -982,6 +1106,14 @@ static void handle_tx(struct vhost_net *net)
 	vhost_disable_notify(&net->dev, vq);
 	vhost_net_disable_vq(net, vq);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+	 *   - drivers/vhost/net.c|1002| <<handle_tx>> if (vhost_sock_zcopy(sock))
+	 *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+	 *
+	 * experimental_zcopytx似乎一直是false
+	 */
 	if (vhost_sock_zcopy(sock))
 		handle_tx_zerocopy(net, sock);
 	else
@@ -1191,6 +1323,9 @@ static void handle_rx(struct vhost_net *net)
 		/* On overrun, truncate and discard */
 		if (unlikely(headcount > UIO_MAXIOV)) {
 			iov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, 1, 1);
+			/*
+			 * tap_recvmsg()
+			 */
 			err = sock->ops->recvmsg(sock, &msg,
 						 1, MSG_DONTWAIT | MSG_TRUNC);
 			pr_debug("Discarded rx packet: len %zd\n", sock_len);
@@ -1205,6 +1340,9 @@ static void handle_rx(struct vhost_net *net)
 			 */
 			iov_iter_advance(&msg.msg_iter, vhost_hlen);
 		}
+		/*
+		 * tap_recvmsg()
+		 */
 		err = sock->ops->recvmsg(sock, &msg,
 					 sock_len, MSG_DONTWAIT | MSG_TRUNC);
 		/* Userspace might have consumed the packet meanwhile:
@@ -1258,6 +1396,10 @@ static void handle_rx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用handle_tx_kick():
+ *   - drivers/vhost/net.c|1349| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;
+ */
 static void handle_tx_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1795,6 +1937,14 @@ static struct miscdevice vhost_net_misc = {
 
 static int __init vhost_net_init(void)
 {
+	/*
+	 * 在以下使用experimental_zcopytx:
+	 *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+	 *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+	 *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+	 *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+	 *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+	 */
 	if (experimental_zcopytx)
 		vhost_net_enable_zcopy(VHOST_NET_VQ_TX);
 	return misc_register(&vhost_net_misc);
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index abef0619c..4bd4df5d4 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -999,6 +999,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1098| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1453| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
diff --git a/include/kvm/arm_arch_timer.h b/include/kvm/arm_arch_timer.h
index bb3cb0058..815154c47 100644
--- a/include/kvm/arm_arch_timer.h
+++ b/include/kvm/arm_arch_timer.h
@@ -32,17 +32,55 @@ struct arch_timer_offset {
 	 * If set, pointer to one of the offsets in the kvm's offset
 	 * structure. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vm_offset:
+	 *   - arch/arm64/kvm/arch_timer.c|136| <<timer_get_offset>> if (ctxt->offset.vm_offset)
+	 *   - arch/arm64/kvm/arch_timer.c|137| <<timer_get_offset>> offset += *ctxt->offset.vm_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|190| <<timer_set_offset>> if (!ctxt->offset.vm_offset) {
+	 *   - arch/arm64/kvm/arch_timer.c|195| <<timer_set_offset>> WRITE_ONCE(*ctxt->offset.vm_offset, offset);
+	 *   - arch/arm64/kvm/arch_timer.c|1024| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1055| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1057| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|430| <<kvm_hyp_handle_cntpct>> if (ctxt->offset.vm_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|431| <<kvm_hyp_handle_cntpct>> val -= *kern_hyp_va(ctxt->offset.vm_offset);
+	 */
 	u64	*vm_offset;
 	/*
 	 * If set, pointer to one of the offsets in the vcpu's sysreg
 	 * array. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vcpu_offset:
+	 *   - arch/arm64/kvm/arch_timer.c|138| <<timer_get_offset>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/arch_timer.c|139| <<timer_get_offset>> offset += *ctxt->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|818| <<kvm_timer_vcpu_load_nested_switch>> offs->vcpu_offset = NULL;
+	 *   - arch/arm64/kvm/arch_timer.c|820| <<kvm_timer_vcpu_load_nested_switch>> offs->vcpu_offset = &__vcpu_sys_reg(vcpu, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_reset>> offs->vcpu_offset = &__vcpu_sys_reg(vcpu, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1220| <<kvm_arm_timer_read>> val = *timer->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|1274| <<kvm_arm_timer_write>> *timer->offset.vcpu_offset = val;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|432| <<kvm_hyp_handle_cntpct>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|433| <<kvm_hyp_handle_cntpct>> val -= *kern_hyp_va(ctxt->offset.vcpu_offset);
+	 */
 	u64	*vcpu_offset;
 };
 
 struct arch_timer_vm_data {
+	/*
+	 * 在以下使用arch_timer_vm_data->voffset:
+	 *   - arch/arm64/kvm/arch_timer.c|1014| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1692| <<kvm_vm_ioctl_set_counter_offset>> kvm->arch.timer_data.voffset = offset->counter_offset;
+	 *   - arch/arm64/kvm/hypercalls.c|47| <<kvm_ptp_get_time>> cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.voffset;
+	 */
 	/* Offset applied to the virtual timer/counter */
 	u64	voffset;
+	/*
+	 * 在以下使用arch_timer_vm_data->poffset:
+	 *   - arch/arm64/kvm/arch_timer.c|983| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1016| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1693| <<kvm_vm_ioctl_set_counter_offset>> kvm->arch.timer_data.poffset = offset->counter_offset;
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|51| <<__timer_enable_traps>> !kern_hyp_va(vcpu->kvm)->arch.timer_data.poffset)
+	 *   - arch/arm64/kvm/hypercalls.c|50| <<kvm_ptp_get_time>> cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.poffset;
+	 */
 	/* Offset applied to the physical timer/counter */
 	u64	poffset;
 
@@ -124,6 +162,26 @@ void kvm_timer_init_vhe(void);
 #define vcpu_hvtimer(v)	(&(v)->arch.timer_cpu.timers[TIMER_HVTIMER])
 #define vcpu_hptimer(v)	(&(v)->arch.timer_cpu.timers[TIMER_HPTIMER])
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|83| <<timer_get_ctl>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|102| <<timer_get_cval>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|148| <<timer_set_ctl>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|170| <<timer_set_cval>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|191| <<timer_set_offset>> WARN(offset, "timer %ld\n", arch_timer_ctx_index(ctxt));
+ *   - arch/arm64/kvm/arch_timer.c|430| <<kvm_timer_should_fire>> index = arch_timer_ctx_index(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|552| <<timer_save_state>> enum kvm_arch_timers index = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|666| <<timer_restore_state>> enum kvm_arch_timers index = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|215| <<__field>> __entry->direct_vtimer = arch_timer_ctx_index(map->direct_vtimer);
+ *   - arch/arm64/kvm/trace_arm.h|217| <<__field>> (map->direct_ptimer) ? arch_timer_ctx_index(map->direct_ptimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|219| <<__field>> (map->emul_vtimer) ? arch_timer_ctx_index(map->emul_vtimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|221| <<__field>> (map->emul_ptimer) ? arch_timer_ctx_index(map->emul_ptimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|245| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|267| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|285| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|301| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - include/kvm/arm_arch_timer.h|156| <<timer_irq>> #define timer_irq(ctx) (timer_vm_data(ctx)->ppi[arch_timer_ctx_index(ctx)])
+ */
 #define arch_timer_ctx_index(ctx)	((ctx) - vcpu_timer((ctx)->vcpu)->timers)
 
 #define timer_vm_data(ctx)		(&(ctx)->vcpu->kvm->arch.timer_data)
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 5b27f94d4..7add08dd0 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -278,6 +278,20 @@ struct vgic_dist {
 	struct list_head	lpi_list_head;
 	int			lpi_list_count;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1929| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1943| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1955| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	/* LPI translation cache */
 	struct list_head	lpi_translation_cache;
 
@@ -309,6 +323,16 @@ struct vgic_v3_cpu_if {
 	u32		vgic_sre;	/* Restored only, change ignored */
 	u32		vgic_ap0r[4];
 	u32		vgic_ap1r[4];
+	/*
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	u64		vgic_lr[VGIC_V3_MAX_LRS];
 
 	/*
@@ -333,6 +357,23 @@ struct vgic_cpu {
 
 	raw_spinlock_t ap_list_lock;	/* Protects the ap_list */
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	/*
 	 * List of IRQs that this VCPU should consider because they are either
 	 * Active or Pending (hence the name; AP list), or because they recently
diff --git a/include/linux/irqchip.h b/include/linux/irqchip.h
index d5e6024cb..5acd01df2 100644
--- a/include/linux/irqchip.h
+++ b/include/linux/irqchip.h
@@ -74,6 +74,16 @@ builtin_platform_driver(drv_name##_driver)
  * @data: data to be checked by the validate function.
  * @fn: initialization function
  */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2687| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V3, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2690| <<global>> IRQCHIP_ACPI_DECLARE(gic_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V4, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2693| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3_or_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_NONE, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1703| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_V2, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1706| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2_maybe, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_NONE, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-loongarch-cpu.c|171| <<global>> IRQCHIP_ACPI_DECLARE(cpuintc_v1, ACPI_MADT_TYPE_CORE_PIC, NULL, ACPI_MADT_CORE_PIC_VERSION_V1, cpuintc_acpi_init);
+ *   - drivers/irqchip/irq-riscv-intc.c|194| <<global>> IRQCHIP_ACPI_DECLARE(riscv_intc, ACPI_MADT_TYPE_RINTC, NULL, ACPI_MADT_RINTC_VERSION_V1, riscv_intc_acpi_init);
+ */
 #define IRQCHIP_ACPI_DECLARE(name, subtable, validate, data, fn)	\
 	ACPI_DECLARE_SUBTABLE_PROBE_ENTRY(irqchip, name,		\
 					  ACPI_SIG_MADT, subtable,	\
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9d3ac7720..43cacb0ce 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -302,6 +302,10 @@ static inline bool kvm_vcpu_mapped(struct kvm_host_map *map)
 	return !!map->hva;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3552| <<kvm_vcpu_halt>> } while (kvm_vcpu_can_poll(cur, stop));
+ */
 static inline bool kvm_vcpu_can_poll(ktime_t cur, ktime_t stop)
 {
 	return single_task_running() && !need_resched() && ktime_before(cur, stop);
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 88a7ede32..22fb9e346 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -306,6 +306,11 @@ static inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state)
 
 void __weak arch_cpuhp_sync_state_poll(void) { cpu_relax(); }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|372| <<cpuhp_bp_sync_dead>> if (cpuhp_wait_for_sync_state(cpu, SYNC_STATE_DEAD, SYNC_STATE_DEAD)) {
+ *   - kernel/cpu.c|444| <<cpuhp_bp_sync_alive>> if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
+ */
 static bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state state,
 				      enum cpuhp_sync_state next_state)
 {
@@ -434,6 +439,11 @@ void __weak arch_cpuhp_cleanup_kick_cpu(unsigned int cpu) { }
  * Early CPU bringup synchronization point. Cannot use cpuhp_state::done_up
  * because the AP cannot issue complete() so early in the bringup.
  */
+/*
+ * called by:
+ *   - kernel/cpu.c|800| <<cpuhp_bringup_ap>> ret = cpuhp_bp_sync_alive(cpu);
+ *   - kernel/cpu.c|844| <<bringup_cpu>> ret = cpuhp_bp_sync_alive(cpu);
+ */
 static int cpuhp_bp_sync_alive(unsigned int cpu)
 {
 	int ret = 0;
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 78ae7b6f9..c224e3ac2 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -12736,6 +12736,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|629| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|959| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+ *   - arch/x86/kvm/pmu.c|346| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|263| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+ *   - kernel/events/hw_breakpoint.c|774| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+ *   - kernel/events/hw_breakpoint.c|884| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|123| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/irq/irqdomain.c b/kernel/irq/irqdomain.c
index 0bdef4fe9..91c726206 100644
--- a/kernel/irq/irqdomain.c
+++ b/kernel/irq/irqdomain.c
@@ -20,6 +20,12 @@
 #include <linux/smp.h>
 #include <linux/fs.h>
 
+/*
+ * 在以下使用irq_domain_list:
+ *   - kernel/irq/irqdomain.c|236| <<__irq_domain_publish>> list_add(&domain->link, &irq_domain_list);
+ *   - kernel/irq/irqdomain.c|450| <<irq_find_matching_fwspec>> list_for_each_entry(h, &irq_domain_list, link) {
+ *   - kernel/irq/irqdomain.c|2017| <<irq_domain_debugfs_init>> list_for_each_entry(d, &irq_domain_list, link)
+ */
 static LIST_HEAD(irq_domain_list);
 static DEFINE_MUTEX(irq_domain_mutex);
 
@@ -229,6 +235,11 @@ static struct irq_domain *__irq_domain_create(struct fwnode_handle *fwnode,
 	return domain;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/irqdomain.c|265| <<__irq_domain_add>> __irq_domain_publish(domain);
+ *   - kernel/irq/irqdomain.c|1181| <<irq_domain_create_hierarchy>> __irq_domain_publish(domain);
+ */
 static void __irq_domain_publish(struct irq_domain *domain)
 {
 	mutex_lock(&irq_domain_mutex);
@@ -252,6 +263,17 @@ static void __irq_domain_publish(struct irq_domain *domain)
  * Allocates and initializes an irq_domain structure.
  * Returns pointer to IRQ domain, or NULL on failure.
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virt-pci.c|1017| <<um_pci_init>> um_pci_inner_domain = __irq_domain_add(um_pci_fwnode, MAX_MSI_VECTORS,
+ *   - include/linux/irqdomain.h|353| <<irq_domain_add_linear>> return __irq_domain_add(of_node_to_fwnode(of_node), size, size, 0, ops, host_data);
+ *   - include/linux/irqdomain.h|362| <<irq_domain_add_nomap>> return __irq_domain_add(of_node_to_fwnode(of_node), 0, max_irq, max_irq, ops, host_data);
+ *   - include/linux/irqdomain.h|372| <<irq_domain_add_tree>> return __irq_domain_add(of_node_to_fwnode(of_node), 0, ~0, 0, ops, host_data);
+ *   - include/linux/irqdomain.h|380| <<irq_domain_create_linear>> return __irq_domain_add(fwnode, size, size, 0, ops, host_data);
+ *   - include/linux/irqdomain.h|387| <<irq_domain_create_tree>> return __irq_domain_add(fwnode, 0, ~0, 0, ops, host_data);
+ *   - kernel/irq/irqdomain.c|364| <<irq_domain_create_simple>> domain = __irq_domain_add(fwnode, size, size, 0, ops, host_data);
+ *   - kernel/irq/irqdomain.c|420| <<irq_domain_create_legacy>> domain = __irq_domain_add(fwnode, first_hwirq + size, first_hwirq + size, 0, ops, host_data);
+ */
 struct irq_domain *__irq_domain_add(struct fwnode_handle *fwnode, unsigned int size,
 				    irq_hw_number_t hwirq_max, int direct_max,
 				    const struct irq_domain_ops *ops,
@@ -788,6 +810,23 @@ void of_phandle_args_to_fwspec(struct device_node *np, const u32 *args,
 }
 EXPORT_SYMBOL_GPL(of_phandle_args_to_fwspec);
 
+/*
+ * called by:
+ *   - drivers/acpi/irq.c|71| <<acpi_register_gsi>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/acpi/irq.c|286| <<acpi_irq_get>> rc = irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/gpio/gpio-uniphier.c|177| <<uniphier_gpio_to_irq>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/gpio/gpio-xgene-sb.c|126| <<xgene_gpio_sb_to_irq>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/gpio/gpiolib.c|1518| <<gpiochip_to_irq>> return irq_create_fwspec_mapping(&spec);
+ *   - drivers/irqchip/irq-apple-aic.c|1059| <<aic_of_ic_init>> vgic_info.maint_irq = irq_create_fwspec_mapping(&mi);
+ *   - drivers/irqchip/irq-bcm2836.c|258| <<bcm2836_arm_irqchip_smp_init>> mux_irq = irq_create_fwspec_mapping(&ipi_fwspec);
+ *   - drivers/irqchip/irq-gic-v3.c|2216| <<gic_populate_ppi_partitions>> irq = irq_create_fwspec_mapping(&ppi_fwspec);
+ *   - drivers/irqchip/irq-loongson-pch-lpc.c|213| <<pch_lpc_acpi_init>> parent_irq = irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/irqchip/irq-ti-sci-inta.c|250| <<ti_sci_inta_alloc_parent_irq>> parent_virq = irq_create_fwspec_mapping(&parent_fwspec);
+ *   - drivers/pinctrl/stm32/pinctrl-stm32.c|270| <<stm32_gpio_to_irq>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/remoteproc/pru_rproc.c|553| <<pru_handle_intrmap>> pru->mapped_irq[i] = irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/soc/xilinx/xlnx_event_manager.c|593| <<xlnx_event_init_sgi>> virq_sgi = irq_create_fwspec_mapping(&sgi_fwspec);
+ *   - kernel/irq/irqdomain.c|896| <<irq_create_of_mapping>> return irq_create_fwspec_mapping(&fwspec);
+ */
 unsigned int irq_create_fwspec_mapping(struct irq_fwspec *fwspec)
 {
 	struct irq_domain *domain;
@@ -886,6 +925,17 @@ unsigned int irq_create_fwspec_mapping(struct irq_fwspec *fwspec)
 }
 EXPORT_SYMBOL_GPL(irq_create_fwspec_mapping);
 
+/*
+ * called by:
+ *   - arch/powerpc/platforms/fsl_uli1575.c|339| <<hpcd_final_uli5288>> dev->irq = irq_create_of_mapping(&oirq);
+ *   - drivers/bcma/main.c|197| <<bcma_of_get_irq>> return irq_create_of_mapping(&out_irq);
+ *   - drivers/irqchip/irq-realtek-rtl.c|153| <<realtek_rtl_of_init>> parent_irq = irq_create_of_mapping(&oirq);
+ *   - drivers/of/irq.c|43| <<irq_of_parse_and_map>> return irq_create_of_mapping(&oirq);
+ *   - drivers/of/irq.c|446| <<of_irq_get>> rc = irq_create_of_mapping(&oirq);
+ *   - drivers/pci/of.c|545| <<of_irq_parse_and_map_pci>> return irq_create_of_mapping(&oirq);
+ *   - drivers/soc/ti/knav_qmss_queue.c|1242| <<knav_setup_queue_range>> range->irqs[i].irq = irq_create_of_mapping(&oirq);
+ *   - drivers/staging/board/board.c|105| <<gic_fixup_resource>> virq = irq_create_of_mapping(&irq_data);
+ */
 unsigned int irq_create_of_mapping(struct of_phandle_args *irq_data)
 {
 	struct irq_fwspec fwspec;
@@ -1453,6 +1503,13 @@ static void irq_domain_free_irqs_hierarchy(struct irq_domain *domain,
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/irq/irqdomain.c|1492| <<irq_domain_alloc_irqs_locked>> ret = irq_domain_alloc_irqs_hierarchy(domain, virq, nr_irqs, arg);
+ *   - kernel/irq/irqdomain.c|1645| <<irq_domain_push_irq>> rv = irq_domain_alloc_irqs_hierarchy(domain, virq, 1, arg);
+ *   - kernel/irq/irqdomain.c|1772| <<irq_domain_alloc_irqs_parent>> return irq_domain_alloc_irqs_hierarchy(domain->parent, irq_base,
+ *   - kernel/irq/msi.c|1102| <<msi_domain_populate_irqs>> ret = irq_domain_alloc_irqs_hierarchy(domain, virq, 1, arg);
+ */
 int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain,
 				    unsigned int irq_base,
 				    unsigned int nr_irqs, void *arg)
@@ -1462,9 +1519,17 @@ int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain,
 		return -ENOSYS;
 	}
 
+	/*
+	 * 比如gic_irq_domain_alloc()
+	 */
 	return domain->ops->alloc(domain, irq_base, nr_irqs, arg);
 }
 
+/*
+ * called by:
+ *   - kernel/irq/irqdomain.c|861| <<irq_create_fwspec_mapping>> virq = irq_domain_alloc_irqs_locked(domain, -1, 1, NUMA_NO_NODE, fwspec, false, NULL);
+ *   - kernel/irq/irqdomain.c|1549| <<__irq_domain_alloc_irqs>> ret = irq_domain_alloc_irqs_locked(domain, irq_base, nr_irqs, node, arg, realloc, affinity);
+ */
 static int irq_domain_alloc_irqs_locked(struct irq_domain *domain, int irq_base,
 					unsigned int nr_irqs, int node, void *arg,
 					bool realloc, const struct irq_affinity_desc *affinity)
@@ -1533,6 +1598,14 @@ static int irq_domain_alloc_irqs_locked(struct irq_domain *domain, int irq_base,
  * resources. In this way, it's easier to rollback when failing to
  * allocate resources.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|974| <<alloc_irq_from_domain>> return __irq_domain_alloc_irqs(domain, irq, 1,
+ *   - arch/x86/kernel/apic/io_apic.c|1010| <<alloc_isa_irq_from_domain>> irq = __irq_domain_alloc_irqs(domain, irq, 1, node, info, true,
+ *   - include/linux/irqdomain.h|507| <<irq_domain_alloc_irqs>> return __irq_domain_alloc_irqs(domain, -1, nr_irqs, node, arg, false,
+ *   - kernel/irq/ipi.c|84| <<irq_reserve_ipi>> virq = __irq_domain_alloc_irqs(domain, virq, nr_irqs, NUMA_NO_NODE,
+ *   - kernel/irq/msi.c|1301| <<__msi_domain_alloc_irqs>> virq = __irq_domain_alloc_irqs(domain, -1, desc->nvec_used,
+ */
 int __irq_domain_alloc_irqs(struct irq_domain *domain, int irq_base,
 			    unsigned int nr_irqs, int node, void *arg,
 			    bool realloc, const struct irq_affinity_desc *affinity)
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 88cbc1181..7308133be 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -1096,6 +1096,12 @@ static void clocksource_enqueue(struct clocksource *cs)
  * __clocksource_update_freq_hz() or __clocksource_update_freq_khz() helper
  * functions.
  */
+/*
+ * called by:
+ *   - include/linux/clocksource.h|256| <<__clocksource_update_freq_hz>> __clocksource_update_freq_scale(cs, 1, hz);
+ *   - include/linux/clocksource.h|261| <<__clocksource_update_freq_khz>> __clocksource_update_freq_scale(cs, 1000, khz);
+ *   - kernel/time/clocksource.c|1202| <<__clocksource_register_scale>> __clocksource_update_freq_scale(cs, scale, freq);
+ */
 void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq)
 {
 	u64 sec;
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 266d02809..5db9424b3 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -937,6 +937,45 @@ ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
 }
 EXPORT_SYMBOL_GPL(ktime_mono_to_any);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2302| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|545| <<intel_dp_hdcp2_read_msg>> msg_end = ktime_add_ms(ktime_get_raw(),
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|562| <<intel_dp_hdcp2_read_msg>> msg_expired = ktime_after(ktime_get_raw(), msg_end);
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|78| <<live_nop_switch>> times[0] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|103| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|114| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|158| <<live_nop_switch>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/gt/intel_rps.c|1755| <<vlv_c0_read>> ei->ktime = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3749| <<nop_virtual_engine>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3804| <<nop_virtual_engine>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/i915_pmu.c|191| <<ktime_since_raw>> return ktime_to_ns(ktime_sub(ktime_get_raw(), kt));
+ *   - drivers/gpu/drm/i915/i915_pmu.c|267| <<init_rc6>> pmu->sleep_last[i] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_pmu.c|277| <<park_rc6>> pmu->sleep_last[gt->info.id] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_utils.h|262| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/i915/i915_utils.h|267| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|593| <<live_nop_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|621| <<live_nop_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1077| <<live_empty_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1090| <<live_empty_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|312| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|317| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|851| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|856| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/ptp/ptp_clockmatrix.c|438| <<_idtcm_gettime>> idtcm->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|750| <<_idtcm_set_dpll_hw_tod>> ktime_t diff = ktime_sub(ktime_get_raw(),
+ *   - drivers/ptp/ptp_clockmatrix.c|1006| <<set_tod_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|1013| <<set_tod_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|393| <<_idt82p33_gettime>> idt82p33->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|433| <<_idt82p33_settime>> dynamic_overhead_ns = ktime_to_ns(ktime_get_raw())
+ *   - drivers/ptp/ptp_idt82p33.c|653| <<idt82p33_measure_one_byte_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|658| <<idt82p33_measure_one_byte_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|686| <<idt82p33_measure_one_byte_read_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|691| <<idt82p33_measure_one_byte_read_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|719| <<idt82p33_measure_tod_write_9_byte_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|730| <<idt82p33_measure_tod_write_9_byte_overhead>> stop = ktime_get_raw();
+ *   - include/linux/timekeeping.h|174| <<ktime_get_raw_ns>> return ktime_to_ns(ktime_get_raw());
+ */
 /**
  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
  */
diff --git a/tools/testing/selftests/kvm/include/x86_64/processor.h b/tools/testing/selftests/kvm/include/x86_64/processor.h
index aa434c8f1..54a397550 100644
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@ -957,6 +957,15 @@ static inline int __vcpu_set_cpuid(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|752| <<vcpu_init_cpuid>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|760| <<vcpu_set_cpuid_maxphyaddr>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|771| <<vcpu_clear_cpuid_entry>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|789| <<vcpu_set_or_clear_cpuid_feature>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_features.c|486| <<guest_test_msrs_access>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_features.c|657| <<guest_test_hcalls_access>> vcpu_set_cpuid(vcpu);
+ */
 static inline void vcpu_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	TEST_ASSERT(vcpu->cpuid, "Must do vcpu_init_cpuid() first");
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 89912a17f..4c7d3c4bd 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -776,6 +776,24 @@ ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 	return _val == p->datamatch;
 }
 
+/*
+ * arm旧的例子
+ *
+ *  => ioeventfd_write
+ *  => __kvm_io_bus_write
+ *  => kvm_io_bus_write
+ *  => io_mem_abort
+ *  => kvm_handle_guest_abort
+ *  => handle_exit
+ *  => kvm_arch_vcpu_ioctl_run
+ *  => kvm_vcpu_ioctl
+ *  => do_vfs_ioctl
+ *  => ksys_ioctl
+ *  => __arm64_sys_ioctl
+ *  => el0_svc_common
+ *  => el0_svc_handler
+ *  => el0_svc
+ */
 /* MMIO/PIO writes trigger an event if the addr/val match */
 static int
 ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..3b24e782f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -85,6 +85,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 5bbb5612b..d4c85cb27 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -3353,6 +3353,15 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|922| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|1173| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4992| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/x86/kvm/x86.c|11710| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ */
 void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3414,6 +3423,11 @@ static void shrink_halt_poll_ns(struct kvm_vcpu *vcpu)
 	trace_kvm_halt_poll_ns_shrink(vcpu->vcpu_id, val, old);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3466| <<kvm_vcpu_block>> if (kvm_vcpu_check_block(vcpu) < 0)
+ *   - virt/kvm/kvm_main.c|3548| <<kvm_vcpu_halt>> if (kvm_vcpu_check_block(vcpu) < 0)
+ */
 static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
 {
 	int ret = -EINTR;
@@ -3439,6 +3453,12 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11508| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|11727| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ *   - virt/kvm/kvm_main.c|3555| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -3517,6 +3537,19 @@ static unsigned int kvm_vcpu_max_halt_poll_ns(struct kvm_vcpu *vcpu)
  * expensive block+unblock sequence if a wake event arrives soon after the vCPU
  * is halted.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|731| <<kvm_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/mips/kvm/emulate.c|955| <<kvm_mips_emul_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|501| <<kvmppc_set_msr_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|395| <<kvmppc_h_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/booke.c|724| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|240| <<kvmppc_kvm_pv>> kvm_vcpu_halt(vcpu);
+ *   - arch/riscv/kvm/vcpu_insn.c|192| <<kvm_riscv_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1339| <<kvm_s390_handle_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/x86.c|11506| <<vcpu_block>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/xen.c|1299| <<kvm_xen_schedop_poll>> kvm_vcpu_halt(vcpu);
+ */
 void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 {
 	unsigned int max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
@@ -3536,6 +3569,11 @@ void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 		ktime_t stop = ktime_add_ns(start, vcpu->halt_poll_ns);
 
 		do {
+			/*
+			 * called by:
+			 *   - virt/kvm/kvm_main.c|3466| <<kvm_vcpu_block>> if (kvm_vcpu_check_block(vcpu) < 0)
+			 *   - virt/kvm/kvm_main.c|3548| <<kvm_vcpu_halt>> if (kvm_vcpu_check_block(vcpu) < 0)
+			 */
 			if (kvm_vcpu_check_block(vcpu) < 0)
 				goto out;
 			cpu_relax();
@@ -3903,6 +3941,10 @@ static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 /*
  * Creates some virtual cpus.  Good luck creating more than one.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4765| <<kvm_vm_ioctl(KVM_CREATE_VCPU)>> r = kvm_vm_ioctl_create_vcpu(kvm, arg);
+ */
 static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 {
 	int r;
@@ -5397,6 +5439,11 @@ static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 	return off;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5490| <<kvm_io_bus_write>> r = __kvm_io_bus_write(vcpu, bus, &range, val);
+ *   - virt/kvm/kvm_main.c|5522| <<kvm_io_bus_write_cookie>> return __kvm_io_bus_write(vcpu, bus, &range, val);
+ */
 static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 			      struct kvm_io_range *range, const void *val)
 {
@@ -5417,6 +5464,18 @@ static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 	return -EOPNOTSUPP;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmio.c|166| <<io_mem_abort>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
+ *   - arch/mips/kvm/emulate.c|1252| <<kvm_mips_emulate_store>> r = kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/powerpc/kvm/book3s.c|1015| <<kvmppc_h_logical_ci_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|447| <<kvmppc_hv_emulate_mmio>> ret = kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, (gpa_t) gpa, 0,
+ *   - arch/powerpc/kvm/powerpc.c|1388| <<kvmppc_handle_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
+ *   - arch/riscv/kvm/vcpu_insn.c|681| <<kvm_riscv_vcpu_mmio_store>> if (!kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/vmx/vmx.c|5805| <<handle_ept_misconfig>> !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
+ *   - arch/x86/kvm/x86.c|7711| <<vcpu_mmio_write>> && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))
+ *   - arch/x86/kvm/x86.c|8324| <<emulator_pio_in_out>> r = kvm_io_bus_write(vcpu, KVM_PIO_BUS, port, size, data);
+ */
 /* kvm_io_bus_write - called under kvm->slots_lock */
 int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val)
-- 
2.34.1

